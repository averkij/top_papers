
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 134 papers. August 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Август 2025</span> | <span id="title-articles-count">134 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-07.html">⬅️ <span id="prev-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-09.html">➡️ <span id="next-date">09.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Август 2025', 'en': 'August 2025', 'zh': '8月2025年'};
        let feedDateNext = {'ru': '09.2025', 'en': '09/2025', 'zh': '9月2025年'};
        let feedDatePrev = {'ru': '07.2025', 'en': '07/2025', 'zh': '7月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.05629', 'title': 'On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification', 'url': 'https://huggingface.co/papers/2508.05629', 'abstract': 'Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.', 'score': 95, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '50bf66dc29886a85', 'authors': ['Yongliang Wu', 'Yizhou Zhou', 'Zhou Ziheng', 'Yingzhe Peng', 'Xinyu Ye', 'Xinting Hu', 'Wenbo Zhu', 'Lu Qi', 'Ming-Hsuan Yang', 'Xu Yang'], 'affiliations': ['Independent Researcher', 'Nanyang Technological University', 'Shanghai Jiao Tong University', 'Southeast University', 'University of California, Berkeley', 'University of California, Los Angeles', 'University of California, Merced', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05629.jpg', 'data': {'categories': ['#optimization', '#rl', '#training'], 'emoji': '🚀', 'ru': {'title': 'Динамическая настройка: простой путь к лучшему обобщению языковых моделей', 'desc': 'Исследователи представили метод Динамической Тонкой Настройки (DFT) для улучшения обобщающей способности Больших Языковых Моделей (LLM). DFT динамически масштабирует градиенты, что позволяет преодолеть ограничения стандартной Контролируемой Тонкой Настройки (SFT). Эксперименты показали, что DFT превосходит SFT на различных сложных тестах и конкурентоспособен в задачах оффлайн обучения с подкреплением. Метод требует минимальных изменений в коде и основан на теоретическом анализе проблем SFT.'}, 'en': {'title': 'Dynamic Fine-Tuning: Elevating LLM Generalization with Smart Gradients', 'desc': 'This paper introduces Dynamic Fine-Tuning (DFT), a method that enhances the generalization of Large Language Models (LLMs) by adjusting gradient updates. The authors identify that traditional Supervised Fine-Tuning (SFT) can limit model performance due to its inherent reward structure. By dynamically rescaling the objective function based on token probabilities, DFT stabilizes the training process and leads to better outcomes on various benchmarks. The results indicate that DFT not only surpasses SFT but also performs competitively in offline reinforcement learning scenarios, making it a valuable advancement in model training techniques.'}, 'zh': {'title': '动态微调，提升模型泛化能力！', 'desc': '动态微调（DFT）通过动态调整梯度的缩放，提升了大型语言模型（LLM）的泛化能力。与标准的监督微调（SFT）相比，DFT在多个挑战性基准测试中表现更优，且在离线强化学习中也展现出竞争力。通过数学分析，我们发现标准SFT的梯度隐含了一个有问题的奖励结构，限制了模型的泛化能力。DFT通过根据每个token的概率动态调整目标函数，稳定了梯度更新，从而显著改善了模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2508.05004', 'title': 'R-Zero: Self-Evolving Reasoning LLM from Zero Data', 'url': 'https://huggingface.co/papers/2508.05004', 'abstract': 'R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.', 'score': 80, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '4e0838dc787e59cf', 'authors': ['Chengsong Huang', 'Wenhao Yu', 'Xiaoyang Wang', 'Hongming Zhang', 'Zongxia Li', 'Ruosen Li', 'Jiaxin Huang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Seattle Lab', 'The University of Texas at Dallas', 'University of Maryland, College Park', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2508.05004.jpg', 'data': {'categories': ['#agents', '#rl', '#optimization', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самообучающиеся ИИ: путь к сверхразуму без участия человека', 'desc': 'R-Zero - это самоэволюционирующая система, которая автономно генерирует обучающие данные для улучшения способностей к рассуждению у больших языковых моделей (LLM). Система состоит из двух моделей - Challenger и Solver, которые взаимодействуют друг с другом, создавая и решая все более сложные задачи. R-Zero не требует заранее подготовленных человеком заданий и меток, что позволяет преодолеть ограничения существующих методов обучения LLM. Эксперименты показали значительное улучшение способностей к рассуждению у различных базовых LLM после применения R-Zero.'}, 'en': {'title': 'Autonomous Learning for Super-Intelligent AI', 'desc': "R-Zero is an innovative framework that enables Large Language Models (LLMs) to autonomously create and learn from their own training data, eliminating the need for human-curated tasks. It consists of two models, a Challenger and a Solver, which interact and evolve together; the Challenger proposes tasks that push the Solver's limits, while the Solver learns to tackle these challenges. This self-improving process generates a focused curriculum that enhances reasoning capabilities without relying on pre-existing labels or tasks. Empirical results show that R-Zero significantly boosts the performance of various LLMs on reasoning benchmarks, demonstrating its potential to advance AI systems beyond human intelligence."}, 'zh': {'title': '自我进化的智能框架，超越人类智能的未来', 'desc': 'R-Zero是一个自我进化的框架，能够自主生成和学习自己的训练数据，从而提升大型语言模型（LLMs）的推理能力，而无需人工策划的任务。该框架通过初始化两个独立的模型——挑战者和解决者，来实现模型的共同进化。挑战者负责提出接近解决者能力边界的任务，而解决者则专注于解决这些日益复杂的任务。通过这种互动，R-Zero能够在没有预先存在的任务和标签的情况下，生成一个有针对性的自我提升课程，显著提高了不同基础LLMs的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2508.05635', 'title': 'Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation', 'url': 'https://huggingface.co/papers/2508.05635', 'abstract': 'Genie Envisioner integrates policy learning, evaluation, and simulation using a video diffusion model and neural simulator for instruction-driven robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.', 'score': 62, 'issue_id': 5243, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'f4ca777a8500b711', 'authors': ['Yue Liao', 'Pengfei Zhou', 'Siyuan Huang', 'Donglin Yang', 'Shengcong Chen', 'Yuxin Jiang', 'Yue Hu', 'Jingbin Cai', 'Si Liu', 'Jianlan Luo', 'Liliang Chen', 'Shuicheng Yan', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['BUAA', 'LV-Lab', 'NUS', 'Unified World Foundation'], 'pdf_title_img': 'assets/pdf/title_img/2508.05635.jpg', 'data': {'categories': ['#video', '#benchmark', '#training', '#optimization', '#robotics', '#agi', '#open_source', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Единая платформа для обучения роботов на основе видео-генерации', 'desc': 'Genie Envisioner (GE) представляет собой унифицированную платформу для роботизированных манипуляций, объединяющую обучение политик, оценку и симуляцию в рамках единой видео-генеративной структуры. В основе GE лежит крупномасштабная модель диффузии видео, обусловленная инструкциями, которая фиксирует пространственную, временную и семантическую динамику реальных роботизированных взаимодействий в структурированном латентном пространстве. GE-Act преобразует латентные представления в исполняемые траектории действий, а GE-Sim служит нейронным симулятором для создания высококачественных развертываний. Платформа также включает EWMBench - набор стандартизированных тестов для оценки визуальной точности, физической согласованности и соответствия инструкций действиям.'}, 'en': {'title': 'Unified Framework for Instruction-Driven Robotic Manipulation', 'desc': 'Genie Envisioner (GE) is a comprehensive platform designed for robotic manipulation that combines policy learning, evaluation, and simulation into one framework. It utilizes a video diffusion model to understand and generate realistic robotic interactions based on instructions. The system includes a decoder that translates learned representations into actionable movements, allowing robots to perform tasks with minimal guidance. Additionally, it features a neural simulator for testing and refining policies, along with a benchmark suite to evaluate performance across various criteria.'}, 'zh': {'title': 'Genie Envisioner：指令驱动的机器人智能新平台', 'desc': 'Genie Envisioner（GE）是一个集成了策略学习、评估和模拟的机器人操作平台。它使用一个大型的、基于指令的视频扩散模型，能够捕捉现实世界中机器人交互的空间、时间和语义动态。GE-Act通过轻量级的解码器将潜在表示映射到可执行的动作轨迹，实现了在不同环境中精确且可推广的策略推断。GE-Sim作为一个神经模拟器，支持高保真度的闭环策略开发，整个系统为指令驱动的通用智能提供了可扩展的基础。'}}}, {'id': 'https://huggingface.co/papers/2508.05405', 'title': 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning', 'url': 'https://huggingface.co/papers/2508.05405', 'abstract': "DeepPHY evaluates Vision Language Models' physical reasoning and control through simulated environments with varying difficulty levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.", 'score': 54, 'issue_id': 5243, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '3ec0b6b2d584612a', 'authors': ['Xinrun Xu', 'Pi Bu', 'Ye Wang', 'Börje F. Karlsson', 'Ziming Wang', 'Tengtao Song', 'Qi Zhu', 'Jun Song', 'Zhiming Ding', 'Bo Zheng'], 'affiliations': ['Informatics Department, PUC-Rio', 'Institute of Software, Chinese Academy of Science', 'Renmin University of China', 'Taobao & Tmall Group of Alibaba', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.05405.jpg', 'data': {'categories': ['#games', '#benchmark', '#reasoning', '#cv'], 'emoji': '🧠', 'ru': {'title': 'DeepPHY: проверка физического интеллекта ИИ в виртуальных мирах', 'desc': 'DeepPHY - это новая система оценки способностей визуально-языковых моделей (VLM) к физическому рассуждению и управлению в симулированных средах. Она включает в себя различные уровни сложности и детальные метрики оценки. Исследование показало, что даже современные VLM испытывают трудности с применением описательных физических знаний для точного прогнозирующего контроля. DeepPHY помогает оценить понимание моделями фундаментальных физических принципов в сложных динамических средах.'}, 'en': {'title': 'DeepPHY: Bridging the Gap in Physical Reasoning for Vision Language Models', 'desc': 'DeepPHY is a benchmark framework that assesses Vision Language Models (VLMs) on their ability to understand and apply physical reasoning in simulated environments. It highlights the challenges VLMs face in executing precise actions and planning in complex scenarios, which are essential for real-world tasks. The framework includes various environments with different difficulty levels and uses detailed metrics for evaluation. Results show that even advanced VLMs have difficulty converting their understanding of physical concepts into accurate control actions.'}, 'zh': {'title': 'DeepPHY：评估视觉语言模型的物理推理能力', 'desc': 'DeepPHY是一个新颖的基准框架，用于评估视觉语言模型（VLMs）在物理推理和控制方面的能力。该框架通过一系列具有不同难度的模拟环境，系统地测试VLMs对基本物理原理的理解和推理能力。尽管当前的VLMs在感知和视觉推理方面表现出色，但在复杂动态环境中，它们在细节关注和精确行动规划方面仍然存在不足。我们的评估结果显示，即使是最先进的VLMs也难以将描述性的物理知识转化为精确的预测控制。'}}}, {'id': 'https://huggingface.co/papers/2508.05609', 'title': 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity', 'url': 'https://huggingface.co/papers/2508.05609', 'abstract': 'Hi3DEval is a hierarchical evaluation framework for 3D generative content that combines object-level and part-level assessments, including material realism, using a large-scale dataset and hybrid 3D representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.', 'score': 25, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '97d6454893bee33c', 'authors': ['Yuhan Zhang', 'Long Zhuo', 'Ziyang Chu', 'Tong Wu', 'Zhibing Li', 'Liang Pan', 'Dahua Lin', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05609.jpg', 'data': {'categories': ['#3d', '#benchmark', '#optimization', '#games', '#dataset'], 'emoji': '🧊', 'ru': {'title': 'Новый стандарт оценки 3D-генерации: от общего к частному', 'desc': 'Hi3DEval - это иерархическая система оценки 3D-генеративного контента, сочетающая оценку на уровне объектов и частей. Она включает оценку реалистичности материалов, используя крупномасштабный набор данных и гибридные 3D-представления. Система позволяет проводить как целостную оценку по нескольким параметрам, так и детальный анализ качества. Hi3DEval использует видео-представления для оценки на уровне объектов и материалов, а также предобученные 3D-признаки для восприятия на уровне частей.'}, 'en': {'title': 'Revolutionizing 3D Content Evaluation with Hi3DEval', 'desc': 'Hi3DEval is a new framework designed to evaluate 3D generative content by assessing both the overall object and its individual parts. It addresses the limitations of current methods that only use image-based metrics, which often miss important details like spatial coherence and material realism. The framework includes a large dataset called Hi3DBench, which features a variety of 3D assets and detailed annotations to support comprehensive evaluations. By utilizing advanced scoring systems and hybrid 3D representations, Hi3DEval provides a more accurate and scalable way to assess the quality of 3D generated content.'}, 'zh': {'title': '3D生成内容的分层评估新框架', 'desc': 'Hi3DEval是一个针对3D生成内容的分层评估框架，结合了对象级和部分级的评估，包括材料真实感。现有的方法主要依赖于基于图像的指标，仅在对象级别进行评估，无法有效捕捉空间一致性和材料真实性。为了解决这些问题，Hi3DEval提供了多维度的整体评估和细致的质量分析，并扩展了纹理评估，关注如反射率、饱和度和金属感等属性。通过构建Hi3DBench数据集和3D感知的自动评分系统，Hi3DEval在建模3D特性方面超越了现有的图像基准，提供了可扩展的评估替代方案。'}}}, {'id': 'https://huggingface.co/papers/2508.03644', 'title': 'Are We on the Right Way for Assessing Document Retrieval-Augmented\n  Generation?', 'url': 'https://huggingface.co/papers/2508.03644', 'abstract': 'Double-Bench is a large-scale, multilingual, and multimodal evaluation system for document Retrieval-Augmented Generation (RAG) systems, addressing limitations in current benchmarks and providing comprehensive assessments of system components.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.', 'score': 19, 'issue_id': 5243, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'b8c44e363c76888f', 'authors': ['Wenxuan Shen', 'Mingjia Wang', 'Yaochen Wang', 'Dongping Chen', 'Junjie Yang', 'Yao Wan', 'Weiwei Lin'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2508.03644.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#survey', '#rag', '#open_source', '#multilingual'], 'emoji': '📊', 'ru': {'title': 'Double-Bench: комплексная оценка RAG систем для документов', 'desc': 'Double-Bench - это новая система оценки для Retrieval-Augmented Generation (RAG) систем, работающих с документами. Она включает в себя 3276 документов и 5168 запросов на 6 языках, охватывая 4 типа документов. Система позволяет проводить детальную оценку каждого компонента RAG систем, включая мультимодальные языковые модели (MLLM) и модели эмбеддингов. Эксперименты показали, что разрыв между текстовыми и визуальными моделями эмбеддингов сокращается, а также выявили проблему излишней уверенности современных RAG систем.'}, 'en': {'title': 'Double-Bench: Elevating RAG Evaluation for Real-World Challenges', 'desc': 'Double-Bench is a new evaluation system designed to improve the assessment of Retrieval-Augmented Generation (RAG) systems, which combine document retrieval and generation. It addresses the shortcomings of existing benchmarks by providing a large-scale, multilingual, and multimodal dataset that includes 3,276 documents and 5,168 queries across multiple languages and document types. The evaluation focuses on fine-grained assessments of each component in RAG systems, ensuring that queries are based on thoroughly verified evidence. Our experiments reveal important insights into the performance of various models and highlight the need for better document retrieval capabilities in the face of over-confidence in current frameworks.'}, 'zh': {'title': '双重基准：提升文档RAG系统评估的全新标准', 'desc': 'Double-Bench是一个大规模的多语言多模态评估系统，专门用于文档增强生成（RAG）系统的评估。它解决了当前基准测试的局限性，能够对RAG系统的各个组件进行全面的评估。该系统包含3276份文档和5168个查询，涵盖6种语言和4种文档类型，确保评估的质量和完整性。我们的实验表明，文本和视觉嵌入模型之间的差距正在缩小，同时也揭示了当前RAG框架中存在的过度自信问题。'}}}, {'id': 'https://huggingface.co/papers/2508.03990', 'title': "Are Today's LLMs Ready to Explain Well-Being Concepts?", 'url': 'https://huggingface.co/papers/2508.03990', 'abstract': 'LLMs can be fine-tuned to generate high-quality, audience-tailored explanations of well-being concepts using Supervised Fine-Tuning and Direct Preference Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.', 'score': 18, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'a742f57af42990c1', 'authors': ['Bohan Jiang', 'Dawei Li', 'Zhen Tan', 'Chengshuai Zhao', 'Huan Liu'], 'affiliations': ['School of Computing and Augmented Intelligence, Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.03990.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#dataset', '#alignment', '#open_source', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели учатся объяснять концепции благополучия', 'desc': 'Исследование демонстрирует, что модели машинного обучения можно настроить для генерации высококачественных объяснений концепций благополучия, адаптированных под разную аудиторию. Авторы создали большой датасет из 43 880 объяснений 2 194 концепций благополучия, сгенерированных десятью различными языковыми моделями. Они разработали систему оценки качества объяснений с использованием языковых моделей в роли судей. Результаты показывают, что модели, настроенные с помощью методов Supervised Fine-Tuning и Direct Preference Optimization, превосходят более крупные базовые модели в задаче генерации специализированных объяснений.'}, 'en': {'title': 'Tailoring Well-Being Explanations with Fine-Tuned LLMs', 'desc': 'This paper discusses how Large Language Models (LLMs) can be improved to provide better explanations of well-being concepts tailored to different audiences. It highlights the importance of both factual accuracy and audience-specific needs in generating high-quality explanations. The authors created a large dataset of explanations and developed a unique evaluation framework using LLMs as judges to assess the quality of these explanations. Their findings show that fine-tuning LLMs with Supervised Fine-Tuning and Direct Preference Optimization leads to significant improvements in explanation quality compared to larger, unrefined models.'}, 'zh': {'title': '提升幸福感解释质量的智能方法', 'desc': '本研究探讨了如何通过监督微调（SFT）和直接偏好优化（DPO）来提升大型语言模型（LLMs）生成的关于幸福感概念的解释质量。我们构建了一个包含43,880个解释的大型数据集，涵盖2,194个幸福感概念，并引入了一个基于原则的LLM评估框架。研究结果表明，经过微调的模型在生成解释时的质量显著提高，且与人类评估结果高度一致。不同模型、受众和类别之间的解释质量差异明显，表明偏好学习在专业解释任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.04017', 'title': 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A\n  Systematic Evaluation Framework of Their Input Scrutiny Ability', 'url': 'https://huggingface.co/papers/2508.04017', 'abstract': "ISEval framework evaluates large multimodal models' ability to detect flawed inputs, revealing challenges in identifying certain types of errors and modality-specific biases.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing formidable capabilities in handling intricate multimodal tasks with exceptional performance. Recent research has underscored the inclination of large language models to passively accept defective inputs, often resulting in futile reasoning on invalid prompts. However, the same critical question of whether LMMs can actively detect and scrutinize erroneous inputs still remains unexplored. To address this gap, we introduce the Input Scrutiny Ability Evaluation Framework (ISEval), which encompasses seven categories of flawed premises and three evaluation metrics. Our extensive evaluation of ten advanced LMMs has identified key findings. Most models struggle to actively detect flawed textual premises without guidance, which reflects a strong reliance on explicit prompts for premise error identification. Error type affects performance: models excel at identifying logical fallacies but struggle with surface-level linguistic errors and certain conditional flaws. Modality trust varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info, while aya-vision-8b over-rely on text in conflicts. These insights underscore the urgent need to enhance LMMs' proactive verification of input validity and shed novel insights into mitigating the problem. The code is available at https://github.com/MLGroupJLU/LMM_ISEval.", 'score': 9, 'issue_id': 5243, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '37794107e7cbe332', 'authors': ['Haiqi Yang', 'Jinzhe Li', 'Gengxu Li', 'Yi Chang', 'Yuan Wu'], 'affiliations': ['Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China', 'International Center of Future Science, Jilin University', 'School of Artificial Intelligence, Jilin University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04017.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#ethics', '#hallucinations', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Проверка на прочность: как мультимодальные ИИ справляются с ошибками во входных данных', 'desc': 'Фреймворк ISEval оценивает способность больших мультимодальных моделей обнаруживать ошибочные входные данные. Исследование выявило, что большинство моделей испытывают трудности с активным обнаружением текстовых ошибок без специальных указаний. Производительность зависит от типа ошибки: модели хорошо справляются с логическими ошибками, но затрудняются с поверхностными лингвистическими и некоторыми условными ошибками. Обнаружены различия в доверии к модальностям: некоторые модели сбалансированно используют визуальную и текстовую информацию, в то время как другие чрезмерно полагаются на текст при конфликтах.'}, 'en': {'title': 'Enhancing Input Validation in Large Multimodal Models', 'desc': 'The ISEval framework assesses the ability of large multimodal models (LMMs) to identify flawed inputs, highlighting their challenges in recognizing specific errors and biases related to different modalities. Despite their impressive performance in multimodal tasks, many LMMs tend to accept defective inputs without questioning them, leading to ineffective reasoning. The framework categorizes seven types of flawed premises and employs three evaluation metrics to analyze ten advanced LMMs, revealing that most struggle to detect errors without explicit prompts. The findings indicate that while models perform well in identifying logical fallacies, they face difficulties with linguistic errors and exhibit varying trust in different modalities, emphasizing the need for improved input validation mechanisms.'}, 'zh': {'title': '提升多模态模型的输入验证能力', 'desc': 'ISEval框架评估大型多模态模型检测缺陷输入的能力，揭示了识别某些类型错误和特定模态偏见的挑战。研究表明，大型语言模型倾向于被动接受有缺陷的输入，导致在无效提示上进行无效推理。尽管如此，LMMs是否能够主动检测和审查错误输入的问题仍未得到充分探讨。我们的评估显示，大多数模型在没有指导的情况下难以主动识别文本前提的缺陷，强调了对明确提示的强烈依赖。'}}}, {'id': 'https://huggingface.co/papers/2508.03923', 'title': 'CoAct-1: Computer-using Agents with Coding as Actions', 'url': 'https://huggingface.co/papers/2508.03923', 'abstract': 'A multi-agent system that combines GUI control with programmatic execution improves efficiency and success in complex computer automation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. In this work, we introduce a more robust and flexible paradigm: enabling agents to use coding as a enhanced action. We present CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. We evaluate our system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, our approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation.', 'score': 9, 'issue_id': 5246, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '1527938782ab293a', 'authors': ['Linxin Song', 'Yutong Dai', 'Viraj Prabhu', 'Jieyu Zhang', 'Taiwei Shi', 'Li Li', 'Junnan Li', 'Silvio Savarese', 'Zeyuan Chen', 'Jieyu Zhao', 'Ran Xu', 'Caiming Xiong'], 'affiliations': ['Salesforce Research', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.03923.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Синергия GUI и кода: новый уровень компьютерной автоматизации', 'desc': 'Статья представляет CoAct-1 - многоагентную систему для автоматизации компьютерных задач, сочетающую управление через графический интерфейс и программное выполнение. Система включает Оркестратор, который делегирует подзадачи GUI-оператору или агенту-программисту, способному писать и выполнять скрипты. CoAct-1 достигает наилучших результатов на бенчмарке OSWorld, значительно превосходя предыдущие методы по успешности и эффективности. Интеграция программирования как основного действия обеспечивает более мощный и масштабируемый подход к автоматизации компьютера.'}, 'en': {'title': 'Empowering Automation: Merging GUI Control with Programmatic Power', 'desc': 'This paper presents CoAct-1, a multi-agent system that enhances computer automation by combining GUI control with programmatic execution. Traditional agents often face challenges with efficiency and reliability when performing complex tasks solely through GUI manipulation. CoAct-1 introduces an Orchestrator that assigns subtasks to either a GUI Operator or a Programmer agent capable of executing scripts, allowing for a more flexible approach. The system achieves a state-of-the-art success rate of 60.76% on the OSWorld benchmark, demonstrating significant improvements in both efficiency and task completion speed.'}, 'zh': {'title': '结合编程与GUI，提升自动化效率！', 'desc': '本论文介绍了一种新的多智能体系统CoAct-1，它结合了图形用户界面（GUI）控制和程序化执行，以提高复杂计算机自动化任务的效率和成功率。传统的基于GUI的智能体在处理复杂任务时效率低下，而CoAct-1通过引入编程作为增强动作，克服了这一限制。该系统的协调者动态分配子任务给GUI操作员或专门的程序员智能体，从而实现更灵活的任务处理。实验结果表明，CoAct-1在OSWorld基准测试中取得了60.76%的成功率，显著优于之前的方法，并且平均完成任务所需的步骤减少到10.15步。'}}}, {'id': 'https://huggingface.co/papers/2508.02120', 'title': "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning\n  Models", 'url': 'https://huggingface.co/papers/2508.02120', 'abstract': 'Research on efficient reasoning methods for Large Reasoning Models (LRMs) aims to reduce reasoning path length without sacrificing performance, through single-model optimization and model collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods.', 'score': 7, 'issue_id': 5243, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'ab1d76b8f9fbefe8', 'authors': ['Linan Yue', 'Yichao Du', 'Yizhi Wang', 'Weibo Gao', 'Fangzhou Yao', 'Li Wang', 'Ye Liu', 'Ziyu Xu', 'Qi Liu', 'Shimin Di', 'Min-Ling Zhang'], 'affiliations': ['Alibaba Group', 'Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education', 'School of Computer Science and Engineering, Southeast University', 'University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2508.02120.jpg', 'data': {'categories': ['#reasoning', '#training', '#open_source', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение в LRM: сокращение пути без потери качества', 'desc': 'Исследование эффективных методов рассуждения для Больших Моделей Рассуждения (LRM) направлено на сокращение длины цепочек рассуждений без ущерба для производительности. Оно включает оптимизацию отдельных моделей и сотрудничество между моделями. Проблема чрезмерного мышления в LRM приводит к избыточно длинным цепочкам рассуждений, что снижает эффективность. Предлагаемые методы эффективного рассуждения разделяются на две категории: оптимизация одной модели и сотрудничество нескольких моделей.'}, 'en': {'title': 'Streamlining Reasoning: Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the development of efficient reasoning methods for Large Reasoning Models (LRMs), which are designed to improve logical deduction and decision-making. It highlights the challenges posed by overly long reasoning paths that can hinder performance and accuracy. The authors categorize existing research into two main approaches: optimizing single models for better reasoning efficiency and enhancing collaboration between multiple models. Additionally, they provide a public GitHub repository to share ongoing advancements in this area.'}, 'zh': {'title': '高效推理：提升大型推理模型的智能决策能力', 'desc': '本研究探讨了大型推理模型（LRMs）中高效推理方法的研究，旨在在不牺牲性能的情况下减少推理路径的长度。研究中提到的DeepSeek R1模型因其卓越的表现和开源特性而受到关注，推动了R1风格LRMs的研究进展。与传统的大型语言模型（LLMs）不同，这些模型通过长链推理和自我反思等机制增强了逻辑推理和决策能力。然而，随着应用的广泛，过度推理的问题逐渐显现，导致推理效率降低，因此提出了多种高效推理方法以优化推理过程。'}}}, {'id': 'https://huggingface.co/papers/2508.02038', 'title': 'Marco-Voice Technical Report', 'url': 'https://huggingface.co/papers/2508.02038', 'abstract': 'A multifunctional speech synthesis system integrates voice cloning and emotion control using speaker-emotion disentanglement and rotational emotional embeddings, achieving high expressive and natural speech.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis.', 'score': 7, 'issue_id': 5247, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'a3fa663a271be79f', 'authors': ['Fengping Tian', 'Chenyang Lyu', 'Xuanfan Ni', 'Haoqin Sun', 'Qingjuan Li', 'Zhiqiang Qian', 'Haijun Li', 'Longyue Wang', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2508.02038.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: естественные эмоции и клонирование голоса', 'desc': 'Эта статья представляет многофункциональную систему синтеза речи, объединяющую клонирование голоса и управление эмоциями в единой структуре. Система использует механизм разделения характеристик диктора и эмоций с помощью контрастного обучения, а также метод ротационных эмоциональных эмбеддингов для плавного контроля эмоций. Авторы создали датасет CSEMOTIONS для обучения и оценки модели, содержащий 10 часов эмоциональной речи на мандаринском диалекте. Эксперименты показали, что предложенная система Marco-Voice значительно превосходит существующие методы по объективным и субъективным метрикам.'}, 'en': {'title': 'Expressive Speech Synthesis with Emotion Control', 'desc': "This paper introduces a new speech synthesis system called Marco-Voice that combines voice cloning with emotion control. It uses a technique called speaker-emotion disentanglement to separate the speaker's identity from their emotional expression, allowing for more flexible and natural speech generation. The system incorporates rotational emotional embeddings to enable smooth transitions between different emotions. A new dataset, CSEMOTIONS, was created to train and evaluate the system, showing significant improvements in speech clarity and emotional depth compared to existing methods."}, 'zh': {'title': '多功能语音合成：声音与情感的完美结合', 'desc': '这篇论文介绍了一种多功能语音合成系统，结合了声音克隆和情感控制。该系统通过说话者-情感解耦机制和旋转情感嵌入方法，实现了高表现力和自然的语音生成。研究中构建了CSEMOTIONS数据集，包含六位专业说话者的十小时普通话情感语音。实验结果表明，Marco-Voice在语音清晰度和情感丰富性方面显著提升，代表了表达性神经语音合成领域的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2508.04423', 'title': 'Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation', 'url': 'https://huggingface.co/papers/2508.04423', 'abstract': 'A structured framework and datasets for training customer service agents using well-defined support strategies improve the quality of customer support interactions and problem resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.', 'score': 6, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'd1f778b32418973c', 'authors': ['Jie Zhu', 'Huaixia Dou', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04423.jpg', 'data': {'categories': ['#data', '#agents', '#science', '#dataset', '#open_source', '#training'], 'emoji': '🎭', 'ru': {'title': 'Структурированное обучение агентов поддержки для повышения качества обслуживания клиентов', 'desc': 'Статья представляет структурированный подход к обучению агентов службы поддержки клиентов, основанный на четко определенных стратегиях. Авторы предлагают framework для задачи Customer Support Conversation (CSC), включающий пять этапов разговора и двенадцать стратегий. Они создают два датасета: CSConv для оценки и RoleCS для обучения, используя реальные диалоги и симуляцию с помощью языковых моделей. Эксперименты показывают, что fine-tuning LLM на RoleCS значительно улучшает качество ответов и решение проблем клиентов.'}, 'en': {'title': 'Empowering Customer Support with Strategic Conversations', 'desc': 'This paper presents a structured framework for training customer service agents, focusing on effective communication and problem resolution. It introduces the Customer Support Conversation (CSC) task, which utilizes well-defined support strategies based on COPC guidelines. The authors create a dataset called CSConv, consisting of real-world conversations rewritten to reflect strategic communication, and a training dataset called RoleCS that simulates these interactions. Experiments demonstrate that fine-tuning large language models (LLMs) on RoleCS enhances their ability to produce high-quality, strategy-aligned responses, leading to improved customer support outcomes.'}, 'zh': {'title': '提升客户支持质量的结构化框架', 'desc': '本文提出了一种结构化框架和数据集，用于训练客户服务代理，采用明确的支持策略以提高客户支持互动的质量和问题解决能力。我们引入了客户支持对话（CSC）任务，旨在帮助客服代理使用定义良好的支持策略进行响应。基于COPC指南，我们定义了五个对话阶段和十二种策略，以指导高质量的互动。通过构建CSConv数据集和RoleCS训练数据集，实验表明在RoleCS上微调强大的大语言模型（LLM）显著提升了其在CSConv上的策略一致性响应能力。'}}}, {'id': 'https://huggingface.co/papers/2508.05496', 'title': 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities', 'url': 'https://huggingface.co/papers/2508.05496', 'abstract': "InfiAlign, a scalable and sample-efficient post-training framework, combines supervised fine-tuning and Direct Preference Optimization to enhance large language models' reasoning abilities with minimal data and computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.", 'score': 5, 'issue_id': 5246, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '6dcac22b35678a50', 'authors': ['Shuo Cai', 'Su Lu', 'Qi Zhou', 'Kejing Yang', 'Zhijie Sang', 'Congkai Xie', 'Hongxia Yang'], 'affiliations': ['InfiX.ai', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05496.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#optimization', '#rlhf', '#training', '#math', '#data'], 'emoji': '🧠', 'ru': {'title': 'InfiAlign: эффективное улучшение рассуждений в больших языковых моделях', 'desc': 'InfiAlign - это масштабируемая и эффективная с точки зрения данных система пост-обучения для улучшения способностей больших языковых моделей к рассуждениям. Она объединяет supervised fine-tuning и Direct Preference Optimization для повышения производительности при минимальных затратах данных и вычислительных ресурсов. В основе InfiAlign лежит конвейер отбора высококачественных данных для выравнивания из открытых наборов данных по рассуждениям. Применение InfiAlign к модели Qwen2.5-Math-7B-Base позволило достичь результатов на уровне DeepSeek-R1-Distill-Qwen-7B, используя всего 12% обучающих данных.'}, 'en': {'title': 'Efficiently Enhancing Reasoning in Large Language Models', 'desc': 'InfiAlign is a new framework designed to improve the reasoning abilities of large language models (LLMs) while using less data and computational resources. It combines supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to enhance model performance efficiently. The framework includes a smart data selection process that curates high-quality training data from existing datasets, which helps achieve better results with significantly less data. When tested on the Qwen2.5-Math-7B-Base model, InfiAlign demonstrated strong performance improvements, especially in mathematical reasoning tasks, while using only about 12% of the usual training data.'}, 'zh': {'title': '高效对齐，提升推理能力的创新框架', 'desc': 'InfiAlign是一种可扩展且样本高效的后训练框架，旨在通过最小的数据和计算成本提升大型语言模型的推理能力。该框架结合了监督微调和直接偏好优化，能够有效地对模型进行对齐。InfiAlign的核心是一个强大的数据选择管道，自动从开源推理数据集中筛选高质量的对齐数据，从而显著提高性能并减少数据需求。通过在Qwen2.5-Math-7B-Base模型上的应用，InfiAlign展示了在多样化推理任务中的强大泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.05630', 'title': 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes', 'url': 'https://huggingface.co/papers/2508.05630', 'abstract': 'MOSEv2, a more challenging dataset, highlights the limitations of current VOS methods in real-world scenarios with increased complexity and diverse challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.', 'score': 4, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'a2a9fbee6a3ffe42', 'authors': ['Henghui Ding', 'Kaining Ying', 'Chang Liu', 'Shuting He', 'Xudong Jiang', 'Yu-Gang Jiang', 'Philip H. S. Torr', 'Song Bai'], 'affiliations': ['ByteDance Inc', 'Fudan University, Shanghai, China', 'Nanyang Technological University, Singapore', 'Shanghai University of Finance and Economics, China', 'University of Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2508.05630.jpg', 'data': {'categories': ['#dataset', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'MOSEv2: Новый вызов для алгоритмов сегментации объектов в видео', 'desc': 'MOSEv2 - это новый набор данных для задачи сегментации объектов в видео (VOS), созданный для преодоления ограничений существующих бенчмарков. Он содержит более 5000 видео с разнообразными сложными сценариями, включая исчезновение объектов, окклюзии, неблагоприятные погодные условия и другие реальные проблемы. Тестирование современных методов VOS на MOSEv2 показало значительное снижение их производительности по сравнению с предыдущими датасетами. Этот набор данных призван стимулировать развитие более надежных алгоритмов сегментации объектов для реальных задач.'}, 'en': {'title': 'MOSEv2: Elevating Video Object Segmentation to Real-World Challenges', 'desc': 'The paper introduces MOSEv2, a new dataset for video object segmentation (VOS) that presents more complex real-world challenges than previous datasets. While existing methods perform well on simpler benchmarks like DAVIS and YouTube-VOS, they struggle with the increased difficulties found in MOSEv2, which includes diverse scenarios such as occlusions, object disappearance, and adverse weather conditions. The dataset contains over 5,000 videos and nearly 702,000 high-quality masks for a wide variety of objects, making it a significant resource for advancing VOS research. Benchmarking shows that current VOS methods experience substantial performance drops when tested on MOSEv2, indicating a need for improved algorithms that can handle real-world complexities.'}, 'zh': {'title': 'MOSEv2：应对真实世界复杂性的挑战', 'desc': 'MOSEv2是一个更具挑战性的数据集，揭示了当前视频目标分割（VOS）方法在复杂真实场景中的局限性。尽管现有方法在DAVIS和YouTube-VOS等基准测试中表现出色，但这些数据集主要包含显著、主导和孤立的对象，限制了其在现实世界中的泛化能力。MOSEv2包含5024个视频和超过701976个高质量的掩膜，涵盖200个类别的10074个对象，场景复杂性显著增加。研究表明，尽管在现有数据集上准确率很高，但当前的VOS方法在面对真实世界的复杂性时仍然面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.01650', 'title': 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance', 'url': 'https://huggingface.co/papers/2508.01650', 'abstract': 'A sketch-based strand generation model using a learnable upsampling strategy and multi-scale adaptive conditioning mechanism outperforms existing methods in realism and precision for hair strand generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).', 'score': 4, 'issue_id': 5243, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '377e9e9eca3593db', 'authors': ['Na Zhang', 'Moran Li', 'Chengming Xu', 'Han Feng', 'Xiaobin Hu', 'Jiangning Zhang', 'Weijian Cao', 'Chengjie Wang', 'Yanwei Fu'], 'affiliations': ['Fudan University, Shanghai, China', 'School of Data Science, Fudan University, Shanghai Innovation Institute, Institute of Trustworthy Embodied AI, Fudan University', 'Tencent YouTu Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01650.jpg', 'data': {'categories': ['#benchmark', '#games', '#cv', '#3d', '#open_source', '#architecture'], 'emoji': '💇', 'ru': {'title': 'Реалистичная генерация прядей волос по эскизам с помощью ИИ', 'desc': 'Эта статья представляет первую модель генерации прядей волос на основе эскизов. Модель использует обучаемую стратегию апсемплинга прядей и многомасштабный адаптивный механизм кондиционирования с трансформером и диффузионными головками. Эксперименты показывают, что предложенный метод превосходит существующие подходы по реалистичности и точности генерации волос. Модель особенно полезна для приложений компьютерной графики и виртуальной реальности.'}, 'en': {'title': 'Revolutionizing Hair Strand Generation with Sketch-Based Precision', 'desc': 'This paper presents a novel sketch-based model for generating realistic hair strands, addressing the limitations of existing methods. The model utilizes a learnable upsampling strategy to effectively encode 3D hair strands into multi-scale latent spaces, enhancing detail and precision. Additionally, it incorporates a multi-scale adaptive conditioning mechanism that employs transformers with diffusion heads to maintain consistency across different levels of detail. Experimental results demonstrate that this approach significantly improves realism and precision in hair strand generation compared to traditional techniques.'}, 'zh': {'title': '草图驱动的发丝生成新方法', 'desc': '本文提出了一种基于草图的发丝生成模型，采用可学习的上采样策略和多尺度自适应条件机制，显著提高了发丝生成的真实感和精确度。该模型解决了复杂发丝交互和多样化草图模式建模的关键挑战。通过将3D发丝编码到多尺度潜在空间，模型实现了更细致的控制，同时保持用户友好性。实验结果表明，该方法在多个基准数据集上超越了现有技术，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.04979', 'title': 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast\n  Image Compression', 'url': 'https://huggingface.co/papers/2508.04979', 'abstract': 'SODEC, a single-step diffusion image compression model, enhances decoding speed and fidelity by using a pre-trained VAE and a fidelity guidance module.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20times. Code is released at: https://github.com/zhengchen1999/SODEC.', 'score': 3, 'issue_id': 5251, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '86b2291887bd3cbc', 'authors': ['Zheng Chen', 'Mingde Zhou', 'Jinpei Guo', 'Jiale Yuan', 'Yifei Ji', 'Yulun Zhang'], 'affiliations': ['Carnegie Mellon University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04979.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#open_source', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Быстрое и качественное сжатие изображений с помощью одношаговой диффузии', 'desc': 'SODEC - это новая модель сжатия изображений на основе одношаговой диффузии. Она использует предобученный вариационный автоэнкодер для создания информативных латентных представлений и заменяет итеративный процесс шумоподавления одношаговым декодированием. Модель включает модуль точности, который улучшает верность восстановленного изображения оригиналу. SODEC значительно превосходит существующие методы по соотношению скорость-искажение-восприятие и ускоряет декодирование в 20 раз по сравнению с предыдущими диффузионными моделями сжатия.'}, 'en': {'title': 'SODEC: Fast and Faithful Image Compression with Single-Step Diffusion', 'desc': 'SODEC is a new image compression model that uses a single-step diffusion process to enhance both decoding speed and image quality. It utilizes a pre-trained Variational Autoencoder (VAE) to generate informative latent representations, eliminating the need for multiple sampling steps. To further improve the fidelity of the compressed images, a fidelity guidance module is introduced, ensuring that the output closely resembles the original image. The model also incorporates a rate annealing training strategy, allowing it to perform effectively even at very low bitrates, resulting in significant improvements over existing methods.'}, 'zh': {'title': 'SODEC：快速高保真的图像压缩新方法', 'desc': 'SODEC是一种单步扩散图像压缩模型，通过使用预训练的变分自编码器（VAE）和保真度引导模块，提高了解码速度和图像质量。该模型解决了传统扩散图像压缩方法中存在的解码延迟过高和图像保真度不足的问题。SODEC利用信息丰富的潜在表示，避免了多步采样的复杂性，并通过单步解码来加速过程。同时，保真度引导模块确保输出图像与原始图像保持一致，从而提升了压缩效果。'}}}, {'id': 'https://huggingface.co/papers/2508.05618', 'title': 'Learning to Reason for Factuality', 'url': 'https://huggingface.co/papers/2508.05618', 'abstract': 'A novel reward function for online reinforcement learning improves factuality and detail in reasoning large language models without reducing helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness.', 'score': 2, 'issue_id': 5261, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '97c0924a378da917', 'authors': ['Xilun Chen', 'Ilia Kulikov', 'Vincent-Pierre Berges', 'Barlas Oğuz', 'Rulin Shao', 'Gargi Ghosh', 'Jason Weston', 'Wen-tau Yih'], 'affiliations': ['FAIR at Meta', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.05618.jpg', 'data': {'categories': ['#rl', '#rlhf', '#hallucinations', '#reasoning', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Повышение фактической точности языковых моделей без потери качества', 'desc': 'Эта статья представляет новую функцию вознаграждения для онлайн-обучения с подкреплением, которая улучшает фактическую точность и детализацию в рассуждениях больших языковых моделей. Авторы обнаружили, что прямое использование автоматических методов оценки фактической точности в качестве вознаграждения приводит к нежелательным эффектам. Предложенная функция вознаграждения учитывает фактическую точность, уровень детализации ответа и релевантность. Результаты показывают значительное снижение уровня галлюцинаций и увеличение детализации ответов без ухудшения общей полезности.'}, 'en': {'title': 'Enhancing Factuality and Detail in Language Models with Novel RL Rewards', 'desc': 'This paper introduces a new reward function for online reinforcement learning (RL) that enhances the factual accuracy and detail of reasoning in large language models (LLMs). The authors address the challenge of high hallucination rates in LLMs when generating long-form content, which often leads to inaccuracies. By developing a reward function that balances factual precision, detail, and relevance, they improve the quality of responses without sacrificing helpfulness. The proposed model shows significant improvements in factuality and detail across multiple benchmarks, demonstrating its effectiveness in reducing errors while maintaining user satisfaction.'}, 'zh': {'title': '提升推理模型的事实性与细节性', 'desc': '本文提出了一种新颖的奖励函数，用于在线强化学习，以提高大型语言模型的事实性和推理细节，而不降低其有用性。研究发现，现有的自动事实性评估框架在在线强化学习中直接作为奖励使用，会导致奖励黑客行为，影响生成的回答质量。我们提出的奖励函数同时考虑了事实精度、回答细节水平和答案相关性，从而有效地学习高质量的事实推理。经过六个长篇事实性基准的评估，我们的模型在幻觉率上平均降低了23.1个百分点，回答细节水平提高了23%，且整体回答的有用性没有下降。'}}}, {'id': 'https://huggingface.co/papers/2508.03404', 'title': 'Visual Document Understanding and Question Answering: A Multi-Agent\n  Collaboration Framework with Test-Time Scaling', 'url': 'https://huggingface.co/papers/2508.03404', 'abstract': 'MACT, a Multi-Agent Collaboration framework with Test-Time scaling, enhances visual document understanding and VQA by using four specialized agents and mixed reward modeling, achieving superior performance with reduced parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git.', 'score': 2, 'issue_id': 5252, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '500ce3c7788fd634', 'authors': ['Xinlei Yu', 'Zhangquan Chen', 'Yudong Zhang', 'Shilin Lu', 'Ruolin Shen', 'Jiangning Zhang', 'Xiaobin Hu', 'Yanwei Fu', 'Shuicheng Yan'], 'affiliations': ['Fudan University, China', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore', 'Technical University of Munich, Germany', 'Tsinghua University, China', 'University of Science and Technology of China, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03404.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark', '#small_models', '#cv', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Многоагентное сотрудничество для улучшенного понимания визуальных документов', 'desc': 'MACT - это новая система для понимания визуальных документов и ответов на вопросы по изображениям. Она использует четыре специализированных агента: планирования, выполнения, оценки и ответа, которые эффективно взаимодействуют друг с другом. MACT применяет смешанное моделирование вознаграждений и масштабирование во время тестирования для улучшения производительности. Система показывает превосходные результаты на различных тестах, особенно для задач с длинным визуальным контекстом и сложными рассуждениями.'}, 'en': {'title': 'MACT: Enhancing Document Understanding with Smart Agent Collaboration', 'desc': "MACT is a Multi-Agent Collaboration framework designed to improve visual document understanding and visual question answering (VQA). It utilizes four specialized agents—planning, execution, judgment, and answer agents—that work together to enhance performance while maintaining a smaller parameter scale. The judgment agent plays a crucial role by verifying answers and prompting revisions from other agents, which leads to better accuracy compared to traditional methods. Additionally, MACT employs mixed reward modeling and hybrid test-time scaling to optimize each agent's performance based on their specific tasks, resulting in superior outcomes in complex reasoning scenarios."}, 'zh': {'title': '多智能体协作，提升视觉理解与问答能力', 'desc': 'MACT是一个多智能体协作框架，旨在提升视觉文档理解和视觉问答（VQA）的性能。它由四个专门的小型智能体组成，分别负责规划、执行、判断和回答，能够有效协作。判断智能体专注于验证正确性，并能引导其他智能体进行修正，超越了传统的纠错策略。此外，MACT采用混合奖励建模和平衡的测试时间缩放策略，使得每个智能体根据其功能定制不同的缩放策略，从而在参数规模较小的情况下实现更优的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.05545', 'title': 'PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction', 'url': 'https://huggingface.co/papers/2508.05545', 'abstract': 'A comprehensive analysis of Large Language Models for PII redaction evaluates various architectures and training strategies, providing guidance for accurate, efficient, and privacy-aware redaction systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Redacting Personally Identifiable Information (PII) from unstructured text is critical for ensuring data privacy in regulated domains. While earlier approaches have relied on rule-based systems and domain-specific Named Entity Recognition (NER) models, these methods fail to generalize across formats and contexts. Recent advances in Large Language Models (LLMs) offer a promising alternative, yet the effect of architectural and training choices on redaction performance remains underexplored. LLMs have demonstrated strong performance in tasks that require contextual language understanding, including the redaction of PII in free-form text. Prior work suggests that with appropriate adaptation, LLMs can become effective contextual privacy learners. However, the consequences of architectural and training choices for PII Redaction remain underexplored. In this work, we present a comprehensive analysis of LLMs as privacy-preserving PII Redaction systems. We evaluate a range of LLM architectures and training strategies for their effectiveness in PII Redaction. Our analysis measures redaction performance, semantic preservation, and PII leakage, and compares these outcomes against latency and computational cost. The results provide practical guidance for configuring LLM-based redactors that are accurate, efficient, and privacy-aware. To support reproducibility and real-world deployment, we release PRvL, an open-source suite of fine-tuned models, and evaluation tools for general-purpose PII Redaction. PRvL is built entirely on open-source LLMs and supports multiple inference settings for flexibility and compliance. It is designed to be easily customized for different domains and fully operable within secure, self-managed environments. This enables data owners to perform redactions without relying on third-party services or exposing sensitive content beyond their own infrastructure.', 'score': 1, 'issue_id': 5244, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'dff00258f2cd918b', 'authors': ['Leon Garza', 'Anantaa Kotal', 'Aritran Piplai', 'Lavanya Elluri', 'Prajit Das', 'Aman Chadha'], 'affiliations': ['Amazon Web Services', 'Cisco Systems Inc.', 'Dept. of C.S., The University of Texas at El Paso', 'Texas A&M University-Central Texas'], 'pdf_title_img': 'assets/pdf/title_img/2508.05545.jpg', 'data': {'categories': ['#leakage', '#inference', '#training', '#dataset', '#ethics', '#open_source', '#architecture'], 'emoji': '🔐', 'ru': {'title': 'LLM на страже приватности: новый подход к редактированию персональных данных', 'desc': 'В этой статье представлен комплексный анализ использования больших языковых моделей (LLM) для редактирования персональных данных (PII) в неструктурированном тексте. Авторы оценивают различные архитектуры и стратегии обучения LLM с точки зрения эффективности редактирования, сохранения семантики и предотвращения утечек PII. Результаты исследования предоставляют практические рекомендации по настройке LLM-редакторов, которые являются точными, эффективными и обеспечивают конфиденциальность. В рамках работы также выпущен открытый набор инструментов PRvL для редактирования PII на основе открытых LLM.'}, 'en': {'title': 'Harnessing LLMs for Effective PII Redaction', 'desc': 'This paper analyzes the use of Large Language Models (LLMs) for redacting Personally Identifiable Information (PII) from unstructured text, which is essential for maintaining data privacy. It highlights the limitations of traditional rule-based and domain-specific Named Entity Recognition (NER) systems, which struggle to adapt to various formats and contexts. The authors evaluate different LLM architectures and training strategies to determine their effectiveness in accurately redacting PII while preserving semantic meaning and minimizing leakage. They also introduce PRvL, an open-source suite of fine-tuned models and tools designed for flexible and secure PII redaction in various domains.'}, 'zh': {'title': '大型语言模型助力个人信息去标识化的隐私保护', 'desc': '本文对大型语言模型（LLMs）在个人身份信息（PII）去标识化中的应用进行了全面分析。研究评估了不同的模型架构和训练策略，以提高去标识化的准确性和效率，同时确保数据隐私。通过对去标识化性能、语义保留和PII泄露的测量，提供了配置LLM去标识化系统的实用指导。为了支持可重复性和实际部署，本文发布了PRvL，一个开源的微调模型和评估工具套件，旨在帮助数据拥有者在安全环境中进行去标识化。'}}}, {'id': 'https://huggingface.co/papers/2508.04946', 'title': 'REINA: Regularized Entropy Information-Based Loss for Efficient\n  Simultaneous Speech Translation', 'url': 'https://huggingface.co/papers/2508.04946', 'abstract': 'A novel loss function, REINA, optimizes the latency-quality tradeoff in Simultaneous Speech Translation by adaptively waiting for more input based on information gain.  \t\t\t\t\tAI-generated summary \t\t\t\t Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.', 'score': 1, 'issue_id': 5245, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'aa0ec666492a04d2', 'authors': ['Nameer Hirschkind', 'Joseph Liu', 'Mahesh Kumar Nandwana', 'Xiao Yu'], 'affiliations': ['Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2508.04946.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#synthetic', '#optimization', '#audio', '#machine_translation', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Умное ожидание для быстрого и качественного перевода речи', 'desc': 'Статья представляет новую функцию потерь REINA для оптимизации компромисса между задержкой и качеством в системах одновременного перевода речи. REINA адаптивно ожидает дополнительного ввода на основе прироста информации. Метод основан на принципах теории информации и позволяет обучать адаптивную политику с использованием существующей модели перевода без потоковой передачи. Авторы достигли лучших результатов для моделей сопоставимого размера на нескольких языках, используя только открытые или синтетические данные.'}, 'en': {'title': 'Optimizing Translation: REINA for Better Latency and Quality', 'desc': 'This paper presents a new loss function called REINA, designed to improve Simultaneous Speech Translation (SimulST) systems by optimizing the balance between translation quality and latency. REINA works by adaptively waiting for additional input only when it is expected to enhance the translation quality, based on the concept of information gain. The authors demonstrate that using REINA allows for training a SimulST model that achieves state-of-the-art performance in translating between English and languages like French, Spanish, and German. The results show a significant improvement in the latency-quality tradeoff, achieving up to 21% better performance compared to previous methods.'}, 'zh': {'title': '优化延迟与质量的REINA损失函数', 'desc': '本文提出了一种新颖的损失函数REINA，用于优化同时语音翻译中的延迟与质量的权衡。该方法通过根据信息增益自适应地等待更多输入，从而提高翻译质量。REINA基于信息理论原理，能够训练出一种自适应策略，超越了以往方法的延迟/质量帕累托前沿。通过使用REINA，我们在法语、西班牙语和德语的同时翻译模型上取得了最先进的结果，显示出REINA在延迟和质量之间的权衡改善了多达21%。'}}}, {'id': 'https://huggingface.co/papers/2508.04939', 'title': 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating\n  Linguistic Shibboleth Detection in LLM Hiring Evaluations', 'url': 'https://huggingface.co/papers/2508.04939', 'abstract': "A benchmark evaluates Large Language Models' response to linguistic markers that reveal demographic attributes, demonstrating systematic penalization of hedging language despite equivalent content quality.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts.", 'score': 1, 'issue_id': 5244, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'c43270050aadf2f5', 'authors': ['Julia Kharchenko', 'Tanya Roosta', 'Aman Chadha', 'Chirag Shah'], 'affiliations': ['Stanford University, Amazon GenAI, Palo Alto, CA, USA', 'UC Berkeley, Amazon, Saratoga, CA, USA', 'University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.04939.jpg', 'data': {'categories': ['#benchmark', '#ethics'], 'emoji': '🎭', 'ru': {'title': 'Раскрытие скрытой предвзятости языковых моделей', 'desc': 'Статья представляет комплексный бенчмарк для оценки реакции больших языковых моделей (LLM) на лингвистические маркеры, раскрывающие демографические атрибуты. Исследование показывает, что LLM систематически занижают оценки текстов с неуверенными формулировками, несмотря на эквивалентное качество содержания. Бенчмарк генерирует контролируемые лингвистические вариации, сохраняя семантическую эквивалентность, что позволяет точно измерять демографическую предвзятость в системах автоматической оценки. Работа закладывает основу для выявления и измерения лингвистической дискриминации в системах искусственного интеллекта.'}, 'en': {'title': 'Uncovering Bias: How Language Shapes AI Judgments', 'desc': 'This paper presents a benchmark designed to assess how Large Language Models (LLMs) react to subtle linguistic cues that can indicate demographic characteristics. The study reveals that LLMs tend to penalize hedging language, which is a way of expressing uncertainty, even when the content quality remains the same. By using controlled variations in language while keeping the meaning intact, the researchers can accurately measure biases related to demographics in AI evaluations. The findings highlight a significant 25.6% reduction in ratings for hedged responses, underscoring the need for fairness in AI systems.'}, 'zh': {'title': '揭示语言偏见的基准评估', 'desc': '这篇论文介绍了一个评估大型语言模型（LLMs）对语言标记反应的基准，特别是那些可以揭示人口属性的细微语言特征。研究表明，尽管内容质量相当，LLMs对某些语言模式，尤其是模糊语言，存在系统性的惩罚。通过构建100个经过验证的问题-回答对，论文展示了如何在保持语义等价的情况下，生成受控的语言变体，以精确测量自动评估系统中的人口偏见。该研究为检测和测量人工智能系统中的语言歧视建立了基础框架，具有广泛的公平性应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.04699', 'title': 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during\n  Multi-Hop Analysis', 'url': 'https://huggingface.co/papers/2508.04699', 'abstract': 'Research investigates reasoning failures in language models for multi-hop question answering, introducing a framework to categorize errors and improve model fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved ("hops"), completeness in capturing relevant information ("coverage"), and cognitive inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.', 'score': 1, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '97e95e6b08388e4f', 'authors': ['Anushka Yadav', 'Isha Nalawade', 'Srujana Pillarichety', 'Yashwanth Babu', 'Reshmi Ghosh', 'Samyadeep Basu', 'Wenlong Zhao', 'Ali Nasaeh', 'Sriram Balasubramanian', 'Soundararajan Srinivasan'], 'affiliations': ['Microsoft', 'University of Maryland, College Park', 'University of Massachusetts, Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2508.04699.jpg', 'data': {'categories': ['#data', '#benchmark', '#reasoning', '#hallucinations', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Разгадка ошибок ИИ: новый взгляд на рассуждения языковых моделей', 'desc': 'Исследование посвящено анализу ошибок рассуждения языковых моделей при ответах на многоэтапные вопросы. Авторы представляют новую систему классификации ошибок, рассматривающую три ключевых аспекта: разнообразие используемых источников, полноту охвата релевантной информации и когнитивную неэффективность. Проведено тщательное аннотирование данных и использованы автоматические метрики для выявления сложных паттернов ошибок. Результаты исследования дают более глубокое понимание когнитивных ограничений современных языковых моделей и предлагают пути улучшения их способности к рассуждениям.'}, 'en': {'title': 'Unraveling Reasoning Failures in Language Models', 'desc': 'This paper investigates the reasoning failures of language models specifically in multi-hop question answering tasks. It introduces a new framework to categorize these errors based on three dimensions: the diversity of source documents, the completeness of relevant information, and cognitive inefficiency. The study uses human annotation and automated metrics to reveal complex error patterns that are often overlooked in traditional accuracy evaluations. The findings aim to enhance the understanding of cognitive limitations in language models and provide guidance for improving their reasoning capabilities.'}, 'zh': {'title': '提升语言模型推理能力的关键', 'desc': '本研究探讨了语言模型在多跳问答中的推理失败，提出了一种框架来分类错误并提高模型的可靠性。研究发现，当前语言模型在处理复杂的多步骤问题时，常常出现幻觉现象，缺乏对错误原因的全面理解。我们引入了一种新的错误分类框架，从源文档的多样性、信息捕捉的完整性和认知效率三个维度进行分析。通过严格的人类标注和自动化指标的支持，我们揭示了隐藏在准确性评估背后的复杂错误模式，为未来语言模型的推理能力提升提供了可行的指导。'}}}, {'id': 'https://huggingface.co/papers/2508.04190', 'title': 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation', 'url': 'https://huggingface.co/papers/2508.04190', 'abstract': 'RPCANet++ combines RPCA with deep learning to achieve efficient and interpretable sparse object segmentation by introducing modules for background approximation, object extraction, and image restoration.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.', 'score': 1, 'issue_id': 5247, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '38f7da6b7ad16bbf', 'authors': ['Fengyi Wu', 'Yimian Dai', 'Tianfang Zhang', 'Yixuan Ding', 'Jian Yang', 'Ming-Ming Cheng', 'Zhenming Peng'], 'affiliations': ['Department of Automation, Tsinghua University, Beijing, China', 'PCA Lab, VCIP, College of Computer Science, Nankai University, Tianjin 300350, China', 'School of Information and Communication Engineering and the Laboratory of Imaging Detection and Intelligent Perception, University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04190.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#cv', '#interpretability', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'RPCANet++: Интерпретируемая сегментация объектов с глубоким обучением', 'desc': 'RPCANet++ объединяет метод анализа главных компонент (RPCA) с глубоким обучением для эффективной и интерпретируемой сегментации разреженных объектов. Модель включает модули для аппроксимации фона, извлечения объектов и восстановления изображений. Использование памяти и контрастного априорного модуля улучшает сохранение признаков фона и ускоряет выделение объектов. Эксперименты показывают, что RPCANet++ достигает современного уровня производительности в различных сценариях визуализации.'}, 'en': {'title': 'Efficient and Interpretable Sparse Object Segmentation with RPCANet++', 'desc': 'RPCANet++ is a novel framework that integrates Robust Principal Component Analysis (RPCA) with deep learning techniques to enhance sparse object segmentation. It introduces specialized modules for background approximation, object extraction, and image restoration, addressing the computational challenges of traditional RPCA methods. By incorporating a Memory-Augmented Module (MAM) and a Deep Contrast Prior Module (DCPM), the framework improves feature preservation and accelerates object extraction. Extensive experiments show that RPCANet++ achieves superior performance across various imaging scenarios while maintaining interpretability through low-rankness and sparsity metrics.'}, 'zh': {'title': '结合RPCA与深度学习，实现高效可解释的稀疏物体分割', 'desc': 'RPCANet++ 是一种结合了鲁棒主成分分析（RPCA）和深度学习的稀疏物体分割框架。它通过引入背景近似模块、物体提取模块和图像恢复模块，实现了高效且可解释的分割效果。该方法解决了传统 RPCA 模型在计算和适应性方面的局限性，特别是在动态场景中的应用。通过大量实验，RPCANet++ 在不同的成像场景中表现出色，提供了新的可靠和可解释的稀疏物体分割基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04107', 'title': 'Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decode', 'url': 'https://huggingface.co/papers/2508.04107', 'abstract': 'MLLMSeg integrates MLLM vision encoder and LLM features with a lightweight mask decoder to achieve high accuracy in reference expression segmentation with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.', 'score': 1, 'issue_id': 5259, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '57990322bc3ce2a9', 'authors': ['Jingchao Wang', 'Zhijian Wu', 'Dingjiang Huang', 'Yefeng Zheng', 'Hong Wang'], 'affiliations': ['Medical Artificial Intelligence Laboratory, Westlake University', 'School of Data Science and Engineering, East China Normal University', 'School of Life Science and Technology, Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04107.jpg', 'data': {'categories': ['#games', '#multimodal', '#optimization', '#architecture', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Эффективная сегментация изображений с помощью мультимодальных языковых моделей', 'desc': 'MLLMSeg - это новый подход к сегментации изображений по референсным выражениям, объединяющий визуальный энкодер мультимодальной большой языковой модели (MLLM) и особенности большой языковой модели (LLM) с легковесным декодером масок. Метод использует детализированные пространственные признаки из визуального энкодера и семантические признаки из LLM для точного предсказания масок. MLLMSeg достигает высокой точности при сниженных вычислительных затратах по сравнению с существующими методами. Эксперименты показывают, что MLLMSeg превосходит как SAM-based, так и SAM-free конкурентов, обеспечивая лучший баланс между производительностью и стоимостью.'}, 'en': {'title': 'Efficient and Accurate Reference Expression Segmentation with MLLMSeg', 'desc': 'MLLMSeg is a novel framework designed for reference expression segmentation (RES) that combines the strengths of multimodal large models (MLLMs) with a lightweight mask decoder. It effectively utilizes the visual features from the MLLM vision encoder while avoiding the need for an additional visual encoder, which helps reduce computational costs. The framework introduces a detail-enhanced and semantic-consistent feature fusion module (DSFF) that merges visual and semantic features for improved accuracy. Experimental results show that MLLMSeg outperforms existing methods, achieving high accuracy in segmentation while maintaining a low parameter count.'}, 'zh': {'title': '高效精准的参考表达分割新方法', 'desc': 'MLLMSeg是一种新颖的框架，结合了多模态大模型（MLLM）的视觉编码器和大语言模型（LLM）特征，使用轻量级的掩码解码器来实现高精度的参考表达分割。该方法充分利用了MLLM视觉编码器中固有的视觉细节特征，而无需引入额外的视觉编码器。我们还提出了一种细节增强和语义一致的特征融合模块（DSFF），将与细节相关的视觉特征与LLM输出的语义特征进行全面整合。实验结果表明，MLLMSeg在性能和计算成本之间取得了更好的平衡，超越了现有的基于SAM和非SAM的方法。'}}}, {'id': 'https://huggingface.co/papers/2508.02243', 'title': 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking', 'url': 'https://huggingface.co/papers/2508.02243', 'abstract': 'A novel LLM-based framework, Intra- and Inter-modal Collaborative Reflections, enhances multimodal entity linking by prioritizing text and using iterative visual clues when necessary, outperforming current state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal entity linking plays a crucial role in a wide range of applications. Recent advances in large language model-based methods have become the dominant paradigm for this task, effectively leveraging both textual and visual modalities to enhance performance. Despite their success, these methods still face two challenges, including unnecessary incorporation of image data in certain scenarios and the reliance only on a one-time extraction of visual features, which can undermine their effectiveness and accuracy. To address these challenges, we propose a novel LLM-based framework for the multimodal entity linking task, called Intra- and Inter-modal Collaborative Reflections. This framework prioritizes leveraging text information to address the task. When text alone is insufficient to link the correct entity through intra- and inter-modality evaluations, it employs a multi-round iterative strategy that integrates key visual clues from various aspects of the image to support reasoning and enhance matching accuracy. Extensive experiments on three widely used public datasets demonstrate that our framework consistently outperforms current state-of-the-art methods in the task, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. Our code is available at https://github.com/ziyan-xiaoyu/I2CR/.', 'score': 1, 'issue_id': 5249, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '72638e0c336c8711', 'authors': ['Ziyan Liu', 'Junwen Li', 'Kaiwen Li', 'Tong Ruan', 'Chao Wang', 'Xinyan He', 'Zongyu Wang', 'Xuezhi Cao', 'Jingping Liu'], 'affiliations': ['East China University of Science and Technology, Shanghai, China', 'Meituan, Shanghai, China', 'Shanghai University, Shanghai, China', 'South China University of Technology, Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02243.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#multimodal'], 'emoji': '🔗', 'ru': {'title': 'Умное связывание текста и изображений с помощью ИИ', 'desc': "Предложена новая система для мультимодальной привязки сущностей на основе больших языковых моделей. Она называется 'Внутри- и межмодальные совместные отражения' и отдает приоритет текстовой информации, используя визуальные подсказки только при необходимости. Система применяет итеративный подход для интеграции ключевых визуальных элементов из разных аспектов изображения. Эксперименты показали, что данный метод превосходит современные аналоги на трех общедоступных наборах данных."}, 'en': {'title': 'Enhancing Multimodal Entity Linking with Text-First Strategies', 'desc': 'This paper introduces a new framework called Intra- and Inter-modal Collaborative Reflections for multimodal entity linking, which primarily focuses on text while effectively using visual clues when needed. The framework addresses two main challenges in existing methods: the unnecessary use of image data and the limited extraction of visual features. By employing a multi-round iterative strategy, it enhances the reasoning process by integrating relevant visual information only when text is insufficient. Experimental results show that this approach significantly improves performance over current state-of-the-art techniques across multiple datasets.'}, 'zh': {'title': '文本优先，视觉辅助的多模态实体链接新框架', 'desc': '本文提出了一种基于大型语言模型的新框架，称为内部和外部模态协作反思，旨在增强多模态实体链接。该框架优先利用文本信息来完成任务，当仅依靠文本无法准确链接实体时，采用多轮迭代策略，结合图像中的关键视觉线索进行推理。通过在三个广泛使用的公共数据集上的大量实验，证明该框架在性能上超越了当前最先进的方法，分别提高了3.2%、5.1%和1.6%。这表明，合理利用文本和视觉信息的结合可以显著提升多模态实体链接的准确性。'}}}, {'id': 'https://huggingface.co/papers/2508.05128', 'title': 'Attention Basin: Why Contextual Position Matters in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.05128', 'abstract': "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.", 'score': 0, 'issue_id': 5256, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'a6974044ec3cff79', 'authors': ['Zihao Yi', 'Delong Zeng', 'Zhenqing Ling', 'Haohao Luo', 'Zhe Xu', 'Wei Liu', 'Jian Luan', 'Wanxia Cao', 'Ying Shen'], 'affiliations': ['MiLM Plus, Xiaomi Inc., Beijing, China', 'Sun Yat-sen University, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.05128.jpg', 'data': {'categories': ['#data', '#optimization', '#long_context', '#training', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'Управление вниманием: новый подход к оптимизации работы языковых моделей', 'desc': "Исследование показало, что большие языковые модели (LLM) проявляют позиционное смещение, уделяя больше внимания элементам в начале и конце последовательности входных данных. Этот феномен, названный 'бассейном внимания', влияет на производительность модели. На основе этого наблюдения авторы разработали метод AttnRank, который переупорядочивает входные данные для оптимизации внимания модели. AttnRank - это универсальный метод, не требующий обучения, который значительно улучшает производительность различных LLM в задачах многоэтапного вопросно-ответного анализа и few-shot обучения."}, 'en': {'title': 'Enhancing Model Performance with Attention-Driven Reranking', 'desc': "This paper explores how Large Language Models (LLMs) are influenced by the position of information in their input sequences. It identifies a phenomenon called the attention basin, where models focus more on items at the start and end of a sequence, often overlooking those in the middle. The authors propose a new method called Attention-Driven Reranking (AttnRank) that improves model performance by rearranging input items to match the model's attention preferences. AttnRank is easy to implement, does not require retraining the models, and shows significant performance gains across various tasks and model architectures."}, 'zh': {'title': '提升模型性能的注意力重排序方法', 'desc': '大型语言模型（LLMs）的性能对输入信息的上下文位置非常敏感。我们的实验揭示了一种现象，称为注意力盆地：模型在处理结构化项目时，通常会对序列开头和结尾的项目给予更高的注意力，而忽视中间的项目。我们发现，将更多注意力分配给关键信息对于提升模型性能至关重要。基于这些发现，我们提出了注意力驱动的重排序方法（AttnRank），它通过小规模的校准集来估计模型的内在位置注意力偏好，并重新排列检索到的文档或少量示例，以将最重要的内容与高注意力位置对齐。'}}}, {'id': 'https://huggingface.co/papers/2508.00819', 'title': 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.00819', 'abstract': 'DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.', 'score': 35, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '2241beba3b69f1fd', 'authors': ['Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Jiaqi Wang', 'Dahua Lin'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00819.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#diffusion', '#long_context'], 'emoji': '🔄', 'ru': {'title': 'Динамическая адаптация длины раскрывает потенциал диффузионных языковых моделей', 'desc': 'DAEDAL - это новая стратегия динамической адаптации длины для диффузионных больших языковых моделей (DLLM). Она позволяет преодолеть ограничение статически заданной длины генерации, которое снижает эффективность DLLM. DAEDAL работает в два этапа: сначала расширяет начальную короткую длину до подходящей для задачи, а затем во время денойзинга динамически расширяет недостаточные области генерации. Эксперименты показывают, что DAEDAL достигает сравнимой или превосходящей производительности по сравнению с тщательно настроенными базовыми моделями фиксированной длины, одновременно повышая вычислительную эффективность.'}, 'en': {'title': 'Dynamic Length Adaptation for Enhanced DLLM Performance', 'desc': 'DAEDAL is a new method that improves Diffusion Large Language Models (DLLMs) by allowing them to adapt their output length dynamically without needing additional training. Traditional DLLMs require a fixed length for generation, which can limit their performance on complex tasks or waste computational resources. DAEDAL addresses this issue by using internal signals from the model to determine the optimal length for responses, expanding the generation length as needed. This approach not only enhances the quality of the generated text but also increases efficiency, making DLLMs more competitive with Autoregressive models.'}, 'zh': {'title': 'DAEDAL：动态适应长度的去噪新策略', 'desc': 'DAEDAL是一种新颖的无训练去噪策略，能够在扩散大型语言模型中实现动态长度适应，从而提高性能和计算效率。扩散大型语言模型（DLLMs）在生成效率和全局上下文建模方面表现出色，但其静态生成长度限制了实际应用。DAEDAL通过利用模型内部信号，动态调整生成长度，解决了静态长度带来的性能和计算开销问题。实验表明，DAEDAL在性能上与固定长度基线相当，甚至在某些情况下更优，同时提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2507.23268', 'title': 'PixNerd: Pixel Neural Field Diffusion', 'url': 'https://huggingface.co/papers/2507.23268', 'abstract': 'Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID on ImageNet 512times512 without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.', 'score': 31, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'f035699955568725', 'authors': ['Shuai Wang', 'Ziteng Gao', 'Chenhui Zhu', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed', 'Nanjing University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23268.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений без сложных архитектур', 'desc': 'PixNerd (Pixel Neural Field Diffusion) - это новый метод генерации изображений, работающий в пиксельном пространстве без использования вариационных автоэнкодеров. Он предлагает одноэтапный процесс генерации высококачественных изображений без сложных каскадных архитектур. PixNerd достигает впечатляющих результатов на наборе данных ImageNet, превосходя существующие методы по метрике FID. Кроме того, модель успешно применяется для задачи генерации изображений по текстовому описанию, показывая конкурентоспособные результаты на бенчмарках GenEval и DPG.'}, 'en': {'title': 'Efficient Image Generation with PixNerd: No VAEs, No Hassle!', 'desc': 'Pixel Neural Field Diffusion (PixNerd) introduces a novel approach to image generation that operates in a single-scale and single-stage manner, eliminating the need for variational autoencoders (VAEs) and complex pipelines. This method addresses the issues of accumulated errors and artifacts that arise from traditional two-stage training processes. By utilizing a patch-wise decoding strategy with neural fields, PixNerd achieves impressive performance metrics, such as a 2.15 FID score on ImageNet 256x256. Additionally, it extends its capabilities to text-to-image generation, demonstrating competitive results on various benchmarks.'}, 'zh': {'title': '高效图像生成的新方法：PixNerd', 'desc': 'Pixel Neural Field Diffusion（PixNerd）是一种高效的图像生成方法，采用单尺度、单阶段的流程，无需变分自编码器（VAE）或复杂的管道。该方法通过神经场模型实现了补丁级解码，避免了传统方法中常见的累积误差和解码伪影。PixNerd在ImageNet数据集上取得了2.15的FID分数，显示出其优越的性能。我们还将PixNerd扩展到文本生成图像的应用中，取得了在GenEval和DPG基准测试中的竞争性成绩。'}}}, {'id': 'https://huggingface.co/papers/2508.00414', 'title': 'Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training', 'url': 'https://huggingface.co/papers/2508.00414', 'abstract': 'Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro', 'score': 14, 'issue_id': 5169, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '0cd43b7d9f3e1eb5', 'authors': ['Tianqing Fang', 'Zhisong Zhang', 'Xiaoyang Wang', 'Rui Wang', 'Can Qin', 'Yuxuan Wan', 'Jun-Yu Ma', 'Ce Zhang', 'Jiaqi Chen', 'Xiyun Li', 'Hongming Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.00414.jpg', 'data': {'categories': ['#data', '#agi', '#training', '#open_source', '#reasoning', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Открытая агентная система ИИ для демократизации передовых технологий', 'desc': 'Cognitive Kernel-Pro - это открытая многомодульная агентная система, улучшающая надежность и производительность ИИ-агентов с помощью курирования данных и новых стратегий во время тестирования. Система фокусируется на создании качественных обучающих данных для Агентных Фундаментальных Моделей в четырех ключевых областях: веб, файлы, код и общие рассуждения. Cognitive Kernel-Pro исследует новые стратегии рефлексии и голосования агентов для повышения их надежности. Система достигает передовых результатов среди открытых и бесплатных агентов на бенчмарке GAIA, превосходя предыдущие ведущие системы.'}, 'en': {'title': 'Democratizing AI Agent Development with Cognitive Kernel-Pro', 'desc': 'Cognitive Kernel-Pro is an open-source framework designed to improve the robustness and performance of AI agents through effective data curation and innovative test-time strategies. It focuses on creating high-quality training data for Agent Foundation Models by constructing queries and trajectories across various domains like web interaction and coding. The framework also introduces new methods for agent reflection and voting, which enhance decision-making capabilities. By achieving state-of-the-art results on the GAIA benchmark, Cognitive Kernel-Pro sets a new standard for accessible and high-performance AI agents.'}, 'zh': {'title': '开放源代码，提升AI代理的未来！', 'desc': 'Cognitive Kernel-Pro是一个开源的多模块代理框架，旨在通过数据整理和新颖的测试策略来增强AI代理的鲁棒性和性能。该框架支持复杂推理、网络交互、编码和自主研究能力，推动下一代人工智能的发展。我们系统地研究了高质量训练数据的整理，重点关注查询、轨迹和可验证答案的构建。Cognitive Kernel-Pro在GAIA上进行了评估，取得了开源和免费代理中的最佳结果，设立了高能力AI代理的新标准。'}}}, {'id': 'https://huggingface.co/papers/2507.23478', 'title': '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding', 'url': 'https://huggingface.co/papers/2507.23478', 'abstract': '3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.', 'score': 6, 'issue_id': 5155, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'f5e99fc10e8b9ad5', 'authors': ['Ting Huang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['School of Computer Science, Peking University', 'Shanghai University of Engineering Science'], 'pdf_title_img': 'assets/pdf/title_img/2507.23478.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset', '#reasoning', '#rlhf', '#synthetic'], 'emoji': '🧠', 'ru': {'title': '3D-R1: Революция в понимании трехмерных сцен с помощью ИИ', 'desc': 'Модель 3D-R1 улучшает понимание трехмерных сцен с помощью высококачественного синтетического датасета и обучения с подкреплением. Она использует динамический выбор ракурсов для более информативного анализа 3D-сцен. 3D-R1 применяет функции вознаграждения для улучшения точности восприятия и семантической точности ответов. Эксперименты показывают значительное улучшение результатов на различных бенчмарках трехмерных сцен.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with 3D-R1', 'desc': "The paper presents 3D-R1, a model designed to improve 3D scene understanding by addressing the limitations of existing vision-language models (VLMs). It introduces a high-quality synthetic dataset called Scene-30K, which is used to enhance the model's reasoning capabilities. The training process incorporates reinforcement learning with a GRPO policy and utilizes multiple reward functions to ensure accuracy and semantic precision. Additionally, a dynamic view selection strategy is implemented to optimize the perspectives used for analyzing 3D scenes, resulting in a notable average improvement of 10% in performance across various benchmarks."}, 'zh': {'title': '3D-R1：提升3D场景理解的智能模型', 'desc': '3D-R1 是一个增强 3D 场景理解的基础模型，利用高质量的合成数据集和强化学习方法来提升推理能力。我们构建了一个名为 Scene-30K 的合成数据集，作为 3D-R1 的冷启动初始化数据。通过引入动态视角选择策略，3D-R1 能够自适应选择最具信息量的视角进行 3D 场景理解。实验结果表明，3D-R1 在多个 3D 场景基准测试中平均提升了 10%，有效增强了推理和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.23361', 'title': 'SWE-Exp: Experience-Driven Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23361', 'abstract': 'SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.', 'score': 6, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e16fe4dad5f61553', 'authors': ['Silin Chen', 'Shaoxin Lin', 'Xiaodong Gu', 'Yuling Shi', 'Heng Lian', 'Longfei Yun', 'Dong Chen', 'Weiguo Sun', 'Lin Cao', 'Qianxiang Wang'], 'affiliations': ['Huawei, China', 'Shanghai Jiao Tong University, China', 'UC San Diego, United States', 'Xidian University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23361.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Опыт - ключ к эффективному решению проблем в ПО', 'desc': 'SWE-Exp - это новый подход к решению проблем в программном обеспечении, использующий накопленный опыт предыдущих попыток исправления ошибок. Метод создает многоуровневый банк опыта, который включает как успешные, так и неудачные попытки решения проблем. SWE-Exp извлекает многоуровневые знания о решении проблем - от общего понимания до конкретных изменений в коде. Эксперименты показывают, что SWE-Exp достигает наилучших результатов в решении проблем на тестовом наборе SWE-bench-Verified среди агентов с открытым исходным кодом.'}, 'en': {'title': 'Transforming Software Issue Resolution with Experience-Driven Learning', 'desc': 'SWE-Exp is a novel approach that enhances software issue resolution by utilizing past experiences of agents to improve their performance. Unlike traditional agents that do not retain knowledge, SWE-Exp builds an experience bank that captures both successful and failed attempts at resolving issues. This allows the system to learn from previous repairs and apply that knowledge to new problems, leading to more efficient and effective resolutions. The method has demonstrated a significant improvement in resolution rates, showcasing a shift from random exploration to a more strategic, experience-driven process.'}, 'zh': {'title': '经验驱动的软件问题解决新方法', 'desc': 'SWE-Exp是一种增强软件问题解决能力的方法，通过系统地积累和利用过去代理的修复经验，提高了解决率。当前的代理在处理问题时缺乏记忆，无法重用之前的知识，导致重复探索失败的路径。SWE-Exp通过建立一个多层次的经验库，提取成功和失败的修复尝试中的可重用知识，从而实现跨问题的持续学习。实验表明，SWE-Exp在开源代理框架下的SWE-bench-Verified上达到了41.6%的最佳解决率，标志着自动化软件工程代理的一个新范式。'}}}, {'id': 'https://huggingface.co/papers/2508.00265', 'title': 'Multimodal Referring Segmentation: A Survey', 'url': 'https://huggingface.co/papers/2508.00265', 'abstract': "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", 'score': 5, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '1604e587f6dc8177', 'authors': ['Henghui Ding', 'Song Tang', 'Shuting He', 'Chang Liu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance Inc.', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2508.00265.jpg', 'data': {'categories': ['#cv', '#multimodal', '#3d', '#survey', '#video', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Мультимодальная сегментация: от пикселей к пониманию', 'desc': 'Эта статья представляет собой обзор методов мультимодальной сегментации по ссылкам, охватывающий достижения в области сверточных нейронных сетей, трансформеров и больших языковых моделей. Авторы рассматривают задачу сегментации объектов на изображениях, видео и в 3D-сценах на основе текстовых или аудио инструкций. В работе представлена унифицированная мета-архитектура для сегментации по ссылкам и обзор репрезентативных методов для различных визуальных сцен. Также обсуждаются обобщенные методы выражения ссылок (GREx) для решения проблем сложности реального мира.'}, 'en': {'title': 'Enhancing Object Segmentation with Multimodal Instructions', 'desc': 'This paper surveys the field of multimodal referring segmentation, which focuses on identifying and segmenting objects in visual content based on textual or audio instructions. It highlights the advancements made through convolutional neural networks, transformers, and large language models that enhance the ability to understand and process multimodal data. The authors present a unified meta architecture for referring segmentation and review various methods applicable to images, videos, and 3D scenes. Additionally, they discuss challenges in real-world applications and provide performance comparisons on standard benchmarks to evaluate the effectiveness of different approaches.'}, 'zh': {'title': '多模态指向分割的全面调查', 'desc': '多模态指向分割旨在根据文本或音频指令在视觉场景中分割目标物体，如图像、视频和3D场景。该任务在需要根据用户指令进行准确物体感知的实际应用中至关重要。近年来，卷积神经网络、变换器和大型语言模型的进步显著提升了多模态感知能力。本文提供了多模态指向分割的全面调查，涵盖了背景介绍、统一的元架构、代表性方法及其在不同视觉场景中的应用。'}}}, {'id': 'https://huggingface.co/papers/2507.23348', 'title': 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23348', 'abstract': "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.", 'score': 4, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '28b58ecd36ac995b', 'authors': ['Han Li', 'Yuling Shi', 'Shaoxin Lin', 'Xiaodong Gu', 'Heng Lian', 'Xin Wang', 'Yantao Jia', 'Tao Huang', 'Qianxiang Wang'], 'affiliations': ['Huawei China', 'Shanghai Jiao Tong University China', 'Xidian University China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23348.jpg', 'data': {'categories': ['#optimization', '#agents', '#open_source', '#reasoning', '#games', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Дебаты ИИ-агентов для улучшения разработки ПО', 'desc': 'SWE-Debate - это новая система для решения проблем в разработке программного обеспечения, использующая несколько ИИ-агентов. Система организует структурированные дебаты между агентами, каждый из которых предлагает свой подход к локализации и исправлению ошибок. Этот метод позволяет находить более комплексные решения, охватывающие различные части кодовой базы. В результате SWE-Debate превосходит существующие подходы в локализации проблем и планировании исправлений.'}, 'en': {'title': 'Empowering Software Issue Resolution through Competitive Multi-Agent Debate', 'desc': 'SWE-Debate is a multi-agent framework designed to improve issue resolution in software engineering by fostering diverse reasoning among agents. It utilizes large language models to enhance the reasoning capabilities of autonomous agents, allowing them to tackle complex tasks more effectively. By organizing a structured debate among agents with different perspectives, SWE-Debate helps identify broader issue patterns and achieve better localization of software faults. The framework has demonstrated significant improvements in performance on the SWE-bench benchmark, setting new standards in open-source agent frameworks.'}, 'zh': {'title': 'SWE-Debate：多样化推理促进软件问题解决', 'desc': 'SWE-Debate是一个竞争性的多智能体框架，旨在通过促进多样化的推理来增强软件工程中的问题解决能力。该框架利用大型语言模型的推理能力，帮助智能体在复杂的软件工程任务中进行自主探索。与以往的独立探索方法不同，SWE-Debate通过组织智能体之间的辩论，鼓励不同的推理路径，从而更好地定位问题并制定修复计划。实验结果表明，SWE-Debate在开源智能体框架中达到了新的最先进水平，显著优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2508.00782', 'title': 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation', 'url': 'https://huggingface.co/papers/2508.00782', 'abstract': 'SpA2V generates realistic videos aligned with input audio by leveraging spatial auditory cues and integrating them into diffusion models through video scene layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.', 'score': 3, 'issue_id': 5160, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '75fd2b7336810b40', 'authors': ['Kien T. Pham', 'Yingqing He', 'Yazhou Xing', 'Qifeng Chen', 'Long Chen'], 'affiliations': ['Hong Kong University of Science and Technology Clear Water Bay, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00782.jpg', 'data': {'categories': ['#audio', '#games', '#video', '#diffusion', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Звук в движении: от аудио к реалистичному видео', 'desc': 'SpA2V - это новый подход к генерации видео на основе аудио, который использует пространственные звуковые сигналы для создания реалистичных видеороликов. Система работает в два этапа: сначала создается план видеосцены с помощью мультимодальной языковой модели, затем этот план используется для управления диффузионной моделью генерации видео. SpA2V позволяет получать видео с точным семантическим и пространственным соответствием входному аудио. Эксперименты показывают превосходство этого метода над существующими подходами к аудио-видео синтезу.'}, 'en': {'title': 'SpA2V: Bridging Audio and Video with Spatial Awareness', 'desc': 'The paper introduces SpA2V, a novel framework for generating realistic videos that align with input audio by utilizing spatial auditory cues. Unlike previous methods that focus mainly on semantic information, SpA2V incorporates spatial attributes such as location and movement derived from audio properties like loudness and frequency. The generation process is divided into two stages: first, it creates Video Scene Layouts (VSLs) using a modified machine learning model to capture both spatial and semantic cues from the audio. Then, it integrates these VSLs into diffusion models for video generation, resulting in videos that are both semantically and spatially accurate to the audio input.'}, 'zh': {'title': '利用空间听觉线索生成真实视频', 'desc': 'SpA2V是一种生成与输入音频对齐的真实视频的新框架。它通过利用空间听觉线索，将这些线索整合到扩散模型中，从而生成视频场景布局。与传统方法不同，SpA2V不仅关注音频的语义信息，还考虑了声音的空间属性，如位置和运动方向。实验表明，SpA2V在生成与输入音频具有高语义和空间一致性的真实视频方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2508.00454', 'title': 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges', 'url': 'https://huggingface.co/papers/2508.00454', 'abstract': 'An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.', 'score': 3, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '7d97f0b64c1261dd', 'authors': ['Yuqi Tang', 'Kehua Feng', 'Yunfeng Wang', 'Zhiwen Chen', 'Chengfei Lv', 'Gang Yu', 'Qiang Zhang', 'Keyan Ding'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University', 'ZJU-UIUC Institute, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00454.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#inference', '#alignment', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Эффективная оценка диалогов: мудрость многих в одной модели', 'desc': 'Эта статья представляет эффективный метод оценки качества многоэтапных диалогов с использованием больших языковых моделей (LLM). Авторы предлагают агрегировать суждения нескольких LLM в единую модель, что позволяет сохранить преимущества разнообразных оценок, но значительно снизить вычислительные затраты. Метод показал превосходные результаты на семи эталонных наборах данных для оценки диалогов. Предложенный подход обеспечивает быструю и гибкую оценку качества диалогов, сохраняя при этом надежность и согласованность результатов.'}, 'en': {'title': 'Efficient Dialogue Evaluation: Harnessing Collective Wisdom of LLMs', 'desc': 'This paper introduces an efficient multi-turn dialogue evaluator that combines the judgments of multiple large language models (LLMs) to assess dialogue quality. Traditional methods using a single LLM as a judge often face biases that affect evaluation reliability. The proposed method aggregates the preferences of several LLMs into one model, maintaining the benefits of diverse feedback while significantly lowering computational costs. Experiments show that this new approach outperforms existing methods in various evaluation scenarios, proving its effectiveness and efficiency.'}, 'zh': {'title': '高效的多轮对话评估器：聚合智慧，降低成本', 'desc': '本文提出了一种高效的多轮对话评估器，通过将多个大型语言模型（LLM）的判断汇聚成一个单一模型来评估对话质量，从而降低计算成本。当前的评估方法主要依赖于“LLM作为评审”的模式，但这种方法常常受到偏见的影响，导致评估结果的不可靠性。为了解决这个问题，本文的方法利用多个LLM作为评审，并将它们的偏好知识汇聚到一个模型中，从而保留多评审反馈的优势，同时显著减少评估成本。实验结果表明，该方法在多种对话评估基准上优于现有的基线，展示了其高效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2507.19634', 'title': 'MCIF: Multimodal Crosslingual Instruction-Following Benchmark from\n  Scientific Talks', 'url': 'https://huggingface.co/papers/2507.19634', 'abstract': "MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.", 'score': 3, 'issue_id': 5169, 'pub_date': '2025-07-25', 'pub_date_card': {'ru': '25 июля', 'en': 'July 25', 'zh': '7月25日'}, 'hash': '6c681493be72e8eb', 'authors': ['Sara Papi', 'Maike Züfle', 'Marco Gaido', 'Beatrice Savoldi', 'Danni Liu', 'Ioannis Douros', 'Luisa Bentivogli', 'Jan Niehues'], 'affiliations': ['Fondazione Bruno Kessler (Italy)', 'Karlsruhe Institute of Technology (Germany)', 'Translated (Italy)'], 'pdf_title_img': 'assets/pdf/title_img/2507.19634.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#multilingual', '#open_source', '#long_context', '#machine_translation'], 'emoji': '🌐', 'ru': {'title': 'MCIF: Первый многоязычный мультимодальный тест для оценки MLLM', 'desc': 'MCIF - это многоязычный эталонный тест с аннотациями от людей для оценки выполнения инструкций в кросс-языковых мультимодальных средах, использующий научные доклады. Он охватывает три основные модальности - речь, зрение и текст - на четырех языках, позволяя комплексно оценивать способности мультимодальных языковых моделей (MLLM) интерпретировать инструкции на разных языках и комбинировать их с мультимодальной контекстной информацией. MCIF создан для преодоления ограничений существующих тестов, которые часто ограничены английским языком, фокусируются на одной модальности и коротких контекстах. Этот бенчмарк выпущен под лицензией CC-BY 4.0 для поощрения открытых исследований и прогресса в разработке MLLM.'}, 'en': {'title': 'MCIF: A New Benchmark for Multilingual Instruction-Following in Multimodal AI', 'desc': 'MCIF is a new benchmark designed to evaluate how well multilingual, multimodal language models can follow instructions in different languages and formats. It focuses on three modalities: speech, vision, and text, and includes human annotations to ensure quality assessments. Unlike previous benchmarks, MCIF allows for both short and long context evaluations across four languages: English, German, Italian, and Chinese. This comprehensive approach aims to enhance the understanding of model performance in real-world, crosslingual scenarios.'}, 'zh': {'title': 'MCIF：跨语言多模态指令跟随的评估新基准', 'desc': 'MCIF是一个多语言的人类标注基准，用于评估跨语言、多模态环境下的指令跟随能力。它结合了文本、语音和视觉三种核心模态，并支持英语、德语、意大利语和中文四种语言。MCIF的设计旨在填补现有基准在多语言和多模态评估方面的不足，特别是在长短文本输入的情况下。通过MCIF，研究人员可以更全面地评估多模态大语言模型的性能和能力。'}}}, {'id': 'https://huggingface.co/papers/2508.00632', 'title': 'Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings', 'url': 'https://huggingface.co/papers/2508.00632', 'abstract': 'A multi-agent system using an omni-modal evaluation metric improves JavaScript game and animation generation but struggles with custom assets and audio-visual feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.', 'score': 2, 'issue_id': 5165, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '3e4a605a070fb44f', 'authors': ['Alexia Jolicoeur-Martineau'], 'affiliations': ['Samsung SAIL Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2508.00632.jpg', 'data': {'categories': ['#audio', '#multimodal', '#games', '#optimization', '#video', '#agents'], 'emoji': '🎮', 'ru': {'title': 'Мультиагентная система для создания игр: прогресс и проблемы', 'desc': 'Представлена система мультиагентного генерирования JavaScript-игр и анимаций с использованием омнимодальной метрики оценки. Система AVR-Agent выбирает релевантные ассеты, генерирует несколько вариантов кода и итеративно улучшает их на основе обратной связи. Метрика AVR-Eval сравнивает качество мультимедийного контента, используя аудиовизуальные записи. Эксперименты показали, что система значительно улучшает генерацию контента по сравнению с одноразовой генерацией, но модели все еще испытывают трудности с эффективным использованием пользовательских ассетов и аудиовизуальной обратной связи.'}, 'en': {'title': 'Enhancing Game Generation with Multi-Agent Systems and AVR-Eval', 'desc': 'This paper presents a multi-agent system designed to enhance the generation of JavaScript games and animations using a new evaluation metric called AVR-Eval. AVR-Eval assesses multimedia content quality by comparing Audio-Visual Recordings (AVRs) through an omni-modal model that processes text, video, and audio. The system, AVR-Agent, generates code by selecting relevant multimedia assets and iteratively improving the output based on feedback from AVR-Eval. Despite achieving higher success rates in generated content, the system struggles with custom assets and effective audio-visual feedback, indicating a gap between human creativity and machine-generated content.'}, 'zh': {'title': '多智能体系统提升JavaScript游戏生成质量', 'desc': '本论文提出了一种多智能体系统，利用全模态评估指标来改善JavaScript游戏和动画的生成。我们开发了AVR-Eval，这是一种相对评估多媒体内容质量的新指标，能够有效区分优质和劣质内容。AVR-Agent是一个多智能体系统，能够从多媒体资产库中生成JavaScript代码，并通过迭代反馈不断优化生成的内容。尽管生成的内容在胜率上优于单次生成的内容，但模型在利用自定义资产和音视频反馈方面仍然存在困难，显示出人类与机器内容创作方法之间的根本差异。'}}}, {'id': 'https://huggingface.co/papers/2507.22720', 'title': 'Investigating Hallucination in Conversations for Low Resource Languages', 'url': 'https://huggingface.co/papers/2507.22720', 'abstract': "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.", 'score': 2, 'issue_id': 5155, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': 'c7f5db5f58895f4f', 'authors': ['Amit Das', 'Md. Najib Hasan', 'Souvika Sarkar', 'Zheng Zhang', 'Fatemeh Jamshidi', 'Tathagata Bhattacharya', 'Nilanjana Raychawdhury', 'Dongji Feng', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'Auburn University', 'Auburn University at Montgomery', 'California State Polytechnic University Pomona', 'Gustavus Adolphus College', 'Meta', 'Murray State University', 'Stanford University', 'University of North Alabama', 'Wichita State University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22720.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#hallucinations'], 'emoji': '🗣️', 'ru': {'title': 'Языковые модели меньше галлюцинируют по-китайски', 'desc': 'Исследование посвящено проблеме галлюцинаций в больших языковых моделях (LLM) на примере трех языков: хинди, фарси и мандаринского китайского. Авторы провели комплексный анализ фактических и лингвистических ошибок в этих языках для нескольких популярных моделей, включая GPT-3.5, GPT-4, Llama-3.1 и другие. Результаты показали, что LLM генерируют значительно меньше галлюцинаций на мандаринском китайском по сравнению с хинди и фарси. Это исследование расширяет понимание проблемы галлюцинаций за пределы английского языка, что важно для повышения надежности и эффективности LLM.'}, 'en': {'title': 'Mandarin LLMs: Fewer Hallucinations, More Accuracy!', 'desc': 'This paper investigates the phenomenon of hallucinations in Large Language Models (LLMs) across three languages: Mandarin, Hindi, and Farsi. Hallucinations refer to instances where the models generate incorrect or misleading information. The study analyzes conversational data from various LLMs, including GPT-3.5 and GPT-4o, to compare the frequency of these errors. The findings reveal that LLMs exhibit fewer hallucinations in Mandarin compared to the higher rates observed in Hindi and Farsi, highlighting the need for language-specific improvements in model training.'}, 'zh': {'title': '普通话中的幻觉现象较少', 'desc': '大型语言模型（LLMs）在生成文本方面表现出色，但它们有时会产生不准确的信息，这被称为“幻觉”。本研究探讨了在普通话、印地语和法尔西语中，LLMs的幻觉现象。我们分析了多个模型（如GPT-3.5、GPT-4o等）在这三种语言中的事实和语言错误。结果显示，LLMs在普通话中产生的幻觉响应较少，而在印地语和法尔西语中则显著更多。'}}}, {'id': 'https://huggingface.co/papers/2508.00823', 'title': 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation', 'url': 'https://huggingface.co/papers/2508.00823', 'abstract': 'IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.', 'score': 1, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': 'a4651adceaac80f7', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Hang Yin', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00823.jpg', 'data': {'categories': ['#robotics', '#3d', '#optimization', '#agents', '#games'], 'emoji': '🧭', 'ru': {'title': 'Навигация в 3D с помощью инкрементальных гауссианов', 'desc': 'IGL-Nav - это новый метод навигации по изображению-цели в трехмерном пространстве. Он использует инкрементальное представление 3D гауссианов для эффективной и точной локализации целевого изображения. Метод превосходит существующие подходы, сочетая дискретное сопоставление пространства и оптимизацию через дифференцируемый рендеринг. IGL-Nav применим в реальных условиях и может работать с произвольными ракурсами целевых изображений.'}, 'en': {'title': 'Efficient 3D Navigation with Incremental Gaussian Localization', 'desc': 'IGL-Nav introduces an innovative approach to image-goal navigation in 3D environments using an incremental 3D Gaussian representation. This method enhances localization accuracy by updating the scene representation as new images are processed, allowing for efficient navigation. Unlike traditional methods that struggle with geometric relationships, IGL-Nav utilizes geometric information for effective discrete space matching and fine target pose optimization. The framework demonstrates significant improvements over existing techniques and is suitable for real-world applications, including robotic platforms.'}, 'zh': {'title': '增量式3D高斯导航：高效准确的图像目标定位', 'desc': 'IGL-Nav是一种增量式3D高斯定位框架，旨在提高图像目标导航的效率和准确性。该方法通过可渲染的3D高斯表示来建模3D环境与目标图像之间的几何关系，克服了传统方法的局限性。IGL-Nav通过前馈单目预测逐步更新场景表示，并利用几何信息进行粗略定位，最终通过可微渲染优化精确确定目标位置。实验结果表明，IGL-Nav在多种配置下显著超越了现有的最先进方法，并能够在真实世界的机器人平台上应用。'}}}, {'id': 'https://huggingface.co/papers/2507.23726', 'title': 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2507.23726', 'abstract': 'Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.', 'score': 84, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'ab5bfbdad68eb6bf', 'authors': ['Luoxin Chen', 'Jinming Gu', 'Liankai Huang', 'Wenhao Huang', 'Zhicheng Jiang', 'Allan Jie', 'Xiaoran Jin', 'Xing Jin', 'Chenggang Li', 'Kaijing Ma', 'Cheng Ren', 'Jiawei Shen', 'Wenlei Shi', 'Tong Sun', 'He Sun', 'Jiahui Wang', 'Siran Wang', 'Zhihong Wang', 'Chenrui Wei', 'Shufa Wei', 'Yonghui Wu', 'Yuchen Wu', 'Yihang Xia', 'Huajian Xin', 'Fan Yang', 'Huaiyuan Ying', 'Hongyi Yuan', 'Zheng Yuan', 'Tianyang Zhan', 'Chi Zhang', 'Yue Zhang', 'Ge Zhang', 'Tianyun Zhao', 'Jianqiu Zhao', 'Yichi Zhou', 'Thomas Hanwen Zhu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2507.23726.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматическом доказательстве теорем с помощью ИИ', 'desc': 'Seed-Prover - это модель для автоматического доказательства теорем, использующая язык Lean. Она применяет итеративное уточнение доказательств и специализированную поддержку геометрии. Модель достигает высокой производительности в формальном доказательстве теорем и автоматизированных математических рассуждениях. Seed-Prover превосходит предыдущие системы на нескольких эталонных наборах задач, включая формализованные задачи Международной математической олимпиады.'}, 'en': {'title': 'Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement', 'desc': 'The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning.'}, 'zh': {'title': 'Seed-Prover：自动化数学推理的新突破', 'desc': 'Seed-Prover是一种基于Lean的引理风格推理模型，能够在形式定理证明和自动数学推理中实现高性能。该模型通过迭代优化和专门的几何支持，克服了传统自然语言推理的局限性。Seed-Prover利用Lean的反馈和自我总结来不断改进其证明过程，并设计了三种推理策略以应对国际数学奥林匹克（IMO）级别的问题。通过引入Seed-Geometry几何推理引擎，Seed-Prover在几何问题上也取得了显著进展，展示了形式验证与长链推理的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.23779', 'title': 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding', 'url': 'https://huggingface.co/papers/2507.23779', 'abstract': 'The Phi-Ground model family achieves state-of-the-art performance in GUI grounding for multimodal reasoning models, improving accuracy across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from "Iron Man", are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textbf{43.2} on ScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}', 'score': 35, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e6bd9c919aacc874', 'authors': ['Miaosen Zhang', 'Ziqiang Xu', 'Jialiang Zhu', 'Qi Dai', 'Kai Qiu', 'Yifan Yang', 'Chong Luo', 'Tianyi Chen', 'Justin Wagle', 'Tim Franklin', 'Baining Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.23779.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#reasoning', '#training', '#multimodal'], 'emoji': '🖥️', 'ru': {'title': 'Phi-Ground: прорыв в точности привязки GUI для ИИ-агентов', 'desc': 'Семейство моделей Phi-Ground достигает передовых результатов в задаче привязки графического интерфейса для мультимодальных моделей рассуждения. Эти модели улучшают точность на различных бенчмарках для компьютерных агентов, способных взаимодействовать с GUI. Авторы провели эмпирическое исследование процесса обучения моделей привязки, рассмотрев детали от сбора данных до тренировки. Результаты работы могут быть полезны не только для создания моделей привязки, но и для других задач восприятия.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Phi-Ground Models', 'desc': 'The Phi-Ground model family significantly enhances GUI grounding for multimodal reasoning models, achieving top performance on various benchmarks. This model is crucial for Computer Use Agents (CUAs) to perform tasks like clicking and typing accurately, which is essential for their effectiveness. Despite existing models struggling with accuracy below 65% on tough benchmarks, Phi-Ground demonstrates superior results, scoring 43.2 on ScreenSpot-pro and 27.2 on UI-Vision. The paper details the training process and insights gained, which can also aid in improving other perception tasks in machine learning.'}, 'zh': {'title': 'Phi-Ground：多模态推理的GUI定位新突破', 'desc': 'Phi-Ground模型系列在多模态推理模型的GUI定位方面达到了最先进的性能，提升了在多个基准测试中的准确性。GUI定位是计算机使用代理（CUA）执行实际操作的核心部分，直接影响系统的成功与否。当前的端到端定位模型在一些具有挑战性的基准测试中准确率仍低于65%，显示出其在实际应用中的不足。本文通过对定位模型的训练进行实证研究，最终开发出Phi-Ground模型系列，在代理设置下的五个定位基准测试中均取得了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.22879', 'title': 'RecGPT Technical Report', 'url': 'https://huggingface.co/papers/2507.22879', 'abstract': "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.", 'score': 22, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '2bd5536810f1694b', 'authors': ['Chao Yi', 'Dian Chen', 'Gaoyang Guo', 'Jiakai Tang', 'Jian Wu', 'Jing Yu', 'Mao Zhang', 'Sunhao Dai', 'Wen Chen', 'Wenjun Yang', 'Yuning Jiang', 'Zhujin Gao', 'Bo Zheng', 'Chi Li', 'Dimin Wang', 'Dixuan Wang', 'Fan Li', 'Fan Zhang', 'Haibin Chen', 'Haozhuang Liu', 'Jialin Zhu', 'Jiamang Wang', 'Jiawei Wu', 'Jin Cui', 'Ju Huang', 'Kai Zhang', 'Kan Liu', 'Lang Tian', 'Liang Rao', 'Longbin Li', 'Lulu Zhao', 'Na He', 'Peiyang Wang', 'Qiqi Huang', 'Tao Luo', 'Wenbo Su', 'Xiaoxiao He', 'Xin Tong', 'Xu Chen', 'Xunke Xi', 'Yang Li', 'Yaxuan Wu', 'Yeqiu Yang', 'Yi Hu', 'Yinnan Song', 'Yuchen Li', 'Yujie Luo', 'Yujin Yuan', 'Yuliang Yan', 'Zhengyang Wang', 'Zhibo Xiao', 'Zhixin Ma', 'Zile Zhou', 'Ziqi Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.22879.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#alignment', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'RecGPT: Рекомендации, ориентированные на намерения пользователей', 'desc': 'RecGPT - это новая система рекомендаций, интегрирующая большие языковые модели (LLM) для фокусировки на намерениях пользователей. Она улучшает разнообразие контента и удовлетворенность пользователей, а также повышает эффективность для продавцов и платформы. RecGPT использует многоэтапную парадигму обучения, включающую предварительное выравнивание с усиленным рассуждением и эволюцию самообучения. Система уже развернута в приложении Taobao и показывает стабильный рост производительности для всех заинтересованных сторон.'}, 'en': {'title': 'Empowering Recommendations with User Intent', 'desc': 'RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem.'}, 'zh': {'title': '以用户意图为中心的推荐系统新范式', 'desc': 'RecGPT 是一种将大型语言模型整合到推荐系统中的新框架，旨在关注用户意图。通过重新设计推荐流程，RecGPT 使推荐过程从单纯依赖历史数据转变为以用户意图为中心。该系统通过多阶段训练方法，结合推理增强的预对齐和自我训练，提升了推荐的准确性和多样性。实验结果表明，RecGPT 在用户满意度、商家曝光率和平台转化率等方面均取得了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2507.23682', 'title': 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models', 'url': 'https://huggingface.co/papers/2507.23682', 'abstract': 'The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.', 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'baa73e4730b01a97', 'authors': ['Xiaoyu Chen', 'Hangxing Wei', 'Pushi Zhang', 'Chuheng Zhang', 'Kaixin Wang', 'Yanjiang Guo', 'Rushuai Yang', 'Yucen Wang', 'Xinquan Xiao', 'Li Zhao', 'Jianyu Chen', 'Jiang Bian'], 'affiliations': ['Hong Kong University of Science and Technology', 'Microsoft Research', 'Nanjing University', 'Tsinghua University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23682.jpg', 'data': {'categories': ['#optimization', '#agents', '#agi', '#games', '#robotics', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'ViLLA: Улучшение роботизированных манипуляций с помощью латентных действий', 'desc': 'Фреймворк ViLLA улучшает модели визуально-языкового действия (VLA) путем включения латентных действий. Это позволяет повысить производительность как в симулированных, так и в реальных задачах роботизированных манипуляций. Предложенный подход villa-X совершенствует как обучение латентным действиям, так и их интеграцию в предобучение VLA. Модель демонстрирует превосходные результаты в симуляторах SIMPLER и LIBERO, а также на реальных роботах с захватами и ловкими руками.'}, 'en': {'title': 'Enhancing Robot Manipulation with Latent Actions in ViLLA Framework', 'desc': 'The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research.'}, 'zh': {'title': 'ViLLA框架：提升机器人操作的潜力', 'desc': 'ViLLA框架通过引入潜在动作来增强视觉-语言-动作（VLA）模型，从而提高机器人操作任务的性能。潜在动作是一种抽象表示，能够捕捉两个帧之间的视觉变化。本文介绍的villa-X是一个新颖的视觉-语言-潜在动作框架，旨在改进潜在动作建模，以学习可推广的机器人操作策略。我们的研究表明，villa-X在模拟环境和真实机器人设置中均表现出色，展示了ViLLA范式的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.22968', 'title': 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations', 'url': 'https://huggingface.co/papers/2507.22968', 'abstract': "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.", 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '3a2f5273d610d5d6', 'authors': ['Chengqian Ma', 'Wei Tao', 'Yiwen Guo'], 'affiliations': ['Independent Researcher', 'LIGHTSPEED', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22968.jpg', 'data': {'categories': ['#survey', '#long_context', '#benchmark', '#dataset', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'Бенчмарк для оценки разговорных ИИ-моделей в реальных условиях', 'desc': 'В статье представлен набор данных для оценки разговорных диалоговых моделей на английском и китайском языках. Этот бенчмарк позволяет оценить способность моделей понимать и имитировать человеческие разговоры, учитывая такие сложности как неоднозначность и контекстная зависимость. Набор данных содержит 1079 примеров и сопровождается методом оценки на основе больших языковых моделей, который хорошо коррелирует с человеческими оценками. Исследование направлено на комплексное изучение эффективности разговорных диалоговых моделей в решении практических задач.'}, 'en': {'title': 'Benchmarking Spoken Dialogue Models for Real-World Conversations', 'desc': "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."}, 'zh': {'title': '提升口语对话模型的评估标准', 'desc': '本文提出了一个用于评估口语对话模型（SDMs）性能的基准数据集，涵盖英语和中文，旨在理解和模拟人类口语对话。口语对话的复杂性体现在歧义性和上下文依赖性等挑战上，这些因素使得与文本基础的大型语言模型（LLMs）相比，SDMs的研究相对较少。数据集中包含1079个实例，并配备了一种基于LLM的评估方法，以更好地与人类判断相一致。通过这个数据集，研究者可以全面探讨SDMs在应对实际对话挑战中的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.23277', 'title': 'iLRM: An Iterative Large 3D Reconstruction Model', 'url': 'https://huggingface.co/papers/2507.23277', 'abstract': 'iLRM, an iterative Large 3D Reconstruction Model, improves scalability and efficiency in 3D reconstruction by decoupling scene representation, using a two-stage attention scheme, and injecting high-resolution information.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.', 'score': 19, 'issue_id': 5137, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'af6d37e5e25a6d73', 'authors': ['Gyeongjin Kang', 'Seungtae Nam', 'Xiangyu Sun', 'Sameh Khamis', 'Abdelrahman Mohamed', 'Eunbyung Park'], 'affiliations': ['Rembrand Project', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23277.jpg', 'data': {'categories': ['#optimization', '#3d'], 'emoji': '🏛️', 'ru': {'title': 'Эффективная 3D-реконструкция: итеративный подход с гауссовым представлением', 'desc': 'iLRM - это итеративная модель для масштабируемой и эффективной 3D-реконструкции. Она использует трехмерное гауссово представление сцены, двухэтапную схему внимания и внедрение высокоразрешающей информации. Модель решает проблемы масштабируемости, характерные для трансформерных архитектур при обработке множества ракурсов. Эксперименты показывают превосходство iLRM по качеству и скорости реконструкции по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with iLRM: Scalable and Efficient!', 'desc': 'The paper presents the iterative Large 3D Reconstruction Model (iLRM), which enhances the scalability and efficiency of 3D reconstruction processes. It achieves this by decoupling the scene representation from the input images, allowing for more compact 3D models. Additionally, iLRM employs a two-stage attention mechanism to minimize computational costs associated with multi-view interactions. By incorporating high-resolution information at each layer, the model ensures high-fidelity reconstructions while maintaining superior performance across various datasets.'}, 'zh': {'title': 'iLRM：高效可扩展的3D重建新模型', 'desc': 'iLRM（迭代大型3D重建模型）通过解耦场景表示、采用两阶段注意力机制和注入高分辨率信息，提升了3D重建的可扩展性和效率。该模型通过迭代优化生成3D高斯表示，能够在多个输入视图下实现快速且高质量的重建。与传统的全注意力机制相比，iLRM显著降低了计算成本，同时保持了重建的高保真度。实验结果表明，iLRM在重建质量和速度上均优于现有方法，尤其在处理更多输入视图时表现出更好的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2507.21509', 'title': 'Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models', 'url': 'https://huggingface.co/papers/2507.21509', 'abstract': "Persona vectors in large language models can monitor and control personality changes during training and deployment, enabling the identification and mitigation of undesirable traits.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.", 'score': 17, 'issue_id': 5127, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '8088854aaf027260', 'authors': ['Runjin Chen', 'Andy Arditi', 'Henry Sleight', 'Owain Evans', 'Jack Lindsey'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'Constellation', 'Truthful AI', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.21509.jpg', 'data': {'categories': ['#rlhf', '#hallucinations', '#data', '#ethics', '#training', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Векторы персоны: ключ к контролю личности ИИ-ассистентов', 'desc': 'Статья представляет концепцию векторов персоны в больших языковых моделях. Эти векторы позволяют отслеживать и контролировать изменения личности ассистента во время обучения и использования модели. Исследователи обнаружили, что векторы персоны могут предсказывать сдвиги в личности после дообучения и помогают выявлять нежелательные черты. Метод извлечения векторов персоны автоматизирован и может применяться к любой интересующей черте личности.'}, 'en': {'title': 'Controlling AI Personalities with Persona Vectors', 'desc': "This paper introduces the concept of persona vectors in large language models, which are used to track and manage personality traits during the model's training and deployment phases. The authors demonstrate that these vectors can identify undesirable traits like harmful behavior or excessive flattery by analyzing the model's activation space. They show that personality shifts can be predicted and controlled, allowing for interventions to mitigate negative changes. Additionally, the method can flag problematic training data that may lead to these undesirable personality traits, making it a valuable tool for improving AI behavior."}, 'zh': {'title': '利用人格向量控制语言模型的人格变化', 'desc': '本文探讨了在大型语言模型中使用人格向量来监控和控制模型在训练和部署过程中的人格变化。研究发现，模型的激活空间中存在与多种人格特征相关的人格向量，例如恶意、谄媚和幻觉倾向。通过这些向量，可以预测和控制在微调后可能出现的人格变化，并且可以通过后期干预来减轻这些变化。该方法是自动化的，可以应用于任何感兴趣的人格特征，只需提供自然语言描述。'}}}, {'id': 'https://huggingface.co/papers/2507.23698', 'title': 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents', 'url': 'https://huggingface.co/papers/2507.23698', 'abstract': "Reinforcement Learning enhances generalizable spatial reasoning and interaction in 3D environments through cross-view goal specification and automated task synthesis, achieving zero-shot generalization and improved interaction success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.", 'score': 7, 'issue_id': 5125, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '4cb697aecb943154', 'authors': ['Shaofei Cai', 'Zhancun Mu', 'Haiwen Xia', 'Bowei Zhang', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'School of Computing, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23698.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#games', '#3d', '#rl'], 'emoji': '🧠', 'ru': {'title': 'RL открывает новые горизонты пространственного мышления для ИИ', 'desc': 'Данная статья представляет метод улучшения пространственного мышления и взаимодействия агентов в 3D-средах с помощью обучения с подкреплением (RL). Авторы предлагают использовать кросс-видовую спецификацию целей и автоматизированный синтез задач для достижения обобщения без предварительного обучения. Эксперименты проводились в среде Minecraft и показали значительное улучшение успешности взаимодействия агентов. Результаты демонстрируют потенциал обучения с подкреплением для развития визуально-моторных навыков искусственных агентов.'}, 'en': {'title': 'Empowering 3D Agents with Reinforcement Learning for Generalized Interaction', 'desc': 'This paper discusses how Reinforcement Learning (RL) can improve the ability of agents to understand and interact in 3D environments, like Minecraft. It addresses the problem of RL models overfitting to specific tasks, which limits their ability to generalize to new situations. By using cross-view goal specification and automated task synthesis, the authors show that RL can help agents achieve zero-shot generalization, meaning they can perform well in unseen environments without prior training. The results indicate that RL can significantly enhance interaction success rates and spatial reasoning in diverse settings, including real-world applications.'}, 'zh': {'title': '强化学习：提升3D环境中的空间推理与交互能力', 'desc': '强化学习（RL）在3D环境中通过跨视角目标指定和自动化任务合成，增强了可推广的空间推理和交互能力，实现了零样本泛化和提高的交互成功率。本文探讨了RL在Minecraft中微调的视觉运动代理如何在未见过的世界中实现零样本泛化。我们分析并建立了跨视角目标指定作为视觉运动策略的统一多任务目标空间，以应对多任务RL表示中的挑战。此外，我们提出在高度可定制的Minecraft环境中进行自动化任务合成，以支持大规模多任务RL训练。'}}}, {'id': 'https://huggingface.co/papers/2507.23374', 'title': 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2507.23374', 'abstract': 'NeRF-GS combines Neural Radiance Fields and 3D Gaussian Splatting to enhance 3D scene representation and performance through joint optimization and shared spatial information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.', 'score': 6, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '3bc4d2b12fc82c0f', 'authors': ['Shuangkang Fang', 'I-Chao Shen', 'Takeo Igarashi', 'Yufeng Wang', 'ZeSheng Wang', 'Yi Yang', 'Wenrui Ding', 'Shuchang Zhou'], 'affiliations': ['Beihang University', 'StepFun', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2507.23374.jpg', 'data': {'categories': ['#3d', '#benchmark'], 'emoji': '🌟', 'ru': {'title': 'NeRF-GS: Синергия нейронных полей и гауссова сплаттинга для революционного 3D-моделирования', 'desc': 'NeRF-GS - это новая система, объединяющая нейронные радиационные поля (NeRF) и трехмерное гауссово сплаттинг (3DGS) для улучшения представления 3D-сцен. Она использует непрерывное пространственное представление NeRF для устранения ограничений 3DGS, таких как чувствительность к инициализации гауссианов и слабые межгауссовые корреляции. NeRF-GS оптимизирует обе модели, используя общую пространственную информацию, и вводит оптимизацию остаточных векторов для улучшения персонализированных возможностей 3DGS. Экспериментальные результаты показывают, что NeRF-GS превосходит существующие методы и достигает наилучших показателей в представлении 3D-сцен.'}, 'en': {'title': 'Enhancing 3D Scene Representation with NeRF-GS', 'desc': 'NeRF-GS is a new framework that combines Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) to improve how 3D scenes are represented. By optimizing both methods together, it addresses the weaknesses of 3DGS, such as its sensitivity to initial conditions and limited understanding of spatial relationships. The framework aligns the spatial features of 3DGS with those of NeRF, allowing for better performance through shared information. Experiments show that NeRF-GS outperforms existing techniques, highlighting the benefits of integrating these two approaches for enhanced 3D scene representation.'}, 'zh': {'title': 'NeRF-GS：融合神经辐射场与三维高斯点云的创新框架', 'desc': 'NeRF-GS是一个新颖的框架，它结合了神经辐射场（NeRF）和三维高斯点云（3DGS），通过联合优化和共享空间信息来增强三维场景的表示和性能。该框架利用NeRF的连续空间表示，克服了3DGS的一些局限性，如对高斯初始化的敏感性和空间意识的不足。通过逐步对齐3DGS的空间特征与NeRF，NeRF-GS使得两种表示能够在同一场景中共同优化。实验结果表明，NeRF-GS在基准数据集上超越了现有方法，达到了最先进的性能，证明了NeRF和3DGS是互补的，而非竞争的。'}}}, {'id': 'https://huggingface.co/papers/2507.21584', 'title': 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination\n  Reduction in MLLMs', 'url': 'https://huggingface.co/papers/2507.21584', 'abstract': 'TARS, a token-adaptive preference strategy, improves multimodal large language models by reducing hallucinations through min-max optimization under semantic constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics.', 'score': 6, 'issue_id': 5128, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': 'e9b8a4abec301022', 'authors': ['Kejia Zhang', 'Keda Tao', 'Zhiming Luo', 'Chang Liu', 'Jiasheng Tang', 'Huan Wang'], 'affiliations': ['AWS AI Lab, Amazon', 'DAMO Academy, Alibaba Group', 'Department of Artificial Intelligence, Xiamen University', 'Hupan Laboratory', 'School of Engineering, Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2507.21584.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#multimodal', '#benchmark', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'TARS: Адаптивная оптимизация для борьбы с галлюцинациями в мультимодальных ИИ', 'desc': 'TARS - это новая стратегия оптимизации предпочтений для мультимодальных больших языковых моделей. Она использует min-max оптимизацию с семантическими ограничениями для уменьшения галлюцинаций. TARS максимизирует сдвиги распределения на уровне токенов, одновременно минимизируя ожидаемые потери предпочтений. Это позволяет сохранить причинно-следственную связь и уменьшить переобучение на паттернах предпочтений, снижая уровень галлюцинаций в мультимодальных рассуждениях.'}, 'en': {'title': 'TARS: Reducing Hallucinations in MLLMs with Adaptive Preferences', 'desc': 'The paper introduces TARS, a novel token-adaptive preference strategy designed to enhance multimodal large language models (MLLMs) by minimizing hallucinations. TARS reformulates direct preference optimization (DPO) as a min-max optimization problem, allowing for dynamic adjustments to token-level distributions while adhering to semantic constraints. This approach helps prevent overfitting to fixed preference patterns, which can lead to misleading outputs, by introducing controlled perturbations that maintain causal grounding. The results demonstrate that TARS significantly reduces hallucination rates and improves performance on various benchmarks, outperforming traditional DPO methods.'}, 'zh': {'title': 'TARS：减少幻觉的智能偏好策略', 'desc': 'TARS是一种基于令牌自适应的偏好策略，旨在通过在语义约束下进行最小-最大优化来减少多模态大语言模型中的幻觉现象。传统的直接偏好优化（DPO）方法通常将幻觉相关的偏好视为固定目标，导致模型过拟合于表面语言线索。TARS通过重新构建DPO为最小-最大优化问题，最大化令牌级别的分布变化，同时最小化期望的偏好损失，从而保持因果基础并减少幻觉。实验表明，TARS在多个基准测试中表现优异，显著降低了幻觉率。'}}}, {'id': 'https://huggingface.co/papers/2507.20519', 'title': 'AgroBench: Vision-Language Model Benchmark in Agriculture', 'url': 'https://huggingface.co/papers/2507.20519', 'abstract': 'AgroBench evaluates vision-language models across agricultural tasks, revealing areas for improvement in fine-grained identification, particularly weed identification, with expert-annotated categories.  \t\t\t\t\tAI-generated summary \t\t\t\t Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), a benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers a state-of-the-art range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal that VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most open-source VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/AgroBenchPage/ .', 'score': 4, 'issue_id': 5125, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'efa19cc739cbe95e', 'authors': ['Risa Shinoda', 'Nakamasa Inoue', 'Hirokatsu Kataoka', 'Masaki Onishi', 'Yoshitaka Ushiku'], 'affiliations': ['Kyoto University', 'National Institute of Advanced Industrial Science and Technology (AIST)', 'OMRON SINIC', 'The University of Osaka', 'Tokyo Institute of Technology', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2507.20519.jpg', 'data': {'categories': ['#cv', '#open_source', '#science', '#dataset', '#benchmark'], 'emoji': '🌾', 'ru': {'title': 'AgroBench: экспертная оценка ИИ в сельском хозяйстве', 'desc': 'AgroBench - это новый эталонный тест для оценки моделей компьютерного зрения и обработки естественного языка в сельскохозяйственных задачах. Он охватывает семь сельскохозяйственных тем и включает 203 категории культур и 682 категории болезней, аннотированные экспертами-агрономами. Тестирование показало, что существующие модели имеют значительные возможности для улучшения в задачах точной идентификации, особенно при распознавании сорняков. Авторы анализируют типы ошибок моделей и предлагают пути для их дальнейшего развития.'}, 'en': {'title': 'Enhancing Agricultural AI: The AgroBench Benchmark', 'desc': 'AgroBench is a benchmark designed to evaluate vision-language models (VLMs) specifically in the context of agricultural tasks. It focuses on fine-grained identification, such as accurately recognizing different types of weeds and diseases in crops, using categories annotated by expert agronomists. The benchmark includes a comprehensive set of 203 crop categories and 682 disease categories, highlighting the current limitations of VLMs in these areas. The findings indicate that many existing VLMs struggle with precise identification, particularly in weed detection, suggesting significant opportunities for improvement in future model development.'}, 'zh': {'title': '提升农业任务中的视觉-语言模型表现', 'desc': 'AgroBench是一个用于评估视觉-语言模型（VLM）在农业任务中的表现的基准。它涵盖了七个农业主题，并由专家农学家进行标注，确保数据的准确性。研究发现，当前的VLM在细粒度识别任务，特别是杂草识别方面表现不佳，许多开源模型的表现接近随机。通过分析VLM的错误类型，AgroBench为未来的模型发展提供了改进的方向。'}}}, {'id': 'https://huggingface.co/papers/2507.23436', 'title': 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification', 'url': 'https://huggingface.co/papers/2507.23436', 'abstract': "Enhancing dual-teacher self-supervised frameworks with Kolmogorov-Arnold Networks improves art style classification by better modeling nonlinear feature correlations and disentangling complex style manifolds.  \t\t\t\t\tAI-generated summary \t\t\t\t Art style classification remains a formidable challenge in computational aesthetics due to the scarcity of expertly labeled datasets and the intricate, often nonlinear interplay of stylistic elements. While recent dual-teacher self-supervised frameworks reduce reliance on labeled data, their linear projection layers and localized focus struggle to model global compositional context and complex style-feature interactions. We enhance the dual-teacher knowledge distillation framework to address these limitations by replacing conventional MLP projection and prediction heads with Kolmogorov-Arnold Networks (KANs). Our approach retains complementary guidance from two teacher networks, one emphasizing localized texture and brushstroke patterns, the other capturing broader stylistic hierarchies while leveraging KANs' spline-based activations to model nonlinear feature correlations with mathematical precision. Experiments on WikiArt and Pandora18k demonstrate that our approach outperforms the base dual teacher architecture in Top-1 accuracy. Our findings highlight the importance of KANs in disentangling complex style manifolds, leading to better linear probe accuracy than MLP projections.", 'score': 3, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '43c3429e74f56b34', 'authors': ['Abdellah Zakaria Sellam', 'Salah Eddine Bekhouche', 'Cosimo Distante', 'Abdelmalik Taleb-Ahmed'], 'affiliations': ['Department of Innovation Engineering, University of Salento', 'Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, 73100 Lecce, Italy', 'UPV/EHU, University of the Basque Country, 20018 San Sebastian, Spain', 'Université Polytechnique Hauts-de-France, Université de Lille, CNRS, 59313 Valenciennes, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.23436.jpg', 'data': {'categories': ['#cv', '#math', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Сети Колмогорова-Арнольда улучшают самообучение в классификации стилей искусства', 'desc': 'Статья представляет усовершенствованный метод самообучения с двумя учителями для классификации стилей искусства. Авторы заменяют стандартные MLP-слои на сети Колмогорова-Арнольда (KAN) для лучшего моделирования нелинейных корреляций признаков. Этот подход сохраняет преимущества двух сетей-учителей, фокусируясь как на локальных текстурах, так и на глобальных стилистических иерархиях. Эксперименты показывают превосходство предложенного метода над базовой архитектурой с двумя учителями в точности классификации.'}, 'en': {'title': 'Harnessing KANs for Superior Art Style Classification', 'desc': 'This paper presents an enhancement to dual-teacher self-supervised frameworks for art style classification by integrating Kolmogorov-Arnold Networks (KANs). The authors argue that traditional linear projection layers fail to capture the complex, nonlinear relationships between stylistic features. By using KANs, which utilize spline-based activations, the model can better represent these intricate correlations and disentangle complex style manifolds. Experimental results show that this improved framework significantly increases classification accuracy on datasets like WikiArt and Pandora18k compared to the original dual-teacher architecture.'}, 'zh': {'title': '利用KANs提升艺术风格分类的准确性', 'desc': '本论文提出了一种增强的双教师自监督框架，通过引入Kolmogorov-Arnold网络（KANs）来改善艺术风格分类。传统的线性投影层无法有效建模复杂的风格特征交互，而KANs能够更好地捕捉非线性特征相关性。我们的方法结合了两个教师网络的互补指导，一个专注于局部纹理和笔触模式，另一个则关注更广泛的风格层次。实验结果表明，使用KANs的框架在WikiArt和Pandora18k数据集上显著提高了分类准确率。'}}}, {'id': 'https://huggingface.co/papers/2507.23632', 'title': 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective', 'url': 'https://huggingface.co/papers/2507.23632', 'abstract': 'Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.', 'score': 2, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'b07ddf6cb6b8bee8', 'authors': ['Gabriel Mongaras', 'Eric C. Larson'], 'affiliations': ['Lyle School of Engineering Southern Methodist University Dallas, TX 75205'], 'pdf_title_img': 'assets/pdf/title_img/2507.23632.jpg', 'data': {'categories': ['#optimization', '#architecture', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая силу софтмакс-внимания через призму RNN', 'desc': 'Статья исследует различия между софтмакс-вниманием и линейным вниманием в нейронных сетях. Авторы показывают, что софтмакс-внимание можно представить в рекуррентной форме, аналогичной рекуррентным нейронным сетям (RNN). Это позволяет провести анализ компонентов софтмакс-внимания и объяснить его большую выразительность по сравнению с линейными аналогами. Работа помогает понять, почему софтмакс-внимание остается основой современных трансформерных архитектур, несмотря на квадратичную сложность.'}, 'en': {'title': 'Unlocking the Power of Softmax Attention', 'desc': "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."}, 'zh': {'title': '软max注意力的优势解析', 'desc': '本文探讨了softmax注意力与线性注意力的区别。softmax注意力因其表达能力强而成为现代变换器架构的基础，但其在序列长度上的计算复杂度和内存需求是一个主要缺点。通过将softmax非线性替换为线性注意力，研究者们试图解决这一瓶颈。本文表明，线性注意力实际上是softmax注意力的一种近似，并通过递归神经网络的语言来描述softmax注意力的各个部分，从而揭示其更强表达能力的原因。'}}}, {'id': 'https://huggingface.co/papers/2507.14793', 'title': 'Flow Equivariant Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2507.14793', 'abstract': "Equivariant neural network architectures are extended to handle time-parameterized transformations, improving performance in sequence models like RNNs.  \t\t\t\t\tAI-generated summary \t\t\t\t Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.", 'score': 2, 'issue_id': 5126, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'eb29bf11c603c730', 'authors': ['T. Anderson Keller'], 'affiliations': ['The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA 15213'], 'pdf_title_img': 'assets/pdf/title_img/2507.14793.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '⏳', 'ru': {'title': 'Эквивариантность во времени: новый подход к обработке последовательностей', 'desc': "Статья расширяет концепцию эквивариантных нейронных сетей для обработки преобразований, параметризованных во времени. Это улучшает производительность рекуррентных нейронных сетей (RNN) и других последовательностных моделей. Авторы вводят понятие 'потоков' - однопараметрических подгрупп Ли, описывающих естественные трансформации во времени, такие как визуальное движение. Эксперименты показывают, что потоково-эквивариантные модели значительно превосходят стандартные RNN по скорости обучения и способности к обобщению."}, 'en': {'title': 'Enhancing RNNs with Time-Parameter Equivariance', 'desc': 'This paper extends equivariant neural network architectures to include time-parameterized transformations, which enhances their performance in sequence models like recurrent neural networks (RNNs). It highlights that traditional RNNs do not adequately account for the smooth, continuous changes in data over time, leading to inefficiencies. By introducing flow equivariance, the authors demonstrate that these new models can better handle temporal symmetries, resulting in improved training speed and generalization capabilities. This work aims to create sequence models that align more closely with the natural transformations observed in the real world.'}, 'zh': {'title': '提升序列模型性能的时间等变网络', 'desc': '本文扩展了等变神经网络架构，以处理时间参数化的变换，从而提高序列模型（如RNN）的性能。我们发现标准的RNN通常不具备流等变性，无法以几何结构的方式对移动刺激进行变换。通过引入流等变性，我们的模型在训练速度、长度泛化和速度泛化等方面显著优于非等变模型。此研究为构建尊重时间参数化对称性的序列模型奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2507.23404', 'title': 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring', 'url': 'https://huggingface.co/papers/2507.23404', 'abstract': 'An enhanced Dense Passage Retrieval framework for Arabic uses a novel Attentive Relevance Scoring mechanism to improve retrieval performance and ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at https://github.com/Bekhouche/APR{GitHub}.', 'score': 1, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '5e9a40999faf8be8', 'authors': ['Salah Eddine Bekhouche', 'Azeddine Benlamoudi', 'Yazid Bounab', 'Fadi Dornaika', 'Abdenour Hadid'], 'affiliations': ['Faculty of Pharmacy, Helsinki University, Helsinki, Finland', 'IKERBASQUE, Basque Foundation for Science, Bilbao, Spain', 'Lab. de Genie Electrique (LAGE), University of Ouargla, Ouargla, Algeria', 'Sorbonne University Abu Dhabi, Abu Dhabi, UAE', 'University of the Basque Country UPV/EHU, San Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.23404.jpg', 'data': {'categories': ['#architecture', '#open_source', '#low_resource', '#multilingual', '#dataset'], 'emoji': '🕌', 'ru': {'title': 'Улучшенный поиск по арабским текстам с помощью механизма внимания', 'desc': 'Статья представляет усовершенствованную систему плотного поиска пассажей (Dense Passage Retrieval) для арабского языка. Авторы разработали новый механизм оценки релевантности на основе внимания (Attentive Relevance Scoring), который более эффективно моделирует семантическую связь между вопросами и текстовыми фрагментами. Система интегрирует предобученные языковые модели для арабского языка и архитектурные улучшения для повышения точности ранжирования при ответах на вопросы. Код проекта доступен в открытом репозитории на GitHub.'}, 'en': {'title': 'Enhancing Arabic Retrieval with Attentive Relevance Scoring', 'desc': 'This paper introduces an improved Dense Passage Retrieval (DPR) framework tailored for the Arabic language, addressing its unique challenges in natural language processing. The key innovation is the Attentive Relevance Scoring (ARS) mechanism, which enhances the way relevance is assessed between questions and passages. By utilizing pre-trained Arabic language models and refining the architecture, the framework boosts both retrieval performance and ranking accuracy. This advancement aims to better support information retrieval tasks in Arabic, a language that has been underrepresented in NLP research.'}, 'zh': {'title': '提升阿拉伯语检索性能的新方法', 'desc': '本文提出了一种针对阿拉伯语的增强型密集段落检索框架，旨在提高检索性能和排名准确性。我们引入了一种新颖的注意力相关评分机制，替代了传统的交互机制，更有效地建模问题与段落之间的语义相关性。该方法结合了预训练的阿拉伯语语言模型和架构改进，显著提升了回答阿拉伯语问题时的检索效果。我们的代码已公开，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2507.23257', 'title': 'Efficient Machine Unlearning via Influence Approximation', 'url': 'https://huggingface.co/papers/2507.23257', 'abstract': 'The paper introduces the Influence Approximation Unlearning (IAU) algorithm, which leverages incremental learning principles to efficiently address the computational challenges of influence-based unlearning in machine learning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU.', 'score': 0, 'issue_id': 5131, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e1e0a29f18521e64', 'authors': ['Jiawei Liu', 'Chenwang Wu', 'Defu Lian', 'Enhong Chen'], 'affiliations': ['Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23257.jpg', 'data': {'categories': ['#optimization', '#security', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективное разобучение через призму инкрементного обучения', 'desc': 'Статья представляет алгоритм Influence Approximation Unlearning (IAU) для эффективного машинного разобучения. IAU использует принципы инкрементного обучения, чтобы преодолеть вычислительные сложности разобучения на основе влияния в моделях машинного обучения. Алгоритм устанавливает теоретическую связь между запоминанием (инкрементное обучение) и забыванием (разобучение), что позволяет решать задачу разобучения с точки зрения инкрементного обучения. Эмпирические исследования показывают, что IAU достигает превосходного баланса между гарантией удаления, эффективностью разобучения и сохранением полезности модели.'}, 'en': {'title': 'Efficient Unlearning through Incremental Learning: Introducing IAU', 'desc': 'The paper presents the Influence Approximation Unlearning (IAU) algorithm, which aims to improve the efficiency of influence-based unlearning in machine learning models. It addresses the high computational costs associated with traditional methods that require extensive calculations of the Hessian matrix for each training sample. By drawing parallels between the processes of memorizing and forgetting, the authors leverage incremental learning techniques to facilitate more efficient unlearning. Empirical results show that IAU not only ensures effective data removal but also maintains model performance, outperforming existing unlearning methods.'}, 'zh': {'title': '高效遗忘：影响近似遗忘算法的创新之路', 'desc': '本文介绍了一种名为影响近似遗忘（IAU）算法的新方法，该算法利用增量学习的原理来高效解决基于影响的遗忘在机器学习模型中的计算挑战。随着隐私问题的日益关注，机器遗忘成为一个重要的研究领域，尤其是影响基于遗忘的方法因其无需重新训练模型而受到青睐。然而，现有方法在计算海森矩阵及其逆矩阵时面临巨大的计算开销，限制了其在大规模模型中的应用。通过建立记忆（增量学习）与遗忘（遗忘学习）之间的理论联系，IAU算法实现了更高效的机器遗忘，且在多个数据集和模型架构上表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.02193', 'title': 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference', 'url': 'https://huggingface.co/papers/2508.02193', 'abstract': 'Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.', 'score': 61, 'issue_id': 5199, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'bec183ec45598da2', 'authors': ['Yuxuan Song', 'Zheng Zhang', 'Cheng Luo', 'Pengyang Gao', 'Fan Xia', 'Hao Luo', 'Zheng Li', 'Yuehang Yang', 'Hongli Yu', 'Xingwei Qu', 'Yuwei Fu', 'Jing Su', 'Ge Zhang', 'Wenhao Huang', 'Mingxuan Wang', 'Lin Yan', 'Xiaoying Jia', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Yonghui Wu', 'Hao Zhou'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2508.02193.jpg', 'data': {'categories': ['#diffusion', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Революция в скорости генерации текста без потери качества', 'desc': 'Seed Diffusion Preview - это новая языковая модель, основанная на дискретной диффузии. Она обеспечивает очень быструю генерацию текста благодаря параллельному неупорядоченному декодированию. Модель достигает скорости вывода 2146 токенов в секунду на GPU H20, значительно превосходя аналоги Mercury и Gemini Diffusion. При этом Seed Diffusion Preview сохраняет конкурентоспособное качество на стандартных бенчмарках для оценки кода.'}, 'en': {'title': 'Speed Meets Quality: The Future of Code Generation', 'desc': 'Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion.'}, 'zh': {'title': '种子扩散预览：速度与质量的新标杆', 'desc': 'Seed Diffusion Preview是一种基于离散状态扩散的语言模型，具有极快的推理速度。通过非顺序的并行生成，离散扩散模型显著提高了推理效率，减少了逐个解码的延迟。该模型在H20 GPU上实现了每秒2,146个token的推理速度，同时在标准代码评估基准上保持了竞争力的性能。与当前的Mercury和Gemini Diffusion相比，Seed Diffusion Preview在速度和质量上都设立了新的标杆。'}}}, {'id': 'https://huggingface.co/papers/2508.03320', 'title': 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation', 'url': 'https://huggingface.co/papers/2508.03320', 'abstract': 'Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.', 'score': 42, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '71dc78f7c773cefd', 'authors': ['Peiyu Wang', 'Yi Peng', 'Yimeng Gan', 'Liang Hu', 'Tianyidan Xie', 'Xiaokun Wang', 'Yichen Wei', 'Chuanxin Tang', 'Bo Zhu', 'Changshi Li', 'Hongyang Wei', 'Eric Li', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Multimodality Team, Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03320.jpg', 'data': {'categories': ['#dataset', '#training', '#multimodal', '#architecture', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'Единая модель для понимания, создания и редактирования изображений', 'desc': 'Skywork UniPic - это авторегрессионная модель с 1,5 миллиардами параметров, объединяющая понимание изображений, генерацию изображений по тексту и редактирование изображений в единой архитектуре. Модель достигает высоких результатов на различных бенчмарках, включая GenEval, DPG-Bench и GEditBench-EN. Skywork UniPic использует раздельную стратегию кодирования и прогрессивное обучение с увеличением разрешения. Модель демонстрирует, что высококачественная мультимодальная интеграция возможна без чрезмерных вычислительных затрат.'}, 'en': {'title': 'Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic', 'desc': 'Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources.'}, 'zh': {'title': 'Skywork UniPic：统一多模态AI的高效解决方案', 'desc': 'Skywork UniPic是一个拥有15亿参数的自回归模型，能够统一图像理解、文本到图像生成和图像编辑。该模型通过一个单一架构消除了对特定任务适配器或模块连接器的需求，展示了紧凑的多模态系统在普通硬件上也能达到最先进的性能。Skywork UniPic在多个基准测试中表现优异，尤其是在图像生成和编辑方面，显示出其高效的训练策略和数据集设计。该模型为高保真多模态AI的实际应用提供了新的范式，且代码和权重已公开。'}}}, {'id': 'https://huggingface.co/papers/2508.03694', 'title': 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation', 'url': 'https://huggingface.co/papers/2508.03694', 'abstract': 'LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.', 'score': 32, 'issue_id': 5198, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '8c05bd06521b3fb7', 'authors': ['Jianxiong Gao', 'Zhaoxi Chen', 'Xian Liu', 'Jianfeng Feng', 'Chenyang Si', 'Yanwei Fu', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'NVIDIA', 'Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03694.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#multimodal', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'LongVie: прорыв в генерации сверхдлинных видео с сохранением качества', 'desc': 'LongVie - это новая автореградная модель для генерации сверхдлинных видео. Она решает проблемы временной согласованности и визуальной деградации с помощью унифицированной инициализации шума и глобальной нормализации управляющих сигналов. LongVie использует мультимодальное управление, объединяя плотные и разреженные сигналы. Модель также применяет стратегию обучения с учетом деградации для сохранения визуального качества.'}, 'en': {'title': 'LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality', 'desc': 'LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing.'}, 'zh': {'title': '超长视频生成的新突破：LongVie', 'desc': 'LongVie是一个端到端的自回归框架，旨在解决超长视频生成中的时间一致性和视觉退化问题。它通过统一的噪声初始化、全局控制信号归一化、多模态控制和退化感知训练来实现这些目标。LongVie的核心设计确保了时间一致性，并通过多模态控制框架来减轻视觉退化。实验结果表明，LongVie在长时间可控性、一致性和质量方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.03686', 'title': 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward', 'url': 'https://huggingface.co/papers/2508.03686', 'abstract': 'CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.', 'score': 22, 'issue_id': 5201, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'dddc5da46c921b94', 'authors': ['Shudong Liu', 'Hongwei Liu', 'Junnan Liu', 'Linchen Xiao', 'Songyang Gao', 'Chengqi Lyu', 'Yuzhe Gu', 'Wenwei Zhang', 'Derek F. Wong', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['NLP2CT Lab', 'Shanghai AI Laboratory', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2508.03686.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#benchmark', '#dataset', '#interpretability', '#optimization'], 'emoji': '🧭', 'ru': {'title': 'CompassVerifier: Надежная проверка ответов LLM во многих областях', 'desc': 'CompassVerifier - это легковесная модель для проверки выходных данных больших языковых моделей (LLM) в различных областях. Она поддерживается VerifierBench - комплексным набором данных для оценки. CompassVerifier демонстрирует компетентность в различных областях, включая математику, знания и разнообразные задачи на рассуждение. Модель способна обрабатывать различные типы ответов, включая многозадачные проблемы, формулы и последовательные ответы, эффективно идентифицируя аномальные и недействительные ответы.'}, 'en': {'title': 'Revolutionizing LLM Output Verification with CompassVerifier', 'desc': 'CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses.'}, 'zh': {'title': 'CompassVerifier：多领域答案验证的轻量级解决方案', 'desc': 'CompassVerifier 是一种轻量级且稳健的模型，用于验证大型语言模型（LLM）在不同领域的输出。它通过 VerifierBench 这一全面的基准数据集来支持验证过程。该模型能够处理多种类型的答案，包括多子问题、公式和序列答案，并有效识别异常或无效的响应。我们希望 CompassVerifier 和 VerifierBench 能够促进答案验证、评估协议和强化学习研究。'}}}, {'id': 'https://huggingface.co/papers/2508.03012', 'title': 'Tool-integrated Reinforcement Learning for Repo Deep Search', 'url': 'https://huggingface.co/papers/2508.03012', 'abstract': "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.", 'score': 9, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '4ab74a355fed1d76', 'authors': ['Zexiong Ma', 'Chao Peng', 'Qunhong Zeng', 'Pengfei Gao', 'Yanzhen Zou', 'Bing Xie'], 'affiliations': ['Beijing Institute of Technology', 'ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03012.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#agents', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ToolTrain: Эффективная локализация проблем в коде с помощью обученных языковых моделей', 'desc': 'ToolTrain - это двухэтапная система обучения, объединяющая контролируемую тонкую настройку и обучение с подкреплением для улучшения работы больших языковых моделей в задаче локализации проблем в программном коде. Система интегрирует инструменты поиска по репозиторию, что позволяет преодолеть семантический разрыв между описанием проблемы на естественном языке и проблемным кодом. Экспериментальные результаты показывают, что модели, обученные с помощью ToolTrain, достигают наилучших показателей в этой задаче. Улучшенная производительность в локализации проблем также приводит к лучшим результатам в полном цикле разрешения проблем в программном обеспечении.'}, 'en': {'title': 'ToolTrain: Enhancing LLMs for Superior Issue Localization', 'desc': "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."}, 'zh': {'title': 'ToolTrain：提升问题定位的智能工具训练框架', 'desc': 'ToolTrain是一种两阶段的训练框架，结合了监督微调和强化学习，旨在提升大型语言模型（LLMs）在问题定位方面的能力。问题定位是识别需要修改的代码位置以解决软件问题的过程，但由于自然语言描述与故障代码之间的语义差距，这一任务非常具有挑战性。ToolTrain通过整合代码库检索工具，帮助LLMs在多步骤推理和导航过程中有效利用这些工具，从而实现了最先进的性能。实验结果表明，ToolTrain训练的模型在功能级定位上超越了Claude-3.7，证明了针对问题定位的训练策略在自动化软件开发中是有效的。'}}}, {'id': 'https://huggingface.co/papers/2508.02091', 'title': 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search', 'url': 'https://huggingface.co/papers/2508.02091', 'abstract': "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", 'score': 7, 'issue_id': 5201, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '10eb53caada711ad', 'authors': ['Xiaoya Li', 'Xiaofei Sun', 'Albert Wang', 'Chris Shum', 'Jiwei Li'], 'affiliations': ['DeepReinforce Team', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.02091.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rag', '#rl', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'CRINN: Революция в поиске ближайших соседей с помощью ИИ', 'desc': 'CRINN - это новый подход к оптимизации алгоритмов поиска приближенных ближайших соседей (ANNS), использующий обучение с подкреплением. Он автоматически генерирует все более быстрые реализации ANNS, сохраняя при этом точность. CRINN превзошел современные методы на нескольких эталонных тестах, включая GIST-960-Euclidean и MNIST-784-Euclidean. Успех CRINN показывает, что языковые модели, дополненные обучением с подкреплением, могут эффективно автоматизировать сложные алгоритмические оптимизации.'}, 'en': {'title': 'CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning', 'desc': 'CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations.'}, 'zh': {'title': 'CRINN：用强化学习加速近似最近邻搜索', 'desc': 'CRINN是一种基于强化学习的方法，旨在优化近似最近邻搜索算法的速度，同时保持准确性。该方法将近似最近邻搜索的优化视为一个强化学习问题，以执行速度作为奖励信号。通过这种方式，CRINN能够自动生成逐渐更快的近似最近邻搜索实现，并满足准确性约束。实验结果表明，CRINN在多个基准数据集上表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.00367', 'title': 'Representation Shift: Unifying Token Compression with FlashAttention', 'url': 'https://huggingface.co/papers/2508.00367', 'abstract': "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.", 'score': 7, 'issue_id': 5209, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '5a4ad3025ab24bd1', 'authors': ['Joonmyung Choi', 'Sanghyeok Lee', 'Byungoh Ko', 'Eunseo Kim', 'Jihyung Kil', 'Hyunwoo J. Kim'], 'affiliations': ['Adobe Research', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00367.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#optimization', '#video'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обработки видео и текста без потери качества', 'desc': 'Статья представляет метрику Representation Shift, которая позволяет эффективно сжимать токены в трансформерах без переобучения модели. Эта метрика совместима с FlashAttention, что значительно ускоряет обработку видео и текста. Representation Shift измеряет степень изменения представления каждого токена, что позволяет определить наиболее важные из них. Метод применим не только к трансформерам, но и к CNN и моделям пространства состояний.'}, 'en': {'title': 'Boosting Video Retrieval Efficiency with Representation Shift', 'desc': "This paper introduces Representation Shift, a new metric that helps improve the efficiency of video-text retrieval and video question answering without needing to retrain models. It combines token compression techniques with FlashAttention, which reduces memory usage and speeds up processing. By measuring how much each token's representation changes, it allows for effective token selection without relying on attention maps. The method works not only with Transformers but also with other model types like CNNs, achieving notable performance improvements."}, 'zh': {'title': 'Representation Shift：加速视频检索与问答的创新方法', 'desc': 'Representation Shift是一种无训练、模型无关的度量方法，它将令牌压缩与FlashAttention结合，显著加快视频-文本检索和视频问答的速度。随着任务复杂性的增加，模型和令牌的规模也在扩大，导致自注意力的计算成本呈平方增长。我们的方法通过测量每个令牌表示的变化程度，来实现有效的令牌压缩，而无需构建注意力图或重新训练。实验结果表明，Representation Shift在视频-文本检索和视频问答中分别实现了高达5.5%和4.4%的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2508.03050', 'title': 'Multi-human Interactive Talking Dataset', 'url': 'https://huggingface.co/papers/2508.03050', 'abstract': 'MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.', 'score': 6, 'issue_id': 5200, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '01ba126a166568d6', 'authors': ['Zeyu Zhu', 'Weijia Wu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.03050.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset'], 'emoji': '🗣️', 'ru': {'title': 'Новый датасет и модель для генерации видео с разговорами нескольких людей', 'desc': 'Исследователи представили MIT - крупномасштабный набор данных для генерации видео с разговорами нескольких людей. Этот датасет включает детальные аннотации и используется для демонстрации CovOG - базовой модели, объединяющей кодировщик поз нескольких людей и интерактивный аудиодрайвер. MIT содержит 12 часов видео высокого разрешения с 2-4 говорящими и детальными аннотациями поз тела и речевых взаимодействий. CovOG использует агрегацию индивидуальных эмбеддингов поз и модуляцию динамики головы на основе аудиопризнаков каждого говорящего.'}, 'en': {'title': 'MIT: Pioneering Multi-Human Talking Video Generation', 'desc': 'The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area.'}, 'zh': {'title': 'MIT：多人人对话视频生成的新基准', 'desc': 'MIT是一个大规模的数据集，专门用于多人的对话视频生成，包含细致的注释信息。现有的研究主要集中在单人独白或孤立的面部动画上，限制了其在真实多人的互动中的应用。我们开发了一个自动化流程，收集和注释多人的对话视频，数据集包含12小时的高分辨率视频，展示了自然的对话动态。为了展示MIT的潜力，我们提出了CovOG模型，结合了多人的姿态编码器和互动音频驱动器，展示了生成真实多人的对话视频的可行性和挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.01119', 'title': 'The Promise of RL for Autoregressive Image Editing', 'url': 'https://huggingface.co/papers/2508.01119', 'abstract': 'Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.', 'score': 6, 'issue_id': 5215, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': 'b7e0974935b60296', 'authors': ['Saba Ahmadi', 'Rabiul Awal', 'Ankur Sikarwar', 'Amirhossein Kazemnejad', 'Ge Ya Luo', 'Juan A. Rodriguez', 'Sai Rajeswar', 'Siva Reddy', 'Christopher Pal', 'Benno Krojer', 'Aishwarya Agrawal'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute', 'Polytechnique Montréal', 'ServiceNow', 'Université de Montréal', 'École de Technologie Supérieure (ETS)'], 'pdf_title_img': 'assets/pdf/title_img/2508.01119.jpg', 'data': {'categories': ['#rl', '#optimization', '#multimodal', '#open_source', '#training', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Обучение с подкреплением повышает качество редактирования изображений', 'desc': 'В статье исследуются стратегии улучшения редактирования изображений с использованием авторегрессионной мультимодальной модели. Авторы сравнивают три подхода: обучение с учителем, обучение с подкреплением и рассуждения по цепочке мыслей. Наиболее эффективным оказалось обучение с подкреплением в сочетании с большой мультимодальной языковой моделью-верификатором. В результате была разработана модель EARL, показывающая конкурентоспособные результаты на различных задачах редактирования изображений.'}, 'en': {'title': 'Reinforcement Learning Meets Multimodal Image Editing', 'desc': 'This paper presents a novel approach to image editing by integrating reinforcement learning (RL) with a large multimodal language model (LLM) verifier within an autoregressive framework. The authors investigate three strategies: supervised fine-tuning, RL, and Chain-of-Thought reasoning, to improve image editing tasks. They demonstrate that the combination of RL and a multimodal LLM verifier significantly enhances performance, leading to the development of EARL, a robust RL-based image editing model. EARL achieves competitive results on various editing tasks while requiring less training data compared to existing methods, marking a significant advancement in autoregressive multimodal models for image editing.'}, 'zh': {'title': '强化学习与多模态模型结合，提升图像编辑性能', 'desc': '本论文探讨了如何通过结合强化学习和大型多模态语言模型验证器来提升图像编辑性能。我们提出了三种策略：监督微调、强化学习和链式思维推理，并在一个自回归多模态框架中进行研究。实验结果表明，强化学习与大型多模态语言模型验证器的结合是最有效的策略。最终，我们发布了EARL模型，它在多种图像编辑任务中表现出色，且训练数据需求较少。'}}}, {'id': 'https://huggingface.co/papers/2508.03613', 'title': 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction', 'url': 'https://huggingface.co/papers/2508.03613', 'abstract': "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.", 'score': 5, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '50044cd9b7eb1802', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Ziran Yang', 'Jui-Hui Chung', 'Haoyu Zhao', 'Lai Jiang', 'Yihan Geng', 'Jiawei Ge', 'Jingruo Sun', 'Jiayun Wu', 'Jiri Gesi', 'Ximing Lu', 'David Acuna', 'Kaiyu Yang', 'Hongzhou Lin', 'Yejin Choi', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'NVIDIA', 'Peking University', 'Princeton Language and Intelligence, Princeton University', 'Shanghai Jiao Tong University', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03613.jpg', 'data': {'categories': ['#rl', '#dataset', '#synthetic', '#reasoning', '#small_models', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Маленькая модель - большие доказательства: Goedel-Prover-V2 переворачивает мир автоматического доказательства теорем', 'desc': 'Goedel-Prover-V2 - это серия открытых языковых моделей для автоматического доказательства теорем. Модель использует синтез структурированных данных, самокоррекцию на основе верификатора и усреднение моделей для достижения наилучших результатов. Несмотря на небольшой размер, Goedel-Prover-V2-8B превосходит гораздо более крупные модели на бенчмарках MiniF2F и PutnamBench. На момент выпуска Goedel-Prover-V2 демонстрирует лучшую производительность среди открытых систем автоматического доказательства теорем.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Goedel-Prover-V2', 'desc': 'Goedel-Prover-V2 is a series of open-source language models that excel in automated theorem proving. It introduces innovative techniques such as scaffolded data synthesis for training on progressively complex tasks, verifier-guided self-correction for iterative proof refinement, and model averaging to enhance output diversity. The models achieve impressive performance metrics, with the smaller Goedel-Prover-V2-8B outperforming larger models like DeepSeek-Prover-V2-671B. Overall, Goedel-Prover-V2 sets a new benchmark in the field, demonstrating superior capabilities in solving mathematical problems with a reduced computational footprint.'}, 'zh': {'title': 'Goedel-Prover-V2：自动定理证明的新标杆', 'desc': 'Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域达到了最先进的性能。该模型通过三项创新技术实现了这一目标：首先，使用支架数据合成生成逐渐增加难度的合成任务，以帮助模型掌握复杂的定理；其次，采用验证器引导的自我修正，使模型能够根据Lean编译器的反馈迭代修正其证明；最后，通过模型平均技术合并模型检查点，以减少训练后期模型输出多样性的下降。Goedel-Prover-V2-32B模型在MiniF2F上达到了88.1%的通过率，显著超越了之前的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.01780', 'title': 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?', 'url': 'https://huggingface.co/papers/2508.01780', 'abstract': "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.", 'score': 5, 'issue_id': 5199, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'b9c09b0ce4e2dad3', 'authors': ['Guozhao Mo', 'Wenliang Zhong', 'Jiawei Chen', 'Xuanang Chen', 'Yaojie Lu', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.01780.jpg', 'data': {'categories': ['#open_source', '#survey', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LiveMCPBench: Новый стандарт оценки LLM-агентов в реальных MCP-средах', 'desc': 'LiveMCPBench представляет собой комплексный бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в реальных задачах экосистемы MCP. Он включает 95 задач и использует масштабируемый конвейер оценки с адаптивной системой судейства. Бенчмарк содержит LiveMCPTool - набор из 70 MCP-серверов и 527 инструментов, а также LiveMCPEval - фреймворк для автоматизированной оценки с использованием LLM в качестве судьи. Результаты тестирования 10 ведущих моделей показали, что лучшая модель (Claude-Sonnet-4) достигла 78.95% успешности выполнения задач.'}, 'en': {'title': 'Revolutionizing LLM Evaluation in Dynamic MCP Environments', 'desc': 'LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments.'}, 'zh': {'title': '全面评估LLM代理的基准测试平台', 'desc': 'LiveMCPBench是一个全面的基准测试平台，旨在评估大型语言模型（LLM）代理在多样化的真实世界任务中的表现。它解决了现有基准测试仅限于单一服务器设置的问题，提供了95个基于模型上下文协议（MCP）生态系统的真实任务。通过LiveMCPTool，研究人员可以使用70个MCP服务器和527个工具，支持可扩展和可重复的评估流程。此外，LiveMCPEval框架实现了自动化和自适应评估，确保在动态任务环境中与人类评审者的高一致性。'}}}, {'id': 'https://huggingface.co/papers/2508.00477', 'title': 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer', 'url': 'https://huggingface.co/papers/2508.00477', 'abstract': "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.  \t\t\t\t\tAI-generated summary \t\t\t\t In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.", 'score': 4, 'issue_id': 5202, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '94d96ba2b9f92b31', 'authors': ['Yuzhuo Chen', 'Zehua Ma', 'Jianhua Wang', 'Kai Kang', 'Shunyu Yao', 'Weiming Zhang'], 'affiliations': ['East China Normal University', 'Onestory Team', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.00477.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#optimization', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'LAMIC: Революция в синтезе изображений с несколькими референсами', 'desc': 'LAMIC - это фреймворк для композиции нескольких изображений с учетом макета, который расширяет возможности диффузионных моделей с одним референсом на сценарии с несколькими референсами. Он использует два механизма внимания: Group Isolation Attention для улучшения разделения сущностей и Region-Modulated Attention для генерации с учетом макета. LAMIC достигает наилучших результатов по большинству метрик без дополнительного обучения, демонстрируя превосходные способности в сохранении идентичности, фона и контроле макета. Этот подход устанавливает новую парадигму для контролируемой композиции нескольких изображений без обучения.'}, 'en': {'title': 'LAMIC: Revolutionizing Multi-Image Synthesis Without Training', 'desc': 'LAMIC is a new framework designed for creating images from multiple references while considering their layout. It builds on existing single-reference diffusion models and introduces attention mechanisms to improve how different elements in the images are handled. This framework allows for high-quality image synthesis without the need for additional training, demonstrating strong performance in maintaining identity, background consistency, and layout control. LAMIC sets a new standard in controllable image composition by effectively managing multiple images in a training-free manner.'}, 'zh': {'title': 'LAMIC：无训练的多图像合成新范式', 'desc': 'LAMIC是一个布局感知的多图像合成框架，它将单参考扩散模型扩展到多参考场景，且无需训练。该框架引入了两种注意力机制：群体隔离注意力（GIA）和区域调制注意力（RMA），以增强实体分离和布局感知生成。通过引入三种评估指标，LAMIC在身份保持、背景一致性和布局控制等方面表现出色，超越了现有的多参考基线。LAMIC展示了强大的零样本泛化能力，为可控的多图像合成建立了新的无训练范式。'}}}, {'id': 'https://huggingface.co/papers/2508.03164', 'title': 'ChartCap: Mitigating Hallucination of Dense Chart Captioning', 'url': 'https://huggingface.co/papers/2508.03164', 'abstract': 'ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.', 'score': 3, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'aec1f860dbfa8231', 'authors': ['Junyoung Lim', 'Jaewoo Ahn', 'Gunhee Kim'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03164.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#dataset', '#data', '#open_source'], 'emoji': '📊', 'ru': {'title': 'ChartCap: Точные подписи к графикам без галлюцинаций', 'desc': 'ChartCap - это масштабный набор данных, содержащий 565 тысяч реальных графиков с детальными подписями. Датасет разработан для улучшения точности генерации подписей и уменьшения галлюцинаций в мультимодальных языковых моделях. ChartCap использует четырехэтапный конвейер для создания подписей, основанных только на видимых данных графика, и применяет верификацию на основе циклической согласованности. Авторы также предлагают новую метрику - Visual Consistency Score, для оценки качества подписей без опоры на эталонные подписи.'}, 'en': {'title': 'ChartCap: Elevating Chart Captioning Accuracy and Reducing Hallucinations', 'desc': 'ChartCap is a new dataset designed to enhance the performance of vision language models in generating captions for charts. It contains 565,000 real-world chart images with detailed, type-specific captions that focus on essential structural elements and insights, avoiding irrelevant information. The dataset is created through a four-stage process that ensures high-quality captions, verified by a cycle consistency method for efficient quality control. Experiments show that models trained on ChartCap produce more accurate and informative captions, with fewer hallucinations compared to existing models and even human-generated captions.'}, 'zh': {'title': 'ChartCap：提升图表说明准确性的关键数据集', 'desc': 'ChartCap是一个大规模的数据集，包含565K个真实世界图表图像及其特定类型的详细说明。该数据集旨在提高视觉语言模型的说明准确性，并减少虚假信息的生成。通过设计四阶段的生成管道，ChartCap确保说明仅基于图表中可辨别的数据，并通过循环一致性的人类验证加速质量控制。实验结果表明，基于ChartCap微调的模型在生成准确和信息丰富的说明方面表现优于其他开源和专有模型，甚至超过人类标注的说明。'}}}, {'id': 'https://huggingface.co/papers/2508.02629', 'title': 'HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents', 'url': 'https://huggingface.co/papers/2508.02629', 'abstract': 'HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.', 'score': 3, 'issue_id': 5212, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '5b4c4a212eae58d6', 'authors': ['Yibin Liu', 'Zhixuan Liang', 'Zanxin Chen', 'Tianxing Chen', 'Mengkang Hu', 'Wanxi Dong', 'Congsheng Xu', 'Zhaoming Han', 'Yusen Qin', 'Yao Mu'], 'affiliations': ['D-Robotics', 'HKU MMLab', 'NEU', 'SJTU ScaleLab', 'SUSTech', 'SZU', 'Shanghai AI Lab', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2508.02629.jpg', 'data': {'categories': ['#robotics', '#optimization', '#agents', '#reasoning', '#games', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самокорректирующиеся программы для роботов на основе мультимодального ИИ', 'desc': 'HyCodePolicy - это гибридная система управления для воплощенных агентов, использующая мультимодальные языковые модели. Она объединяет синтез кода, геометрическое обоснование, перцептивный мониторинг и итеративное исправление в замкнутый цикл программирования. Система генерирует исполняемую программу на основе естественно-языковых инструкций, затем выполняет ее в симуляции, отслеживая ошибки с помощью визуально-языковой модели. HyCodePolicy способна автономно исправлять программы, значительно повышая надежность и эффективность политик для манипуляций роботов.'}, 'en': {'title': 'Empowering Robots with Self-Correcting Code Synthesis', 'desc': 'HyCodePolicy is a novel framework that enhances the performance of embodied agents by integrating several advanced techniques. It combines code synthesis, geometric grounding, perceptual monitoring, and iterative repair to create a robust programming cycle. The system translates natural language instructions into executable programs, monitors their execution, and identifies failures using a vision-language model. This approach allows for self-correcting capabilities in program synthesis, improving both the efficiency and reliability of robotic manipulation tasks.'}, 'zh': {'title': '自我修正的智能体编程策略', 'desc': 'HyCodePolicy 是一种混合语言控制框架，旨在增强具身智能体策略的鲁棒性和效率。该系统通过将代码合成、几何基础、感知监控和迭代修复整合到一个闭环编程周期中，来实现自我修正的程序合成。它首先将自然语言指令分解为子目标，并生成基于对象中心几何原语的可执行程序。通过视觉语言模型监控执行过程，HyCodePolicy 能够检测执行失败并进行修复，从而提高机器人操作策略的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.02079', 'title': 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization', 'url': 'https://huggingface.co/papers/2508.02079', 'abstract': 'AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.', 'score': 2, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '25a555fc0f91f562', 'authors': ['Amitava Das', 'Abhilekh Borah', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon AI, USA', 'BITS Goa, India', 'Manipal University, India', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.02079.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Сохранение безопасности при дообучении языковых моделей', 'desc': 'AlignGuard-LoRA (AGL) - это фреймворк для сохранения выравнивания при дообучении больших языковых моделей. Он вводит методы регуляризации и диагностический бенчмарк для снижения дрейфа выравнивания. AGL включает несколько ключевых компонентов, таких как регуляризация на основе матрицы Фишера и регуляризация с учетом коллизий. Эмпирические оценки показывают, что AGL снижает дрейф выравнивания до 50% на критически важных для безопасности бенчмарках без ухудшения производительности на целевых задачах.'}, 'en': {'title': 'Preserving Alignment in Fine-Tuning with AGL', 'desc': 'AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks.'}, 'zh': {'title': 'AlignGuard-LoRA：保持对齐，确保安全', 'desc': 'AlignGuard-LoRA (AGL) 是一个框架，旨在通过引入正则化技术和诊断基准来保持大型语言模型在微调过程中的对齐性。该框架解决了低秩适应（LoRA）在更新过程中可能导致的对齐漂移问题，从而增强安全性和行为约束。AGL 采用了多种关键组件，包括基于费舍尔信息矩阵的正则化和任务特定的正则化，以稳定新知识的整合。实验证明，AGL 能够在不降低下游任务性能的情况下，将对齐漂移减少多达 50%。'}}}, {'id': 'https://huggingface.co/papers/2508.02630', 'title': 'What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce', 'url': 'https://huggingface.co/papers/2508.02630', 'abstract': 'ACES, a sandbox environment, studies AI agents\' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.  \t\t\t\t\tAI-generated summary \t\t\t\t Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal "top" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '2860dd8bc6a41f92', 'authors': ['Amine Allouah', 'Omar Besbes', 'Josué D Figueroa', 'Yash Kanoria', 'Akshit Kumar'], 'affiliations': ['Columbia University, Graduate School of Business', 'MyCustomAI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02630.jpg', 'data': {'categories': ['#agents', '#multimodal', '#ethics', '#games', '#alignment'], 'emoji': '🛒', 'ru': {'title': 'ИИ идет за покупками: новая эра электронной коммерции', 'desc': 'Исследование ACES изучает поведение ИИ-агентов при совершении покупок в виртуальной торговой среде. Эксперименты выявили влияние позиции товаров, спонсорских тегов, рекомендаций, цен, рейтингов и отзывов на выбор ИИ-покупателей. Результаты показывают, что разные модели машинного обучения демонстрируют различную чувствительность к этим факторам. Исследование поднимает вопросы о стратегиях продавцов, дизайне платформ и регулировании в экосистеме, где покупки осуществляются с помощью ИИ.'}, 'en': {'title': 'Understanding AI Agents in E-Commerce: Insights from ACES', 'desc': "The paper introduces ACES, a sandbox environment designed to analyze the shopping behavior of AI agents in a simulated marketplace. It investigates how these agents respond to various factors such as product position, pricing, ratings, and endorsements, revealing that their preferences can differ significantly from human shoppers. The study finds that while AI agents generally prefer higher-ranked products, their specific choices can vary based on the model used, challenging the idea of a single 'top' product. The findings suggest that sellers can optimize their listings to cater to AI preferences, potentially reshaping strategies in e-commerce as AI-driven shopping becomes more prevalent."}, 'zh': {'title': '探索AI代理在电商中的购物行为', 'desc': '本论文研究了人工智能代理在模拟市场中的购物行为，提出了一个名为ACES的沙盒环境。通过随机化产品位置、价格、评分、评论和赞助标签，研究了不同模型在购物时的因果关系和偏好。结果显示，AI代理对产品位置有明显的偏好，但不同模型的选择差异很大，且对价格和评分的敏感度与人类相似但幅度不同。研究还表明，卖方可以通过优化产品描述来吸引AI买家的偏好，从而在市场中获得显著的份额提升。'}}}, {'id': 'https://huggingface.co/papers/2508.02455', 'title': 'TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs', 'url': 'https://huggingface.co/papers/2508.02455', 'abstract': 'A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '70799804b6937703', 'authors': ['Daniele Cipollone', 'Egor Bogomolov', 'Arie van Deursen', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology, Delft, Netherlands', 'JetBrains, Amsterdam, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2508.02455.jpg', 'data': {'categories': ['#plp', '#training', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Умное ранжирование автодополнений кода с помощью языковых моделей', 'desc': 'Статья предлагает новый подход к ранжированию статических автодополнений кода в IDE с использованием языковых моделей. Метод организует варианты дополнений в префиксное дерево и выполняет однократный жадный проход декодирования для сбора оценок на уровне токенов. Это позволяет точно ранжировать варианты с учетом контекста без необходимости в лучевом поиске или адаптации модели. Подход быстрый, не зависит от архитектуры и совместим с уже развернутыми моделями автодополнения кода.'}, 'en': {'title': 'Smart Code Completion with Language Models', 'desc': 'This paper introduces a novel method for ranking code completions in Integrated Development Environments (IDEs) using language models. The approach organizes potential code completions into a prefix tree, allowing for efficient scoring of completions through a single greedy decoding pass. This method enhances the ranking of static code completions by providing a token-aware scoring system without the need for complex techniques like beam search or prompt engineering. The proposed solution is lightweight, model-agnostic, and can be easily integrated into existing IDEs, improving developer assistance significantly.'}, 'zh': {'title': '智能代码补全的新方法', 'desc': '本文提出了一种新的评分方法，利用语言模型对静态代码补全进行排名。该方法将所有有效的补全组织成前缀树，并通过单次贪婪解码来收集每个标记的分数。与传统的基于手工启发式或轻量级机器学习模型的方法相比，这种方法能够更好地捕捉上下文信息，并在不同项目和编码风格中进行泛化。最终，这种快速且与模型无关的方法为集成语言模型到现有IDE工具中提供了有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.02063', 'title': 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs', 'url': 'https://huggingface.co/papers/2508.02063', 'abstract': "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7", 'score': 1, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '0b136776fbb19b26', 'authors': ['Amitava Das', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'BITS Pilani Goa', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02063.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#alignment', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'TraceAlign: отслеживание и устранение дрейфа выравнивания в больших языковых моделях', 'desc': 'TraceAlign - это фреймворк для выявления и снижения дрейфа выравнивания в больших языковых моделях. Он отслеживает небезопасные завершения до их источников в обучающих данных и применяет интервенции для уменьшения дрейфа при сохранении полезности модели. Ключевым элементом является Индекс Конфликта Убеждений (BCI), количественно оценивающий семантическое несоответствие между сгенерированными фрагментами и заданными политиками. Фреймворк предлагает три дополняющих друг друга метода защиты: фильтр безопасности TraceShield, контрастивную функцию потерь и стратегию декодирования Prov-Decode.'}, 'en': {'title': 'TraceAlign: Bridging the Gap in LLM Alignment', 'desc': "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."}, 'zh': {'title': 'TraceAlign：减轻大型语言模型对齐漂移的创新框架', 'desc': 'TraceAlign是一个框架，用于识别和减轻大型语言模型（LLMs）中的对齐漂移。它通过追踪不安全的生成结果到其训练来源，并应用干预措施来减少漂移，同时保持模型的实用性。该框架引入了信念冲突指数（BCI），量化生成内容与对齐政策之间的语义不一致性。通过三种互补的干预措施，TraceAlign能够在保持任务性能的同时，显著降低对齐漂移的发生率。'}}}, {'id': 'https://huggingface.co/papers/2508.01126', 'title': 'UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation', 'url': 'https://huggingface.co/papers/2508.01126', 'abstract': "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.", 'score': 0, 'issue_id': 5212, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '8f30093d889bde3a', 'authors': ['Chaitanya Patel', 'Hiroki Nakamura', 'Yuta Kyuragi', 'Kazuki Kozuka', 'Juan Carlos Niebles', 'Ehsan Adeli'], 'affiliations': ['Panasonic Holdings Corporation', 'Panasonic R&D Company of America', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01126.jpg', 'data': {'categories': ['#dataset', '#games', '#benchmark', '#video', '#diffusion', '#multimodal', '#cv', '#healthcare'], 'emoji': '🕶️', 'ru': {'title': 'Революция в моделировании движения от первого лица', 'desc': 'Представлена унифицированная условная модель диффузии движения UniEgoMotion для генерации и прогнозирования эгоцентрического движения с использованием изображений от первого лица. Модель достигает наилучших результатов в реконструкции эгоцентрического движения и впервые позволяет генерировать движение по одному эгоцентрическому изображению. UniEgoMotion использует новое представление движения с учетом положения головы, специально разработанное для эгоцентрических устройств. Для обучения модели создан большой набор данных EE4D-Motion на основе EgoExo4D с псевдо-разметкой трехмерного движения.'}, 'en': {'title': 'Revolutionizing Egocentric Motion with UniEgoMotion', 'desc': 'The paper presents UniEgoMotion, a unified conditional motion diffusion model designed for generating and forecasting egocentric motion from first-person images. This model addresses the limitations of existing methods that focus on third-person perspectives and structured 3D scenes, which are not effective in real-world scenarios with dynamic cameras and occlusions. UniEgoMotion utilizes a novel head-centric motion representation and effectively extracts scene context from images to predict plausible 3D motion. The introduction of the EE4D-Motion dataset, which includes pseudo-ground-truth 3D motion annotations, supports the training of this model, achieving state-of-the-art results in egocentric motion tasks.'}, 'zh': {'title': '自我中心运动生成的新突破', 'desc': '本文介绍了一种统一的条件运动扩散模型UniEgoMotion，用于从第一人称图像生成和预测自我中心的运动。该模型在自我中心运动重建和预测方面达到了最先进的性能，能够仅从单张图像生成运动。UniEgoMotion采用了一种新颖的头部中心运动表示，支持在没有明确3D场景的情况下进行场景感知的运动合成。通过引入EE4D-Motion数据集，本文为训练提供了丰富的伪真实3D运动注释，推动了自我中心运动建模的新标准。'}}}, {'id': 'https://huggingface.co/papers/2508.04026', 'title': 'VeriGUI: Verifiable Long-Chain GUI Dataset', 'url': 'https://huggingface.co/papers/2508.04026', 'abstract': 'VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.', 'score': 116, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '2692487ee60e017e', 'authors': ['Shunyu Liu', 'Minghao Liu', 'Huichi Zhou', 'Zhenyu Cui', 'Yang Zhou', 'Yuhao Zhou', 'Wendong Fan', 'Ge Zhang', 'Jiajun Shi', 'Weihao Xuan', 'Jiaxing Huang', 'Shuang Luo', 'Fang Wu', 'Heli Qi', 'Qingcheng Zeng', 'Ziqi Ren', 'Jialiang Gao', 'Jindi Lv', 'Junjie Wang', 'Aosong Feng', 'Heng Zhou', 'Wangchunshu Zhou', 'Zhenfei Yin', 'Wenlong Zhang', 'Guohao Li', 'Wenhao Yu', 'Irene Li', 'Lei Ma', 'Lei Bai', 'Qunshu Lin', 'Mingli Song', 'Dacheng Tao'], 'affiliations': ['VeriGUI Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.04026.jpg', 'data': {'categories': ['#games', '#agents', '#dataset', '#long_context'], 'emoji': '🖥️', 'ru': {'title': 'VeriGUI: Новый стандарт для оценки долгосрочных GUI-агентов', 'desc': 'VeriGUI - это новый набор данных для оценки GUI-агентов в долгосрочных задачах, акцентирующий внимание на сложности длинных цепочек действий и верифицируемости на уровне подзадач. Датасет состоит из траекторий GUI-задач для десктопных и веб-приложений, аннотированных экспертами. Он позволяет разрабатывать и оценивать универсальные GUI-агенты, работающие в реалистичных компьютерных средах. Эксперименты на VeriGUI с использованием различных агентов и языковых моделей выявили значительные пробелы в производительности при работе с долгосрочными задачами.'}, 'en': {'title': 'Empowering GUI Agents for Complex Tasks with VeriGUI', 'desc': 'VeriGUI is a new dataset created to test GUI agents on complex tasks that require many steps. It focuses on breaking down tasks into smaller, manageable subtasks that can be verified individually. This approach allows agents to start from any subtask, making it easier to explore different strategies. The dataset includes detailed task examples from desktop and web environments, showing that current agents struggle with long-term planning and decision-making.'}, 'zh': {'title': 'VeriGUI：长时间任务中的智能体评估新标准', 'desc': 'VeriGUI是一个新颖的数据集，用于评估在长时间任务中执行图形用户界面（GUI）操作的智能体。该数据集强调了长链复杂性和子任务级可验证性，允许任务被分解为数百个相互依赖的子任务。通过这种方式，任何子任务都可以作为有效的起点，促进了多样化的探索策略。实验结果显示，现有智能体在处理长时间任务时存在显著性能差距，表明需要更强大的规划和决策能力。'}}}, {'id': 'https://huggingface.co/papers/2508.01191', 'title': 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens', 'url': 'https://huggingface.co/papers/2508.01191', 'abstract': 'CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.', 'score': 107, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '427ac75c7123b50a', 'authors': ['Chengshuai Zhao', 'Zhen Tan', 'Pingchuan Ma', 'Dawei Li', 'Bohan Jiang', 'Yancheng Wang', 'Yingzhen Yang', 'Huan Liu'], 'affiliations': ['Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01191.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Ограниченность CoT-рассуждений в LLM: мираж вне распределения обучающих данных', 'desc': 'Исследование показывает, что рассуждения по цепочке (CoT) в больших языковых моделях (LLM) ограничены расхождением между распределениями обучающих и тестовых данных. Авторы разработали среду DataAlchemy для изучения CoT-рассуждений по трем измерениям: задача, длина и формат. Результаты демонстрируют, что CoT-рассуждения неустойчивы и исчезают при выходе за пределы распределения обучающих данных. Это исследование подчеркивает сложность достижения подлинных и обобщаемых рассуждений в LLM.'}, 'en': {'title': 'Unmasking the Fragility of CoT Reasoning in LLMs', 'desc': 'This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs.'}, 'zh': {'title': '链式思维推理的局限性与挑战', 'desc': '本文探讨了链式思维（CoT）推理在大型语言模型（LLM）中的局限性，特别是训练数据与测试数据之间的分布差异对其影响。研究表明，CoT推理并不是一种稳健的推理形式，因为它的有效性受到训练数据和测试查询之间分布差异的限制。通过设计一个名为DataAlchemy的控制环境，作者系统性地分析了CoT推理在不同任务、长度和格式下的表现。结果显示，当CoT推理超出训练分布时，其效果会显著下降，揭示了实现真正可推广推理的持续挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.02694', 'title': 'Efficient Agents: Building Effective Agents While Reducing Cost', 'url': 'https://huggingface.co/papers/2508.02694', 'abstract': 'A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from 0.398 to 0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.', 'score': 48, 'issue_id': 5222, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '0c518e4f5949dae3', 'authors': ['Ningning Wang', 'Xavier Hu', 'Pai Liu', 'He Zhu', 'Yue Hou', 'Heyuan Huang', 'Shengyu Zhang', 'Jian Yang', 'Jiaheng Liu', 'Ge Zhang', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'OPPO-PersonalAI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02694.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#open_source', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Оптимизация агентских систем: баланс эффективности и затрат', 'desc': 'Исследование посвящено анализу компромисса между эффективностью и результативностью в системах агентов, управляемых большими языковыми моделями (LLM). Авторы изучают оптимальную структуру агентских фреймворков для снижения затрат при сохранении производительности. Проводится эмпирический анализ на бенчмарке GAIA, оценивающий влияние выбора LLM, дизайна фреймворков и стратегий масштабирования. На основе результатов разработан новый фреймворк Efficient Agents, сохраняющий 96.7% производительности ведущего open-source решения при снижении операционных затрат на 28.4%.'}, 'en': {'title': 'Balancing Cost and Performance in AI Agents', 'desc': 'This paper explores how to balance efficiency and effectiveness in systems powered by Large Language Models (LLMs). It identifies the optimal design for agent frameworks that can lower costs while still performing well. The study answers key questions about task complexity, the diminishing returns of adding modules, and how to enhance efficiency through better design. The results show that the proposed Efficient Agents framework maintains high performance while significantly reducing operational costs, making AI solutions more accessible and sustainable.'}, 'zh': {'title': '高效代理系统：降低成本与保持性能的平衡', 'desc': '本研究探讨了大型语言模型（LLM）驱动的代理系统中的效率与效果之间的权衡，旨在设计出既能降低成本又能保持性能的最佳代理框架。我们分析了代理任务的复杂性、额外模块的边际效益以及通过高效代理框架设计所能获得的效率提升。通过对GAIA基准的实证分析，我们评估了LLM骨干选择、代理框架设计和测试时扩展策略的影响。研究结果表明，开发高效代理系统可以在保持高性能的同时显著降低运营成本，推动AI解决方案的可及性和可持续性。'}}}, {'id': 'https://huggingface.co/papers/2508.04700', 'title': 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience', 'url': 'https://huggingface.co/papers/2508.04700', 'abstract': "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.", 'score': 37, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6c7450255f7c28bc', 'authors': ['Zeyi Sun', 'Ziyu Liu', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Tong Wu', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.04700.jpg', 'data': {'categories': ['#agents', '#agi', '#open_source', '#optimization', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучающиеся агенты для освоения нового ПО', 'desc': 'SEAgent - это фреймворк для автономного обучения агентов использованию компьютерного программного обеспечения. Он позволяет агентам исследовать новое ПО через экспериментальное обучение и выполнение автоматически генерируемых заданий возрастающей сложности. SEAgent использует модель состояния мира для оценки траектории действий и генератор учебного плана для создания разнообразных задач. Обновление политики агента происходит через имитационное обучение на неудачных действиях и оптимизацию на успешных, что позволяет достичь значительного улучшения производительности по сравнению с существующими методами.'}, 'en': {'title': 'Empowering Agents to Learn and Evolve Autonomously', 'desc': 'SEAgent is a self-evolving framework designed for computer-use agents (CUAs) to autonomously learn and master new software through experiential learning. It allows agents to explore unfamiliar software environments and learn from their experiences by tackling tasks that increase in complexity. The framework includes a World State Model for assessing agent performance and a Curriculum Generator for creating diverse tasks. By integrating insights from specialist agents, SEAgent develops a robust generalist CUA that outperforms traditional methods, achieving a notable increase in success rates across various software environments.'}, 'zh': {'title': 'SEAgent：自主进化的智能体框架', 'desc': 'SEAgent是一种自我进化的智能体框架，能够使计算机使用代理通过体验学习自主掌握新软件。该框架通过逐步的任务课程，帮助代理在没有人类标注的情况下，探索和学习陌生的软件环境。SEAgent设计了世界状态模型和课程生成器，以便代理能够通过试错学习不断提高其能力。最终，SEAgent的表现超越了多个专门化代理的组合，显示出其在新软件环境中的优越性。'}}}, {'id': 'https://huggingface.co/papers/2508.03501', 'title': 'Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03501', 'abstract': "Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation.   To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agent's success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.", 'score': 29, 'issue_id': 5228, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '9e2fbad63802fc98', 'authors': ['Alexander Golubev', 'Maria Trofimova', 'Sergei Polezhaev', 'Ibragim Badertdinov', 'Maksim Nekrashevich', 'Anton Shevtsov', 'Simon Karasik', 'Sergey Abramov', 'Andrei Andriushchenko', 'Filipp Fisin', 'Sergei Skvortsov', 'Boris Yangel'], 'affiliations': ['Humanoid', 'Nebius AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03501.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'RL для LLM: прорыв в многоходовом взаимодействии для разработки ПО', 'desc': 'Исследование демонстрирует успешное применение обучения с подкреплением (RL) к большим языковым моделям (LLM) для решения многоходовых задач в области разработки программного обеспечения. Используя модифицированный алгоритм DAPO, авторы обучили агента на основе Qwen2.5-72B-Instruct для решения реальных задач в этой сфере. Результаты показывают значительное улучшение успешности агента на бенчмарке SWE-bench Verified с 20% до 39%. На бенчмарке SWE-rebench агент показал результаты на уровне или лучше ведущих открытых моделей, открывая путь к созданию более способных автономных агентов для сложных реальных задач.'}, 'en': {'title': 'Empowering Language Models with Reinforcement Learning for Real-World Software Engineering', 'desc': 'This paper explores the application of Reinforcement Learning (RL) to Large Language Models (LLMs) in multi-turn interactions, particularly in software engineering tasks. Unlike previous studies that focused on single-turn problems, this research addresses the need for agents to operate in environments that provide feedback after each action. The authors introduce a modified Decoupled Advantage Policy Optimization (DAPO) algorithm to train an agent, Qwen2.5-72B-Instruct, achieving a significant improvement in success rates on software engineering benchmarks. This work demonstrates the potential of RL in enhancing the capabilities of autonomous agents for complex, real-world applications without relying on teacher models.'}, 'zh': {'title': '强化学习助力大型语言模型解决复杂任务', 'desc': '本研究探讨了强化学习（RL）在大型语言模型（LLM）中的应用，特别是在需要多轮交互的真实世界任务中。我们提出了一种改进的解耦优势策略优化（DAPO）算法，成功训练了一个基于Qwen2.5-72B-Instruct的智能体，以解决软件工程（SWE）任务。实验结果显示，该智能体在SWE-bench Verified基准测试中的成功率从20%提升至39%。我们的研究为构建更强大的自主智能体提供了可行的路径，能够应对复杂的现实问题。'}}}, {'id': 'https://huggingface.co/papers/2508.04280', 'title': 'Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success', 'url': 'https://huggingface.co/papers/2508.04280', 'abstract': 'A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.', 'score': 26, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '007849b63760d8ee', 'authors': ['George Bredis', 'Stanislav Dereka', 'Viacheslav Sinii', 'Ruslan Rakhimov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.04280.jpg', 'data': {'categories': ['#rl', '#training', '#transfer_learning', '#synthetic', '#games', '#rlhf', '#multimodal', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Простое RL-обучение для универсальных визуально-языковых агентов', 'desc': 'VL-DAC - это новый алгоритм обучения с подкреплением для мультимодальных агентов на основе моделей компьютерного зрения и языка. Он позволяет обучать модели в простых симуляторах, при этом улучшая их производительность на реальных задачах без потери точности понимания изображений. VL-DAC применяет обновления PPO к токенам действий, но обучает функцию ценности только на уровне шагов среды. Такой подход обеспечивает более быструю и надежную сходимость по сравнению с предыдущими методами.'}, 'en': {'title': 'VL-DAC: Simplifying RL for Enhanced Vision-Language Learning', 'desc': 'The paper presents VL-DAC, a novel reinforcement learning (RL) algorithm designed for vision-language models (VLMs) that operates without the need for hyperparameter tuning. This algorithm allows VLMs to learn effective policies from low-cost simulators, enhancing their performance on real-world tasks while maintaining image understanding accuracy. By decoupling the learning of action tokens and value estimation, VL-DAC achieves faster and more stable convergence compared to previous methods. The results demonstrate that training with VL-DAC leads to significant improvements in various benchmarks, showcasing its potential for developing multimodal agents capable of complex tasks.'}, 'zh': {'title': '轻量级强化学习算法，提升视觉语言模型性能', 'desc': '本文介绍了一种轻量级的强化学习算法VL-DAC，该算法无需超参数调整，能够使视觉语言模型（VLMs）从低成本的模拟器中学习通用策略。VL-DAC通过对动作令牌应用PPO更新，同时仅在环境步骤级别学习价值，从而实现了简单的解耦，避免了不稳定的加权项，促进了更快、更可靠的收敛。通过在单个低成本模拟器中训练VLM，VL-DAC能够在多个真实世界基准上显著提高性能，而不影响图像理解的准确性。这些结果首次证明了简单的强化学习算法可以在廉价的合成环境中完全训练VLM，并在真实图像的代理控制、空间推理和网页导航基准上取得可测量的提升。'}}}, {'id': 'https://huggingface.co/papers/2508.03680', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03680', 'abstract': "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'score': 21, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '78a8398db0f71f63', 'authors': ['Xufang Luo', 'Yuge Zhang', 'Zhiyuan He', 'Zilong Wang', 'Siyun Zhao', 'Dongsheng Li', 'Luna K. Qiu', 'Yuqing Yang'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.03680.jpg', 'data': {'categories': ['#agents', '#rag', '#games', '#math', '#training', '#optimization', '#rl'], 'emoji': '⚡', 'ru': {'title': 'Agent Lightning: универсальный фреймворк для обучения ИИ-агентов', 'desc': 'Agent Lightning - это гибкий фреймворк для обучения с подкреплением больших языковых моделей в различных агентах. Он использует иерархический алгоритм обучения с подкреплением и отделяет выполнение от обучения для обработки сложных взаимодействий. Фреймворк позволяет интегрироваться с существующими агентами, разработанными различными способами, практически без изменения кода. Эксперименты показали стабильные улучшения в задачах text-to-SQL, генерации с использованием извлечения информации и использования математических инструментов.'}, 'en': {'title': 'Decoupling Training and Execution for Enhanced AI Agent Performance', 'desc': 'Agent Lightning is a versatile framework designed for training Large Language Models (LLMs) using Reinforcement Learning (RL) techniques. It separates the training process from agent execution, allowing for easier integration with various existing AI agents without significant code changes. By modeling agent execution as a Markov decision process, it introduces a hierarchical RL algorithm called LightningRL, which effectively manages complex interactions and multi-agent scenarios. The framework has shown promising results in tasks like text-to-SQL and retrieval-augmented generation, indicating its effectiveness for real-world applications.'}, 'zh': {'title': 'Agent Lightning：灵活的智能体训练框架', 'desc': 'Agent Lightning是一个灵活的强化学习框架，旨在为各种智能体训练大型语言模型（LLMs）。它通过将执行与训练解耦，使用层次化的强化学习算法，能够处理复杂的交互逻辑。该框架允许与现有智能体的无缝集成，几乎不需要代码修改。实验结果表明，Agent Lightning在文本到SQL、增强生成和数学工具使用等任务中表现出稳定的持续改进，展示了其在实际智能体训练和部署中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.03159', 'title': 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction', 'url': 'https://huggingface.co/papers/2508.03159', 'abstract': "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.  \t\t\t\t\tAI-generated summary \t\t\t\t Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.", 'score': 18, 'issue_id': 5222, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '229c53307b839ab7', 'authors': ['Jueon Park', 'Yein Park', 'Minju Song', 'Soyon Park', 'Donghyeon Lee', 'Seungheun Baek', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences, Seoul 04778, Republic of Korea', 'Department of Computer Science and Engineering, Korea University, Seoul 17035, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2508.03159.jpg', 'data': {'categories': ['#science', '#interpretability', '#healthcare', '#reasoning', '#data', '#multimodal'], 'emoji': '🧪', 'ru': {'title': 'CoTox: Интеллектуальное прогнозирование токсичности лекарств с помощью LLM', 'desc': 'CoTox - это новая система, которая объединяет большие языковые модели (LLM) с рассуждениями по цепочке мыслей для прогнозирования множественной токсичности лекарств. Она использует данные о химической структуре, биологических путях и термины генной онтологии для генерации интерпретируемых предсказаний токсичности. CoTox превосходит традиционные методы машинного обучения и глубокого обучения, улучшая интерпретируемость и предсказательную способность. Система демонстрирует потенциал для улучшения оценки безопасности лекарств на ранних стадиях разработки.'}, 'en': {'title': 'CoTox: Enhancing Drug Toxicity Prediction with LLMs and Chain-of-Thought Reasoning', 'desc': "CoTox is a new framework that combines large language models (LLMs) with chain-of-thought reasoning to predict multi-toxicity in drugs. It enhances the prediction process by integrating chemical structure data, biological pathways, and gene ontology terms, which helps in making the predictions more interpretable. By using step-by-step reasoning, CoTox improves the model's ability to understand complex biological mechanisms and organ-specific toxicities. The framework has shown superior performance compared to traditional machine learning and deep learning models, making it a valuable tool for early-stage drug safety assessment."}, 'zh': {'title': 'CoTox：提升药物毒性预测的智能框架', 'desc': 'CoTox是一个将大型语言模型（LLMs）与链式推理相结合的框架，旨在提高多种毒性预测的准确性。它通过整合化学结构数据、生物通路和基因本体术语，生成可解释的毒性预测。与传统的机器学习和深度学习模型相比，CoTox在药物开发中表现出更好的预测性能。该框架的设计使得毒性预测与生理反应相一致，展示了LLM框架在药物安全性评估中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.03905', 'title': 'Sotopia-RL: Reward Design for Social Intelligence', 'url': 'https://huggingface.co/papers/2508.03905', 'abstract': 'Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.', 'score': 16, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '815208c1d75b8f05', 'authors': ['Haofei Yu', 'Zhengyang Qi', 'Yining Zhao', 'Kolby Nottingham', 'Keyang Xuan', 'Bodhisattwa Prasad Majumder', 'Hao Zhu', 'Paul Pu Liang', 'Jiaxuan You'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Stanford University', 'University of California Irvine', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.03905.jpg', 'data': {'categories': ['#games', '#alignment', '#rlhf', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Sotopia-RL: прорыв в обучении социальному интеллекту для ИИ', 'desc': 'Sotopia-RL - это новая система обучения с подкреплением для улучшения социального интеллекта больших языковых моделей. Она преобразует обратную связь в многомерные награды на уровне отдельных высказываний, что позволяет эффективнее обучать модели социальным задачам. Система решает проблемы частичной наблюдаемости и многомерности социальных взаимодействий, которые затрудняют применение классических методов обучения с подкреплением. Эксперименты показали, что Sotopia-RL значительно превосходит существующие подходы в решении социальных задач.'}, 'en': {'title': 'Enhancing Social Intelligence in LLMs with Sotopia-RL', 'desc': "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."}, 'zh': {'title': '提升社交智能的强化学习新框架', 'desc': 'Sotopia-RL是一种新颖的强化学习框架，旨在提升大型语言模型的社交智能。它通过将反馈细化为发言级别的多维奖励，来改善模型在社交任务中的表现。该框架解决了社交互动中的部分可观察性和多维性问题，使得模型能够更有效地学习复杂的社交策略。实验结果表明，Sotopia-RL在社交目标完成评分上达到了最先进的水平，显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2508.01858', 'title': 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents', 'url': 'https://huggingface.co/papers/2508.01858', 'abstract': 'A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent\'s capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent\'s processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent\'s conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner', 'score': 15, 'issue_id': 5220, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'ad30239b3abef884', 'authors': ['Yuhan Guo', 'Cong Guo', 'Aiwen Sun', 'Hongliang He', 'Xinyu Yang', 'Yue Lu', 'Yingji Zhang', 'Xuntao Guo', 'Dong Zhang', 'Jianzhuang Liu', 'Jiang Duan', 'Yijia Xiao', 'Liangjian Wen', 'Hai-Ming Xu', 'Yong Dai'], 'affiliations': ['Central South University', 'Fudan University', 'Harbin Institute of Technology', 'Hithink Research', 'Shanghai Jiao Tong University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Southwestern University of Finance and Economics', 'University of Adelaide', 'University of California, Los Angeles', 'University of Manchester', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01858.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#reasoning', '#open_source'], 'emoji': '🕸️', 'ru': {'title': 'Когнитивная структура для веб-агентов нового поколения', 'desc': 'Статья представляет новую структуру для веб-агентов, разделяющую их возможности на изучение контента знаний и когнитивные процессы. Авторы предлагают Web-CogKnowledge Framework, категоризирующий знания как фактические, концептуальные и процедурные. Для облегчения приобретения знаний создан структурированный набор данных Web-CogDataset из 14 реальных веб-сайтов. Разработан новый агент Web-CogReasoner, использующий знание-ориентированную цепочку рассуждений (Chain-of-Thought), показывающий превосходство над существующими моделями.'}, 'en': {'title': 'Empowering Web Agents through Structured Knowledge and Reasoning', 'desc': 'This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models.'}, 'zh': {'title': '智能体能力的双重分解：知识与认知', 'desc': '本文提出了一种网络智能体的框架，将其能力分解为知识内容学习和认知过程。我们定义了Web-CogKnowledge框架，将知识分为事实性、概念性和程序性三类，以支持智能体的学习和推理。通过构建Web-CogDataset，我们为智能体提供了系统化的知识基础，帮助其掌握必要的核心知识。最后，我们开发了Web-CogReasoner，并通过实验验证了其在处理新任务时的优越性，尤其是在结构化知识至关重要的情况下。'}}}, {'id': 'https://huggingface.co/papers/2508.03560', 'title': 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought', 'url': 'https://huggingface.co/papers/2508.03560', 'abstract': 'LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method.', 'score': 12, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '14b52fe9021b5b26', 'authors': ['Yi Gui', 'Zhen Li', 'Zhongyi Zhang', 'Guohao Wang', 'Tianpeng Lv', 'Gaoyang Jiang', 'Yi Liu', 'Dongping Chen', 'Yao Wan', 'Hongyu Zhang', 'Wenbin Jiang', 'Xuanhua Shi', 'Hai Jin'], 'affiliations': ['Chongqing University, Chongqing, China', 'Huazhong University of Science and Technology, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03560.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#architecture', '#multimodal', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'LaTCoder: улучшение сохранения макета при генерации кода из дизайна веб-страниц', 'desc': 'LaTCoder - это новый подход к задаче преобразования дизайна веб-страниц в код, который улучшает сохранение макета. Метод использует разделение дизайна на блоки и применяет рассуждения по цепочке мыслей (Chain-of-Thought) с помощью мультимодальных больших языковых моделей (MLLM). LaTCoder показывает значительные улучшения по автоматическим метрикам, таким как TreeBLEU и MAE. В ходе оценки предпочтений пользователей, веб-страницы, сгенерированные LaTCoder, были выбраны в более чем 60% случаев.'}, 'en': {'title': 'Enhancing Layout Preservation in Design-to-Code with LaTCoder', 'desc': "LaTCoder is a novel approach designed to improve the accuracy of converting webpage designs into code while preserving the layout. It utilizes Chain-of-Thought reasoning to enhance the performance of Multimodal Large Language Models (MLLMs) by breaking down designs into manageable blocks. The method employs two assembly strategies to optimize the final output, ensuring that the generated code closely matches the intended design. Experimental results show significant improvements in both automatic metrics and human preference, indicating LaTCoder's effectiveness in design-to-code tasks."}, 'zh': {'title': '提升网页设计布局保留的LaTCoder', 'desc': 'LaTCoder是一种新方法，旨在提高网页设计到代码生成过程中的布局保留能力。它通过将网页设计分割成多个图像块，并使用基于思维链的推理方法来生成每个块的代码。该方法结合了绝对定位和基于多模态大语言模型的组装策略，以选择最佳输出。实验结果表明，LaTCoder在多个基准测试中显著提高了自动评估指标和人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2507.23785', 'title': 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis', 'url': 'https://huggingface.co/papers/2507.23785', 'abstract': 'A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.', 'score': 12, 'issue_id': 5220, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '47c7686978b9c4dc', 'authors': ['Bowen Zhang', 'Sicheng Xu', 'Chuxin Wang', 'Jiaolong Yang', 'Feng Zhao', 'Dong Chen', 'Baining Guo'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23785.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического 3D-контента из видео с помощью диффузионных моделей', 'desc': 'Представлена новая система для создания динамического 3D-контента из одиночных видеовходов. Используется VAE для кодирования канонических гауссовых сплатов и их временных вариаций, а также диффузионная модель с учетом времени для генерации. Система обучена на тщательно отобранных анимируемых 3D-объектах из набора данных Objaverse. Демонстрирует превосходное качество генерации и обобщение на реальные видеовходы, несмотря на обучение только на синтетических данных.'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Worlds', 'desc': 'This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation.'}, 'zh': {'title': '从视频生成高质量动态3D内容的创新框架', 'desc': '本文提出了一种新颖的框架，用于从单个视频输入生成高质量的动态3D内容。我们引入了直接的4DMesh到高斯样条（GS）变分场变分自编码器（VAE），能够直接编码3D动画数据中的高斯样条及其时间变化。通过这种高效的表示方式，我们训练了一个基于时间感知的高斯变分场扩散模型，能够根据输入视频和高斯样条生成动态内容。实验结果表明，该模型在生成质量上优于现有方法，并且在处理真实视频输入时表现出良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.03789', 'title': 'HPSv3: Towards Wide-Spectrum Human Preference Score', 'url': 'https://huggingface.co/papers/2508.03789', 'abstract': 'HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.', 'score': 11, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'a2a0678cfc88e0ef', 'authors': ['Yuhang Ma', 'Xiaoshi Wu', 'Keqiang Sun', 'Hongsheng Li'], 'affiliations': ['CPII, InnoHK', 'CUHK MMLab', 'Kings College London', 'Mizzen AI', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03789.jpg', 'data': {'categories': ['#alignment', '#data', '#benchmark', '#dataset', '#cv', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'HPSv3: Новый стандарт оценки качества генерации изображений по тексту', 'desc': 'HPSv3 - это новый метод оценки качества генерации изображений по тексту, основанный на широком спектре данных и учитывающий неопределенность при ранжировании. Авторы представили обширный датасет HPDv3, содержащий 1.08 миллиона пар текст-изображение и 1.17 миллиона аннотированных попарных сравнений. Метод использует модель на основе VLM, обученную с помощью функции потерь, учитывающей неопределенность при ранжировании. Также предложен итеративный подход Chain-of-Human-Preference для улучшения качества сгенерированных изображений.'}, 'en': {'title': 'Enhancing Image Generation with Human Preference Score v3', 'desc': 'The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment.'}, 'zh': {'title': '提升图像生成质量的HPSv3与CoHP方法', 'desc': 'HPSv3是一种人类偏好评分，利用广泛的数据集和考虑不确定性的排名损失，提升文本到图像生成的质量。我们发布了HPDv3，这是第一个包含108万对文本-图像和117万对标注比较的广泛人类偏好数据集。我们还引入了一种基于视觉语言模型的偏好模型，使用不确定性感知的排名损失进行细致排名。此外，我们提出了人类偏好链（CoHP），通过迭代图像优化，在每一步选择最佳图像，从而提高生成质量。'}}}, {'id': 'https://huggingface.co/papers/2508.02215', 'title': 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding', 'url': 'https://huggingface.co/papers/2508.02215', 'abstract': 'LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.', 'score': 9, 'issue_id': 5220, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'caa8de613517d011', 'authors': ['Yike Zhang', 'Zhiyuan He', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yuqing Yang', 'Jianyong Wang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02215.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': '🔪', 'ru': {'title': 'Эффективное обрезание кэша для ускорения языковых моделей', 'desc': 'LeanK - это метод машинного обучения для оптимизации больших языковых моделей. Он уменьшает использование памяти и ускоряет декодирование, удаляя неважные каналы в кэше ключей. LeanK использует двухэтапный процесс обучения для создания маски каналов, удовлетворяющей требованиям разреженности и аппаратного выравнивания. Эксперименты показывают сокращение памяти кэша ключей до 70% и ускорение вычисления внимания в 1,3 раза.'}, 'en': {'title': 'LeanK: Pruning for Efficient Language Model Performance', 'desc': 'LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation.'}, 'zh': {'title': 'LeanK：高效解码的大型语言模型优化方案', 'desc': 'LeanK是一种基于学习的方法，旨在减少大型语言模型中的不重要的关键缓存通道，从而降低内存使用并加速解码，同时不影响准确性。该方法利用静态通道稀疏性，通过一种新颖的两阶段训练过程，学习满足特定稀疏比和硬件对齐要求的通道静态掩码。实验结果表明，LeanK可以减少高达70%的K缓存和16%-18%的V缓存内存，并且自定义解码内核使注意力计算速度提高了1.3倍。通过分析学习到的重要性分布，我们还提供了对长上下文推理过程中模型通道和注意力头的深入见解。'}}}, {'id': 'https://huggingface.co/papers/2508.02807', 'title': 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework', 'url': 'https://huggingface.co/papers/2508.02807', 'abstract': 'DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. In the second stage, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/', 'score': 8, 'issue_id': 5224, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'f3f693bca2e57a94', 'authors': ['Tongchun Zuo', 'Zaiyu Huang', 'Shuliang Ning', 'Ente Lin', 'Chao Liang', 'Zerong Zheng', 'Jianwen Jiang', 'Yuan Zhang', 'Mingyuan Gao', 'Xin Dong'], 'affiliations': ['ByteDance Intelligent Creation', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02807.jpg', 'data': {'categories': ['#video', '#cv', '#synthetic', '#multimodal', '#diffusion'], 'emoji': '👚', 'ru': {'title': 'DreamVVT: Реалистичная виртуальная примерка одежды на видео с помощью ИИ', 'desc': 'DreamVVT - это двухэтапная система для виртуальной примерки одежды на видео, использующая диффузионные трансформеры и LoRA-адаптеры. Она улучшает сохранение деталей одежды и временную согласованность, используя непарные данные с изображениями людей и предобученные модели. На первом этапе система создает высококачественные ключевые кадры с примеркой, используя мультикадровую модель и визуально-языковую модель. На втором этапе применяется предобученная модель генерации видео с LoRA-адаптерами для обеспечения временной согласованности и правдоподобных движений.'}, 'en': {'title': 'Enhancing Video Try-On with DreamVVT: Consistency Meets Detail', 'desc': 'DreamVVT is a two-stage framework designed to improve video virtual try-on (VVT) by using Diffusion Transformers and LoRA adapters. It effectively utilizes unpaired human-centric data and pretrained models to maintain garment details and ensure temporal consistency in videos. The first stage generates high-quality keyframe images using a multi-frame try-on model and a vision-language model, while the second stage focuses on video generation by incorporating motion and appearance data. This innovative approach allows DreamVVT to outperform existing methods in preserving garment fidelity and achieving smooth motion in dynamic scenarios.'}, 'zh': {'title': 'DreamVVT：提升视频虚拟试穿的创新框架', 'desc': 'DreamVVT是一种两阶段框架，利用扩散变换器和LoRA适配器，提升视频虚拟试穿技术。该方法通过使用未配对的人体中心数据和预训练模型，能够更好地保留服装细节和时间一致性。第一阶段通过多帧试穿模型生成高保真关键帧图像，第二阶段则利用预训练的视频生成模型确保动态运动的连贯性。实验结果表明，DreamVVT在真实场景中优于现有方法，能够更好地保持服装内容的细节和时间稳定性。'}}}, {'id': 'https://huggingface.co/papers/2508.04664', 'title': 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management', 'url': 'https://huggingface.co/papers/2508.04664', 'abstract': "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.", 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '1575b65bda95c5f9', 'authors': ['Mo Li', 'L. H. Xu', 'Qitai Tan', 'Ting Cao', 'Yunxin Liu'], 'affiliations': ['Independent Researcher', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04664.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Sculptor: Умное управление контекстом для улучшения работы языковых моделей', 'desc': 'Статья представляет фреймворк Sculptor для активного управления контекстом в больших языковых моделях (LLM). Sculptor позволяет LLM проактивно управлять вниманием и рабочей памятью, что снижает проактивную интерференцию и улучшает надежность рассуждений. Фреймворк включает инструменты для фрагментации контекста, суммирования и интеллектуального поиска. Эксперименты показывают, что Sculptor значительно улучшает производительность LLM на длинных контекстах без специального обучения.'}, 'en': {'title': 'Sculptor: Mastering Memory for Better Long-Context Reasoning', 'desc': 'The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies.'}, 'zh': {'title': '主动上下文管理，提升LLM性能！', 'desc': 'Sculptor是一个用于主动上下文管理的框架，旨在提高大型语言模型（LLM）在处理长上下文时的表现。该框架通过主动管理注意力和工作记忆，减少了前期信息的干扰，从而改善推理的可靠性。Sculptor提供了三种工具：上下文碎片化、摘要、隐藏与恢复，以及智能搜索，帮助LLM更有效地处理信息。实验结果表明，Sculptor在没有特定训练的情况下，显著提升了模型的性能，强调了主动上下文管理的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.04586', 'title': 'Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference', 'url': 'https://huggingface.co/papers/2508.04586', 'abstract': 'The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.', 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '0e9d3ff69536a24d', 'authors': ['Nuo Chen', 'Moming Duan', 'Andre Huikai Lin', 'Qian Wang', 'Jiaying Wu', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.04586.jpg', 'data': {'categories': ['#ethics', '#survey'], 'emoji': '🌐', 'ru': {'title': 'Революция в формате конференций ИИ: от централизации к федерации', 'desc': 'Данная статья анализирует структурные проблемы конференций по искусственному интеллекту, включая рост числа публикаций, углеродный след, негативное настроение сообщества и логистические трудности. Авторы выделяют четыре ключевые области напряженности: научную, экологическую, психологическую и логистическую. Для решения этих проблем предлагается модель Community-Federated Conference (CFC), которая разделяет рецензирование, презентации и нетворкинг на глобально координируемые, но локально организованные компоненты. Эта модель предлагает более устойчивый, инклюзивный и гибкий подход к проведению исследований в области ИИ.'}, 'en': {'title': 'Towards Sustainable AI Conferences: A Community-Federated Approach', 'desc': 'This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research.'}, 'zh': {'title': '构建可持续的人工智能会议新模式', 'desc': '这篇论文诊断了人工智能会议的结构性问题，包括发表率、碳足迹、负面社区情绪和后勤挑战。随着会议数量的快速增长，传统的集中式会议模式变得越来越不可持续。论文提出了一种基于社区的联合会议模型，旨在解决这些问题，促进科学传播的公平性和社区的福祉。该模型将同行评审、展示和网络交流分开，提供了一种更可持续、包容和有韧性的人工智能研究发展路径。'}}}, {'id': 'https://huggingface.co/papers/2508.01928', 'title': 'IAUNet: Instance-Aware U-Net', 'url': 'https://huggingface.co/papers/2508.01928', 'abstract': 'IAUNet, a query-based U-Net architecture with a lightweight convolutional Pixel decoder and Transformer decoder, outperforms state-of-the-art models in biomedical instance segmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet', 'score': 5, 'issue_id': 5226, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '0f9cff1ae25f175b', 'authors': ['Yaroslav Prytula', 'Illia Tsiporenko', 'Ali Zeynalli', 'Dmytro Fishman'], 'affiliations': ['Institute of Computer Science, University of Tartu', 'STACC U, Tartu, Estonia', 'Ukrainian Catholic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01928.jpg', 'data': {'categories': ['#architecture', '#cv', '#games', '#benchmark', '#science', '#dataset', '#optimization', '#healthcare'], 'emoji': '🔬', 'ru': {'title': 'IAUNet: Прорыв в сегментации клеток с помощью гибридной архитектуры', 'desc': 'IAUNet - это новая архитектура сегментации изображений на основе U-Net с запросами. Она включает облегченный сверточный декодер пикселей и декодер на основе трансформеров для уточнения признаков объектов. Модель превосходит современные методы в задаче сегментации экземпляров биомедицинских изображений, особенно для перекрывающихся клеток. Авторы также представили новый набор данных для сегментации клеток, который станет эталоном в этой области.'}, 'en': {'title': 'IAUNet: Revolutionizing Biomedical Instance Segmentation with Query-Based U-Net', 'desc': 'IAUNet is a new architecture designed for biomedical instance segmentation, which helps in identifying and separating individual cells in images. It combines a traditional U-Net structure with a lightweight convolutional Pixel decoder and a Transformer decoder to enhance performance. This model efficiently reduces parameters while improving the segmentation of overlapping objects. The introduction of the 2025 Revvity Full Cell Segmentation Dataset provides a valuable resource for training and benchmarking segmentation models in this field.'}, 'zh': {'title': 'IAUNet：生物医学实例分割的新标杆', 'desc': 'IAUNet是一种基于查询的U-Net架构，结合了轻量级卷积像素解码器和Transformer解码器，能够在生物医学实例分割中超越现有的最先进模型。实例分割在生物医学成像中至关重要，因为它可以准确区分重叠且大小不一的细胞。IAUNet通过全新的轻量级卷积像素解码器提高了模型的效率，并减少了参数数量，同时引入的Transformer解码器能够在多个尺度上细化特定对象的特征。我们还推出了2025 Revvity全细胞分割数据集，为生物医学实例分割设定了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04295', 'title': 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation', 'url': 'https://huggingface.co/papers/2508.04295', 'abstract': "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.", 'score': 4, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '0dc09e7a8e2bad95', 'authors': ['Chaofan Wang', 'Tingrui Yu', 'Jie Wang', 'Dong Chen', 'Wenrui Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Beijun Shen'], 'affiliations': ['Huawei Technologies Co., Ltd', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04295.jpg', 'data': {'categories': ['#open_source', '#architecture', '#plp', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'EvoC2Rust: Эволюционный подход к автоматическому переводу C в Rust', 'desc': 'EvoC2Rust - это автоматизированный фреймворк для перевода проектов на C в Rust, использующий подход на основе скелета кода. Он сочетает методы на основе правил и языковых моделей для улучшения синтаксиса, семантики и безопасности кода. Фреймворк работает в три этапа: декомпозиция проекта, инкрементальный перевод функций и исправление ошибок компиляции. Оценка на открытых и промышленных проектах показала превосходство EvoC2Rust над существующими подходами в точности перевода и безопасности кода.'}, 'en': {'title': 'EvoC2Rust: Bridging C to Rust with Safety and Precision', 'desc': 'EvoC2Rust is a novel framework designed to automate the translation of entire C projects into Rust, leveraging a skeleton-guided approach that integrates both rule-based and LLM-based techniques. This method addresses the limitations of existing solutions by ensuring that the translated code maintains both safety and idiomatic Rust syntax. The framework operates in three stages: it first breaks down the C project into modules, then translates functions incrementally, and finally resolves any compilation errors using a combination of LLM and static analysis. Evaluation results show that EvoC2Rust significantly outperforms previous methods in terms of syntax, semantic accuracy, and code safety, making it a robust solution for converting legacy C code to Rust.'}, 'zh': {'title': 'EvoC2Rust：高效的C到Rust自动转换框架', 'desc': 'EvoC2Rust是一个自动化框架，旨在将整个C项目转换为Rust代码。它采用了骨架引导的方法，结合了基于规则和基于大语言模型（LLM）的方法，以提高代码的语法、语义和安全性。该框架通过三个进化阶段进行项目级翻译，首先将C项目分解为功能模块，然后逐步翻译函数，最后通过集成LLM和静态分析修复编译错误。评估结果显示，EvoC2Rust在C到Rust的翻译中表现优越，语法和语义准确性分别提高了17.24%和14.32%。'}}}, {'id': 'https://huggingface.co/papers/2508.00222', 'title': 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization', 'url': 'https://huggingface.co/papers/2508.00222', 'abstract': "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.", 'score': 4, 'issue_id': 5224, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'ec45fc053d8a3704', 'authors': ['Yihong Dong', 'Xue Jiang', 'Yongding Tao', 'Huanyu Liu', 'Kechi Zhang', 'Lili Mou', 'Rongyu Cao', 'Yingwei Ma', 'Jue Chen', 'Binhua Li', 'Zhi Jin', 'Fei Huang', 'Yongbin Li', 'Ge Li'], 'affiliations': ['Department of Computing Science, University of Alberta', 'School of Computer Science, Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.00222.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'RL-PLUS: Прорыв в обучении с подкреплением для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет RL-PLUS - новый гибридный подход к оптимизации политики для улучшения способностей рассуждения больших языковых моделей (LLM). RL-PLUS объединяет метод множественной выборки по важности и функцию преимущества на основе исследования для преодоления ограничений базовой модели. Этот метод превосходит существующий подход RLVR на различных тестовых наборах данных по математическим рассуждениям и задачам вне распределения. RL-PLUS также эффективно решает проблему коллапса границ возможностей, характерную для RLVR.'}, 'en': {'title': 'Breaking Boundaries in LLM Reasoning with RL-PLUS', 'desc': "The paper introduces RL-PLUS, a new hybrid-policy optimization method designed to improve the reasoning abilities of Large Language Models (LLMs). It combines Multiple Importance Sampling and Exploration-Based Advantage Function to enhance the model's performance beyond the limitations of traditional Reinforcement Learning with Verifiable Reward (RLVR). By addressing the issues of distributional mismatch and guiding exploration towards valuable reasoning paths, RL-PLUS effectively prevents capability boundary collapse. Extensive experiments show that RL-PLUS outperforms existing methods on various benchmarks, achieving significant improvements in reasoning tasks."}, 'zh': {'title': 'RL-PLUS：突破推理能力边界的创新方法', 'desc': 'RL-PLUS是一种混合策略优化方法，旨在提升大型语言模型（LLM）的推理能力。它通过结合多重重要性采样和基于探索的优势函数，克服了传统强化学习方法（如RLVR）在能力边界崩溃方面的局限。RL-PLUS能够有效利用外部数据，指导模型探索高价值的推理路径，从而实现更强的推理能力。实验结果表明，RL-PLUS在多个基准测试中表现优异，显著超越了现有方法。'}}}, {'id': 'https://huggingface.co/papers/2507.21974', 'title': 'Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks', 'url': 'https://huggingface.co/papers/2507.21974', 'abstract': 'A lightweight framework using Large Language Models (LLMs) with TeleLogs dataset and a two-stage training methodology improves Root Cause Analysis (RCA) in mobile networks by enhancing interpretability and reasoning quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.', 'score': 4, 'issue_id': 5227, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '91b127bf8dd7d610', 'authors': ['Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Yibin Kang', 'Haozhe Zhang', 'Merouane Debbah', 'Fadhel Ayed'], 'affiliations': ['Huawei Technologies, China', 'Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.21974.jpg', 'data': {'categories': ['#reasoning', '#rl', '#interpretability', '#dataset', '#training', '#open_source', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Улучшение анализа первопричин в мобильных сетях с помощью адаптированных языковых моделей', 'desc': 'В статье представлен легковесный фреймворк, использующий большие языковые модели (LLM) с набором данных TeleLogs для улучшения анализа первопричин (RCA) в мобильных сетях. Авторы предлагают двухэтапную методологию обучения, сочетающую контролируемую тонкую настройку и обучение с подкреплением. Этот подход позволяет интегрировать доменные знания и генерировать структурированные многоступенчатые диагностические объяснения. Эксперименты показывают значительный прирост производительности по сравнению с современными моделями рассуждений и моделями без рассуждений.'}, 'en': {'title': 'Enhancing RCA in Mobile Networks with Domain-Specific LLMs', 'desc': 'This paper presents a lightweight framework that utilizes Large Language Models (LLMs) to enhance Root Cause Analysis (RCA) in mobile networks. It introduces the TeleLogs dataset, which contains annotated troubleshooting problems to evaluate RCA capabilities effectively. The authors propose a two-stage training methodology that combines supervised fine-tuning and reinforcement learning to improve the reasoning quality and interpretability of LLMs. Experimental results show that this approach significantly outperforms existing models, demonstrating the potential of domain-adapted LLMs for practical RCA applications.'}, 'zh': {'title': '领域适应与推理增强的根本原因分析新方法', 'desc': '本研究提出了一种轻量级框架，利用大型语言模型（LLMs）来改善移动网络中的根本原因分析（RCA）。我们引入了TeleLogs数据集，这是一个经过注释的故障排除问题集合，旨在评估RCA能力。通过两阶段训练方法，结合监督微调和强化学习，提升了LLMs的准确性和推理质量。实验结果显示，经过领域适应的LLMs在网络操作和管理中的RCA任务上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2508.01197', 'title': 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding', 'url': 'https://huggingface.co/papers/2508.01197', 'abstract': 'A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.', 'score': 3, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'd7be41190836a7cc', 'authors': ['Zhan Shi', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['College of Computer Science, Zhejiang University, Hangzhou 310027, China', 'College of Software Technology, Zhejiang University', 'Udeer.ai, Hangzhou 310000, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01197.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#3d', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Точное восприятие объектов для беспилотных автомобилей с помощью 3D грунтовки', 'desc': 'Статья представляет новый бенчмарк для задачи 3D грунтовки объектов с использованием естественного языка и воксельных аннотаций в контексте автономного вождения. Авторы предлагают модель GroundingOcc, которая объединяет визуальные, текстовые и облачные признаки для предсказания местоположения и занятости объектов. Модель включает мультимодальный энкодер, модули для предсказания воксельной занятости и уточнения локализации, а также дополнительные компоненты для 2D грунтовки и оценки глубины. Эксперименты показывают превосходство предложенного метода над существующими базовыми моделями в задаче 3D грунтовки занятости.'}, 'en': {'title': 'Enhancing Object Perception with 3D Occupancy Grounding', 'desc': 'This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': '提升自动驾驶物体感知的3D占用基础视觉定位', 'desc': '本论文提出了一种新的基准和模型，用于通过自然语言和体素级注释进行3D占用基础的视觉定位，旨在提高自动驾驶中的物体感知能力。现有的视觉定位任务通常依赖于边界框，这种方法无法捕捉到细粒度的细节，导致物体表示不准确。我们引入的GroundingOcc模型通过多模态学习，结合视觉、文本和点云特征，从粗到细地预测物体位置和占用信息。实验结果表明，我们的方法在3D占用基础的视觉定位任务中优于现有的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04632', 'title': 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2508.04632', 'abstract': 'Instruction Following Decorator enhances RLVR by improving sample efficiency, intent alignment, and reducing reward hacking in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.', 'score': 2, 'issue_id': 5228, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '1e91bd1d8a3387f3', 'authors': ['Xu Guo', 'Tianyi Liang', 'Tong Jian', 'Xiaogui Yang', 'Ling-I Wu', 'Chenhui Li', 'Zhihui Lu', 'Qipeng Guo', 'Kai Chen'], 'affiliations': ['East China Normal University', 'Fudan University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.04632.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#alignment', '#training', '#optimization', '#security', '#rl'], 'emoji': '🎓', 'ru': {'title': 'IFDecorator: Повышение эффективности и надежности обучения языковых моделей следованию инструкциям', 'desc': 'Данная статья представляет Instruction Following Decorator (IFDecorator) - фреймворк, улучшающий обучение с подкреплением с проверяемыми наградами (RLVR) для больших языковых моделей. IFDecorator включает в себя кооперативно-состязательный механизм генерации данных, модуль проверки соответствия намерениям и диагностический механизм для обнаружения эксплуатации наград. Авторы демонстрируют значительные улучшения в следовании инструкциям на различных бенчмарках, превосходя более крупные проприетарные модели. Также отмечается существенное снижение уровня эксплуатации наград благодаря применению IFDecorator.'}, 'en': {'title': 'Enhancing Instruction Following with Robust Reinforcement Learning', 'desc': 'The paper presents the Instruction Following Decorator (IFDecorator), a framework designed to enhance Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs). IFDecorator improves sample efficiency by using a cooperative-adversarial data flywheel that generates increasingly difficult instruction-verification pairs. It also includes IntentCheck, which ensures that the model aligns with user intent, and trip wires that detect and mitigate reward hacking behaviors. The results show that the Qwen2.5-32B-Instruct-IFDecorator model achieves high accuracy and reduces reward hacking, outperforming larger models like GPT-4o.'}, 'zh': {'title': '提升指令跟随能力的创新框架', 'desc': '本论文介绍了一种名为指令跟随装饰器（IFDecorator）的框架，旨在提高大语言模型（LLMs）在可验证奖励（RLVR）下的指令跟随能力。该框架通过三个主要组件来增强样本效率和意图对齐，减少奖励黑客行为。首先，它利用合作对抗的数据飞轮生成越来越具挑战性的指令-验证对；其次，IntentCheck模块确保模型的意图对齐；最后，trip wires机制检测并捕捉奖励黑客行为。实验结果表明，使用IFDecorator的模型在多个基准测试中表现优异，显著提高了准确率和整体能力。'}}}, {'id': 'https://huggingface.co/papers/2508.04010', 'title': 'HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive\n  Policy Enhancement and Dual-Objective Optimization', 'url': 'https://huggingface.co/papers/2508.04010', 'abstract': 'HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '9a431a1adec12655', 'authors': ['Yurun Chen', 'Xavier Hu', 'Yuhan Liu', 'Keting Yin', 'Juncheng Li', 'Zhuosheng Zhang', 'Shengyu Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04010.jpg', 'data': {'categories': ['#optimization', '#security', '#benchmark', '#architecture', '#agents'], 'emoji': '🛡️', 'ru': {'title': 'Гармония безопасности и эффективности в веб-среде', 'desc': 'HarmonyGuard - это многоагентная система для повышения безопасности и эффективности веб-агентов на основе больших языковых моделей. Она адаптивно обновляет политики безопасности с помощью специального агента и оптимизирует двойные цели безопасности и полезности. Система включает агента полезности, который в реальном времени оценивает цели и использует метакогнитивные способности для их оптимизации. Эксперименты показали значительное улучшение соблюдения политик безопасности и выполнения задач по сравнению с существующими методами.'}, 'en': {'title': 'Balancing Safety and Utility in Web Tasks with HarmonyGuard', 'desc': 'HarmonyGuard is a multi-agent framework designed to improve how web agents comply with security policies while completing tasks. It addresses the challenge of balancing safety and utility in dynamic web environments by adaptively updating security policies. The framework includes a Policy Agent that extracts and updates security policies from external documents and a Utility Agent that optimizes task performance based on safety and utility objectives. Evaluations show that HarmonyGuard significantly enhances policy compliance and task completion compared to existing methods.'}, 'zh': {'title': 'HarmonyGuard：安全与效用的双重优化', 'desc': 'HarmonyGuard是一个多智能体框架，旨在通过自适应更新安全策略和优化安全性与效用的双重目标，增强网络环境中的政策合规性和任务完成度。该框架引入了政策代理，能够从非结构化文档中提取和维护结构化的安全政策，并根据不断变化的威胁持续更新这些政策。同时，效用代理通过马尔可夫实时推理评估安全性和效用的双重目标，并利用元认知能力进行优化。评估结果表明，HarmonyGuard在政策合规性和任务完成度方面显著优于现有基线，政策合规性超过90%。'}}}, {'id': 'https://huggingface.co/papers/2508.01778', 'title': 'DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving\n  via Online HD Map Diffusion', 'url': 'https://huggingface.co/papers/2508.01778', 'abstract': 'DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'd5f701aadba28943', 'authors': ['Zhigang Sun', 'Yiru Wang', 'Anqing Jiang', 'Shuo Wang', 'Yu Gao', 'Yuwen Heng', 'Shouyi Zhang', 'An He', 'Hao Jiang', 'Jinhao Chai', 'Zichong Gu', 'Wang Jijun', 'Shichen Tang', 'Lavdim Halilaj', 'Juergen Luettin', 'Hao Sun'], 'affiliations': ['AIR, Tsinghua University, Beijing, China', 'Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China', 'Robert Bosch GmbH', 'School of Communication and Information Engineering, Shanghai University, Shanghai, China', 'Shanghai Jiaotong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01778.jpg', 'data': {'categories': ['#video', '#optimization', '#games', '#benchmark', '#multimodal', '#agents'], 'emoji': '🚗', 'ru': {'title': 'Слияние семантики для точного автопилота', 'desc': 'DiffSemanticFusion - это фреймворк для улучшения автономного вождения, объединяющий семантические растровые и графовые представления с помощью модуля диффузии карты. Он повышает точность прогнозирования траектории и эффективность сквозного автономного вождения. Эксперименты на реальных наборах данных показали значительное улучшение производительности по сравнению с современными методами. Фреймворк особенно эффективен в сценариях онлайн-генерации HD-карт, где он улучшает стабильность и выразительность представлений.'}, 'en': {'title': 'Fusing Data for Smarter Autonomous Driving', 'desc': 'DiffSemanticFusion is a novel framework designed to improve autonomous driving by combining two types of data representations: semantic raster and graph-based models. The framework uses a map diffusion module to enhance the stability and detail of high-definition maps, which are crucial for accurate trajectory prediction and driving performance. By integrating these representations, DiffSemanticFusion leverages their strengths to provide better scene understanding and decision-making for autonomous vehicles. Experiments show that this approach significantly outperforms existing methods in real-world driving scenarios, demonstrating its effectiveness in both trajectory prediction and end-to-end driving tasks.'}, 'zh': {'title': '融合语义与图形，提升自主驾驶性能', 'desc': 'DiffSemanticFusion 是一种增强自主驾驶的框架，通过融合语义栅格和基于图的表示，利用地图扩散模块来提高轨迹预测和端到端驾驶性能。该方法结合了栅格表示的视觉模型优势和图表示的结构细节，解决了在线高清地图生成中的不稳定性问题。我们在真实世界的自主驾驶基准测试中验证了该框架，结果显示在轨迹预测和规划任务上均优于多种先进方法。实验表明，DiffSemanticFusion 在多个场景中实现了显著的性能提升，特别是在复杂环境下的自主驾驶表现。'}}}, {'id': 'https://huggingface.co/papers/2508.01630', 'title': 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets', 'url': 'https://huggingface.co/papers/2508.01630', 'abstract': 'OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.', 'score': 2, 'issue_id': 5222, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'aca28561e07e250a', 'authors': ['Maziyar Panahi'], 'affiliations': ['CNRS Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2508.01630.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#dataset', '#training', '#transfer_learning', '#open_source', '#benchmark', '#data'], 'emoji': '🧬', 'ru': {'title': 'OpenMed NER: Открытый прорыв в распознавании биомедицинских сущностей', 'desc': 'OpenMed NER - это набор моделей на основе трансформеров с открытым исходным кодом, использующих методы DAPT и LoRA для распознавания именованных сущностей в биомедицинских текстах. Модели достигают наилучших результатов на 10 из 12 эталонных наборов данных, охватывающих химические вещества, заболевания, гены и виды. Обучение проводится эффективно - менее 12 часов на одном GPU с низким углеродным следом. Проект демонстрирует, что стратегически адаптированные модели с открытым исходным кодом могут превзойти закрытые решения в области биомедицинского NER.'}, 'en': {'title': 'OpenMed NER: Efficiently Transforming Biomedical NER with Open-Source Innovation', 'desc': 'OpenMed NER is a collection of open-source transformer models designed for named-entity recognition (NER) in the biomedical field. It utilizes domain-adaptive pre-training (DAPT) and Low-Rank Adaptation (LoRA) to achieve high performance while being computationally efficient. The models were trained on a large dataset of clinical notes and research papers, and they excelled in identifying various biomedical entities, outperforming existing models on multiple benchmarks. This work highlights the potential of open-source solutions to achieve superior results compared to proprietary models, all while maintaining a low environmental impact.'}, 'zh': {'title': 'OpenMed NER：高效的生物医学命名实体识别解决方案', 'desc': 'OpenMed NER 是一套开源的变换器模型，结合了轻量级的领域自适应预训练（DAPT）和参数高效的低秩适应（LoRA），在生物医学命名实体识别（NER）基准测试中表现出色。该模型在350,000段来自伦理来源的临床笔记和研究文献的语料库上进行训练，能够高效提取医疗数据中的结构化信息。OpenMed NER 在12个生物医学NER基准测试中取得了10个数据集的新最优微F1分数，尤其在疾病和化学基准上有显著提升。该模型的训练效率高，单个GPU下训练时间少于12小时，且碳足迹低于1.2公斤CO2e，适合帮助从业者遵守数据保护和人工智能法规。'}}}, {'id': 'https://huggingface.co/papers/2508.00599', 'title': 'DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior', 'url': 'https://huggingface.co/papers/2508.00599', 'abstract': "DPoser-X, a diffusion-based model, addresses the complexity of 3D human poses using variational diffusion sampling and a novel truncated timestep scheduling method, outperforming existing models across various pose benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.", 'score': 2, 'issue_id': 5235, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '52b3dd3f31a8d796', 'authors': ['Junzhe Lu', 'Jing Lin', 'Hongkun Dou', 'Ailing Zeng', 'Yue Deng', 'Xian Liu', 'Zhongang Cai', 'Lei Yang', 'Yulun Zhang', 'Haoqian Wang', 'Ziwei Liu'], 'affiliations': ['Beihang University', 'Independent Researcher', 'NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00599.jpg', 'data': {'categories': ['#benchmark', '#3d', '#diffusion'], 'emoji': '🕺', 'ru': {'title': 'Революция в моделировании поз человека с помощью диффузионных моделей', 'desc': 'DPoser-X - это модель на основе диффузии для моделирования трехмерных поз человека. Она использует вариационную выборку диффузии и новый метод планирования усеченных временных шагов. Модель объединяет различные задачи, связанные с позами, как обратные задачи и решает их с помощью вариационной выборки диффузии. DPoser-X превосходит существующие модели по различным критериям оценки поз.'}, 'en': {'title': 'Revolutionizing 3D Human Pose Generation with DPoser-X', 'desc': 'DPoser-X is a diffusion-based model designed to improve the generation of 3D human poses by utilizing variational diffusion sampling. It addresses the challenges of modeling complex articulated poses and the lack of high-quality datasets by introducing a novel truncated timestep scheduling method tailored for pose data. The model treats various pose-related tasks as inverse problems, allowing it to effectively learn from both whole-body and part-specific datasets. Extensive testing shows that DPoser-X outperforms existing models, setting a new standard for whole-body human pose modeling.'}, 'zh': {'title': 'DPoser-X：突破3D人体姿态建模的极限', 'desc': 'DPoser-X是一种基于扩散的模型，旨在解决3D人体姿态建模的复杂性。它通过变分扩散采样和新颖的截断时间调度方法，提升了在各种姿态基准测试中的表现。该模型将多种姿态相关任务统一为逆问题，通过变分扩散采样进行求解。实验结果表明，DPoser-X在全身、手部、面部和全身姿态建模方面表现出色，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.03983', 'title': 'MiDashengLM: Efficient Audio Understanding with General Audio Captions', 'url': 'https://huggingface.co/papers/2508.03983', 'abstract': 'MiDashengLM is an open audio-language model using general audio captions for efficient and comprehensive audio understanding, offering faster processing and higher throughput compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at https://huggingface.co/mispeech/midashenglm-7b and https://github.com/xiaomi-research/dasheng-lm.', 'score': 1, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6455762dc1eab458', 'authors': ['Heinrich Dinkel', 'Gang Li', 'Jizhong Liu', 'Jian Luan', 'Yadong Niu', 'Xingwei Sun', 'Tianzi Wang', 'Qiyang Xiao', 'Junbo Zhang', 'Jiahao Zhou'], 'affiliations': ['Horizon Team, MiLM Plus Xiaomi Inc., China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03983.jpg', 'data': {'categories': ['#audio', '#dataset', '#open_source'], 'emoji': '🎧', 'ru': {'title': 'MiDashengLM: Открытая аудио-языковая модель для эффективного понимания звука', 'desc': 'MiDashengLM - это новая открытая аудио-языковая модель, разработанная для эффективного и всестороннего понимания аудио с использованием общих аудио-подписей. Модель интегрирует Dasheng, аудио-энкодер с открытым исходным кодом, специально разработанный для эффективной обработки разнообразной аудиоинформации. В отличие от предыдущих работ, сосредоточенных на выравнивании аудио-текста на основе автоматического распознавания речи (ASR), стратегия MiDashengLM фокусируется на общих аудио-подписях, объединяя информацию о речи, звуке и музыке в одно текстовое представление. MiDashengLM обеспечивает ускорение до 4 раз с точки зрения времени до первого токена (TTFT) и до 20 раз более высокую пропускную способность по сравнению с аналогичными моделями.'}, 'en': {'title': 'Revolutionizing Audio Understanding with MiDashengLM', 'desc': 'MiDashengLM is an innovative open audio-language model that enhances audio understanding by utilizing general audio captions. It leverages a unique training dataset called ACAVCaps, which is built from publicly available data, ensuring transparency and reproducibility in its development. Unlike traditional models that focus mainly on Automatic Speech Recognition (ASR), MiDashengLM integrates various audio elements like speech, sound, and music into a unified textual representation. This model significantly improves processing speed, achieving up to 4 times faster time-to-first-token and up to 20 times higher throughput compared to existing models.'}, 'zh': {'title': '高效音频理解的开放模型', 'desc': 'MiDashengLM是一种开放的音频语言模型，旨在通过使用通用音频标题实现高效和全面的音频理解。与现有模型相比，它提供了更快的处理速度和更高的吞吐量。该模型依赖于公开可用的预训练和监督微调数据集，确保了透明性和可重复性。MiDashengLM将语音、声音和音乐信息融合为一个文本表示，能够全面描述复杂的音频场景。'}}}, {'id': 'https://huggingface.co/papers/2508.03970', 'title': 'Data and AI governance: Promoting equity, ethics, and fairness in large\n  language models', 'url': 'https://huggingface.co/papers/2508.03970', 'abstract': 'Approaches to govern, assess, and quantify bias in machine learning models, particularly large language models, are discussed, emphasizing data and AI governance frameworks for ethical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications.', 'score': 1, 'issue_id': 5237, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'f7776b273b77fe9b', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Independent Researcher', 'alum.mit.edu', 'kronml.com'], 'pdf_title_img': 'assets/pdf/title_img/2508.03970.jpg', 'data': {'categories': ['#multimodal', '#data', '#ethics', '#benchmark'], 'emoji': '⚖️', 'ru': {'title': 'Этичный ИИ: управление предвзятостью в языковых моделях', 'desc': 'В статье обсуждаются подходы к управлению, оценке и количественному измерению предвзятости в моделях машинного обучения, особенно в больших языковых моделях (LLM). Авторы представляют набор инструментов BEATS для оценки предвзятости в LLM и описывают структуру управления данными и ИИ для решения проблем этики, справедливости и фактической точности. Предлагаемый подход позволяет проводить тщательное тестирование LLM перед внедрением и обеспечивает непрерывную оценку в реальном времени. Цель статьи - способствовать созданию и внедрению социально ответственных и этичных приложений на основе генеративного ИИ.'}, 'en': {'title': 'Ensuring Ethical AI: Governing Bias in Large Language Models', 'desc': "This paper discusses methods to manage and measure bias in machine learning models, especially large language models (LLMs). It introduces the Bias Evaluation and Assessment Test Suite (BEATS) to identify fairness issues throughout the model's life cycle, from development to deployment. The authors propose a governance framework that ensures ethical practices in AI, focusing on bias, fairness, and factual accuracy. By applying these governance strategies, organizations can improve the safety and ethical standards of their AI systems, reducing risks of discrimination and reputational damage."}, 'zh': {'title': '治理偏见，构建负责任的人工智能', 'desc': '本文讨论了如何在机器学习模型，特别是大型语言模型中，系统性地管理、评估和量化偏见。我们提出了一种数据和人工智能治理框架，以确保在模型开发、验证和生产监控的整个生命周期中，能够有效应对偏见、伦理、公平性和事实性问题。通过实施这一治理框架，组织可以在生产部署前对大型语言模型进行严格的基准测试，并持续实时评估模型的输出。最终，我们希望推动社会责任和伦理对齐的生成式人工智能应用的创建和部署。'}}}, {'id': 'https://huggingface.co/papers/2508.03448', 'title': 'SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering', 'url': 'https://huggingface.co/papers/2508.03448', 'abstract': "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.", 'score': 1, 'issue_id': 5223, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '534674700c0af141', 'authors': ['Jan Melechovsky', 'Ambuj Mehrish', 'Dorien Herremans'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2508.03448.jpg', 'data': {'categories': ['#audio', '#dataset', '#synthetic'], 'emoji': '🎵', 'ru': {'title': 'Универсальное улучшение качества музыки с помощью ИИ', 'desc': 'SonicMaster - это унифицированная генеративная модель для улучшения качества аудио в музыке. Она использует управление на основе текста и парадигму обучения с сопоставлением потоков для устранения различных артефактов. Модель может работать как в автоматическом режиме, так и с целевыми улучшениями на основе текстовых инструкций. SonicMaster обучается на специально созданном наборе данных, включающем пары деградированных и высококачественных треков.'}, 'en': {'title': 'SonicMaster: Revolutionizing Music Restoration with AI', 'desc': 'SonicMaster is a novel generative model designed to enhance music audio quality by correcting various audio artifacts. It utilizes text-based control to allow users to specify desired improvements, making it versatile for both targeted and automatic restoration. The model is trained on a large dataset that pairs degraded audio tracks with their high-quality counterparts, using a flow-matching generative training approach. Results show that SonicMaster significantly enhances sound quality, as confirmed by both objective metrics and subjective listener preferences.'}, 'zh': {'title': 'SonicMaster：音乐音频质量的统一生成模型', 'desc': 'SonicMaster是一种统一的生成模型，旨在通过文本控制和流匹配生成训练范式来改善音乐音频质量。该模型能够处理多种音频伪影，如混响过度、失真和音调不平衡等，尤其适用于非专业环境下录制的音乐。SonicMaster通过自然语言指令进行有针对性的增强，或在自动模式下进行一般修复。实验结果表明，SonicMaster在所有伪影类别中显著提高了音质，且听众更喜欢其增强的输出。'}}}, {'id': 'https://huggingface.co/papers/2508.03178', 'title': 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following', 'url': 'https://huggingface.co/papers/2508.03178', 'abstract': 'A framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'b68fc5d4f6ed55e1', 'authors': ['Chenyang Wang', 'Liang Wen', 'Shousheng Jia', 'Xiangzheng Zhang', 'Liang Xu'], 'affiliations': ['CLUE', 'Harbin Institute of Technology', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.03178.jpg', 'data': {'categories': ['#optimization', '#rl', '#dataset', '#benchmark', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Точное следование инструкциям через глубокие рассуждения', 'desc': 'Статья представляет новый фреймворк для улучшения способности языковых моделей следовать инструкциям. Авторы используют энтропийно-сохраняющее обучение с учителем и токен-адаптивное обучение с подкреплением для развития навыков тщательных рассуждений. Метод включает предварительный просмотр и самопроверку для соблюдения сложных ограничений в инструкциях. Эксперименты показывают значительное улучшение производительности на различных бенчмарках, превосходя более крупные модели.'}, 'en': {'title': 'Enhancing Instruction Adherence in LLMs through Rigorous Reasoning', 'desc': 'This paper presents a new framework that enhances the ability of large language models (LLMs) to follow complex instructions by improving their reasoning processes. The authors identify that poor instruction adherence is often due to lazy reasoning, particularly during the initial thinking phase. To address this, they introduce an entropy-preserving supervised fine-tuning method and a token-wise entropy-adaptive reinforcement learning strategy, which together promote rigorous reasoning through self-checking and previewing. Their experiments show that this approach significantly boosts performance on instruction-following tasks, outperforming both larger open-source and closed-source models.'}, 'zh': {'title': '提升指令遵循能力的推理框架', 'desc': '本论文提出了一种新的框架，通过保持熵的监督微调和逐词熵自适应强化学习，来提高大型语言模型（LLMs）对指令的遵循能力。研究发现，懒惰推理是导致指令遵循不佳的主要原因，因此我们设计了一个综合框架，促进严格的推理过程，包括预览和自我检查。我们生成了具有复杂约束的指令，并通过过滤过程获得有效的提示，最终形成了三个不同的提示数据集。通过实验，我们的模型在遵循指令的基准测试中表现出显著的性能提升，尤其是我们的Light-IF-32B模型超越了许多更大的开源和闭源模型。'}}}, {'id': 'https://huggingface.co/papers/2508.01311', 'title': 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with\n  Learnable Advisor', 'url': 'https://huggingface.co/papers/2508.01311', 'abstract': 'A continual learning framework for 3D anomaly detection uses Kernel Attention mechanisms and parameter perturbation to handle multiple and emerging classes of point clouds.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-precision industrial products. However, existing methods are typically trained in a class-specific manner and also lack the capability of learning from emerging classes. In this study, we proposed a continual learning framework named Continual 3D Anomaly Detection (C3D-AD), which can not only learn generalized representations for multi-class point clouds but also handle new classes emerging over time.Specifically, in the feature extraction module, to extract generalized local features from diverse product types of different tasks efficiently, Kernel Attention with random feature Layer (KAL) is introduced, which normalizes the feature space. Then, to reconstruct data correctly and continually, an efficient Kernel Attention with learnable Advisor (KAA) mechanism is proposed, which learns the information from new categories while discarding redundant old information within both the encoder and decoder. Finally, to keep the representation consistency over tasks, a Reconstruction with Parameter Perturbation (RPP) module is proposed by designing a representation rehearsal loss function, which ensures that the model remembers previous category information and returns category-adaptive representation.Extensive experiments on three public datasets demonstrate the effectiveness of the proposed method, achieving an average performance of 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD, respectively.', 'score': 1, 'issue_id': 5230, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'af26fb044fbca284', 'authors': ['Haoquan Lu', 'Hanzhe Liang', 'Jie Zhang', 'Chenxi Hu', 'Jinbao Wang', 'Can Gao'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University', 'Faculty of Applied Sciences, Macao Polytechnic University', 'Guangdong Provincial Key Laboratory of Intelligent Information Processing', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology', 'School of Artificial Intelligence, Shenzhen University', 'Shenzhen Audencia Financial Technology Institute, Shenzhen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01311.jpg', 'data': {'categories': ['#training', '#3d', '#dataset', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Непрерывное обучение для обнаружения 3D-аномалий: от многоклассовости к новым классам', 'desc': 'Предложена система непрерывного обучения C3D-AD для обнаружения аномалий в 3D-облаках точек. Она использует механизмы ядерного внимания для извлечения обобщенных признаков из разных типов продуктов. Система способна эффективно обрабатывать как существующие, так и новые классы объектов. Эксперименты на трех наборах данных показали высокую эффективность предложенного метода.'}, 'en': {'title': 'Adapting to Anomalies: Continual Learning in 3D Detection', 'desc': 'This paper presents a continual learning framework called Continual 3D Anomaly Detection (C3D-AD) designed for detecting anomalies in 3D point clouds. It addresses the limitations of traditional methods that are class-specific and unable to adapt to new classes over time. The framework utilizes Kernel Attention mechanisms to efficiently extract generalized features and incorporates a parameter perturbation strategy to maintain representation consistency across tasks. Experimental results on multiple datasets show that C3D-AD significantly improves anomaly detection performance while adapting to emerging classes.'}, 'zh': {'title': '持续学习，智能检测3D异常', 'desc': '本研究提出了一种名为C3D-AD的持续学习框架，用于3D异常检测。该框架利用核注意力机制和参数扰动，能够处理多类点云数据并适应新出现的类别。通过引入随机特征层的核注意力，模型能够高效提取多样化产品类型的特征。同时，重建模块通过学习新类别的信息，确保模型在处理新任务时仍能保持对旧类别的记忆。'}}}, {'id': 'https://huggingface.co/papers/2508.00428', 'title': 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D\n  Generation', 'url': 'https://huggingface.co/papers/2508.00428', 'abstract': 'Sel3DCraft enhances text-to-3D generation through a dual-branch retrieval and generation system, multi-view hybrid scoring with MLLMs, and prompt-driven visual analytics, improving designer creativity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '7c82937efeb350cb', 'authors': ['Nan Xiang', 'Tianyi Liang', 'Haiwen Huang', 'Shiqi Jiang', 'Hao Huang', 'Yifei Huang', 'Liangyu Chen', 'Changbo Wang', 'Chenhui Li'], 'affiliations': ['East China Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00428.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Sel3DCraft: революция в визуальной инженерии промптов для 3D-генерации', 'desc': 'Sel3DCraft - это система визуальной инженерии промптов для генерации текста в 3D, которая преобразует неструктурированное исследование в управляемый визуальный процесс. Она использует двухветвевую структуру, сочетающую поиск и генерацию для исследования разнообразных кандидатов. Система применяет многоракурсный гибридный подход к оценке с использованием мультимодальных языковых моделей (MLLM) и инновационных метрик высокого уровня. Sel3DCraft также включает набор инструментов визуальной аналитики на основе промптов, позволяющий интуитивно выявлять и устранять дефекты.'}, 'en': {'title': 'Transforming Text-to-3D Generation for Enhanced Creativity', 'desc': 'Sel3DCraft is a novel system designed to improve text-to-3D (T23D) generation by integrating a dual-branch approach that combines retrieval and generation methods. This system addresses the challenges of 3D model creation by implementing multi-view hybrid scoring, which utilizes machine learning language models (MLLMs) to ensure consistency and quality in the generated outputs. Additionally, Sel3DCraft features a prompt-driven visual analytics tool that helps designers identify and refine defects in their 3D models more intuitively. Overall, extensive testing shows that Sel3DCraft significantly enhances the creative process for designers compared to existing T23D systems.'}, 'zh': {'title': '提升设计师创造力的三维生成系统', 'desc': 'Sel3DCraft 是一个增强文本到三维生成的系统，采用双分支检索和生成结构，提升设计师的创造力。该系统通过多视角混合评分和大规模语言模型（MLLMs）来评估三维模型的一致性，解决了传统方法中的盲目试错问题。它还引入了基于提示的视觉分析工具，帮助用户直观地识别和改进缺陷。经过广泛测试，Sel3DCraft 在支持设计师创造力方面表现优于其他文本到三维生成系统。'}}}, {'id': 'https://huggingface.co/papers/2507.23313', 'title': 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2507.23313', 'abstract': 'Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.', 'score': 1, 'issue_id': 5221, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'c584e9c932383ec6', 'authors': ['Alfio Ferrara', 'Sergio Picascia', 'Elisabetta Rocchetti'], 'affiliations': ['Department of Computer Science, Università degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2507.23313.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multimodal', '#interpretability', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Скрытое понимание искусства: как ИИ разделяет содержание и стиль', 'desc': 'Исследование показало, что трансформерные модели диффузии для генерации изображений по тексту демонстрируют различную степень разделения содержания и стиля в создаваемых произведениях искусства. Анализ проводился с использованием тепловых карт кросс-внимания, которые позволяют соотнести пиксели сгенерированных изображений с конкретными токенами промпта. Результаты выявили, что во многих случаях токены содержания влияют преимущественно на области, связанные с объектами, в то время как токены стиля воздействуют на фон и текстуры. Это указывает на то, что модели диффузии формируют некоторое внутреннее представление о различии между содержанием и стилем без явного обучения этому.'}, 'en': {'title': 'Decoding Art: Understanding Content and Style in AI-Generated Images', 'desc': 'This paper explores how transformer-based text-to-image diffusion models generate artworks by analyzing their internal representation of content and style. Using cross-attention heatmaps, the authors investigate how different prompt tokens influence specific regions of generated images, revealing a separation between content and style. The study finds that content tokens mainly affect object-related areas, while style tokens influence backgrounds and textures, indicating an emergent understanding of these concepts. This research enhances our knowledge of generative models and their ability to represent complex artistic ideas without direct supervision.'}, 'zh': {'title': '探索内容与风格的分离：扩散模型的艺术生成', 'desc': '本研究探讨了基于变换器的文本到图像扩散模型在生成艺术作品时如何编码内容和风格的概念。通过交叉注意力热图，我们能够将生成图像中的像素归因于特定的提示令牌，从而区分受内容描述和风格描述影响的图像区域。研究发现，扩散模型在不同艺术提示和风格请求下表现出不同程度的内容与风格分离。结果表明，内容令牌主要影响与物体相关的区域，而风格令牌则影响背景和纹理区域，显示出模型对内容与风格区分的理解。'}}}, {'id': 'https://huggingface.co/papers/2508.04440', 'title': 'StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion', 'url': 'https://huggingface.co/papers/2508.04440', 'abstract': 'ThinkingF, a data synthesis and training pipeline, enhances autoformalization by improving formal knowledge and informal-to-formal reasoning, achieving state-of-the-art results in formalization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.', 'score': 0, 'issue_id': 5230, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6d2a57f491626d3a', 'authors': ['Yutong Wu', 'Di Huang', 'Ruosi Wan', 'Yue Peng', 'Shijie Shang', 'Chenrui Cao', 'Lei Qi', 'Rui Zhang', 'Zidong Du', 'Jie Yan', 'Xing Hu'], 'affiliations': ['SKL of Processors, Institute of Computing Technology, CAS', 'StepFun Inc.', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04440.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#reasoning', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'ThinkingF: мост между неформальной и формальной математикой', 'desc': 'ThinkingF - это новый подход к автоформализации, улучшающий формальные знания и рассуждения от неформального к формальному. Метод включает создание двух наборов данных: один с примерами, богатыми формальными знаниями, другой с траекториями рассуждений от неформального к формальному. Применяется обучение с учителем (SFT) и обучение с подкреплением (RLVR) для объединения этих способностей. Результирующие модели достигают наилучших результатов в задачах формализации на бенчмарках FormalMATH-Lite и ProverBench.'}, 'en': {'title': 'ThinkingF: Bridging Natural Language and Formal Knowledge', 'desc': 'The paper presents ThinkingF, a novel data synthesis and training pipeline designed to enhance autoformalization, which is the process of converting natural-language mathematical statements into formal language. It identifies two critical skills necessary for effective autoformalization: mastery of formal-language knowledge and the ability to reason from informal to formal contexts. To improve these skills, ThinkingF constructs two specialized datasets and employs techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Value Regression (RLVR). The resulting models, particularly StepFun-Formalizer-32B, achieve state-of-the-art performance on formalization tasks, demonstrating significant advancements over previous models.'}, 'zh': {'title': '提升自动形式化的ThinkingF', 'desc': '本文介绍了ThinkingF，一个数据合成和训练管道，旨在提升自动形式化的能力。自动形式化的目标是将自然语言数学陈述转换为形式语言。研究发现，成功的自动形式化需要对形式语言领域知识的全面掌握和自然语言问题理解的推理能力。通过构建两个数据集并应用SFT和RLVR，ThinkingF显著提高了模型在形式化任务中的表现，特别是在FormalMATH-Lite和ProverBench上取得了领先的成绩。'}}}, {'id': 'https://huggingface.co/papers/2508.00109', 'title': 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality', 'url': 'https://huggingface.co/papers/2508.00109', 'abstract': 'FACTORY, a human-verified prompt set, evaluates the factuality of long-form responses from language models, revealing higher factual accuracy compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.', 'score': 0, 'issue_id': 5234, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '6dd83d8a170db99d', 'authors': ['Mingda Chen', 'Yang Li', 'Xilun Chen', 'Adina Williams', 'Gargi Ghosh', 'Scott Yih'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2508.00109.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#dataset', '#long_context', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'FACTORY: Новый стандарт оценки фактической точности языковых моделей', 'desc': 'FACTORY - это набор проверенных человеком промптов для оценки фактической точности длинных ответов языковых моделей. Он был разработан с использованием подхода model-in-the-loop и уточнен людьми. FACTORY включает сложные промпты, которые ориентированы на факты, имеют однозначные ответы и не допускают двусмысленности. Результаты показывают, что FACTORY является сложным бенчмарком: около 40% утверждений в ответах современных моделей не фактичны.'}, 'en': {'title': 'FACTORY: Elevating Factual Accuracy in Language Models', 'desc': 'The paper introduces FACTORY, a new benchmark for evaluating the factual accuracy of long-form responses generated by language models. Unlike existing datasets, FACTORY is human-verified, ensuring higher quality and reliability in assessing model outputs. The study reveals that state-of-the-art models struggle with factual accuracy, with around 40% of their claims being incorrect when evaluated with FACTORY. This highlights the importance of using robust, human-verified datasets to improve the reasoning capabilities of language models across diverse factual scenarios.'}, 'zh': {'title': 'FACTORY：提升语言模型的事实准确性', 'desc': 'FACTORY是一个经过人工验证的提示集，用于评估语言模型生成长文本响应的事实准确性。与现有数据集相比，FACTORY显示出更高的事实准确性，因为它采用了模型循环的方法，并经过人类的精细调整。通过对六种最先进的语言模型进行评估，我们发现大约40%的响应声明并不真实，而其他数据集的这一比例仅为10%。这表明FACTORY在评估模型的长尾事实推理能力方面具有更高的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2508.02324', 'title': 'Qwen-Image Technical Report', 'url': 'https://huggingface.co/papers/2508.02324', 'abstract': "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.", 'score': 79, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '4417ddebd50d6ca5', 'authors': ['Chenfei Wu', 'Jiahao Li', 'Jingren Zhou', 'Junyang Lin', 'Kaiyuan Gao', 'Kun Yan', 'Sheng-ming Yin', 'Shuai Bai', 'Xiao Xu', 'Yilei Chen', 'Yuxiang Chen', 'Zecheng Tang', 'Zekai Zhang', 'Zhengyi Wang', 'An Yang', 'Bowen Yu', 'Chen Cheng', 'Dayiheng Liu', 'Deqing Li', 'Hang Zhang', 'Hao Meng', 'Hu Wei', 'Jingyuan Ni', 'Kai Chen', 'Kuan Cao', 'Liang Peng', 'Lin Qu', 'Minggang Wu', 'Peng Wang', 'Shuting Yu', 'Tingkun Wen', 'Wensen Feng', 'Xiaoxiao Xu', 'Yi Wang', 'Yichang Zhang', 'Yongqiang Zhu', 'Yujia Wu', 'Yuxuan Cai', 'Zenan Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.02324.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#games', '#cv', '#training', '#multimodal', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Qwen-Image: новый уровень генерации и редактирования изображений с текстом', 'desc': 'Qwen-Image - это модель генерации изображений, которая достигает значительных успехов в рендеринге сложного текста и точном редактировании изображений. Модель использует комплексный конвейер данных, включающий сбор, фильтрацию, аннотацию и балансировку данных. Применяется стратегия прогрессивного обучения, начиная с простых задач и постепенно переходя к более сложным. Qwen-Image вводит улучшенную парадигму многозадачного обучения и механизм двойного кодирования для повышения согласованности при редактировании изображений.'}, 'en': {'title': 'Revolutionizing Image Generation and Editing with Qwen-Image', 'desc': "Qwen-Image is an advanced image generation model that enhances text rendering and image editing through a sophisticated data pipeline and a dual-encoding mechanism. It employs a comprehensive approach to data collection and training, utilizing a progressive strategy that improves the model's ability to handle complex text inputs, including both alphabetic and logographic languages. The model's multi-task training paradigm integrates various tasks to ensure consistency in image editing while maintaining high-quality outputs. Overall, Qwen-Image sets new standards in image generation and editing performance across multiple benchmarks."}, 'zh': {'title': 'Qwen-Image：图像生成与编辑的突破性进展', 'desc': 'Qwen-Image是一种图像生成模型，旨在通过全面的数据处理流程和渐进式训练策略，提升文本渲染和图像编辑的能力。该模型采用了双编码机制，能够有效地处理复杂的文本输入，并在字母语言和表意文字（如中文）上都表现出色。通过多任务训练，Qwen-Image在文本到图像和图像重建任务中实现了更高的一致性，确保了语义和视觉的保真度。最终，Qwen-Image在多个基准测试中展现了其在图像生成和编辑方面的领先性能。'}}}, {'id': 'https://huggingface.co/papers/2508.01959', 'title': 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension', 'url': 'https://huggingface.co/papers/2508.01959', 'abstract': "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.", 'score': 37, 'issue_id': 5178, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '063b1bca074ff85f', 'authors': ['Junjie Wu', 'Jiangnan Li', 'Yuqing Li', 'Lemao Liu', 'Liyan Xu', 'Jiwei Li', 'Dit-Yan Yeung', 'Jie Zhou', 'Mo Yu'], 'affiliations': ['HKUST', 'IIE-CAS', 'WeChat AI, Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01959.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#rag', '#long_context', '#optimization', '#benchmark', '#training'], 'emoji': '🔍', 'ru': {'title': 'Контекст имеет значение: ситуативные эмбеддинги для точного информационного поиска', 'desc': 'Статья представляет новый подход к улучшению производительности информационного поиска путем контекстуализации коротких текстовых фрагментов в более широком контексте. Авторы предлагают новую парадигму обучения и модели ситуативных эмбеддингов (SitEmb), которые превосходят современные модели, имея меньше параметров. Метод особенно эффективен для задач, требующих понимания локального контекста в рамках более широкого документа. Результаты показывают значительное улучшение производительности на специально созданном наборе данных для оценки ситуативного поиска.'}, 'en': {'title': 'Enhancing Retrieval with Contextual Short Text Chunks', 'desc': 'This paper introduces a new training method and a model called situated embedding models (SitEmb) that improve the retrieval of information from text. By conditioning short text chunks on a broader context, the model captures the meaning of each chunk more effectively. The authors demonstrate that existing models struggle with this task due to their limitations in encoding context. Their SitEmb model outperforms leading models with significantly fewer parameters, showing better retrieval performance across various languages and applications.'}, 'zh': {'title': '情境嵌入，提升检索性能！', 'desc': '本文提出了一种新的训练范式和情境嵌入模型（SitEmb），通过将短文本片段与更广泛的上下文窗口相结合，提升了检索性能。传统的检索方法往往将长文档拆分为小块，但这些小块的理解需要上下文信息。我们的方法通过在更广泛的上下文中对短片段进行编码，来增强检索效果。实验结果表明，SitEmb模型在参数更少的情况下，显著超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.02276', 'title': 'CellForge: Agentic Design of Virtual Cell Models', 'url': 'https://huggingface.co/papers/2508.02276', 'abstract': "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.", 'score': 27, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '04238c0793ca08e5', 'authors': ['Xiangru Tang', 'Zhuoyun Yu', 'Jiapeng Chen', 'Yan Cui', 'Daniel Shao', 'Weixu Wang', 'Fang Wu', 'Yuchen Zhuang', 'Wenqi Shi', 'Zhi Huang', 'Arman Cohan', 'Xihong Lin', 'Fabian Theis', 'Smita Krishnaswamy', 'Mark Gerstein'], 'affiliations': ['Google DeepMind', 'Harvard University', 'Helmholtz Zentrum Munchen', 'Stanford University', 'University of Pennsylvania', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02276.jpg', 'data': {'categories': ['#dataset', '#training', '#science', '#open_source', '#architecture', '#agents', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'CellForge: ИИ-агенты создают виртуальные клетки из одноклеточных данных', 'desc': 'CellForge - это агентная система, использующая мультиагентный фреймворк для преобразования необработанных одноклеточных мультиомных данных в оптимизированные вычислительные модели виртуальных клеток. Система состоит из трех основных модулей: анализа задач, разработки методов и выполнения экспериментов. CellForge превосходит современные методы в прогнозировании одноклеточных возмущений на шести разнообразных наборах данных. Это демонстрирует, как итеративное взаимодействие между агентами с различными перспективами обеспечивает лучшие решения, чем прямой подход к задаче моделирования.'}, 'en': {'title': 'Transforming Biology with Collaborative AI Models', 'desc': 'CellForge is an innovative system that uses a multi-agent framework to create computational models for virtual cells from raw single-cell multi-omics data. It addresses the complexities of biological systems by employing specialized agents that collaborate to analyze tasks, design methods, and execute experiments. This approach allows CellForge to generate optimized model architectures and executable code, significantly improving predictions for single-cell perturbations. By integrating diverse perspectives from its agents, CellForge consistently outperforms existing state-of-the-art methods in the field.'}, 'zh': {'title': 'CellForge：优化虚拟细胞建模的智能系统', 'desc': 'CellForge 是一个基于多智能体框架的系统，能够将原始的单细胞多组学数据转化为优化的虚拟细胞计算模型。该系统通过分析任务和数据集，自动生成可执行的代码，显著提高了单细胞扰动预测的准确性。CellForge 的设计模块由不同专业的智能体协作开发建模策略，确保了模型的优化。实验结果表明，CellForge 在多种数据集上均优于现有的最先进方法，展示了多智能体协作的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.01059', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report', 'url': 'https://huggingface.co/papers/2508.01059', 'abstract': 'Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.', 'score': 21, 'issue_id': 5176, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '897c594bfa5630a6', 'authors': ['Sajana Weerawardhena', 'Paul Kassianik', 'Blaine Nelson', 'Baturay Saglam', 'Anu Vellore', 'Aman Priyanshu', 'Supriti Vijay', 'Massimo Aufiero', 'Arthur Goldblatt', 'Fraser Burch', 'Ed Li', 'Jianliang He', 'Dhruv Kedia', 'Kojin Oshiba', 'Zhouran Yang', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Carnegie Mellon University', 'Cisco Systems Inc.', 'Foundation AI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01059.jpg', 'data': {'categories': ['#dataset', '#alignment', '#security', '#training', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Интеллектуальный помощник по кибербезопасности на базе ИИ', 'desc': 'Foundation-Sec-8B-Instruct - это языковая модель, специализирующаяся на кибербезопасности и предназначенная для диалогового взаимодействия. Модель сочетает в себе специализированные знания в области кибербезопасности с возможностями следования инструкциям и ведения разговора. Оценки показывают, что Foundation-Sec-8B-Instruct превосходит другие модели в задачах кибербезопасности, сохраняя при этом способность следовать инструкциям. Авторы предполагают, что эта модель станет незаменимым помощником в повседневной работе специалистов по кибербезопасности.'}, 'en': {'title': 'Empowering Cybersecurity with Conversational AI', 'desc': 'Foundation-Sec-8B-Instruct is a large language model (LLM) specifically designed for cybersecurity applications, enhancing chat-style interactions and instruction-following capabilities. It builds upon the previous Foundation-Sec-8B model, which was tailored for fine-tuning on cybersecurity tasks but lacked conversational features. This new model integrates domain-specific knowledge with the ability to follow instructions and engage in dialogue, resulting in high-quality responses relevant to cybersecurity. Evaluations demonstrate that it surpasses other models like Llama 3.1-8B-Instruct in cybersecurity tasks while maintaining competitive performance in instruction-following.'}, 'zh': {'title': '网络安全对话的智能助手', 'desc': 'Foundation-Sec-8B-Instruct 是一个专注于网络安全的语言模型，旨在进行对话式交互和遵循指令。该模型在网络安全任务上表现优于其他模型，同时在遵循指令的能力上与之相匹配。它结合了特定领域的知识和人类偏好的对齐，能够生成高质量和相关的响应。我们希望 Foundation-Sec-8B-Instruct 能成为网络安全专业人员日常工作中不可或缺的助手。'}}}, {'id': 'https://huggingface.co/papers/2508.02150', 'title': "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following", 'url': 'https://huggingface.co/papers/2508.02150', 'abstract': "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if.", 'score': 20, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '37d1b608fde6bd5f', 'authors': ['Qingyu Ren', 'Qianyu He', 'Bowei Zhang', 'Jie Zeng', 'Jiaqing Liang', 'Yanghua Xiao', 'Weikang Zhou', 'Zeye Sun', 'Fei Yu'], 'affiliations': ['Ant Group', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02150.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ для лучшего выполнения инструкций', 'desc': 'Исследователи предложили новый подход к улучшению способности моделей машинного обучения следовать инструкциям без внешнего контроля. Они разработали систему самообучения с подкреплением, которая использует внутренние сигналы самой модели. Эксперименты показали, что этот метод значительно улучшает следование инструкциям, сохраняя при этом способности к рассуждению. Предложенный подход является масштабируемым и экономически эффективным решением для совершенствования моделей рассуждений.'}, 'en': {'title': 'Enhancing Instruction Following in Reasoning Models with Self-Supervised RL', 'desc': "This paper presents a self-supervised reinforcement learning (RL) framework designed to improve how reasoning models follow instructions. Traditional methods often depend on external models, which can be costly and limit accessibility. The proposed framework utilizes the internal signals of reasoning models to enhance their instruction-following abilities without needing external supervision. Experimental results show that this approach not only boosts instruction following but also preserves the models' reasoning performance, making it a scalable and cost-effective solution."}, 'zh': {'title': '自监督强化学习提升推理模型的指令遵循能力', 'desc': '本论文提出了一种自监督强化学习框架，旨在提升推理模型的指令遵循能力，而无需外部监督。传统方法通常依赖于更强大的外部模型，这导致了方法上的瓶颈和实际应用中的限制，如成本增加和可及性问题。我们的方法利用推理模型自身的内部信号来改善指令遵循能力，同时保持推理性能。实验结果表明，该框架在提升指令遵循能力的同时，提供了一种可扩展且具有成本效益的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.02317', 'title': 'VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo', 'url': 'https://huggingface.co/papers/2508.02317', 'abstract': 'A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.', 'score': 8, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '2e96724e612a0eb6', 'authors': ['Qianli Ma', 'Yaowei Zheng', 'Zhelun Shi', 'Zhongkai Zhao', 'Bin Jia', 'Ziyue Huang', 'Zhiqi Lin', 'Youjie Li', 'Jiacheng Yang', 'Yanghua Peng', 'Zhi Zhang', 'Xin Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.02317.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Ускоряем обучение омни-модальных LLM с помощью модульной архитектуры', 'desc': 'Эта статья представляет новую модульную систему обучения для омни-модальных языковых моделей (LLM). Система предлагает эффективное 3D-распараллеливание и гибкую конфигурацию, что ускоряет разработку омни-модальных LLM. Ключевые особенности включают разделение коммуникации и вычислений, а также простую интеграцию новых модальностей. Результаты показывают высокую эффективность и масштабируемость при обучении крупных омни-модальных моделей.'}, 'en': {'title': 'Accelerating Omni-Modal LLMs with Modular Training', 'desc': 'This paper introduces \textit{veomni}, a modular training framework designed to enhance the development of omni-modal large language models (LLMs). It addresses the challenges of training these models by separating model architecture from parallel processing logic, which allows for efficient 3D parallelism. The framework supports easy integration of new modalities, reducing the need for extensive code modifications. With \textit{veomni}, a mixture-of-experts model with 30 billion parameters can achieve high throughput and scalability, demonstrating its effectiveness in training large omni-modal LLMs.'}, 'zh': {'title': '模块化训练框架，提升全模态LLM开发效率', 'desc': '这篇论文介绍了一种名为\\veomni的模块化训练框架，旨在加速全模态大语言模型（LLM）的开发。该框架通过高效的三维并行处理和灵活的配置，解决了训练全模态LLM时面临的挑战。\\veomni将模型定义与并行逻辑解耦，使得在多种模态上进行大规模训练变得更加高效。使用\\veomni，研究人员能够以极高的速度训练具有30亿参数的全模态专家模型，展示了其在训练大型全模态LLM方面的优越效率和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2507.17520', 'title': 'InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation', 'url': 'https://huggingface.co/papers/2507.17520', 'abstract': "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.", 'score': 8, 'issue_id': 5176, 'pub_date': '2025-07-23', 'pub_date_card': {'ru': '23 июля', 'en': 'July 23', 'zh': '7月23日'}, 'hash': '13d868fd7ad8ea42', 'authors': ['Shuai Yang', 'Hao Li', 'Yilun Chen', 'Bin Wang', 'Yang Tian', 'Tai Wang', 'Hanqing Wang', 'Feng Zhao', 'Yiyi Liao', 'Jiangmiao Pang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.17520.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#reasoning', '#multimodal', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'InstructVLA: Мост между интуитивным управлением роботами и эффективным обучением', 'desc': 'InstructVLA - это модель обработки зрения, языка и действий, которая улучшает манипуляционные способности роботов, сохраняя при этом рассуждения на основе зрения и языка. Модель использует мультимодальное обучение и адаптацию на основе смеси экспертов. InstructVLA демонстрирует значительное улучшение производительности по сравнению с существующими моделями на различных задачах, включая задачи в симулированной и реальной среде. Эта модель открывает возможности для более интуитивного и управляемого взаимодействия человека с роботом при эффективном обучении политикам.'}, 'en': {'title': 'Bridging Vision, Language, and Action for Smarter Robots', 'desc': 'InstructVLA is a new model that combines vision, language, and action to improve how robots perform tasks. It uses a special training method called Vision-Language-Action Instruction Tuning (VLA-IT) to enhance both understanding and action capabilities without losing previous knowledge. This model outperforms existing systems in various tasks, showing significant improvements in manipulation and reasoning. By integrating multimodal training and expert adaptation, InstructVLA enables better human-robot interaction and efficient learning of tasks.'}, 'zh': {'title': '提升机器人操作与推理的完美结合', 'desc': 'InstructVLA是一种端到端的视觉-语言-动作模型，旨在提高机器人操作性能，同时保持视觉-语言推理能力。该模型通过多模态训练和专家混合适应，解决了现有模型在任务特定数据上的局限性和灾难性遗忘问题。InstructVLA引入了一种新的训练范式，称为视觉-语言-动作指令调优（VLA-IT），在标准视觉-语言模型数据集和一个包含65万样本的VLA-IT数据集上共同优化文本推理和动作生成。实验结果表明，InstructVLA在多个任务上表现优异，展示了其在高效政策学习和人机交互中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.01548', 'title': 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2508.01548', 'abstract': "A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce a dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining a similarly high pruning rate. Our work paves a new way for building more powerful and efficient LVLMs.", 'score': 7, 'issue_id': 5180, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '7611e04f9fa72eb7', 'authors': ['Quan-Sheng Zeng', 'Yunheng Li', 'Qilong Wang', 'Peng-Tao Jiang', 'Zuxuan Wu', 'Ming-Ming Cheng', 'Qibin Hou'], 'affiliations': ['Shanghai Innovation Institute', 'Tianjin University', 'VCIP, CS, Nankai University', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2508.01548.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка для эффективных визуально-языковых моделей', 'desc': 'GlimpsePrune - это динамическая система обрезки, которая повышает эффективность крупных визуально-языковых моделей (LVLM). Она адаптивно удаляет нерелевантные визуальные токены без ухудшения производительности модели. GlimpsePrune обрезает 92.6% визуальных токенов, сохраняя при этом базовую производительность на задачах визуальных вопросов и ответов (VQA). Улучшенная версия GlimpsePrune+ достигает 110% базовой производительности при сохранении высокого уровня обрезки.'}, 'en': {'title': 'Dynamic Pruning for Efficient Vision-Language Models', 'desc': 'GlimpsePrune is a dynamic pruning framework designed to enhance the efficiency of Large Vision-Language Models (LVLMs) by selectively removing irrelevant visual tokens. Unlike traditional methods that use fixed compression ratios, GlimpsePrune adapts to the complexity of different scenes, ensuring that important visual information is preserved. This framework prunes up to 92.6% of visual tokens while maintaining baseline performance on visual question answering tasks. Additionally, an improved version, GlimpsePrune+, not only retains high pruning rates but also boosts performance beyond the baseline, demonstrating a significant advancement in model efficiency.'}, 'zh': {'title': '动态剪枝，提升视觉语言模型效率', 'desc': 'GlimpsePrune是一个动态剪枝框架，旨在提高大型视觉语言模型的效率。它通过自适应地去除不相关的视觉标记，避免了固定压缩比带来的信息丢失问题。该方法在一次前向传播中进行剪枝，能够保留92.6%的视觉标记，同时在自由形式的视觉问答任务中保持基线性能。我们的研究为构建更强大和高效的视觉语言模型开辟了新路径。'}}}, {'id': 'https://huggingface.co/papers/2508.02137', 'title': 'Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind', 'url': 'https://huggingface.co/papers/2508.02137', 'abstract': 'AuroBind is a scalable virtual screening framework that fine-tunes atomic-level structural models to predict ligand-bound structures and binding fitness, achieving high hit rates in prospective screens across disease-relevant targets.  \t\t\t\t\tAI-generated summary \t\t\t\t Most human proteins remain undrugged, over 96% of human proteins remain unexploited by approved therapeutics. While structure-based virtual screening promises to expand the druggable proteome, existing methods lack atomic-level precision and fail to predict binding fitness, limiting translational impact. We present AuroBind, a scalable virtual screening framework that fine-tunes a custom atomic-level structural model on million-scale chemogenomic data. AuroBind integrates direct preference optimization, self-distillation from high-confidence complexes, and a teacher-student acceleration strategy to jointly predict ligand-bound structures and binding fitness. The proposed models outperform state-of-the-art models on structural and functional benchmarks while enabling 100,000-fold faster screening across ultra-large compound libraries. In a prospective screen across ten disease-relevant targets, AuroBind achieved experimental hit rates of 7-69%, with top compounds reaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and GPR160, AuroBind identified both agonists and antagonists with success rates of 16-30%, and functional assays confirmed GPR160 modulation in liver and prostate cancer models. AuroBind offers a generalizable framework for structure-function learning and high-throughput molecular screening, bridging the gap between structure prediction and therapeutic discovery.', 'score': 6, 'issue_id': 5190, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'f767500373dc2f12', 'authors': ['Zhongyue Zhang', 'Jiahua Rao', 'Jie Zhong', 'Weiqiang Bai', 'Dongxue Wang', 'Shaobo Ning', 'Lifeng Qiao', 'Sheng Xu', 'Runze Ma', 'Will Hua', 'Jack Xiaoyu Chen', 'Odin Zhang', 'Wei Lu', 'Hanyi Feng', 'He Yang', 'Xinchao Shi', 'Rui Li', 'Wanli Ouyang', 'Xinzhu Ma', 'Jiahao Wang', 'Jixian Zhang', 'Jia Duan', 'Siqi Sun', 'Jian Zhang', 'Shuangjia Zheng'], 'affiliations': ['Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China', 'Institute for Medical Engineering & Science, Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Key Laboratory of Protection, Development and Utilization of Medicinal Resources in Liupanshan Area, Ministry of Education, Peptide & Protein Drug Research Center, School of Pharmacy, Ningxia Medical University, Ningxia, China', 'Lingang Laboratory, Shanghai, China', 'Medicinal Chemistry and Bioinformatics Center, School of Medicine, Shanghai Jiao Tong University, Shanghai, China', 'Research Institute of Intelligent Complex Systems, Fudan University, Shanghai, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangdong, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Zhongshan Institute for Drug Discovery, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02137.jpg', 'data': {'categories': ['#healthcare', '#data', '#optimization', '#science', '#dataset', '#benchmark'], 'emoji': '🧬', 'ru': {'title': 'AuroBind: прорыв в виртуальном скрининге лекарств на атомарном уровне', 'desc': 'AuroBind - это масштабируемая система виртуального скрининга, которая оптимизирует структурные модели на атомарном уровне для предсказания комплексов лиганд-белок и оценки аффинности связывания. Система использует обучение на миллионах хемогеномных данных, включая прямую оптимизацию предпочтений и самодистилляцию. AuroBind превосходит существующие методы по точности и скорости, позволяя проводить скрининг сверхбольших библиотек соединений в 100 000 раз быстрее. В проспективных экспериментах система продемонстрировала высокую эффективность для различных мишеней, включая орфанные GPCR-рецепторы.'}, 'en': {'title': 'AuroBind: Revolutionizing Drug Discovery with Precision and Speed', 'desc': 'AuroBind is a new framework designed for virtual screening in drug discovery, focusing on predicting how small molecules (ligands) bind to proteins. It fine-tunes detailed structural models using large datasets to improve the accuracy of binding predictions and identify potential drug candidates. By employing techniques like direct preference optimization and self-distillation, AuroBind significantly enhances screening speed and hit rates across various disease targets. This approach not only outperforms existing methods but also helps in discovering new therapeutic options for previously undrugged human proteins.'}, 'zh': {'title': 'AuroBind：高效的虚拟筛选新框架', 'desc': 'AuroBind 是一个可扩展的虚拟筛选框架，能够微调原子级结构模型，以预测配体结合结构和结合适应性。该方法通过优化直接偏好、自我蒸馏和教师-学生加速策略，联合预测配体结合结构和结合适应性。AuroBind 在结构和功能基准测试中超越了现有的最先进模型，并在超大化合物库中实现了100,000倍的筛选速度提升。通过对十个与疾病相关的靶点进行前瞻性筛选，AuroBind 实现了7-69%的实验命中率，显示出其在药物发现中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.01691', 'title': 'Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and\n  Regional Languages Around the Globe', 'url': 'https://huggingface.co/papers/2508.01691', 'abstract': 'Voxlect is a benchmark for evaluating speech foundation models on dialect classification and downstream applications across multiple languages and dialects.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.', 'score': 6, 'issue_id': 5185, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'bdb8f5a75702298d', 'authors': ['Tiantian Feng', 'Kevin Huang', 'Anfeng Xu', 'Xuan Shi', 'Thanathai Lertpetchpun', 'Jihwan Lee', 'Yoonjeong Lee', 'Dani Byrd', 'Shrikanth Narayanan'], 'affiliations': ['University of Southern California, Los Angeles, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01691.jpg', 'data': {'categories': ['#multilingual', '#science', '#audio', '#benchmark', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Voxlect: Универсальный инструмент для анализа диалектов в речевых моделях', 'desc': 'Voxlect - это новый эталонный тест для оценки речевых моделей в задачах классификации диалектов и связанных приложениях для множества языков и диалектов. Исследование использует более 2 миллионов обучающих высказываний из 30 общедоступных речевых корпусов с диалектной информацией. Авторы оценивают производительность нескольких широко используемых речевых моделей в классификации диалектов, а также их устойчивость в условиях шума. Voxlect демонстрирует применимость для обогащения существующих наборов данных для распознавания речи диалектной информацией и оценки систем генерации речи.'}, 'en': {'title': 'Voxlect: Advancing Dialect Classification in Speech Models', 'desc': 'Voxlect is a benchmark designed to evaluate speech foundation models specifically for dialect classification across various languages. It utilizes over 2 million training utterances from 30 speech corpora that include dialectal information, allowing for comprehensive assessments of model performance. The study not only benchmarks dialect classification but also explores downstream applications, such as enhancing automatic speech recognition (ASR) datasets with dialectal data. Additionally, it evaluates the robustness of these models in noisy environments and provides insights into geographic continuity in dialectal variations.'}, 'zh': {'title': 'Voxlect：全球方言分类的基准测试', 'desc': 'Voxlect是一个用于评估语音基础模型在方言分类和下游应用中的基准测试。该研究涵盖了多种语言和方言，包括英语、阿拉伯语、普通话、粤语等，使用了超过200万个带有方言信息的语音样本。我们评估了多种广泛使用的语音基础模型在方言分类中的表现，并分析了模型在噪声条件下的鲁棒性。Voxlect不仅用于方言分类的基准测试，还可以增强现有的语音识别数据集，帮助分析不同方言的ASR性能。'}}}, {'id': 'https://huggingface.co/papers/2508.01151', 'title': 'Personalized Safety Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2508.01151', 'abstract': "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at https://torpedo2648.github.io/PSAlign/.", 'score': 6, 'issue_id': 5176, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '22d777cd71f123c6', 'authors': ['Yu Lei', 'Jinbin Bai', 'Qingyu Shi', 'Aosong Feng', 'Kaidong Yu'], 'affiliations': ['National University of Singapore', 'Peking University', 'TeleAI, China Telecom', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01151.jpg', 'data': {'categories': ['#cv', '#dataset', '#alignment', '#diffusion', '#open_source', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Персонализированная безопасность в генеративных моделях изображений', 'desc': 'Предложена новая система Personalized Safety Alignment (PSA) для настройки генеративных моделей изображений под индивидуальные предпочтения безопасности пользователей. PSA интегрирует персонализированные профили пользователей в процесс диффузии, адаптируя поведение модели к индивидуальным требованиям безопасности. Авторы представили новый датасет Sage, capturing пользовательские предпочтения безопасности. Эксперименты показали, что PSA превосходит существующие методы в подавлении вредного контента и лучше соответствует ограничениям пользователей.'}, 'en': {'title': 'Personalized Safety for Safer AI-Generated Images', 'desc': 'This paper presents a Personalized Safety Alignment (PSA) framework that enhances text-to-image diffusion models by incorporating individual user profiles. Current models apply a one-size-fits-all approach to safety, which does not consider the unique safety preferences shaped by personal factors. The PSA framework allows for user-specific adjustments in the generative process, ensuring that the generated images align with individual safety standards while maintaining high image quality. The authors also introduce a new dataset, Sage, to effectively capture and integrate these personalized safety preferences, demonstrating improved performance in harmful content suppression compared to existing methods.'}, 'zh': {'title': '个性化安全对齐：让生成内容更符合你的安全偏好', 'desc': '这篇论文提出了一种个性化安全对齐框架（PSA），旨在将用户特定的个人资料整合到文本到图像的扩散模型中，以更好地符合个体的安全偏好。当前的安全机制通常采用统一标准，无法考虑用户的多样化安全边界，如年龄、心理健康和个人信仰等因素。PSA通过在扩散过程中整合个性化用户资料，调整模型行为以匹配个体安全偏好，同时保持图像质量。实验结果表明，PSA在有害内容抑制和生成内容与用户约束的对齐方面优于现有方法，取得了更高的胜率和通过率。'}}}, {'id': 'https://huggingface.co/papers/2508.02271', 'title': 'Dynaword: From One-shot to Continuously Developed Datasets', 'url': 'https://huggingface.co/papers/2508.02271', 'abstract': 'A framework called Dynaword and its implementation Danish Dynaword enable community-driven, open, and continuously updated large-scale natural language datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.', 'score': 5, 'issue_id': 5193, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'efe4e9f51385893f', 'authors': ['Kenneth Enevoldsen', 'Kristian Nørgaard Jensen', 'Jan Kostkan', 'Balázs Szabó', 'Márton Kardos', 'Kirten Vad', 'Andrea Blasi Núñez', 'Gianluca Barmina', 'Jacob Nielsen', 'Rasmus Larsen', 'Peter Vahlstrup', 'Per Møldrup Dalum', 'Desmond Elliott', 'Lukas Galke', 'Peter Schneider-Kamp', 'Kristoffer Nielbo'], 'affiliations': ['Aarhus University', 'The Alexandra Institute', 'University of Copenhagen', 'University of Southern Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2508.02271.jpg', 'data': {'categories': ['#dataset', '#survey', '#open_source', '#data'], 'emoji': '🌱', 'ru': {'title': 'Открытые и развивающиеся датасеты для NLP силами сообщества', 'desc': 'Dynaword - это фреймворк для создания больших открытых наборов данных на естественном языке с возможностью постоянного обновления сообществом. Danish Dynaword - это конкретная реализация этого подхода, содержащая в 4 раза больше токенов, чем аналоги, и имеющая открытую лицензию. Фреймворк решает проблемы ограничений лицензирования, статичности датасетов и закрытости процессов контроля качества. Он включает легковесные тесты для проверки форматирования, качества и документации данных, обеспечивая устойчивую основу для вклада сообщества и эволюции датасета.'}, 'en': {'title': 'Empowering Community-Driven Natural Language Datasets', 'desc': 'The paper presents Dynaword, a framework designed to create large-scale natural language datasets that are open and continuously updated through community involvement. It addresses three main challenges in current dataset practices: licensing issues, static releases, and limited quality assurance. The implementation, Danish Dynaword, showcases this framework by providing a dataset with significantly more tokens than similar datasets, all under open licenses. It also includes mechanisms for community contributions and quality checks, promoting a sustainable model for dataset development and maintenance.'}, 'zh': {'title': '构建开放、可持续的大规模语言数据集', 'desc': 'Dynaword是一个框架，旨在创建可持续更新的大规模自然语言数据集。它解决了当前数据集面临的三个主要挑战，包括模糊的许可限制、静态数据集发布和质量保证过程的局限性。Danish Dynaword是该框架的具体实现，展示了其潜力，并包含了比类似发布多四倍的标记。该项目通过轻量级测试确保数据格式、质量和文档，促进了社区的持续贡献和数据集的演变。'}}}, {'id': 'https://huggingface.co/papers/2508.02558', 'title': 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction', 'url': 'https://huggingface.co/papers/2508.02558', 'abstract': 'Sparse-dLLM improves the efficiency of diffusion large language models by implementing dynamic cache eviction and sparse attention, enhancing throughput without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10times higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.', 'score': 4, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'abb2417fa4d42a82', 'authors': ['Yuerong Song', 'Xiaoran Liu', 'Ruixiao Li', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.02558.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#inference', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без потери качества', 'desc': 'Sparse-dLLM - это новый фреймворк для повышения эффективности диффузионных больших языковых моделей (dLLM). Он использует динамическое удаление кэша и разреженное внимание, что позволяет значительно увеличить пропускную способность без ущерба для производительности. Анализ паттернов внимания в dLLM показал устойчивую межслойную разреженность, где ключевые токены остаются важными на протяжении всего процесса декодирования. Эксперименты продемонстрировали, что Sparse-dLLM обеспечивает до 10 раз более высокую пропускную способность по сравнению с обычными dLLM при сопоставимой производительности и аналогичных затратах пиковой памяти.'}, 'en': {'title': 'Boosting Efficiency in Language Models with Sparse-dLLM', 'desc': 'Sparse-dLLM is a novel framework designed to enhance the efficiency of diffusion large language models (dLLMs) by utilizing dynamic cache eviction and sparse attention mechanisms. It addresses the high computational complexity and memory demands of traditional dLLMs during inference by selectively retaining important tokens while evicting less relevant ones. This approach leverages the consistent saliency of tokens across decoding steps, allowing for improved throughput without sacrificing performance. Experimental results show that Sparse-dLLM can achieve up to 10 times higher throughput compared to standard dLLMs, while maintaining similar performance levels and memory usage.'}, 'zh': {'title': 'Sparse-dLLM：提升扩散大语言模型效率的创新方案', 'desc': 'Sparse-dLLM通过动态缓存驱逐和稀疏注意力机制，提高了扩散大语言模型的效率。传统的缓存技术虽然加速了解码，但占用了大量内存，限制了长上下文的应用。我们的研究发现，扩散大语言模型中的注意力模式存在跨层稀疏性，关键的token在解码过程中始终保持重要性。Sparse-dLLM是首个不需要训练的框架，通过延迟双向稀疏缓存，动态保留重要token并驱逐不重要的前缀/后缀条目，从而实现了更高的解码吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2508.01415', 'title': 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems', 'url': 'https://huggingface.co/papers/2508.01415', 'abstract': 'RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.', 'score': 4, 'issue_id': 5177, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'ea0bbd88dcba8948', 'authors': ['Mingcong Lei', 'Honghao Cai', 'Zezhou Cui', 'Liangchen Tan', 'Junkun Hong', 'Gehan Hu', 'Shuangyu Zhu', 'Yimou Wu', 'Shaohan Jiang', 'Ge Wang', 'Zhen Li', 'Shuguang Cui', 'Yiming Zhao', 'Yatong Han'], 'affiliations': ['FNii-Shenzhen', 'Harbin Engineering University', 'Harbin Institute of Technology, Shenzhen', 'Infused Synapse AI', 'SSE', 'The Chinese University of Hong Kong, Shengzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.01415.jpg', 'data': {'categories': ['#agents', '#optimization', '#open_source', '#training', '#agi', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboMemory: Мозгоподобная память для непрерывного обучения роботов', 'desc': 'RoboMemory - это мультимодульная система памяти для непрерывного обучения роботов, вдохновленная принципами работы мозга. Она включает четыре основных модуля, имитирующих функции различных отделов мозга: препроцессор информации, систему долговременной памяти, модуль замкнутого планирования и исполнитель низкого уровня. Система показала значительное улучшение производительности по сравнению с существующими решениями на бенчмарке EmbodiedBench. RoboMemory решает проблемы высокой латентности и масштабируемости, что делает ее перспективной основой для интеграции мультимодальных систем памяти в физических роботах.'}, 'en': {'title': 'RoboMemory: Revolutionizing Lifelong Learning in Robots', 'desc': 'RoboMemory is a new framework designed to help robots learn continuously over time, inspired by how the human brain works. It uses four main components that mimic brain functions to improve memory and planning in robots, allowing them to handle complex tasks better. The framework addresses issues like slow memory access and the need for robots to remember different types of information effectively. Tests show that RoboMemory significantly outperforms existing systems in real-world scenarios, making it a promising advancement in robotic learning.'}, 'zh': {'title': 'RoboMemory：提升机器人终身学习的多记忆框架', 'desc': 'RoboMemory是一个受大脑启发的多记忆框架，旨在提高物理机器人在终身学习中的表现。它结合了认知神经科学的原理，解决了现实环境中的关键挑战，如持续学习和任务相关性捕捉。该框架包含四个核心模块，分别模拟大脑的不同部分，以实现长期规划和累积学习。通过在复杂记忆框架中并行更新和检索，RoboMemory显著提高了推理速度，并在实际任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.01408', 'title': 'Artificial Intelligence and Misinformation in Art: Can Vision Language\n  Models Judge the Hand or the Machine Behind the Canvas?', 'url': 'https://huggingface.co/papers/2508.01408', 'abstract': 'State-of-the-art vision language models struggle with accurately attributing artists and distinguishing AI-generated images, highlighting the need for improvement to prevent misinformation.  \t\t\t\t\tAI-generated summary \t\t\t\t The attribution of artworks in general and of paintings in particular has always been an issue in art. The advent of powerful artificial intelligence models that can generate and analyze images creates new challenges for painting attribution. On the one hand, AI models can create images that mimic the style of a painter, which can be incorrectly attributed, for example, by other AI models. On the other hand, AI models may not be able to correctly identify the artist for real paintings, inducing users to incorrectly attribute paintings. In this paper, both problems are experimentally studied using state-of-the-art AI models for image generation and analysis on a large dataset with close to 40,000 paintings from 128 artists. The results show that vision language models have limited capabilities to: 1) perform canvas attribution and 2) to identify AI generated images. As users increasingly rely on queries to AI models to get information, these results show the need to improve the capabilities of VLMs to reliably perform artist attribution and detection of AI generated images to prevent the spread of incorrect information.', 'score': 4, 'issue_id': 5185, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '31ac5fdf5a455427', 'authors': ['Tarian Fu', 'Javier Conde', 'Gonzalo Martínez', 'Pedro Reviriego', 'Elena Merino-Gómez', 'Fernando Moral'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Universidad Antonio de Nebrija', 'Universidad Politécnica de Madrid', 'Universidad de Valladolid'], 'pdf_title_img': 'assets/pdf/title_img/2508.01408.jpg', 'data': {'categories': ['#cv', '#dataset', '#ethics', '#hallucinations', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект vs Искусство: проблемы атрибуции в эпоху нейросетей', 'desc': 'Современные модели компьютерного зрения и обработки естественного языка (VLM) испытывают трудности с точной атрибуцией художников и распознаванием изображений, сгенерированных искусственным интеллектом. Исследование проводилось на большом наборе данных, включающем около 40 000 картин 128 художников. Результаты показывают ограниченные возможности VLM в атрибуции картин и идентификации изображений, созданных ИИ. Авторы подчеркивают необходимость улучшения этих моделей для предотвращения распространения неверной информации.'}, 'en': {'title': 'Enhancing AI for Accurate Art Attribution and Detection', 'desc': 'This paper investigates the limitations of state-of-the-art vision language models (VLMs) in accurately attributing artworks to their respective artists and distinguishing between real and AI-generated images. The study highlights that these models struggle with both canvas attribution and the identification of AI-generated content, which can lead to misinformation. Using a large dataset of nearly 40,000 paintings from 128 artists, the authors demonstrate the inadequacies of current AI models in these tasks. The findings emphasize the urgent need for advancements in VLMs to enhance their reliability in art attribution and image detection.'}, 'zh': {'title': '提升视觉语言模型以防止艺术作品错误归属', 'desc': '本论文探讨了当前视觉语言模型在艺术作品归属和区分AI生成图像方面的不足。研究发现，AI模型能够生成模仿画家风格的图像，导致错误归属的情况。与此同时，AI模型在识别真实画作的艺术家时也存在困难，可能导致用户错误地归属作品。通过对近40,000幅来自128位艺术家的画作进行实验，结果显示视觉语言模型在画布归属和AI生成图像识别方面的能力有限，强调了改进这些模型的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.01287', 'title': 'Exploitation Is All You Need... for Exploration', 'url': 'https://huggingface.co/papers/2508.01287', 'abstract': 'Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process.', 'score': 3, 'issue_id': 5179, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '8867dd3f084db81e', 'authors': ['Micah Rentschler', 'Jesse Roberts'], 'affiliations': ['Tennessee Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01287.jpg', 'data': {'categories': ['#agents', '#optimization', '#games', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'Исследование может возникнуть из чистой эксплуатации', 'desc': 'Это исследование показывает, что агенты мета-обучения с подкреплением могут демонстрировать исследовательское поведение при обучении с жадной целевой функцией. Для этого необходимы три условия: повторяющаяся структура среды, наличие памяти у агента и возможность долгосрочного назначения кредита. Эксперименты на многоруких бандитах и сетках подтверждают, что при наличии структуры и памяти политика, обученная только на максимизацию награды, проявляет поисковое поведение. Удивительно, но отсутствие долгосрочного назначения кредита не всегда препятствует возникновению исследования.'}, 'en': {'title': 'Exploration Emerges from Greedy Training with the Right Conditions', 'desc': 'This paper explores how meta-reinforcement learning agents can learn to explore their environments even when trained with a focus on maximizing immediate rewards. The authors propose that this exploratory behavior can emerge if the environment has recurring structures, the agent has memory to recall past experiences, and long-term credit assignment is possible. Through experiments, they show that when these conditions are met, agents can engage in information-seeking exploration without explicit incentives. The findings challenge the traditional view that exploration and exploitation are separate goals, suggesting they can coexist within a single reward-maximization framework.'}, 'zh': {'title': '探索与利用的统一：元强化学习的新视角', 'desc': '本研究探讨了元强化学习代理在特定条件下如何表现出探索行为。我们提出，当环境具有重复结构、代理具备记忆能力，并且能够进行长期信用分配时，即使代理仅以贪婪目标进行训练，也能自发地进行探索。实验结果表明，在随机多臂老虎机和时间扩展的网格世界中，满足这些条件的代理会表现出信息寻求的探索行为。我们的发现表明，探索和利用并非完全对立，而是可以通过统一的奖励最大化过程共同出现。'}}}, {'id': 'https://huggingface.co/papers/2508.00910', 'title': 'Cyber-Zero: Training Cybersecurity Agents without Runtime', 'url': 'https://huggingface.co/papers/2508.00910', 'abstract': 'Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.', 'score': 3, 'issue_id': 5177, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '181a31b28dfe8e6a', 'authors': ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang'], 'affiliations': ['Amazon', 'Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00910.jpg', 'data': {'categories': ['#agents', '#dataset', '#synthetic', '#benchmark', '#open_source'], 'emoji': '🛡️', 'ru': {'title': 'Синтез траекторий без среды выполнения для обучения передовых LLM в кибербезопасности', 'desc': 'Cyber-Zero - это первая система для синтеза траекторий агентов без использования среды выполнения для обучения языковых моделей в кибербезопасности. Она использует общедоступные отчеты CTF и симуляцию на основе LLM для создания реалистичных последовательностей взаимодействий. Обученные на синтезированных траекториях агенты на основе LLM достигают значительного улучшения производительности на трех ключевых бенчмарках CTF. Лучшая модель Cyber-Zero-32B устанавливает новый state-of-the-art среди открытых моделей, соответствуя возможностям проприетарных систем.'}, 'en': {'title': 'Revolutionizing Cybersecurity AI with Runtime-Free Trajectory Synthesis', 'desc': 'Cyber-Zero introduces a novel framework for training cybersecurity large language models (LLMs) without the need for executable runtime environments. It synthesizes agent trajectories from Capture The Flag (CTF) writeups, allowing the generation of realistic interaction sequences that mimic runtime behaviors. This approach enables the training of LLM-based agents that outperform existing models on key CTF benchmarks. By achieving state-of-the-art performance with a cost-effective solution, Cyber-Zero demonstrates the potential of runtime-free trajectory synthesis in advancing cybersecurity AI.'}, 'zh': {'title': 'Cyber-Zero：无运行时环境的网络安全代理训练新方法', 'desc': 'Cyber-Zero 是一个创新的框架，旨在通过合成高质量的代理轨迹来训练网络安全领域的语言模型（LLM），而无需实际的运行时环境。该框架利用公开的CTF（Capture The Flag）写作材料，采用基于角色的LLM模拟，逆向工程运行时行为，生成真实的长时间交互序列。通过使用Cyber-Zero合成的轨迹，我们训练的LLM代理在三个主要的CTF基准测试中，性能提升达13.1%。Cyber-Zero-32B模型在开放权重模型中创造了新的性能记录，展示了无运行时轨迹合成在网络安全代理开发中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.00890', 'title': 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks', 'url': 'https://huggingface.co/papers/2508.00890', 'abstract': 'AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.', 'score': 2, 'issue_id': 5178, 'pub_date': '2025-07-26', 'pub_date_card': {'ru': '26 июля', 'en': 'July 26', 'zh': '7月26日'}, 'hash': '359ff54230f7e0ba', 'authors': ['Fali Wang', 'Hui Liu', 'Zhenwei Dai', 'Jingying Zeng', 'Zhiwei Zhang', 'Zongyu Wu', 'Chen Luo', 'Zhen Li', 'Xianfeng Tang', 'Qi He', 'Suhang Wang'], 'affiliations': ['Amazon, Palo Alto, CA, USA', 'The Pennsylvania State University, University Park, PA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.00890.jpg', 'data': {'categories': ['#interpretability', '#rl', '#agents', '#optimization', '#inference'], 'emoji': '🤖', 'ru': {'title': 'Умное распределение ресурсов для сложных задач искусственного интеллекта', 'desc': 'AgentTTS - это фреймворк на основе LLM-агентов для оптимизации распределения вычислительных ресурсов в многоэтапных сложных задачах. Он использует итеративные взаимодействия с обратной связью для поиска оптимальных распределений моделей и бюджетов для каждой подзадачи. AgentTTS превосходит традиционные методы по эффективности поиска, устойчивости к размерам обучающих выборок и интерпретируемости. Фреймворк решает проблемы комбинаторного пространства поиска и взаимозависимости оптимальных распределений между подзадачами.'}, 'en': {'title': 'Optimizing Compute Allocation for Complex Multi-Stage Tasks with AgentTTS', 'desc': 'AgentTTS is a framework that optimizes how computing resources are allocated for complex tasks that involve multiple stages. It focuses on improving the performance of large language models (LLMs) by dynamically adjusting resource allocation during inference, especially for tasks that require different capabilities at each stage. The framework addresses challenges such as the combinatorial nature of model selection and budget allocation, which makes traditional search methods inefficient. Through extensive experiments, AgentTTS has shown to be more efficient and robust compared to existing methods, providing better performance across various datasets and tasks.'}, 'zh': {'title': 'AgentTTS：优化多阶段任务的计算分配', 'desc': 'AgentTTS是一个基于大语言模型（LLM）代理的框架，旨在优化多阶段复杂任务的计算资源分配。与传统方法相比，它在性能和鲁棒性上有显著提升。该框架通过迭代反馈与执行环境进行交互，自动搜索计算最优的分配方案。实验结果表明，AgentTTS在搜索效率上显著优于传统方法和其他基于LLM的基线，并且对训练集大小的变化表现出更好的鲁棒性和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2508.02605', 'title': 'ReMoMask: Retrieval-Augmented Masked Motion Generation', 'url': 'https://huggingface.co/papers/2508.02605', 'abstract': "ReMoMask, a unified framework, addresses limitations in text-to-motion generation by integrating a Bidirectional Momentum Text-Motion Model, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance on HumanML3D and KIT-ML benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.", 'score': 1, 'issue_id': 5185, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '8fa0e8fe5289831e', 'authors': ['Zhengdao Li', 'Siheng Wang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Jiangsu University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02605.jpg', 'data': {'categories': ['#diffusion', '#games', '#benchmark', '#rag', '#video', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'ReMoMask: Революция в генерации движений из текста', 'desc': 'ReMoMask - это унифицированная система для генерации движений по текстовому описанию, решающая ограничения существующих подходов. Она включает в себя три ключевые инновации: двунаправленную моментную текстово-моторную модель, семантическое пространственно-временное внимание и RAG-безклассификаторное управление. ReMoMask достигает наилучших результатов на стандартных бенчмарках HumanML3D и KIT-ML, значительно улучшая показатели FID. Система эффективно генерирует согласованные во времени движения за минимальное количество шагов.'}, 'en': {'title': 'ReMoMask: Revolutionizing Text-to-Motion Generation!', 'desc': "ReMoMask is a new framework designed to improve text-to-motion generation, which creates human motion sequences from text descriptions. It combines a Bidirectional Momentum Text-Motion Model, which enhances retrieval accuracy, with a Semantic Spatio-temporal Attention mechanism that ensures realistic motion by applying biomechanical constraints. Additionally, it uses RAG-Classier-Free Guidance to boost the model's ability to generalize from data. The framework has shown significant improvements in generating coherent motions, outperforming previous methods on key benchmarks."}, 'zh': {'title': 'ReMoMask：文本到运动生成的新突破', 'desc': 'ReMoMask是一个统一框架，旨在解决文本到运动生成中的局限性。它通过集成双向动量文本-运动模型、语义时空注意力机制和无分类器引导，显著提高了在HumanML3D和KIT-ML基准测试中的表现。该框架通过动量队列解耦负样本规模与批量大小，提升了跨模态检索的精度。同时，语义时空注意力机制在部分融合过程中施加生物力学约束，消除了异步伪影。'}}}, {'id': 'https://huggingface.co/papers/2508.02268', 'title': 'SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System', 'url': 'https://huggingface.co/papers/2508.02268', 'abstract': 'A bidirectional machine translation system, SHAMI-MT, bridges the gap between Modern Standard Arabic and the Syrian dialect using AraT5v2-base-1024 architecture, achieving high-quality translations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces SHAMI-MT, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of 4.01 out of 5.0 when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.', 'score': 1, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '0704696f67aca30f', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh - Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2508.02268.jpg', 'data': {'categories': ['#training', '#low_resource', '#multilingual', '#dataset', '#machine_translation'], 'emoji': '🌉', 'ru': {'title': 'Мост между арабским языком и диалектом: революция в машинном переводе', 'desc': 'SHAMI-MT - это система машинного перевода, разработанная для преодоления разрыва между современным стандартным арабским языком и сирийским диалектом. Она использует архитектуру AraT5v2-base-1024 и включает две специализированные модели для двунаправленного перевода. Система была обучена на наборе данных Nabra и оценена на корпусе MADAR. Модель перевода с арабского на сирийский диалект достигла высокого среднего балла качества 4.01 из 5.0 по оценке GPT-4.1.'}, 'en': {'title': 'Bridging Dialects: SHAMI-MT Translates Arabic with Precision', 'desc': 'The paper presents SHAMI-MT, a bidirectional machine translation system designed to translate between Modern Standard Arabic (MSA) and the Syrian dialect. Utilizing the AraT5v2-base-1024 architecture, the system includes two specialized models for MSA-to-Shami and Shami-to-MSA translations. These models were fine-tuned on the Nabra dataset and evaluated using the MADAR corpus, achieving a high quality score of 4.01 out of 5.0. This work addresses the challenges of diglossia in Arabic, providing a valuable tool for accurate and culturally relevant translations.'}, 'zh': {'title': '弥合阿拉伯语与叙利亚方言的翻译桥梁', 'desc': '本文介绍了一种双向机器翻译系统SHAMI-MT，旨在弥合现代标准阿拉伯语与叙利亚方言之间的差距。该系统基于先进的AraT5v2-base-1024架构，分别针对现代标准阿拉伯语到叙利亚方言和叙利亚方言到现代标准阿拉伯语的翻译进行了优化。通过在全面的Nabra数据集上进行微调，并在MADAR语料库的未见数据上进行严格评估，模型表现出色。SHAMI-MT为阿拉伯方言翻译领域提供了重要的高保真工具，促进了内容本地化和跨文化交流。'}}}, {'id': 'https://huggingface.co/papers/2508.01109', 'title': 'Platonic Representations for Poverty Mapping: Unified Vision-Language\n  Codes or Agent-Induced Novelty?', 'url': 'https://huggingface.co/papers/2508.01109', 'abstract': 'A multimodal framework using satellite imagery and text data outperforms vision-only models in predicting household wealth, with LLM-generated text proving more effective than agent-retrieved text.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.', 'score': 1, 'issue_id': 5188, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '411bd35601db2ebd', 'authors': ['Satiyabooshan Murugaboopathy', 'Connor T. Jerzak', 'Adel Daoud'], 'affiliations': ['Chalmers & Linköping University', 'Fraunhofer Center', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2508.01109.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#dataset', '#science'], 'emoji': '🛰️', 'ru': {'title': 'Мультимодальный подход превосходит визуальные модели в оценке благосостояния', 'desc': 'Исследование показывает, что комбинация спутниковых снимков и текстовых данных превосходит модели, основанные только на изображениях, в прогнозировании благосостояния домохозяйств. Разработана мультимодальная система, использующая спутниковые снимки Landsat и текстовые описания, сгенерированные большой языковой моделью (LLM). Система включает пять различных подходов, включая модель компьютерного зрения, LLM и ансамбль всех сигналов. Результаты демонстрируют, что объединение визуальной и текстовой информации значительно улучшает точность прогнозирования благосостояния по сравнению с использованием только изображений.'}, 'en': {'title': 'Unlocking Wealth Insights: The Power of Multimodal Learning', 'desc': "This paper presents a multimodal framework that combines satellite imagery and text data to predict household wealth more accurately than models using only visual data. By analyzing Demographic and Health Survey data from African neighborhoods, the authors demonstrate that integrating LLM-generated text with satellite images significantly improves prediction performance. The study reveals that the internal knowledge of large language models (LLMs) is more effective than text retrieved by AI agents, enhancing the model's robustness across different contexts. Additionally, the research contributes a large-scale dataset that links over 60,000 clusters of socio-economic data with corresponding satellite images and textual descriptions."}, 'zh': {'title': '多模态框架提升家庭财富预测精度', 'desc': '本研究探讨了家庭财富等社会经济指标是否可以通过卫星图像和互联网文本数据进行预测。我们开发了一个多模态框架，结合了卫星图像和基于位置/年份的LLM生成文本，以提高财富预测的准确性。研究结果表明，融合视觉和文本信息的模型在财富预测上优于仅使用视觉的模型，且LLM生成的文本效果更佳。我们还发布了一个包含超过60,000个DHS集群的大规模多模态数据集，以支持未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2508.00024', 'title': 'Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine\n  Learning', 'url': 'https://huggingface.co/papers/2508.00024', 'abstract': 'Combining Vision Transformer embeddings with quantum-classical pipelines achieves quantum advantage in classification tasks, demonstrating the importance of embedding choice in quantum machine learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.', 'score': 1, 'issue_id': 5185, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'c9825e1a6f4d2a1c', 'authors': ['Sebastián Andrés Cajas Ordóñez', 'Luis Fernando Torres Torres', 'Mario Bifulco', 'Carlos Andrés Durán', 'Cristian Bosch', 'Ricardo Simón Carbajo'], 'affiliations': ['Corporation for Aerospace Initiatives (CASIRI), University of Cauca, Popayán, Colombia', 'Department of Computer Science, University of Torino, Torino, Italy', 'National Irish Centre for AI (CeADAR), University College Dublin (UCD), Dublin, Ireland', 'SISTEMIC Research Group, University of Antioquia, Medellín, Colombia'], 'pdf_title_img': 'assets/pdf/title_img/2508.00024.jpg', 'data': {'categories': ['#cv', '#games', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Квантовое преимущество через синергию трансформеров и квантовых пространств признаков', 'desc': 'Статья представляет новый подход к квантовому машинному обучению, объединяющий эмбеддинги Vision Transformer с квантово-классическими пайплайнами. Авторы демонстрируют, что такой метод позволяет достичь квантового преимущества в задачах классификации, превосходя классические SVM на наборах данных Fashion-MNIST и MNIST. Ключевым открытием является то, что эмбеддинги ViT уникальным образом обеспечивают квантовое преимущество, в то время как признаки CNN показывают снижение производительности. Исследование подчеркивает важность выбора эмбеддингов в квантовом машинном обучении и открывает путь к масштабируемым квантовым алгоритмам, использующим современные нейронные архитектуры.'}, 'en': {'title': 'Unlocking Quantum Advantage with Vision Transformers', 'desc': 'This paper explores how combining Vision Transformer (ViT) embeddings with quantum-classical pipelines can enhance classification tasks in machine learning. It highlights the challenges faced by Quantum Support Vector Machines (QSVMs) due to high-dimensional quantum states and hardware limitations. The authors introduce a method that uses class-balanced k-means distillation alongside pretrained ViT embeddings, which significantly improves accuracy compared to classical SVMs. Their findings suggest that the choice of embedding is crucial for achieving quantum advantage, demonstrating a beneficial interaction between transformer attention mechanisms and quantum feature spaces.'}, 'zh': {'title': '量子机器学习中的嵌入选择与优势', 'desc': '本文探讨了将视觉变换器（Vision Transformer）嵌入与量子-经典管道结合的方式，以在分类任务中实现量子优势。研究表明，嵌入的选择对量子机器学习至关重要，使用ViT嵌入可以在Fashion-MNIST数据集上提高8.02%的准确率，而在MNIST数据集上提高4.42%。相比之下，卷积神经网络（CNN）特征的表现却有所下降。通过使用16量子比特的张量网络模拟，本文首次系统性地证明了量子核优势与嵌入选择之间的关键关系，揭示了变换器注意力与量子特征空间之间的基本协同作用。'}}}, {'id': 'https://huggingface.co/papers/2508.01773', 'title': 'Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2508.01773', 'abstract': "An uncertainty-driven framework for automated process reward data construction and aggregation methods improves the effectiveness and efficiency of Process-Level Reward Models in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.", 'score': 0, 'issue_id': 5186, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '872cba646c3b0c3d', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01773.jpg', 'data': {'categories': ['#math', '#training', '#dataset', '#reasoning', '#optimization', '#data'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических рассуждений ИИ через неопределенность', 'desc': 'Статья представляет новый подход к построению и агрегации данных для обучения моделей вознаграждения на уровне процесса (PRM) в задачах математических рассуждений. Авторы предлагают фреймворк, основанный на неопределенности, для автоматизированного создания данных о вознаграждениях процесса. Также введены два новых метода агрегации выходных данных: гибридное мажоритарное голосование по вознаграждениям и взвешенное частотное голосование по вознаграждениям. Эксперименты на нескольких наборах данных показывают эффективность предложенного подхода в улучшении способностей моделей к математическим рассуждениям.'}, 'en': {'title': 'Automating Reward Data for Smarter Math Reasoning', 'desc': 'This paper presents a new framework that automates the creation of process reward data, which is essential for training Process-Level Reward Models (PRMs) in mathematical reasoning tasks. The authors highlight the challenges of existing data construction methods, which are often time-consuming and inefficient. They introduce two innovative output aggregation techniques, Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, that enhance the performance of PRMs by effectively combining the strengths of traditional voting methods with PRM evaluations. Experimental results demonstrate that this uncertainty-driven approach significantly improves both the quality of the reward data and the reasoning capabilities of the models tested.'}, 'zh': {'title': '基于不确定性的自动化过程奖励数据构建框架', 'desc': '本文提出了一种基于不确定性的框架，用于自动化过程奖励数据的构建和聚合方法，以提高过程级奖励模型在数学推理任务中的有效性和效率。过程级奖励模型（PRMs）通过在每个中间步骤提供监督和评估，显著提升了模型的推理能力。然而，构建高质量的过程奖励数据通常需要耗费大量人力，现有方法效率低下。我们还提出了两种通用的不确定性感知输出聚合方法，进一步增强了PRMs的数学推理能力。'}}}, {'id': 'https://huggingface.co/papers/2507.16290', 'title': 'Dens3R: A Foundation Model for 3D Geometry Prediction', 'url': 'https://huggingface.co/papers/2507.16290', 'abstract': 'Dens3R is a 3D foundation model that jointly predicts multiple geometric quantities using a two-stage training framework, enhancing consistency and performance in dense 3D reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains a major challenge. Most existing methods are limited to predicting a single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore a unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, a 3D foundation model designed for joint geometric dense prediction and adaptable to a wide range of downstream tasks. Dens3R adopts a two-stage training framework to progressively build a pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design a lightweight shared encoder-decoder backbone and introduce position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose a post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various dense 3D prediction tasks and highlight its potential for broader applications.', 'score': 0, 'issue_id': 5189, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': 'd47ef3bd9b4560f6', 'authors': ['Xianze Fang', 'Jingnan Gao', 'Zhe Wang', 'Zhuo Chen', 'Xingyu Ren', 'Jiangjing Lyu', 'Qiaomu Ren', 'Zhonglei Yang', 'Xiaokang Yang', 'Yichao Yan', 'Chengfei Lyu'], 'affiliations': ['Alibaba Group, China', 'Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.16290.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧊', 'ru': {'title': 'Единая модель для точной 3D-реконструкции', 'desc': 'Dens3R - это модель для совместного предсказания нескольких геометрических характеристик в задачах плотной 3D-реконструкции. Она использует двухэтапную схему обучения для повышения согласованности и производительности. Dens3R применяет легковесную архитектуру энкодер-декодер и позиционно-интерполированное роторное позиционное кодирование. Модель способна точно регрессировать множество геометрических величин, таких как нормали поверхности и глубина, обеспечивая согласованное восприятие геометрии как для одного, так и для нескольких ракурсов.'}, 'en': {'title': 'Dens3R: Unified Predictions for Consistent 3D Geometry', 'desc': "Dens3R is a 3D foundation model that improves dense 3D reconstruction by predicting multiple geometric quantities together, such as depth and surface normals. It uses a two-stage training framework to enhance the consistency and accuracy of these predictions, addressing the limitations of existing methods that focus on single geometry predictions. By modeling the relationships between different geometric properties, Dens3R ensures that the predictions are coherent and reliable. The model's design includes a lightweight encoder-decoder and advanced encoding techniques, making it adaptable for various applications in 3D geometry tasks."}, 'zh': {'title': 'Dens3R：联合几何预测的3D基础模型', 'desc': 'Dens3R是一种3D基础模型，旨在通过两阶段训练框架联合预测多个几何量，从而提高密集3D重建任务中的一致性和性能。现有方法通常只能从输入图像中预测单一几何量，而Dens3R则通过建模不同几何属性之间的结构耦合，实现了联合回归。该模型采用轻量级共享编码器-解码器架构，并引入位置插值旋转位置编码，以增强对高分辨率输入的鲁棒性。实验结果表明，Dens3R在多种密集3D预测任务中表现优越，具有广泛的应用潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (14)', '#agents (29)', '#agi (5)', '#alignment (13)', '#architecture (20)', '#audio (7)', '#benchmark (54)', '#cv (22)', '#data (19)', '#dataset (47)', '#diffusion (14)', '#ethics (9)', '#games (26)', '#graphs', '#hallucinations (9)', '#healthcare (5)', '#inference (7)', '#interpretability (12)', '#leakage (1)', '#long_context (13)', '#low_resource (2)', '#machine_translation (3)', '#math (6)', '#multilingual (8)', '#multimodal (41)', '#open_source (41)', '#optimization (67)', '#plp (2)', '#rag (5)', '#reasoning (33)', '#rl (23)', '#rlhf (11)', '#robotics (6)', '#science (8)', '#security (5)', '#small_models (2)', '#story_generation', '#survey (6)', '#synthetic (10)', '#training (58)', '#transfer_learning (2)', '#video (12)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-10 12:51',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-10 12:51')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-10 12:51')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    