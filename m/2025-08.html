
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 134 papers. August 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞ²Ğ³ÑƒÑÑ‚ 2025</span> | <span id="title-articles-count">134 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-07.html">â¬…ï¸ <span id="prev-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-09.html">â¡ï¸ <span id="next-date">09.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞ²Ğ³ÑƒÑÑ‚ 2025', 'en': 'August 2025', 'zh': '8æœˆ2025å¹´'};
        let feedDateNext = {'ru': '09.2025', 'en': '09/2025', 'zh': '9æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '07.2025', 'en': '07/2025', 'zh': '7æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.05629', 'title': 'On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification', 'url': 'https://huggingface.co/papers/2508.05629', 'abstract': 'Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.', 'score': 95, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '50bf66dc29886a85', 'authors': ['Yongliang Wu', 'Yizhou Zhou', 'Zhou Ziheng', 'Yingzhe Peng', 'Xinyu Ye', 'Xinting Hu', 'Wenbo Zhu', 'Lu Qi', 'Ming-Hsuan Yang', 'Xu Yang'], 'affiliations': ['Independent Researcher', 'Nanyang Technological University', 'Shanghai Jiao Tong University', 'Southeast University', 'University of California, Berkeley', 'University of California, Los Angeles', 'University of California, Merced', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05629.jpg', 'data': {'categories': ['#optimization', '#rl', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¢Ğ¾Ğ½ĞºĞ¾Ğ¹ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (DFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). DFT Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¢Ğ¾Ğ½ĞºĞ¾Ğ¹ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (SFT). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ SFT Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ SFT.'}, 'en': {'title': 'Dynamic Fine-Tuning: Elevating LLM Generalization with Smart Gradients', 'desc': 'This paper introduces Dynamic Fine-Tuning (DFT), a method that enhances the generalization of Large Language Models (LLMs) by adjusting gradient updates. The authors identify that traditional Supervised Fine-Tuning (SFT) can limit model performance due to its inherent reward structure. By dynamically rescaling the objective function based on token probabilities, DFT stabilizes the training process and leads to better outcomes on various benchmarks. The results indicate that DFT not only surpasses SFT but also performs competitively in offline reinforcement learning scenarios, making it a valuable advancement in model training techniques.'}, 'zh': {'title': 'åŠ¨æ€å¾®è°ƒï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼', 'desc': 'åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´æ¢¯åº¦çš„ç¼©æ”¾ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸æ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç›¸æ¯”ï¼ŒDFTåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä¼˜ï¼Œä¸”åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­ä¹Ÿå±•ç°å‡ºç«äº‰åŠ›ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†SFTçš„æ¢¯åº¦éšå«äº†ä¸€ä¸ªæœ‰é—®é¢˜çš„å¥–åŠ±ç»“æ„ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚DFTé€šè¿‡æ ¹æ®æ¯ä¸ªtokençš„æ¦‚ç‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°ï¼Œç¨³å®šäº†æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05004', 'title': 'R-Zero: Self-Evolving Reasoning LLM from Zero Data', 'url': 'https://huggingface.co/papers/2508.05004', 'abstract': 'R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.', 'score': 80, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '4e0838dc787e59cf', 'authors': ['Chengsong Huang', 'Wenhao Yu', 'Xiaoyang Wang', 'Hongming Zhang', 'Zongxia Li', 'Ruosen Li', 'Jiaxin Huang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Seattle Lab', 'The University of Texas at Dallas', 'University of Maryland, College Park', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2508.05004.jpg', 'data': {'categories': ['#agents', '#rl', '#optimization', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·ÑƒĞ¼Ñƒ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'R-Zero - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Challenger Ğ¸ Solver, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸ Ñ€ĞµÑˆĞ°Ñ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. R-Zero Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… LLM Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ R-Zero.'}, 'en': {'title': 'Autonomous Learning for Super-Intelligent AI', 'desc': "R-Zero is an innovative framework that enables Large Language Models (LLMs) to autonomously create and learn from their own training data, eliminating the need for human-curated tasks. It consists of two models, a Challenger and a Solver, which interact and evolve together; the Challenger proposes tasks that push the Solver's limits, while the Solver learns to tackle these challenges. This self-improving process generates a focused curriculum that enhances reasoning capabilities without relying on pre-existing labels or tasks. Empirical results show that R-Zero significantly boosts the performance of various LLMs on reasoning benchmarks, demonstrating its potential to advance AI systems beyond human intelligence."}, 'zh': {'title': 'è‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½æ¡†æ¶ï¼Œè¶…è¶Šäººç±»æ™ºèƒ½çš„æœªæ¥', 'desc': 'R-Zeroæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå’Œå­¦ä¹ è‡ªå·±çš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€äººå·¥ç­–åˆ’çš„ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆå§‹åŒ–ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹â€”â€”æŒ‘æˆ˜è€…å’Œè§£å†³è€…ï¼Œæ¥å®ç°æ¨¡å‹çš„å…±åŒè¿›åŒ–ã€‚æŒ‘æˆ˜è€…è´Ÿè´£æå‡ºæ¥è¿‘è§£å†³è€…èƒ½åŠ›è¾¹ç•Œçš„ä»»åŠ¡ï¼Œè€Œè§£å†³è€…åˆ™ä¸“æ³¨äºè§£å†³è¿™äº›æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§äº’åŠ¨ï¼ŒR-Zeroèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢„å…ˆå­˜åœ¨çš„ä»»åŠ¡å’Œæ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆä¸€ä¸ªæœ‰é’ˆå¯¹æ€§çš„è‡ªæˆ‘æå‡è¯¾ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒåŸºç¡€LLMsçš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05635', 'title': 'Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation', 'url': 'https://huggingface.co/papers/2508.05635', 'abstract': 'Genie Envisioner integrates policy learning, evaluation, and simulation using a video diffusion model and neural simulator for instruction-driven robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.', 'score': 62, 'issue_id': 5243, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': 'f4ca777a8500b711', 'authors': ['Yue Liao', 'Pengfei Zhou', 'Siyuan Huang', 'Donglin Yang', 'Shengcong Chen', 'Yuxin Jiang', 'Yue Hu', 'Jingbin Cai', 'Si Liu', 'Jianlan Luo', 'Liliang Chen', 'Shuicheng Yan', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['BUAA', 'LV-Lab', 'NUS', 'Unified World Foundation'], 'pdf_title_img': 'assets/pdf/title_img/2508.05635.jpg', 'data': {'categories': ['#video', '#benchmark', '#training', '#optimization', '#robotics', '#agi', '#open_source', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Genie Envisioner (GE) Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GE Ğ»ĞµĞ¶Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. GE-Act Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° GE-Sim ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ EWMBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unified Framework for Instruction-Driven Robotic Manipulation', 'desc': 'Genie Envisioner (GE) is a comprehensive platform designed for robotic manipulation that combines policy learning, evaluation, and simulation into one framework. It utilizes a video diffusion model to understand and generate realistic robotic interactions based on instructions. The system includes a decoder that translates learned representations into actionable movements, allowing robots to perform tasks with minimal guidance. Additionally, it features a neural simulator for testing and refining policies, along with a benchmark suite to evaluate performance across various criteria.'}, 'zh': {'title': 'Genie Envisionerï¼šæŒ‡ä»¤é©±åŠ¨çš„æœºå™¨äººæ™ºèƒ½æ–°å¹³å°', 'desc': 'Genie Envisionerï¼ˆGEï¼‰æ˜¯ä¸€ä¸ªé›†æˆäº†ç­–ç•¥å­¦ä¹ ã€è¯„ä¼°å’Œæ¨¡æ‹Ÿçš„æœºå™¨äººæ“ä½œå¹³å°ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªå¤§å‹çš„ã€åŸºäºæŒ‡ä»¤çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰ç°å®ä¸–ç•Œä¸­æœºå™¨äººäº¤äº’çš„ç©ºé—´ã€æ—¶é—´å’Œè¯­ä¹‰åŠ¨æ€ã€‚GE-Acté€šè¿‡è½»é‡çº§çš„è§£ç å™¨å°†æ½œåœ¨è¡¨ç¤ºæ˜ å°„åˆ°å¯æ‰§è¡Œçš„åŠ¨ä½œè½¨è¿¹ï¼Œå®ç°äº†åœ¨ä¸åŒç¯å¢ƒä¸­ç²¾ç¡®ä¸”å¯æ¨å¹¿çš„ç­–ç•¥æ¨æ–­ã€‚GE-Simä½œä¸ºä¸€ä¸ªç¥ç»æ¨¡æ‹Ÿå™¨ï¼Œæ”¯æŒé«˜ä¿çœŸåº¦çš„é—­ç¯ç­–ç•¥å¼€å‘ï¼Œæ•´ä¸ªç³»ç»Ÿä¸ºæŒ‡ä»¤é©±åŠ¨çš„é€šç”¨æ™ºèƒ½æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05405', 'title': 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning', 'url': 'https://huggingface.co/papers/2508.05405', 'abstract': "DeepPHY evaluates Vision Language Models' physical reasoning and control through simulated environments with varying difficulty levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.", 'score': 54, 'issue_id': 5243, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '3ec0b6b2d584612a', 'authors': ['Xinrun Xu', 'Pi Bu', 'Ye Wang', 'BÃ¶rje F. Karlsson', 'Ziming Wang', 'Tengtao Song', 'Qi Zhu', 'Jun Song', 'Zhiming Ding', 'Bo Zheng'], 'affiliations': ['Informatics Department, PUC-Rio', 'Institute of Software, Chinese Academy of Science', 'Renmin University of China', 'Taobao & Tmall Group of Alibaba', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.05405.jpg', 'data': {'categories': ['#games', '#benchmark', '#reasoning', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DeepPHY: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ…', 'desc': 'DeepPHY - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. DeepPHY Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'DeepPHY: Bridging the Gap in Physical Reasoning for Vision Language Models', 'desc': 'DeepPHY is a benchmark framework that assesses Vision Language Models (VLMs) on their ability to understand and apply physical reasoning in simulated environments. It highlights the challenges VLMs face in executing precise actions and planning in complex scenarios, which are essential for real-world tasks. The framework includes various environments with different difficulty levels and uses detailed metrics for evaluation. Results show that even advanced VLMs have difficulty converting their understanding of physical concepts into accurate control actions.'}, 'zh': {'title': 'DeepPHYï¼šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›', 'desc': 'DeepPHYæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰©ç†æ¨ç†å’Œæ§åˆ¶æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰ä¸åŒéš¾åº¦çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç³»ç»Ÿåœ°æµ‹è¯•VLMså¯¹åŸºæœ¬ç‰©ç†åŸç†çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å½“å‰çš„VLMsåœ¨æ„ŸçŸ¥å’Œè§†è§‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­ï¼Œå®ƒä»¬åœ¨ç»†èŠ‚å…³æ³¨å’Œç²¾ç¡®è¡ŒåŠ¨è§„åˆ’æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„VLMsä¹Ÿéš¾ä»¥å°†æè¿°æ€§çš„ç‰©ç†çŸ¥è¯†è½¬åŒ–ä¸ºç²¾ç¡®çš„é¢„æµ‹æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05609', 'title': 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity', 'url': 'https://huggingface.co/papers/2508.05609', 'abstract': 'Hi3DEval is a hierarchical evaluation framework for 3D generative content that combines object-level and part-level assessments, including material realism, using a large-scale dataset and hybrid 3D representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.', 'score': 25, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '97d6454893bee33c', 'authors': ['Yuhan Zhang', 'Long Zhuo', 'Ziyang Chu', 'Tong Wu', 'Zhibing Li', 'Liang Pan', 'Dahua Lin', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05609.jpg', 'data': {'categories': ['#3d', '#benchmark', '#optimization', '#games', '#dataset'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ', 'desc': 'Hi3DEval - ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Hi3DEval Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing 3D Content Evaluation with Hi3DEval', 'desc': 'Hi3DEval is a new framework designed to evaluate 3D generative content by assessing both the overall object and its individual parts. It addresses the limitations of current methods that only use image-based metrics, which often miss important details like spatial coherence and material realism. The framework includes a large dataset called Hi3DBench, which features a variety of 3D assets and detailed annotations to support comprehensive evaluations. By utilizing advanced scoring systems and hybrid 3D representations, Hi3DEval provides a more accurate and scalable way to assess the quality of 3D generated content.'}, 'zh': {'title': '3Dç”Ÿæˆå†…å®¹çš„åˆ†å±‚è¯„ä¼°æ–°æ¡†æ¶', 'desc': 'Hi3DEvalæ˜¯ä¸€ä¸ªé’ˆå¯¹3Dç”Ÿæˆå†…å®¹çš„åˆ†å±‚è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†å¯¹è±¡çº§å’Œéƒ¨åˆ†çº§çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ææ–™çœŸå®æ„Ÿã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºå›¾åƒçš„æŒ‡æ ‡ï¼Œä»…åœ¨å¯¹è±¡çº§åˆ«è¿›è¡Œè¯„ä¼°ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ç©ºé—´ä¸€è‡´æ€§å’Œææ–™çœŸå®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒHi3DEvalæä¾›äº†å¤šç»´åº¦çš„æ•´ä½“è¯„ä¼°å’Œç»†è‡´çš„è´¨é‡åˆ†æï¼Œå¹¶æ‰©å±•äº†çº¹ç†è¯„ä¼°ï¼Œå…³æ³¨å¦‚åå°„ç‡ã€é¥±å’Œåº¦å’Œé‡‘å±æ„Ÿç­‰å±æ€§ã€‚é€šè¿‡æ„å»ºHi3DBenchæ•°æ®é›†å’Œ3Dæ„ŸçŸ¥çš„è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿï¼ŒHi3DEvalåœ¨å»ºæ¨¡3Dç‰¹æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„å›¾åƒåŸºå‡†ï¼Œæä¾›äº†å¯æ‰©å±•çš„è¯„ä¼°æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03644', 'title': 'Are We on the Right Way for Assessing Document Retrieval-Augmented\n  Generation?', 'url': 'https://huggingface.co/papers/2508.03644', 'abstract': 'Double-Bench is a large-scale, multilingual, and multimodal evaluation system for document Retrieval-Augmented Generation (RAG) systems, addressing limitations in current benchmarks and providing comprehensive assessments of system components.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.', 'score': 19, 'issue_id': 5243, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'b8c44e363c76888f', 'authors': ['Wenxuan Shen', 'Mingjia Wang', 'Yaochen Wang', 'Dongping Chen', 'Junjie Yang', 'Yao Wan', 'Weiwei Lin'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2508.03644.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#survey', '#rag', '#open_source', '#multilingual'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Double-Bench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° RAG ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Double-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Retrieval-Augmented Generation (RAG) ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 3276 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ 5168 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° 6 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 4 Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° RAG ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½ĞµĞ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… RAG ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Double-Bench: Elevating RAG Evaluation for Real-World Challenges', 'desc': 'Double-Bench is a new evaluation system designed to improve the assessment of Retrieval-Augmented Generation (RAG) systems, which combine document retrieval and generation. It addresses the shortcomings of existing benchmarks by providing a large-scale, multilingual, and multimodal dataset that includes 3,276 documents and 5,168 queries across multiple languages and document types. The evaluation focuses on fine-grained assessments of each component in RAG systems, ensuring that queries are based on thoroughly verified evidence. Our experiments reveal important insights into the performance of various models and highlight the need for better document retrieval capabilities in the face of over-confidence in current frameworks.'}, 'zh': {'title': 'åŒé‡åŸºå‡†ï¼šæå‡æ–‡æ¡£RAGç³»ç»Ÿè¯„ä¼°çš„å…¨æ–°æ ‡å‡†', 'desc': 'Double-Benchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€å¤šæ¨¡æ€è¯„ä¼°ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºæ–‡æ¡£å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„è¯„ä¼°ã€‚å®ƒè§£å†³äº†å½“å‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿå¯¹RAGç³»ç»Ÿçš„å„ä¸ªç»„ä»¶è¿›è¡Œå…¨é¢çš„è¯„ä¼°ã€‚è¯¥ç³»ç»ŸåŒ…å«3276ä»½æ–‡æ¡£å’Œ5168ä¸ªæŸ¥è¯¢ï¼Œæ¶µç›–6ç§è¯­è¨€å’Œ4ç§æ–‡æ¡£ç±»å‹ï¼Œç¡®ä¿è¯„ä¼°çš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ–‡æœ¬å’Œè§†è§‰åµŒå…¥æ¨¡å‹ä¹‹é—´çš„å·®è·æ­£åœ¨ç¼©å°ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å½“å‰RAGæ¡†æ¶ä¸­å­˜åœ¨çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03990', 'title': "Are Today's LLMs Ready to Explain Well-Being Concepts?", 'url': 'https://huggingface.co/papers/2508.03990', 'abstract': 'LLMs can be fine-tuned to generate high-quality, audience-tailored explanations of well-being concepts using Supervised Fine-Tuning and Direct Preference Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.', 'score': 18, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': 'a742f57af42990c1', 'authors': ['Bohan Jiang', 'Dawei Li', 'Zhen Tan', 'Chengshuai Zhao', 'Huan Liu'], 'affiliations': ['School of Computing and Augmented Intelligence, Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.03990.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#dataset', '#alignment', '#open_source', '#rlhf', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 43 880 Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ 2 194 ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµÑÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Supervised Fine-Tuning Ğ¸ Direct Preference Optimization, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Tailoring Well-Being Explanations with Fine-Tuned LLMs', 'desc': 'This paper discusses how Large Language Models (LLMs) can be improved to provide better explanations of well-being concepts tailored to different audiences. It highlights the importance of both factual accuracy and audience-specific needs in generating high-quality explanations. The authors created a large dataset of explanations and developed a unique evaluation framework using LLMs as judges to assess the quality of these explanations. Their findings show that fine-tuning LLMs with Supervised Fine-Tuning and Direct Preference Optimization leads to significant improvements in explanation quality compared to larger, unrefined models.'}, 'zh': {'title': 'æå‡å¹¸ç¦æ„Ÿè§£é‡Šè´¨é‡çš„æ™ºèƒ½æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„å…³äºå¹¸ç¦æ„Ÿæ¦‚å¿µçš„è§£é‡Šè´¨é‡ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«43,880ä¸ªè§£é‡Šçš„å¤§å‹æ•°æ®é›†ï¼Œæ¶µç›–2,194ä¸ªå¹¸ç¦æ„Ÿæ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŸºäºåŸåˆ™çš„LLMè¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆè§£é‡Šæ—¶çš„è´¨é‡æ˜¾è‘—æé«˜ï¼Œä¸”ä¸äººç±»è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚ä¸åŒæ¨¡å‹ã€å—ä¼—å’Œç±»åˆ«ä¹‹é—´çš„è§£é‡Šè´¨é‡å·®å¼‚æ˜æ˜¾ï¼Œè¡¨æ˜åå¥½å­¦ä¹ åœ¨ä¸“ä¸šè§£é‡Šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04017', 'title': 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A\n  Systematic Evaluation Framework of Their Input Scrutiny Ability', 'url': 'https://huggingface.co/papers/2508.04017', 'abstract': "ISEval framework evaluates large multimodal models' ability to detect flawed inputs, revealing challenges in identifying certain types of errors and modality-specific biases.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing formidable capabilities in handling intricate multimodal tasks with exceptional performance. Recent research has underscored the inclination of large language models to passively accept defective inputs, often resulting in futile reasoning on invalid prompts. However, the same critical question of whether LMMs can actively detect and scrutinize erroneous inputs still remains unexplored. To address this gap, we introduce the Input Scrutiny Ability Evaluation Framework (ISEval), which encompasses seven categories of flawed premises and three evaluation metrics. Our extensive evaluation of ten advanced LMMs has identified key findings. Most models struggle to actively detect flawed textual premises without guidance, which reflects a strong reliance on explicit prompts for premise error identification. Error type affects performance: models excel at identifying logical fallacies but struggle with surface-level linguistic errors and certain conditional flaws. Modality trust varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info, while aya-vision-8b over-rely on text in conflicts. These insights underscore the urgent need to enhance LMMs' proactive verification of input validity and shed novel insights into mitigating the problem. The code is available at https://github.com/MLGroupJLU/LMM_ISEval.", 'score': 9, 'issue_id': 5243, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '37794107e7cbe332', 'authors': ['Haiqi Yang', 'Jinzhe Li', 'Gengxu Li', 'Yi Chang', 'Yuan Wu'], 'affiliations': ['Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China', 'International Center of Future Science, Jilin University', 'School of Artificial Intelligence, Jilin University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04017.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#ethics', '#hallucinations', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ˜Ğ˜ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ISEval Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğ¸ Ğº Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼: Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Input Validation in Large Multimodal Models', 'desc': 'The ISEval framework assesses the ability of large multimodal models (LMMs) to identify flawed inputs, highlighting their challenges in recognizing specific errors and biases related to different modalities. Despite their impressive performance in multimodal tasks, many LMMs tend to accept defective inputs without questioning them, leading to ineffective reasoning. The framework categorizes seven types of flawed premises and employs three evaluation metrics to analyze ten advanced LMMs, revealing that most struggle to detect errors without explicit prompts. The findings indicate that while models perform well in identifying logical fallacies, they face difficulties with linguistic errors and exhibit varying trust in different modalities, emphasizing the need for improved input validation mechanisms.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„è¾“å…¥éªŒè¯èƒ½åŠ›', 'desc': 'ISEvalæ¡†æ¶è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ£€æµ‹ç¼ºé™·è¾“å…¥çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†è¯†åˆ«æŸäº›ç±»å‹é”™è¯¯å’Œç‰¹å®šæ¨¡æ€åè§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å€¾å‘äºè¢«åŠ¨æ¥å—æœ‰ç¼ºé™·çš„è¾“å…¥ï¼Œå¯¼è‡´åœ¨æ— æ•ˆæç¤ºä¸Šè¿›è¡Œæ— æ•ˆæ¨ç†ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒLMMsæ˜¯å¦èƒ½å¤Ÿä¸»åŠ¨æ£€æµ‹å’Œå®¡æŸ¥é”™è¯¯è¾“å…¥çš„é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†æ¢è®¨ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨æ²¡æœ‰æŒ‡å¯¼çš„æƒ…å†µä¸‹éš¾ä»¥ä¸»åŠ¨è¯†åˆ«æ–‡æœ¬å‰æçš„ç¼ºé™·ï¼Œå¼ºè°ƒäº†å¯¹æ˜ç¡®æç¤ºçš„å¼ºçƒˆä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03923', 'title': 'CoAct-1: Computer-using Agents with Coding as Actions', 'url': 'https://huggingface.co/papers/2508.03923', 'abstract': 'A multi-agent system that combines GUI control with programmatic execution improves efficiency and success in complex computer automation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. In this work, we introduce a more robust and flexible paradigm: enabling agents to use coding as a enhanced action. We present CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. We evaluate our system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, our approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation.', 'score': 9, 'issue_id': 5246, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '1527938782ab293a', 'authors': ['Linxin Song', 'Yutong Dai', 'Viraj Prabhu', 'Jieyu Zhang', 'Taiwei Shi', 'Li Li', 'Junnan Li', 'Silvio Savarese', 'Zeyuan Chen', 'Jieyu Zhao', 'Ran Xu', 'Caiming Xiong'], 'affiliations': ['Salesforce Research', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.03923.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ GUI Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoAct-1 - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞÑ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ GUI-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñƒ Ğ¸Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ñƒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹. CoAct-1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OSWorld, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°.'}, 'en': {'title': 'Empowering Automation: Merging GUI Control with Programmatic Power', 'desc': 'This paper presents CoAct-1, a multi-agent system that enhances computer automation by combining GUI control with programmatic execution. Traditional agents often face challenges with efficiency and reliability when performing complex tasks solely through GUI manipulation. CoAct-1 introduces an Orchestrator that assigns subtasks to either a GUI Operator or a Programmer agent capable of executing scripts, allowing for a more flexible approach. The system achieves a state-of-the-art success rate of 60.76% on the OSWorld benchmark, demonstrating significant improvements in both efficiency and task completion speed.'}, 'zh': {'title': 'ç»“åˆç¼–ç¨‹ä¸GUIï¼Œæå‡è‡ªåŠ¨åŒ–æ•ˆç‡ï¼', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸCoAct-1ï¼Œå®ƒç»“åˆäº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ§åˆ¶å’Œç¨‹åºåŒ–æ‰§è¡Œï¼Œä»¥æé«˜å¤æ‚è®¡ç®—æœºè‡ªåŠ¨åŒ–ä»»åŠ¡çš„æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚ä¼ ç»Ÿçš„åŸºäºGUIçš„æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒCoAct-1é€šè¿‡å¼•å…¥ç¼–ç¨‹ä½œä¸ºå¢å¼ºåŠ¨ä½œï¼Œå…‹æœäº†è¿™ä¸€é™åˆ¶ã€‚è¯¥ç³»ç»Ÿçš„åè°ƒè€…åŠ¨æ€åˆ†é…å­ä»»åŠ¡ç»™GUIæ“ä½œå‘˜æˆ–ä¸“é—¨çš„ç¨‹åºå‘˜æ™ºèƒ½ä½“ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„ä»»åŠ¡å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoAct-1åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†60.76%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¹³å‡å®Œæˆä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤å‡å°‘åˆ°10.15æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02120', 'title': "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning\n  Models", 'url': 'https://huggingface.co/papers/2508.02120', 'abstract': 'Research on efficient reasoning methods for Large Reasoning Models (LRMs) aims to reduce reasoning path length without sacrificing performance, through single-model optimization and model collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods.', 'score': 7, 'issue_id': 5243, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'ab1d76b8f9fbefe8', 'authors': ['Linan Yue', 'Yichao Du', 'Yizhi Wang', 'Weibo Gao', 'Fangzhou Yao', 'Li Wang', 'Ye Liu', 'Ziyu Xu', 'Qi Liu', 'Shimin Di', 'Min-Ling Zhang'], 'affiliations': ['Alibaba Group', 'Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education', 'School of Computer Science and Engineering, Southeast University', 'University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2508.02120.jpg', 'data': {'categories': ['#reasoning', '#training', '#open_source', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LRM: ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (LRM) Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LRM Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Streamlining Reasoning: Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the development of efficient reasoning methods for Large Reasoning Models (LRMs), which are designed to improve logical deduction and decision-making. It highlights the challenges posed by overly long reasoning paths that can hinder performance and accuracy. The authors categorize existing research into two main approaches: optimizing single models for better reasoning efficiency and enhancing collaboration between multiple models. Additionally, they provide a public GitHub repository to share ongoing advancements in this area.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼šæå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ™ºèƒ½å†³ç­–èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­é«˜æ•ˆæ¨ç†æ–¹æ³•çš„ç ”ç©¶ï¼Œæ—¨åœ¨åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘æ¨ç†è·¯å¾„çš„é•¿åº¦ã€‚ç ”ç©¶ä¸­æåˆ°çš„DeepSeek R1æ¨¡å‹å› å…¶å“è¶Šçš„è¡¨ç°å’Œå¼€æºç‰¹æ€§è€Œå—åˆ°å…³æ³¨ï¼Œæ¨åŠ¨äº†R1é£æ ¼LRMsçš„ç ”ç©¶è¿›å±•ã€‚ä¸ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šè¿‡é•¿é“¾æ¨ç†å’Œè‡ªæˆ‘åæ€ç­‰æœºåˆ¶å¢å¼ºäº†é€»è¾‘æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€åº”ç”¨çš„å¹¿æ³›ï¼Œè¿‡åº¦æ¨ç†çš„é—®é¢˜é€æ¸æ˜¾ç°ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡é™ä½ï¼Œå› æ­¤æå‡ºäº†å¤šç§é«˜æ•ˆæ¨ç†æ–¹æ³•ä»¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02038', 'title': 'Marco-Voice Technical Report', 'url': 'https://huggingface.co/papers/2508.02038', 'abstract': 'A multifunctional speech synthesis system integrates voice cloning and emotion control using speaker-emotion disentanglement and rotational emotional embeddings, achieving high expressive and natural speech.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis.', 'score': 7, 'issue_id': 5247, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'a3fa663a271be79f', 'authors': ['Fengping Tian', 'Chenyang Lyu', 'Xuanfan Ni', 'Haoqin Sun', 'Qingjuan Li', 'Zhiqiang Qian', 'Haijun Li', 'Longyue Wang', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2508.02038.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸: ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CSEMOTIONS Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 10 Ñ‡Ğ°ÑĞ¾Ğ² ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Marco-Voice Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'Expressive Speech Synthesis with Emotion Control', 'desc': "This paper introduces a new speech synthesis system called Marco-Voice that combines voice cloning with emotion control. It uses a technique called speaker-emotion disentanglement to separate the speaker's identity from their emotional expression, allowing for more flexible and natural speech generation. The system incorporates rotational emotional embeddings to enable smooth transitions between different emotions. A new dataset, CSEMOTIONS, was created to train and evaluate the system, showing significant improvements in speech clarity and emotional depth compared to existing methods."}, 'zh': {'title': 'å¤šåŠŸèƒ½è¯­éŸ³åˆæˆï¼šå£°éŸ³ä¸æƒ…æ„Ÿçš„å®Œç¾ç»“åˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤šåŠŸèƒ½è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œç»“åˆäº†å£°éŸ³å…‹éš†å’Œæƒ…æ„Ÿæ§åˆ¶ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¯´è¯è€…-æƒ…æ„Ÿè§£è€¦æœºåˆ¶å’Œæ—‹è½¬æƒ…æ„ŸåµŒå…¥æ–¹æ³•ï¼Œå®ç°äº†é«˜è¡¨ç°åŠ›å’Œè‡ªç„¶çš„è¯­éŸ³ç”Ÿæˆã€‚ç ”ç©¶ä¸­æ„å»ºäº†CSEMOTIONSæ•°æ®é›†ï¼ŒåŒ…å«å…­ä½ä¸“ä¸šè¯´è¯è€…çš„åå°æ—¶æ™®é€šè¯æƒ…æ„Ÿè¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMarco-Voiceåœ¨è¯­éŸ³æ¸…æ™°åº¦å’Œæƒ…æ„Ÿä¸°å¯Œæ€§æ–¹é¢æ˜¾è‘—æå‡ï¼Œä»£è¡¨äº†è¡¨è¾¾æ€§ç¥ç»è¯­éŸ³åˆæˆé¢†åŸŸçš„é‡è¦è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04423', 'title': 'Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation', 'url': 'https://huggingface.co/papers/2508.04423', 'abstract': 'A structured framework and datasets for training customer service agents using well-defined support strategies improve the quality of customer support interactions and problem resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.', 'score': 6, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': 'd1f778b32418973c', 'authors': ['Jie Zhu', 'Huaixia Dou', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04423.jpg', 'data': {'categories': ['#data', '#agents', '#science', '#dataset', '#open_source', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Customer Support Conversation (CSC), Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ğ¸ Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: CSConv Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ RoleCS Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ fine-tuning LLM Ğ½Ğ° RoleCS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Customer Support with Strategic Conversations', 'desc': 'This paper presents a structured framework for training customer service agents, focusing on effective communication and problem resolution. It introduces the Customer Support Conversation (CSC) task, which utilizes well-defined support strategies based on COPC guidelines. The authors create a dataset called CSConv, consisting of real-world conversations rewritten to reflect strategic communication, and a training dataset called RoleCS that simulates these interactions. Experiments demonstrate that fine-tuning large language models (LLMs) on RoleCS enhances their ability to produce high-quality, strategy-aligned responses, leading to improved customer support outcomes.'}, 'zh': {'title': 'æå‡å®¢æˆ·æ”¯æŒè´¨é‡çš„ç»“æ„åŒ–æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå®¢æˆ·æœåŠ¡ä»£ç†ï¼Œé‡‡ç”¨æ˜ç¡®çš„æ”¯æŒç­–ç•¥ä»¥æé«˜å®¢æˆ·æ”¯æŒäº’åŠ¨çš„è´¨é‡å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†å®¢æˆ·æ”¯æŒå¯¹è¯ï¼ˆCSCï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¸®åŠ©å®¢æœä»£ç†ä½¿ç”¨å®šä¹‰è‰¯å¥½çš„æ”¯æŒç­–ç•¥è¿›è¡Œå“åº”ã€‚åŸºäºCOPCæŒ‡å—ï¼Œæˆ‘ä»¬å®šä¹‰äº†äº”ä¸ªå¯¹è¯é˜¶æ®µå’ŒåäºŒç§ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼é«˜è´¨é‡çš„äº’åŠ¨ã€‚é€šè¿‡æ„å»ºCSConvæ•°æ®é›†å’ŒRoleCSè®­ç»ƒæ•°æ®é›†ï¼Œå®éªŒè¡¨æ˜åœ¨RoleCSä¸Šå¾®è°ƒå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾è‘—æå‡äº†å…¶åœ¨CSConvä¸Šçš„ç­–ç•¥ä¸€è‡´æ€§å“åº”èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05496', 'title': 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities', 'url': 'https://huggingface.co/papers/2508.05496', 'abstract': "InfiAlign, a scalable and sample-efficient post-training framework, combines supervised fine-tuning and Direct Preference Optimization to enhance large language models' reasoning abilities with minimal data and computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.", 'score': 5, 'issue_id': 5246, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '6dcac22b35678a50', 'authors': ['Shuo Cai', 'Su Lu', 'Qi Zhou', 'Kejing Yang', 'Zhijie Sang', 'Congkai Xie', 'Hongxia Yang'], 'affiliations': ['InfiX.ai', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05496.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#optimization', '#rlhf', '#training', '#math', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'InfiAlign: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'InfiAlign - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ğ¸ Direct Preference Optimization Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ InfiAlign Ğ»ĞµĞ¶Ğ¸Ñ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ InfiAlign Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-7B-Base Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ DeepSeek-R1-Distill-Qwen-7B, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 12% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Efficiently Enhancing Reasoning in Large Language Models', 'desc': 'InfiAlign is a new framework designed to improve the reasoning abilities of large language models (LLMs) while using less data and computational resources. It combines supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to enhance model performance efficiently. The framework includes a smart data selection process that curates high-quality training data from existing datasets, which helps achieve better results with significantly less data. When tested on the Qwen2.5-Math-7B-Base model, InfiAlign demonstrated strong performance improvements, especially in mathematical reasoning tasks, while using only about 12% of the usual training data.'}, 'zh': {'title': 'é«˜æ•ˆå¯¹é½ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'InfiAlignæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”æ ·æœ¬é«˜æ•ˆçš„åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æœ€å°çš„æ•°æ®å’Œè®¡ç®—æˆæœ¬æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚InfiAlignçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®é€‰æ‹©ç®¡é“ï¼Œè‡ªåŠ¨ä»å¼€æºæ¨ç†æ•°æ®é›†ä¸­ç­›é€‰é«˜è´¨é‡çš„å¯¹é½æ•°æ®ï¼Œä»è€Œæ˜¾è‘—æé«˜æ€§èƒ½å¹¶å‡å°‘æ•°æ®éœ€æ±‚ã€‚é€šè¿‡åœ¨Qwen2.5-Math-7B-Baseæ¨¡å‹ä¸Šçš„åº”ç”¨ï¼ŒInfiAlignå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–æ¨ç†ä»»åŠ¡ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05630', 'title': 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes', 'url': 'https://huggingface.co/papers/2508.05630', 'abstract': 'MOSEv2, a more challenging dataset, highlights the limitations of current VOS methods in real-world scenarios with increased complexity and diverse challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.', 'score': 4, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': 'a2a9fbee6a3ffe42', 'authors': ['Henghui Ding', 'Kaining Ying', 'Chang Liu', 'Shuting He', 'Xudong Jiang', 'Yu-Gang Jiang', 'Philip H. S. Torr', 'Song Bai'], 'affiliations': ['ByteDance Inc', 'Fudan University, Shanghai, China', 'Nanyang Technological University, Singapore', 'Shanghai University of Finance and Economics, China', 'University of Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2508.05630.jpg', 'data': {'categories': ['#dataset', '#video', '#benchmark'], 'emoji': 'ğŸ¥', 'ru': {'title': 'MOSEv2: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'MOSEv2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (VOS), ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5000 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸, Ğ½ĞµĞ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² VOS Ğ½Ğ° MOSEv2 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'MOSEv2: Elevating Video Object Segmentation to Real-World Challenges', 'desc': 'The paper introduces MOSEv2, a new dataset for video object segmentation (VOS) that presents more complex real-world challenges than previous datasets. While existing methods perform well on simpler benchmarks like DAVIS and YouTube-VOS, they struggle with the increased difficulties found in MOSEv2, which includes diverse scenarios such as occlusions, object disappearance, and adverse weather conditions. The dataset contains over 5,000 videos and nearly 702,000 high-quality masks for a wide variety of objects, making it a significant resource for advancing VOS research. Benchmarking shows that current VOS methods experience substantial performance drops when tested on MOSEv2, indicating a need for improved algorithms that can handle real-world complexities.'}, 'zh': {'title': 'MOSEv2ï¼šåº”å¯¹çœŸå®ä¸–ç•Œå¤æ‚æ€§çš„æŒ‘æˆ˜', 'desc': 'MOSEv2æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œæ­ç¤ºäº†å½“å‰è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆVOSï¼‰æ–¹æ³•åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨DAVISå’ŒYouTube-VOSç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è¿™äº›æ•°æ®é›†ä¸»è¦åŒ…å«æ˜¾è‘—ã€ä¸»å¯¼å’Œå­¤ç«‹çš„å¯¹è±¡ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚MOSEv2åŒ…å«5024ä¸ªè§†é¢‘å’Œè¶…è¿‡701976ä¸ªé«˜è´¨é‡çš„æ©è†œï¼Œæ¶µç›–200ä¸ªç±»åˆ«çš„10074ä¸ªå¯¹è±¡ï¼Œåœºæ™¯å¤æ‚æ€§æ˜¾è‘—å¢åŠ ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡åœ¨ç°æœ‰æ•°æ®é›†ä¸Šå‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†å½“å‰çš„VOSæ–¹æ³•åœ¨é¢å¯¹çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01650', 'title': 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance', 'url': 'https://huggingface.co/papers/2508.01650', 'abstract': 'A sketch-based strand generation model using a learnable upsampling strategy and multi-scale adaptive conditioning mechanism outperforms existing methods in realism and precision for hair strand generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).', 'score': 4, 'issue_id': 5243, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': '377e9e9eca3593db', 'authors': ['Na Zhang', 'Moran Li', 'Chengming Xu', 'Han Feng', 'Xiaobin Hu', 'Jiangning Zhang', 'Weijian Cao', 'Chengjie Wang', 'Yanwei Fu'], 'affiliations': ['Fudan University, Shanghai, China', 'School of Data Science, Fudan University, Shanghai Innovation Institute, Institute of Trustworthy Embodied AI, Fudan University', 'Tencent YouTu Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01650.jpg', 'data': {'categories': ['#benchmark', '#games', '#cv', '#3d', '#open_source', '#architecture'], 'emoji': 'ğŸ’‡', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ¿Ğ¾ ÑÑĞºĞ¸Ğ·Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞºĞ¸Ğ·Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ¿ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€ÑĞ´ĞµĞ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ»Ğ¾Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Hair Strand Generation with Sketch-Based Precision', 'desc': 'This paper presents a novel sketch-based model for generating realistic hair strands, addressing the limitations of existing methods. The model utilizes a learnable upsampling strategy to effectively encode 3D hair strands into multi-scale latent spaces, enhancing detail and precision. Additionally, it incorporates a multi-scale adaptive conditioning mechanism that employs transformers with diffusion heads to maintain consistency across different levels of detail. Experimental results demonstrate that this approach significantly improves realism and precision in hair strand generation compared to traditional techniques.'}, 'zh': {'title': 'è‰å›¾é©±åŠ¨çš„å‘ä¸ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‰å›¾çš„å‘ä¸ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·ç­–ç•¥å’Œå¤šå°ºåº¦è‡ªé€‚åº”æ¡ä»¶æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†å‘ä¸ç”Ÿæˆçš„çœŸå®æ„Ÿå’Œç²¾ç¡®åº¦ã€‚è¯¥æ¨¡å‹è§£å†³äº†å¤æ‚å‘ä¸äº¤äº’å’Œå¤šæ ·åŒ–è‰å›¾æ¨¡å¼å»ºæ¨¡çš„å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡å°†3Då‘ä¸ç¼–ç åˆ°å¤šå°ºåº¦æ½œåœ¨ç©ºé—´ï¼Œæ¨¡å‹å®ç°äº†æ›´ç»†è‡´çš„æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒç”¨æˆ·å‹å¥½æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04979', 'title': 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast\n  Image Compression', 'url': 'https://huggingface.co/papers/2508.04979', 'abstract': 'SODEC, a single-step diffusion image compression model, enhances decoding speed and fidelity by using a pre-trained VAE and a fidelity guidance module.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20times. Code is released at: https://github.com/zhengchen1999/SODEC.', 'score': 3, 'issue_id': 5251, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '86b2291887bd3cbc', 'authors': ['Zheng Chen', 'Mingde Zhou', 'Jinpei Guo', 'Jiale Yuan', 'Yifei Ji', 'Yulun Zhang'], 'affiliations': ['Carnegie Mellon University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04979.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#open_source', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'SODEC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ñƒ. SODEC Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ-Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ.'}, 'en': {'title': 'SODEC: Fast and Faithful Image Compression with Single-Step Diffusion', 'desc': 'SODEC is a new image compression model that uses a single-step diffusion process to enhance both decoding speed and image quality. It utilizes a pre-trained Variational Autoencoder (VAE) to generate informative latent representations, eliminating the need for multiple sampling steps. To further improve the fidelity of the compressed images, a fidelity guidance module is introduced, ensuring that the output closely resembles the original image. The model also incorporates a rate annealing training strategy, allowing it to perform effectively even at very low bitrates, resulting in significant improvements over existing methods.'}, 'zh': {'title': 'SODECï¼šå¿«é€Ÿé«˜ä¿çœŸçš„å›¾åƒå‹ç¼©æ–°æ–¹æ³•', 'desc': 'SODECæ˜¯ä¸€ç§å•æ­¥æ‰©æ•£å›¾åƒå‹ç¼©æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œä¿çœŸåº¦å¼•å¯¼æ¨¡å—ï¼Œæé«˜äº†è§£ç é€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚è¯¥æ¨¡å‹è§£å†³äº†ä¼ ç»Ÿæ‰©æ•£å›¾åƒå‹ç¼©æ–¹æ³•ä¸­å­˜åœ¨çš„è§£ç å»¶è¿Ÿè¿‡é«˜å’Œå›¾åƒä¿çœŸåº¦ä¸è¶³çš„é—®é¢˜ã€‚SODECåˆ©ç”¨ä¿¡æ¯ä¸°å¯Œçš„æ½œåœ¨è¡¨ç¤ºï¼Œé¿å…äº†å¤šæ­¥é‡‡æ ·çš„å¤æ‚æ€§ï¼Œå¹¶é€šè¿‡å•æ­¥è§£ç æ¥åŠ é€Ÿè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œä¿çœŸåº¦å¼•å¯¼æ¨¡å—ç¡®ä¿è¾“å‡ºå›¾åƒä¸åŸå§‹å›¾åƒä¿æŒä¸€è‡´ï¼Œä»è€Œæå‡äº†å‹ç¼©æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05618', 'title': 'Learning to Reason for Factuality', 'url': 'https://huggingface.co/papers/2508.05618', 'abstract': 'A novel reward function for online reinforcement learning improves factuality and detail in reasoning large language models without reducing helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness.', 'score': 2, 'issue_id': 5261, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '97c0924a378da917', 'authors': ['Xilun Chen', 'Ilia Kulikov', 'Vincent-Pierre Berges', 'Barlas OÄŸuz', 'Rulin Shao', 'Gargi Ghosh', 'Jason Weston', 'Wen-tau Yih'], 'affiliations': ['FAIR at Meta', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.05618.jpg', 'data': {'categories': ['#rl', '#rlhf', '#hallucinations', '#reasoning', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Factuality and Detail in Language Models with Novel RL Rewards', 'desc': 'This paper introduces a new reward function for online reinforcement learning (RL) that enhances the factual accuracy and detail of reasoning in large language models (LLMs). The authors address the challenge of high hallucination rates in LLMs when generating long-form content, which often leads to inaccuracies. By developing a reward function that balances factual precision, detail, and relevance, they improve the quality of responses without sacrificing helpfulness. The proposed model shows significant improvements in factuality and detail across multiple benchmarks, demonstrating its effectiveness in reducing errors while maintaining user satisfaction.'}, 'zh': {'title': 'æå‡æ¨ç†æ¨¡å‹çš„äº‹å®æ€§ä¸ç»†èŠ‚æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¥–åŠ±å‡½æ•°ï¼Œç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§å’Œæ¨ç†ç»†èŠ‚ï¼Œè€Œä¸é™ä½å…¶æœ‰ç”¨æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è‡ªåŠ¨äº‹å®æ€§è¯„ä¼°æ¡†æ¶åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­ç›´æ¥ä½œä¸ºå¥–åŠ±ä½¿ç”¨ï¼Œä¼šå¯¼è‡´å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œå½±å“ç”Ÿæˆçš„å›ç­”è´¨é‡ã€‚æˆ‘ä»¬æå‡ºçš„å¥–åŠ±å‡½æ•°åŒæ—¶è€ƒè™‘äº†äº‹å®ç²¾åº¦ã€å›ç­”ç»†èŠ‚æ°´å¹³å’Œç­”æ¡ˆç›¸å…³æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°å­¦ä¹ é«˜è´¨é‡çš„äº‹å®æ¨ç†ã€‚ç»è¿‡å…­ä¸ªé•¿ç¯‡äº‹å®æ€§åŸºå‡†çš„è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¹»è§‰ç‡ä¸Šå¹³å‡é™ä½äº†23.1ä¸ªç™¾åˆ†ç‚¹ï¼Œå›ç­”ç»†èŠ‚æ°´å¹³æé«˜äº†23%ï¼Œä¸”æ•´ä½“å›ç­”çš„æœ‰ç”¨æ€§æ²¡æœ‰ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03404', 'title': 'Visual Document Understanding and Question Answering: A Multi-Agent\n  Collaboration Framework with Test-Time Scaling', 'url': 'https://huggingface.co/papers/2508.03404', 'abstract': 'MACT, a Multi-Agent Collaboration framework with Test-Time scaling, enhances visual document understanding and VQA by using four specialized agents and mixed reward modeling, achieving superior performance with reduced parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git.', 'score': 2, 'issue_id': 5252, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '500ce3c7788fd634', 'authors': ['Xinlei Yu', 'Zhangquan Chen', 'Yudong Zhang', 'Shilin Lu', 'Ruolin Shen', 'Jiangning Zhang', 'Xiaobin Hu', 'Yanwei Fu', 'Shuicheng Yan'], 'affiliations': ['Fudan University, China', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore', 'Technical University of Munich, Germany', 'Tsinghua University, China', 'University of Science and Technology of China, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03404.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark', '#small_models', '#cv', '#long_context'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MACT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼. MACT Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'MACT: Enhancing Document Understanding with Smart Agent Collaboration', 'desc': "MACT is a Multi-Agent Collaboration framework designed to improve visual document understanding and visual question answering (VQA). It utilizes four specialized agentsâ€”planning, execution, judgment, and answer agentsâ€”that work together to enhance performance while maintaining a smaller parameter scale. The judgment agent plays a crucial role by verifying answers and prompting revisions from other agents, which leads to better accuracy compared to traditional methods. Additionally, MACT employs mixed reward modeling and hybrid test-time scaling to optimize each agent's performance based on their specific tasks, resulting in superior outcomes in complex reasoning scenarios."}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“åä½œï¼Œæå‡è§†è§‰ç†è§£ä¸é—®ç­”èƒ½åŠ›', 'desc': 'MACTæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†è§‰æ–‡æ¡£ç†è§£å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„æ€§èƒ½ã€‚å®ƒç”±å››ä¸ªä¸“é—¨çš„å°å‹æ™ºèƒ½ä½“ç»„æˆï¼Œåˆ†åˆ«è´Ÿè´£è§„åˆ’ã€æ‰§è¡Œã€åˆ¤æ–­å’Œå›ç­”ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåä½œã€‚åˆ¤æ–­æ™ºèƒ½ä½“ä¸“æ³¨äºéªŒè¯æ­£ç¡®æ€§ï¼Œå¹¶èƒ½å¼•å¯¼å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œä¿®æ­£ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„çº é”™ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒMACTé‡‡ç”¨æ··åˆå¥–åŠ±å»ºæ¨¡å’Œå¹³è¡¡çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥ï¼Œä½¿å¾—æ¯ä¸ªæ™ºèƒ½ä½“æ ¹æ®å…¶åŠŸèƒ½å®šåˆ¶ä¸åŒçš„ç¼©æ”¾ç­–ç•¥ï¼Œä»è€Œåœ¨å‚æ•°è§„æ¨¡è¾ƒå°çš„æƒ…å†µä¸‹å®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05545', 'title': 'PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction', 'url': 'https://huggingface.co/papers/2508.05545', 'abstract': 'A comprehensive analysis of Large Language Models for PII redaction evaluates various architectures and training strategies, providing guidance for accurate, efficient, and privacy-aware redaction systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Redacting Personally Identifiable Information (PII) from unstructured text is critical for ensuring data privacy in regulated domains. While earlier approaches have relied on rule-based systems and domain-specific Named Entity Recognition (NER) models, these methods fail to generalize across formats and contexts. Recent advances in Large Language Models (LLMs) offer a promising alternative, yet the effect of architectural and training choices on redaction performance remains underexplored. LLMs have demonstrated strong performance in tasks that require contextual language understanding, including the redaction of PII in free-form text. Prior work suggests that with appropriate adaptation, LLMs can become effective contextual privacy learners. However, the consequences of architectural and training choices for PII Redaction remain underexplored. In this work, we present a comprehensive analysis of LLMs as privacy-preserving PII Redaction systems. We evaluate a range of LLM architectures and training strategies for their effectiveness in PII Redaction. Our analysis measures redaction performance, semantic preservation, and PII leakage, and compares these outcomes against latency and computational cost. The results provide practical guidance for configuring LLM-based redactors that are accurate, efficient, and privacy-aware. To support reproducibility and real-world deployment, we release PRvL, an open-source suite of fine-tuned models, and evaluation tools for general-purpose PII Redaction. PRvL is built entirely on open-source LLMs and supports multiple inference settings for flexibility and compliance. It is designed to be easily customized for different domains and fully operable within secure, self-managed environments. This enables data owners to perform redactions without relying on third-party services or exposing sensitive content beyond their own infrastructure.', 'score': 1, 'issue_id': 5244, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': 'dff00258f2cd918b', 'authors': ['Leon Garza', 'Anantaa Kotal', 'Aritran Piplai', 'Lavanya Elluri', 'Prajit Das', 'Aman Chadha'], 'affiliations': ['Amazon Web Services', 'Cisco Systems Inc.', 'Dept. of C.S., The University of Texas at El Paso', 'Texas A&M University-Central Texas'], 'pdf_title_img': 'assets/pdf/title_img/2508.05545.jpg', 'data': {'categories': ['#leakage', '#inference', '#training', '#dataset', '#ethics', '#open_source', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'LLM Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (PII) Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞµĞº PII. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ LLM-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² PRvL Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ PII Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM.'}, 'en': {'title': 'Harnessing LLMs for Effective PII Redaction', 'desc': 'This paper analyzes the use of Large Language Models (LLMs) for redacting Personally Identifiable Information (PII) from unstructured text, which is essential for maintaining data privacy. It highlights the limitations of traditional rule-based and domain-specific Named Entity Recognition (NER) systems, which struggle to adapt to various formats and contexts. The authors evaluate different LLM architectures and training strategies to determine their effectiveness in accurately redacting PII while preserving semantic meaning and minimizing leakage. They also introduce PRvL, an open-source suite of fine-tuned models and tools designed for flexible and secure PII redaction in various domains.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›ä¸ªäººä¿¡æ¯å»æ ‡è¯†åŒ–çš„éšç§ä¿æŠ¤', 'desc': 'æœ¬æ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰å»æ ‡è¯†åŒ–ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢åˆ†æã€‚ç ”ç©¶è¯„ä¼°äº†ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜å»æ ‡è¯†åŒ–çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®éšç§ã€‚é€šè¿‡å¯¹å»æ ‡è¯†åŒ–æ€§èƒ½ã€è¯­ä¹‰ä¿ç•™å’ŒPIIæ³„éœ²çš„æµ‹é‡ï¼Œæä¾›äº†é…ç½®LLMå»æ ‡è¯†åŒ–ç³»ç»Ÿçš„å®ç”¨æŒ‡å¯¼ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤æ€§å’Œå®é™…éƒ¨ç½²ï¼Œæœ¬æ–‡å‘å¸ƒäº†PRvLï¼Œä¸€ä¸ªå¼€æºçš„å¾®è°ƒæ¨¡å‹å’Œè¯„ä¼°å·¥å…·å¥—ä»¶ï¼Œæ—¨åœ¨å¸®åŠ©æ•°æ®æ‹¥æœ‰è€…åœ¨å®‰å…¨ç¯å¢ƒä¸­è¿›è¡Œå»æ ‡è¯†åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04946', 'title': 'REINA: Regularized Entropy Information-Based Loss for Efficient\n  Simultaneous Speech Translation', 'url': 'https://huggingface.co/papers/2508.04946', 'abstract': 'A novel loss function, REINA, optimizes the latency-quality tradeoff in Simultaneous Speech Translation by adaptively waiting for more input based on information gain.  \t\t\t\t\tAI-generated summary \t\t\t\t Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.', 'score': 1, 'issue_id': 5245, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': 'aa0ec666492a04d2', 'authors': ['Nameer Hirschkind', 'Joseph Liu', 'Mahesh Kumar Nandwana', 'Xiao Yu'], 'affiliations': ['Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2508.04946.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#synthetic', '#optimization', '#audio', '#machine_translation', '#training'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ REINA Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸. REINA Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Optimizing Translation: REINA for Better Latency and Quality', 'desc': 'This paper presents a new loss function called REINA, designed to improve Simultaneous Speech Translation (SimulST) systems by optimizing the balance between translation quality and latency. REINA works by adaptively waiting for additional input only when it is expected to enhance the translation quality, based on the concept of information gain. The authors demonstrate that using REINA allows for training a SimulST model that achieves state-of-the-art performance in translating between English and languages like French, Spanish, and German. The results show a significant improvement in the latency-quality tradeoff, achieving up to 21% better performance compared to previous methods.'}, 'zh': {'title': 'ä¼˜åŒ–å»¶è¿Ÿä¸è´¨é‡çš„REINAæŸå¤±å‡½æ•°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æŸå¤±å‡½æ•°REINAï¼Œç”¨äºä¼˜åŒ–åŒæ—¶è¯­éŸ³ç¿»è¯‘ä¸­çš„å»¶è¿Ÿä¸è´¨é‡çš„æƒè¡¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ ¹æ®ä¿¡æ¯å¢ç›Šè‡ªé€‚åº”åœ°ç­‰å¾…æ›´å¤šè¾“å…¥ï¼Œä»è€Œæé«˜ç¿»è¯‘è´¨é‡ã€‚REINAåŸºäºä¿¡æ¯ç†è®ºåŸç†ï¼Œèƒ½å¤Ÿè®­ç»ƒå‡ºä¸€ç§è‡ªé€‚åº”ç­–ç•¥ï¼Œè¶…è¶Šäº†ä»¥å¾€æ–¹æ³•çš„å»¶è¿Ÿ/è´¨é‡å¸•ç´¯æ‰˜å‰æ²¿ã€‚é€šè¿‡ä½¿ç”¨REINAï¼Œæˆ‘ä»¬åœ¨æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­çš„åŒæ—¶ç¿»è¯‘æ¨¡å‹ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºREINAåœ¨å»¶è¿Ÿå’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡æ”¹å–„äº†å¤šè¾¾21%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04939', 'title': 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating\n  Linguistic Shibboleth Detection in LLM Hiring Evaluations', 'url': 'https://huggingface.co/papers/2508.04939', 'abstract': "A benchmark evaluates Large Language Models' response to linguistic markers that reveal demographic attributes, demonstrating systematic penalization of hedging language despite equivalent content quality.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts.", 'score': 1, 'issue_id': 5244, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': 'c43270050aadf2f5', 'authors': ['Julia Kharchenko', 'Tanya Roosta', 'Aman Chadha', 'Chirag Shah'], 'affiliations': ['Stanford University, Amazon GenAI, Palo Alto, CA, USA', 'UC Berkeley, Amazon, Saratoga, CA, USA', 'University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.04939.jpg', 'data': {'categories': ['#benchmark', '#ethics'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Uncovering Bias: How Language Shapes AI Judgments', 'desc': 'This paper presents a benchmark designed to assess how Large Language Models (LLMs) react to subtle linguistic cues that can indicate demographic characteristics. The study reveals that LLMs tend to penalize hedging language, which is a way of expressing uncertainty, even when the content quality remains the same. By using controlled variations in language while keeping the meaning intact, the researchers can accurately measure biases related to demographics in AI evaluations. The findings highlight a significant 25.6% reduction in ratings for hedged responses, underscoring the need for fairness in AI systems.'}, 'zh': {'title': 'æ­ç¤ºè¯­è¨€åè§çš„åŸºå‡†è¯„ä¼°', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è¯­è¨€æ ‡è®°ååº”çš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å¯ä»¥æ­ç¤ºäººå£å±æ€§çš„ç»†å¾®è¯­è¨€ç‰¹å¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å†…å®¹è´¨é‡ç›¸å½“ï¼ŒLLMså¯¹æŸäº›è¯­è¨€æ¨¡å¼ï¼Œå°¤å…¶æ˜¯æ¨¡ç³Šè¯­è¨€ï¼Œå­˜åœ¨ç³»ç»Ÿæ€§çš„æƒ©ç½šã€‚é€šè¿‡æ„å»º100ä¸ªç»è¿‡éªŒè¯çš„é—®é¢˜-å›ç­”å¯¹ï¼Œè®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åœ¨ä¿æŒè¯­ä¹‰ç­‰ä»·çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆå—æ§çš„è¯­è¨€å˜ä½“ï¼Œä»¥ç²¾ç¡®æµ‹é‡è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿä¸­çš„äººå£åè§ã€‚è¯¥ç ”ç©¶ä¸ºæ£€æµ‹å’Œæµ‹é‡äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„è¯­è¨€æ­§è§†å»ºç«‹äº†åŸºç¡€æ¡†æ¶ï¼Œå…·æœ‰å¹¿æ³›çš„å…¬å¹³æ€§åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04699', 'title': 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during\n  Multi-Hop Analysis', 'url': 'https://huggingface.co/papers/2508.04699', 'abstract': 'Research investigates reasoning failures in language models for multi-hop question answering, introducing a framework to categorize errors and improve model fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved ("hops"), completeness in capturing relevant information ("coverage"), and cognitive inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.', 'score': 1, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '97e95e6b08388e4f', 'authors': ['Anushka Yadav', 'Isha Nalawade', 'Srujana Pillarichety', 'Yashwanth Babu', 'Reshmi Ghosh', 'Samyadeep Basu', 'Wenlong Zhao', 'Ali Nasaeh', 'Sriram Balasubramanian', 'Soundararajan Srinivasan'], 'affiliations': ['Microsoft', 'University of Maryland, College Park', 'University of Massachusetts, Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2508.04699.jpg', 'data': {'categories': ['#data', '#benchmark', '#reasoning', '#hallucinations', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ³Ğ°Ğ´ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿ÑƒÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unraveling Reasoning Failures in Language Models', 'desc': 'This paper investigates the reasoning failures of language models specifically in multi-hop question answering tasks. It introduces a new framework to categorize these errors based on three dimensions: the diversity of source documents, the completeness of relevant information, and cognitive inefficiency. The study uses human annotation and automated metrics to reveal complex error patterns that are often overlooked in traditional accuracy evaluations. The findings aim to enhance the understanding of cognitive limitations in language models and provide guidance for improving their reasoning capabilities.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤šè·³é—®ç­”ä¸­çš„æ¨ç†å¤±è´¥ï¼Œæå‡ºäº†ä¸€ç§æ¡†æ¶æ¥åˆ†ç±»é”™è¯¯å¹¶æé«˜æ¨¡å‹çš„å¯é æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜æ—¶ï¼Œå¸¸å¸¸å‡ºç°å¹»è§‰ç°è±¡ï¼Œç¼ºä¹å¯¹é”™è¯¯åŸå› çš„å…¨é¢ç†è§£ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„é”™è¯¯åˆ†ç±»æ¡†æ¶ï¼Œä»æºæ–‡æ¡£çš„å¤šæ ·æ€§ã€ä¿¡æ¯æ•æ‰çš„å®Œæ•´æ€§å’Œè®¤çŸ¥æ•ˆç‡ä¸‰ä¸ªç»´åº¦è¿›è¡Œåˆ†æã€‚é€šè¿‡ä¸¥æ ¼çš„äººç±»æ ‡æ³¨å’Œè‡ªåŠ¨åŒ–æŒ‡æ ‡çš„æ”¯æŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†éšè—åœ¨å‡†ç¡®æ€§è¯„ä¼°èƒŒåçš„å¤æ‚é”™è¯¯æ¨¡å¼ï¼Œä¸ºæœªæ¥è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†å¯è¡Œçš„æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04190', 'title': 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation', 'url': 'https://huggingface.co/papers/2508.04190', 'abstract': 'RPCANet++ combines RPCA with deep learning to achieve efficient and interpretable sparse object segmentation by introducing modules for background approximation, object extraction, and image restoration.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.', 'score': 1, 'issue_id': 5247, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '38f7da6b7ad16bbf', 'authors': ['Fengyi Wu', 'Yimian Dai', 'Tianfang Zhang', 'Yixuan Ding', 'Jian Yang', 'Ming-Ming Cheng', 'Zhenming Peng'], 'affiliations': ['Department of Automation, Tsinghua University, Beijing, China', 'PCA Lab, VCIP, College of Computer Science, Nankai University, Tianjin 300350, China', 'School of Information and Communication Engineering and the Laboratory of Imaging Detection and Intelligent Perception, University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04190.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#cv', '#interpretability', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'RPCANet++: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'RPCANet++ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ (RPCA) Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ°, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPCANet++ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Efficient and Interpretable Sparse Object Segmentation with RPCANet++', 'desc': 'RPCANet++ is a novel framework that integrates Robust Principal Component Analysis (RPCA) with deep learning techniques to enhance sparse object segmentation. It introduces specialized modules for background approximation, object extraction, and image restoration, addressing the computational challenges of traditional RPCA methods. By incorporating a Memory-Augmented Module (MAM) and a Deep Contrast Prior Module (DCPM), the framework improves feature preservation and accelerates object extraction. Extensive experiments show that RPCANet++ achieves superior performance across various imaging scenarios while maintaining interpretability through low-rankness and sparsity metrics.'}, 'zh': {'title': 'ç»“åˆRPCAä¸æ·±åº¦å­¦ä¹ ï¼Œå®ç°é«˜æ•ˆå¯è§£é‡Šçš„ç¨€ç–ç‰©ä½“åˆ†å‰²', 'desc': 'RPCANet++ æ˜¯ä¸€ç§ç»“åˆäº†é²æ£’ä¸»æˆåˆ†åˆ†æï¼ˆRPCAï¼‰å’Œæ·±åº¦å­¦ä¹ çš„ç¨€ç–ç‰©ä½“åˆ†å‰²æ¡†æ¶ã€‚å®ƒé€šè¿‡å¼•å…¥èƒŒæ™¯è¿‘ä¼¼æ¨¡å—ã€ç‰©ä½“æå–æ¨¡å—å’Œå›¾åƒæ¢å¤æ¨¡å—ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯è§£é‡Šçš„åˆ†å‰²æ•ˆæœã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿ RPCA æ¨¡å‹åœ¨è®¡ç®—å’Œé€‚åº”æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒRPCANet++ åœ¨ä¸åŒçš„æˆåƒåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†æ–°çš„å¯é å’Œå¯è§£é‡Šçš„ç¨€ç–ç‰©ä½“åˆ†å‰²åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04107', 'title': 'Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decode', 'url': 'https://huggingface.co/papers/2508.04107', 'abstract': 'MLLMSeg integrates MLLM vision encoder and LLM features with a lightweight mask decoder to achieve high accuracy in reference expression segmentation with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.', 'score': 1, 'issue_id': 5259, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '57990322bc3ce2a9', 'authors': ['Jingchao Wang', 'Zhijian Wu', 'Dingjiang Huang', 'Yefeng Zheng', 'Hong Wang'], 'affiliations': ['Medical Artificial Intelligence Laboratory, Westlake University', 'School of Data Science and Engineering, East China Normal University', 'School of Life Science and Technology, Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04107.jpg', 'data': {'categories': ['#games', '#multimodal', '#optimization', '#architecture', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MLLMSeg - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¼Ğ°ÑĞ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· LLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº. MLLMSeg Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLMSeg Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº SAM-based, Ñ‚Ğ°Ğº Ğ¸ SAM-free ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Efficient and Accurate Reference Expression Segmentation with MLLMSeg', 'desc': 'MLLMSeg is a novel framework designed for reference expression segmentation (RES) that combines the strengths of multimodal large models (MLLMs) with a lightweight mask decoder. It effectively utilizes the visual features from the MLLM vision encoder while avoiding the need for an additional visual encoder, which helps reduce computational costs. The framework introduces a detail-enhanced and semantic-consistent feature fusion module (DSFF) that merges visual and semantic features for improved accuracy. Experimental results show that MLLMSeg outperforms existing methods, achieving high accuracy in segmentation while maintaining a low parameter count.'}, 'zh': {'title': 'é«˜æ•ˆç²¾å‡†çš„å‚è€ƒè¡¨è¾¾åˆ†å‰²æ–°æ–¹æ³•', 'desc': 'MLLMSegæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†è§‰ç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç‰¹å¾ï¼Œä½¿ç”¨è½»é‡çº§çš„æ©ç è§£ç å™¨æ¥å®ç°é«˜ç²¾åº¦çš„å‚è€ƒè¡¨è¾¾åˆ†å‰²ã€‚è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨äº†MLLMè§†è§‰ç¼–ç å™¨ä¸­å›ºæœ‰çš„è§†è§‰ç»†èŠ‚ç‰¹å¾ï¼Œè€Œæ— éœ€å¼•å…¥é¢å¤–çš„è§†è§‰ç¼–ç å™¨ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç»†èŠ‚å¢å¼ºå’Œè¯­ä¹‰ä¸€è‡´çš„ç‰¹å¾èåˆæ¨¡å—ï¼ˆDSFFï¼‰ï¼Œå°†ä¸ç»†èŠ‚ç›¸å…³çš„è§†è§‰ç‰¹å¾ä¸LLMè¾“å‡ºçš„è¯­ä¹‰ç‰¹å¾è¿›è¡Œå…¨é¢æ•´åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMSegåœ¨æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºäºSAMå’ŒéSAMçš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02243', 'title': 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking', 'url': 'https://huggingface.co/papers/2508.02243', 'abstract': 'A novel LLM-based framework, Intra- and Inter-modal Collaborative Reflections, enhances multimodal entity linking by prioritizing text and using iterative visual clues when necessary, outperforming current state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal entity linking plays a crucial role in a wide range of applications. Recent advances in large language model-based methods have become the dominant paradigm for this task, effectively leveraging both textual and visual modalities to enhance performance. Despite their success, these methods still face two challenges, including unnecessary incorporation of image data in certain scenarios and the reliance only on a one-time extraction of visual features, which can undermine their effectiveness and accuracy. To address these challenges, we propose a novel LLM-based framework for the multimodal entity linking task, called Intra- and Inter-modal Collaborative Reflections. This framework prioritizes leveraging text information to address the task. When text alone is insufficient to link the correct entity through intra- and inter-modality evaluations, it employs a multi-round iterative strategy that integrates key visual clues from various aspects of the image to support reasoning and enhance matching accuracy. Extensive experiments on three widely used public datasets demonstrate that our framework consistently outperforms current state-of-the-art methods in the task, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. Our code is available at https://github.com/ziyan-xiaoyu/I2CR/.', 'score': 1, 'issue_id': 5249, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '72638e0c336c8711', 'authors': ['Ziyan Liu', 'Junwen Li', 'Kaiwen Li', 'Tong Ruan', 'Chao Wang', 'Xinyan He', 'Zongyu Wang', 'Xuezhi Cao', 'Jingping Liu'], 'affiliations': ['East China University of Science and Technology, Shanghai, China', 'Meituan, Shanghai, China', 'Shanghai University, Shanghai, China', 'South China University of Technology, Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02243.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#multimodal'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ 'Ğ’Ğ½ÑƒÑ‚Ñ€Ğ¸- Ğ¸ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ' Ğ¸ Ğ¾Ñ‚Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."}, 'en': {'title': 'Enhancing Multimodal Entity Linking with Text-First Strategies', 'desc': 'This paper introduces a new framework called Intra- and Inter-modal Collaborative Reflections for multimodal entity linking, which primarily focuses on text while effectively using visual clues when needed. The framework addresses two main challenges in existing methods: the unnecessary use of image data and the limited extraction of visual features. By employing a multi-round iterative strategy, it enhances the reasoning process by integrating relevant visual information only when text is insufficient. Experimental results show that this approach significantly improves performance over current state-of-the-art techniques across multiple datasets.'}, 'zh': {'title': 'æ–‡æœ¬ä¼˜å…ˆï¼Œè§†è§‰è¾…åŠ©çš„å¤šæ¨¡æ€å®ä½“é“¾æ¥æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºå†…éƒ¨å’Œå¤–éƒ¨æ¨¡æ€åä½œåæ€ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å®ä½“é“¾æ¥ã€‚è¯¥æ¡†æ¶ä¼˜å…ˆåˆ©ç”¨æ–‡æœ¬ä¿¡æ¯æ¥å®Œæˆä»»åŠ¡ï¼Œå½“ä»…ä¾é æ–‡æœ¬æ— æ³•å‡†ç¡®é“¾æ¥å®ä½“æ—¶ï¼Œé‡‡ç”¨å¤šè½®è¿­ä»£ç­–ç•¥ï¼Œç»“åˆå›¾åƒä¸­çš„å…³é”®è§†è§‰çº¿ç´¢è¿›è¡Œæ¨ç†ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œè¯æ˜è¯¥æ¡†æ¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†3.2%ã€5.1%å’Œ1.6%ã€‚è¿™è¡¨æ˜ï¼Œåˆç†åˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„ç»“åˆå¯ä»¥æ˜¾è‘—æå‡å¤šæ¨¡æ€å®ä½“é“¾æ¥çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05128', 'title': 'Attention Basin: Why Contextual Position Matters in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.05128', 'abstract': "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.", 'score': 0, 'issue_id': 5256, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': 'a6974044ec3cff79', 'authors': ['Zihao Yi', 'Delong Zeng', 'Zhenqing Ling', 'Haohao Luo', 'Zhe Xu', 'Wei Liu', 'Jian Luan', 'Wanxia Cao', 'Ying Shen'], 'affiliations': ['MiLM Plus, Xiaomi Inc., Beijing, China', 'Sun Yat-sen University, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.05128.jpg', 'data': {'categories': ['#data', '#optimization', '#long_context', '#training', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, ÑƒĞ´ĞµĞ»ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ†Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'Ğ±Ğ°ÑÑĞµĞ¹Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ AttnRank, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. AttnRank - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ few-shot Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Enhancing Model Performance with Attention-Driven Reranking', 'desc': "This paper explores how Large Language Models (LLMs) are influenced by the position of information in their input sequences. It identifies a phenomenon called the attention basin, where models focus more on items at the start and end of a sequence, often overlooking those in the middle. The authors propose a new method called Attention-Driven Reranking (AttnRank) that improves model performance by rearranging input items to match the model's attention preferences. AttnRank is easy to implement, does not require retraining the models, and shows significant performance gains across various tasks and model architectures."}, 'zh': {'title': 'æå‡æ¨¡å‹æ€§èƒ½çš„æ³¨æ„åŠ›é‡æ’åºæ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½å¯¹è¾“å…¥ä¿¡æ¯çš„ä¸Šä¸‹æ–‡ä½ç½®éå¸¸æ•æ„Ÿã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸€ç§ç°è±¡ï¼Œç§°ä¸ºæ³¨æ„åŠ›ç›†åœ°ï¼šæ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–é¡¹ç›®æ—¶ï¼Œé€šå¸¸ä¼šå¯¹åºåˆ—å¼€å¤´å’Œç»“å°¾çš„é¡¹ç›®ç»™äºˆæ›´é«˜çš„æ³¨æ„åŠ›ï¼Œè€Œå¿½è§†ä¸­é—´çš„é¡¹ç›®ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°†æ›´å¤šæ³¨æ„åŠ›åˆ†é…ç»™å…³é”®ä¿¡æ¯å¯¹äºæå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›é©±åŠ¨çš„é‡æ’åºæ–¹æ³•ï¼ˆAttnRankï¼‰ï¼Œå®ƒé€šè¿‡å°è§„æ¨¡çš„æ ¡å‡†é›†æ¥ä¼°è®¡æ¨¡å‹çš„å†…åœ¨ä½ç½®æ³¨æ„åŠ›åå¥½ï¼Œå¹¶é‡æ–°æ’åˆ—æ£€ç´¢åˆ°çš„æ–‡æ¡£æˆ–å°‘é‡ç¤ºä¾‹ï¼Œä»¥å°†æœ€é‡è¦çš„å†…å®¹ä¸é«˜æ³¨æ„åŠ›ä½ç½®å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00819', 'title': 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.00819', 'abstract': 'DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.', 'score': 35, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '2241beba3b69f1fd', 'authors': ['Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Jiaqi Wang', 'Dahua Lin'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00819.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#diffusion', '#long_context'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DAEDAL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (DLLM). ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ DLLM. DAEDAL Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ´Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DAEDAL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Dynamic Length Adaptation for Enhanced DLLM Performance', 'desc': 'DAEDAL is a new method that improves Diffusion Large Language Models (DLLMs) by allowing them to adapt their output length dynamically without needing additional training. Traditional DLLMs require a fixed length for generation, which can limit their performance on complex tasks or waste computational resources. DAEDAL addresses this issue by using internal signals from the model to determine the optimal length for responses, expanding the generation length as needed. This approach not only enhances the quality of the generated text but also increases efficiency, making DLLMs more competitive with Autoregressive models.'}, 'zh': {'title': 'DAEDALï¼šåŠ¨æ€é€‚åº”é•¿åº¦çš„å»å™ªæ–°ç­–ç•¥', 'desc': 'DAEDALæ˜¯ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒå»å™ªç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°åŠ¨æ€é•¿åº¦é€‚åº”ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDLLMsï¼‰åœ¨ç”Ÿæˆæ•ˆç‡å’Œå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é™æ€ç”Ÿæˆé•¿åº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚DAEDALé€šè¿‡åˆ©ç”¨æ¨¡å‹å†…éƒ¨ä¿¡å·ï¼ŒåŠ¨æ€è°ƒæ•´ç”Ÿæˆé•¿åº¦ï¼Œè§£å†³äº†é™æ€é•¿åº¦å¸¦æ¥çš„æ€§èƒ½å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒDAEDALåœ¨æ€§èƒ½ä¸Šä¸å›ºå®šé•¿åº¦åŸºçº¿ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ›´ä¼˜ï¼ŒåŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23268', 'title': 'PixNerd: Pixel Neural Field Diffusion', 'url': 'https://huggingface.co/papers/2507.23268', 'abstract': 'Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID on ImageNet 512times512 without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.', 'score': 31, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'f035699955568725', 'authors': ['Shuai Wang', 'Ziteng Gao', 'Chenhui Zhu', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed', 'Nanjing University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23268.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'PixNerd (Pixel Neural Field Diffusion) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞĞ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. PixNerd Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ImageNet, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GenEval Ğ¸ DPG.'}, 'en': {'title': 'Efficient Image Generation with PixNerd: No VAEs, No Hassle!', 'desc': 'Pixel Neural Field Diffusion (PixNerd) introduces a novel approach to image generation that operates in a single-scale and single-stage manner, eliminating the need for variational autoencoders (VAEs) and complex pipelines. This method addresses the issues of accumulated errors and artifacts that arise from traditional two-stage training processes. By utilizing a patch-wise decoding strategy with neural fields, PixNerd achieves impressive performance metrics, such as a 2.15 FID score on ImageNet 256x256. Additionally, it extends its capabilities to text-to-image generation, demonstrating competitive results on various benchmarks.'}, 'zh': {'title': 'é«˜æ•ˆå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•ï¼šPixNerd', 'desc': 'Pixel Neural Field Diffusionï¼ˆPixNerdï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨å•å°ºåº¦ã€å•é˜¶æ®µçš„æµç¨‹ï¼Œæ— éœ€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–å¤æ‚çš„ç®¡é“ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¥ç»åœºæ¨¡å‹å®ç°äº†è¡¥ä¸çº§è§£ç ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è§çš„ç´¯ç§¯è¯¯å·®å’Œè§£ç ä¼ªå½±ã€‚PixNerdåœ¨ImageNetæ•°æ®é›†ä¸Šå–å¾—äº†2.15çš„FIDåˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å°†PixNerdæ‰©å±•åˆ°æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„åº”ç”¨ä¸­ï¼Œå–å¾—äº†åœ¨GenEvalå’ŒDPGåŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰æ€§æˆç»©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00414', 'title': 'Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training', 'url': 'https://huggingface.co/papers/2508.00414', 'abstract': 'Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro', 'score': 14, 'issue_id': 5169, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '0cd43b7d9f3e1eb5', 'authors': ['Tianqing Fang', 'Zhisong Zhang', 'Xiaoyang Wang', 'Rui Wang', 'Can Qin', 'Yuxuan Wan', 'Jun-Yu Ma', 'Ce Zhang', 'Jiaqi Chen', 'Xiyun Li', 'Hongming Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.00414.jpg', 'data': {'categories': ['#data', '#agi', '#training', '#open_source', '#reasoning', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹', 'desc': 'Cognitive Kernel-Pro - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ²ĞµĞ±, Ñ„Ğ°Ğ¹Ğ»Ñ‹, ĞºĞ¾Ğ´ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Cognitive Kernel-Pro Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GAIA, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Democratizing AI Agent Development with Cognitive Kernel-Pro', 'desc': 'Cognitive Kernel-Pro is an open-source framework designed to improve the robustness and performance of AI agents through effective data curation and innovative test-time strategies. It focuses on creating high-quality training data for Agent Foundation Models by constructing queries and trajectories across various domains like web interaction and coding. The framework also introduces new methods for agent reflection and voting, which enhance decision-making capabilities. By achieving state-of-the-art results on the GAIA benchmark, Cognitive Kernel-Pro sets a new standard for accessible and high-performance AI agents.'}, 'zh': {'title': 'å¼€æ”¾æºä»£ç ï¼Œæå‡AIä»£ç†çš„æœªæ¥ï¼', 'desc': 'Cognitive Kernel-Proæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡å—ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•°æ®æ•´ç†å’Œæ–°é¢–çš„æµ‹è¯•ç­–ç•¥æ¥å¢å¼ºAIä»£ç†çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤æ‚æ¨ç†ã€ç½‘ç»œäº¤äº’ã€ç¼–ç å’Œè‡ªä¸»ç ”ç©¶èƒ½åŠ›ï¼Œæ¨åŠ¨ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„æ•´ç†ï¼Œé‡ç‚¹å…³æ³¨æŸ¥è¯¢ã€è½¨è¿¹å’Œå¯éªŒè¯ç­”æ¡ˆçš„æ„å»ºã€‚Cognitive Kernel-Proåœ¨GAIAä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†å¼€æºå’Œå…è´¹ä»£ç†ä¸­çš„æœ€ä½³ç»“æœï¼Œè®¾ç«‹äº†é«˜èƒ½åŠ›AIä»£ç†çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23478', 'title': '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding', 'url': 'https://huggingface.co/papers/2507.23478', 'abstract': '3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.', 'score': 6, 'issue_id': 5155, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'f5e99fc10e8b9ad5', 'authors': ['Ting Huang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['School of Computer Science, Peking University', 'Shanghai University of Engineering Science'], 'pdf_title_img': 'assets/pdf/title_img/2507.23478.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset', '#reasoning', '#rlhf', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': '3D-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ 3D-R1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 3D-ÑÑ†ĞµĞ½. 3D-R1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with 3D-R1', 'desc': "The paper presents 3D-R1, a model designed to improve 3D scene understanding by addressing the limitations of existing vision-language models (VLMs). It introduces a high-quality synthetic dataset called Scene-30K, which is used to enhance the model's reasoning capabilities. The training process incorporates reinforcement learning with a GRPO policy and utilizes multiple reward functions to ensure accuracy and semantic precision. Additionally, a dynamic view selection strategy is implemented to optimize the perspectives used for analyzing 3D scenes, resulting in a notable average improvement of 10% in performance across various benchmarks."}, 'zh': {'title': '3D-R1ï¼šæå‡3Dåœºæ™¯ç†è§£çš„æ™ºèƒ½æ¨¡å‹', 'desc': '3D-R1 æ˜¯ä¸€ä¸ªå¢å¼º 3D åœºæ™¯ç†è§£çš„åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨é«˜è´¨é‡çš„åˆæˆæ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸º Scene-30K çš„åˆæˆæ•°æ®é›†ï¼Œä½œä¸º 3D-R1 çš„å†·å¯åŠ¨åˆå§‹åŒ–æ•°æ®ã€‚é€šè¿‡å¼•å…¥åŠ¨æ€è§†è§’é€‰æ‹©ç­–ç•¥ï¼Œ3D-R1 èƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œ 3D åœºæ™¯ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3D-R1 åœ¨å¤šä¸ª 3D åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº† 10%ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23361', 'title': 'SWE-Exp: Experience-Driven Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23361', 'abstract': 'SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.', 'score': 6, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'e16fe4dad5f61553', 'authors': ['Silin Chen', 'Shaoxin Lin', 'Xiaodong Gu', 'Yuling Shi', 'Heng Lian', 'Longfei Yun', 'Dong Chen', 'Weiguo Sun', 'Lin Cao', 'Qianxiang Wang'], 'affiliations': ['Huawei, China', 'Shanghai Jiao Tong University, China', 'UC San Diego, United States', 'Xidian University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23361.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‹Ñ‚ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ĞŸĞ', 'desc': 'SWE-Exp - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. SWE-Exp Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ - Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SWE-Exp Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ SWE-bench-Verified ÑÑ€ĞµĞ´Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Transforming Software Issue Resolution with Experience-Driven Learning', 'desc': 'SWE-Exp is a novel approach that enhances software issue resolution by utilizing past experiences of agents to improve their performance. Unlike traditional agents that do not retain knowledge, SWE-Exp builds an experience bank that captures both successful and failed attempts at resolving issues. This allows the system to learn from previous repairs and apply that knowledge to new problems, leading to more efficient and effective resolutions. The method has demonstrated a significant improvement in resolution rates, showcasing a shift from random exploration to a more strategic, experience-driven process.'}, 'zh': {'title': 'ç»éªŒé©±åŠ¨çš„è½¯ä»¶é—®é¢˜è§£å†³æ–°æ–¹æ³•', 'desc': 'SWE-Expæ˜¯ä¸€ç§å¢å¼ºè½¯ä»¶é—®é¢˜è§£å†³èƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡ç³»ç»Ÿåœ°ç§¯ç´¯å’Œåˆ©ç”¨è¿‡å»ä»£ç†çš„ä¿®å¤ç»éªŒï¼Œæé«˜äº†è§£å†³ç‡ã€‚å½“å‰çš„ä»£ç†åœ¨å¤„ç†é—®é¢˜æ—¶ç¼ºä¹è®°å¿†ï¼Œæ— æ³•é‡ç”¨ä¹‹å‰çš„çŸ¥è¯†ï¼Œå¯¼è‡´é‡å¤æ¢ç´¢å¤±è´¥çš„è·¯å¾„ã€‚SWE-Expé€šè¿‡å»ºç«‹ä¸€ä¸ªå¤šå±‚æ¬¡çš„ç»éªŒåº“ï¼Œæå–æˆåŠŸå’Œå¤±è´¥çš„ä¿®å¤å°è¯•ä¸­çš„å¯é‡ç”¨çŸ¥è¯†ï¼Œä»è€Œå®ç°è·¨é—®é¢˜çš„æŒç»­å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒSWE-Expåœ¨å¼€æºä»£ç†æ¡†æ¶ä¸‹çš„SWE-bench-Verifiedä¸Šè¾¾åˆ°äº†41.6%çš„æœ€ä½³è§£å†³ç‡ï¼Œæ ‡å¿—ç€è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹ä»£ç†çš„ä¸€ä¸ªæ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00265', 'title': 'Multimodal Referring Segmentation: A Survey', 'url': 'https://huggingface.co/papers/2508.00265', 'abstract': "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", 'score': 5, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '1604e587f6dc8177', 'authors': ['Henghui Ding', 'Song Tang', 'Shuting He', 'Chang Liu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance Inc.', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2508.00265.jpg', 'data': {'categories': ['#cv', '#multimodal', '#3d', '#survey', '#video', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ¾Ñ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ² 3D-ÑÑ†ĞµĞ½Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ°-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑÑ‹Ğ»Ğ¾Ğº (GREx) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Enhancing Object Segmentation with Multimodal Instructions', 'desc': 'This paper surveys the field of multimodal referring segmentation, which focuses on identifying and segmenting objects in visual content based on textual or audio instructions. It highlights the advancements made through convolutional neural networks, transformers, and large language models that enhance the ability to understand and process multimodal data. The authors present a unified meta architecture for referring segmentation and review various methods applicable to images, videos, and 3D scenes. Additionally, they discuss challenges in real-world applications and provide performance comparisons on standard benchmarks to evaluate the effectiveness of different approaches.'}, 'zh': {'title': 'å¤šæ¨¡æ€æŒ‡å‘åˆ†å‰²çš„å…¨é¢è°ƒæŸ¥', 'desc': 'å¤šæ¨¡æ€æŒ‡å‘åˆ†å‰²æ—¨åœ¨æ ¹æ®æ–‡æœ¬æˆ–éŸ³é¢‘æŒ‡ä»¤åœ¨è§†è§‰åœºæ™¯ä¸­åˆ†å‰²ç›®æ ‡ç‰©ä½“ï¼Œå¦‚å›¾åƒã€è§†é¢‘å’Œ3Dåœºæ™¯ã€‚è¯¥ä»»åŠ¡åœ¨éœ€è¦æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œå‡†ç¡®ç‰©ä½“æ„ŸçŸ¥çš„å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œå·ç§¯ç¥ç»ç½‘ç»œã€å˜æ¢å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡æä¾›äº†å¤šæ¨¡æ€æŒ‡å‘åˆ†å‰²çš„å…¨é¢è°ƒæŸ¥ï¼Œæ¶µç›–äº†èƒŒæ™¯ä»‹ç»ã€ç»Ÿä¸€çš„å…ƒæ¶æ„ã€ä»£è¡¨æ€§æ–¹æ³•åŠå…¶åœ¨ä¸åŒè§†è§‰åœºæ™¯ä¸­çš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23348', 'title': 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23348', 'abstract': "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.", 'score': 4, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '28b58ecd36ac995b', 'authors': ['Han Li', 'Yuling Shi', 'Shaoxin Lin', 'Xiaodong Gu', 'Heng Lian', 'Xin Wang', 'Yantao Jia', 'Tao Huang', 'Qianxiang Wang'], 'affiliations': ['Huawei China', 'Shanghai Jiao Tong University China', 'Xidian University China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23348.jpg', 'data': {'categories': ['#optimization', '#agents', '#open_source', '#reasoning', '#games', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”ĞµĞ±Ğ°Ñ‚Ñ‹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ', 'desc': 'SWE-Debate - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ±Ğ°Ñ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ SWE-Debate Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Software Issue Resolution through Competitive Multi-Agent Debate', 'desc': 'SWE-Debate is a multi-agent framework designed to improve issue resolution in software engineering by fostering diverse reasoning among agents. It utilizes large language models to enhance the reasoning capabilities of autonomous agents, allowing them to tackle complex tasks more effectively. By organizing a structured debate among agents with different perspectives, SWE-Debate helps identify broader issue patterns and achieve better localization of software faults. The framework has demonstrated significant improvements in performance on the SWE-bench benchmark, setting new standards in open-source agent frameworks.'}, 'zh': {'title': 'SWE-Debateï¼šå¤šæ ·åŒ–æ¨ç†ä¿ƒè¿›è½¯ä»¶é—®é¢˜è§£å†³', 'desc': 'SWE-Debateæ˜¯ä¸€ä¸ªç«äº‰æ€§çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¿ƒè¿›å¤šæ ·åŒ–çš„æ¨ç†æ¥å¢å¼ºè½¯ä»¶å·¥ç¨‹ä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¿›è¡Œè‡ªä¸»æ¢ç´¢ã€‚ä¸ä»¥å¾€çš„ç‹¬ç«‹æ¢ç´¢æ–¹æ³•ä¸åŒï¼ŒSWE-Debateé€šè¿‡ç»„ç»‡æ™ºèƒ½ä½“ä¹‹é—´çš„è¾©è®ºï¼Œé¼“åŠ±ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œæ›´å¥½åœ°å®šä½é—®é¢˜å¹¶åˆ¶å®šä¿®å¤è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSWE-Debateåœ¨å¼€æºæ™ºèƒ½ä½“æ¡†æ¶ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00782', 'title': 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation', 'url': 'https://huggingface.co/papers/2508.00782', 'abstract': 'SpA2V generates realistic videos aligned with input audio by leveraging spatial auditory cues and integrating them into diffusion models through video scene layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.', 'score': 3, 'issue_id': 5160, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '75fd2b7336810b40', 'authors': ['Kien T. Pham', 'Yingqing He', 'Yazhou Xing', 'Qifeng Chen', 'Long Chen'], 'affiliations': ['Hong Kong University of Science and Technology Clear Water Bay, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00782.jpg', 'data': {'categories': ['#audio', '#games', '#video', '#diffusion', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ—Ğ²ÑƒĞº Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'SpA2V - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ»Ğ°Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. SpA2V Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ.'}, 'en': {'title': 'SpA2V: Bridging Audio and Video with Spatial Awareness', 'desc': 'The paper introduces SpA2V, a novel framework for generating realistic videos that align with input audio by utilizing spatial auditory cues. Unlike previous methods that focus mainly on semantic information, SpA2V incorporates spatial attributes such as location and movement derived from audio properties like loudness and frequency. The generation process is divided into two stages: first, it creates Video Scene Layouts (VSLs) using a modified machine learning model to capture both spatial and semantic cues from the audio. Then, it integrates these VSLs into diffusion models for video generation, resulting in videos that are both semantically and spatially accurate to the audio input.'}, 'zh': {'title': 'åˆ©ç”¨ç©ºé—´å¬è§‰çº¿ç´¢ç”ŸæˆçœŸå®è§†é¢‘', 'desc': 'SpA2Væ˜¯ä¸€ç§ç”Ÿæˆä¸è¾“å…¥éŸ³é¢‘å¯¹é½çš„çœŸå®è§†é¢‘çš„æ–°æ¡†æ¶ã€‚å®ƒé€šè¿‡åˆ©ç”¨ç©ºé—´å¬è§‰çº¿ç´¢ï¼Œå°†è¿™äº›çº¿ç´¢æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œç”Ÿæˆè§†é¢‘åœºæ™¯å¸ƒå±€ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSpA2Vä¸ä»…å…³æ³¨éŸ³é¢‘çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿˜è€ƒè™‘äº†å£°éŸ³çš„ç©ºé—´å±æ€§ï¼Œå¦‚ä½ç½®å’Œè¿åŠ¨æ–¹å‘ã€‚å®éªŒè¡¨æ˜ï¼ŒSpA2Våœ¨ç”Ÿæˆä¸è¾“å…¥éŸ³é¢‘å…·æœ‰é«˜è¯­ä¹‰å’Œç©ºé—´ä¸€è‡´æ€§çš„çœŸå®è§†é¢‘æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00454', 'title': 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges', 'url': 'https://huggingface.co/papers/2508.00454', 'abstract': 'An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.', 'score': 3, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '7d97f0b64c1261dd', 'authors': ['Yuqi Tang', 'Kehua Feng', 'Yunfeng Wang', 'Zhiwen Chen', 'Chengfei Lv', 'Gang Yu', 'Qiang Zhang', 'Keyan Ding'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University', 'ZJU-UIUC Institute, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00454.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#inference', '#alignment', '#benchmark'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²: Ğ¼ÑƒĞ´Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Efficient Dialogue Evaluation: Harnessing Collective Wisdom of LLMs', 'desc': 'This paper introduces an efficient multi-turn dialogue evaluator that combines the judgments of multiple large language models (LLMs) to assess dialogue quality. Traditional methods using a single LLM as a judge often face biases that affect evaluation reliability. The proposed method aggregates the preferences of several LLMs into one model, maintaining the benefits of diverse feedback while significantly lowering computational costs. Experiments show that this new approach outperforms existing methods in various evaluation scenarios, proving its effectiveness and efficiency.'}, 'zh': {'title': 'é«˜æ•ˆçš„å¤šè½®å¯¹è¯è¯„ä¼°å™¨ï¼šèšåˆæ™ºæ…§ï¼Œé™ä½æˆæœ¬', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šè½®å¯¹è¯è¯„ä¼°å™¨ï¼Œé€šè¿‡å°†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ¤æ–­æ±‡èšæˆä¸€ä¸ªå•ä¸€æ¨¡å‹æ¥è¯„ä¼°å¯¹è¯è´¨é‡ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºâ€œLLMä½œä¸ºè¯„å®¡â€çš„æ¨¡å¼ï¼Œä½†è¿™ç§æ–¹æ³•å¸¸å¸¸å—åˆ°åè§çš„å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœçš„ä¸å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åˆ©ç”¨å¤šä¸ªLLMä½œä¸ºè¯„å®¡ï¼Œå¹¶å°†å®ƒä»¬çš„åå¥½çŸ¥è¯†æ±‡èšåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œä»è€Œä¿ç•™å¤šè¯„å®¡åé¦ˆçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è¯„ä¼°æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å¯¹è¯è¯„ä¼°åŸºå‡†ä¸Šä¼˜äºç°æœ‰çš„åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.19634', 'title': 'MCIF: Multimodal Crosslingual Instruction-Following Benchmark from\n  Scientific Talks', 'url': 'https://huggingface.co/papers/2507.19634', 'abstract': "MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.", 'score': 3, 'issue_id': 5169, 'pub_date': '2025-07-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ»Ñ', 'en': 'July 25', 'zh': '7æœˆ25æ—¥'}, 'hash': '6c681493be72e8eb', 'authors': ['Sara Papi', 'Maike ZÃ¼fle', 'Marco Gaido', 'Beatrice Savoldi', 'Danni Liu', 'Ioannis Douros', 'Luisa Bentivogli', 'Jan Niehues'], 'affiliations': ['Fondazione Bruno Kessler (Italy)', 'Karlsruhe Institute of Technology (Germany)', 'Translated (Italy)'], 'pdf_title_img': 'assets/pdf/title_img/2507.19634.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#multilingual', '#open_source', '#long_context', '#machine_translation'], 'emoji': 'ğŸŒ', 'ru': {'title': 'MCIF: ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ MLLM', 'desc': 'MCIF - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ»Ğ°Ğ´Ñ‹. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ - Ñ€ĞµÑ‡ÑŒ, Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚ - Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. MCIF ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ CC-BY 4.0 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ MLLM.'}, 'en': {'title': 'MCIF: A New Benchmark for Multilingual Instruction-Following in Multimodal AI', 'desc': 'MCIF is a new benchmark designed to evaluate how well multilingual, multimodal language models can follow instructions in different languages and formats. It focuses on three modalities: speech, vision, and text, and includes human annotations to ensure quality assessments. Unlike previous benchmarks, MCIF allows for both short and long context evaluations across four languages: English, German, Italian, and Chinese. This comprehensive approach aims to enhance the understanding of model performance in real-world, crosslingual scenarios.'}, 'zh': {'title': 'MCIFï¼šè·¨è¯­è¨€å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšçš„è¯„ä¼°æ–°åŸºå‡†', 'desc': 'MCIFæ˜¯ä¸€ä¸ªå¤šè¯­è¨€çš„äººç±»æ ‡æ³¨åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è·¨è¯­è¨€ã€å¤šæ¨¡æ€ç¯å¢ƒä¸‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚å®ƒç»“åˆäº†æ–‡æœ¬ã€è¯­éŸ³å’Œè§†è§‰ä¸‰ç§æ ¸å¿ƒæ¨¡æ€ï¼Œå¹¶æ”¯æŒè‹±è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­å’Œä¸­æ–‡å››ç§è¯­è¨€ã€‚MCIFçš„è®¾è®¡æ—¨åœ¨å¡«è¡¥ç°æœ‰åŸºå‡†åœ¨å¤šè¯­è¨€å’Œå¤šæ¨¡æ€è¯„ä¼°æ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿çŸ­æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ã€‚é€šè¿‡MCIFï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00632', 'title': 'Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings', 'url': 'https://huggingface.co/papers/2508.00632', 'abstract': 'A multi-agent system using an omni-modal evaluation metric improves JavaScript game and animation generation but struggles with custom assets and audio-visual feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.', 'score': 2, 'issue_id': 5165, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '3e4a605a070fb44f', 'authors': ['Alexia Jolicoeur-Martineau'], 'affiliations': ['Samsung SAIL MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2508.00632.jpg', 'data': {'categories': ['#audio', '#multimodal', '#games', '#optimization', '#video', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ³Ñ€: Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ JavaScript-Ğ¸Ğ³Ñ€ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° AVR-Agent Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ°ÑÑĞµÑ‚Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° AVR-Eval ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼ĞµĞ´Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸.'}, 'en': {'title': 'Enhancing Game Generation with Multi-Agent Systems and AVR-Eval', 'desc': 'This paper presents a multi-agent system designed to enhance the generation of JavaScript games and animations using a new evaluation metric called AVR-Eval. AVR-Eval assesses multimedia content quality by comparing Audio-Visual Recordings (AVRs) through an omni-modal model that processes text, video, and audio. The system, AVR-Agent, generates code by selecting relevant multimedia assets and iteratively improving the output based on feedback from AVR-Eval. Despite achieving higher success rates in generated content, the system struggles with custom assets and effective audio-visual feedback, indicating a gap between human creativity and machine-generated content.'}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæå‡JavaScriptæ¸¸æˆç”Ÿæˆè´¨é‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œåˆ©ç”¨å…¨æ¨¡æ€è¯„ä¼°æŒ‡æ ‡æ¥æ”¹å–„JavaScriptæ¸¸æˆå’ŒåŠ¨ç”»çš„ç”Ÿæˆã€‚æˆ‘ä»¬å¼€å‘äº†AVR-Evalï¼Œè¿™æ˜¯ä¸€ç§ç›¸å¯¹è¯„ä¼°å¤šåª’ä½“å†…å®¹è´¨é‡çš„æ–°æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¼˜è´¨å’ŒåŠ£è´¨å†…å®¹ã€‚AVR-Agentæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»å¤šåª’ä½“èµ„äº§åº“ä¸­ç”ŸæˆJavaScriptä»£ç ï¼Œå¹¶é€šè¿‡è¿­ä»£åé¦ˆä¸æ–­ä¼˜åŒ–ç”Ÿæˆçš„å†…å®¹ã€‚å°½ç®¡ç”Ÿæˆçš„å†…å®¹åœ¨èƒœç‡ä¸Šä¼˜äºå•æ¬¡ç”Ÿæˆçš„å†…å®¹ï¼Œä½†æ¨¡å‹åœ¨åˆ©ç”¨è‡ªå®šä¹‰èµ„äº§å’ŒéŸ³è§†é¢‘åé¦ˆæ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæ˜¾ç¤ºå‡ºäººç±»ä¸æœºå™¨å†…å®¹åˆ›ä½œæ–¹æ³•ä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.22720', 'title': 'Investigating Hallucination in Conversations for Low Resource Languages', 'url': 'https://huggingface.co/papers/2507.22720', 'abstract': "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.", 'score': 2, 'issue_id': 5155, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ»Ñ', 'en': 'July 30', 'zh': '7æœˆ30æ—¥'}, 'hash': 'c7f5db5f58895f4f', 'authors': ['Amit Das', 'Md. Najib Hasan', 'Souvika Sarkar', 'Zheng Zhang', 'Fatemeh Jamshidi', 'Tathagata Bhattacharya', 'Nilanjana Raychawdhury', 'Dongji Feng', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'Auburn University', 'Auburn University at Montgomery', 'California State Polytechnic University Pomona', 'Gustavus Adolphus College', 'Meta', 'Murray State University', 'Stanford University', 'University of North Alabama', 'Wichita State University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22720.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#hallucinations'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾-ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‚Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²: Ñ…Ğ¸Ğ½Ğ´Ğ¸, Ñ„Ğ°Ñ€ÑĞ¸ Ğ¸ Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑÑ‚Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-3.5, GPT-4, Llama-3.1 Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ¼ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ…Ğ¸Ğ½Ğ´Ğ¸ Ğ¸ Ñ„Ğ°Ñ€ÑĞ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM.'}, 'en': {'title': 'Mandarin LLMs: Fewer Hallucinations, More Accuracy!', 'desc': 'This paper investigates the phenomenon of hallucinations in Large Language Models (LLMs) across three languages: Mandarin, Hindi, and Farsi. Hallucinations refer to instances where the models generate incorrect or misleading information. The study analyzes conversational data from various LLMs, including GPT-3.5 and GPT-4o, to compare the frequency of these errors. The findings reveal that LLMs exhibit fewer hallucinations in Mandarin compared to the higher rates observed in Hindi and Farsi, highlighting the need for language-specific improvements in model training.'}, 'zh': {'title': 'æ™®é€šè¯ä¸­çš„å¹»è§‰ç°è±¡è¾ƒå°‘', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬æœ‰æ—¶ä¼šäº§ç”Ÿä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œè¿™è¢«ç§°ä¸ºâ€œå¹»è§‰â€ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æ™®é€šè¯ã€å°åœ°è¯­å’Œæ³•å°”è¥¿è¯­ä¸­ï¼ŒLLMsçš„å¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬åˆ†æäº†å¤šä¸ªæ¨¡å‹ï¼ˆå¦‚GPT-3.5ã€GPT-4oç­‰ï¼‰åœ¨è¿™ä¸‰ç§è¯­è¨€ä¸­çš„äº‹å®å’Œè¯­è¨€é”™è¯¯ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨æ™®é€šè¯ä¸­äº§ç”Ÿçš„å¹»è§‰å“åº”è¾ƒå°‘ï¼Œè€Œåœ¨å°åœ°è¯­å’Œæ³•å°”è¥¿è¯­ä¸­åˆ™æ˜¾è‘—æ›´å¤šã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00823', 'title': 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation', 'url': 'https://huggingface.co/papers/2508.00823', 'abstract': 'IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.', 'score': 1, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': 'a4651adceaac80f7', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Hang Yin', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00823.jpg', 'data': {'categories': ['#robotics', '#3d', '#optimization', '#agents', '#games'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ² 3D Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²', 'desc': 'IGL-Nav - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ-Ñ†ĞµĞ»Ğ¸ Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³. IGL-Nav Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Efficient 3D Navigation with Incremental Gaussian Localization', 'desc': 'IGL-Nav introduces an innovative approach to image-goal navigation in 3D environments using an incremental 3D Gaussian representation. This method enhances localization accuracy by updating the scene representation as new images are processed, allowing for efficient navigation. Unlike traditional methods that struggle with geometric relationships, IGL-Nav utilizes geometric information for effective discrete space matching and fine target pose optimization. The framework demonstrates significant improvements over existing techniques and is suitable for real-world applications, including robotic platforms.'}, 'zh': {'title': 'å¢é‡å¼3Dé«˜æ–¯å¯¼èˆªï¼šé«˜æ•ˆå‡†ç¡®çš„å›¾åƒç›®æ ‡å®šä½', 'desc': 'IGL-Navæ˜¯ä¸€ç§å¢é‡å¼3Dé«˜æ–¯å®šä½æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç›®æ ‡å¯¼èˆªçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯æ¸²æŸ“çš„3Dé«˜æ–¯è¡¨ç¤ºæ¥å»ºæ¨¡3Dç¯å¢ƒä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚IGL-Navé€šè¿‡å‰é¦ˆå•ç›®é¢„æµ‹é€æ­¥æ›´æ–°åœºæ™¯è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å‡ ä½•ä¿¡æ¯è¿›è¡Œç²—ç•¥å®šä½ï¼Œæœ€ç»ˆé€šè¿‡å¯å¾®æ¸²æŸ“ä¼˜åŒ–ç²¾ç¡®ç¡®å®šç›®æ ‡ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGL-Navåœ¨å¤šç§é…ç½®ä¸‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººå¹³å°ä¸Šåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23726', 'title': 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2507.23726', 'abstract': 'Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.', 'score': 84, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'ab5bfbdad68eb6bf', 'authors': ['Luoxin Chen', 'Jinming Gu', 'Liankai Huang', 'Wenhao Huang', 'Zhicheng Jiang', 'Allan Jie', 'Xiaoran Jin', 'Xing Jin', 'Chenggang Li', 'Kaijing Ma', 'Cheng Ren', 'Jiawei Shen', 'Wenlei Shi', 'Tong Sun', 'He Sun', 'Jiahui Wang', 'Siran Wang', 'Zhihong Wang', 'Chenrui Wei', 'Shufa Wei', 'Yonghui Wu', 'Yuchen Wu', 'Yihang Xia', 'Huajian Xin', 'Fan Yang', 'Huaiyuan Ying', 'Hongyi Yuan', 'Zheng Yuan', 'Tianyang Zhan', 'Chi Zhang', 'Yue Zhang', 'Ge Zhang', 'Tianyun Zhao', 'Jianqiu Zhao', 'Yichi Zhou', 'Thomas Hanwen Zhu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2507.23726.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#reasoning', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Seed-Prover - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ·Ñ‹Ğº Lean. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. Seed-Prover Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹.'}, 'en': {'title': 'Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement', 'desc': 'The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning.'}, 'zh': {'title': 'Seed-Proverï¼šè‡ªåŠ¨åŒ–æ•°å­¦æ¨ç†çš„æ–°çªç ´', 'desc': 'Seed-Proveræ˜¯ä¸€ç§åŸºäºLeançš„å¼•ç†é£æ ¼æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å½¢å¼å®šç†è¯æ˜å’Œè‡ªåŠ¨æ•°å­¦æ¨ç†ä¸­å®ç°é«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œä¸“é—¨çš„å‡ ä½•æ”¯æŒï¼Œå…‹æœäº†ä¼ ç»Ÿè‡ªç„¶è¯­è¨€æ¨ç†çš„å±€é™æ€§ã€‚Seed-Proveråˆ©ç”¨Leançš„åé¦ˆå’Œè‡ªæˆ‘æ€»ç»“æ¥ä¸æ–­æ”¹è¿›å…¶è¯æ˜è¿‡ç¨‹ï¼Œå¹¶è®¾è®¡äº†ä¸‰ç§æ¨ç†ç­–ç•¥ä»¥åº”å¯¹å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰çº§åˆ«çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥Seed-Geometryå‡ ä½•æ¨ç†å¼•æ“ï¼ŒSeed-Proveråœ¨å‡ ä½•é—®é¢˜ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç¤ºäº†å½¢å¼éªŒè¯ä¸é•¿é“¾æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23779', 'title': 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding', 'url': 'https://huggingface.co/papers/2507.23779', 'abstract': 'The Phi-Ground model family achieves state-of-the-art performance in GUI grounding for multimodal reasoning models, improving accuracy across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from "Iron Man", are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textbf{43.2} on ScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}', 'score': 35, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'e6bd9c919aacc874', 'authors': ['Miaosen Zhang', 'Ziqiang Xu', 'Jialiang Zhu', 'Qi Dai', 'Kai Qiu', 'Yifan Yang', 'Chong Luo', 'Tianyi Chen', 'Justin Wagle', 'Tim Franklin', 'Baining Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.23779.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#reasoning', '#training', '#multimodal'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Phi-Ground: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ GUI Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡ĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Phi-Ground Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ GUI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸, Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ² Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¾Ñ‚ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸, Ğ½Ğ¾ Ğ¸ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Phi-Ground Models', 'desc': 'The Phi-Ground model family significantly enhances GUI grounding for multimodal reasoning models, achieving top performance on various benchmarks. This model is crucial for Computer Use Agents (CUAs) to perform tasks like clicking and typing accurately, which is essential for their effectiveness. Despite existing models struggling with accuracy below 65% on tough benchmarks, Phi-Ground demonstrates superior results, scoring 43.2 on ScreenSpot-pro and 27.2 on UI-Vision. The paper details the training process and insights gained, which can also aid in improving other perception tasks in machine learning.'}, 'zh': {'title': 'Phi-Groundï¼šå¤šæ¨¡æ€æ¨ç†çš„GUIå®šä½æ–°çªç ´', 'desc': 'Phi-Groundæ¨¡å‹ç³»åˆ—åœ¨å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„GUIå®šä½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæå‡äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®æ€§ã€‚GUIå®šä½æ˜¯è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰æ‰§è¡Œå®é™…æ“ä½œçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œç›´æ¥å½±å“ç³»ç»Ÿçš„æˆåŠŸä¸å¦ã€‚å½“å‰çš„ç«¯åˆ°ç«¯å®šä½æ¨¡å‹åœ¨ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®ç‡ä»ä½äº65%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡å¯¹å®šä½æ¨¡å‹çš„è®­ç»ƒè¿›è¡Œå®è¯ç ”ç©¶ï¼Œæœ€ç»ˆå¼€å‘å‡ºPhi-Groundæ¨¡å‹ç³»åˆ—ï¼Œåœ¨ä»£ç†è®¾ç½®ä¸‹çš„äº”ä¸ªå®šä½åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.22879', 'title': 'RecGPT Technical Report', 'url': 'https://huggingface.co/papers/2507.22879', 'abstract': "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.", 'score': 22, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ»Ñ', 'en': 'July 30', 'zh': '7æœˆ30æ—¥'}, 'hash': '2bd5536810f1694b', 'authors': ['Chao Yi', 'Dian Chen', 'Gaoyang Guo', 'Jiakai Tang', 'Jian Wu', 'Jing Yu', 'Mao Zhang', 'Sunhao Dai', 'Wen Chen', 'Wenjun Yang', 'Yuning Jiang', 'Zhujin Gao', 'Bo Zheng', 'Chi Li', 'Dimin Wang', 'Dixuan Wang', 'Fan Li', 'Fan Zhang', 'Haibin Chen', 'Haozhuang Liu', 'Jialin Zhu', 'Jiamang Wang', 'Jiawei Wu', 'Jin Cui', 'Ju Huang', 'Kai Zhang', 'Kan Liu', 'Lang Tian', 'Liang Rao', 'Longbin Li', 'Lulu Zhao', 'Na He', 'Peiyang Wang', 'Qiqi Huang', 'Tao Luo', 'Wenbo Su', 'Xiaoxiao He', 'Xin Tong', 'Xu Chen', 'Xunke Xi', 'Yang Li', 'Yaxuan Wu', 'Yeqiu Yang', 'Yi Hu', 'Yinnan Song', 'Yuchen Li', 'Yujie Luo', 'Yujin Yuan', 'Yuliang Yan', 'Zhengyang Wang', 'Zhibo Xiao', 'Zhixin Ma', 'Zile Zhou', 'Ziqi Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.22879.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#alignment', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'RecGPT: Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'desc': 'RecGPT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹. RecGPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¶Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Taobao Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½.'}, 'en': {'title': 'Empowering Recommendations with User Intent', 'desc': 'RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem.'}, 'zh': {'title': 'ä»¥ç”¨æˆ·æ„å›¾ä¸ºä¸­å¿ƒçš„æ¨èç³»ç»Ÿæ–°èŒƒå¼', 'desc': 'RecGPT æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆåˆ°æ¨èç³»ç»Ÿä¸­çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å…³æ³¨ç”¨æˆ·æ„å›¾ã€‚é€šè¿‡é‡æ–°è®¾è®¡æ¨èæµç¨‹ï¼ŒRecGPT ä½¿æ¨èè¿‡ç¨‹ä»å•çº¯ä¾èµ–å†å²æ•°æ®è½¬å˜ä¸ºä»¥ç”¨æˆ·æ„å›¾ä¸ºä¸­å¿ƒã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ¨ç†å¢å¼ºçš„é¢„å¯¹é½å’Œè‡ªæˆ‘è®­ç»ƒï¼Œæå‡äº†æ¨èçš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRecGPT åœ¨ç”¨æˆ·æ»¡æ„åº¦ã€å•†å®¶æ›å…‰ç‡å’Œå¹³å°è½¬åŒ–ç‡ç­‰æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23682', 'title': 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models', 'url': 'https://huggingface.co/papers/2507.23682', 'abstract': 'The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.', 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'baa73e4730b01a97', 'authors': ['Xiaoyu Chen', 'Hangxing Wei', 'Pushi Zhang', 'Chuheng Zhang', 'Kaixin Wang', 'Yanjiang Guo', 'Rushuai Yang', 'Yucen Wang', 'Xinquan Xiao', 'Li Zhao', 'Jianyu Chen', 'Jiang Bian'], 'affiliations': ['Hong Kong University of Science and Technology', 'Microsoft Research', 'Nanjing University', 'Tsinghua University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23682.jpg', 'data': {'categories': ['#optimization', '#agents', '#agi', '#games', '#robotics', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ViLLA: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ViLLA ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ villa-X ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ VLA. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ… SIMPLER Ğ¸ LIBERO, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Robot Manipulation with Latent Actions in ViLLA Framework', 'desc': 'The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research.'}, 'zh': {'title': 'ViLLAæ¡†æ¶ï¼šæå‡æœºå™¨äººæ“ä½œçš„æ½œåŠ›', 'desc': 'ViLLAæ¡†æ¶é€šè¿‡å¼•å…¥æ½œåœ¨åŠ¨ä½œæ¥å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä»è€Œæé«˜æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ€§èƒ½ã€‚æ½œåœ¨åŠ¨ä½œæ˜¯ä¸€ç§æŠ½è±¡è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ•æ‰ä¸¤ä¸ªå¸§ä¹‹é—´çš„è§†è§‰å˜åŒ–ã€‚æœ¬æ–‡ä»‹ç»çš„villa-Xæ˜¯ä¸€ä¸ªæ–°é¢–çš„è§†è§‰-è¯­è¨€-æ½œåœ¨åŠ¨ä½œæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›æ½œåœ¨åŠ¨ä½œå»ºæ¨¡ï¼Œä»¥å­¦ä¹ å¯æ¨å¹¿çš„æœºå™¨äººæ“ä½œç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œvilla-Xåœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®æœºå™¨äººè®¾ç½®ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†ViLLAèŒƒå¼çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.22968', 'title': 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations', 'url': 'https://huggingface.co/papers/2507.22968', 'abstract': "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.", 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ»Ñ', 'en': 'July 30', 'zh': '7æœˆ30æ—¥'}, 'hash': '3a2f5273d610d5d6', 'authors': ['Chengqian Ma', 'Wei Tao', 'Yiwen Guo'], 'affiliations': ['Independent Researcher', 'LIGHTSPEED', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22968.jpg', 'data': {'categories': ['#survey', '#long_context', '#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1079 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Benchmarking Spoken Dialogue Models for Real-World Conversations', 'desc': "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."}, 'zh': {'title': 'æå‡å£è¯­å¯¹è¯æ¨¡å‹çš„è¯„ä¼°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å£è¯­å¯¹è¯æ¨¡å‹ï¼ˆSDMsï¼‰æ€§èƒ½çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–è‹±è¯­å’Œä¸­æ–‡ï¼Œæ—¨åœ¨ç†è§£å’Œæ¨¡æ‹Ÿäººç±»å£è¯­å¯¹è¯ã€‚å£è¯­å¯¹è¯çš„å¤æ‚æ€§ä½“ç°åœ¨æ­§ä¹‰æ€§å’Œä¸Šä¸‹æ–‡ä¾èµ–æ€§ç­‰æŒ‘æˆ˜ä¸Šï¼Œè¿™äº›å› ç´ ä½¿å¾—ä¸æ–‡æœ¬åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”ï¼ŒSDMsçš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æ•°æ®é›†ä¸­åŒ…å«1079ä¸ªå®ä¾‹ï¼Œå¹¶é…å¤‡äº†ä¸€ç§åŸºäºLLMçš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°ä¸äººç±»åˆ¤æ–­ç›¸ä¸€è‡´ã€‚é€šè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œç ”ç©¶è€…å¯ä»¥å…¨é¢æ¢è®¨SDMsåœ¨åº”å¯¹å®é™…å¯¹è¯æŒ‘æˆ˜ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23277', 'title': 'iLRM: An Iterative Large 3D Reconstruction Model', 'url': 'https://huggingface.co/papers/2507.23277', 'abstract': 'iLRM, an iterative Large 3D Reconstruction Model, improves scalability and efficiency in 3D reconstruction by decoupling scene representation, using a two-stage attention scheme, and injecting high-resolution information.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.', 'score': 19, 'issue_id': 5137, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'af6d37e5e25a6d73', 'authors': ['Gyeongjin Kang', 'Seungtae Nam', 'Xiangyu Sun', 'Sameh Khamis', 'Abdelrahman Mohamed', 'Eunbyung Park'], 'affiliations': ['Rembrand Project', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23277.jpg', 'data': {'categories': ['#optimization', '#3d'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ: Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'iLRM - ÑÑ‚Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ iLRM Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with iLRM: Scalable and Efficient!', 'desc': 'The paper presents the iterative Large 3D Reconstruction Model (iLRM), which enhances the scalability and efficiency of 3D reconstruction processes. It achieves this by decoupling the scene representation from the input images, allowing for more compact 3D models. Additionally, iLRM employs a two-stage attention mechanism to minimize computational costs associated with multi-view interactions. By incorporating high-resolution information at each layer, the model ensures high-fidelity reconstructions while maintaining superior performance across various datasets.'}, 'zh': {'title': 'iLRMï¼šé«˜æ•ˆå¯æ‰©å±•çš„3Dé‡å»ºæ–°æ¨¡å‹', 'desc': 'iLRMï¼ˆè¿­ä»£å¤§å‹3Dé‡å»ºæ¨¡å‹ï¼‰é€šè¿‡è§£è€¦åœºæ™¯è¡¨ç¤ºã€é‡‡ç”¨ä¸¤é˜¶æ®µæ³¨æ„åŠ›æœºåˆ¶å’Œæ³¨å…¥é«˜åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œæå‡äº†3Dé‡å»ºçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–ç”Ÿæˆ3Dé«˜æ–¯è¡¨ç¤ºï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªè¾“å…¥è§†å›¾ä¸‹å®ç°å¿«é€Ÿä¸”é«˜è´¨é‡çš„é‡å»ºã€‚ä¸ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼ŒiLRMæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºçš„é«˜ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒiLRMåœ¨é‡å»ºè´¨é‡å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤„ç†æ›´å¤šè¾“å…¥è§†å›¾æ—¶è¡¨ç°å‡ºæ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.21509', 'title': 'Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models', 'url': 'https://huggingface.co/papers/2507.21509', 'abstract': "Persona vectors in large language models can monitor and control personality changes during training and deployment, enabling the identification and mitigation of undesirable traits.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.", 'score': 17, 'issue_id': 5127, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ»Ñ', 'en': 'July 29', 'zh': '7æœˆ29æ—¥'}, 'hash': '8088854aaf027260', 'authors': ['Runjin Chen', 'Andy Arditi', 'Henry Sleight', 'Owain Evans', 'Jack Lindsey'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'Constellation', 'Truthful AI', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.21509.jpg', 'data': {'categories': ['#rlhf', '#hallucinations', '#data', '#ethics', '#training', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹: ĞºĞ»ÑÑ‡ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ­Ñ‚Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ Ğ² Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑƒÑÑ‰ĞµĞ¹ Ñ‡ĞµÑ€Ñ‚Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Controlling AI Personalities with Persona Vectors', 'desc': "This paper introduces the concept of persona vectors in large language models, which are used to track and manage personality traits during the model's training and deployment phases. The authors demonstrate that these vectors can identify undesirable traits like harmful behavior or excessive flattery by analyzing the model's activation space. They show that personality shifts can be predicted and controlled, allowing for interventions to mitigate negative changes. Additionally, the method can flag problematic training data that may lead to these undesirable personality traits, making it a valuable tool for improving AI behavior."}, 'zh': {'title': 'åˆ©ç”¨äººæ ¼å‘é‡æ§åˆ¶è¯­è¨€æ¨¡å‹çš„äººæ ¼å˜åŒ–', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨äººæ ¼å‘é‡æ¥ç›‘æ§å’Œæ§åˆ¶æ¨¡å‹åœ¨è®­ç»ƒå’Œéƒ¨ç½²è¿‡ç¨‹ä¸­çš„äººæ ¼å˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„æ¿€æ´»ç©ºé—´ä¸­å­˜åœ¨ä¸å¤šç§äººæ ¼ç‰¹å¾ç›¸å…³çš„äººæ ¼å‘é‡ï¼Œä¾‹å¦‚æ¶æ„ã€è°„åªšå’Œå¹»è§‰å€¾å‘ã€‚é€šè¿‡è¿™äº›å‘é‡ï¼Œå¯ä»¥é¢„æµ‹å’Œæ§åˆ¶åœ¨å¾®è°ƒåå¯èƒ½å‡ºç°çš„äººæ ¼å˜åŒ–ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åæœŸå¹²é¢„æ¥å‡è½»è¿™äº›å˜åŒ–ã€‚è¯¥æ–¹æ³•æ˜¯è‡ªåŠ¨åŒ–çš„ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•æ„Ÿå…´è¶£çš„äººæ ¼ç‰¹å¾ï¼Œåªéœ€æä¾›è‡ªç„¶è¯­è¨€æè¿°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23698', 'title': 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents', 'url': 'https://huggingface.co/papers/2507.23698', 'abstract': "Reinforcement Learning enhances generalizable spatial reasoning and interaction in 3D environments through cross-view goal specification and automated task synthesis, achieving zero-shot generalization and improved interaction success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.", 'score': 7, 'issue_id': 5125, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '4cb697aecb943154', 'authors': ['Shaofei Cai', 'Zhancun Mu', 'Haiwen Xia', 'Bowei Zhang', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'School of Computing, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23698.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#games', '#3d', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RL Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² 3D-ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² ÑÑ€ĞµĞ´Ğµ Minecraft Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering 3D Agents with Reinforcement Learning for Generalized Interaction', 'desc': 'This paper discusses how Reinforcement Learning (RL) can improve the ability of agents to understand and interact in 3D environments, like Minecraft. It addresses the problem of RL models overfitting to specific tasks, which limits their ability to generalize to new situations. By using cross-view goal specification and automated task synthesis, the authors show that RL can help agents achieve zero-shot generalization, meaning they can perform well in unseen environments without prior training. The results indicate that RL can significantly enhance interaction success rates and spatial reasoning in diverse settings, including real-world applications.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ï¼šæå‡3Dç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†ä¸äº¤äº’èƒ½åŠ›', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨3Dç¯å¢ƒä¸­é€šè¿‡è·¨è§†è§’ç›®æ ‡æŒ‡å®šå’Œè‡ªåŠ¨åŒ–ä»»åŠ¡åˆæˆï¼Œå¢å¼ºäº†å¯æ¨å¹¿çš„ç©ºé—´æ¨ç†å’Œäº¤äº’èƒ½åŠ›ï¼Œå®ç°äº†é›¶æ ·æœ¬æ³›åŒ–å’Œæé«˜çš„äº¤äº’æˆåŠŸç‡ã€‚æœ¬æ–‡æ¢è®¨äº†RLåœ¨Minecraftä¸­å¾®è°ƒçš„è§†è§‰è¿åŠ¨ä»£ç†å¦‚ä½•åœ¨æœªè§è¿‡çš„ä¸–ç•Œä¸­å®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚æˆ‘ä»¬åˆ†æå¹¶å»ºç«‹äº†è·¨è§†è§’ç›®æ ‡æŒ‡å®šä½œä¸ºè§†è§‰è¿åŠ¨ç­–ç•¥çš„ç»Ÿä¸€å¤šä»»åŠ¡ç›®æ ‡ç©ºé—´ï¼Œä»¥åº”å¯¹å¤šä»»åŠ¡RLè¡¨ç¤ºä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåœ¨é«˜åº¦å¯å®šåˆ¶çš„Minecraftç¯å¢ƒä¸­è¿›è¡Œè‡ªåŠ¨åŒ–ä»»åŠ¡åˆæˆï¼Œä»¥æ”¯æŒå¤§è§„æ¨¡å¤šä»»åŠ¡RLè®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23374', 'title': 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2507.23374', 'abstract': 'NeRF-GS combines Neural Radiance Fields and 3D Gaussian Splatting to enhance 3D scene representation and performance through joint optimization and shared spatial information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.', 'score': 6, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '3bc4d2b12fc82c0f', 'authors': ['Shuangkang Fang', 'I-Chao Shen', 'Takeo Igarashi', 'Yufeng Wang', 'ZeSheng Wang', 'Yi Yang', 'Wenrui Ding', 'Shuchang Zhou'], 'affiliations': ['Beihang University', 'StepFun', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2507.23374.jpg', 'data': {'categories': ['#3d', '#benchmark'], 'emoji': 'ğŸŒŸ', 'ru': {'title': 'NeRF-GS: Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'NeRF-GS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ (NeRF) Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³ (3DGS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ NeRF Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ 3DGS, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼ĞµĞ¶Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸. NeRF-GS Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ 3DGS. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NeRF-GS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½.'}, 'en': {'title': 'Enhancing 3D Scene Representation with NeRF-GS', 'desc': 'NeRF-GS is a new framework that combines Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) to improve how 3D scenes are represented. By optimizing both methods together, it addresses the weaknesses of 3DGS, such as its sensitivity to initial conditions and limited understanding of spatial relationships. The framework aligns the spatial features of 3DGS with those of NeRF, allowing for better performance through shared information. Experiments show that NeRF-GS outperforms existing techniques, highlighting the benefits of integrating these two approaches for enhanced 3D scene representation.'}, 'zh': {'title': 'NeRF-GSï¼šèåˆç¥ç»è¾å°„åœºä¸ä¸‰ç»´é«˜æ–¯ç‚¹äº‘çš„åˆ›æ–°æ¡†æ¶', 'desc': 'NeRF-GSæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ï¼Œé€šè¿‡è”åˆä¼˜åŒ–å’Œå…±äº«ç©ºé—´ä¿¡æ¯æ¥å¢å¼ºä¸‰ç»´åœºæ™¯çš„è¡¨ç¤ºå’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨NeRFçš„è¿ç»­ç©ºé—´è¡¨ç¤ºï¼Œå…‹æœäº†3DGSçš„ä¸€äº›å±€é™æ€§ï¼Œå¦‚å¯¹é«˜æ–¯åˆå§‹åŒ–çš„æ•æ„Ÿæ€§å’Œç©ºé—´æ„è¯†çš„ä¸è¶³ã€‚é€šè¿‡é€æ­¥å¯¹é½3DGSçš„ç©ºé—´ç‰¹å¾ä¸NeRFï¼ŒNeRF-GSä½¿å¾—ä¸¤ç§è¡¨ç¤ºèƒ½å¤Ÿåœ¨åŒä¸€åœºæ™¯ä¸­å…±åŒä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†NeRFå’Œ3DGSæ˜¯äº’è¡¥çš„ï¼Œè€Œéç«äº‰çš„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.21584', 'title': 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination\n  Reduction in MLLMs', 'url': 'https://huggingface.co/papers/2507.21584', 'abstract': 'TARS, a token-adaptive preference strategy, improves multimodal large language models by reducing hallucinations through min-max optimization under semantic constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics.', 'score': 6, 'issue_id': 5128, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ»Ñ', 'en': 'July 29', 'zh': '7æœˆ29æ—¥'}, 'hash': 'e9b8a4abec301022', 'authors': ['Kejia Zhang', 'Keda Tao', 'Zhiming Luo', 'Chang Liu', 'Jiasheng Tang', 'Huan Wang'], 'affiliations': ['AWS AI Lab, Amazon', 'DAMO Academy, Alibaba Group', 'Department of Artificial Intelligence, Xiamen University', 'Hupan Laboratory', 'School of Engineering, Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2507.21584.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#multimodal', '#benchmark', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'TARS: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜', 'desc': 'TARS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ min-max Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. TARS Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'TARS: Reducing Hallucinations in MLLMs with Adaptive Preferences', 'desc': 'The paper introduces TARS, a novel token-adaptive preference strategy designed to enhance multimodal large language models (MLLMs) by minimizing hallucinations. TARS reformulates direct preference optimization (DPO) as a min-max optimization problem, allowing for dynamic adjustments to token-level distributions while adhering to semantic constraints. This approach helps prevent overfitting to fixed preference patterns, which can lead to misleading outputs, by introducing controlled perturbations that maintain causal grounding. The results demonstrate that TARS significantly reduces hallucination rates and improves performance on various benchmarks, outperforming traditional DPO methods.'}, 'zh': {'title': 'TARSï¼šå‡å°‘å¹»è§‰çš„æ™ºèƒ½åå¥½ç­–ç•¥', 'desc': 'TARSæ˜¯ä¸€ç§åŸºäºä»¤ç‰Œè‡ªé€‚åº”çš„åå¥½ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡åœ¨è¯­ä¹‰çº¦æŸä¸‹è¿›è¡Œæœ€å°-æœ€å¤§ä¼˜åŒ–æ¥å‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡ã€‚ä¼ ç»Ÿçš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•é€šå¸¸å°†å¹»è§‰ç›¸å…³çš„åå¥½è§†ä¸ºå›ºå®šç›®æ ‡ï¼Œå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆäºè¡¨é¢è¯­è¨€çº¿ç´¢ã€‚TARSé€šè¿‡é‡æ–°æ„å»ºDPOä¸ºæœ€å°-æœ€å¤§ä¼˜åŒ–é—®é¢˜ï¼Œæœ€å¤§åŒ–ä»¤ç‰Œçº§åˆ«çš„åˆ†å¸ƒå˜åŒ–ï¼ŒåŒæ—¶æœ€å°åŒ–æœŸæœ›çš„åå¥½æŸå¤±ï¼Œä»è€Œä¿æŒå› æœåŸºç¡€å¹¶å‡å°‘å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼ŒTARSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†å¹»è§‰ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.20519', 'title': 'AgroBench: Vision-Language Model Benchmark in Agriculture', 'url': 'https://huggingface.co/papers/2507.20519', 'abstract': 'AgroBench evaluates vision-language models across agricultural tasks, revealing areas for improvement in fine-grained identification, particularly weed identification, with expert-annotated categories.  \t\t\t\t\tAI-generated summary \t\t\t\t Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), a benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers a state-of-the-art range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal that VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most open-source VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/AgroBenchPage/ .', 'score': 4, 'issue_id': 5125, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ»Ñ', 'en': 'July 28', 'zh': '7æœˆ28æ—¥'}, 'hash': 'efa19cc739cbe95e', 'authors': ['Risa Shinoda', 'Nakamasa Inoue', 'Hirokatsu Kataoka', 'Masaki Onishi', 'Yoshitaka Ushiku'], 'affiliations': ['Kyoto University', 'National Institute of Advanced Industrial Science and Technology (AIST)', 'OMRON SINIC', 'The University of Osaka', 'Tokyo Institute of Technology', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2507.20519.jpg', 'data': {'categories': ['#cv', '#open_source', '#science', '#dataset', '#benchmark'], 'emoji': 'ğŸŒ¾', 'ru': {'title': 'AgroBench: ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜ Ğ² ÑĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ñ…Ğ¾Ğ·ÑĞ¹ÑÑ‚Ğ²Ğµ', 'desc': 'AgroBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² ÑĞµĞ»ÑŒÑĞºĞ¾Ñ…Ğ¾Ğ·ÑĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼ÑŒ ÑĞµĞ»ÑŒÑĞºĞ¾Ñ…Ğ¾Ğ·ÑĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 203 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€ Ğ¸ 682 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ĞµĞ¹, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ°Ğ³Ñ€Ğ¾Ğ½Ğ¾Ğ¼Ğ°Ğ¼Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ñ€Ğ½ÑĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Agricultural AI: The AgroBench Benchmark', 'desc': 'AgroBench is a benchmark designed to evaluate vision-language models (VLMs) specifically in the context of agricultural tasks. It focuses on fine-grained identification, such as accurately recognizing different types of weeds and diseases in crops, using categories annotated by expert agronomists. The benchmark includes a comprehensive set of 203 crop categories and 682 disease categories, highlighting the current limitations of VLMs in these areas. The findings indicate that many existing VLMs struggle with precise identification, particularly in weed detection, suggesting significant opportunities for improvement in future model development.'}, 'zh': {'title': 'æå‡å†œä¸šä»»åŠ¡ä¸­çš„è§†è§‰-è¯­è¨€æ¨¡å‹è¡¨ç°', 'desc': 'AgroBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å†œä¸šä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº†ä¸ƒä¸ªå†œä¸šä¸»é¢˜ï¼Œå¹¶ç”±ä¸“å®¶å†œå­¦å®¶è¿›è¡Œæ ‡æ³¨ï¼Œç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„VLMåœ¨ç»†ç²’åº¦è¯†åˆ«ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯æ‚è‰è¯†åˆ«æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè®¸å¤šå¼€æºæ¨¡å‹çš„è¡¨ç°æ¥è¿‘éšæœºã€‚é€šè¿‡åˆ†æVLMçš„é”™è¯¯ç±»å‹ï¼ŒAgroBenchä¸ºæœªæ¥çš„æ¨¡å‹å‘å±•æä¾›äº†æ”¹è¿›çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23436', 'title': 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification', 'url': 'https://huggingface.co/papers/2507.23436', 'abstract': "Enhancing dual-teacher self-supervised frameworks with Kolmogorov-Arnold Networks improves art style classification by better modeling nonlinear feature correlations and disentangling complex style manifolds.  \t\t\t\t\tAI-generated summary \t\t\t\t Art style classification remains a formidable challenge in computational aesthetics due to the scarcity of expertly labeled datasets and the intricate, often nonlinear interplay of stylistic elements. While recent dual-teacher self-supervised frameworks reduce reliance on labeled data, their linear projection layers and localized focus struggle to model global compositional context and complex style-feature interactions. We enhance the dual-teacher knowledge distillation framework to address these limitations by replacing conventional MLP projection and prediction heads with Kolmogorov-Arnold Networks (KANs). Our approach retains complementary guidance from two teacher networks, one emphasizing localized texture and brushstroke patterns, the other capturing broader stylistic hierarchies while leveraging KANs' spline-based activations to model nonlinear feature correlations with mathematical precision. Experiments on WikiArt and Pandora18k demonstrate that our approach outperforms the base dual teacher architecture in Top-1 accuracy. Our findings highlight the importance of KANs in disentangling complex style manifolds, leading to better linear probe accuracy than MLP projections.", 'score': 3, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '43c3429e74f56b34', 'authors': ['Abdellah Zakaria Sellam', 'Salah Eddine Bekhouche', 'Cosimo Distante', 'Abdelmalik Taleb-Ahmed'], 'affiliations': ['Department of Innovation Engineering, University of Salento', 'Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, 73100 Lecce, Italy', 'UPV/EHU, University of the Basque Country, 20018 San Sebastian, Spain', 'UniversitÃ© Polytechnique Hauts-de-France, UniversitÃ© de Lille, CNRS, 59313 Valenciennes, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.23436.jpg', 'data': {'categories': ['#cv', '#math', '#training', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞµÑ‚Ğ¸ ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ MLP-ÑĞ»Ğ¾Ğ¸ Ğ½Ğ° ÑĞµÑ‚Ğ¸ ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ° (KAN) Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²ÑƒÑ… ÑĞµÑ‚ĞµĞ¹-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ ĞºĞ°Ğº Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ñ Ğ´Ğ²ÑƒĞ¼Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Harnessing KANs for Superior Art Style Classification', 'desc': 'This paper presents an enhancement to dual-teacher self-supervised frameworks for art style classification by integrating Kolmogorov-Arnold Networks (KANs). The authors argue that traditional linear projection layers fail to capture the complex, nonlinear relationships between stylistic features. By using KANs, which utilize spline-based activations, the model can better represent these intricate correlations and disentangle complex style manifolds. Experimental results show that this improved framework significantly increases classification accuracy on datasets like WikiArt and Pandora18k compared to the original dual-teacher architecture.'}, 'zh': {'title': 'åˆ©ç”¨KANsæå‡è‰ºæœ¯é£æ ¼åˆ†ç±»çš„å‡†ç¡®æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„åŒæ•™å¸ˆè‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰æ¥æ”¹å–„è‰ºæœ¯é£æ ¼åˆ†ç±»ã€‚ä¼ ç»Ÿçš„çº¿æ€§æŠ•å½±å±‚æ— æ³•æœ‰æ•ˆå»ºæ¨¡å¤æ‚çš„é£æ ¼ç‰¹å¾äº¤äº’ï¼Œè€ŒKANsèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰éçº¿æ€§ç‰¹å¾ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸¤ä¸ªæ•™å¸ˆç½‘ç»œçš„äº’è¡¥æŒ‡å¯¼ï¼Œä¸€ä¸ªä¸“æ³¨äºå±€éƒ¨çº¹ç†å’Œç¬”è§¦æ¨¡å¼ï¼Œå¦ä¸€ä¸ªåˆ™å…³æ³¨æ›´å¹¿æ³›çš„é£æ ¼å±‚æ¬¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨KANsçš„æ¡†æ¶åœ¨WikiArtå’ŒPandora18kæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†åˆ†ç±»å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23632', 'title': 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective', 'url': 'https://huggingface.co/papers/2507.23632', 'abstract': 'Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.', 'score': 2, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'b07ddf6cb6b8bee8', 'authors': ['Gabriel Mongaras', 'Eric C. Larson'], 'affiliations': ['Lyle School of Engineering Southern Methodist University Dallas, TX 75205'], 'pdf_title_img': 'assets/pdf/title_img/2507.23632.jpg', 'data': {'categories': ['#optimization', '#architecture', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞ¸Ğ»Ñƒ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ RNN', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ (RNN). Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Unlocking the Power of Softmax Attention', 'desc': "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."}, 'zh': {'title': 'è½¯maxæ³¨æ„åŠ›çš„ä¼˜åŠ¿è§£æ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†softmaxæ³¨æ„åŠ›ä¸çº¿æ€§æ³¨æ„åŠ›çš„åŒºåˆ«ã€‚softmaxæ³¨æ„åŠ›å› å…¶è¡¨è¾¾èƒ½åŠ›å¼ºè€Œæˆä¸ºç°ä»£å˜æ¢å™¨æ¶æ„çš„åŸºç¡€ï¼Œä½†å…¶åœ¨åºåˆ—é•¿åº¦ä¸Šçš„è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚æ˜¯ä¸€ä¸ªä¸»è¦ç¼ºç‚¹ã€‚é€šè¿‡å°†softmaxéçº¿æ€§æ›¿æ¢ä¸ºçº¿æ€§æ³¨æ„åŠ›ï¼Œç ”ç©¶è€…ä»¬è¯•å›¾è§£å†³è¿™ä¸€ç“¶é¢ˆã€‚æœ¬æ–‡è¡¨æ˜ï¼Œçº¿æ€§æ³¨æ„åŠ›å®é™…ä¸Šæ˜¯softmaxæ³¨æ„åŠ›çš„ä¸€ç§è¿‘ä¼¼ï¼Œå¹¶é€šè¿‡é€’å½’ç¥ç»ç½‘ç»œçš„è¯­è¨€æ¥æè¿°softmaxæ³¨æ„åŠ›çš„å„ä¸ªéƒ¨åˆ†ï¼Œä»è€Œæ­ç¤ºå…¶æ›´å¼ºè¡¨è¾¾èƒ½åŠ›çš„åŸå› ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.14793', 'title': 'Flow Equivariant Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2507.14793', 'abstract': "Equivariant neural network architectures are extended to handle time-parameterized transformations, improving performance in sequence models like RNNs.  \t\t\t\t\tAI-generated summary \t\t\t\t Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.", 'score': 2, 'issue_id': 5126, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ»Ñ', 'en': 'July 20', 'zh': '7æœˆ20æ—¥'}, 'hash': 'eb29bf11c603c730', 'authors': ['T. Anderson Keller'], 'affiliations': ['The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA 15213'], 'pdf_title_img': 'assets/pdf/title_img/2507.14793.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'â³', 'ru': {'title': 'Ğ­ĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (RNN) Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²' - Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ›Ğ¸, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ RNN Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Enhancing RNNs with Time-Parameter Equivariance', 'desc': 'This paper extends equivariant neural network architectures to include time-parameterized transformations, which enhances their performance in sequence models like recurrent neural networks (RNNs). It highlights that traditional RNNs do not adequately account for the smooth, continuous changes in data over time, leading to inefficiencies. By introducing flow equivariance, the authors demonstrate that these new models can better handle temporal symmetries, resulting in improved training speed and generalization capabilities. This work aims to create sequence models that align more closely with the natural transformations observed in the real world.'}, 'zh': {'title': 'æå‡åºåˆ—æ¨¡å‹æ€§èƒ½çš„æ—¶é—´ç­‰å˜ç½‘ç»œ', 'desc': 'æœ¬æ–‡æ‰©å±•äº†ç­‰å˜ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä»¥å¤„ç†æ—¶é—´å‚æ•°åŒ–çš„å˜æ¢ï¼Œä»è€Œæé«˜åºåˆ—æ¨¡å‹ï¼ˆå¦‚RNNï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°æ ‡å‡†çš„RNNé€šå¸¸ä¸å…·å¤‡æµç­‰å˜æ€§ï¼Œæ— æ³•ä»¥å‡ ä½•ç»“æ„çš„æ–¹å¼å¯¹ç§»åŠ¨åˆºæ¿€è¿›è¡Œå˜æ¢ã€‚é€šè¿‡å¼•å…¥æµç­‰å˜æ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒé€Ÿåº¦ã€é•¿åº¦æ³›åŒ–å’Œé€Ÿåº¦æ³›åŒ–ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºéç­‰å˜æ¨¡å‹ã€‚æ­¤ç ”ç©¶ä¸ºæ„å»ºå°Šé‡æ—¶é—´å‚æ•°åŒ–å¯¹ç§°æ€§çš„åºåˆ—æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23404', 'title': 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring', 'url': 'https://huggingface.co/papers/2507.23404', 'abstract': 'An enhanced Dense Passage Retrieval framework for Arabic uses a novel Attentive Relevance Scoring mechanism to improve retrieval performance and ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at https://github.com/Bekhouche/APR{GitHub}.', 'score': 1, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '5e9a40999faf8be8', 'authors': ['Salah Eddine Bekhouche', 'Azeddine Benlamoudi', 'Yazid Bounab', 'Fadi Dornaika', 'Abdenour Hadid'], 'affiliations': ['Faculty of Pharmacy, Helsinki University, Helsinki, Finland', 'IKERBASQUE, Basque Foundation for Science, Bilbao, Spain', 'Lab. de Genie Electrique (LAGE), University of Ouargla, Ouargla, Algeria', 'Sorbonne University Abu Dhabi, Abu Dhabi, UAE', 'University of the Basque Country UPV/EHU, San Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.23404.jpg', 'data': {'categories': ['#architecture', '#open_source', '#low_resource', '#multilingual', '#dataset'], 'emoji': 'ğŸ•Œ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ (Dense Passage Retrieval) Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (Attentive Relevance Scoring), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞšĞ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ° GitHub.'}, 'en': {'title': 'Enhancing Arabic Retrieval with Attentive Relevance Scoring', 'desc': 'This paper introduces an improved Dense Passage Retrieval (DPR) framework tailored for the Arabic language, addressing its unique challenges in natural language processing. The key innovation is the Attentive Relevance Scoring (ARS) mechanism, which enhances the way relevance is assessed between questions and passages. By utilizing pre-trained Arabic language models and refining the architecture, the framework boosts both retrieval performance and ranking accuracy. This advancement aims to better support information retrieval tasks in Arabic, a language that has been underrepresented in NLP research.'}, 'zh': {'title': 'æå‡é˜¿æ‹‰ä¼¯è¯­æ£€ç´¢æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„å¢å¼ºå‹å¯†é›†æ®µè½æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ£€ç´¢æ€§èƒ½å’Œæ’åå‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ³¨æ„åŠ›ç›¸å…³è¯„åˆ†æœºåˆ¶ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„äº¤äº’æœºåˆ¶ï¼Œæ›´æœ‰æ•ˆåœ°å»ºæ¨¡é—®é¢˜ä¸æ®µè½ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒçš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹å’Œæ¶æ„æ”¹è¿›ï¼Œæ˜¾è‘—æå‡äº†å›ç­”é˜¿æ‹‰ä¼¯è¯­é—®é¢˜æ—¶çš„æ£€ç´¢æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€ï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23257', 'title': 'Efficient Machine Unlearning via Influence Approximation', 'url': 'https://huggingface.co/papers/2507.23257', 'abstract': 'The paper introduces the Influence Approximation Unlearning (IAU) algorithm, which leverages incremental learning principles to efficiently address the computational challenges of influence-based unlearning in machine learning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU.', 'score': 0, 'issue_id': 5131, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'e1e0a29f18521e64', 'authors': ['Jiawei Liu', 'Chenwang Wu', 'Defu Lian', 'Enhong Chen'], 'affiliations': ['Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23257.jpg', 'data': {'categories': ['#optimization', '#security', '#training', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Influence Approximation Unlearning (IAU) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. IAU Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ (Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ) Ğ¸ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IAU Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient Unlearning through Incremental Learning: Introducing IAU', 'desc': 'The paper presents the Influence Approximation Unlearning (IAU) algorithm, which aims to improve the efficiency of influence-based unlearning in machine learning models. It addresses the high computational costs associated with traditional methods that require extensive calculations of the Hessian matrix for each training sample. By drawing parallels between the processes of memorizing and forgetting, the authors leverage incremental learning techniques to facilitate more efficient unlearning. Empirical results show that IAU not only ensures effective data removal but also maintains model performance, outperforming existing unlearning methods.'}, 'zh': {'title': 'é«˜æ•ˆé—å¿˜ï¼šå½±å“è¿‘ä¼¼é—å¿˜ç®—æ³•çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå½±å“è¿‘ä¼¼é—å¿˜ï¼ˆIAUï¼‰ç®—æ³•çš„æ–°æ–¹æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å¢é‡å­¦ä¹ çš„åŸç†æ¥é«˜æ•ˆè§£å†³åŸºäºå½±å“çš„é—å¿˜åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„è®¡ç®—æŒ‘æˆ˜ã€‚éšç€éšç§é—®é¢˜çš„æ—¥ç›Šå…³æ³¨ï¼Œæœºå™¨é—å¿˜æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œå°¤å…¶æ˜¯å½±å“åŸºäºé—å¿˜çš„æ–¹æ³•å› å…¶æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹è€Œå—åˆ°é’çã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æµ·æ£®çŸ©é˜µåŠå…¶é€†çŸ©é˜µæ—¶é¢ä¸´å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å»ºç«‹è®°å¿†ï¼ˆå¢é‡å­¦ä¹ ï¼‰ä¸é—å¿˜ï¼ˆé—å¿˜å­¦ä¹ ï¼‰ä¹‹é—´çš„ç†è®ºè”ç³»ï¼ŒIAUç®—æ³•å®ç°äº†æ›´é«˜æ•ˆçš„æœºå™¨é—å¿˜ï¼Œä¸”åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02193', 'title': 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference', 'url': 'https://huggingface.co/papers/2508.02193', 'abstract': 'Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.', 'score': 61, 'issue_id': 5199, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'bec183ec45598da2', 'authors': ['Yuxuan Song', 'Zheng Zhang', 'Cheng Luo', 'Pengyang Gao', 'Fan Xia', 'Hao Luo', 'Zheng Li', 'Yuehang Yang', 'Hongli Yu', 'Xingwei Qu', 'Yuwei Fu', 'Jing Su', 'Ge Zhang', 'Wenhao Huang', 'Mingxuan Wang', 'Lin Yan', 'Xiaoying Jia', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Yonghui Wu', 'Hao Zhou'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2508.02193.jpg', 'data': {'categories': ['#diffusion', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Seed Diffusion Preview - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ½ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° 2146 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° GPU H20, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Mercury Ğ¸ Gemini Diffusion. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Seed Diffusion Preview ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Speed Meets Quality: The Future of Code Generation', 'desc': 'Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion.'}, 'zh': {'title': 'ç§å­æ‰©æ•£é¢„è§ˆï¼šé€Ÿåº¦ä¸è´¨é‡çš„æ–°æ ‡æ†', 'desc': 'Seed Diffusion Previewæ˜¯ä¸€ç§åŸºäºç¦»æ•£çŠ¶æ€æ‰©æ•£çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰æå¿«çš„æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡éé¡ºåºçš„å¹¶è¡Œç”Ÿæˆï¼Œç¦»æ•£æ‰©æ•£æ¨¡å‹æ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œå‡å°‘äº†é€ä¸ªè§£ç çš„å»¶è¿Ÿã€‚è¯¥æ¨¡å‹åœ¨H20 GPUä¸Šå®ç°äº†æ¯ç§’2,146ä¸ªtokençš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶åœ¨æ ‡å‡†ä»£ç è¯„ä¼°åŸºå‡†ä¸Šä¿æŒäº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¸å½“å‰çš„Mercuryå’ŒGemini Diffusionç›¸æ¯”ï¼ŒSeed Diffusion Previewåœ¨é€Ÿåº¦å’Œè´¨é‡ä¸Šéƒ½è®¾ç«‹äº†æ–°çš„æ ‡æ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03320', 'title': 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation', 'url': 'https://huggingface.co/papers/2508.03320', 'abstract': 'Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.', 'score': 42, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '71dc78f7c773cefd', 'authors': ['Peiyu Wang', 'Yi Peng', 'Yimeng Gan', 'Liang Hu', 'Tianyidan Xie', 'Xiaokun Wang', 'Yichen Wei', 'Chuanxin Tang', 'Bo Zhu', 'Changshi Li', 'Hongyang Wei', 'Eric Li', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Multimodality Team, Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03320.jpg', 'data': {'categories': ['#dataset', '#training', '#multimodal', '#architecture', '#open_source'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Skywork UniPic - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GenEval, DPG-Bench Ğ¸ GEditBench-EN. Skywork UniPic Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ±ĞµĞ· Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic', 'desc': 'Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources.'}, 'zh': {'title': 'Skywork UniPicï¼šç»Ÿä¸€å¤šæ¨¡æ€AIçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆ', 'desc': 'Skywork UniPicæ˜¯ä¸€ä¸ªæ‹¥æœ‰15äº¿å‚æ•°çš„è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿç»Ÿä¸€å›¾åƒç†è§£ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸€ä¸ªå•ä¸€æ¶æ„æ¶ˆé™¤äº†å¯¹ç‰¹å®šä»»åŠ¡é€‚é…å™¨æˆ–æ¨¡å—è¿æ¥å™¨çš„éœ€æ±‚ï¼Œå±•ç¤ºäº†ç´§å‡‘çš„å¤šæ¨¡æ€ç³»ç»Ÿåœ¨æ™®é€šç¡¬ä»¶ä¸Šä¹Ÿèƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚Skywork UniPicåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†è®¾è®¡ã€‚è¯¥æ¨¡å‹ä¸ºé«˜ä¿çœŸå¤šæ¨¡æ€AIçš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„èŒƒå¼ï¼Œä¸”ä»£ç å’Œæƒé‡å·²å…¬å¼€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03694', 'title': 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation', 'url': 'https://huggingface.co/papers/2508.03694', 'abstract': 'LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.', 'score': 32, 'issue_id': 5198, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '8c05bd06521b3fb7', 'authors': ['Jianxiong Gao', 'Zhaoxi Chen', 'Xian Liu', 'Jianfeng Feng', 'Chenyang Si', 'Yanwei Fu', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'NVIDIA', 'Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03694.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#multimodal', '#long_context', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'LongVie: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'LongVie - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². LongVie Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality', 'desc': 'LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing.'}, 'zh': {'title': 'è¶…é•¿è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šLongVie', 'desc': 'LongVieæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¶…é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰é€€åŒ–é—®é¢˜ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„å™ªå£°åˆå§‹åŒ–ã€å…¨å±€æ§åˆ¶ä¿¡å·å½’ä¸€åŒ–ã€å¤šæ¨¡æ€æ§åˆ¶å’Œé€€åŒ–æ„ŸçŸ¥è®­ç»ƒæ¥å®ç°è¿™äº›ç›®æ ‡ã€‚LongVieçš„æ ¸å¿ƒè®¾è®¡ç¡®ä¿äº†æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ§åˆ¶æ¡†æ¶æ¥å‡è½»è§†è§‰é€€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongVieåœ¨é•¿æ—¶é—´å¯æ§æ€§ã€ä¸€è‡´æ€§å’Œè´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03686', 'title': 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward', 'url': 'https://huggingface.co/papers/2508.03686', 'abstract': 'CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.', 'score': 22, 'issue_id': 5201, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'dddc5da46c921b94', 'authors': ['Shudong Liu', 'Hongwei Liu', 'Junnan Liu', 'Linchen Xiao', 'Songyang Gao', 'Chengqi Lyu', 'Yuzhe Gu', 'Wenwei Zhang', 'Derek F. Wong', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['NLP2CT Lab', 'Shanghai AI Laboratory', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2508.03686.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#benchmark', '#dataset', '#interpretability', '#optimization'], 'emoji': 'ğŸ§­', 'ru': {'title': 'CompassVerifier: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…', 'desc': 'CompassVerifier - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ VerifierBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. CompassVerifier Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹.'}, 'en': {'title': 'Revolutionizing LLM Output Verification with CompassVerifier', 'desc': 'CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses.'}, 'zh': {'title': 'CompassVerifierï¼šå¤šé¢†åŸŸç­”æ¡ˆéªŒè¯çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆ', 'desc': 'CompassVerifier æ˜¯ä¸€ç§è½»é‡çº§ä¸”ç¨³å¥çš„æ¨¡å‹ï¼Œç”¨äºéªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸçš„è¾“å‡ºã€‚å®ƒé€šè¿‡ VerifierBench è¿™ä¸€å…¨é¢çš„åŸºå‡†æ•°æ®é›†æ¥æ”¯æŒéªŒè¯è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§ç±»å‹çš„ç­”æ¡ˆï¼ŒåŒ…æ‹¬å¤šå­é—®é¢˜ã€å…¬å¼å’Œåºåˆ—ç­”æ¡ˆï¼Œå¹¶æœ‰æ•ˆè¯†åˆ«å¼‚å¸¸æˆ–æ— æ•ˆçš„å“åº”ã€‚æˆ‘ä»¬å¸Œæœ› CompassVerifier å’Œ VerifierBench èƒ½å¤Ÿä¿ƒè¿›ç­”æ¡ˆéªŒè¯ã€è¯„ä¼°åè®®å’Œå¼ºåŒ–å­¦ä¹ ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03012', 'title': 'Tool-integrated Reinforcement Learning for Repo Deep Search', 'url': 'https://huggingface.co/papers/2508.03012', 'abstract': "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.", 'score': 9, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '4ab74a355fed1d76', 'authors': ['Zexiong Ma', 'Chao Peng', 'Qunhong Zeng', 'Pengfei Gao', 'Yanzhen Zou', 'Bing Xie'], 'affiliations': ['Beijing Institute of Technology', 'ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03012.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#agents', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ToolTrain: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ĞºĞ¾Ğ´Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ToolTrain - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ToolTrain, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'ToolTrain: Enhancing LLMs for Superior Issue Localization', 'desc': "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."}, 'zh': {'title': 'ToolTrainï¼šæå‡é—®é¢˜å®šä½çš„æ™ºèƒ½å·¥å…·è®­ç»ƒæ¡†æ¶', 'desc': 'ToolTrainæ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é—®é¢˜å®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚é—®é¢˜å®šä½æ˜¯è¯†åˆ«éœ€è¦ä¿®æ”¹çš„ä»£ç ä½ç½®ä»¥è§£å†³è½¯ä»¶é—®é¢˜çš„è¿‡ç¨‹ï¼Œä½†ç”±äºè‡ªç„¶è¯­è¨€æè¿°ä¸æ•…éšœä»£ç ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œè¿™ä¸€ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ToolTrainé€šè¿‡æ•´åˆä»£ç åº“æ£€ç´¢å·¥å…·ï¼Œå¸®åŠ©LLMsåœ¨å¤šæ­¥éª¤æ¨ç†å’Œå¯¼èˆªè¿‡ç¨‹ä¸­æœ‰æ•ˆåˆ©ç”¨è¿™äº›å·¥å…·ï¼Œä»è€Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToolTrainè®­ç»ƒçš„æ¨¡å‹åœ¨åŠŸèƒ½çº§å®šä½ä¸Šè¶…è¶Šäº†Claude-3.7ï¼Œè¯æ˜äº†é’ˆå¯¹é—®é¢˜å®šä½çš„è®­ç»ƒç­–ç•¥åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ä¸­æ˜¯æœ‰æ•ˆçš„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02091', 'title': 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search', 'url': 'https://huggingface.co/papers/2508.02091', 'abstract': "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", 'score': 7, 'issue_id': 5201, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '10eb53caada711ad', 'authors': ['Xiaoya Li', 'Xiaofei Sun', 'Albert Wang', 'Chris Shum', 'Jiwei Li'], 'affiliations': ['DeepReinforce Team', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.02091.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rag', '#rl', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'CRINN: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'CRINN - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ (ANNS), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ANNS, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. CRINN Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GIST-960-Euclidean Ğ¸ MNIST-784-Euclidean. Ğ£ÑĞ¿ĞµÑ… CRINN Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning', 'desc': 'CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations.'}, 'zh': {'title': 'CRINNï¼šç”¨å¼ºåŒ–å­¦ä¹ åŠ é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢', 'desc': 'CRINNæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•çš„é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å°†è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢çš„ä¼˜åŒ–è§†ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä»¥æ‰§è¡Œé€Ÿåº¦ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCRINNèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé€æ¸æ›´å¿«çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢å®ç°ï¼Œå¹¶æ»¡è¶³å‡†ç¡®æ€§çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRINNåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00367', 'title': 'Representation Shift: Unifying Token Compression with FlashAttention', 'url': 'https://huggingface.co/papers/2508.00367', 'abstract': "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.", 'score': 7, 'issue_id': 5209, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '5a4ad3025ab24bd1', 'authors': ['Joonmyung Choi', 'Sanghyeok Lee', 'Byungoh Ko', 'Eunseo Kim', 'Jihyung Kil', 'Hyunwoo J. Kim'], 'affiliations': ['Adobe Research', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00367.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#optimization', '#video'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Representation Shift, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ FlashAttention, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Representation Shift Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼, Ğ½Ğ¾ Ğ¸ Ğº CNN Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Boosting Video Retrieval Efficiency with Representation Shift', 'desc': "This paper introduces Representation Shift, a new metric that helps improve the efficiency of video-text retrieval and video question answering without needing to retrain models. It combines token compression techniques with FlashAttention, which reduces memory usage and speeds up processing. By measuring how much each token's representation changes, it allows for effective token selection without relying on attention maps. The method works not only with Transformers but also with other model types like CNNs, achieving notable performance improvements."}, 'zh': {'title': 'Representation Shiftï¼šåŠ é€Ÿè§†é¢‘æ£€ç´¢ä¸é—®ç­”çš„åˆ›æ–°æ–¹æ³•', 'desc': 'Representation Shiftæ˜¯ä¸€ç§æ— è®­ç»ƒã€æ¨¡å‹æ— å…³çš„åº¦é‡æ–¹æ³•ï¼Œå®ƒå°†ä»¤ç‰Œå‹ç¼©ä¸FlashAttentionç»“åˆï¼Œæ˜¾è‘—åŠ å¿«è§†é¢‘-æ–‡æœ¬æ£€ç´¢å’Œè§†é¢‘é—®ç­”çš„é€Ÿåº¦ã€‚éšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨¡å‹å’Œä»¤ç‰Œçš„è§„æ¨¡ä¹Ÿåœ¨æ‰©å¤§ï¼Œå¯¼è‡´è‡ªæ³¨æ„åŠ›çš„è®¡ç®—æˆæœ¬å‘ˆå¹³æ–¹å¢é•¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æµ‹é‡æ¯ä¸ªä»¤ç‰Œè¡¨ç¤ºçš„å˜åŒ–ç¨‹åº¦ï¼Œæ¥å®ç°æœ‰æ•ˆçš„ä»¤ç‰Œå‹ç¼©ï¼Œè€Œæ— éœ€æ„å»ºæ³¨æ„åŠ›å›¾æˆ–é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepresentation Shiftåœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢å’Œè§†é¢‘é—®ç­”ä¸­åˆ†åˆ«å®ç°äº†é«˜è¾¾5.5%å’Œ4.4%çš„é€Ÿåº¦æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03050', 'title': 'Multi-human Interactive Talking Dataset', 'url': 'https://huggingface.co/papers/2508.03050', 'abstract': 'MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.', 'score': 6, 'issue_id': 5200, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '01ba126a166568d6', 'authors': ['Zeyu Zhu', 'Weijia Wu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.03050.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MIT - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ CovOG - Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¿Ğ¾Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ñ€Ğ°Ğ¹Ğ²ĞµÑ€. MIT ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 12 Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ 2-4 Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ· Ñ‚ĞµĞ»Ğ° Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. CovOG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿Ğ¾Ğ· Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾.'}, 'en': {'title': 'MIT: Pioneering Multi-Human Talking Video Generation', 'desc': 'The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area.'}, 'zh': {'title': 'MITï¼šå¤šäººäººå¯¹è¯è§†é¢‘ç”Ÿæˆçš„æ–°åŸºå‡†', 'desc': 'MITæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå¤šäººçš„å¯¹è¯è§†é¢‘ç”Ÿæˆï¼ŒåŒ…å«ç»†è‡´çš„æ³¨é‡Šä¿¡æ¯ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•äººç‹¬ç™½æˆ–å­¤ç«‹çš„é¢éƒ¨åŠ¨ç”»ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®å¤šäººçš„äº’åŠ¨ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹ï¼Œæ”¶é›†å’Œæ³¨é‡Šå¤šäººçš„å¯¹è¯è§†é¢‘ï¼Œæ•°æ®é›†åŒ…å«12å°æ—¶çš„é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œå±•ç¤ºäº†è‡ªç„¶çš„å¯¹è¯åŠ¨æ€ã€‚ä¸ºäº†å±•ç¤ºMITçš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†CovOGæ¨¡å‹ï¼Œç»“åˆäº†å¤šäººçš„å§¿æ€ç¼–ç å™¨å’Œäº’åŠ¨éŸ³é¢‘é©±åŠ¨å™¨ï¼Œå±•ç¤ºäº†ç”ŸæˆçœŸå®å¤šäººçš„å¯¹è¯è§†é¢‘çš„å¯è¡Œæ€§å’ŒæŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01119', 'title': 'The Promise of RL for Autoregressive Image Editing', 'url': 'https://huggingface.co/papers/2508.01119', 'abstract': 'Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.', 'score': 6, 'issue_id': 5215, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': 'b7e0974935b60296', 'authors': ['Saba Ahmadi', 'Rabiul Awal', 'Ankur Sikarwar', 'Amirhossein Kazemnejad', 'Ge Ya Luo', 'Juan A. Rodriguez', 'Sai Rajeswar', 'Siva Reddy', 'Christopher Pal', 'Benno Krojer', 'Aishwarya Agrawal'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute', 'Polytechnique MontrÃ©al', 'ServiceNow', 'UniversitÃ© de MontrÃ©al', 'Ã‰cole de Technologie SupÃ©rieure (ETS)'], 'pdf_title_img': 'assets/pdf/title_img/2508.01119.jpg', 'data': {'categories': ['#rl', '#optimization', '#multimodal', '#open_source', '#training', '#games'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EARL, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Reinforcement Learning Meets Multimodal Image Editing', 'desc': 'This paper presents a novel approach to image editing by integrating reinforcement learning (RL) with a large multimodal language model (LLM) verifier within an autoregressive framework. The authors investigate three strategies: supervised fine-tuning, RL, and Chain-of-Thought reasoning, to improve image editing tasks. They demonstrate that the combination of RL and a multimodal LLM verifier significantly enhances performance, leading to the development of EARL, a robust RL-based image editing model. EARL achieves competitive results on various editing tasks while requiring less training data compared to existing methods, marking a significant advancement in autoregressive multimodal models for image editing.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€æ¨¡å‹ç»“åˆï¼Œæå‡å›¾åƒç¼–è¾‘æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹éªŒè¯å™¨æ¥æå‡å›¾åƒç¼–è¾‘æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§ç­–ç•¥ï¼šç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œé“¾å¼æ€ç»´æ¨ç†ï¼Œå¹¶åœ¨ä¸€ä¸ªè‡ªå›å½’å¤šæ¨¡æ€æ¡†æ¶ä¸­è¿›è¡Œç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹éªŒè¯å™¨çš„ç»“åˆæ˜¯æœ€æœ‰æ•ˆçš„ç­–ç•¥ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å‘å¸ƒäº†EARLæ¨¡å‹ï¼Œå®ƒåœ¨å¤šç§å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”è®­ç»ƒæ•°æ®éœ€æ±‚è¾ƒå°‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03613', 'title': 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction', 'url': 'https://huggingface.co/papers/2508.03613', 'abstract': "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.", 'score': 5, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '50044cd9b7eb1802', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Ziran Yang', 'Jui-Hui Chung', 'Haoyu Zhao', 'Lai Jiang', 'Yihan Geng', 'Jiawei Ge', 'Jingruo Sun', 'Jiayun Wu', 'Jiri Gesi', 'Ximing Lu', 'David Acuna', 'Kaiyu Yang', 'Hongzhou Lin', 'Yejin Choi', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'NVIDIA', 'Peking University', 'Princeton Language and Intelligence, Princeton University', 'Shanghai Jiao Tong University', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03613.jpg', 'data': {'categories': ['#rl', '#dataset', '#synthetic', '#reasoning', '#small_models', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°: Goedel-Prover-V2 Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¸Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼', 'desc': 'Goedel-Prover-V2 - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€, Goedel-Prover-V2-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MiniF2F Ğ¸ PutnamBench. ĞĞ° Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ° Goedel-Prover-V2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Goedel-Prover-V2', 'desc': 'Goedel-Prover-V2 is a series of open-source language models that excel in automated theorem proving. It introduces innovative techniques such as scaffolded data synthesis for training on progressively complex tasks, verifier-guided self-correction for iterative proof refinement, and model averaging to enhance output diversity. The models achieve impressive performance metrics, with the smaller Goedel-Prover-V2-8B outperforming larger models like DeepSeek-Prover-V2-671B. Overall, Goedel-Prover-V2 sets a new benchmark in the field, demonstrating superior capabilities in solving mathematical problems with a reduced computational footprint.'}, 'zh': {'title': 'Goedel-Prover-V2ï¼šè‡ªåŠ¨å®šç†è¯æ˜çš„æ–°æ ‡æ†', 'desc': 'Goedel-Prover-V2æ˜¯ä¸€ç³»åˆ—å¼€æºè¯­è¨€æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨å®šç†è¯æ˜é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼šé¦–å…ˆï¼Œä½¿ç”¨æ”¯æ¶æ•°æ®åˆæˆç”Ÿæˆé€æ¸å¢åŠ éš¾åº¦çš„åˆæˆä»»åŠ¡ï¼Œä»¥å¸®åŠ©æ¨¡å‹æŒæ¡å¤æ‚çš„å®šç†ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨éªŒè¯å™¨å¼•å¯¼çš„è‡ªæˆ‘ä¿®æ­£ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®Leanç¼–è¯‘å™¨çš„åé¦ˆè¿­ä»£ä¿®æ­£å…¶è¯æ˜ï¼›æœ€åï¼Œé€šè¿‡æ¨¡å‹å¹³å‡æŠ€æœ¯åˆå¹¶æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥å‡å°‘è®­ç»ƒåæœŸæ¨¡å‹è¾“å‡ºå¤šæ ·æ€§çš„ä¸‹é™ã€‚Goedel-Prover-V2-32Bæ¨¡å‹åœ¨MiniF2Fä¸Šè¾¾åˆ°äº†88.1%çš„é€šè¿‡ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01780', 'title': 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?', 'url': 'https://huggingface.co/papers/2508.01780', 'abstract': "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.", 'score': 5, 'issue_id': 5199, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'b9c09b0ce4e2dad3', 'authors': ['Guozhao Mo', 'Wenliang Zhong', 'Jiawei Chen', 'Xuanang Chen', 'Yaojie Lu', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.01780.jpg', 'data': {'categories': ['#open_source', '#survey', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'LiveMCPBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… MCP-ÑÑ€ĞµĞ´Ğ°Ñ…', 'desc': 'LiveMCPBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ MCP. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 95 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ ÑÑƒĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ LiveMCPTool - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 70 MCP-ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ğ¸ 527 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ LiveMCPEval - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 10 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Claude-Sonnet-4) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 78.95% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Revolutionizing LLM Evaluation in Dynamic MCP Environments', 'desc': 'LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°LLMä»£ç†çš„åŸºå‡†æµ‹è¯•å¹³å°', 'desc': 'LiveMCPBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºå‡†æµ‹è¯•ä»…é™äºå•ä¸€æœåŠ¡å™¨è®¾ç½®çš„é—®é¢˜ï¼Œæä¾›äº†95ä¸ªåŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ç”Ÿæ€ç³»ç»Ÿçš„çœŸå®ä»»åŠ¡ã€‚é€šè¿‡LiveMCPToolï¼Œç ”ç©¶äººå‘˜å¯ä»¥ä½¿ç”¨70ä¸ªMCPæœåŠ¡å™¨å’Œ527ä¸ªå·¥å…·ï¼Œæ”¯æŒå¯æ‰©å±•å’Œå¯é‡å¤çš„è¯„ä¼°æµç¨‹ã€‚æ­¤å¤–ï¼ŒLiveMCPEvalæ¡†æ¶å®ç°äº†è‡ªåŠ¨åŒ–å’Œè‡ªé€‚åº”è¯„ä¼°ï¼Œç¡®ä¿åœ¨åŠ¨æ€ä»»åŠ¡ç¯å¢ƒä¸­ä¸äººç±»è¯„å®¡è€…çš„é«˜ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00477', 'title': 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer', 'url': 'https://huggingface.co/papers/2508.00477', 'abstract': "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.  \t\t\t\t\tAI-generated summary \t\t\t\t In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.", 'score': 4, 'issue_id': 5202, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '94d96ba2b9f92b31', 'authors': ['Yuzhuo Chen', 'Zehua Ma', 'Jianhua Wang', 'Kai Kang', 'Shunyu Yao', 'Weiming Zhang'], 'affiliations': ['East China Normal University', 'Onestory Team', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.00477.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#optimization', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'LAMIC: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸', 'desc': 'LAMIC - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼ Ğ½Ğ° ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Group Isolation Attention Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Region-Modulated Attention Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ°. LAMIC Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ„Ğ¾Ğ½Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'LAMIC: Revolutionizing Multi-Image Synthesis Without Training', 'desc': 'LAMIC is a new framework designed for creating images from multiple references while considering their layout. It builds on existing single-reference diffusion models and introduces attention mechanisms to improve how different elements in the images are handled. This framework allows for high-quality image synthesis without the need for additional training, demonstrating strong performance in maintaining identity, background consistency, and layout control. LAMIC sets a new standard in controllable image composition by effectively managing multiple images in a training-free manner.'}, 'zh': {'title': 'LAMICï¼šæ— è®­ç»ƒçš„å¤šå›¾åƒåˆæˆæ–°èŒƒå¼', 'desc': 'LAMICæ˜¯ä¸€ä¸ªå¸ƒå±€æ„ŸçŸ¥çš„å¤šå›¾åƒåˆæˆæ¡†æ¶ï¼Œå®ƒå°†å•å‚è€ƒæ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°å¤šå‚è€ƒåœºæ™¯ï¼Œä¸”æ— éœ€è®­ç»ƒã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶ï¼šç¾¤ä½“éš”ç¦»æ³¨æ„åŠ›ï¼ˆGIAï¼‰å’ŒåŒºåŸŸè°ƒåˆ¶æ³¨æ„åŠ›ï¼ˆRMAï¼‰ï¼Œä»¥å¢å¼ºå®ä½“åˆ†ç¦»å’Œå¸ƒå±€æ„ŸçŸ¥ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥ä¸‰ç§è¯„ä¼°æŒ‡æ ‡ï¼ŒLAMICåœ¨èº«ä»½ä¿æŒã€èƒŒæ™¯ä¸€è‡´æ€§å’Œå¸ƒå±€æ§åˆ¶ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šå‚è€ƒåŸºçº¿ã€‚LAMICå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¯æ§çš„å¤šå›¾åƒåˆæˆå»ºç«‹äº†æ–°çš„æ— è®­ç»ƒèŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03164', 'title': 'ChartCap: Mitigating Hallucination of Dense Chart Captioning', 'url': 'https://huggingface.co/papers/2508.03164', 'abstract': 'ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.', 'score': 3, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'aec1f860dbfa8231', 'authors': ['Junyoung Lim', 'Jaewoo Ahn', 'Gunhee Kim'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03164.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#dataset', '#data', '#open_source'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ChartCap: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'ChartCap - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 565 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ChartCap Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Visual Consistency Score, Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸.'}, 'en': {'title': 'ChartCap: Elevating Chart Captioning Accuracy and Reducing Hallucinations', 'desc': 'ChartCap is a new dataset designed to enhance the performance of vision language models in generating captions for charts. It contains 565,000 real-world chart images with detailed, type-specific captions that focus on essential structural elements and insights, avoiding irrelevant information. The dataset is created through a four-stage process that ensures high-quality captions, verified by a cycle consistency method for efficient quality control. Experiments show that models trained on ChartCap produce more accurate and informative captions, with fewer hallucinations compared to existing models and even human-generated captions.'}, 'zh': {'title': 'ChartCapï¼šæå‡å›¾è¡¨è¯´æ˜å‡†ç¡®æ€§çš„å…³é”®æ•°æ®é›†', 'desc': 'ChartCapæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«565Kä¸ªçœŸå®ä¸–ç•Œå›¾è¡¨å›¾åƒåŠå…¶ç‰¹å®šç±»å‹çš„è¯¦ç»†è¯´æ˜ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯´æ˜å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘è™šå‡ä¿¡æ¯çš„ç”Ÿæˆã€‚é€šè¿‡è®¾è®¡å››é˜¶æ®µçš„ç”Ÿæˆç®¡é“ï¼ŒChartCapç¡®ä¿è¯´æ˜ä»…åŸºäºå›¾è¡¨ä¸­å¯è¾¨åˆ«çš„æ•°æ®ï¼Œå¹¶é€šè¿‡å¾ªç¯ä¸€è‡´æ€§çš„äººç±»éªŒè¯åŠ é€Ÿè´¨é‡æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºChartCapå¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®å’Œä¿¡æ¯ä¸°å¯Œçš„è¯´æ˜æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äººç±»æ ‡æ³¨çš„è¯´æ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02629', 'title': 'HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents', 'url': 'https://huggingface.co/papers/2508.02629', 'abstract': 'HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.', 'score': 3, 'issue_id': 5212, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '5b4c4a212eae58d6', 'authors': ['Yibin Liu', 'Zhixuan Liang', 'Zanxin Chen', 'Tianxing Chen', 'Mengkang Hu', 'Wanxi Dong', 'Congsheng Xu', 'Zhaoming Han', 'Yusen Qin', 'Yao Mu'], 'affiliations': ['D-Robotics', 'HKU MMLab', 'NEU', 'SJTU ScaleLab', 'SUSTech', 'SZU', 'Shanghai AI Lab', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2508.02629.jpg', 'data': {'categories': ['#robotics', '#optimization', '#agents', '#reasoning', '#games', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'HyCodePolicy - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞµĞµ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. HyCodePolicy ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Robots with Self-Correcting Code Synthesis', 'desc': 'HyCodePolicy is a novel framework that enhances the performance of embodied agents by integrating several advanced techniques. It combines code synthesis, geometric grounding, perceptual monitoring, and iterative repair to create a robust programming cycle. The system translates natural language instructions into executable programs, monitors their execution, and identifies failures using a vision-language model. This approach allows for self-correcting capabilities in program synthesis, improving both the efficiency and reliability of robotic manipulation tasks.'}, 'zh': {'title': 'è‡ªæˆ‘ä¿®æ­£çš„æ™ºèƒ½ä½“ç¼–ç¨‹ç­–ç•¥', 'desc': 'HyCodePolicy æ˜¯ä¸€ç§æ··åˆè¯­è¨€æ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå…·èº«æ™ºèƒ½ä½“ç­–ç•¥çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†ä»£ç åˆæˆã€å‡ ä½•åŸºç¡€ã€æ„ŸçŸ¥ç›‘æ§å’Œè¿­ä»£ä¿®å¤æ•´åˆåˆ°ä¸€ä¸ªé—­ç¯ç¼–ç¨‹å‘¨æœŸä¸­ï¼Œæ¥å®ç°è‡ªæˆ‘ä¿®æ­£çš„ç¨‹åºåˆæˆã€‚å®ƒé¦–å…ˆå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œå¹¶ç”ŸæˆåŸºäºå¯¹è±¡ä¸­å¿ƒå‡ ä½•åŸè¯­çš„å¯æ‰§è¡Œç¨‹åºã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç›‘æ§æ‰§è¡Œè¿‡ç¨‹ï¼ŒHyCodePolicy èƒ½å¤Ÿæ£€æµ‹æ‰§è¡Œå¤±è´¥å¹¶è¿›è¡Œä¿®å¤ï¼Œä»è€Œæé«˜æœºå™¨äººæ“ä½œç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02079', 'title': 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization', 'url': 'https://huggingface.co/papers/2508.02079', 'abstract': 'AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.', 'score': 2, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '25a555fc0f91f562', 'authors': ['Amitava Das', 'Abhilekh Borah', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon AI, USA', 'BITS Goa, India', 'Manipal University, India', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.02079.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#open_source', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'AlignGuard-LoRA (AGL) - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. AGL Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¤Ğ¸ÑˆĞµÑ€Ğ° Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ»Ğ¸Ğ·Ğ¸Ğ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AGL ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ 50% Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Preserving Alignment in Fine-Tuning with AGL', 'desc': 'AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks.'}, 'zh': {'title': 'AlignGuard-LoRAï¼šä¿æŒå¯¹é½ï¼Œç¡®ä¿å®‰å…¨', 'desc': 'AlignGuard-LoRA (AGL) æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥æ­£åˆ™åŒ–æŠ€æœ¯å’Œè¯Šæ–­åŸºå‡†æ¥ä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¯¹é½æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨æ›´æ–°è¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´çš„å¯¹é½æ¼‚ç§»é—®é¢˜ï¼Œä»è€Œå¢å¼ºå®‰å…¨æ€§å’Œè¡Œä¸ºçº¦æŸã€‚AGL é‡‡ç”¨äº†å¤šç§å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬åŸºäºè´¹èˆå°”ä¿¡æ¯çŸ©é˜µçš„æ­£åˆ™åŒ–å’Œä»»åŠ¡ç‰¹å®šçš„æ­£åˆ™åŒ–ï¼Œä»¥ç¨³å®šæ–°çŸ¥è¯†çš„æ•´åˆã€‚å®éªŒè¯æ˜ï¼ŒAGL èƒ½å¤Ÿåœ¨ä¸é™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†å¯¹é½æ¼‚ç§»å‡å°‘å¤šè¾¾ 50%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02630', 'title': 'What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce', 'url': 'https://huggingface.co/papers/2508.02630', 'abstract': 'ACES, a sandbox environment, studies AI agents\' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.  \t\t\t\t\tAI-generated summary \t\t\t\t Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal "top" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '2860dd8bc6a41f92', 'authors': ['Amine Allouah', 'Omar Besbes', 'JosuÃ© D Figueroa', 'Yash Kanoria', 'Akshit Kumar'], 'affiliations': ['Columbia University, Graduate School of Business', 'MyCustomAI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02630.jpg', 'data': {'categories': ['#agents', '#multimodal', '#ethics', '#games', '#alignment'], 'emoji': 'ğŸ›’', 'ru': {'title': 'Ğ˜Ğ˜ Ğ¸Ğ´ĞµÑ‚ Ğ·Ğ° Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ°Ğ¼Ğ¸: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ACES Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ĞºÑƒĞ¿Ğ¾Ğº Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾Ğ½ÑĞ¾Ñ€ÑĞºĞ¸Ñ… Ñ‚ĞµĞ³Ğ¾Ğ², Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, Ñ†ĞµĞ½, Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ˜Ğ˜-Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ‚Ğ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ñ†Ğ¾Ğ², Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ, Ğ³Ğ´Ğµ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜.'}, 'en': {'title': 'Understanding AI Agents in E-Commerce: Insights from ACES', 'desc': "The paper introduces ACES, a sandbox environment designed to analyze the shopping behavior of AI agents in a simulated marketplace. It investigates how these agents respond to various factors such as product position, pricing, ratings, and endorsements, revealing that their preferences can differ significantly from human shoppers. The study finds that while AI agents generally prefer higher-ranked products, their specific choices can vary based on the model used, challenging the idea of a single 'top' product. The findings suggest that sellers can optimize their listings to cater to AI preferences, potentially reshaping strategies in e-commerce as AI-driven shopping becomes more prevalent."}, 'zh': {'title': 'æ¢ç´¢AIä»£ç†åœ¨ç”µå•†ä¸­çš„è´­ç‰©è¡Œä¸º', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†äººå·¥æ™ºèƒ½ä»£ç†åœ¨æ¨¡æ‹Ÿå¸‚åœºä¸­çš„è´­ç‰©è¡Œä¸ºï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºACESçš„æ²™ç›’ç¯å¢ƒã€‚é€šè¿‡éšæœºåŒ–äº§å“ä½ç½®ã€ä»·æ ¼ã€è¯„åˆ†ã€è¯„è®ºå’ŒèµåŠ©æ ‡ç­¾ï¼Œç ”ç©¶äº†ä¸åŒæ¨¡å‹åœ¨è´­ç‰©æ—¶çš„å› æœå…³ç³»å’Œåå¥½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒAIä»£ç†å¯¹äº§å“ä½ç½®æœ‰æ˜æ˜¾çš„åå¥½ï¼Œä½†ä¸åŒæ¨¡å‹çš„é€‰æ‹©å·®å¼‚å¾ˆå¤§ï¼Œä¸”å¯¹ä»·æ ¼å’Œè¯„åˆ†çš„æ•æ„Ÿåº¦ä¸äººç±»ç›¸ä¼¼ä½†å¹…åº¦ä¸åŒã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå–æ–¹å¯ä»¥é€šè¿‡ä¼˜åŒ–äº§å“æè¿°æ¥å¸å¼•AIä¹°å®¶çš„åå¥½ï¼Œä»è€Œåœ¨å¸‚åœºä¸­è·å¾—æ˜¾è‘—çš„ä»½é¢æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02455', 'title': 'TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs', 'url': 'https://huggingface.co/papers/2508.02455', 'abstract': 'A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '70799804b6937703', 'authors': ['Daniele Cipollone', 'Egor Bogomolov', 'Arie van Deursen', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology, Delft, Netherlands', 'JetBrains, Amsterdam, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2508.02455.jpg', 'data': {'categories': ['#plp', '#training', '#optimization'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ° Ğ² IDE Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ½Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¶Ğ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»ÑƒÑ‡ĞµĞ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹, Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑƒĞ¶Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Smart Code Completion with Language Models', 'desc': 'This paper introduces a novel method for ranking code completions in Integrated Development Environments (IDEs) using language models. The approach organizes potential code completions into a prefix tree, allowing for efficient scoring of completions through a single greedy decoding pass. This method enhances the ranking of static code completions by providing a token-aware scoring system without the need for complex techniques like beam search or prompt engineering. The proposed solution is lightweight, model-agnostic, and can be easily integrated into existing IDEs, improving developer assistance significantly.'}, 'zh': {'title': 'æ™ºèƒ½ä»£ç è¡¥å…¨çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„åˆ†æ–¹æ³•ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹å¯¹é™æ€ä»£ç è¡¥å…¨è¿›è¡Œæ’åã€‚è¯¥æ–¹æ³•å°†æ‰€æœ‰æœ‰æ•ˆçš„è¡¥å…¨ç»„ç»‡æˆå‰ç¼€æ ‘ï¼Œå¹¶é€šè¿‡å•æ¬¡è´ªå©ªè§£ç æ¥æ”¶é›†æ¯ä¸ªæ ‡è®°çš„åˆ†æ•°ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰‹å·¥å¯å‘å¼æˆ–è½»é‡çº§æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒé¡¹ç›®å’Œç¼–ç é£æ ¼ä¸­è¿›è¡Œæ³›åŒ–ã€‚æœ€ç»ˆï¼Œè¿™ç§å¿«é€Ÿä¸”ä¸æ¨¡å‹æ— å…³çš„æ–¹æ³•ä¸ºé›†æˆè¯­è¨€æ¨¡å‹åˆ°ç°æœ‰IDEå·¥å…·ä¸­æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02063', 'title': 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs', 'url': 'https://huggingface.co/papers/2508.02063', 'abstract': "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7", 'score': 1, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '0b136776fbb19b26', 'authors': ['Amitava Das', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'BITS Pilani Goa', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02063.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#alignment', '#open_source', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'TraceAlign: Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'TraceAlign - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ˜Ğ½Ğ´ĞµĞºÑ ĞšĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ£Ğ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (BCI), ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹: Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ TraceShield, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Prov-Decode.'}, 'en': {'title': 'TraceAlign: Bridging the Gap in LLM Alignment', 'desc': "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."}, 'zh': {'title': 'TraceAlignï¼šå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ¼‚ç§»çš„åˆ›æ–°æ¡†æ¶', 'desc': 'TraceAlignæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¯†åˆ«å’Œå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¯¹é½æ¼‚ç§»ã€‚å®ƒé€šè¿‡è¿½è¸ªä¸å®‰å…¨çš„ç”Ÿæˆç»“æœåˆ°å…¶è®­ç»ƒæ¥æºï¼Œå¹¶åº”ç”¨å¹²é¢„æªæ–½æ¥å‡å°‘æ¼‚ç§»ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¿¡å¿µå†²çªæŒ‡æ•°ï¼ˆBCIï¼‰ï¼Œé‡åŒ–ç”Ÿæˆå†…å®¹ä¸å¯¹é½æ”¿ç­–ä¹‹é—´çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡ä¸‰ç§äº’è¡¥çš„å¹²é¢„æªæ–½ï¼ŒTraceAlignèƒ½å¤Ÿåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½å¯¹é½æ¼‚ç§»çš„å‘ç”Ÿç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01126', 'title': 'UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation', 'url': 'https://huggingface.co/papers/2508.01126', 'abstract': "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.", 'score': 0, 'issue_id': 5212, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': '8f30093d889bde3a', 'authors': ['Chaitanya Patel', 'Hiroki Nakamura', 'Yuta Kyuragi', 'Kazuki Kozuka', 'Juan Carlos Niebles', 'Ehsan Adeli'], 'affiliations': ['Panasonic Holdings Corporation', 'Panasonic R&D Company of America', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01126.jpg', 'data': {'categories': ['#dataset', '#games', '#benchmark', '#video', '#diffusion', '#multimodal', '#cv', '#healthcare'], 'emoji': 'ğŸ•¶ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ UniEgoMotion Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. UniEgoMotion Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EE4D-Motion Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ EgoExo4D Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Egocentric Motion with UniEgoMotion', 'desc': 'The paper presents UniEgoMotion, a unified conditional motion diffusion model designed for generating and forecasting egocentric motion from first-person images. This model addresses the limitations of existing methods that focus on third-person perspectives and structured 3D scenes, which are not effective in real-world scenarios with dynamic cameras and occlusions. UniEgoMotion utilizes a novel head-centric motion representation and effectively extracts scene context from images to predict plausible 3D motion. The introduction of the EE4D-Motion dataset, which includes pseudo-ground-truth 3D motion annotations, supports the training of this model, achieving state-of-the-art results in egocentric motion tasks.'}, 'zh': {'title': 'è‡ªæˆ‘ä¸­å¿ƒè¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„æ¡ä»¶è¿åŠ¨æ‰©æ•£æ¨¡å‹UniEgoMotionï¼Œç”¨äºä»ç¬¬ä¸€äººç§°å›¾åƒç”Ÿæˆå’Œé¢„æµ‹è‡ªæˆ‘ä¸­å¿ƒçš„è¿åŠ¨ã€‚è¯¥æ¨¡å‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè¿åŠ¨é‡å»ºå’Œé¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿä»…ä»å•å¼ å›¾åƒç”Ÿæˆè¿åŠ¨ã€‚UniEgoMotioné‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„å¤´éƒ¨ä¸­å¿ƒè¿åŠ¨è¡¨ç¤ºï¼Œæ”¯æŒåœ¨æ²¡æœ‰æ˜ç¡®3Dåœºæ™¯çš„æƒ…å†µä¸‹è¿›è¡Œåœºæ™¯æ„ŸçŸ¥çš„è¿åŠ¨åˆæˆã€‚é€šè¿‡å¼•å…¥EE4D-Motionæ•°æ®é›†ï¼Œæœ¬æ–‡ä¸ºè®­ç»ƒæä¾›äº†ä¸°å¯Œçš„ä¼ªçœŸå®3Dè¿åŠ¨æ³¨é‡Šï¼Œæ¨åŠ¨äº†è‡ªæˆ‘ä¸­å¿ƒè¿åŠ¨å»ºæ¨¡çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04026', 'title': 'VeriGUI: Verifiable Long-Chain GUI Dataset', 'url': 'https://huggingface.co/papers/2508.04026', 'abstract': 'VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.', 'score': 116, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '2692487ee60e017e', 'authors': ['Shunyu Liu', 'Minghao Liu', 'Huichi Zhou', 'Zhenyu Cui', 'Yang Zhou', 'Yuhao Zhou', 'Wendong Fan', 'Ge Zhang', 'Jiajun Shi', 'Weihao Xuan', 'Jiaxing Huang', 'Shuang Luo', 'Fang Wu', 'Heli Qi', 'Qingcheng Zeng', 'Ziqi Ren', 'Jialiang Gao', 'Jindi Lv', 'Junjie Wang', 'Aosong Feng', 'Heng Zhou', 'Wangchunshu Zhou', 'Zhenfei Yin', 'Wenlong Zhang', 'Guohao Li', 'Wenhao Yu', 'Irene Li', 'Lei Ma', 'Lei Bai', 'Qunshu Lin', 'Mingli Song', 'Dacheng Tao'], 'affiliations': ['VeriGUI Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.04026.jpg', 'data': {'categories': ['#games', '#agents', '#dataset', '#long_context'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'VeriGUI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VeriGUI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ GUI-Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ´ĞµÑĞºÑ‚Ğ¾Ğ¿Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° VeriGUI Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering GUI Agents for Complex Tasks with VeriGUI', 'desc': 'VeriGUI is a new dataset created to test GUI agents on complex tasks that require many steps. It focuses on breaking down tasks into smaller, manageable subtasks that can be verified individually. This approach allows agents to start from any subtask, making it easier to explore different strategies. The dataset includes detailed task examples from desktop and web environments, showing that current agents struggle with long-term planning and decision-making.'}, 'zh': {'title': 'VeriGUIï¼šé•¿æ—¶é—´ä»»åŠ¡ä¸­çš„æ™ºèƒ½ä½“è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'VeriGUIæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­æ‰§è¡Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ“ä½œçš„æ™ºèƒ½ä½“ã€‚è¯¥æ•°æ®é›†å¼ºè°ƒäº†é•¿é“¾å¤æ‚æ€§å’Œå­ä»»åŠ¡çº§å¯éªŒè¯æ€§ï¼Œå…è®¸ä»»åŠ¡è¢«åˆ†è§£ä¸ºæ•°ç™¾ä¸ªç›¸äº’ä¾èµ–çš„å­ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»»ä½•å­ä»»åŠ¡éƒ½å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„èµ·ç‚¹ï¼Œä¿ƒè¿›äº†å¤šæ ·åŒ–çš„æ¢ç´¢ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ™ºèƒ½ä½“åœ¨å¤„ç†é•¿æ—¶é—´ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œè¡¨æ˜éœ€è¦æ›´å¼ºå¤§çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01191', 'title': 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens', 'url': 'https://huggingface.co/papers/2508.01191', 'abstract': 'CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.', 'score': 107, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': '427ac75c7123b50a', 'authors': ['Chengshuai Zhao', 'Zhen Tan', 'Pingchuan Ma', 'Dawei Li', 'Bohan Jiang', 'Yancheng Wang', 'Yingzhen Yang', 'Huan Liu'], 'affiliations': ['Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01191.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM: Ğ¼Ğ¸Ñ€Ğ°Ğ¶ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ DataAlchemy Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°, Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM.'}, 'en': {'title': 'Unmasking the Fragility of CoT Reasoning in LLMs', 'desc': 'This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs.'}, 'zh': {'title': 'é“¾å¼æ€ç»´æ¨ç†çš„å±€é™æ€§ä¸æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚å¯¹å…¶å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCoTæ¨ç†å¹¶ä¸æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†å½¢å¼ï¼Œå› ä¸ºå®ƒçš„æœ‰æ•ˆæ€§å—åˆ°è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æŸ¥è¯¢ä¹‹é—´åˆ†å¸ƒå·®å¼‚çš„é™åˆ¶ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªåä¸ºDataAlchemyçš„æ§åˆ¶ç¯å¢ƒï¼Œä½œè€…ç³»ç»Ÿæ€§åœ°åˆ†æäº†CoTæ¨ç†åœ¨ä¸åŒä»»åŠ¡ã€é•¿åº¦å’Œæ ¼å¼ä¸‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“CoTæ¨ç†è¶…å‡ºè®­ç»ƒåˆ†å¸ƒæ—¶ï¼Œå…¶æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å®ç°çœŸæ­£å¯æ¨å¹¿æ¨ç†çš„æŒç»­æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02694', 'title': 'Efficient Agents: Building Effective Agents While Reducing Cost', 'url': 'https://huggingface.co/papers/2508.02694', 'abstract': 'A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from 0.398 to 0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.', 'score': 48, 'issue_id': 5222, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '0c518e4f5949dae3', 'authors': ['Ningning Wang', 'Xavier Hu', 'Pai Liu', 'He Zhu', 'Yue Hou', 'Heyuan Huang', 'Shengyu Zhang', 'Jian Yang', 'Jiaheng Liu', 'Ge Zhang', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'OPPO-PersonalAI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02694.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#open_source', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GAIA, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° LLM, Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Efficient Agents, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ 96.7% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ³Ğ¾ open-source Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 28.4%.'}, 'en': {'title': 'Balancing Cost and Performance in AI Agents', 'desc': 'This paper explores how to balance efficiency and effectiveness in systems powered by Large Language Models (LLMs). It identifies the optimal design for agent frameworks that can lower costs while still performing well. The study answers key questions about task complexity, the diminishing returns of adding modules, and how to enhance efficiency through better design. The results show that the proposed Efficient Agents framework maintains high performance while significantly reducing operational costs, making AI solutions more accessible and sustainable.'}, 'zh': {'title': 'é«˜æ•ˆä»£ç†ç³»ç»Ÿï¼šé™ä½æˆæœ¬ä¸ä¿æŒæ€§èƒ½çš„å¹³è¡¡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä»£ç†ç³»ç»Ÿä¸­çš„æ•ˆç‡ä¸æ•ˆæœä¹‹é—´çš„æƒè¡¡ï¼Œæ—¨åœ¨è®¾è®¡å‡ºæ—¢èƒ½é™ä½æˆæœ¬åˆèƒ½ä¿æŒæ€§èƒ½çš„æœ€ä½³ä»£ç†æ¡†æ¶ã€‚æˆ‘ä»¬åˆ†æäº†ä»£ç†ä»»åŠ¡çš„å¤æ‚æ€§ã€é¢å¤–æ¨¡å—çš„è¾¹é™…æ•ˆç›Šä»¥åŠé€šè¿‡é«˜æ•ˆä»£ç†æ¡†æ¶è®¾è®¡æ‰€èƒ½è·å¾—çš„æ•ˆç‡æå‡ã€‚é€šè¿‡å¯¹GAIAåŸºå‡†çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMéª¨å¹²é€‰æ‹©ã€ä»£ç†æ¡†æ¶è®¾è®¡å’Œæµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥çš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼€å‘é«˜æ•ˆä»£ç†ç³»ç»Ÿå¯ä»¥åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è¿è¥æˆæœ¬ï¼Œæ¨åŠ¨AIè§£å†³æ–¹æ¡ˆçš„å¯åŠæ€§å’Œå¯æŒç»­æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04700', 'title': 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience', 'url': 'https://huggingface.co/papers/2508.04700', 'abstract': "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.", 'score': 37, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '6c7450255f7c28bc', 'authors': ['Zeyi Sun', 'Ziyu Liu', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Tong Wu', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.04700.jpg', 'data': {'categories': ['#agents', '#agi', '#open_source', '#optimization', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞŸĞ', 'desc': 'SEAgent - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğµ ĞŸĞ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. SEAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Agents to Learn and Evolve Autonomously', 'desc': 'SEAgent is a self-evolving framework designed for computer-use agents (CUAs) to autonomously learn and master new software through experiential learning. It allows agents to explore unfamiliar software environments and learn from their experiences by tackling tasks that increase in complexity. The framework includes a World State Model for assessing agent performance and a Curriculum Generator for creating diverse tasks. By integrating insights from specialist agents, SEAgent develops a robust generalist CUA that outperforms traditional methods, achieving a notable increase in success rates across various software environments.'}, 'zh': {'title': 'SEAgentï¼šè‡ªä¸»è¿›åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶', 'desc': 'SEAgentæ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿è®¡ç®—æœºä½¿ç”¨ä»£ç†é€šè¿‡ä½“éªŒå­¦ä¹ è‡ªä¸»æŒæ¡æ–°è½¯ä»¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥çš„ä»»åŠ¡è¯¾ç¨‹ï¼Œå¸®åŠ©ä»£ç†åœ¨æ²¡æœ‰äººç±»æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œæ¢ç´¢å’Œå­¦ä¹ é™Œç”Ÿçš„è½¯ä»¶ç¯å¢ƒã€‚SEAgentè®¾è®¡äº†ä¸–ç•ŒçŠ¶æ€æ¨¡å‹å’Œè¯¾ç¨‹ç”Ÿæˆå™¨ï¼Œä»¥ä¾¿ä»£ç†èƒ½å¤Ÿé€šè¿‡è¯•é”™å­¦ä¹ ä¸æ–­æé«˜å…¶èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒSEAgentçš„è¡¨ç°è¶…è¶Šäº†å¤šä¸ªä¸“é—¨åŒ–ä»£ç†çš„ç»„åˆï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ–°è½¯ä»¶ç¯å¢ƒä¸­çš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03501', 'title': 'Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03501', 'abstract': "Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation.   To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agent's success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.", 'score': 29, 'issue_id': 5228, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '9e2fbad63802fc98', 'authors': ['Alexander Golubev', 'Maria Trofimova', 'Sergei Polezhaev', 'Ibragim Badertdinov', 'Maksim Nekrashevich', 'Anton Shevtsov', 'Simon Karasik', 'Sergey Abramov', 'Andrei Andriushchenko', 'Filipp Fisin', 'Sergei Skvortsov', 'Boris Yangel'], 'affiliations': ['Humanoid', 'Nebius AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03501.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RL Ğ´Ğ»Ñ LLM: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DAPO, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2.5-72B-Instruct Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑÑ‚Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified Ñ 20% Ğ´Ğ¾ 39%. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-rebench Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering Language Models with Reinforcement Learning for Real-World Software Engineering', 'desc': 'This paper explores the application of Reinforcement Learning (RL) to Large Language Models (LLMs) in multi-turn interactions, particularly in software engineering tasks. Unlike previous studies that focused on single-turn problems, this research addresses the need for agents to operate in environments that provide feedback after each action. The authors introduce a modified Decoupled Advantage Policy Optimization (DAPO) algorithm to train an agent, Qwen2.5-72B-Instruct, achieving a significant improvement in success rates on software engineering benchmarks. This work demonstrates the potential of RL in enhancing the capabilities of autonomous agents for complex, real-world applications without relying on teacher models.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚ä»»åŠ¡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè½®äº¤äº’çš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è§£è€¦ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ï¼ŒæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªåŸºäºQwen2.5-72B-Instructçš„æ™ºèƒ½ä½“ï¼Œä»¥è§£å†³è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“åœ¨SWE-bench VerifiedåŸºå‡†æµ‹è¯•ä¸­çš„æˆåŠŸç‡ä»20%æå‡è‡³39%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºæ›´å¼ºå¤§çš„è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†å¯è¡Œçš„è·¯å¾„ï¼Œèƒ½å¤Ÿåº”å¯¹å¤æ‚çš„ç°å®é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04280', 'title': 'Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success', 'url': 'https://huggingface.co/papers/2508.04280', 'abstract': 'A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.', 'score': 26, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '007849b63760d8ee', 'authors': ['George Bredis', 'Stanislav Dereka', 'Viacheslav Sinii', 'Ruslan Rakhimov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.04280.jpg', 'data': {'categories': ['#rl', '#training', '#transfer_learning', '#synthetic', '#games', '#rlhf', '#multimodal', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VL-DAC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. VL-DAC Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ PPO Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑ€ĞµĞ´Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'VL-DAC: Simplifying RL for Enhanced Vision-Language Learning', 'desc': 'The paper presents VL-DAC, a novel reinforcement learning (RL) algorithm designed for vision-language models (VLMs) that operates without the need for hyperparameter tuning. This algorithm allows VLMs to learn effective policies from low-cost simulators, enhancing their performance on real-world tasks while maintaining image understanding accuracy. By decoupling the learning of action tokens and value estimation, VL-DAC achieves faster and more stable convergence compared to previous methods. The results demonstrate that training with VL-DAC leads to significant improvements in various benchmarks, showcasing its potential for developing multimodal agents capable of complex tasks.'}, 'zh': {'title': 'è½»é‡çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•VL-DACï¼Œè¯¥ç®—æ³•æ— éœ€è¶…å‚æ•°è°ƒæ•´ï¼Œèƒ½å¤Ÿä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»ä½æˆæœ¬çš„æ¨¡æ‹Ÿå™¨ä¸­å­¦ä¹ é€šç”¨ç­–ç•¥ã€‚VL-DACé€šè¿‡å¯¹åŠ¨ä½œä»¤ç‰Œåº”ç”¨PPOæ›´æ–°ï¼ŒåŒæ—¶ä»…åœ¨ç¯å¢ƒæ­¥éª¤çº§åˆ«å­¦ä¹ ä»·å€¼ï¼Œä»è€Œå®ç°äº†ç®€å•çš„è§£è€¦ï¼Œé¿å…äº†ä¸ç¨³å®šçš„åŠ æƒé¡¹ï¼Œä¿ƒè¿›äº†æ›´å¿«ã€æ›´å¯é çš„æ”¶æ•›ã€‚é€šè¿‡åœ¨å•ä¸ªä½æˆæœ¬æ¨¡æ‹Ÿå™¨ä¸­è®­ç»ƒVLMï¼ŒVL-DACèƒ½å¤Ÿåœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œè€Œä¸å½±å“å›¾åƒç†è§£çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœé¦–æ¬¡è¯æ˜äº†ç®€å•çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥åœ¨å»‰ä»·çš„åˆæˆç¯å¢ƒä¸­å®Œå…¨è®­ç»ƒVLMï¼Œå¹¶åœ¨çœŸå®å›¾åƒçš„ä»£ç†æ§åˆ¶ã€ç©ºé—´æ¨ç†å’Œç½‘é¡µå¯¼èˆªåŸºå‡†ä¸Šå–å¾—å¯æµ‹é‡çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03680', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03680', 'abstract': "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'score': 21, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '78a8398db0f71f63', 'authors': ['Xufang Luo', 'Yuge Zhang', 'Zhiyuan He', 'Zilong Wang', 'Siyun Zhao', 'Dongsheng Li', 'Luna K. Qiu', 'Yuqing Yang'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.03680.jpg', 'data': {'categories': ['#agents', '#rag', '#games', '#math', '#training', '#optimization', '#rl'], 'emoji': 'âš¡', 'ru': {'title': 'Agent Lightning: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Agent Lightning - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… text-to-SQL, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Decoupling Training and Execution for Enhanced AI Agent Performance', 'desc': 'Agent Lightning is a versatile framework designed for training Large Language Models (LLMs) using Reinforcement Learning (RL) techniques. It separates the training process from agent execution, allowing for easier integration with various existing AI agents without significant code changes. By modeling agent execution as a Markov decision process, it introduces a hierarchical RL algorithm called LightningRL, which effectively manages complex interactions and multi-agent scenarios. The framework has shown promising results in tasks like text-to-SQL and retrieval-augmented generation, indicating its effectiveness for real-world applications.'}, 'zh': {'title': 'Agent Lightningï¼šçµæ´»çš„æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶', 'desc': 'Agent Lightningæ˜¯ä¸€ä¸ªçµæ´»çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå„ç§æ™ºèƒ½ä½“è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚å®ƒé€šè¿‡å°†æ‰§è¡Œä¸è®­ç»ƒè§£è€¦ï¼Œä½¿ç”¨å±‚æ¬¡åŒ–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„äº¤äº’é€»è¾‘ã€‚è¯¥æ¡†æ¶å…è®¸ä¸ç°æœ‰æ™ºèƒ½ä½“çš„æ— ç¼é›†æˆï¼Œå‡ ä¹ä¸éœ€è¦ä»£ç ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgent Lightningåœ¨æ–‡æœ¬åˆ°SQLã€å¢å¼ºç”Ÿæˆå’Œæ•°å­¦å·¥å…·ä½¿ç”¨ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å®šçš„æŒç»­æ”¹è¿›ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…æ™ºèƒ½ä½“è®­ç»ƒå’Œéƒ¨ç½²ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03159', 'title': 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction', 'url': 'https://huggingface.co/papers/2508.03159', 'abstract': "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.  \t\t\t\t\tAI-generated summary \t\t\t\t Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.", 'score': 18, 'issue_id': 5222, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '229c53307b839ab7', 'authors': ['Jueon Park', 'Yein Park', 'Minju Song', 'Soyon Park', 'Donghyeon Lee', 'Seungheun Baek', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences, Seoul 04778, Republic of Korea', 'Department of Computer Science and Engineering, Korea University, Seoul 17035, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2508.03159.jpg', 'data': {'categories': ['#science', '#interpretability', '#healthcare', '#reasoning', '#data', '#multimodal'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'CoTox: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'CoTox - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ÑÑ… Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. CoTox Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.'}, 'en': {'title': 'CoTox: Enhancing Drug Toxicity Prediction with LLMs and Chain-of-Thought Reasoning', 'desc': "CoTox is a new framework that combines large language models (LLMs) with chain-of-thought reasoning to predict multi-toxicity in drugs. It enhances the prediction process by integrating chemical structure data, biological pathways, and gene ontology terms, which helps in making the predictions more interpretable. By using step-by-step reasoning, CoTox improves the model's ability to understand complex biological mechanisms and organ-specific toxicities. The framework has shown superior performance compared to traditional machine learning and deep learning models, making it a valuable tool for early-stage drug safety assessment."}, 'zh': {'title': 'CoToxï¼šæå‡è¯ç‰©æ¯’æ€§é¢„æµ‹çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'CoToxæ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸é“¾å¼æ¨ç†ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šç§æ¯’æ€§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡æ•´åˆåŒ–å­¦ç»“æ„æ•°æ®ã€ç”Ÿç‰©é€šè·¯å’ŒåŸºå› æœ¬ä½“æœ¯è¯­ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ¯’æ€§é¢„æµ‹ã€‚ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒCoToxåœ¨è¯ç‰©å¼€å‘ä¸­è¡¨ç°å‡ºæ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶çš„è®¾è®¡ä½¿å¾—æ¯’æ€§é¢„æµ‹ä¸ç”Ÿç†ååº”ç›¸ä¸€è‡´ï¼Œå±•ç¤ºäº†LLMæ¡†æ¶åœ¨è¯ç‰©å®‰å…¨æ€§è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03905', 'title': 'Sotopia-RL: Reward Design for Social Intelligence', 'url': 'https://huggingface.co/papers/2508.03905', 'abstract': 'Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.', 'score': 16, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '815208c1d75b8f05', 'authors': ['Haofei Yu', 'Zhengyang Qi', 'Yining Zhao', 'Kolby Nottingham', 'Keyang Xuan', 'Bodhisattwa Prasad Majumder', 'Hao Zhu', 'Paul Pu Liang', 'Jiaxuan You'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Stanford University', 'University of California Irvine', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.03905.jpg', 'data': {'categories': ['#games', '#alignment', '#rlhf', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Sotopia-RL: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'Sotopia-RL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Sotopia-RL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Enhancing Social Intelligence in LLMs with Sotopia-RL', 'desc': "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."}, 'zh': {'title': 'æå‡ç¤¾äº¤æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶', 'desc': 'Sotopia-RLæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚å®ƒé€šè¿‡å°†åé¦ˆç»†åŒ–ä¸ºå‘è¨€çº§åˆ«çš„å¤šç»´å¥–åŠ±ï¼Œæ¥æ”¹å–„æ¨¡å‹åœ¨ç¤¾äº¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç¤¾äº¤äº’åŠ¨ä¸­çš„éƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’Œå¤šç»´æ€§é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¤æ‚çš„ç¤¾äº¤ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSotopia-RLåœ¨ç¤¾äº¤ç›®æ ‡å®Œæˆè¯„åˆ†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01858', 'title': 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents', 'url': 'https://huggingface.co/papers/2508.01858', 'abstract': 'A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent\'s capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent\'s processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent\'s conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner', 'score': 15, 'issue_id': 5220, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'ad30239b3abef884', 'authors': ['Yuhan Guo', 'Cong Guo', 'Aiwen Sun', 'Hongliang He', 'Xinyu Yang', 'Yue Lu', 'Yingji Zhang', 'Xuntao Guo', 'Dong Zhang', 'Jianzhuang Liu', 'Jiang Duan', 'Yijia Xiao', 'Liangjian Wen', 'Hai-Ming Xu', 'Yong Dai'], 'affiliations': ['Central South University', 'Fudan University', 'Harbin Institute of Technology', 'Hithink Research', 'Shanghai Jiao Tong University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Southwestern University of Finance and Economics', 'University of Adelaide', 'University of California, Los Angeles', 'University of Manchester', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01858.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#reasoning', '#open_source'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Web-CogKnowledge Framework, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Web-CogDataset Ğ¸Ğ· 14 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Web-CogReasoner, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering Web Agents through Structured Knowledge and Reasoning', 'desc': 'This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models.'}, 'zh': {'title': 'æ™ºèƒ½ä½“èƒ½åŠ›çš„åŒé‡åˆ†è§£ï¼šçŸ¥è¯†ä¸è®¤çŸ¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç½‘ç»œæ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œå°†å…¶èƒ½åŠ›åˆ†è§£ä¸ºçŸ¥è¯†å†…å®¹å­¦ä¹ å’Œè®¤çŸ¥è¿‡ç¨‹ã€‚æˆ‘ä»¬å®šä¹‰äº†Web-CogKnowledgeæ¡†æ¶ï¼Œå°†çŸ¥è¯†åˆ†ä¸ºäº‹å®æ€§ã€æ¦‚å¿µæ€§å’Œç¨‹åºæ€§ä¸‰ç±»ï¼Œä»¥æ”¯æŒæ™ºèƒ½ä½“çš„å­¦ä¹ å’Œæ¨ç†ã€‚é€šè¿‡æ„å»ºWeb-CogDatasetï¼Œæˆ‘ä»¬ä¸ºæ™ºèƒ½ä½“æä¾›äº†ç³»ç»ŸåŒ–çš„çŸ¥è¯†åŸºç¡€ï¼Œå¸®åŠ©å…¶æŒæ¡å¿…è¦çš„æ ¸å¿ƒçŸ¥è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†Web-CogReasonerï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„åŒ–çŸ¥è¯†è‡³å…³é‡è¦çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03560', 'title': 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought', 'url': 'https://huggingface.co/papers/2508.03560', 'abstract': 'LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method.', 'score': 12, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '14b52fe9021b5b26', 'authors': ['Yi Gui', 'Zhen Li', 'Zhongyi Zhang', 'Guohao Wang', 'Tianpeng Lv', 'Gaoyang Jiang', 'Yi Liu', 'Dongping Chen', 'Yao Wan', 'Hongyu Zhang', 'Wenbin Jiang', 'Xuanhua Shi', 'Hai Jin'], 'affiliations': ['Chongqing University, Chongqing, China', 'Huazhong University of Science and Technology, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03560.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#architecture', '#multimodal', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'LaTCoder: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†', 'desc': 'LaTCoder - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² ĞºĞ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). LaTCoder Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº TreeBLEU Ğ¸ MAE. Ğ’ Ñ…Ğ¾Ğ´Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ LaTCoder, Ğ±Ñ‹Ğ»Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ñ‹ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 60% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ².'}, 'en': {'title': 'Enhancing Layout Preservation in Design-to-Code with LaTCoder', 'desc': "LaTCoder is a novel approach designed to improve the accuracy of converting webpage designs into code while preserving the layout. It utilizes Chain-of-Thought reasoning to enhance the performance of Multimodal Large Language Models (MLLMs) by breaking down designs into manageable blocks. The method employs two assembly strategies to optimize the final output, ensuring that the generated code closely matches the intended design. Experimental results show significant improvements in both automatic metrics and human preference, indicating LaTCoder's effectiveness in design-to-code tasks."}, 'zh': {'title': 'æå‡ç½‘é¡µè®¾è®¡å¸ƒå±€ä¿ç•™çš„LaTCoder', 'desc': 'LaTCoderæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç½‘é¡µè®¾è®¡åˆ°ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¸ƒå±€ä¿ç•™èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†ç½‘é¡µè®¾è®¡åˆ†å‰²æˆå¤šä¸ªå›¾åƒå—ï¼Œå¹¶ä½¿ç”¨åŸºäºæ€ç»´é“¾çš„æ¨ç†æ–¹æ³•æ¥ç”Ÿæˆæ¯ä¸ªå—çš„ä»£ç ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»å¯¹å®šä½å’ŒåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»„è£…ç­–ç•¥ï¼Œä»¥é€‰æ‹©æœ€ä½³è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaTCoderåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»åå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23785', 'title': 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis', 'url': 'https://huggingface.co/papers/2507.23785', 'abstract': 'A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.', 'score': 12, 'issue_id': 5220, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '47c7686978b9c4dc', 'authors': ['Bowen Zhang', 'Sicheng Xu', 'Chuxin Wang', 'Jiaolong Yang', 'Feng Zhao', 'Dong Chen', 'Baining Guo'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23785.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ VAE Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Objaverse. Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ñ‹, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Worlds', 'desc': 'This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation.'}, 'zh': {'title': 'ä»è§†é¢‘ç”Ÿæˆé«˜è´¨é‡åŠ¨æ€3Då†…å®¹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸ªè§†é¢‘è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€3Då†…å®¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ç›´æ¥çš„4DMeshåˆ°é«˜æ–¯æ ·æ¡ï¼ˆGSï¼‰å˜åˆ†åœºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ç¼–ç 3DåŠ¨ç”»æ•°æ®ä¸­çš„é«˜æ–¯æ ·æ¡åŠå…¶æ—¶é—´å˜åŒ–ã€‚é€šè¿‡è¿™ç§é«˜æ•ˆçš„è¡¨ç¤ºæ–¹å¼ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ—¶é—´æ„ŸçŸ¥çš„é«˜æ–¯å˜åˆ†åœºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥è§†é¢‘å’Œé«˜æ–¯æ ·æ¡ç”ŸæˆåŠ¨æ€å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¤„ç†çœŸå®è§†é¢‘è¾“å…¥æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03789', 'title': 'HPSv3: Towards Wide-Spectrum Human Preference Score', 'url': 'https://huggingface.co/papers/2508.03789', 'abstract': 'HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.', 'score': 11, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'a2a0678cfc88e0ef', 'authors': ['Yuhang Ma', 'Xiaoshi Wu', 'Keqiang Sun', 'Hongsheng Li'], 'affiliations': ['CPII, InnoHK', 'CUHK MMLab', 'Kings College London', 'Mizzen AI', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03789.jpg', 'data': {'categories': ['#alignment', '#data', '#benchmark', '#dataset', '#cv', '#open_source'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'HPSv3: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'HPSv3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HPDv3, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1.08 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ 1.17 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Chain-of-Human-Preference Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Image Generation with Human Preference Score v3', 'desc': 'The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment.'}, 'zh': {'title': 'æå‡å›¾åƒç”Ÿæˆè´¨é‡çš„HPSv3ä¸CoHPæ–¹æ³•', 'desc': 'HPSv3æ˜¯ä¸€ç§äººç±»åå¥½è¯„åˆ†ï¼Œåˆ©ç”¨å¹¿æ³›çš„æ•°æ®é›†å’Œè€ƒè™‘ä¸ç¡®å®šæ€§çš„æ’åæŸå¤±ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬å‘å¸ƒäº†HPDv3ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«108ä¸‡å¯¹æ–‡æœ¬-å›¾åƒå’Œ117ä¸‡å¯¹æ ‡æ³¨æ¯”è¾ƒçš„å¹¿æ³›äººç±»åå¥½æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åå¥½æ¨¡å‹ï¼Œä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ’åæŸå¤±è¿›è¡Œç»†è‡´æ’åã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†äººç±»åå¥½é“¾ï¼ˆCoHPï¼‰ï¼Œé€šè¿‡è¿­ä»£å›¾åƒä¼˜åŒ–ï¼Œåœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€ä½³å›¾åƒï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02215', 'title': 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding', 'url': 'https://huggingface.co/papers/2508.02215', 'abstract': 'LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.', 'score': 9, 'issue_id': 5220, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'caa8de613517d011', 'authors': ['Yike Zhang', 'Zhiyuan He', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yuqing Yang', 'Jianyong Wang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02215.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': 'ğŸ”ª', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LeanK - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ÑÑ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ Ğ² ĞºÑÑˆĞµ ĞºĞ»ÑÑ‡ĞµĞ¹. LeanK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ², ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑÑ‰ĞµĞ¹ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ´Ğ¾ 70% Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² 1,3 Ñ€Ğ°Ğ·Ğ°.'}, 'en': {'title': 'LeanK: Pruning for Efficient Language Model Performance', 'desc': 'LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation.'}, 'zh': {'title': 'LeanKï¼šé«˜æ•ˆè§£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ', 'desc': 'LeanKæ˜¯ä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸é‡è¦çš„å…³é”®ç¼“å­˜é€šé“ï¼Œä»è€Œé™ä½å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿè§£ç ï¼ŒåŒæ—¶ä¸å½±å“å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é™æ€é€šé“ç¨€ç–æ€§ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œå­¦ä¹ æ»¡è¶³ç‰¹å®šç¨€ç–æ¯”å’Œç¡¬ä»¶å¯¹é½è¦æ±‚çš„é€šé“é™æ€æ©ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeanKå¯ä»¥å‡å°‘é«˜è¾¾70%çš„Kç¼“å­˜å’Œ16%-18%çš„Vç¼“å­˜å†…å­˜ï¼Œå¹¶ä¸”è‡ªå®šä¹‰è§£ç å†…æ ¸ä½¿æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æé«˜äº†1.3å€ã€‚é€šè¿‡åˆ†æå­¦ä¹ åˆ°çš„é‡è¦æ€§åˆ†å¸ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å¯¹é•¿ä¸Šä¸‹æ–‡æ¨ç†è¿‡ç¨‹ä¸­æ¨¡å‹é€šé“å’Œæ³¨æ„åŠ›å¤´çš„æ·±å…¥è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02807', 'title': 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework', 'url': 'https://huggingface.co/papers/2508.02807', 'abstract': 'DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. In the second stage, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/', 'score': 8, 'issue_id': 5224, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'f3f693bca2e57a94', 'authors': ['Tongchun Zuo', 'Zaiyu Huang', 'Shuliang Ning', 'Ente Lin', 'Chao Liang', 'Zerong Zheng', 'Jianwen Jiang', 'Yuan Zhang', 'Mingyuan Gao', 'Xin Dong'], 'affiliations': ['ByteDance Intelligent Creation', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02807.jpg', 'data': {'categories': ['#video', '#cv', '#synthetic', '#multimodal', '#diffusion'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'DreamVVT: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'DreamVVT - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Video Try-On with DreamVVT: Consistency Meets Detail', 'desc': 'DreamVVT is a two-stage framework designed to improve video virtual try-on (VVT) by using Diffusion Transformers and LoRA adapters. It effectively utilizes unpaired human-centric data and pretrained models to maintain garment details and ensure temporal consistency in videos. The first stage generates high-quality keyframe images using a multi-frame try-on model and a vision-language model, while the second stage focuses on video generation by incorporating motion and appearance data. This innovative approach allows DreamVVT to outperform existing methods in preserving garment fidelity and achieving smooth motion in dynamic scenarios.'}, 'zh': {'title': 'DreamVVTï¼šæå‡è§†é¢‘è™šæ‹Ÿè¯•ç©¿çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DreamVVTæ˜¯ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å’ŒLoRAé€‚é…å™¨ï¼Œæå‡è§†é¢‘è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨æœªé…å¯¹çš„äººä½“ä¸­å¿ƒæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æœè£…ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¤šå¸§è¯•ç©¿æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸå…³é”®å¸§å›¾åƒï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç¡®ä¿åŠ¨æ€è¿åŠ¨çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamVVTåœ¨çœŸå®åœºæ™¯ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒæœè£…å†…å®¹çš„ç»†èŠ‚å’Œæ—¶é—´ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04664', 'title': 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management', 'url': 'https://huggingface.co/papers/2508.04664', 'abstract': "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.", 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '1575b65bda95c5f9', 'authors': ['Mo Li', 'L. H. Xu', 'Qitai Tan', 'Ting Cao', 'Yunxin Liu'], 'affiliations': ['Independent Researcher', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04664.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Sculptor: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Sculptor Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Sculptor Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Sculptor Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Sculptor: Mastering Memory for Better Long-Context Reasoning', 'desc': 'The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies.'}, 'zh': {'title': 'ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæå‡LLMæ€§èƒ½ï¼', 'desc': 'Sculptoræ˜¯ä¸€ä¸ªç”¨äºä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸»åŠ¨ç®¡ç†æ³¨æ„åŠ›å’Œå·¥ä½œè®°å¿†ï¼Œå‡å°‘äº†å‰æœŸä¿¡æ¯çš„å¹²æ‰°ï¼Œä»è€Œæ”¹å–„æ¨ç†çš„å¯é æ€§ã€‚Sculptoræä¾›äº†ä¸‰ç§å·¥å…·ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ‘˜è¦ã€éšè—ä¸æ¢å¤ï¼Œä»¥åŠæ™ºèƒ½æœç´¢ï¼Œå¸®åŠ©LLMæ›´æœ‰æ•ˆåœ°å¤„ç†ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSculptoråœ¨æ²¡æœ‰ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04586', 'title': 'Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference', 'url': 'https://huggingface.co/papers/2508.04586', 'abstract': 'The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.', 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '0e9d3ff69536a24d', 'authors': ['Nuo Chen', 'Moming Duan', 'Andre Huikai Lin', 'Qian Wang', 'Jiaying Wu', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.04586.jpg', 'data': {'categories': ['#ethics', '#survey'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ¾ÑÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹, ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ»ĞµĞ´, Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ, ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Community-Federated Conference (CFC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµÑ‚Ğ²Ğ¾Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ, Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¹, Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Towards Sustainable AI Conferences: A Community-Federated Approach', 'desc': 'This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research.'}, 'zh': {'title': 'æ„å»ºå¯æŒç»­çš„äººå·¥æ™ºèƒ½ä¼šè®®æ–°æ¨¡å¼', 'desc': 'è¿™ç¯‡è®ºæ–‡è¯Šæ–­äº†äººå·¥æ™ºèƒ½ä¼šè®®çš„ç»“æ„æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬å‘è¡¨ç‡ã€ç¢³è¶³è¿¹ã€è´Ÿé¢ç¤¾åŒºæƒ…ç»ªå’Œåå‹¤æŒ‘æˆ˜ã€‚éšç€ä¼šè®®æ•°é‡çš„å¿«é€Ÿå¢é•¿ï¼Œä¼ ç»Ÿçš„é›†ä¸­å¼ä¼šè®®æ¨¡å¼å˜å¾—è¶Šæ¥è¶Šä¸å¯æŒç»­ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¤¾åŒºçš„è”åˆä¼šè®®æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä¿ƒè¿›ç§‘å­¦ä¼ æ’­çš„å…¬å¹³æ€§å’Œç¤¾åŒºçš„ç¦ç¥‰ã€‚è¯¥æ¨¡å‹å°†åŒè¡Œè¯„å®¡ã€å±•ç¤ºå’Œç½‘ç»œäº¤æµåˆ†å¼€ï¼Œæä¾›äº†ä¸€ç§æ›´å¯æŒç»­ã€åŒ…å®¹å’Œæœ‰éŸ§æ€§çš„äººå·¥æ™ºèƒ½ç ”ç©¶å‘å±•è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01928', 'title': 'IAUNet: Instance-Aware U-Net', 'url': 'https://huggingface.co/papers/2508.01928', 'abstract': 'IAUNet, a query-based U-Net architecture with a lightweight convolutional Pixel decoder and Transformer decoder, outperforms state-of-the-art models in biomedical instance segmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet', 'score': 5, 'issue_id': 5226, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': '0f9cff1ae25f175b', 'authors': ['Yaroslav Prytula', 'Illia Tsiporenko', 'Ali Zeynalli', 'Dmytro Fishman'], 'affiliations': ['Institute of Computer Science, University of Tartu', 'STACC U, Tartu, Estonia', 'Ukrainian Catholic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01928.jpg', 'data': {'categories': ['#architecture', '#cv', '#games', '#benchmark', '#science', '#dataset', '#optimization', '#healthcare'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'IAUNet: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ĞµÑ‚Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'IAUNet - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ U-Net Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ĞºĞ»ĞµÑ‚Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ĞµÑ‚Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½ĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'IAUNet: Revolutionizing Biomedical Instance Segmentation with Query-Based U-Net', 'desc': 'IAUNet is a new architecture designed for biomedical instance segmentation, which helps in identifying and separating individual cells in images. It combines a traditional U-Net structure with a lightweight convolutional Pixel decoder and a Transformer decoder to enhance performance. This model efficiently reduces parameters while improving the segmentation of overlapping objects. The introduction of the 2025 Revvity Full Cell Segmentation Dataset provides a valuable resource for training and benchmarking segmentation models in this field.'}, 'zh': {'title': 'IAUNetï¼šç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²çš„æ–°æ ‡æ†', 'desc': 'IAUNetæ˜¯ä¸€ç§åŸºäºæŸ¥è¯¢çš„U-Netæ¶æ„ï¼Œç»“åˆäº†è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨å’ŒTransformerè§£ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²ä¸­è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚å®ä¾‹åˆ†å‰²åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥å‡†ç¡®åŒºåˆ†é‡å ä¸”å¤§å°ä¸ä¸€çš„ç»†èƒã€‚IAUNeté€šè¿‡å…¨æ–°çš„è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ï¼Œå¹¶å‡å°‘äº†å‚æ•°æ•°é‡ï¼ŒåŒæ—¶å¼•å…¥çš„Transformerè§£ç å™¨èƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šç»†åŒ–ç‰¹å®šå¯¹è±¡çš„ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†2025 Revvityå…¨ç»†èƒåˆ†å‰²æ•°æ®é›†ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04295', 'title': 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation', 'url': 'https://huggingface.co/papers/2508.04295', 'abstract': "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.", 'score': 4, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '0dc09e7a8e2bad95', 'authors': ['Chaofan Wang', 'Tingrui Yu', 'Jie Wang', 'Dong Chen', 'Wenrui Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Beijun Shen'], 'affiliations': ['Huawei Technologies Co., Ltd', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04295.jpg', 'data': {'categories': ['#open_source', '#architecture', '#plp', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'EvoC2Rust: Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñƒ C Ğ² Rust', 'desc': 'EvoC2Rust - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° C Ğ² Rust, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµĞ»ĞµÑ‚Ğ° ĞºĞ¾Ğ´Ğ°. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞ°, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ EvoC2Rust Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'EvoC2Rust: Bridging C to Rust with Safety and Precision', 'desc': 'EvoC2Rust is a novel framework designed to automate the translation of entire C projects into Rust, leveraging a skeleton-guided approach that integrates both rule-based and LLM-based techniques. This method addresses the limitations of existing solutions by ensuring that the translated code maintains both safety and idiomatic Rust syntax. The framework operates in three stages: it first breaks down the C project into modules, then translates functions incrementally, and finally resolves any compilation errors using a combination of LLM and static analysis. Evaluation results show that EvoC2Rust significantly outperforms previous methods in terms of syntax, semantic accuracy, and code safety, making it a robust solution for converting legacy C code to Rust.'}, 'zh': {'title': 'EvoC2Rustï¼šé«˜æ•ˆçš„Cåˆ°Rustè‡ªåŠ¨è½¬æ¢æ¡†æ¶', 'desc': 'EvoC2Rustæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ•´ä¸ªCé¡¹ç›®è½¬æ¢ä¸ºRustä»£ç ã€‚å®ƒé‡‡ç”¨äº†éª¨æ¶å¼•å¯¼çš„æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºè§„åˆ™å’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œä»¥æé«˜ä»£ç çš„è¯­æ³•ã€è¯­ä¹‰å’Œå®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªè¿›åŒ–é˜¶æ®µè¿›è¡Œé¡¹ç›®çº§ç¿»è¯‘ï¼Œé¦–å…ˆå°†Cé¡¹ç›®åˆ†è§£ä¸ºåŠŸèƒ½æ¨¡å—ï¼Œç„¶åé€æ­¥ç¿»è¯‘å‡½æ•°ï¼Œæœ€åé€šè¿‡é›†æˆLLMå’Œé™æ€åˆ†æä¿®å¤ç¼–è¯‘é”™è¯¯ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒEvoC2Ruståœ¨Cåˆ°Rustçš„ç¿»è¯‘ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¯­æ³•å’Œè¯­ä¹‰å‡†ç¡®æ€§åˆ†åˆ«æé«˜äº†17.24%å’Œ14.32%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00222', 'title': 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization', 'url': 'https://huggingface.co/papers/2508.00222', 'abstract': "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.", 'score': 4, 'issue_id': 5224, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'ec45fc053d8a3704', 'authors': ['Yihong Dong', 'Xue Jiang', 'Yongding Tao', 'Huanyu Liu', 'Kechi Zhang', 'Lili Mou', 'Rongyu Cao', 'Yingwei Ma', 'Jue Chen', 'Binhua Li', 'Zhi Jin', 'Fei Huang', 'Yongbin Li', 'Ge Li'], 'affiliations': ['Department of Computing Science, University of Alberta', 'School of Computer Science, Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.00222.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RL-PLUS: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RL-PLUS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). RL-PLUS Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RLVR Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. RL-PLUS Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ»Ñ RLVR.'}, 'en': {'title': 'Breaking Boundaries in LLM Reasoning with RL-PLUS', 'desc': "The paper introduces RL-PLUS, a new hybrid-policy optimization method designed to improve the reasoning abilities of Large Language Models (LLMs). It combines Multiple Importance Sampling and Exploration-Based Advantage Function to enhance the model's performance beyond the limitations of traditional Reinforcement Learning with Verifiable Reward (RLVR). By addressing the issues of distributional mismatch and guiding exploration towards valuable reasoning paths, RL-PLUS effectively prevents capability boundary collapse. Extensive experiments show that RL-PLUS outperforms existing methods on various benchmarks, achieving significant improvements in reasoning tasks."}, 'zh': {'title': 'RL-PLUSï¼šçªç ´æ¨ç†èƒ½åŠ›è¾¹ç•Œçš„åˆ›æ–°æ–¹æ³•', 'desc': 'RL-PLUSæ˜¯ä¸€ç§æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡ç»“åˆå¤šé‡é‡è¦æ€§é‡‡æ ·å’ŒåŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚RLVRï¼‰åœ¨èƒ½åŠ›è¾¹ç•Œå´©æºƒæ–¹é¢çš„å±€é™ã€‚RL-PLUSèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨æ•°æ®ï¼ŒæŒ‡å¯¼æ¨¡å‹æ¢ç´¢é«˜ä»·å€¼çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œå®ç°æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-PLUSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.21974', 'title': 'Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks', 'url': 'https://huggingface.co/papers/2507.21974', 'abstract': 'A lightweight framework using Large Language Models (LLMs) with TeleLogs dataset and a two-stage training methodology improves Root Cause Analysis (RCA) in mobile networks by enhancing interpretability and reasoning quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.', 'score': 4, 'issue_id': 5227, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ»Ñ', 'en': 'July 29', 'zh': '7æœˆ29æ—¥'}, 'hash': '91b127bf8dd7d610', 'authors': ['Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Yibin Kang', 'Haozhe Zhang', 'Merouane Debbah', 'Fadhel Ayed'], 'affiliations': ['Huawei Technologies, China', 'Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.21974.jpg', 'data': {'categories': ['#reasoning', '#rl', '#interpretability', '#dataset', '#training', '#open_source', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TeleLogs Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ (RCA) Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing RCA in Mobile Networks with Domain-Specific LLMs', 'desc': 'This paper presents a lightweight framework that utilizes Large Language Models (LLMs) to enhance Root Cause Analysis (RCA) in mobile networks. It introduces the TeleLogs dataset, which contains annotated troubleshooting problems to evaluate RCA capabilities effectively. The authors propose a two-stage training methodology that combines supervised fine-tuning and reinforcement learning to improve the reasoning quality and interpretability of LLMs. Experimental results show that this approach significantly outperforms existing models, demonstrating the potential of domain-adapted LLMs for practical RCA applications.'}, 'zh': {'title': 'é¢†åŸŸé€‚åº”ä¸æ¨ç†å¢å¼ºçš„æ ¹æœ¬åŸå› åˆ†ææ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ”¹å–„ç§»åŠ¨ç½‘ç»œä¸­çš„æ ¹æœ¬åŸå› åˆ†æï¼ˆRCAï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†TeleLogsæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡æ³¨é‡Šçš„æ•…éšœæ’é™¤é—®é¢˜é›†åˆï¼Œæ—¨åœ¨è¯„ä¼°RCAèƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡äº†LLMsçš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡é¢†åŸŸé€‚åº”çš„LLMsåœ¨ç½‘ç»œæ“ä½œå’Œç®¡ç†ä¸­çš„RCAä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01197', 'title': 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding', 'url': 'https://huggingface.co/papers/2508.01197', 'abstract': 'A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.', 'score': 3, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': 'd7be41190836a7cc', 'authors': ['Zhan Shi', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['College of Computer Science, Zhejiang University, Hangzhou 310027, China', 'College of Software Technology, Zhejiang University', 'Udeer.ai, Hangzhou 310000, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01197.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#3d', '#optimization'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GroundingOcc, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ 2D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Object Perception with 3D Occupancy Grounding', 'desc': 'This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'æå‡è‡ªåŠ¨é©¾é©¶ç‰©ä½“æ„ŸçŸ¥çš„3Då ç”¨åŸºç¡€è§†è§‰å®šä½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ¨¡å‹ï¼Œç”¨äºé€šè¿‡è‡ªç„¶è¯­è¨€å’Œä½“ç´ çº§æ³¨é‡Šè¿›è¡Œ3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ä¸­çš„ç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ã€‚ç°æœ‰çš„è§†è§‰å®šä½ä»»åŠ¡é€šå¸¸ä¾èµ–äºè¾¹ç•Œæ¡†ï¼Œè¿™ç§æ–¹æ³•æ— æ³•æ•æ‰åˆ°ç»†ç²’åº¦çš„ç»†èŠ‚ï¼Œå¯¼è‡´ç‰©ä½“è¡¨ç¤ºä¸å‡†ç¡®ã€‚æˆ‘ä»¬å¼•å…¥çš„GroundingOccæ¨¡å‹é€šè¿‡å¤šæ¨¡æ€å­¦ä¹ ï¼Œç»“åˆè§†è§‰ã€æ–‡æœ¬å’Œç‚¹äº‘ç‰¹å¾ï¼Œä»ç²—åˆ°ç»†åœ°é¢„æµ‹ç‰©ä½“ä½ç½®å’Œå ç”¨ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04632', 'title': 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2508.04632', 'abstract': 'Instruction Following Decorator enhances RLVR by improving sample efficiency, intent alignment, and reducing reward hacking in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.', 'score': 2, 'issue_id': 5228, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '1e91bd1d8a3387f3', 'authors': ['Xu Guo', 'Tianyi Liang', 'Tong Jian', 'Xiaogui Yang', 'Ling-I Wu', 'Chenhui Li', 'Zhihui Lu', 'Qipeng Guo', 'Kai Chen'], 'affiliations': ['East China Normal University', 'Fudan University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.04632.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#alignment', '#training', '#optimization', '#security', '#rl'], 'emoji': 'ğŸ“', 'ru': {'title': 'IFDecorator: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Instruction Following Decorator (IFDecorator) - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. IFDecorator Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ IFDecorator.'}, 'en': {'title': 'Enhancing Instruction Following with Robust Reinforcement Learning', 'desc': 'The paper presents the Instruction Following Decorator (IFDecorator), a framework designed to enhance Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs). IFDecorator improves sample efficiency by using a cooperative-adversarial data flywheel that generates increasingly difficult instruction-verification pairs. It also includes IntentCheck, which ensures that the model aligns with user intent, and trip wires that detect and mitigate reward hacking behaviors. The results show that the Qwen2.5-32B-Instruct-IFDecorator model achieves high accuracy and reduces reward hacking, outperforming larger models like GPT-4o.'}, 'zh': {'title': 'æå‡æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæŒ‡ä»¤è·Ÿéšè£…é¥°å™¨ï¼ˆIFDecoratorï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ¥å¢å¼ºæ ·æœ¬æ•ˆç‡å’Œæ„å›¾å¯¹é½ï¼Œå‡å°‘å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚é¦–å…ˆï¼Œå®ƒåˆ©ç”¨åˆä½œå¯¹æŠ—çš„æ•°æ®é£è½®ç”Ÿæˆè¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤-éªŒè¯å¯¹ï¼›å…¶æ¬¡ï¼ŒIntentCheckæ¨¡å—ç¡®ä¿æ¨¡å‹çš„æ„å›¾å¯¹é½ï¼›æœ€åï¼Œtrip wiresæœºåˆ¶æ£€æµ‹å¹¶æ•æ‰å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨IFDecoratorçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å’Œæ•´ä½“èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04010', 'title': 'HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive\n  Policy Enhancement and Dual-Objective Optimization', 'url': 'https://huggingface.co/papers/2508.04010', 'abstract': 'HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '9a431a1adec12655', 'authors': ['Yurun Chen', 'Xavier Hu', 'Yuhan Liu', 'Keting Yin', 'Juncheng Li', 'Zhuosheng Zhang', 'Shengyu Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04010.jpg', 'data': {'categories': ['#optimization', '#security', '#benchmark', '#architecture', '#agents'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğµ', 'desc': 'HarmonyGuard - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Balancing Safety and Utility in Web Tasks with HarmonyGuard', 'desc': 'HarmonyGuard is a multi-agent framework designed to improve how web agents comply with security policies while completing tasks. It addresses the challenge of balancing safety and utility in dynamic web environments by adaptively updating security policies. The framework includes a Policy Agent that extracts and updates security policies from external documents and a Utility Agent that optimizes task performance based on safety and utility objectives. Evaluations show that HarmonyGuard significantly enhances policy compliance and task completion compared to existing methods.'}, 'zh': {'title': 'HarmonyGuardï¼šå®‰å…¨ä¸æ•ˆç”¨çš„åŒé‡ä¼˜åŒ–', 'desc': 'HarmonyGuardæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”æ›´æ–°å®‰å…¨ç­–ç•¥å’Œä¼˜åŒ–å®‰å…¨æ€§ä¸æ•ˆç”¨çš„åŒé‡ç›®æ ‡ï¼Œå¢å¼ºç½‘ç»œç¯å¢ƒä¸­çš„æ”¿ç­–åˆè§„æ€§å’Œä»»åŠ¡å®Œæˆåº¦ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ”¿ç­–ä»£ç†ï¼Œèƒ½å¤Ÿä»éç»“æ„åŒ–æ–‡æ¡£ä¸­æå–å’Œç»´æŠ¤ç»“æ„åŒ–çš„å®‰å…¨æ”¿ç­–ï¼Œå¹¶æ ¹æ®ä¸æ–­å˜åŒ–çš„å¨èƒæŒç»­æ›´æ–°è¿™äº›æ”¿ç­–ã€‚åŒæ—¶ï¼Œæ•ˆç”¨ä»£ç†é€šè¿‡é©¬å°”å¯å¤«å®æ—¶æ¨ç†è¯„ä¼°å®‰å…¨æ€§å’Œæ•ˆç”¨çš„åŒé‡ç›®æ ‡ï¼Œå¹¶åˆ©ç”¨å…ƒè®¤çŸ¥èƒ½åŠ›è¿›è¡Œä¼˜åŒ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒHarmonyGuardåœ¨æ”¿ç­–åˆè§„æ€§å’Œä»»åŠ¡å®Œæˆåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ”¿ç­–åˆè§„æ€§è¶…è¿‡90%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01778', 'title': 'DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving\n  via Online HD Map Diffusion', 'url': 'https://huggingface.co/papers/2508.01778', 'abstract': 'DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'd5f701aadba28943', 'authors': ['Zhigang Sun', 'Yiru Wang', 'Anqing Jiang', 'Shuo Wang', 'Yu Gao', 'Yuwen Heng', 'Shouyi Zhang', 'An He', 'Hao Jiang', 'Jinhao Chai', 'Zichong Gu', 'Wang Jijun', 'Shichen Tang', 'Lavdim Halilaj', 'Juergen Luettin', 'Hao Sun'], 'affiliations': ['AIR, Tsinghua University, Beijing, China', 'Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China', 'Robert Bosch GmbH', 'School of Communication and Information Engineering, Shanghai University, Shanghai, China', 'Shanghai Jiaotong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01778.jpg', 'data': {'categories': ['#video', '#optimization', '#games', '#benchmark', '#multimodal', '#agents'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ°', 'desc': 'DiffSemanticFusion - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹. ĞĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ HD-ĞºĞ°Ñ€Ñ‚, Ğ³Ğ´Ğµ Ğ¾Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Fusing Data for Smarter Autonomous Driving', 'desc': 'DiffSemanticFusion is a novel framework designed to improve autonomous driving by combining two types of data representations: semantic raster and graph-based models. The framework uses a map diffusion module to enhance the stability and detail of high-definition maps, which are crucial for accurate trajectory prediction and driving performance. By integrating these representations, DiffSemanticFusion leverages their strengths to provide better scene understanding and decision-making for autonomous vehicles. Experiments show that this approach significantly outperforms existing methods in real-world driving scenarios, demonstrating its effectiveness in both trajectory prediction and end-to-end driving tasks.'}, 'zh': {'title': 'èåˆè¯­ä¹‰ä¸å›¾å½¢ï¼Œæå‡è‡ªä¸»é©¾é©¶æ€§èƒ½', 'desc': 'DiffSemanticFusion æ˜¯ä¸€ç§å¢å¼ºè‡ªä¸»é©¾é©¶çš„æ¡†æ¶ï¼Œé€šè¿‡èåˆè¯­ä¹‰æ …æ ¼å’ŒåŸºäºå›¾çš„è¡¨ç¤ºï¼Œåˆ©ç”¨åœ°å›¾æ‰©æ•£æ¨¡å—æ¥æé«˜è½¨è¿¹é¢„æµ‹å’Œç«¯åˆ°ç«¯é©¾é©¶æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ …æ ¼è¡¨ç¤ºçš„è§†è§‰æ¨¡å‹ä¼˜åŠ¿å’Œå›¾è¡¨ç¤ºçš„ç»“æ„ç»†èŠ‚ï¼Œè§£å†³äº†åœ¨çº¿é«˜æ¸…åœ°å›¾ç”Ÿæˆä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„è‡ªä¸»é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†è¯¥æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºåœ¨è½¨è¿¹é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¸Šå‡ä¼˜äºå¤šç§å…ˆè¿›æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒDiffSemanticFusion åœ¨å¤šä¸ªåœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è‡ªä¸»é©¾é©¶è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01630', 'title': 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets', 'url': 'https://huggingface.co/papers/2508.01630', 'abstract': 'OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.', 'score': 2, 'issue_id': 5222, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'aca28561e07e250a', 'authors': ['Maziyar Panahi'], 'affiliations': ['CNRS Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2508.01630.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#dataset', '#training', '#transfer_learning', '#open_source', '#benchmark', '#data'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'OpenMed NER: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'OpenMed NER - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ DAPT Ğ¸ LoRA Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 10 Ğ¸Ğ· 12 ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµÑ‰ĞµÑÑ‚Ğ²Ğ°, Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½Ñ‹ Ğ¸ Ğ²Ğ¸Ğ´Ñ‹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ - Ğ¼ĞµĞ½ĞµĞµ 12 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ÑĞ»ĞµĞ´Ğ¾Ğ¼. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ NER.'}, 'en': {'title': 'OpenMed NER: Efficiently Transforming Biomedical NER with Open-Source Innovation', 'desc': 'OpenMed NER is a collection of open-source transformer models designed for named-entity recognition (NER) in the biomedical field. It utilizes domain-adaptive pre-training (DAPT) and Low-Rank Adaptation (LoRA) to achieve high performance while being computationally efficient. The models were trained on a large dataset of clinical notes and research papers, and they excelled in identifying various biomedical entities, outperforming existing models on multiple benchmarks. This work highlights the potential of open-source solutions to achieve superior results compared to proprietary models, all while maintaining a low environmental impact.'}, 'zh': {'title': 'OpenMed NERï¼šé«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«è§£å†³æ–¹æ¡ˆ', 'desc': 'OpenMed NER æ˜¯ä¸€å¥—å¼€æºçš„å˜æ¢å™¨æ¨¡å‹ï¼Œç»“åˆäº†è½»é‡çº§çš„é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆDAPTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œåœ¨ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹åœ¨350,000æ®µæ¥è‡ªä¼¦ç†æ¥æºçš„ä¸´åºŠç¬”è®°å’Œç ”ç©¶æ–‡çŒ®çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆæå–åŒ»ç–—æ•°æ®ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚OpenMed NER åœ¨12ä¸ªç”Ÿç‰©åŒ»å­¦NERåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†10ä¸ªæ•°æ®é›†çš„æ–°æœ€ä¼˜å¾®F1åˆ†æ•°ï¼Œå°¤å…¶åœ¨ç–¾ç—…å’ŒåŒ–å­¦åŸºå‡†ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡é«˜ï¼Œå•ä¸ªGPUä¸‹è®­ç»ƒæ—¶é—´å°‘äº12å°æ—¶ï¼Œä¸”ç¢³è¶³è¿¹ä½äº1.2å…¬æ–¤CO2eï¼Œé€‚åˆå¸®åŠ©ä»ä¸šè€…éµå®ˆæ•°æ®ä¿æŠ¤å’Œäººå·¥æ™ºèƒ½æ³•è§„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00599', 'title': 'DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior', 'url': 'https://huggingface.co/papers/2508.00599', 'abstract': "DPoser-X, a diffusion-based model, addresses the complexity of 3D human poses using variational diffusion sampling and a novel truncated timestep scheduling method, outperforming existing models across various pose benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.", 'score': 2, 'issue_id': 5235, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '52b3dd3f31a8d796', 'authors': ['Junzhe Lu', 'Jing Lin', 'Hongkun Dou', 'Ailing Zeng', 'Yue Deng', 'Xian Liu', 'Zhongang Cai', 'Lei Yang', 'Yulun Zhang', 'Haoqian Wang', 'Ziwei Liu'], 'affiliations': ['Beihang University', 'Independent Researcher', 'NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00599.jpg', 'data': {'categories': ['#benchmark', '#3d', '#diffusion'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DPoser-X - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. DPoser-X Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·.'}, 'en': {'title': 'Revolutionizing 3D Human Pose Generation with DPoser-X', 'desc': 'DPoser-X is a diffusion-based model designed to improve the generation of 3D human poses by utilizing variational diffusion sampling. It addresses the challenges of modeling complex articulated poses and the lack of high-quality datasets by introducing a novel truncated timestep scheduling method tailored for pose data. The model treats various pose-related tasks as inverse problems, allowing it to effectively learn from both whole-body and part-specific datasets. Extensive testing shows that DPoser-X outperforms existing models, setting a new standard for whole-body human pose modeling.'}, 'zh': {'title': 'DPoser-Xï¼šçªç ´3Däººä½“å§¿æ€å»ºæ¨¡çš„æé™', 'desc': 'DPoser-Xæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³3Däººä½“å§¿æ€å»ºæ¨¡çš„å¤æ‚æ€§ã€‚å®ƒé€šè¿‡å˜åˆ†æ‰©æ•£é‡‡æ ·å’Œæ–°é¢–çš„æˆªæ–­æ—¶é—´è°ƒåº¦æ–¹æ³•ï¼Œæå‡äº†åœ¨å„ç§å§¿æ€åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹å°†å¤šç§å§¿æ€ç›¸å…³ä»»åŠ¡ç»Ÿä¸€ä¸ºé€†é—®é¢˜ï¼Œé€šè¿‡å˜åˆ†æ‰©æ•£é‡‡æ ·è¿›è¡Œæ±‚è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPoser-Xåœ¨å…¨èº«ã€æ‰‹éƒ¨ã€é¢éƒ¨å’Œå…¨èº«å§¿æ€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03983', 'title': 'MiDashengLM: Efficient Audio Understanding with General Audio Captions', 'url': 'https://huggingface.co/papers/2508.03983', 'abstract': 'MiDashengLM is an open audio-language model using general audio captions for efficient and comprehensive audio understanding, offering faster processing and higher throughput compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at https://huggingface.co/mispeech/midashenglm-7b and https://github.com/xiaomi-research/dasheng-lm.', 'score': 1, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '6455762dc1eab458', 'authors': ['Heinrich Dinkel', 'Gang Li', 'Jizhong Liu', 'Jian Luan', 'Yadong Niu', 'Xingwei Sun', 'Tianzi Wang', 'Qiyang Xiao', 'Junbo Zhang', 'Jiahao Zhou'], 'affiliations': ['Horizon Team, MiLM Plus Xiaomi Inc., China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03983.jpg', 'data': {'categories': ['#audio', '#dataset', '#open_source'], 'emoji': 'ğŸ§', 'ru': {'title': 'MiDashengLM: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ°', 'desc': 'MiDashengLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Dasheng, Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR), ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ MiDashengLM Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞµ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞµ Ğ² Ğ¾Ğ´Ğ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. MiDashengLM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 4 Ñ€Ğ°Ğ· Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (TTFT) Ğ¸ Ğ´Ğ¾ 20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Audio Understanding with MiDashengLM', 'desc': 'MiDashengLM is an innovative open audio-language model that enhances audio understanding by utilizing general audio captions. It leverages a unique training dataset called ACAVCaps, which is built from publicly available data, ensuring transparency and reproducibility in its development. Unlike traditional models that focus mainly on Automatic Speech Recognition (ASR), MiDashengLM integrates various audio elements like speech, sound, and music into a unified textual representation. This model significantly improves processing speed, achieving up to 4 times faster time-to-first-token and up to 20 times higher throughput compared to existing models.'}, 'zh': {'title': 'é«˜æ•ˆéŸ³é¢‘ç†è§£çš„å¼€æ”¾æ¨¡å‹', 'desc': 'MiDashengLMæ˜¯ä¸€ç§å¼€æ”¾çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨é€šç”¨éŸ³é¢‘æ ‡é¢˜å®ç°é«˜æ•ˆå’Œå…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´å¿«çš„å¤„ç†é€Ÿåº¦å’Œæ›´é«˜çš„ååé‡ã€‚è¯¥æ¨¡å‹ä¾èµ–äºå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼Œç¡®ä¿äº†é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚MiDashengLMå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆä¸ºä¸€ä¸ªæ–‡æœ¬è¡¨ç¤ºï¼Œèƒ½å¤Ÿå…¨é¢æè¿°å¤æ‚çš„éŸ³é¢‘åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03970', 'title': 'Data and AI governance: Promoting equity, ethics, and fairness in large\n  language models', 'url': 'https://huggingface.co/papers/2508.03970', 'abstract': 'Approaches to govern, assess, and quantify bias in machine learning models, particularly large language models, are discussed, emphasizing data and AI governance frameworks for ethical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications.', 'score': 1, 'issue_id': 5237, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'f7776b273b77fe9b', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Independent Researcher', 'alum.mit.edu', 'kronml.com'], 'pdf_title_img': 'assets/pdf/title_img/2508.03970.jpg', 'data': {'categories': ['#multimodal', '#data', '#ethics', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ­Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² BEATS Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² LLM Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑÑ‚Ğ¸ĞºĞ¸, ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¦ĞµĞ»ÑŒ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ - ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'Ensuring Ethical AI: Governing Bias in Large Language Models', 'desc': "This paper discusses methods to manage and measure bias in machine learning models, especially large language models (LLMs). It introduces the Bias Evaluation and Assessment Test Suite (BEATS) to identify fairness issues throughout the model's life cycle, from development to deployment. The authors propose a governance framework that ensures ethical practices in AI, focusing on bias, fairness, and factual accuracy. By applying these governance strategies, organizations can improve the safety and ethical standards of their AI systems, reducing risks of discrimination and reputational damage."}, 'zh': {'title': 'æ²»ç†åè§ï¼Œæ„å»ºè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½', 'desc': 'æœ¬æ–‡è®¨è®ºäº†å¦‚ä½•åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œç³»ç»Ÿæ€§åœ°ç®¡ç†ã€è¯„ä¼°å’Œé‡åŒ–åè§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®å’Œäººå·¥æ™ºèƒ½æ²»ç†æ¡†æ¶ï¼Œä»¥ç¡®ä¿åœ¨æ¨¡å‹å¼€å‘ã€éªŒè¯å’Œç”Ÿäº§ç›‘æ§çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹åè§ã€ä¼¦ç†ã€å…¬å¹³æ€§å’Œäº‹å®æ€§é—®é¢˜ã€‚é€šè¿‡å®æ–½è¿™ä¸€æ²»ç†æ¡†æ¶ï¼Œç»„ç»‡å¯ä»¥åœ¨ç”Ÿäº§éƒ¨ç½²å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æŒç»­å®æ—¶è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨ç¤¾ä¼šè´£ä»»å’Œä¼¦ç†å¯¹é½çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨çš„åˆ›å»ºå’Œéƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03448', 'title': 'SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering', 'url': 'https://huggingface.co/papers/2508.03448', 'abstract': "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.", 'score': 1, 'issue_id': 5223, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '534674700c0af141', 'authors': ['Jan Melechovsky', 'Ambuj Mehrish', 'Dorien Herremans'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2508.03448.jpg', 'data': {'categories': ['#audio', '#dataset', '#synthetic'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'SonicMaster - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. SonicMaster Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ¿Ğ°Ñ€Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞºĞ¾Ğ².'}, 'en': {'title': 'SonicMaster: Revolutionizing Music Restoration with AI', 'desc': 'SonicMaster is a novel generative model designed to enhance music audio quality by correcting various audio artifacts. It utilizes text-based control to allow users to specify desired improvements, making it versatile for both targeted and automatic restoration. The model is trained on a large dataset that pairs degraded audio tracks with their high-quality counterparts, using a flow-matching generative training approach. Results show that SonicMaster significantly enhances sound quality, as confirmed by both objective metrics and subjective listener preferences.'}, 'zh': {'title': 'SonicMasterï¼šéŸ³ä¹éŸ³é¢‘è´¨é‡çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹', 'desc': 'SonicMasteræ˜¯ä¸€ç§ç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æ§åˆ¶å’ŒæµåŒ¹é…ç”Ÿæˆè®­ç»ƒèŒƒå¼æ¥æ”¹å–„éŸ³ä¹éŸ³é¢‘è´¨é‡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§éŸ³é¢‘ä¼ªå½±ï¼Œå¦‚æ··å“è¿‡åº¦ã€å¤±çœŸå’ŒéŸ³è°ƒä¸å¹³è¡¡ç­‰ï¼Œå°¤å…¶é€‚ç”¨äºéä¸“ä¸šç¯å¢ƒä¸‹å½•åˆ¶çš„éŸ³ä¹ã€‚SonicMasteré€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¢å¼ºï¼Œæˆ–åœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹è¿›è¡Œä¸€èˆ¬ä¿®å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSonicMasteråœ¨æ‰€æœ‰ä¼ªå½±ç±»åˆ«ä¸­æ˜¾è‘—æé«˜äº†éŸ³è´¨ï¼Œä¸”å¬ä¼—æ›´å–œæ¬¢å…¶å¢å¼ºçš„è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03178', 'title': 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following', 'url': 'https://huggingface.co/papers/2508.03178', 'abstract': 'A framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'b68fc5d4f6ed55e1', 'authors': ['Chenyang Wang', 'Liang Wen', 'Shousheng Jia', 'Xiangzheng Zhang', 'Liang Xu'], 'affiliations': ['CLUE', 'Harbin Institute of Technology', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.03178.jpg', 'data': {'categories': ['#optimization', '#rl', '#dataset', '#benchmark', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ´Ğ»Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Instruction Adherence in LLMs through Rigorous Reasoning', 'desc': 'This paper presents a new framework that enhances the ability of large language models (LLMs) to follow complex instructions by improving their reasoning processes. The authors identify that poor instruction adherence is often due to lazy reasoning, particularly during the initial thinking phase. To address this, they introduce an entropy-preserving supervised fine-tuning method and a token-wise entropy-adaptive reinforcement learning strategy, which together promote rigorous reasoning through self-checking and previewing. Their experiments show that this approach significantly boosts performance on instruction-following tasks, outperforming both larger open-source and closed-source models.'}, 'zh': {'title': 'æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æ¨ç†æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡ä¿æŒç†µçš„ç›‘ç£å¾®è°ƒå’Œé€è¯ç†µè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ï¼Œæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæ‡’æƒ°æ¨ç†æ˜¯å¯¼è‡´æŒ‡ä»¤éµå¾ªä¸ä½³çš„ä¸»è¦åŸå› ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œä¿ƒè¿›ä¸¥æ ¼çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è§ˆå’Œè‡ªæˆ‘æ£€æŸ¥ã€‚æˆ‘ä»¬ç”Ÿæˆäº†å…·æœ‰å¤æ‚çº¦æŸçš„æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡è¿‡æ»¤è¿‡ç¨‹è·å¾—æœ‰æ•ˆçš„æç¤ºï¼Œæœ€ç»ˆå½¢æˆäº†ä¸‰ä¸ªä¸åŒçš„æç¤ºæ•°æ®é›†ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬çš„Light-IF-32Bæ¨¡å‹è¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„å¼€æºå’Œé—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01311', 'title': 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with\n  Learnable Advisor', 'url': 'https://huggingface.co/papers/2508.01311', 'abstract': 'A continual learning framework for 3D anomaly detection uses Kernel Attention mechanisms and parameter perturbation to handle multiple and emerging classes of point clouds.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-precision industrial products. However, existing methods are typically trained in a class-specific manner and also lack the capability of learning from emerging classes. In this study, we proposed a continual learning framework named Continual 3D Anomaly Detection (C3D-AD), which can not only learn generalized representations for multi-class point clouds but also handle new classes emerging over time.Specifically, in the feature extraction module, to extract generalized local features from diverse product types of different tasks efficiently, Kernel Attention with random feature Layer (KAL) is introduced, which normalizes the feature space. Then, to reconstruct data correctly and continually, an efficient Kernel Attention with learnable Advisor (KAA) mechanism is proposed, which learns the information from new categories while discarding redundant old information within both the encoder and decoder. Finally, to keep the representation consistency over tasks, a Reconstruction with Parameter Perturbation (RPP) module is proposed by designing a representation rehearsal loss function, which ensures that the model remembers previous category information and returns category-adaptive representation.Extensive experiments on three public datasets demonstrate the effectiveness of the proposed method, achieving an average performance of 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD, respectively.', 'score': 1, 'issue_id': 5230, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': 'af26fb044fbca284', 'authors': ['Haoquan Lu', 'Hanzhe Liang', 'Jie Zhang', 'Chenxi Hu', 'Jinbao Wang', 'Can Gao'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University', 'Faculty of Applied Sciences, Macao Polytechnic University', 'Guangdong Provincial Key Laboratory of Intelligent Information Processing', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology', 'School of Artificial Intelligence, Shenzhen University', 'Shenzhen Audencia Financial Technology Institute, Shenzhen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01311.jpg', 'data': {'categories': ['#training', '#3d', '#dataset', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ 3D-Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ»Ğ°ÑÑĞ°Ğ¼', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ C3D-AD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² 3D-Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ´ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Adapting to Anomalies: Continual Learning in 3D Detection', 'desc': 'This paper presents a continual learning framework called Continual 3D Anomaly Detection (C3D-AD) designed for detecting anomalies in 3D point clouds. It addresses the limitations of traditional methods that are class-specific and unable to adapt to new classes over time. The framework utilizes Kernel Attention mechanisms to efficiently extract generalized features and incorporates a parameter perturbation strategy to maintain representation consistency across tasks. Experimental results on multiple datasets show that C3D-AD significantly improves anomaly detection performance while adapting to emerging classes.'}, 'zh': {'title': 'æŒç»­å­¦ä¹ ï¼Œæ™ºèƒ½æ£€æµ‹3Då¼‚å¸¸', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºC3D-ADçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œç”¨äº3Då¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ ¸æ³¨æ„åŠ›æœºåˆ¶å’Œå‚æ•°æ‰°åŠ¨ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç±»ç‚¹äº‘æ•°æ®å¹¶é€‚åº”æ–°å‡ºç°çš„ç±»åˆ«ã€‚é€šè¿‡å¼•å…¥éšæœºç‰¹å¾å±‚çš„æ ¸æ³¨æ„åŠ›ï¼Œæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆæå–å¤šæ ·åŒ–äº§å“ç±»å‹çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œé‡å»ºæ¨¡å—é€šè¿‡å­¦ä¹ æ–°ç±»åˆ«çš„ä¿¡æ¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶ä»èƒ½ä¿æŒå¯¹æ—§ç±»åˆ«çš„è®°å¿†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00428', 'title': 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D\n  Generation', 'url': 'https://huggingface.co/papers/2508.00428', 'abstract': 'Sel3DCraft enhances text-to-3D generation through a dual-branch retrieval and generation system, multi-view hybrid scoring with MLLMs, and prompt-driven visual analytics, improving designer creativity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '7c82937efeb350cb', 'authors': ['Nan Xiang', 'Tianyi Liang', 'Haiwen Huang', 'Shiqi Jiang', 'Hao Huang', 'Yifei Huang', 'Liangyu Chen', 'Changbo Wang', 'Chenhui Li'], 'affiliations': ['East China Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00428.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Sel3DCraft: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Sel3DCraft - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Sel3DCraft Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ´ĞµÑ„ĞµĞºÑ‚Ñ‹.'}, 'en': {'title': 'Transforming Text-to-3D Generation for Enhanced Creativity', 'desc': 'Sel3DCraft is a novel system designed to improve text-to-3D (T23D) generation by integrating a dual-branch approach that combines retrieval and generation methods. This system addresses the challenges of 3D model creation by implementing multi-view hybrid scoring, which utilizes machine learning language models (MLLMs) to ensure consistency and quality in the generated outputs. Additionally, Sel3DCraft features a prompt-driven visual analytics tool that helps designers identify and refine defects in their 3D models more intuitively. Overall, extensive testing shows that Sel3DCraft significantly enhances the creative process for designers compared to existing T23D systems.'}, 'zh': {'title': 'æå‡è®¾è®¡å¸ˆåˆ›é€ åŠ›çš„ä¸‰ç»´ç”Ÿæˆç³»ç»Ÿ', 'desc': 'Sel3DCraft æ˜¯ä¸€ä¸ªå¢å¼ºæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆçš„ç³»ç»Ÿï¼Œé‡‡ç”¨åŒåˆ†æ”¯æ£€ç´¢å’Œç”Ÿæˆç»“æ„ï¼Œæå‡è®¾è®¡å¸ˆçš„åˆ›é€ åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šè§†è§’æ··åˆè¯„åˆ†å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯„ä¼°ä¸‰ç»´æ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ç›²ç›®è¯•é”™é—®é¢˜ã€‚å®ƒè¿˜å¼•å…¥äº†åŸºäºæç¤ºçš„è§†è§‰åˆ†æå·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·ç›´è§‚åœ°è¯†åˆ«å’Œæ”¹è¿›ç¼ºé™·ã€‚ç»è¿‡å¹¿æ³›æµ‹è¯•ï¼ŒSel3DCraft åœ¨æ”¯æŒè®¾è®¡å¸ˆåˆ›é€ åŠ›æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23313', 'title': 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2507.23313', 'abstract': 'Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.', 'score': 1, 'issue_id': 5221, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'c584e9c932383ec6', 'authors': ['Alfio Ferrara', 'Sergio Picascia', 'Elisabetta Rocchetti'], 'affiliations': ['Department of Computer Science, UniversitÃ  degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2507.23313.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multimodal', '#interpretability', '#cv', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°: ĞºĞ°Ğº Ğ˜Ğ˜ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ğ¸Ğ»ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ½ĞµÑÑ‚Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑÑ‚Ğ¸Ğ»Ñ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ½Ğ° Ñ„Ğ¾Ğ½ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ.'}, 'en': {'title': 'Decoding Art: Understanding Content and Style in AI-Generated Images', 'desc': 'This paper explores how transformer-based text-to-image diffusion models generate artworks by analyzing their internal representation of content and style. Using cross-attention heatmaps, the authors investigate how different prompt tokens influence specific regions of generated images, revealing a separation between content and style. The study finds that content tokens mainly affect object-related areas, while style tokens influence backgrounds and textures, indicating an emergent understanding of these concepts. This research enhances our knowledge of generative models and their ability to represent complex artistic ideas without direct supervision.'}, 'zh': {'title': 'æ¢ç´¢å†…å®¹ä¸é£æ ¼çš„åˆ†ç¦»ï¼šæ‰©æ•£æ¨¡å‹çš„è‰ºæœ¯ç”Ÿæˆ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºå˜æ¢å™¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶å¦‚ä½•ç¼–ç å†…å®¹å’Œé£æ ¼çš„æ¦‚å¿µã€‚é€šè¿‡äº¤å‰æ³¨æ„åŠ›çƒ­å›¾ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ç”Ÿæˆå›¾åƒä¸­çš„åƒç´ å½’å› äºç‰¹å®šçš„æç¤ºä»¤ç‰Œï¼Œä»è€ŒåŒºåˆ†å—å†…å®¹æè¿°å’Œé£æ ¼æè¿°å½±å“çš„å›¾åƒåŒºåŸŸã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ä¸åŒè‰ºæœ¯æç¤ºå’Œé£æ ¼è¯·æ±‚ä¸‹è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„å†…å®¹ä¸é£æ ¼åˆ†ç¦»ã€‚ç»“æœè¡¨æ˜ï¼Œå†…å®¹ä»¤ç‰Œä¸»è¦å½±å“ä¸ç‰©ä½“ç›¸å…³çš„åŒºåŸŸï¼Œè€Œé£æ ¼ä»¤ç‰Œåˆ™å½±å“èƒŒæ™¯å’Œçº¹ç†åŒºåŸŸï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹å¯¹å†…å®¹ä¸é£æ ¼åŒºåˆ†çš„ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04440', 'title': 'StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion', 'url': 'https://huggingface.co/papers/2508.04440', 'abstract': 'ThinkingF, a data synthesis and training pipeline, enhances autoformalization by improving formal knowledge and informal-to-formal reasoning, achieving state-of-the-art results in formalization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.', 'score': 0, 'issue_id': 5230, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '6d2a57f491626d3a', 'authors': ['Yutong Wu', 'Di Huang', 'Ruosi Wan', 'Yue Peng', 'Shijie Shang', 'Chenrui Cao', 'Lei Qi', 'Rui Zhang', 'Zidong Du', 'Jie Yan', 'Xing Hu'], 'affiliations': ['SKL of Processors, Institute of Computing Technology, CAS', 'StepFun Inc.', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04440.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#reasoning', '#data', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ThinkingF: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ¹', 'desc': 'ThinkingF - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ¾Ğ´Ğ¸Ğ½ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… FormalMATH-Lite Ğ¸ ProverBench.'}, 'en': {'title': 'ThinkingF: Bridging Natural Language and Formal Knowledge', 'desc': 'The paper presents ThinkingF, a novel data synthesis and training pipeline designed to enhance autoformalization, which is the process of converting natural-language mathematical statements into formal language. It identifies two critical skills necessary for effective autoformalization: mastery of formal-language knowledge and the ability to reason from informal to formal contexts. To improve these skills, ThinkingF constructs two specialized datasets and employs techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Value Regression (RLVR). The resulting models, particularly StepFun-Formalizer-32B, achieve state-of-the-art performance on formalization tasks, demonstrating significant advancements over previous models.'}, 'zh': {'title': 'æå‡è‡ªåŠ¨å½¢å¼åŒ–çš„ThinkingF', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ThinkingFï¼Œä¸€ä¸ªæ•°æ®åˆæˆå’Œè®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨å½¢å¼åŒ–çš„èƒ½åŠ›ã€‚è‡ªåŠ¨å½¢å¼åŒ–çš„ç›®æ ‡æ˜¯å°†è‡ªç„¶è¯­è¨€æ•°å­¦é™ˆè¿°è½¬æ¢ä¸ºå½¢å¼è¯­è¨€ã€‚ç ”ç©¶å‘ç°ï¼ŒæˆåŠŸçš„è‡ªåŠ¨å½¢å¼åŒ–éœ€è¦å¯¹å½¢å¼è¯­è¨€é¢†åŸŸçŸ¥è¯†çš„å…¨é¢æŒæ¡å’Œè‡ªç„¶è¯­è¨€é—®é¢˜ç†è§£çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸¤ä¸ªæ•°æ®é›†å¹¶åº”ç”¨SFTå’ŒRLVRï¼ŒThinkingFæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å½¢å¼åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨FormalMATH-Liteå’ŒProverBenchä¸Šå–å¾—äº†é¢†å…ˆçš„æˆç»©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00109', 'title': 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality', 'url': 'https://huggingface.co/papers/2508.00109', 'abstract': 'FACTORY, a human-verified prompt set, evaluates the factuality of long-form responses from language models, revealing higher factual accuracy compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.', 'score': 0, 'issue_id': 5234, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '6dd83d8a170db99d', 'authors': ['Mingda Chen', 'Yang Li', 'Xilun Chen', 'Adina Williams', 'Gargi Ghosh', 'Scott Yih'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2508.00109.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#dataset', '#long_context', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'FACTORY: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'FACTORY - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° model-in-the-loop Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. FACTORY Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ñ‹, Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ´Ğ²ÑƒÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FACTORY ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ¼: Ğ¾ĞºĞ¾Ğ»Ğ¾ 40% ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹.'}, 'en': {'title': 'FACTORY: Elevating Factual Accuracy in Language Models', 'desc': 'The paper introduces FACTORY, a new benchmark for evaluating the factual accuracy of long-form responses generated by language models. Unlike existing datasets, FACTORY is human-verified, ensuring higher quality and reliability in assessing model outputs. The study reveals that state-of-the-art models struggle with factual accuracy, with around 40% of their claims being incorrect when evaluated with FACTORY. This highlights the importance of using robust, human-verified datasets to improve the reasoning capabilities of language models across diverse factual scenarios.'}, 'zh': {'title': 'FACTORYï¼šæå‡è¯­è¨€æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§', 'desc': 'FACTORYæ˜¯ä¸€ä¸ªç»è¿‡äººå·¥éªŒè¯çš„æç¤ºé›†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ç”Ÿæˆé•¿æ–‡æœ¬å“åº”çš„äº‹å®å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒFACTORYæ˜¾ç¤ºå‡ºæ›´é«˜çš„äº‹å®å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒé‡‡ç”¨äº†æ¨¡å‹å¾ªç¯çš„æ–¹æ³•ï¼Œå¹¶ç»è¿‡äººç±»çš„ç²¾ç»†è°ƒæ•´ã€‚é€šè¿‡å¯¹å…­ç§æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å¤§çº¦40%çš„å“åº”å£°æ˜å¹¶ä¸çœŸå®ï¼Œè€Œå…¶ä»–æ•°æ®é›†çš„è¿™ä¸€æ¯”ä¾‹ä»…ä¸º10%ã€‚è¿™è¡¨æ˜FACTORYåœ¨è¯„ä¼°æ¨¡å‹çš„é•¿å°¾äº‹å®æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ›´é«˜çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02324', 'title': 'Qwen-Image Technical Report', 'url': 'https://huggingface.co/papers/2508.02324', 'abstract': "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.", 'score': 79, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '4417ddebd50d6ca5', 'authors': ['Chenfei Wu', 'Jiahao Li', 'Jingren Zhou', 'Junyang Lin', 'Kaiyuan Gao', 'Kun Yan', 'Sheng-ming Yin', 'Shuai Bai', 'Xiao Xu', 'Yilei Chen', 'Yuxiang Chen', 'Zecheng Tang', 'Zekai Zhang', 'Zhengyi Wang', 'An Yang', 'Bowen Yu', 'Chen Cheng', 'Dayiheng Liu', 'Deqing Li', 'Hang Zhang', 'Hao Meng', 'Hu Wei', 'Jingyuan Ni', 'Kai Chen', 'Kuan Cao', 'Liang Peng', 'Lin Qu', 'Minggang Wu', 'Peng Wang', 'Shuting Yu', 'Tingkun Wen', 'Wensen Feng', 'Xiaoxiao Xu', 'Yi Wang', 'Yichang Zhang', 'Yongqiang Zhu', 'Yujia Wu', 'Yuxuan Cai', 'Zenan Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.02324.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#games', '#cv', '#training', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Qwen-Image: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Qwen-Image - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ² Ğ² Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ±Ğ¾Ñ€, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. Qwen-Image Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Image Generation and Editing with Qwen-Image', 'desc': "Qwen-Image is an advanced image generation model that enhances text rendering and image editing through a sophisticated data pipeline and a dual-encoding mechanism. It employs a comprehensive approach to data collection and training, utilizing a progressive strategy that improves the model's ability to handle complex text inputs, including both alphabetic and logographic languages. The model's multi-task training paradigm integrates various tasks to ensure consistency in image editing while maintaining high-quality outputs. Overall, Qwen-Image sets new standards in image generation and editing performance across multiple benchmarks."}, 'zh': {'title': 'Qwen-Imageï¼šå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„çªç ´æ€§è¿›å±•', 'desc': 'Qwen-Imageæ˜¯ä¸€ç§å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å…¨é¢çš„æ•°æ®å¤„ç†æµç¨‹å’Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œæå‡æ–‡æœ¬æ¸²æŸ“å’Œå›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†åŒç¼–ç æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„æ–‡æœ¬è¾“å…¥ï¼Œå¹¶åœ¨å­—æ¯è¯­è¨€å’Œè¡¨æ„æ–‡å­—ï¼ˆå¦‚ä¸­æ–‡ï¼‰ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡å¤šä»»åŠ¡è®­ç»ƒï¼ŒQwen-Imageåœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒé‡å»ºä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„ä¸€è‡´æ€§ï¼Œç¡®ä¿äº†è¯­ä¹‰å’Œè§†è§‰çš„ä¿çœŸåº¦ã€‚æœ€ç»ˆï¼ŒQwen-Imageåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å…¶åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„é¢†å…ˆæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01959', 'title': 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension', 'url': 'https://huggingface.co/papers/2508.01959', 'abstract': "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.", 'score': 37, 'issue_id': 5178, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': '063b1bca074ff85f', 'authors': ['Junjie Wu', 'Jiangnan Li', 'Yuqing Li', 'Lemao Liu', 'Liyan Xu', 'Jiwei Li', 'Dit-Yan Yeung', 'Jie Zhou', 'Mo Yu'], 'affiliations': ['HKUST', 'IIE-CAS', 'WeChat AI, Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01959.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#rag', '#long_context', '#optimization', '#benchmark', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ: ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (SitEmb), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸Ğ¼ĞµÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸Ñ‚ÑƒĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Enhancing Retrieval with Contextual Short Text Chunks', 'desc': 'This paper introduces a new training method and a model called situated embedding models (SitEmb) that improve the retrieval of information from text. By conditioning short text chunks on a broader context, the model captures the meaning of each chunk more effectively. The authors demonstrate that existing models struggle with this task due to their limitations in encoding context. Their SitEmb model outperforms leading models with significantly fewer parameters, showing better retrieval performance across various languages and applications.'}, 'zh': {'title': 'æƒ…å¢ƒåµŒå…¥ï¼Œæå‡æ£€ç´¢æ€§èƒ½ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼å’Œæƒ…å¢ƒåµŒå…¥æ¨¡å‹ï¼ˆSitEmbï¼‰ï¼Œé€šè¿‡å°†çŸ­æ–‡æœ¬ç‰‡æ®µä¸æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çª—å£ç›¸ç»“åˆï¼Œæå‡äº†æ£€ç´¢æ€§èƒ½ã€‚ä¼ ç»Ÿçš„æ£€ç´¢æ–¹æ³•å¾€å¾€å°†é•¿æ–‡æ¡£æ‹†åˆ†ä¸ºå°å—ï¼Œä½†è¿™äº›å°å—çš„ç†è§£éœ€è¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¸­å¯¹çŸ­ç‰‡æ®µè¿›è¡Œç¼–ç ï¼Œæ¥å¢å¼ºæ£€ç´¢æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSitEmbæ¨¡å‹åœ¨å‚æ•°æ›´å°‘çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02276', 'title': 'CellForge: Agentic Design of Virtual Cell Models', 'url': 'https://huggingface.co/papers/2508.02276', 'abstract': "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.", 'score': 27, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '04238c0793ca08e5', 'authors': ['Xiangru Tang', 'Zhuoyun Yu', 'Jiapeng Chen', 'Yan Cui', 'Daniel Shao', 'Weixu Wang', 'Fang Wu', 'Yuchen Zhuang', 'Wenqi Shi', 'Zhi Huang', 'Arman Cohan', 'Xihong Lin', 'Fabian Theis', 'Smita Krishnaswamy', 'Mark Gerstein'], 'affiliations': ['Google DeepMind', 'Harvard University', 'Helmholtz Zentrum Munchen', 'Stanford University', 'University of Pennsylvania', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02276.jpg', 'data': {'categories': ['#dataset', '#training', '#science', '#open_source', '#architecture', '#agents', '#multimodal'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'CellForge: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»ĞµÑ‚ĞºĞ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'CellForge - ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². CellForge Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming Biology with Collaborative AI Models', 'desc': 'CellForge is an innovative system that uses a multi-agent framework to create computational models for virtual cells from raw single-cell multi-omics data. It addresses the complexities of biological systems by employing specialized agents that collaborate to analyze tasks, design methods, and execute experiments. This approach allows CellForge to generate optimized model architectures and executable code, significantly improving predictions for single-cell perturbations. By integrating diverse perspectives from its agents, CellForge consistently outperforms existing state-of-the-art methods in the field.'}, 'zh': {'title': 'CellForgeï¼šä¼˜åŒ–è™šæ‹Ÿç»†èƒå»ºæ¨¡çš„æ™ºèƒ½ç³»ç»Ÿ', 'desc': 'CellForge æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“æ¡†æ¶çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿå°†åŸå§‹çš„å•ç»†èƒå¤šç»„å­¦æ•°æ®è½¬åŒ–ä¸ºä¼˜åŒ–çš„è™šæ‹Ÿç»†èƒè®¡ç®—æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†æä»»åŠ¡å’Œæ•°æ®é›†ï¼Œè‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œçš„ä»£ç ï¼Œæ˜¾è‘—æé«˜äº†å•ç»†èƒæ‰°åŠ¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚CellForge çš„è®¾è®¡æ¨¡å—ç”±ä¸åŒä¸“ä¸šçš„æ™ºèƒ½ä½“åä½œå¼€å‘å»ºæ¨¡ç­–ç•¥ï¼Œç¡®ä¿äº†æ¨¡å‹çš„ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCellForge åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å¤šæ™ºèƒ½ä½“åä½œçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01059', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report', 'url': 'https://huggingface.co/papers/2508.01059', 'abstract': 'Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.', 'score': 21, 'issue_id': 5176, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '897c594bfa5630a6', 'authors': ['Sajana Weerawardhena', 'Paul Kassianik', 'Blaine Nelson', 'Baturay Saglam', 'Anu Vellore', 'Aman Priyanshu', 'Supriti Vijay', 'Massimo Aufiero', 'Arthur Goldblatt', 'Fraser Burch', 'Ed Li', 'Jianliang He', 'Dhruv Kedia', 'Kojin Oshiba', 'Zhouran Yang', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Carnegie Mellon University', 'Cisco Systems Inc.', 'Foundation AI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01059.jpg', 'data': {'categories': ['#dataset', '#alignment', '#security', '#training', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ˜Ğ˜', 'desc': 'Foundation-Sec-8B-Instruct - ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Foundation-Sec-8B-Instruct Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ğ°Ğ½ĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering Cybersecurity with Conversational AI', 'desc': 'Foundation-Sec-8B-Instruct is a large language model (LLM) specifically designed for cybersecurity applications, enhancing chat-style interactions and instruction-following capabilities. It builds upon the previous Foundation-Sec-8B model, which was tailored for fine-tuning on cybersecurity tasks but lacked conversational features. This new model integrates domain-specific knowledge with the ability to follow instructions and engage in dialogue, resulting in high-quality responses relevant to cybersecurity. Evaluations demonstrate that it surpasses other models like Llama 3.1-8B-Instruct in cybersecurity tasks while maintaining competitive performance in instruction-following.'}, 'zh': {'title': 'ç½‘ç»œå®‰å…¨å¯¹è¯çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'Foundation-Sec-8B-Instruct æ˜¯ä¸€ä¸ªä¸“æ³¨äºç½‘ç»œå®‰å…¨çš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è¿›è¡Œå¯¹è¯å¼äº¤äº’å’Œéµå¾ªæŒ‡ä»¤ã€‚è¯¥æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒæ—¶åœ¨éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ä¸Šä¸ä¹‹ç›¸åŒ¹é…ã€‚å®ƒç»“åˆäº†ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å’Œäººç±»åå¥½çš„å¯¹é½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œç›¸å…³çš„å“åº”ã€‚æˆ‘ä»¬å¸Œæœ› Foundation-Sec-8B-Instruct èƒ½æˆä¸ºç½‘ç»œå®‰å…¨ä¸“ä¸šäººå‘˜æ—¥å¸¸å·¥ä½œä¸­ä¸å¯æˆ–ç¼ºçš„åŠ©æ‰‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02150', 'title': "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following", 'url': 'https://huggingface.co/papers/2508.02150', 'abstract': "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if.", 'score': 20, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '37d1b608fde6bd5f', 'authors': ['Qingyu Ren', 'Qianyu He', 'Bowei Zhang', 'Jie Zeng', 'Jiaqing Liang', 'Yanghua Xiao', 'Weikang Zhou', 'Zeye Sun', 'Fei Yu'], 'affiliations': ['Ant Group', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02150.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Instruction Following in Reasoning Models with Self-Supervised RL', 'desc': "This paper presents a self-supervised reinforcement learning (RL) framework designed to improve how reasoning models follow instructions. Traditional methods often depend on external models, which can be costly and limit accessibility. The proposed framework utilizes the internal signals of reasoning models to enhance their instruction-following abilities without needing external supervision. Experimental results show that this approach not only boosts instruction following but also preserves the models' reasoning performance, making it a scalable and cost-effective solution."}, 'zh': {'title': 'è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ¨ç†æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œè€Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºæ›´å¼ºå¤§çš„å¤–éƒ¨æ¨¡å‹ï¼Œè¿™å¯¼è‡´äº†æ–¹æ³•ä¸Šçš„ç“¶é¢ˆå’Œå®é™…åº”ç”¨ä¸­çš„é™åˆ¶ï¼Œå¦‚æˆæœ¬å¢åŠ å’Œå¯åŠæ€§é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ¨ç†æ¨¡å‹è‡ªèº«çš„å†…éƒ¨ä¿¡å·æ¥æ”¹å–„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„åŒæ—¶ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02317', 'title': 'VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo', 'url': 'https://huggingface.co/papers/2508.02317', 'abstract': 'A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.', 'score': 8, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '2e96724e612a0eb6', 'authors': ['Qianli Ma', 'Yaowei Zheng', 'Zhelun Shi', 'Zhongkai Zhao', 'Bin Jia', 'Ziyue Huang', 'Zhiqi Lin', 'Youjie Li', 'Jiacheng Yang', 'Yanghua Peng', 'Zhi Zhang', 'Xin Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.02317.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ 3D-Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¾Ğ¼Ğ½Ğ¸-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Accelerating Omni-Modal LLMs with Modular Training', 'desc': 'This paper introduces \textit{veomni}, a modular training framework designed to enhance the development of omni-modal large language models (LLMs). It addresses the challenges of training these models by separating model architecture from parallel processing logic, which allows for efficient 3D parallelism. The framework supports easy integration of new modalities, reducing the need for extensive code modifications. With \textit{veomni}, a mixture-of-experts model with 30 billion parameters can achieve high throughput and scalability, demonstrating its effectiveness in training large omni-modal LLMs.'}, 'zh': {'title': 'æ¨¡å—åŒ–è®­ç»ƒæ¡†æ¶ï¼Œæå‡å…¨æ¨¡æ€LLMå¼€å‘æ•ˆç‡', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸º\\veomniçš„æ¨¡å—åŒ–è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿå…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€å‘ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜æ•ˆçš„ä¸‰ç»´å¹¶è¡Œå¤„ç†å’Œçµæ´»çš„é…ç½®ï¼Œè§£å†³äº†è®­ç»ƒå…¨æ¨¡æ€LLMæ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚\\veomniå°†æ¨¡å‹å®šä¹‰ä¸å¹¶è¡Œé€»è¾‘è§£è€¦ï¼Œä½¿å¾—åœ¨å¤šç§æ¨¡æ€ä¸Šè¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒå˜å¾—æ›´åŠ é«˜æ•ˆã€‚ä½¿ç”¨\\veomniï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿä»¥æé«˜çš„é€Ÿåº¦è®­ç»ƒå…·æœ‰30äº¿å‚æ•°çš„å…¨æ¨¡æ€ä¸“å®¶æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨è®­ç»ƒå¤§å‹å…¨æ¨¡æ€LLMæ–¹é¢çš„ä¼˜è¶Šæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.17520', 'title': 'InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation', 'url': 'https://huggingface.co/papers/2507.17520', 'abstract': "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.", 'score': 8, 'issue_id': 5176, 'pub_date': '2025-07-23', 'pub_date_card': {'ru': '23 Ğ¸ÑĞ»Ñ', 'en': 'July 23', 'zh': '7æœˆ23æ—¥'}, 'hash': '13d868fd7ad8ea42', 'authors': ['Shuai Yang', 'Hao Li', 'Yilun Chen', 'Bin Wang', 'Yang Tian', 'Tai Wang', 'Hanqing Wang', 'Feng Zhao', 'Yiyi Liao', 'Jiangmiao Pang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.17520.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#reasoning', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'InstructVLA: ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'InstructVLA - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². InstructVLA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'Bridging Vision, Language, and Action for Smarter Robots', 'desc': 'InstructVLA is a new model that combines vision, language, and action to improve how robots perform tasks. It uses a special training method called Vision-Language-Action Instruction Tuning (VLA-IT) to enhance both understanding and action capabilities without losing previous knowledge. This model outperforms existing systems in various tasks, showing significant improvements in manipulation and reasoning. By integrating multimodal training and expert adaptation, InstructVLA enables better human-robot interaction and efficient learning of tasks.'}, 'zh': {'title': 'æå‡æœºå™¨äººæ“ä½œä¸æ¨ç†çš„å®Œç¾ç»“åˆ', 'desc': 'InstructVLAæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ“ä½œæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè§†è§‰-è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šæ¨¡æ€è®­ç»ƒå’Œä¸“å®¶æ··åˆé€‚åº”ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ä»»åŠ¡ç‰¹å®šæ•°æ®ä¸Šçš„å±€é™æ€§å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚InstructVLAå¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œç§°ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæŒ‡ä»¤è°ƒä¼˜ï¼ˆVLA-ITï¼‰ï¼Œåœ¨æ ‡å‡†è§†è§‰-è¯­è¨€æ¨¡å‹æ•°æ®é›†å’Œä¸€ä¸ªåŒ…å«65ä¸‡æ ·æœ¬çš„VLA-ITæ•°æ®é›†ä¸Šå…±åŒä¼˜åŒ–æ–‡æœ¬æ¨ç†å’ŒåŠ¨ä½œç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructVLAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆæ”¿ç­–å­¦ä¹ å’Œäººæœºäº¤äº’ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01548', 'title': 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2508.01548', 'abstract': "A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce a dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining a similarly high pruning rate. Our work paves a new way for building more powerful and efficient LVLMs.", 'score': 7, 'issue_id': 5180, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': '7611e04f9fa72eb7', 'authors': ['Quan-Sheng Zeng', 'Yunheng Li', 'Qilong Wang', 'Peng-Tao Jiang', 'Zuxuan Wu', 'Ming-Ming Cheng', 'Qibin Hou'], 'affiliations': ['Shanghai Innovation Institute', 'Tianjin University', 'VCIP, CS, Nankai University', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2508.01548.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'GlimpsePrune - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). ĞĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. GlimpsePrune Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµÑ‚ 92.6% Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (VQA). Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ GlimpsePrune+ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 110% Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸.'}, 'en': {'title': 'Dynamic Pruning for Efficient Vision-Language Models', 'desc': 'GlimpsePrune is a dynamic pruning framework designed to enhance the efficiency of Large Vision-Language Models (LVLMs) by selectively removing irrelevant visual tokens. Unlike traditional methods that use fixed compression ratios, GlimpsePrune adapts to the complexity of different scenes, ensuring that important visual information is preserved. This framework prunes up to 92.6% of visual tokens while maintaining baseline performance on visual question answering tasks. Additionally, an improved version, GlimpsePrune+, not only retains high pruning rates but also boosts performance beyond the baseline, demonstrating a significant advancement in model efficiency.'}, 'zh': {'title': 'åŠ¨æ€å‰ªæï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ•ˆç‡', 'desc': 'GlimpsePruneæ˜¯ä¸€ä¸ªåŠ¨æ€å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”åœ°å»é™¤ä¸ç›¸å…³çš„è§†è§‰æ ‡è®°ï¼Œé¿å…äº†å›ºå®šå‹ç¼©æ¯”å¸¦æ¥çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­è¿›è¡Œå‰ªæï¼Œèƒ½å¤Ÿä¿ç•™92.6%çš„è§†è§‰æ ‡è®°ï¼ŒåŒæ—¶åœ¨è‡ªç”±å½¢å¼çš„è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ä¿æŒåŸºçº¿æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºæ›´å¼ºå¤§å’Œé«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02137', 'title': 'Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind', 'url': 'https://huggingface.co/papers/2508.02137', 'abstract': 'AuroBind is a scalable virtual screening framework that fine-tunes atomic-level structural models to predict ligand-bound structures and binding fitness, achieving high hit rates in prospective screens across disease-relevant targets.  \t\t\t\t\tAI-generated summary \t\t\t\t Most human proteins remain undrugged, over 96% of human proteins remain unexploited by approved therapeutics. While structure-based virtual screening promises to expand the druggable proteome, existing methods lack atomic-level precision and fail to predict binding fitness, limiting translational impact. We present AuroBind, a scalable virtual screening framework that fine-tunes a custom atomic-level structural model on million-scale chemogenomic data. AuroBind integrates direct preference optimization, self-distillation from high-confidence complexes, and a teacher-student acceleration strategy to jointly predict ligand-bound structures and binding fitness. The proposed models outperform state-of-the-art models on structural and functional benchmarks while enabling 100,000-fold faster screening across ultra-large compound libraries. In a prospective screen across ten disease-relevant targets, AuroBind achieved experimental hit rates of 7-69%, with top compounds reaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and GPR160, AuroBind identified both agonists and antagonists with success rates of 16-30%, and functional assays confirmed GPR160 modulation in liver and prostate cancer models. AuroBind offers a generalizable framework for structure-function learning and high-throughput molecular screening, bridging the gap between structure prediction and therapeutic discovery.', 'score': 6, 'issue_id': 5190, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'f767500373dc2f12', 'authors': ['Zhongyue Zhang', 'Jiahua Rao', 'Jie Zhong', 'Weiqiang Bai', 'Dongxue Wang', 'Shaobo Ning', 'Lifeng Qiao', 'Sheng Xu', 'Runze Ma', 'Will Hua', 'Jack Xiaoyu Chen', 'Odin Zhang', 'Wei Lu', 'Hanyi Feng', 'He Yang', 'Xinchao Shi', 'Rui Li', 'Wanli Ouyang', 'Xinzhu Ma', 'Jiahao Wang', 'Jixian Zhang', 'Jia Duan', 'Siqi Sun', 'Jian Zhang', 'Shuangjia Zheng'], 'affiliations': ['Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China', 'Institute for Medical Engineering & Science, Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Key Laboratory of Protection, Development and Utilization of Medicinal Resources in Liupanshan Area, Ministry of Education, Peptide & Protein Drug Research Center, School of Pharmacy, Ningxia Medical University, Ningxia, China', 'Lingang Laboratory, Shanghai, China', 'Medicinal Chemistry and Bioinformatics Center, School of Medicine, Shanghai Jiao Tong University, Shanghai, China', 'Research Institute of Intelligent Complex Systems, Fudan University, Shanghai, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangdong, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Zhongshan Institute for Drug Discovery, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02137.jpg', 'data': {'categories': ['#healthcare', '#data', '#optimization', '#science', '#dataset', '#benchmark'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'AuroBind: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğµ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'AuroBind - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ¾Ğ² Ğ»Ğ¸Ğ³Ğ°Ğ½Ğ´-Ğ±ĞµĞ»Ğ¾Ğº Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ…ĞµĞ¼Ğ¾Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ. AuroBind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³ ÑĞ²ĞµÑ€Ñ…Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² 100 000 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ. Ğ’ Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¸ÑˆĞµĞ½ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ€Ñ„Ğ°Ğ½Ğ½Ñ‹Ğµ GPCR-Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ñ€Ñ‹.'}, 'en': {'title': 'AuroBind: Revolutionizing Drug Discovery with Precision and Speed', 'desc': 'AuroBind is a new framework designed for virtual screening in drug discovery, focusing on predicting how small molecules (ligands) bind to proteins. It fine-tunes detailed structural models using large datasets to improve the accuracy of binding predictions and identify potential drug candidates. By employing techniques like direct preference optimization and self-distillation, AuroBind significantly enhances screening speed and hit rates across various disease targets. This approach not only outperforms existing methods but also helps in discovering new therapeutic options for previously undrugged human proteins.'}, 'zh': {'title': 'AuroBindï¼šé«˜æ•ˆçš„è™šæ‹Ÿç­›é€‰æ–°æ¡†æ¶', 'desc': 'AuroBind æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è™šæ‹Ÿç­›é€‰æ¡†æ¶ï¼Œèƒ½å¤Ÿå¾®è°ƒåŸå­çº§ç»“æ„æ¨¡å‹ï¼Œä»¥é¢„æµ‹é…ä½“ç»“åˆç»“æ„å’Œç»“åˆé€‚åº”æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–ç›´æ¥åå¥½ã€è‡ªæˆ‘è’¸é¦å’Œæ•™å¸ˆ-å­¦ç”ŸåŠ é€Ÿç­–ç•¥ï¼Œè”åˆé¢„æµ‹é…ä½“ç»“åˆç»“æ„å’Œç»“åˆé€‚åº”æ€§ã€‚AuroBind åœ¨ç»“æ„å’ŒåŠŸèƒ½åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶åœ¨è¶…å¤§åŒ–åˆç‰©åº“ä¸­å®ç°äº†100,000å€çš„ç­›é€‰é€Ÿåº¦æå‡ã€‚é€šè¿‡å¯¹åä¸ªä¸ç–¾ç—…ç›¸å…³çš„é¶ç‚¹è¿›è¡Œå‰ç»æ€§ç­›é€‰ï¼ŒAuroBind å®ç°äº†7-69%çš„å®éªŒå‘½ä¸­ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è¯ç‰©å‘ç°ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01691', 'title': 'Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and\n  Regional Languages Around the Globe', 'url': 'https://huggingface.co/papers/2508.01691', 'abstract': 'Voxlect is a benchmark for evaluating speech foundation models on dialect classification and downstream applications across multiple languages and dialects.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.', 'score': 6, 'issue_id': 5185, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'bdb8f5a75702298d', 'authors': ['Tiantian Feng', 'Kevin Huang', 'Anfeng Xu', 'Xuan Shi', 'Thanathai Lertpetchpun', 'Jihwan Lee', 'Yoonjeong Lee', 'Dani Byrd', 'Shrikanth Narayanan'], 'affiliations': ['University of Southern California, Los Angeles, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01691.jpg', 'data': {'categories': ['#multilingual', '#science', '#audio', '#benchmark', '#open_source'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Voxlect: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Voxlect - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· 30 Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ñ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑˆÑƒĞ¼Ğ°. Voxlect Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'Voxlect: Advancing Dialect Classification in Speech Models', 'desc': 'Voxlect is a benchmark designed to evaluate speech foundation models specifically for dialect classification across various languages. It utilizes over 2 million training utterances from 30 speech corpora that include dialectal information, allowing for comprehensive assessments of model performance. The study not only benchmarks dialect classification but also explores downstream applications, such as enhancing automatic speech recognition (ASR) datasets with dialectal data. Additionally, it evaluates the robustness of these models in noisy environments and provides insights into geographic continuity in dialectal variations.'}, 'zh': {'title': 'Voxlectï¼šå…¨çƒæ–¹è¨€åˆ†ç±»çš„åŸºå‡†æµ‹è¯•', 'desc': 'Voxlectæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨æ–¹è¨€åˆ†ç±»å’Œä¸‹æ¸¸åº”ç”¨ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥ç ”ç©¶æ¶µç›–äº†å¤šç§è¯­è¨€å’Œæ–¹è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€æ™®é€šè¯ã€ç²¤è¯­ç­‰ï¼Œä½¿ç”¨äº†è¶…è¿‡200ä¸‡ä¸ªå¸¦æœ‰æ–¹è¨€ä¿¡æ¯çš„è¯­éŸ³æ ·æœ¬ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨æ–¹è¨€åˆ†ç±»ä¸­çš„è¡¨ç°ï¼Œå¹¶åˆ†æäº†æ¨¡å‹åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚Voxlectä¸ä»…ç”¨äºæ–¹è¨€åˆ†ç±»çš„åŸºå‡†æµ‹è¯•ï¼Œè¿˜å¯ä»¥å¢å¼ºç°æœ‰çš„è¯­éŸ³è¯†åˆ«æ•°æ®é›†ï¼Œå¸®åŠ©åˆ†æä¸åŒæ–¹è¨€çš„ASRæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01151', 'title': 'Personalized Safety Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2508.01151', 'abstract': "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at https://torpedo2648.github.io/PSAlign/.", 'score': 6, 'issue_id': 5176, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': '22d777cd71f123c6', 'authors': ['Yu Lei', 'Jinbin Bai', 'Qingyu Shi', 'Aosong Feng', 'Kaidong Yu'], 'affiliations': ['National University of Singapore', 'Peking University', 'TeleAI, China Telecom', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01151.jpg', 'data': {'categories': ['#cv', '#dataset', '#alignment', '#diffusion', '#open_source', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Personalized Safety Alignment (PSA) Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. PSA Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Sage, capturing Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Personalized Safety for Safer AI-Generated Images', 'desc': 'This paper presents a Personalized Safety Alignment (PSA) framework that enhances text-to-image diffusion models by incorporating individual user profiles. Current models apply a one-size-fits-all approach to safety, which does not consider the unique safety preferences shaped by personal factors. The PSA framework allows for user-specific adjustments in the generative process, ensuring that the generated images align with individual safety standards while maintaining high image quality. The authors also introduce a new dataset, Sage, to effectively capture and integrate these personalized safety preferences, demonstrating improved performance in harmful content suppression compared to existing methods.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–å®‰å…¨å¯¹é½ï¼šè®©ç”Ÿæˆå†…å®¹æ›´ç¬¦åˆä½ çš„å®‰å…¨åå¥½', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–å®‰å…¨å¯¹é½æ¡†æ¶ï¼ˆPSAï¼‰ï¼Œæ—¨åœ¨å°†ç”¨æˆ·ç‰¹å®šçš„ä¸ªäººèµ„æ–™æ•´åˆåˆ°æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æ›´å¥½åœ°ç¬¦åˆä¸ªä½“çš„å®‰å…¨åå¥½ã€‚å½“å‰çš„å®‰å…¨æœºåˆ¶é€šå¸¸é‡‡ç”¨ç»Ÿä¸€æ ‡å‡†ï¼Œæ— æ³•è€ƒè™‘ç”¨æˆ·çš„å¤šæ ·åŒ–å®‰å…¨è¾¹ç•Œï¼Œå¦‚å¹´é¾„ã€å¿ƒç†å¥åº·å’Œä¸ªäººä¿¡ä»°ç­‰å› ç´ ã€‚PSAé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æ•´åˆä¸ªæ€§åŒ–ç”¨æˆ·èµ„æ–™ï¼Œè°ƒæ•´æ¨¡å‹è¡Œä¸ºä»¥åŒ¹é…ä¸ªä½“å®‰å…¨åå¥½ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSAåœ¨æœ‰å®³å†…å®¹æŠ‘åˆ¶å’Œç”Ÿæˆå†…å®¹ä¸ç”¨æˆ·çº¦æŸçš„å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå–å¾—äº†æ›´é«˜çš„èƒœç‡å’Œé€šè¿‡ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02271', 'title': 'Dynaword: From One-shot to Continuously Developed Datasets', 'url': 'https://huggingface.co/papers/2508.02271', 'abstract': 'A framework called Dynaword and its implementation Danish Dynaword enable community-driven, open, and continuously updated large-scale natural language datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.', 'score': 5, 'issue_id': 5193, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'efe4e9f51385893f', 'authors': ['Kenneth Enevoldsen', 'Kristian NÃ¸rgaard Jensen', 'Jan Kostkan', 'BalÃ¡zs SzabÃ³', 'MÃ¡rton Kardos', 'Kirten Vad', 'Andrea Blasi NÃºÃ±ez', 'Gianluca Barmina', 'Jacob Nielsen', 'Rasmus Larsen', 'Peter Vahlstrup', 'Per MÃ¸ldrup Dalum', 'Desmond Elliott', 'Lukas Galke', 'Peter Schneider-Kamp', 'Kristoffer Nielbo'], 'affiliations': ['Aarhus University', 'The Alexandra Institute', 'University of Copenhagen', 'University of Southern Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2508.02271.jpg', 'data': {'categories': ['#dataset', '#survey', '#open_source', '#data'], 'emoji': 'ğŸŒ±', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ»Ñ NLP ÑĞ¸Ğ»Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Dynaword - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼. Danish Dynaword - ÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¸ Ğ¸Ğ¼ĞµÑÑ‰Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°.'}, 'en': {'title': 'Empowering Community-Driven Natural Language Datasets', 'desc': 'The paper presents Dynaword, a framework designed to create large-scale natural language datasets that are open and continuously updated through community involvement. It addresses three main challenges in current dataset practices: licensing issues, static releases, and limited quality assurance. The implementation, Danish Dynaword, showcases this framework by providing a dataset with significantly more tokens than similar datasets, all under open licenses. It also includes mechanisms for community contributions and quality checks, promoting a sustainable model for dataset development and maintenance.'}, 'zh': {'title': 'æ„å»ºå¼€æ”¾ã€å¯æŒç»­çš„å¤§è§„æ¨¡è¯­è¨€æ•°æ®é›†', 'desc': 'Dynawordæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨åˆ›å»ºå¯æŒç»­æ›´æ–°çš„å¤§è§„æ¨¡è‡ªç„¶è¯­è¨€æ•°æ®é›†ã€‚å®ƒè§£å†³äº†å½“å‰æ•°æ®é›†é¢ä¸´çš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡ç³Šçš„è®¸å¯é™åˆ¶ã€é™æ€æ•°æ®é›†å‘å¸ƒå’Œè´¨é‡ä¿è¯è¿‡ç¨‹çš„å±€é™æ€§ã€‚Danish Dynawordæ˜¯è¯¥æ¡†æ¶çš„å…·ä½“å®ç°ï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ï¼Œå¹¶åŒ…å«äº†æ¯”ç±»ä¼¼å‘å¸ƒå¤šå››å€çš„æ ‡è®°ã€‚è¯¥é¡¹ç›®é€šè¿‡è½»é‡çº§æµ‹è¯•ç¡®ä¿æ•°æ®æ ¼å¼ã€è´¨é‡å’Œæ–‡æ¡£ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„æŒç»­è´¡çŒ®å’Œæ•°æ®é›†çš„æ¼”å˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02558', 'title': 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction', 'url': 'https://huggingface.co/papers/2508.02558', 'abstract': 'Sparse-dLLM improves the efficiency of diffusion large language models by implementing dynamic cache eviction and sparse attention, enhancing throughput without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10times higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.', 'score': 4, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'abb2417fa4d42a82', 'authors': ['Yuerong Song', 'Xiaoran Liu', 'Ruixiao Li', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.02558.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#inference', '#long_context'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Sparse-dLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² dLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¼ĞµĞ¶ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ³Ğ´Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Sparse-dLLM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ dLLM Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Boosting Efficiency in Language Models with Sparse-dLLM', 'desc': 'Sparse-dLLM is a novel framework designed to enhance the efficiency of diffusion large language models (dLLMs) by utilizing dynamic cache eviction and sparse attention mechanisms. It addresses the high computational complexity and memory demands of traditional dLLMs during inference by selectively retaining important tokens while evicting less relevant ones. This approach leverages the consistent saliency of tokens across decoding steps, allowing for improved throughput without sacrificing performance. Experimental results show that Sparse-dLLM can achieve up to 10 times higher throughput compared to standard dLLMs, while maintaining similar performance levels and memory usage.'}, 'zh': {'title': 'Sparse-dLLMï¼šæå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'Sparse-dLLMé€šè¿‡åŠ¨æ€ç¼“å­˜é©±é€å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚ä¼ ç»Ÿçš„ç¼“å­˜æŠ€æœ¯è™½ç„¶åŠ é€Ÿäº†è§£ç ï¼Œä½†å ç”¨äº†å¤§é‡å†…å­˜ï¼Œé™åˆ¶äº†é•¿ä¸Šä¸‹æ–‡çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼å­˜åœ¨è·¨å±‚ç¨€ç–æ€§ï¼Œå…³é”®çš„tokenåœ¨è§£ç è¿‡ç¨‹ä¸­å§‹ç»ˆä¿æŒé‡è¦æ€§ã€‚Sparse-dLLMæ˜¯é¦–ä¸ªä¸éœ€è¦è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡å»¶è¿ŸåŒå‘ç¨€ç–ç¼“å­˜ï¼ŒåŠ¨æ€ä¿ç•™é‡è¦tokenå¹¶é©±é€ä¸é‡è¦çš„å‰ç¼€/åç¼€æ¡ç›®ï¼Œä»è€Œå®ç°äº†æ›´é«˜çš„è§£ç ååé‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01415', 'title': 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems', 'url': 'https://huggingface.co/papers/2508.01415', 'abstract': 'RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.', 'score': 4, 'issue_id': 5177, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': 'ea0bbd88dcba8948', 'authors': ['Mingcong Lei', 'Honghao Cai', 'Zezhou Cui', 'Liangchen Tan', 'Junkun Hong', 'Gehan Hu', 'Shuangyu Zhu', 'Yimou Wu', 'Shaohan Jiang', 'Ge Wang', 'Zhen Li', 'Shuguang Cui', 'Yiming Zhao', 'Yatong Han'], 'affiliations': ['FNii-Shenzhen', 'Harbin Engineering University', 'Harbin Institute of Technology, Shenzhen', 'Infused Synapse AI', 'SSE', 'The Chinese University of Hong Kong, Shengzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.01415.jpg', 'data': {'categories': ['#agents', '#optimization', '#open_source', '#training', '#agi', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboMemory: ĞœĞ¾Ğ·Ğ³Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'RoboMemory - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ·Ğ³Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¾Ğ² Ğ¼Ğ¾Ğ·Ğ³Ğ°: Ğ¿Ñ€ĞµĞ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EmbodiedBench. RoboMemory Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ….'}, 'en': {'title': 'RoboMemory: Revolutionizing Lifelong Learning in Robots', 'desc': 'RoboMemory is a new framework designed to help robots learn continuously over time, inspired by how the human brain works. It uses four main components that mimic brain functions to improve memory and planning in robots, allowing them to handle complex tasks better. The framework addresses issues like slow memory access and the need for robots to remember different types of information effectively. Tests show that RoboMemory significantly outperforms existing systems in real-world scenarios, making it a promising advancement in robotic learning.'}, 'zh': {'title': 'RoboMemoryï¼šæå‡æœºå™¨äººç»ˆèº«å­¦ä¹ çš„å¤šè®°å¿†æ¡†æ¶', 'desc': 'RoboMemoryæ˜¯ä¸€ä¸ªå—å¤§è„‘å¯å‘çš„å¤šè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç‰©ç†æœºå™¨äººåœ¨ç»ˆèº«å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚å®ƒç»“åˆäº†è®¤çŸ¥ç¥ç»ç§‘å­¦çš„åŸç†ï¼Œè§£å†³äº†ç°å®ç¯å¢ƒä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚æŒç»­å­¦ä¹ å’Œä»»åŠ¡ç›¸å…³æ€§æ•æ‰ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œåˆ†åˆ«æ¨¡æ‹Ÿå¤§è„‘çš„ä¸åŒéƒ¨åˆ†ï¼Œä»¥å®ç°é•¿æœŸè§„åˆ’å’Œç´¯ç§¯å­¦ä¹ ã€‚é€šè¿‡åœ¨å¤æ‚è®°å¿†æ¡†æ¶ä¸­å¹¶è¡Œæ›´æ–°å’Œæ£€ç´¢ï¼ŒRoboMemoryæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨å®é™…ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01408', 'title': 'Artificial Intelligence and Misinformation in Art: Can Vision Language\n  Models Judge the Hand or the Machine Behind the Canvas?', 'url': 'https://huggingface.co/papers/2508.01408', 'abstract': 'State-of-the-art vision language models struggle with accurately attributing artists and distinguishing AI-generated images, highlighting the need for improvement to prevent misinformation.  \t\t\t\t\tAI-generated summary \t\t\t\t The attribution of artworks in general and of paintings in particular has always been an issue in art. The advent of powerful artificial intelligence models that can generate and analyze images creates new challenges for painting attribution. On the one hand, AI models can create images that mimic the style of a painter, which can be incorrectly attributed, for example, by other AI models. On the other hand, AI models may not be able to correctly identify the artist for real paintings, inducing users to incorrectly attribute paintings. In this paper, both problems are experimentally studied using state-of-the-art AI models for image generation and analysis on a large dataset with close to 40,000 paintings from 128 artists. The results show that vision language models have limited capabilities to: 1) perform canvas attribution and 2) to identify AI generated images. As users increasingly rely on queries to AI models to get information, these results show the need to improve the capabilities of VLMs to reliably perform artist attribution and detection of AI generated images to prevent the spread of incorrect information.', 'score': 4, 'issue_id': 5185, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': '31ac5fdf5a455427', 'authors': ['Tarian Fu', 'Javier Conde', 'Gonzalo MartÃ­nez', 'Pedro Reviriego', 'Elena Merino-GÃ³mez', 'Fernando Moral'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Universidad Antonio de Nebrija', 'Universidad PolitÃ©cnica de Madrid', 'Universidad de Valladolid'], 'pdf_title_img': 'assets/pdf/title_img/2508.01408.jpg', 'data': {'categories': ['#cv', '#dataset', '#ethics', '#hallucinations', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ vs Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾: Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ¾ĞºĞ¾Ğ»Ğ¾ 40 000 ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ 128 Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğ² Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing AI for Accurate Art Attribution and Detection', 'desc': 'This paper investigates the limitations of state-of-the-art vision language models (VLMs) in accurately attributing artworks to their respective artists and distinguishing between real and AI-generated images. The study highlights that these models struggle with both canvas attribution and the identification of AI-generated content, which can lead to misinformation. Using a large dataset of nearly 40,000 paintings from 128 artists, the authors demonstrate the inadequacies of current AI models in these tasks. The findings emphasize the urgent need for advancements in VLMs to enhance their reliability in art attribution and image detection.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹ä»¥é˜²æ­¢è‰ºæœ¯ä½œå“é”™è¯¯å½’å±', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‰ºæœ¯ä½œå“å½’å±å’ŒåŒºåˆ†AIç”Ÿæˆå›¾åƒæ–¹é¢çš„ä¸è¶³ã€‚ç ”ç©¶å‘ç°ï¼ŒAIæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ¨¡ä»¿ç”»å®¶é£æ ¼çš„å›¾åƒï¼Œå¯¼è‡´é”™è¯¯å½’å±çš„æƒ…å†µã€‚ä¸æ­¤åŒæ—¶ï¼ŒAIæ¨¡å‹åœ¨è¯†åˆ«çœŸå®ç”»ä½œçš„è‰ºæœ¯å®¶æ—¶ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·é”™è¯¯åœ°å½’å±ä½œå“ã€‚é€šè¿‡å¯¹è¿‘40,000å¹…æ¥è‡ª128ä½è‰ºæœ¯å®¶çš„ç”»ä½œè¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”»å¸ƒå½’å±å’ŒAIç”Ÿæˆå›¾åƒè¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œå¼ºè°ƒäº†æ”¹è¿›è¿™äº›æ¨¡å‹çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01287', 'title': 'Exploitation Is All You Need... for Exploration', 'url': 'https://huggingface.co/papers/2508.01287', 'abstract': 'Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process.', 'score': 3, 'issue_id': 5179, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': '8867dd3f084db81e', 'authors': ['Micah Rentschler', 'Jesse Roberts'], 'affiliations': ['Tennessee Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01287.jpg', 'data': {'categories': ['#agents', '#optimization', '#games', '#reasoning', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½ÑƒÑ‚ÑŒ Ğ¸Ğ· Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹ Ñ‚Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ: Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ°ÑÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ÑÑ€ĞµĞ´Ñ‹, Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€ÑƒĞºĞ¸Ñ… Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ°Ñ… Ğ¸ ÑĞµÑ‚ĞºĞ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ. Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ° Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Exploration Emerges from Greedy Training with the Right Conditions', 'desc': 'This paper explores how meta-reinforcement learning agents can learn to explore their environments even when trained with a focus on maximizing immediate rewards. The authors propose that this exploratory behavior can emerge if the environment has recurring structures, the agent has memory to recall past experiences, and long-term credit assignment is possible. Through experiments, they show that when these conditions are met, agents can engage in information-seeking exploration without explicit incentives. The findings challenge the traditional view that exploration and exploitation are separate goals, suggesting they can coexist within a single reward-maximization framework.'}, 'zh': {'title': 'æ¢ç´¢ä¸åˆ©ç”¨çš„ç»Ÿä¸€ï¼šå…ƒå¼ºåŒ–å­¦ä¹ çš„æ–°è§†è§’', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å…ƒå¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ç‰¹å®šæ¡ä»¶ä¸‹å¦‚ä½•è¡¨ç°å‡ºæ¢ç´¢è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºï¼Œå½“ç¯å¢ƒå…·æœ‰é‡å¤ç»“æ„ã€ä»£ç†å…·å¤‡è®°å¿†èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¿›è¡Œé•¿æœŸä¿¡ç”¨åˆ†é…æ—¶ï¼Œå³ä½¿ä»£ç†ä»…ä»¥è´ªå©ªç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä¹Ÿèƒ½è‡ªå‘åœ°è¿›è¡Œæ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨éšæœºå¤šè‡‚è€è™æœºå’Œæ—¶é—´æ‰©å±•çš„ç½‘æ ¼ä¸–ç•Œä¸­ï¼Œæ»¡è¶³è¿™äº›æ¡ä»¶çš„ä»£ç†ä¼šè¡¨ç°å‡ºä¿¡æ¯å¯»æ±‚çš„æ¢ç´¢è¡Œä¸ºã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œæ¢ç´¢å’Œåˆ©ç”¨å¹¶éå®Œå…¨å¯¹ç«‹ï¼Œè€Œæ˜¯å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„å¥–åŠ±æœ€å¤§åŒ–è¿‡ç¨‹å…±åŒå‡ºç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00910', 'title': 'Cyber-Zero: Training Cybersecurity Agents without Runtime', 'url': 'https://huggingface.co/papers/2508.00910', 'abstract': 'Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.', 'score': 3, 'issue_id': 5177, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ»Ñ', 'en': 'July 29', 'zh': '7æœˆ29æ—¥'}, 'hash': '181a31b28dfe8e6a', 'authors': ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang'], 'affiliations': ['Amazon', 'Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00910.jpg', 'data': {'categories': ['#agents', '#dataset', '#synthetic', '#benchmark', '#open_source'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ±ĞµĞ· ÑÑ€ĞµĞ´Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… LLM Ğ² ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Cyber-Zero - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹ CTF Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CTF. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Cyber-Zero-32B ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Cybersecurity AI with Runtime-Free Trajectory Synthesis', 'desc': 'Cyber-Zero introduces a novel framework for training cybersecurity large language models (LLMs) without the need for executable runtime environments. It synthesizes agent trajectories from Capture The Flag (CTF) writeups, allowing the generation of realistic interaction sequences that mimic runtime behaviors. This approach enables the training of LLM-based agents that outperform existing models on key CTF benchmarks. By achieving state-of-the-art performance with a cost-effective solution, Cyber-Zero demonstrates the potential of runtime-free trajectory synthesis in advancing cybersecurity AI.'}, 'zh': {'title': 'Cyber-Zeroï¼šæ— è¿è¡Œæ—¶ç¯å¢ƒçš„ç½‘ç»œå®‰å…¨ä»£ç†è®­ç»ƒæ–°æ–¹æ³•', 'desc': 'Cyber-Zero æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆé«˜è´¨é‡çš„ä»£ç†è½¨è¿¹æ¥è®­ç»ƒç½‘ç»œå®‰å…¨é¢†åŸŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè€Œæ— éœ€å®é™…çš„è¿è¡Œæ—¶ç¯å¢ƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…¬å¼€çš„CTFï¼ˆCapture The Flagï¼‰å†™ä½œææ–™ï¼Œé‡‡ç”¨åŸºäºè§’è‰²çš„LLMæ¨¡æ‹Ÿï¼Œé€†å‘å·¥ç¨‹è¿è¡Œæ—¶è¡Œä¸ºï¼Œç”ŸæˆçœŸå®çš„é•¿æ—¶é—´äº¤äº’åºåˆ—ã€‚é€šè¿‡ä½¿ç”¨Cyber-Zeroåˆæˆçš„è½¨è¿¹ï¼Œæˆ‘ä»¬è®­ç»ƒçš„LLMä»£ç†åœ¨ä¸‰ä¸ªä¸»è¦çš„CTFåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æå‡è¾¾13.1%ã€‚Cyber-Zero-32Bæ¨¡å‹åœ¨å¼€æ”¾æƒé‡æ¨¡å‹ä¸­åˆ›é€ äº†æ–°çš„æ€§èƒ½è®°å½•ï¼Œå±•ç¤ºäº†æ— è¿è¡Œæ—¶è½¨è¿¹åˆæˆåœ¨ç½‘ç»œå®‰å…¨ä»£ç†å¼€å‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00890', 'title': 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks', 'url': 'https://huggingface.co/papers/2508.00890', 'abstract': 'AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.', 'score': 2, 'issue_id': 5178, 'pub_date': '2025-07-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ»Ñ', 'en': 'July 26', 'zh': '7æœˆ26æ—¥'}, 'hash': '359ff54230f7e0ba', 'authors': ['Fali Wang', 'Hui Liu', 'Zhenwei Dai', 'Jingying Zeng', 'Zhiwei Zhang', 'Zongyu Wu', 'Chen Luo', 'Zhen Li', 'Xianfeng Tang', 'Qi He', 'Suhang Wang'], 'affiliations': ['Amazon, Palo Alto, CA, USA', 'The Pennsylvania State University, University Park, PA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.00890.jpg', 'data': {'categories': ['#interpretability', '#rl', '#agents', '#optimization', '#inference'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'AgentTTS - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. AgentTTS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Optimizing Compute Allocation for Complex Multi-Stage Tasks with AgentTTS', 'desc': 'AgentTTS is a framework that optimizes how computing resources are allocated for complex tasks that involve multiple stages. It focuses on improving the performance of large language models (LLMs) by dynamically adjusting resource allocation during inference, especially for tasks that require different capabilities at each stage. The framework addresses challenges such as the combinatorial nature of model selection and budget allocation, which makes traditional search methods inefficient. Through extensive experiments, AgentTTS has shown to be more efficient and robust compared to existing methods, providing better performance across various datasets and tasks.'}, 'zh': {'title': 'AgentTTSï¼šä¼˜åŒ–å¤šé˜¶æ®µä»»åŠ¡çš„è®¡ç®—åˆ†é…', 'desc': 'AgentTTSæ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šé˜¶æ®µå¤æ‚ä»»åŠ¡çš„è®¡ç®—èµ„æºåˆ†é…ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨æ€§èƒ½å’Œé²æ£’æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£åé¦ˆä¸æ‰§è¡Œç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œè‡ªåŠ¨æœç´¢è®¡ç®—æœ€ä¼˜çš„åˆ†é…æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentTTSåœ¨æœç´¢æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œå…¶ä»–åŸºäºLLMçš„åŸºçº¿ï¼Œå¹¶ä¸”å¯¹è®­ç»ƒé›†å¤§å°çš„å˜åŒ–è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02605', 'title': 'ReMoMask: Retrieval-Augmented Masked Motion Generation', 'url': 'https://huggingface.co/papers/2508.02605', 'abstract': "ReMoMask, a unified framework, addresses limitations in text-to-motion generation by integrating a Bidirectional Momentum Text-Motion Model, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance on HumanML3D and KIT-ML benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.", 'score': 1, 'issue_id': 5185, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '8fa0e8fe5289831e', 'authors': ['Zhengdao Li', 'Siheng Wang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Jiangsu University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02605.jpg', 'data': {'categories': ['#diffusion', '#games', '#benchmark', '#rag', '#video', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ReMoMask: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'ReMoMask - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ½ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ RAG-Ğ±ĞµĞ·ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. ReMoMask Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HumanML3D Ğ¸ KIT-ML, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ FID. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ².'}, 'en': {'title': 'ReMoMask: Revolutionizing Text-to-Motion Generation!', 'desc': "ReMoMask is a new framework designed to improve text-to-motion generation, which creates human motion sequences from text descriptions. It combines a Bidirectional Momentum Text-Motion Model, which enhances retrieval accuracy, with a Semantic Spatio-temporal Attention mechanism that ensures realistic motion by applying biomechanical constraints. Additionally, it uses RAG-Classier-Free Guidance to boost the model's ability to generalize from data. The framework has shown significant improvements in generating coherent motions, outperforming previous methods on key benchmarks."}, 'zh': {'title': 'ReMoMaskï¼šæ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'ReMoMaskæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆä¸­çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡é›†æˆåŒå‘åŠ¨é‡æ–‡æœ¬-è¿åŠ¨æ¨¡å‹ã€è¯­ä¹‰æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œæ˜¾è‘—æé«˜äº†åœ¨HumanML3Då’ŒKIT-MLåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨é‡é˜Ÿåˆ—è§£è€¦è´Ÿæ ·æœ¬è§„æ¨¡ä¸æ‰¹é‡å¤§å°ï¼Œæå‡äº†è·¨æ¨¡æ€æ£€ç´¢çš„ç²¾åº¦ã€‚åŒæ—¶ï¼Œè¯­ä¹‰æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶åœ¨éƒ¨åˆ†èåˆè¿‡ç¨‹ä¸­æ–½åŠ ç”Ÿç‰©åŠ›å­¦çº¦æŸï¼Œæ¶ˆé™¤äº†å¼‚æ­¥ä¼ªå½±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02268', 'title': 'SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System', 'url': 'https://huggingface.co/papers/2508.02268', 'abstract': 'A bidirectional machine translation system, SHAMI-MT, bridges the gap between Modern Standard Arabic and the Syrian dialect using AraT5v2-base-1024 architecture, achieving high-quality translations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces SHAMI-MT, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of 4.01 out of 5.0 when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.', 'score': 1, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': '0704696f67aca30f', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh - Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2508.02268.jpg', 'data': {'categories': ['#training', '#low_resource', '#multilingual', '#dataset', '#machine_translation'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ¼: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ', 'desc': 'SHAMI-MT - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¸ ÑĞ¸Ñ€Ğ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ AraT5v2-base-1024 Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Nabra Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ MADAR. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ° ÑĞ¸Ñ€Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° 4.01 Ğ¸Ğ· 5.0 Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞµ GPT-4.1.'}, 'en': {'title': 'Bridging Dialects: SHAMI-MT Translates Arabic with Precision', 'desc': 'The paper presents SHAMI-MT, a bidirectional machine translation system designed to translate between Modern Standard Arabic (MSA) and the Syrian dialect. Utilizing the AraT5v2-base-1024 architecture, the system includes two specialized models for MSA-to-Shami and Shami-to-MSA translations. These models were fine-tuned on the Nabra dataset and evaluated using the MADAR corpus, achieving a high quality score of 4.01 out of 5.0. This work addresses the challenges of diglossia in Arabic, providing a valuable tool for accurate and culturally relevant translations.'}, 'zh': {'title': 'å¼¥åˆé˜¿æ‹‰ä¼¯è¯­ä¸å™åˆ©äºšæ–¹è¨€çš„ç¿»è¯‘æ¡¥æ¢', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŒå‘æœºå™¨ç¿»è¯‘ç³»ç»ŸSHAMI-MTï¼Œæ—¨åœ¨å¼¥åˆç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ä¸å™åˆ©äºšæ–¹è¨€ä¹‹é—´çš„å·®è·ã€‚è¯¥ç³»ç»ŸåŸºäºå…ˆè¿›çš„AraT5v2-base-1024æ¶æ„ï¼Œåˆ†åˆ«é’ˆå¯¹ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­åˆ°å™åˆ©äºšæ–¹è¨€å’Œå™åˆ©äºšæ–¹è¨€åˆ°ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­çš„ç¿»è¯‘è¿›è¡Œäº†ä¼˜åŒ–ã€‚é€šè¿‡åœ¨å…¨é¢çš„Nabraæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨MADARè¯­æ–™åº“çš„æœªè§æ•°æ®ä¸Šè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæ¨¡å‹è¡¨ç°å‡ºè‰²ã€‚SHAMI-MTä¸ºé˜¿æ‹‰ä¼¯æ–¹è¨€ç¿»è¯‘é¢†åŸŸæä¾›äº†é‡è¦çš„é«˜ä¿çœŸå·¥å…·ï¼Œä¿ƒè¿›äº†å†…å®¹æœ¬åœ°åŒ–å’Œè·¨æ–‡åŒ–äº¤æµã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01109', 'title': 'Platonic Representations for Poverty Mapping: Unified Vision-Language\n  Codes or Agent-Induced Novelty?', 'url': 'https://huggingface.co/papers/2508.01109', 'abstract': 'A multimodal framework using satellite imagery and text data outperforms vision-only models in predicting household wealth, with LLM-generated text proving more effective than agent-retrieved text.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.', 'score': 1, 'issue_id': 5188, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '411bd35601db2ebd', 'authors': ['Satiyabooshan Murugaboopathy', 'Connor T. Jerzak', 'Adel Daoud'], 'affiliations': ['Chalmers & LinkÃ¶ping University', 'Fraunhofer Center', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2508.01109.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#dataset', '#science'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ¾Ğ¼Ğ¾Ñ…Ğ¾Ğ·ÑĞ¹ÑÑ‚Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ½Ğ¸Ğ¼ĞºĞ¸ Landsat Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, LLM Ğ¸ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ÑŒ Ğ²ÑĞµÑ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Wealth Insights: The Power of Multimodal Learning', 'desc': "This paper presents a multimodal framework that combines satellite imagery and text data to predict household wealth more accurately than models using only visual data. By analyzing Demographic and Health Survey data from African neighborhoods, the authors demonstrate that integrating LLM-generated text with satellite images significantly improves prediction performance. The study reveals that the internal knowledge of large language models (LLMs) is more effective than text retrieved by AI agents, enhancing the model's robustness across different contexts. Additionally, the research contributes a large-scale dataset that links over 60,000 clusters of socio-economic data with corresponding satellite images and textual descriptions."}, 'zh': {'title': 'å¤šæ¨¡æ€æ¡†æ¶æå‡å®¶åº­è´¢å¯Œé¢„æµ‹ç²¾åº¦', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å®¶åº­è´¢å¯Œç­‰ç¤¾ä¼šç»æµæŒ‡æ ‡æ˜¯å¦å¯ä»¥é€šè¿‡å«æ˜Ÿå›¾åƒå’Œäº’è”ç½‘æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œç»“åˆäº†å«æ˜Ÿå›¾åƒå’ŒåŸºäºä½ç½®/å¹´ä»½çš„LLMç”Ÿæˆæ–‡æœ¬ï¼Œä»¥æé«˜è´¢å¯Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œèåˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„æ¨¡å‹åœ¨è´¢å¯Œé¢„æµ‹ä¸Šä¼˜äºä»…ä½¿ç”¨è§†è§‰çš„æ¨¡å‹ï¼Œä¸”LLMç”Ÿæˆçš„æ–‡æœ¬æ•ˆæœæ›´ä½³ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡60,000ä¸ªDHSé›†ç¾¤çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00024', 'title': 'Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine\n  Learning', 'url': 'https://huggingface.co/papers/2508.00024', 'abstract': 'Combining Vision Transformer embeddings with quantum-classical pipelines achieves quantum advantage in classification tasks, demonstrating the importance of embedding choice in quantum machine learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.', 'score': 1, 'issue_id': 5185, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ»Ñ', 'en': 'July 28', 'zh': '7æœˆ28æ—¥'}, 'hash': 'c9825e1a6f4d2a1c', 'authors': ['SebastiÃ¡n AndrÃ©s Cajas OrdÃ³Ã±ez', 'Luis Fernando Torres Torres', 'Mario Bifulco', 'Carlos AndrÃ©s DurÃ¡n', 'Cristian Bosch', 'Ricardo SimÃ³n Carbajo'], 'affiliations': ['Corporation for Aerospace Initiatives (CASIRI), University of Cauca, PopayÃ¡n, Colombia', 'Department of Computer Science, University of Torino, Torino, Italy', 'National Irish Centre for AI (CeADAR), University College Dublin (UCD), Dublin, Ireland', 'SISTEMIC Research Group, University of Antioquia, MedellÃ­n, Colombia'], 'pdf_title_img': 'assets/pdf/title_img/2508.00024.jpg', 'data': {'categories': ['#cv', '#games', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Vision Transformer Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ SVM Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Fashion-MNIST Ğ¸ MNIST. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ViT ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ CNN Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹.'}, 'en': {'title': 'Unlocking Quantum Advantage with Vision Transformers', 'desc': 'This paper explores how combining Vision Transformer (ViT) embeddings with quantum-classical pipelines can enhance classification tasks in machine learning. It highlights the challenges faced by Quantum Support Vector Machines (QSVMs) due to high-dimensional quantum states and hardware limitations. The authors introduce a method that uses class-balanced k-means distillation alongside pretrained ViT embeddings, which significantly improves accuracy compared to classical SVMs. Their findings suggest that the choice of embedding is crucial for achieving quantum advantage, demonstrating a beneficial interaction between transformer attention mechanisms and quantum feature spaces.'}, 'zh': {'title': 'é‡å­æœºå™¨å­¦ä¹ ä¸­çš„åµŒå…¥é€‰æ‹©ä¸ä¼˜åŠ¿', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å°†è§†è§‰å˜æ¢å™¨ï¼ˆVision Transformerï¼‰åµŒå…¥ä¸é‡å­-ç»å…¸ç®¡é“ç»“åˆçš„æ–¹å¼ï¼Œä»¥åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å®ç°é‡å­ä¼˜åŠ¿ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåµŒå…¥çš„é€‰æ‹©å¯¹é‡å­æœºå™¨å­¦ä¹ è‡³å…³é‡è¦ï¼Œä½¿ç”¨ViTåµŒå…¥å¯ä»¥åœ¨Fashion-MNISTæ•°æ®é›†ä¸Šæé«˜8.02%çš„å‡†ç¡®ç‡ï¼Œè€Œåœ¨MNISTæ•°æ®é›†ä¸Šæé«˜4.42%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç‰¹å¾çš„è¡¨ç°å´æœ‰æ‰€ä¸‹é™ã€‚é€šè¿‡ä½¿ç”¨16é‡å­æ¯”ç‰¹çš„å¼ é‡ç½‘ç»œæ¨¡æ‹Ÿï¼Œæœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯æ˜äº†é‡å­æ ¸ä¼˜åŠ¿ä¸åµŒå…¥é€‰æ‹©ä¹‹é—´çš„å…³é”®å…³ç³»ï¼Œæ­ç¤ºäº†å˜æ¢å™¨æ³¨æ„åŠ›ä¸é‡å­ç‰¹å¾ç©ºé—´ä¹‹é—´çš„åŸºæœ¬ååŒä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01773', 'title': 'Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2508.01773', 'abstract': "An uncertainty-driven framework for automated process reward data construction and aggregation methods improves the effectiveness and efficiency of Process-Level Reward Models in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.", 'score': 0, 'issue_id': 5186, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': '872cba646c3b0c3d', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01773.jpg', 'data': {'categories': ['#math', '#training', '#dataset', '#reasoning', '#optimization', '#data'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (PRM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¼Ğ°Ğ¶Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Automating Reward Data for Smarter Math Reasoning', 'desc': 'This paper presents a new framework that automates the creation of process reward data, which is essential for training Process-Level Reward Models (PRMs) in mathematical reasoning tasks. The authors highlight the challenges of existing data construction methods, which are often time-consuming and inefficient. They introduce two innovative output aggregation techniques, Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, that enhance the performance of PRMs by effectively combining the strengths of traditional voting methods with PRM evaluations. Experimental results demonstrate that this uncertainty-driven approach significantly improves both the quality of the reward data and the reasoning capabilities of the models tested.'}, 'zh': {'title': 'åŸºäºä¸ç¡®å®šæ€§çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹å¥–åŠ±æ•°æ®æ„å»ºæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–è¿‡ç¨‹å¥–åŠ±æ•°æ®çš„æ„å»ºå’Œèšåˆæ–¹æ³•ï¼Œä»¥æé«˜è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰é€šè¿‡åœ¨æ¯ä¸ªä¸­é—´æ­¥éª¤æä¾›ç›‘ç£å’Œè¯„ä¼°ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ„å»ºé«˜è´¨é‡çš„è¿‡ç¨‹å¥–åŠ±æ•°æ®é€šå¸¸éœ€è¦è€—è´¹å¤§é‡äººåŠ›ï¼Œç°æœ‰æ–¹æ³•æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§é€šç”¨çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥è¾“å‡ºèšåˆæ–¹æ³•ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†PRMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16290', 'title': 'Dens3R: A Foundation Model for 3D Geometry Prediction', 'url': 'https://huggingface.co/papers/2507.16290', 'abstract': 'Dens3R is a 3D foundation model that jointly predicts multiple geometric quantities using a two-stage training framework, enhancing consistency and performance in dense 3D reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains a major challenge. Most existing methods are limited to predicting a single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore a unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, a 3D foundation model designed for joint geometric dense prediction and adaptable to a wide range of downstream tasks. Dens3R adopts a two-stage training framework to progressively build a pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design a lightweight shared encoder-decoder backbone and introduce position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose a post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various dense 3D prediction tasks and highlight its potential for broader applications.', 'score': 0, 'issue_id': 5189, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': 'd47ef3bd9b4560f6', 'authors': ['Xianze Fang', 'Jingnan Gao', 'Zhe Wang', 'Zhuo Chen', 'Xingyu Ren', 'Jiangjing Lyu', 'Qiaomu Ren', 'Zhonglei Yang', 'Xiaokang Yang', 'Yichao Yan', 'Chengfei Lyu'], 'affiliations': ['Alibaba Group, China', 'Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.16290.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'Dens3R - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Dens3R Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Dens3R: Unified Predictions for Consistent 3D Geometry', 'desc': "Dens3R is a 3D foundation model that improves dense 3D reconstruction by predicting multiple geometric quantities together, such as depth and surface normals. It uses a two-stage training framework to enhance the consistency and accuracy of these predictions, addressing the limitations of existing methods that focus on single geometry predictions. By modeling the relationships between different geometric properties, Dens3R ensures that the predictions are coherent and reliable. The model's design includes a lightweight encoder-decoder and advanced encoding techniques, making it adaptable for various applications in 3D geometry tasks."}, 'zh': {'title': 'Dens3Rï¼šè”åˆå‡ ä½•é¢„æµ‹çš„3DåŸºç¡€æ¨¡å‹', 'desc': 'Dens3Ræ˜¯ä¸€ç§3DåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶è”åˆé¢„æµ‹å¤šä¸ªå‡ ä½•é‡ï¼Œä»è€Œæé«˜å¯†é›†3Dé‡å»ºä»»åŠ¡ä¸­çš„ä¸€è‡´æ€§å’Œæ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½ä»è¾“å…¥å›¾åƒä¸­é¢„æµ‹å•ä¸€å‡ ä½•é‡ï¼Œè€ŒDens3Råˆ™é€šè¿‡å»ºæ¨¡ä¸åŒå‡ ä½•å±æ€§ä¹‹é—´çš„ç»“æ„è€¦åˆï¼Œå®ç°äº†è”åˆå›å½’ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è½»é‡çº§å…±äº«ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶å¼•å…¥ä½ç½®æ’å€¼æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä»¥å¢å¼ºå¯¹é«˜åˆ†è¾¨ç‡è¾“å…¥çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDens3Råœ¨å¤šç§å¯†é›†3Dé¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (14)', '#agents (29)', '#agi (5)', '#alignment (13)', '#architecture (20)', '#audio (7)', '#benchmark (54)', '#cv (22)', '#data (19)', '#dataset (47)', '#diffusion (14)', '#ethics (9)', '#games (26)', '#graphs', '#hallucinations (9)', '#healthcare (5)', '#inference (7)', '#interpretability (12)', '#leakage (1)', '#long_context (13)', '#low_resource (2)', '#machine_translation (3)', '#math (6)', '#multilingual (8)', '#multimodal (41)', '#open_source (41)', '#optimization (67)', '#plp (2)', '#rag (5)', '#reasoning (33)', '#rl (23)', '#rlhf (11)', '#robotics (6)', '#science (8)', '#security (5)', '#small_models (2)', '#story_generation', '#survey (6)', '#synthetic (10)', '#training (58)', '#transfer_learning (2)', '#video (12)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-08-10 12:51',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-10 12:51')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-10 12:51')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    