
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 87 papers. August 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Август 2025</span> | <span id="title-articles-count">87 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-07.html">⬅️ <span id="prev-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-09.html">➡️ <span id="next-date">09.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Август 2025', 'en': 'August 2025', 'zh': '8月2025年'};
        let feedDateNext = {'ru': '09.2025', 'en': '09/2025', 'zh': '9月2025年'};
        let feedDatePrev = {'ru': '07.2025', 'en': '07/2025', 'zh': '7月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.00819', 'title': 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.00819', 'abstract': 'DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.', 'score': 35, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '2241beba3b69f1fd', 'authors': ['Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Jiaqi Wang', 'Dahua Lin'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00819.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#diffusion', '#long_context'], 'emoji': '🔄', 'ru': {'title': 'Динамическая адаптация длины раскрывает потенциал диффузионных языковых моделей', 'desc': 'DAEDAL - это новая стратегия динамической адаптации длины для диффузионных больших языковых моделей (DLLM). Она позволяет преодолеть ограничение статически заданной длины генерации, которое снижает эффективность DLLM. DAEDAL работает в два этапа: сначала расширяет начальную короткую длину до подходящей для задачи, а затем во время денойзинга динамически расширяет недостаточные области генерации. Эксперименты показывают, что DAEDAL достигает сравнимой или превосходящей производительности по сравнению с тщательно настроенными базовыми моделями фиксированной длины, одновременно повышая вычислительную эффективность.'}, 'en': {'title': 'Dynamic Length Adaptation for Enhanced DLLM Performance', 'desc': 'DAEDAL is a new method that improves Diffusion Large Language Models (DLLMs) by allowing them to adapt their output length dynamically without needing additional training. Traditional DLLMs require a fixed length for generation, which can limit their performance on complex tasks or waste computational resources. DAEDAL addresses this issue by using internal signals from the model to determine the optimal length for responses, expanding the generation length as needed. This approach not only enhances the quality of the generated text but also increases efficiency, making DLLMs more competitive with Autoregressive models.'}, 'zh': {'title': 'DAEDAL：动态适应长度的去噪新策略', 'desc': 'DAEDAL是一种新颖的无训练去噪策略，能够在扩散大型语言模型中实现动态长度适应，从而提高性能和计算效率。扩散大型语言模型（DLLMs）在生成效率和全局上下文建模方面表现出色，但其静态生成长度限制了实际应用。DAEDAL通过利用模型内部信号，动态调整生成长度，解决了静态长度带来的性能和计算开销问题。实验表明，DAEDAL在性能上与固定长度基线相当，甚至在某些情况下更优，同时提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2507.23268', 'title': 'PixNerd: Pixel Neural Field Diffusion', 'url': 'https://huggingface.co/papers/2507.23268', 'abstract': 'Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID on ImageNet 512times512 without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.', 'score': 31, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'f035699955568725', 'authors': ['Shuai Wang', 'Ziteng Gao', 'Chenhui Zhu', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed', 'Nanjing University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23268.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений без сложных архитектур', 'desc': 'PixNerd (Pixel Neural Field Diffusion) - это новый метод генерации изображений, работающий в пиксельном пространстве без использования вариационных автоэнкодеров. Он предлагает одноэтапный процесс генерации высококачественных изображений без сложных каскадных архитектур. PixNerd достигает впечатляющих результатов на наборе данных ImageNet, превосходя существующие методы по метрике FID. Кроме того, модель успешно применяется для задачи генерации изображений по текстовому описанию, показывая конкурентоспособные результаты на бенчмарках GenEval и DPG.'}, 'en': {'title': 'Efficient Image Generation with PixNerd: No VAEs, No Hassle!', 'desc': 'Pixel Neural Field Diffusion (PixNerd) introduces a novel approach to image generation that operates in a single-scale and single-stage manner, eliminating the need for variational autoencoders (VAEs) and complex pipelines. This method addresses the issues of accumulated errors and artifacts that arise from traditional two-stage training processes. By utilizing a patch-wise decoding strategy with neural fields, PixNerd achieves impressive performance metrics, such as a 2.15 FID score on ImageNet 256x256. Additionally, it extends its capabilities to text-to-image generation, demonstrating competitive results on various benchmarks.'}, 'zh': {'title': '高效图像生成的新方法：PixNerd', 'desc': 'Pixel Neural Field Diffusion（PixNerd）是一种高效的图像生成方法，采用单尺度、单阶段的流程，无需变分自编码器（VAE）或复杂的管道。该方法通过神经场模型实现了补丁级解码，避免了传统方法中常见的累积误差和解码伪影。PixNerd在ImageNet数据集上取得了2.15的FID分数，显示出其优越的性能。我们还将PixNerd扩展到文本生成图像的应用中，取得了在GenEval和DPG基准测试中的竞争性成绩。'}}}, {'id': 'https://huggingface.co/papers/2508.00414', 'title': 'Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training', 'url': 'https://huggingface.co/papers/2508.00414', 'abstract': 'Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro', 'score': 14, 'issue_id': 5169, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '0cd43b7d9f3e1eb5', 'authors': ['Tianqing Fang', 'Zhisong Zhang', 'Xiaoyang Wang', 'Rui Wang', 'Can Qin', 'Yuxuan Wan', 'Jun-Yu Ma', 'Ce Zhang', 'Jiaqi Chen', 'Xiyun Li', 'Hongming Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.00414.jpg', 'data': {'categories': ['#data', '#agi', '#training', '#open_source', '#reasoning', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Открытая агентная система ИИ для демократизации передовых технологий', 'desc': 'Cognitive Kernel-Pro - это открытая многомодульная агентная система, улучшающая надежность и производительность ИИ-агентов с помощью курирования данных и новых стратегий во время тестирования. Система фокусируется на создании качественных обучающих данных для Агентных Фундаментальных Моделей в четырех ключевых областях: веб, файлы, код и общие рассуждения. Cognitive Kernel-Pro исследует новые стратегии рефлексии и голосования агентов для повышения их надежности. Система достигает передовых результатов среди открытых и бесплатных агентов на бенчмарке GAIA, превосходя предыдущие ведущие системы.'}, 'en': {'title': 'Democratizing AI Agent Development with Cognitive Kernel-Pro', 'desc': 'Cognitive Kernel-Pro is an open-source framework designed to improve the robustness and performance of AI agents through effective data curation and innovative test-time strategies. It focuses on creating high-quality training data for Agent Foundation Models by constructing queries and trajectories across various domains like web interaction and coding. The framework also introduces new methods for agent reflection and voting, which enhance decision-making capabilities. By achieving state-of-the-art results on the GAIA benchmark, Cognitive Kernel-Pro sets a new standard for accessible and high-performance AI agents.'}, 'zh': {'title': '开放源代码，提升AI代理的未来！', 'desc': 'Cognitive Kernel-Pro是一个开源的多模块代理框架，旨在通过数据整理和新颖的测试策略来增强AI代理的鲁棒性和性能。该框架支持复杂推理、网络交互、编码和自主研究能力，推动下一代人工智能的发展。我们系统地研究了高质量训练数据的整理，重点关注查询、轨迹和可验证答案的构建。Cognitive Kernel-Pro在GAIA上进行了评估，取得了开源和免费代理中的最佳结果，设立了高能力AI代理的新标准。'}}}, {'id': 'https://huggingface.co/papers/2507.23478', 'title': '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding', 'url': 'https://huggingface.co/papers/2507.23478', 'abstract': '3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.', 'score': 6, 'issue_id': 5155, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'f5e99fc10e8b9ad5', 'authors': ['Ting Huang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['School of Computer Science, Peking University', 'Shanghai University of Engineering Science'], 'pdf_title_img': 'assets/pdf/title_img/2507.23478.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset', '#reasoning', '#rlhf', '#synthetic'], 'emoji': '🧠', 'ru': {'title': '3D-R1: Революция в понимании трехмерных сцен с помощью ИИ', 'desc': 'Модель 3D-R1 улучшает понимание трехмерных сцен с помощью высококачественного синтетического датасета и обучения с подкреплением. Она использует динамический выбор ракурсов для более информативного анализа 3D-сцен. 3D-R1 применяет функции вознаграждения для улучшения точности восприятия и семантической точности ответов. Эксперименты показывают значительное улучшение результатов на различных бенчмарках трехмерных сцен.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with 3D-R1', 'desc': "The paper presents 3D-R1, a model designed to improve 3D scene understanding by addressing the limitations of existing vision-language models (VLMs). It introduces a high-quality synthetic dataset called Scene-30K, which is used to enhance the model's reasoning capabilities. The training process incorporates reinforcement learning with a GRPO policy and utilizes multiple reward functions to ensure accuracy and semantic precision. Additionally, a dynamic view selection strategy is implemented to optimize the perspectives used for analyzing 3D scenes, resulting in a notable average improvement of 10% in performance across various benchmarks."}, 'zh': {'title': '3D-R1：提升3D场景理解的智能模型', 'desc': '3D-R1 是一个增强 3D 场景理解的基础模型，利用高质量的合成数据集和强化学习方法来提升推理能力。我们构建了一个名为 Scene-30K 的合成数据集，作为 3D-R1 的冷启动初始化数据。通过引入动态视角选择策略，3D-R1 能够自适应选择最具信息量的视角进行 3D 场景理解。实验结果表明，3D-R1 在多个 3D 场景基准测试中平均提升了 10%，有效增强了推理和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.23361', 'title': 'SWE-Exp: Experience-Driven Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23361', 'abstract': 'SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.', 'score': 6, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e16fe4dad5f61553', 'authors': ['Silin Chen', 'Shaoxin Lin', 'Xiaodong Gu', 'Yuling Shi', 'Heng Lian', 'Longfei Yun', 'Dong Chen', 'Weiguo Sun', 'Lin Cao', 'Qianxiang Wang'], 'affiliations': ['Huawei, China', 'Shanghai Jiao Tong University, China', 'UC San Diego, United States', 'Xidian University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23361.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Опыт - ключ к эффективному решению проблем в ПО', 'desc': 'SWE-Exp - это новый подход к решению проблем в программном обеспечении, использующий накопленный опыт предыдущих попыток исправления ошибок. Метод создает многоуровневый банк опыта, который включает как успешные, так и неудачные попытки решения проблем. SWE-Exp извлекает многоуровневые знания о решении проблем - от общего понимания до конкретных изменений в коде. Эксперименты показывают, что SWE-Exp достигает наилучших результатов в решении проблем на тестовом наборе SWE-bench-Verified среди агентов с открытым исходным кодом.'}, 'en': {'title': 'Transforming Software Issue Resolution with Experience-Driven Learning', 'desc': 'SWE-Exp is a novel approach that enhances software issue resolution by utilizing past experiences of agents to improve their performance. Unlike traditional agents that do not retain knowledge, SWE-Exp builds an experience bank that captures both successful and failed attempts at resolving issues. This allows the system to learn from previous repairs and apply that knowledge to new problems, leading to more efficient and effective resolutions. The method has demonstrated a significant improvement in resolution rates, showcasing a shift from random exploration to a more strategic, experience-driven process.'}, 'zh': {'title': '经验驱动的软件问题解决新方法', 'desc': 'SWE-Exp是一种增强软件问题解决能力的方法，通过系统地积累和利用过去代理的修复经验，提高了解决率。当前的代理在处理问题时缺乏记忆，无法重用之前的知识，导致重复探索失败的路径。SWE-Exp通过建立一个多层次的经验库，提取成功和失败的修复尝试中的可重用知识，从而实现跨问题的持续学习。实验表明，SWE-Exp在开源代理框架下的SWE-bench-Verified上达到了41.6%的最佳解决率，标志着自动化软件工程代理的一个新范式。'}}}, {'id': 'https://huggingface.co/papers/2508.00265', 'title': 'Multimodal Referring Segmentation: A Survey', 'url': 'https://huggingface.co/papers/2508.00265', 'abstract': "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", 'score': 5, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '1604e587f6dc8177', 'authors': ['Henghui Ding', 'Song Tang', 'Shuting He', 'Chang Liu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance Inc.', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2508.00265.jpg', 'data': {'categories': ['#cv', '#multimodal', '#3d', '#survey', '#video', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Мультимодальная сегментация: от пикселей к пониманию', 'desc': 'Эта статья представляет собой обзор методов мультимодальной сегментации по ссылкам, охватывающий достижения в области сверточных нейронных сетей, трансформеров и больших языковых моделей. Авторы рассматривают задачу сегментации объектов на изображениях, видео и в 3D-сценах на основе текстовых или аудио инструкций. В работе представлена унифицированная мета-архитектура для сегментации по ссылкам и обзор репрезентативных методов для различных визуальных сцен. Также обсуждаются обобщенные методы выражения ссылок (GREx) для решения проблем сложности реального мира.'}, 'en': {'title': 'Enhancing Object Segmentation with Multimodal Instructions', 'desc': 'This paper surveys the field of multimodal referring segmentation, which focuses on identifying and segmenting objects in visual content based on textual or audio instructions. It highlights the advancements made through convolutional neural networks, transformers, and large language models that enhance the ability to understand and process multimodal data. The authors present a unified meta architecture for referring segmentation and review various methods applicable to images, videos, and 3D scenes. Additionally, they discuss challenges in real-world applications and provide performance comparisons on standard benchmarks to evaluate the effectiveness of different approaches.'}, 'zh': {'title': '多模态指向分割的全面调查', 'desc': '多模态指向分割旨在根据文本或音频指令在视觉场景中分割目标物体，如图像、视频和3D场景。该任务在需要根据用户指令进行准确物体感知的实际应用中至关重要。近年来，卷积神经网络、变换器和大型语言模型的进步显著提升了多模态感知能力。本文提供了多模态指向分割的全面调查，涵盖了背景介绍、统一的元架构、代表性方法及其在不同视觉场景中的应用。'}}}, {'id': 'https://huggingface.co/papers/2507.23348', 'title': 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23348', 'abstract': "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.", 'score': 4, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '28b58ecd36ac995b', 'authors': ['Han Li', 'Yuling Shi', 'Shaoxin Lin', 'Xiaodong Gu', 'Heng Lian', 'Xin Wang', 'Yantao Jia', 'Tao Huang', 'Qianxiang Wang'], 'affiliations': ['Huawei China', 'Shanghai Jiao Tong University China', 'Xidian University China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23348.jpg', 'data': {'categories': ['#optimization', '#agents', '#open_source', '#reasoning', '#games', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Дебаты ИИ-агентов для улучшения разработки ПО', 'desc': 'SWE-Debate - это новая система для решения проблем в разработке программного обеспечения, использующая несколько ИИ-агентов. Система организует структурированные дебаты между агентами, каждый из которых предлагает свой подход к локализации и исправлению ошибок. Этот метод позволяет находить более комплексные решения, охватывающие различные части кодовой базы. В результате SWE-Debate превосходит существующие подходы в локализации проблем и планировании исправлений.'}, 'en': {'title': 'Empowering Software Issue Resolution through Competitive Multi-Agent Debate', 'desc': 'SWE-Debate is a multi-agent framework designed to improve issue resolution in software engineering by fostering diverse reasoning among agents. It utilizes large language models to enhance the reasoning capabilities of autonomous agents, allowing them to tackle complex tasks more effectively. By organizing a structured debate among agents with different perspectives, SWE-Debate helps identify broader issue patterns and achieve better localization of software faults. The framework has demonstrated significant improvements in performance on the SWE-bench benchmark, setting new standards in open-source agent frameworks.'}, 'zh': {'title': 'SWE-Debate：多样化推理促进软件问题解决', 'desc': 'SWE-Debate是一个竞争性的多智能体框架，旨在通过促进多样化的推理来增强软件工程中的问题解决能力。该框架利用大型语言模型的推理能力，帮助智能体在复杂的软件工程任务中进行自主探索。与以往的独立探索方法不同，SWE-Debate通过组织智能体之间的辩论，鼓励不同的推理路径，从而更好地定位问题并制定修复计划。实验结果表明，SWE-Debate在开源智能体框架中达到了新的最先进水平，显著优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2508.00782', 'title': 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation', 'url': 'https://huggingface.co/papers/2508.00782', 'abstract': 'SpA2V generates realistic videos aligned with input audio by leveraging spatial auditory cues and integrating them into diffusion models through video scene layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.', 'score': 3, 'issue_id': 5160, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '75fd2b7336810b40', 'authors': ['Kien T. Pham', 'Yingqing He', 'Yazhou Xing', 'Qifeng Chen', 'Long Chen'], 'affiliations': ['Hong Kong University of Science and Technology Clear Water Bay, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00782.jpg', 'data': {'categories': ['#audio', '#games', '#video', '#diffusion', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Звук в движении: от аудио к реалистичному видео', 'desc': 'SpA2V - это новый подход к генерации видео на основе аудио, который использует пространственные звуковые сигналы для создания реалистичных видеороликов. Система работает в два этапа: сначала создается план видеосцены с помощью мультимодальной языковой модели, затем этот план используется для управления диффузионной моделью генерации видео. SpA2V позволяет получать видео с точным семантическим и пространственным соответствием входному аудио. Эксперименты показывают превосходство этого метода над существующими подходами к аудио-видео синтезу.'}, 'en': {'title': 'SpA2V: Bridging Audio and Video with Spatial Awareness', 'desc': 'The paper introduces SpA2V, a novel framework for generating realistic videos that align with input audio by utilizing spatial auditory cues. Unlike previous methods that focus mainly on semantic information, SpA2V incorporates spatial attributes such as location and movement derived from audio properties like loudness and frequency. The generation process is divided into two stages: first, it creates Video Scene Layouts (VSLs) using a modified machine learning model to capture both spatial and semantic cues from the audio. Then, it integrates these VSLs into diffusion models for video generation, resulting in videos that are both semantically and spatially accurate to the audio input.'}, 'zh': {'title': '利用空间听觉线索生成真实视频', 'desc': 'SpA2V是一种生成与输入音频对齐的真实视频的新框架。它通过利用空间听觉线索，将这些线索整合到扩散模型中，从而生成视频场景布局。与传统方法不同，SpA2V不仅关注音频的语义信息，还考虑了声音的空间属性，如位置和运动方向。实验表明，SpA2V在生成与输入音频具有高语义和空间一致性的真实视频方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2508.00454', 'title': 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges', 'url': 'https://huggingface.co/papers/2508.00454', 'abstract': 'An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.', 'score': 3, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '7d97f0b64c1261dd', 'authors': ['Yuqi Tang', 'Kehua Feng', 'Yunfeng Wang', 'Zhiwen Chen', 'Chengfei Lv', 'Gang Yu', 'Qiang Zhang', 'Keyan Ding'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University', 'ZJU-UIUC Institute, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00454.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#inference', '#alignment', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Эффективная оценка диалогов: мудрость многих в одной модели', 'desc': 'Эта статья представляет эффективный метод оценки качества многоэтапных диалогов с использованием больших языковых моделей (LLM). Авторы предлагают агрегировать суждения нескольких LLM в единую модель, что позволяет сохранить преимущества разнообразных оценок, но значительно снизить вычислительные затраты. Метод показал превосходные результаты на семи эталонных наборах данных для оценки диалогов. Предложенный подход обеспечивает быструю и гибкую оценку качества диалогов, сохраняя при этом надежность и согласованность результатов.'}, 'en': {'title': 'Efficient Dialogue Evaluation: Harnessing Collective Wisdom of LLMs', 'desc': 'This paper introduces an efficient multi-turn dialogue evaluator that combines the judgments of multiple large language models (LLMs) to assess dialogue quality. Traditional methods using a single LLM as a judge often face biases that affect evaluation reliability. The proposed method aggregates the preferences of several LLMs into one model, maintaining the benefits of diverse feedback while significantly lowering computational costs. Experiments show that this new approach outperforms existing methods in various evaluation scenarios, proving its effectiveness and efficiency.'}, 'zh': {'title': '高效的多轮对话评估器：聚合智慧，降低成本', 'desc': '本文提出了一种高效的多轮对话评估器，通过将多个大型语言模型（LLM）的判断汇聚成一个单一模型来评估对话质量，从而降低计算成本。当前的评估方法主要依赖于“LLM作为评审”的模式，但这种方法常常受到偏见的影响，导致评估结果的不可靠性。为了解决这个问题，本文的方法利用多个LLM作为评审，并将它们的偏好知识汇聚到一个模型中，从而保留多评审反馈的优势，同时显著减少评估成本。实验结果表明，该方法在多种对话评估基准上优于现有的基线，展示了其高效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2507.19634', 'title': 'MCIF: Multimodal Crosslingual Instruction-Following Benchmark from\n  Scientific Talks', 'url': 'https://huggingface.co/papers/2507.19634', 'abstract': "MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.", 'score': 3, 'issue_id': 5169, 'pub_date': '2025-07-25', 'pub_date_card': {'ru': '25 июля', 'en': 'July 25', 'zh': '7月25日'}, 'hash': '6c681493be72e8eb', 'authors': ['Sara Papi', 'Maike Züfle', 'Marco Gaido', 'Beatrice Savoldi', 'Danni Liu', 'Ioannis Douros', 'Luisa Bentivogli', 'Jan Niehues'], 'affiliations': ['Fondazione Bruno Kessler (Italy)', 'Karlsruhe Institute of Technology (Germany)', 'Translated (Italy)'], 'pdf_title_img': 'assets/pdf/title_img/2507.19634.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#multilingual', '#open_source', '#long_context', '#machine_translation'], 'emoji': '🌐', 'ru': {'title': 'MCIF: Первый многоязычный мультимодальный тест для оценки MLLM', 'desc': 'MCIF - это многоязычный эталонный тест с аннотациями от людей для оценки выполнения инструкций в кросс-языковых мультимодальных средах, использующий научные доклады. Он охватывает три основные модальности - речь, зрение и текст - на четырех языках, позволяя комплексно оценивать способности мультимодальных языковых моделей (MLLM) интерпретировать инструкции на разных языках и комбинировать их с мультимодальной контекстной информацией. MCIF создан для преодоления ограничений существующих тестов, которые часто ограничены английским языком, фокусируются на одной модальности и коротких контекстах. Этот бенчмарк выпущен под лицензией CC-BY 4.0 для поощрения открытых исследований и прогресса в разработке MLLM.'}, 'en': {'title': 'MCIF: A New Benchmark for Multilingual Instruction-Following in Multimodal AI', 'desc': 'MCIF is a new benchmark designed to evaluate how well multilingual, multimodal language models can follow instructions in different languages and formats. It focuses on three modalities: speech, vision, and text, and includes human annotations to ensure quality assessments. Unlike previous benchmarks, MCIF allows for both short and long context evaluations across four languages: English, German, Italian, and Chinese. This comprehensive approach aims to enhance the understanding of model performance in real-world, crosslingual scenarios.'}, 'zh': {'title': 'MCIF：跨语言多模态指令跟随的评估新基准', 'desc': 'MCIF是一个多语言的人类标注基准，用于评估跨语言、多模态环境下的指令跟随能力。它结合了文本、语音和视觉三种核心模态，并支持英语、德语、意大利语和中文四种语言。MCIF的设计旨在填补现有基准在多语言和多模态评估方面的不足，特别是在长短文本输入的情况下。通过MCIF，研究人员可以更全面地评估多模态大语言模型的性能和能力。'}}}, {'id': 'https://huggingface.co/papers/2508.00632', 'title': 'Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings', 'url': 'https://huggingface.co/papers/2508.00632', 'abstract': 'A multi-agent system using an omni-modal evaluation metric improves JavaScript game and animation generation but struggles with custom assets and audio-visual feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.', 'score': 2, 'issue_id': 5165, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '3e4a605a070fb44f', 'authors': ['Alexia Jolicoeur-Martineau'], 'affiliations': ['Samsung SAIL Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2508.00632.jpg', 'data': {'categories': ['#audio', '#multimodal', '#games', '#optimization', '#video', '#agents'], 'emoji': '🎮', 'ru': {'title': 'Мультиагентная система для создания игр: прогресс и проблемы', 'desc': 'Представлена система мультиагентного генерирования JavaScript-игр и анимаций с использованием омнимодальной метрики оценки. Система AVR-Agent выбирает релевантные ассеты, генерирует несколько вариантов кода и итеративно улучшает их на основе обратной связи. Метрика AVR-Eval сравнивает качество мультимедийного контента, используя аудиовизуальные записи. Эксперименты показали, что система значительно улучшает генерацию контента по сравнению с одноразовой генерацией, но модели все еще испытывают трудности с эффективным использованием пользовательских ассетов и аудиовизуальной обратной связи.'}, 'en': {'title': 'Enhancing Game Generation with Multi-Agent Systems and AVR-Eval', 'desc': 'This paper presents a multi-agent system designed to enhance the generation of JavaScript games and animations using a new evaluation metric called AVR-Eval. AVR-Eval assesses multimedia content quality by comparing Audio-Visual Recordings (AVRs) through an omni-modal model that processes text, video, and audio. The system, AVR-Agent, generates code by selecting relevant multimedia assets and iteratively improving the output based on feedback from AVR-Eval. Despite achieving higher success rates in generated content, the system struggles with custom assets and effective audio-visual feedback, indicating a gap between human creativity and machine-generated content.'}, 'zh': {'title': '多智能体系统提升JavaScript游戏生成质量', 'desc': '本论文提出了一种多智能体系统，利用全模态评估指标来改善JavaScript游戏和动画的生成。我们开发了AVR-Eval，这是一种相对评估多媒体内容质量的新指标，能够有效区分优质和劣质内容。AVR-Agent是一个多智能体系统，能够从多媒体资产库中生成JavaScript代码，并通过迭代反馈不断优化生成的内容。尽管生成的内容在胜率上优于单次生成的内容，但模型在利用自定义资产和音视频反馈方面仍然存在困难，显示出人类与机器内容创作方法之间的根本差异。'}}}, {'id': 'https://huggingface.co/papers/2507.22720', 'title': 'Investigating Hallucination in Conversations for Low Resource Languages', 'url': 'https://huggingface.co/papers/2507.22720', 'abstract': "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.", 'score': 2, 'issue_id': 5155, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': 'c7f5db5f58895f4f', 'authors': ['Amit Das', 'Md. Najib Hasan', 'Souvika Sarkar', 'Zheng Zhang', 'Fatemeh Jamshidi', 'Tathagata Bhattacharya', 'Nilanjana Raychawdhury', 'Dongji Feng', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'Auburn University', 'Auburn University at Montgomery', 'California State Polytechnic University Pomona', 'Gustavus Adolphus College', 'Meta', 'Murray State University', 'Stanford University', 'University of North Alabama', 'Wichita State University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22720.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#hallucinations'], 'emoji': '🗣️', 'ru': {'title': 'Языковые модели меньше галлюцинируют по-китайски', 'desc': 'Исследование посвящено проблеме галлюцинаций в больших языковых моделях (LLM) на примере трех языков: хинди, фарси и мандаринского китайского. Авторы провели комплексный анализ фактических и лингвистических ошибок в этих языках для нескольких популярных моделей, включая GPT-3.5, GPT-4, Llama-3.1 и другие. Результаты показали, что LLM генерируют значительно меньше галлюцинаций на мандаринском китайском по сравнению с хинди и фарси. Это исследование расширяет понимание проблемы галлюцинаций за пределы английского языка, что важно для повышения надежности и эффективности LLM.'}, 'en': {'title': 'Mandarin LLMs: Fewer Hallucinations, More Accuracy!', 'desc': 'This paper investigates the phenomenon of hallucinations in Large Language Models (LLMs) across three languages: Mandarin, Hindi, and Farsi. Hallucinations refer to instances where the models generate incorrect or misleading information. The study analyzes conversational data from various LLMs, including GPT-3.5 and GPT-4o, to compare the frequency of these errors. The findings reveal that LLMs exhibit fewer hallucinations in Mandarin compared to the higher rates observed in Hindi and Farsi, highlighting the need for language-specific improvements in model training.'}, 'zh': {'title': '普通话中的幻觉现象较少', 'desc': '大型语言模型（LLMs）在生成文本方面表现出色，但它们有时会产生不准确的信息，这被称为“幻觉”。本研究探讨了在普通话、印地语和法尔西语中，LLMs的幻觉现象。我们分析了多个模型（如GPT-3.5、GPT-4o等）在这三种语言中的事实和语言错误。结果显示，LLMs在普通话中产生的幻觉响应较少，而在印地语和法尔西语中则显著更多。'}}}, {'id': 'https://huggingface.co/papers/2508.00823', 'title': 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation', 'url': 'https://huggingface.co/papers/2508.00823', 'abstract': 'IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.', 'score': 1, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': 'a4651adceaac80f7', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Hang Yin', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00823.jpg', 'data': {'categories': ['#robotics', '#3d', '#optimization', '#agents', '#games'], 'emoji': '🧭', 'ru': {'title': 'Навигация в 3D с помощью инкрементальных гауссианов', 'desc': 'IGL-Nav - это новый метод навигации по изображению-цели в трехмерном пространстве. Он использует инкрементальное представление 3D гауссианов для эффективной и точной локализации целевого изображения. Метод превосходит существующие подходы, сочетая дискретное сопоставление пространства и оптимизацию через дифференцируемый рендеринг. IGL-Nav применим в реальных условиях и может работать с произвольными ракурсами целевых изображений.'}, 'en': {'title': 'Efficient 3D Navigation with Incremental Gaussian Localization', 'desc': 'IGL-Nav introduces an innovative approach to image-goal navigation in 3D environments using an incremental 3D Gaussian representation. This method enhances localization accuracy by updating the scene representation as new images are processed, allowing for efficient navigation. Unlike traditional methods that struggle with geometric relationships, IGL-Nav utilizes geometric information for effective discrete space matching and fine target pose optimization. The framework demonstrates significant improvements over existing techniques and is suitable for real-world applications, including robotic platforms.'}, 'zh': {'title': '增量式3D高斯导航：高效准确的图像目标定位', 'desc': 'IGL-Nav是一种增量式3D高斯定位框架，旨在提高图像目标导航的效率和准确性。该方法通过可渲染的3D高斯表示来建模3D环境与目标图像之间的几何关系，克服了传统方法的局限性。IGL-Nav通过前馈单目预测逐步更新场景表示，并利用几何信息进行粗略定位，最终通过可微渲染优化精确确定目标位置。实验结果表明，IGL-Nav在多种配置下显著超越了现有的最先进方法，并能够在真实世界的机器人平台上应用。'}}}, {'id': 'https://huggingface.co/papers/2508.02193', 'title': 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference', 'url': 'https://huggingface.co/papers/2508.02193', 'abstract': 'Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.', 'score': 61, 'issue_id': 5199, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'bec183ec45598da2', 'authors': ['Yuxuan Song', 'Zheng Zhang', 'Cheng Luo', 'Pengyang Gao', 'Fan Xia', 'Hao Luo', 'Zheng Li', 'Yuehang Yang', 'Hongli Yu', 'Xingwei Qu', 'Yuwei Fu', 'Jing Su', 'Ge Zhang', 'Wenhao Huang', 'Mingxuan Wang', 'Lin Yan', 'Xiaoying Jia', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Yonghui Wu', 'Hao Zhou'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2508.02193.jpg', 'data': {'categories': ['#diffusion', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Революция в скорости генерации текста без потери качества', 'desc': 'Seed Diffusion Preview - это новая языковая модель, основанная на дискретной диффузии. Она обеспечивает очень быструю генерацию текста благодаря параллельному неупорядоченному декодированию. Модель достигает скорости вывода 2146 токенов в секунду на GPU H20, значительно превосходя аналоги Mercury и Gemini Diffusion. При этом Seed Diffusion Preview сохраняет конкурентоспособное качество на стандартных бенчмарках для оценки кода.'}, 'en': {'title': 'Speed Meets Quality: The Future of Code Generation', 'desc': 'Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion.'}, 'zh': {'title': '种子扩散预览：速度与质量的新标杆', 'desc': 'Seed Diffusion Preview是一种基于离散状态扩散的语言模型，具有极快的推理速度。通过非顺序的并行生成，离散扩散模型显著提高了推理效率，减少了逐个解码的延迟。该模型在H20 GPU上实现了每秒2,146个token的推理速度，同时在标准代码评估基准上保持了竞争力的性能。与当前的Mercury和Gemini Diffusion相比，Seed Diffusion Preview在速度和质量上都设立了新的标杆。'}}}, {'id': 'https://huggingface.co/papers/2508.03320', 'title': 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation', 'url': 'https://huggingface.co/papers/2508.03320', 'abstract': 'Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.', 'score': 42, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '71dc78f7c773cefd', 'authors': ['Peiyu Wang', 'Yi Peng', 'Yimeng Gan', 'Liang Hu', 'Tianyidan Xie', 'Xiaokun Wang', 'Yichen Wei', 'Chuanxin Tang', 'Bo Zhu', 'Changshi Li', 'Hongyang Wei', 'Eric Li', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Multimodality Team, Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03320.jpg', 'data': {'categories': ['#dataset', '#training', '#multimodal', '#architecture', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'Единая модель для понимания, создания и редактирования изображений', 'desc': 'Skywork UniPic - это авторегрессионная модель с 1,5 миллиардами параметров, объединяющая понимание изображений, генерацию изображений по тексту и редактирование изображений в единой архитектуре. Модель достигает высоких результатов на различных бенчмарках, включая GenEval, DPG-Bench и GEditBench-EN. Skywork UniPic использует раздельную стратегию кодирования и прогрессивное обучение с увеличением разрешения. Модель демонстрирует, что высококачественная мультимодальная интеграция возможна без чрезмерных вычислительных затрат.'}, 'en': {'title': 'Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic', 'desc': 'Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources.'}, 'zh': {'title': 'Skywork UniPic：统一多模态AI的高效解决方案', 'desc': 'Skywork UniPic是一个拥有15亿参数的自回归模型，能够统一图像理解、文本到图像生成和图像编辑。该模型通过一个单一架构消除了对特定任务适配器或模块连接器的需求，展示了紧凑的多模态系统在普通硬件上也能达到最先进的性能。Skywork UniPic在多个基准测试中表现优异，尤其是在图像生成和编辑方面，显示出其高效的训练策略和数据集设计。该模型为高保真多模态AI的实际应用提供了新的范式，且代码和权重已公开。'}}}, {'id': 'https://huggingface.co/papers/2508.03694', 'title': 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation', 'url': 'https://huggingface.co/papers/2508.03694', 'abstract': 'LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.', 'score': 32, 'issue_id': 5198, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '8c05bd06521b3fb7', 'authors': ['Jianxiong Gao', 'Zhaoxi Chen', 'Xian Liu', 'Jianfeng Feng', 'Chenyang Si', 'Yanwei Fu', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'NVIDIA', 'Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03694.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#multimodal', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'LongVie: прорыв в генерации сверхдлинных видео с сохранением качества', 'desc': 'LongVie - это новая автореградная модель для генерации сверхдлинных видео. Она решает проблемы временной согласованности и визуальной деградации с помощью унифицированной инициализации шума и глобальной нормализации управляющих сигналов. LongVie использует мультимодальное управление, объединяя плотные и разреженные сигналы. Модель также применяет стратегию обучения с учетом деградации для сохранения визуального качества.'}, 'en': {'title': 'LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality', 'desc': 'LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing.'}, 'zh': {'title': '超长视频生成的新突破：LongVie', 'desc': 'LongVie是一个端到端的自回归框架，旨在解决超长视频生成中的时间一致性和视觉退化问题。它通过统一的噪声初始化、全局控制信号归一化、多模态控制和退化感知训练来实现这些目标。LongVie的核心设计确保了时间一致性，并通过多模态控制框架来减轻视觉退化。实验结果表明，LongVie在长时间可控性、一致性和质量方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.03686', 'title': 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward', 'url': 'https://huggingface.co/papers/2508.03686', 'abstract': 'CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.', 'score': 22, 'issue_id': 5201, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'dddc5da46c921b94', 'authors': ['Shudong Liu', 'Hongwei Liu', 'Junnan Liu', 'Linchen Xiao', 'Songyang Gao', 'Chengqi Lyu', 'Yuzhe Gu', 'Wenwei Zhang', 'Derek F. Wong', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['NLP2CT Lab', 'Shanghai AI Laboratory', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2508.03686.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#benchmark', '#dataset', '#interpretability', '#optimization'], 'emoji': '🧭', 'ru': {'title': 'CompassVerifier: Надежная проверка ответов LLM во многих областях', 'desc': 'CompassVerifier - это легковесная модель для проверки выходных данных больших языковых моделей (LLM) в различных областях. Она поддерживается VerifierBench - комплексным набором данных для оценки. CompassVerifier демонстрирует компетентность в различных областях, включая математику, знания и разнообразные задачи на рассуждение. Модель способна обрабатывать различные типы ответов, включая многозадачные проблемы, формулы и последовательные ответы, эффективно идентифицируя аномальные и недействительные ответы.'}, 'en': {'title': 'Revolutionizing LLM Output Verification with CompassVerifier', 'desc': 'CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses.'}, 'zh': {'title': 'CompassVerifier：多领域答案验证的轻量级解决方案', 'desc': 'CompassVerifier 是一种轻量级且稳健的模型，用于验证大型语言模型（LLM）在不同领域的输出。它通过 VerifierBench 这一全面的基准数据集来支持验证过程。该模型能够处理多种类型的答案，包括多子问题、公式和序列答案，并有效识别异常或无效的响应。我们希望 CompassVerifier 和 VerifierBench 能够促进答案验证、评估协议和强化学习研究。'}}}, {'id': 'https://huggingface.co/papers/2508.03012', 'title': 'Tool-integrated Reinforcement Learning for Repo Deep Search', 'url': 'https://huggingface.co/papers/2508.03012', 'abstract': "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.", 'score': 9, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '4ab74a355fed1d76', 'authors': ['Zexiong Ma', 'Chao Peng', 'Qunhong Zeng', 'Pengfei Gao', 'Yanzhen Zou', 'Bing Xie'], 'affiliations': ['Beijing Institute of Technology', 'ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03012.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#agents', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ToolTrain: Эффективная локализация проблем в коде с помощью обученных языковых моделей', 'desc': 'ToolTrain - это двухэтапная система обучения, объединяющая контролируемую тонкую настройку и обучение с подкреплением для улучшения работы больших языковых моделей в задаче локализации проблем в программном коде. Система интегрирует инструменты поиска по репозиторию, что позволяет преодолеть семантический разрыв между описанием проблемы на естественном языке и проблемным кодом. Экспериментальные результаты показывают, что модели, обученные с помощью ToolTrain, достигают наилучших показателей в этой задаче. Улучшенная производительность в локализации проблем также приводит к лучшим результатам в полном цикле разрешения проблем в программном обеспечении.'}, 'en': {'title': 'ToolTrain: Enhancing LLMs for Superior Issue Localization', 'desc': "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."}, 'zh': {'title': 'ToolTrain：提升问题定位的智能工具训练框架', 'desc': 'ToolTrain是一种两阶段的训练框架，结合了监督微调和强化学习，旨在提升大型语言模型（LLMs）在问题定位方面的能力。问题定位是识别需要修改的代码位置以解决软件问题的过程，但由于自然语言描述与故障代码之间的语义差距，这一任务非常具有挑战性。ToolTrain通过整合代码库检索工具，帮助LLMs在多步骤推理和导航过程中有效利用这些工具，从而实现了最先进的性能。实验结果表明，ToolTrain训练的模型在功能级定位上超越了Claude-3.7，证明了针对问题定位的训练策略在自动化软件开发中是有效的。'}}}, {'id': 'https://huggingface.co/papers/2508.02091', 'title': 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search', 'url': 'https://huggingface.co/papers/2508.02091', 'abstract': "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", 'score': 7, 'issue_id': 5201, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '10eb53caada711ad', 'authors': ['Xiaoya Li', 'Xiaofei Sun', 'Albert Wang', 'Chris Shum', 'Jiwei Li'], 'affiliations': ['DeepReinforce Team', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.02091.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rag', '#rl', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'CRINN: Революция в поиске ближайших соседей с помощью ИИ', 'desc': 'CRINN - это новый подход к оптимизации алгоритмов поиска приближенных ближайших соседей (ANNS), использующий обучение с подкреплением. Он автоматически генерирует все более быстрые реализации ANNS, сохраняя при этом точность. CRINN превзошел современные методы на нескольких эталонных тестах, включая GIST-960-Euclidean и MNIST-784-Euclidean. Успех CRINN показывает, что языковые модели, дополненные обучением с подкреплением, могут эффективно автоматизировать сложные алгоритмические оптимизации.'}, 'en': {'title': 'CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning', 'desc': 'CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations.'}, 'zh': {'title': 'CRINN：用强化学习加速近似最近邻搜索', 'desc': 'CRINN是一种基于强化学习的方法，旨在优化近似最近邻搜索算法的速度，同时保持准确性。该方法将近似最近邻搜索的优化视为一个强化学习问题，以执行速度作为奖励信号。通过这种方式，CRINN能够自动生成逐渐更快的近似最近邻搜索实现，并满足准确性约束。实验结果表明，CRINN在多个基准数据集上表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.00367', 'title': 'Representation Shift: Unifying Token Compression with FlashAttention', 'url': 'https://huggingface.co/papers/2508.00367', 'abstract': "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.", 'score': 7, 'issue_id': 5209, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '5a4ad3025ab24bd1', 'authors': ['Joonmyung Choi', 'Sanghyeok Lee', 'Byungoh Ko', 'Eunseo Kim', 'Jihyung Kil', 'Hyunwoo J. Kim'], 'affiliations': ['Adobe Research', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00367.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#optimization', '#video'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обработки видео и текста без потери качества', 'desc': 'Статья представляет метрику Representation Shift, которая позволяет эффективно сжимать токены в трансформерах без переобучения модели. Эта метрика совместима с FlashAttention, что значительно ускоряет обработку видео и текста. Representation Shift измеряет степень изменения представления каждого токена, что позволяет определить наиболее важные из них. Метод применим не только к трансформерам, но и к CNN и моделям пространства состояний.'}, 'en': {'title': 'Boosting Video Retrieval Efficiency with Representation Shift', 'desc': "This paper introduces Representation Shift, a new metric that helps improve the efficiency of video-text retrieval and video question answering without needing to retrain models. It combines token compression techniques with FlashAttention, which reduces memory usage and speeds up processing. By measuring how much each token's representation changes, it allows for effective token selection without relying on attention maps. The method works not only with Transformers but also with other model types like CNNs, achieving notable performance improvements."}, 'zh': {'title': 'Representation Shift：加速视频检索与问答的创新方法', 'desc': 'Representation Shift是一种无训练、模型无关的度量方法，它将令牌压缩与FlashAttention结合，显著加快视频-文本检索和视频问答的速度。随着任务复杂性的增加，模型和令牌的规模也在扩大，导致自注意力的计算成本呈平方增长。我们的方法通过测量每个令牌表示的变化程度，来实现有效的令牌压缩，而无需构建注意力图或重新训练。实验结果表明，Representation Shift在视频-文本检索和视频问答中分别实现了高达5.5%和4.4%的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2508.03050', 'title': 'Multi-human Interactive Talking Dataset', 'url': 'https://huggingface.co/papers/2508.03050', 'abstract': 'MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.', 'score': 6, 'issue_id': 5200, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '01ba126a166568d6', 'authors': ['Zeyu Zhu', 'Weijia Wu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.03050.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset'], 'emoji': '🗣️', 'ru': {'title': 'Новый датасет и модель для генерации видео с разговорами нескольких людей', 'desc': 'Исследователи представили MIT - крупномасштабный набор данных для генерации видео с разговорами нескольких людей. Этот датасет включает детальные аннотации и используется для демонстрации CovOG - базовой модели, объединяющей кодировщик поз нескольких людей и интерактивный аудиодрайвер. MIT содержит 12 часов видео высокого разрешения с 2-4 говорящими и детальными аннотациями поз тела и речевых взаимодействий. CovOG использует агрегацию индивидуальных эмбеддингов поз и модуляцию динамики головы на основе аудиопризнаков каждого говорящего.'}, 'en': {'title': 'MIT: Pioneering Multi-Human Talking Video Generation', 'desc': 'The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area.'}, 'zh': {'title': 'MIT：多人人对话视频生成的新基准', 'desc': 'MIT是一个大规模的数据集，专门用于多人的对话视频生成，包含细致的注释信息。现有的研究主要集中在单人独白或孤立的面部动画上，限制了其在真实多人的互动中的应用。我们开发了一个自动化流程，收集和注释多人的对话视频，数据集包含12小时的高分辨率视频，展示了自然的对话动态。为了展示MIT的潜力，我们提出了CovOG模型，结合了多人的姿态编码器和互动音频驱动器，展示了生成真实多人的对话视频的可行性和挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.01119', 'title': 'The Promise of RL for Autoregressive Image Editing', 'url': 'https://huggingface.co/papers/2508.01119', 'abstract': 'Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.', 'score': 6, 'issue_id': 5215, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': 'b7e0974935b60296', 'authors': ['Saba Ahmadi', 'Rabiul Awal', 'Ankur Sikarwar', 'Amirhossein Kazemnejad', 'Ge Ya Luo', 'Juan A. Rodriguez', 'Sai Rajeswar', 'Siva Reddy', 'Christopher Pal', 'Benno Krojer', 'Aishwarya Agrawal'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute', 'Polytechnique Montréal', 'ServiceNow', 'Université de Montréal', 'École de Technologie Supérieure (ETS)'], 'pdf_title_img': 'assets/pdf/title_img/2508.01119.jpg', 'data': {'categories': ['#rl', '#optimization', '#multimodal', '#open_source', '#training', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Обучение с подкреплением повышает качество редактирования изображений', 'desc': 'В статье исследуются стратегии улучшения редактирования изображений с использованием авторегрессионной мультимодальной модели. Авторы сравнивают три подхода: обучение с учителем, обучение с подкреплением и рассуждения по цепочке мыслей. Наиболее эффективным оказалось обучение с подкреплением в сочетании с большой мультимодальной языковой моделью-верификатором. В результате была разработана модель EARL, показывающая конкурентоспособные результаты на различных задачах редактирования изображений.'}, 'en': {'title': 'Reinforcement Learning Meets Multimodal Image Editing', 'desc': 'This paper presents a novel approach to image editing by integrating reinforcement learning (RL) with a large multimodal language model (LLM) verifier within an autoregressive framework. The authors investigate three strategies: supervised fine-tuning, RL, and Chain-of-Thought reasoning, to improve image editing tasks. They demonstrate that the combination of RL and a multimodal LLM verifier significantly enhances performance, leading to the development of EARL, a robust RL-based image editing model. EARL achieves competitive results on various editing tasks while requiring less training data compared to existing methods, marking a significant advancement in autoregressive multimodal models for image editing.'}, 'zh': {'title': '强化学习与多模态模型结合，提升图像编辑性能', 'desc': '本论文探讨了如何通过结合强化学习和大型多模态语言模型验证器来提升图像编辑性能。我们提出了三种策略：监督微调、强化学习和链式思维推理，并在一个自回归多模态框架中进行研究。实验结果表明，强化学习与大型多模态语言模型验证器的结合是最有效的策略。最终，我们发布了EARL模型，它在多种图像编辑任务中表现出色，且训练数据需求较少。'}}}, {'id': 'https://huggingface.co/papers/2508.03613', 'title': 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction', 'url': 'https://huggingface.co/papers/2508.03613', 'abstract': "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.", 'score': 5, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '50044cd9b7eb1802', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Ziran Yang', 'Jui-Hui Chung', 'Haoyu Zhao', 'Lai Jiang', 'Yihan Geng', 'Jiawei Ge', 'Jingruo Sun', 'Jiayun Wu', 'Jiri Gesi', 'Ximing Lu', 'David Acuna', 'Kaiyu Yang', 'Hongzhou Lin', 'Yejin Choi', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'NVIDIA', 'Peking University', 'Princeton Language and Intelligence, Princeton University', 'Shanghai Jiao Tong University', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03613.jpg', 'data': {'categories': ['#rl', '#dataset', '#synthetic', '#reasoning', '#small_models', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Маленькая модель - большие доказательства: Goedel-Prover-V2 переворачивает мир автоматического доказательства теорем', 'desc': 'Goedel-Prover-V2 - это серия открытых языковых моделей для автоматического доказательства теорем. Модель использует синтез структурированных данных, самокоррекцию на основе верификатора и усреднение моделей для достижения наилучших результатов. Несмотря на небольшой размер, Goedel-Prover-V2-8B превосходит гораздо более крупные модели на бенчмарках MiniF2F и PutnamBench. На момент выпуска Goedel-Prover-V2 демонстрирует лучшую производительность среди открытых систем автоматического доказательства теорем.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Goedel-Prover-V2', 'desc': 'Goedel-Prover-V2 is a series of open-source language models that excel in automated theorem proving. It introduces innovative techniques such as scaffolded data synthesis for training on progressively complex tasks, verifier-guided self-correction for iterative proof refinement, and model averaging to enhance output diversity. The models achieve impressive performance metrics, with the smaller Goedel-Prover-V2-8B outperforming larger models like DeepSeek-Prover-V2-671B. Overall, Goedel-Prover-V2 sets a new benchmark in the field, demonstrating superior capabilities in solving mathematical problems with a reduced computational footprint.'}, 'zh': {'title': 'Goedel-Prover-V2：自动定理证明的新标杆', 'desc': 'Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域达到了最先进的性能。该模型通过三项创新技术实现了这一目标：首先，使用支架数据合成生成逐渐增加难度的合成任务，以帮助模型掌握复杂的定理；其次，采用验证器引导的自我修正，使模型能够根据Lean编译器的反馈迭代修正其证明；最后，通过模型平均技术合并模型检查点，以减少训练后期模型输出多样性的下降。Goedel-Prover-V2-32B模型在MiniF2F上达到了88.1%的通过率，显著超越了之前的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.01780', 'title': 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?', 'url': 'https://huggingface.co/papers/2508.01780', 'abstract': "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.", 'score': 5, 'issue_id': 5199, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'b9c09b0ce4e2dad3', 'authors': ['Guozhao Mo', 'Wenliang Zhong', 'Jiawei Chen', 'Xuanang Chen', 'Yaojie Lu', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.01780.jpg', 'data': {'categories': ['#open_source', '#survey', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LiveMCPBench: Новый стандарт оценки LLM-агентов в реальных MCP-средах', 'desc': 'LiveMCPBench представляет собой комплексный бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в реальных задачах экосистемы MCP. Он включает 95 задач и использует масштабируемый конвейер оценки с адаптивной системой судейства. Бенчмарк содержит LiveMCPTool - набор из 70 MCP-серверов и 527 инструментов, а также LiveMCPEval - фреймворк для автоматизированной оценки с использованием LLM в качестве судьи. Результаты тестирования 10 ведущих моделей показали, что лучшая модель (Claude-Sonnet-4) достигла 78.95% успешности выполнения задач.'}, 'en': {'title': 'Revolutionizing LLM Evaluation in Dynamic MCP Environments', 'desc': 'LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments.'}, 'zh': {'title': '全面评估LLM代理的基准测试平台', 'desc': 'LiveMCPBench是一个全面的基准测试平台，旨在评估大型语言模型（LLM）代理在多样化的真实世界任务中的表现。它解决了现有基准测试仅限于单一服务器设置的问题，提供了95个基于模型上下文协议（MCP）生态系统的真实任务。通过LiveMCPTool，研究人员可以使用70个MCP服务器和527个工具，支持可扩展和可重复的评估流程。此外，LiveMCPEval框架实现了自动化和自适应评估，确保在动态任务环境中与人类评审者的高一致性。'}}}, {'id': 'https://huggingface.co/papers/2508.00477', 'title': 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer', 'url': 'https://huggingface.co/papers/2508.00477', 'abstract': "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.  \t\t\t\t\tAI-generated summary \t\t\t\t In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.", 'score': 4, 'issue_id': 5202, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '94d96ba2b9f92b31', 'authors': ['Yuzhuo Chen', 'Zehua Ma', 'Jianhua Wang', 'Kai Kang', 'Shunyu Yao', 'Weiming Zhang'], 'affiliations': ['East China Normal University', 'Onestory Team', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.00477.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#optimization', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'LAMIC: Революция в синтезе изображений с несколькими референсами', 'desc': 'LAMIC - это фреймворк для композиции нескольких изображений с учетом макета, который расширяет возможности диффузионных моделей с одним референсом на сценарии с несколькими референсами. Он использует два механизма внимания: Group Isolation Attention для улучшения разделения сущностей и Region-Modulated Attention для генерации с учетом макета. LAMIC достигает наилучших результатов по большинству метрик без дополнительного обучения, демонстрируя превосходные способности в сохранении идентичности, фона и контроле макета. Этот подход устанавливает новую парадигму для контролируемой композиции нескольких изображений без обучения.'}, 'en': {'title': 'LAMIC: Revolutionizing Multi-Image Synthesis Without Training', 'desc': 'LAMIC is a new framework designed for creating images from multiple references while considering their layout. It builds on existing single-reference diffusion models and introduces attention mechanisms to improve how different elements in the images are handled. This framework allows for high-quality image synthesis without the need for additional training, demonstrating strong performance in maintaining identity, background consistency, and layout control. LAMIC sets a new standard in controllable image composition by effectively managing multiple images in a training-free manner.'}, 'zh': {'title': 'LAMIC：无训练的多图像合成新范式', 'desc': 'LAMIC是一个布局感知的多图像合成框架，它将单参考扩散模型扩展到多参考场景，且无需训练。该框架引入了两种注意力机制：群体隔离注意力（GIA）和区域调制注意力（RMA），以增强实体分离和布局感知生成。通过引入三种评估指标，LAMIC在身份保持、背景一致性和布局控制等方面表现出色，超越了现有的多参考基线。LAMIC展示了强大的零样本泛化能力，为可控的多图像合成建立了新的无训练范式。'}}}, {'id': 'https://huggingface.co/papers/2508.03164', 'title': 'ChartCap: Mitigating Hallucination of Dense Chart Captioning', 'url': 'https://huggingface.co/papers/2508.03164', 'abstract': 'ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.', 'score': 3, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'aec1f860dbfa8231', 'authors': ['Junyoung Lim', 'Jaewoo Ahn', 'Gunhee Kim'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03164.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#dataset', '#data', '#open_source'], 'emoji': '📊', 'ru': {'title': 'ChartCap: Точные подписи к графикам без галлюцинаций', 'desc': 'ChartCap - это масштабный набор данных, содержащий 565 тысяч реальных графиков с детальными подписями. Датасет разработан для улучшения точности генерации подписей и уменьшения галлюцинаций в мультимодальных языковых моделях. ChartCap использует четырехэтапный конвейер для создания подписей, основанных только на видимых данных графика, и применяет верификацию на основе циклической согласованности. Авторы также предлагают новую метрику - Visual Consistency Score, для оценки качества подписей без опоры на эталонные подписи.'}, 'en': {'title': 'ChartCap: Elevating Chart Captioning Accuracy and Reducing Hallucinations', 'desc': 'ChartCap is a new dataset designed to enhance the performance of vision language models in generating captions for charts. It contains 565,000 real-world chart images with detailed, type-specific captions that focus on essential structural elements and insights, avoiding irrelevant information. The dataset is created through a four-stage process that ensures high-quality captions, verified by a cycle consistency method for efficient quality control. Experiments show that models trained on ChartCap produce more accurate and informative captions, with fewer hallucinations compared to existing models and even human-generated captions.'}, 'zh': {'title': 'ChartCap：提升图表说明准确性的关键数据集', 'desc': 'ChartCap是一个大规模的数据集，包含565K个真实世界图表图像及其特定类型的详细说明。该数据集旨在提高视觉语言模型的说明准确性，并减少虚假信息的生成。通过设计四阶段的生成管道，ChartCap确保说明仅基于图表中可辨别的数据，并通过循环一致性的人类验证加速质量控制。实验结果表明，基于ChartCap微调的模型在生成准确和信息丰富的说明方面表现优于其他开源和专有模型，甚至超过人类标注的说明。'}}}, {'id': 'https://huggingface.co/papers/2508.02629', 'title': 'HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents', 'url': 'https://huggingface.co/papers/2508.02629', 'abstract': 'HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.', 'score': 3, 'issue_id': 5212, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '5b4c4a212eae58d6', 'authors': ['Yibin Liu', 'Zhixuan Liang', 'Zanxin Chen', 'Tianxing Chen', 'Mengkang Hu', 'Wanxi Dong', 'Congsheng Xu', 'Zhaoming Han', 'Yusen Qin', 'Yao Mu'], 'affiliations': ['D-Robotics', 'HKU MMLab', 'NEU', 'SJTU ScaleLab', 'SUSTech', 'SZU', 'Shanghai AI Lab', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2508.02629.jpg', 'data': {'categories': ['#robotics', '#optimization', '#agents', '#reasoning', '#games', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самокорректирующиеся программы для роботов на основе мультимодального ИИ', 'desc': 'HyCodePolicy - это гибридная система управления для воплощенных агентов, использующая мультимодальные языковые модели. Она объединяет синтез кода, геометрическое обоснование, перцептивный мониторинг и итеративное исправление в замкнутый цикл программирования. Система генерирует исполняемую программу на основе естественно-языковых инструкций, затем выполняет ее в симуляции, отслеживая ошибки с помощью визуально-языковой модели. HyCodePolicy способна автономно исправлять программы, значительно повышая надежность и эффективность политик для манипуляций роботов.'}, 'en': {'title': 'Empowering Robots with Self-Correcting Code Synthesis', 'desc': 'HyCodePolicy is a novel framework that enhances the performance of embodied agents by integrating several advanced techniques. It combines code synthesis, geometric grounding, perceptual monitoring, and iterative repair to create a robust programming cycle. The system translates natural language instructions into executable programs, monitors their execution, and identifies failures using a vision-language model. This approach allows for self-correcting capabilities in program synthesis, improving both the efficiency and reliability of robotic manipulation tasks.'}, 'zh': {'title': '自我修正的智能体编程策略', 'desc': 'HyCodePolicy 是一种混合语言控制框架，旨在增强具身智能体策略的鲁棒性和效率。该系统通过将代码合成、几何基础、感知监控和迭代修复整合到一个闭环编程周期中，来实现自我修正的程序合成。它首先将自然语言指令分解为子目标，并生成基于对象中心几何原语的可执行程序。通过视觉语言模型监控执行过程，HyCodePolicy 能够检测执行失败并进行修复，从而提高机器人操作策略的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.02079', 'title': 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization', 'url': 'https://huggingface.co/papers/2508.02079', 'abstract': 'AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.', 'score': 2, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '25a555fc0f91f562', 'authors': ['Amitava Das', 'Abhilekh Borah', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon AI, USA', 'BITS Goa, India', 'Manipal University, India', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.02079.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Сохранение безопасности при дообучении языковых моделей', 'desc': 'AlignGuard-LoRA (AGL) - это фреймворк для сохранения выравнивания при дообучении больших языковых моделей. Он вводит методы регуляризации и диагностический бенчмарк для снижения дрейфа выравнивания. AGL включает несколько ключевых компонентов, таких как регуляризация на основе матрицы Фишера и регуляризация с учетом коллизий. Эмпирические оценки показывают, что AGL снижает дрейф выравнивания до 50% на критически важных для безопасности бенчмарках без ухудшения производительности на целевых задачах.'}, 'en': {'title': 'Preserving Alignment in Fine-Tuning with AGL', 'desc': 'AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks.'}, 'zh': {'title': 'AlignGuard-LoRA：保持对齐，确保安全', 'desc': 'AlignGuard-LoRA (AGL) 是一个框架，旨在通过引入正则化技术和诊断基准来保持大型语言模型在微调过程中的对齐性。该框架解决了低秩适应（LoRA）在更新过程中可能导致的对齐漂移问题，从而增强安全性和行为约束。AGL 采用了多种关键组件，包括基于费舍尔信息矩阵的正则化和任务特定的正则化，以稳定新知识的整合。实验证明，AGL 能够在不降低下游任务性能的情况下，将对齐漂移减少多达 50%。'}}}, {'id': 'https://huggingface.co/papers/2508.02630', 'title': 'What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce', 'url': 'https://huggingface.co/papers/2508.02630', 'abstract': 'ACES, a sandbox environment, studies AI agents\' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.  \t\t\t\t\tAI-generated summary \t\t\t\t Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal "top" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '2860dd8bc6a41f92', 'authors': ['Amine Allouah', 'Omar Besbes', 'Josué D Figueroa', 'Yash Kanoria', 'Akshit Kumar'], 'affiliations': ['Columbia University, Graduate School of Business', 'MyCustomAI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02630.jpg', 'data': {'categories': ['#agents', '#multimodal', '#ethics', '#games', '#alignment'], 'emoji': '🛒', 'ru': {'title': 'ИИ идет за покупками: новая эра электронной коммерции', 'desc': 'Исследование ACES изучает поведение ИИ-агентов при совершении покупок в виртуальной торговой среде. Эксперименты выявили влияние позиции товаров, спонсорских тегов, рекомендаций, цен, рейтингов и отзывов на выбор ИИ-покупателей. Результаты показывают, что разные модели машинного обучения демонстрируют различную чувствительность к этим факторам. Исследование поднимает вопросы о стратегиях продавцов, дизайне платформ и регулировании в экосистеме, где покупки осуществляются с помощью ИИ.'}, 'en': {'title': 'Understanding AI Agents in E-Commerce: Insights from ACES', 'desc': "The paper introduces ACES, a sandbox environment designed to analyze the shopping behavior of AI agents in a simulated marketplace. It investigates how these agents respond to various factors such as product position, pricing, ratings, and endorsements, revealing that their preferences can differ significantly from human shoppers. The study finds that while AI agents generally prefer higher-ranked products, their specific choices can vary based on the model used, challenging the idea of a single 'top' product. The findings suggest that sellers can optimize their listings to cater to AI preferences, potentially reshaping strategies in e-commerce as AI-driven shopping becomes more prevalent."}, 'zh': {'title': '探索AI代理在电商中的购物行为', 'desc': '本论文研究了人工智能代理在模拟市场中的购物行为，提出了一个名为ACES的沙盒环境。通过随机化产品位置、价格、评分、评论和赞助标签，研究了不同模型在购物时的因果关系和偏好。结果显示，AI代理对产品位置有明显的偏好，但不同模型的选择差异很大，且对价格和评分的敏感度与人类相似但幅度不同。研究还表明，卖方可以通过优化产品描述来吸引AI买家的偏好，从而在市场中获得显著的份额提升。'}}}, {'id': 'https://huggingface.co/papers/2508.02455', 'title': 'TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs', 'url': 'https://huggingface.co/papers/2508.02455', 'abstract': 'A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '70799804b6937703', 'authors': ['Daniele Cipollone', 'Egor Bogomolov', 'Arie van Deursen', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology, Delft, Netherlands', 'JetBrains, Amsterdam, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2508.02455.jpg', 'data': {'categories': ['#plp', '#training', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Умное ранжирование автодополнений кода с помощью языковых моделей', 'desc': 'Статья предлагает новый подход к ранжированию статических автодополнений кода в IDE с использованием языковых моделей. Метод организует варианты дополнений в префиксное дерево и выполняет однократный жадный проход декодирования для сбора оценок на уровне токенов. Это позволяет точно ранжировать варианты с учетом контекста без необходимости в лучевом поиске или адаптации модели. Подход быстрый, не зависит от архитектуры и совместим с уже развернутыми моделями автодополнения кода.'}, 'en': {'title': 'Smart Code Completion with Language Models', 'desc': 'This paper introduces a novel method for ranking code completions in Integrated Development Environments (IDEs) using language models. The approach organizes potential code completions into a prefix tree, allowing for efficient scoring of completions through a single greedy decoding pass. This method enhances the ranking of static code completions by providing a token-aware scoring system without the need for complex techniques like beam search or prompt engineering. The proposed solution is lightweight, model-agnostic, and can be easily integrated into existing IDEs, improving developer assistance significantly.'}, 'zh': {'title': '智能代码补全的新方法', 'desc': '本文提出了一种新的评分方法，利用语言模型对静态代码补全进行排名。该方法将所有有效的补全组织成前缀树，并通过单次贪婪解码来收集每个标记的分数。与传统的基于手工启发式或轻量级机器学习模型的方法相比，这种方法能够更好地捕捉上下文信息，并在不同项目和编码风格中进行泛化。最终，这种快速且与模型无关的方法为集成语言模型到现有IDE工具中提供了有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.02063', 'title': 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs', 'url': 'https://huggingface.co/papers/2508.02063', 'abstract': "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7", 'score': 1, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '0b136776fbb19b26', 'authors': ['Amitava Das', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'BITS Pilani Goa', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02063.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#alignment', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'TraceAlign: отслеживание и устранение дрейфа выравнивания в больших языковых моделях', 'desc': 'TraceAlign - это фреймворк для выявления и снижения дрейфа выравнивания в больших языковых моделях. Он отслеживает небезопасные завершения до их источников в обучающих данных и применяет интервенции для уменьшения дрейфа при сохранении полезности модели. Ключевым элементом является Индекс Конфликта Убеждений (BCI), количественно оценивающий семантическое несоответствие между сгенерированными фрагментами и заданными политиками. Фреймворк предлагает три дополняющих друг друга метода защиты: фильтр безопасности TraceShield, контрастивную функцию потерь и стратегию декодирования Prov-Decode.'}, 'en': {'title': 'TraceAlign: Bridging the Gap in LLM Alignment', 'desc': "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."}, 'zh': {'title': 'TraceAlign：减轻大型语言模型对齐漂移的创新框架', 'desc': 'TraceAlign是一个框架，用于识别和减轻大型语言模型（LLMs）中的对齐漂移。它通过追踪不安全的生成结果到其训练来源，并应用干预措施来减少漂移，同时保持模型的实用性。该框架引入了信念冲突指数（BCI），量化生成内容与对齐政策之间的语义不一致性。通过三种互补的干预措施，TraceAlign能够在保持任务性能的同时，显著降低对齐漂移的发生率。'}}}, {'id': 'https://huggingface.co/papers/2508.01126', 'title': 'UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation', 'url': 'https://huggingface.co/papers/2508.01126', 'abstract': "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.", 'score': 0, 'issue_id': 5212, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '8f30093d889bde3a', 'authors': ['Chaitanya Patel', 'Hiroki Nakamura', 'Yuta Kyuragi', 'Kazuki Kozuka', 'Juan Carlos Niebles', 'Ehsan Adeli'], 'affiliations': ['Panasonic Holdings Corporation', 'Panasonic R&D Company of America', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01126.jpg', 'data': {'categories': ['#dataset', '#games', '#benchmark', '#video', '#diffusion', '#multimodal', '#cv', '#healthcare'], 'emoji': '🕶️', 'ru': {'title': 'Революция в моделировании движения от первого лица', 'desc': 'Представлена унифицированная условная модель диффузии движения UniEgoMotion для генерации и прогнозирования эгоцентрического движения с использованием изображений от первого лица. Модель достигает наилучших результатов в реконструкции эгоцентрического движения и впервые позволяет генерировать движение по одному эгоцентрическому изображению. UniEgoMotion использует новое представление движения с учетом положения головы, специально разработанное для эгоцентрических устройств. Для обучения модели создан большой набор данных EE4D-Motion на основе EgoExo4D с псевдо-разметкой трехмерного движения.'}, 'en': {'title': 'Revolutionizing Egocentric Motion with UniEgoMotion', 'desc': 'The paper presents UniEgoMotion, a unified conditional motion diffusion model designed for generating and forecasting egocentric motion from first-person images. This model addresses the limitations of existing methods that focus on third-person perspectives and structured 3D scenes, which are not effective in real-world scenarios with dynamic cameras and occlusions. UniEgoMotion utilizes a novel head-centric motion representation and effectively extracts scene context from images to predict plausible 3D motion. The introduction of the EE4D-Motion dataset, which includes pseudo-ground-truth 3D motion annotations, supports the training of this model, achieving state-of-the-art results in egocentric motion tasks.'}, 'zh': {'title': '自我中心运动生成的新突破', 'desc': '本文介绍了一种统一的条件运动扩散模型UniEgoMotion，用于从第一人称图像生成和预测自我中心的运动。该模型在自我中心运动重建和预测方面达到了最先进的性能，能够仅从单张图像生成运动。UniEgoMotion采用了一种新颖的头部中心运动表示，支持在没有明确3D场景的情况下进行场景感知的运动合成。通过引入EE4D-Motion数据集，本文为训练提供了丰富的伪真实3D运动注释，推动了自我中心运动建模的新标准。'}}}, {'id': 'https://huggingface.co/papers/2508.02324', 'title': 'Qwen-Image Technical Report', 'url': 'https://huggingface.co/papers/2508.02324', 'abstract': "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.", 'score': 79, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '4417ddebd50d6ca5', 'authors': ['Chenfei Wu', 'Jiahao Li', 'Jingren Zhou', 'Junyang Lin', 'Kaiyuan Gao', 'Kun Yan', 'Sheng-ming Yin', 'Shuai Bai', 'Xiao Xu', 'Yilei Chen', 'Yuxiang Chen', 'Zecheng Tang', 'Zekai Zhang', 'Zhengyi Wang', 'An Yang', 'Bowen Yu', 'Chen Cheng', 'Dayiheng Liu', 'Deqing Li', 'Hang Zhang', 'Hao Meng', 'Hu Wei', 'Jingyuan Ni', 'Kai Chen', 'Kuan Cao', 'Liang Peng', 'Lin Qu', 'Minggang Wu', 'Peng Wang', 'Shuting Yu', 'Tingkun Wen', 'Wensen Feng', 'Xiaoxiao Xu', 'Yi Wang', 'Yichang Zhang', 'Yongqiang Zhu', 'Yujia Wu', 'Yuxuan Cai', 'Zenan Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.02324.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#games', '#cv', '#training', '#multimodal', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Qwen-Image: новый уровень генерации и редактирования изображений с текстом', 'desc': 'Qwen-Image - это модель генерации изображений, которая достигает значительных успехов в рендеринге сложного текста и точном редактировании изображений. Модель использует комплексный конвейер данных, включающий сбор, фильтрацию, аннотацию и балансировку данных. Применяется стратегия прогрессивного обучения, начиная с простых задач и постепенно переходя к более сложным. Qwen-Image вводит улучшенную парадигму многозадачного обучения и механизм двойного кодирования для повышения согласованности при редактировании изображений.'}, 'en': {'title': 'Revolutionizing Image Generation and Editing with Qwen-Image', 'desc': "Qwen-Image is an advanced image generation model that enhances text rendering and image editing through a sophisticated data pipeline and a dual-encoding mechanism. It employs a comprehensive approach to data collection and training, utilizing a progressive strategy that improves the model's ability to handle complex text inputs, including both alphabetic and logographic languages. The model's multi-task training paradigm integrates various tasks to ensure consistency in image editing while maintaining high-quality outputs. Overall, Qwen-Image sets new standards in image generation and editing performance across multiple benchmarks."}, 'zh': {'title': 'Qwen-Image：图像生成与编辑的突破性进展', 'desc': 'Qwen-Image是一种图像生成模型，旨在通过全面的数据处理流程和渐进式训练策略，提升文本渲染和图像编辑的能力。该模型采用了双编码机制，能够有效地处理复杂的文本输入，并在字母语言和表意文字（如中文）上都表现出色。通过多任务训练，Qwen-Image在文本到图像和图像重建任务中实现了更高的一致性，确保了语义和视觉的保真度。最终，Qwen-Image在多个基准测试中展现了其在图像生成和编辑方面的领先性能。'}}}, {'id': 'https://huggingface.co/papers/2508.01959', 'title': 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension', 'url': 'https://huggingface.co/papers/2508.01959', 'abstract': "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.", 'score': 37, 'issue_id': 5178, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '063b1bca074ff85f', 'authors': ['Junjie Wu', 'Jiangnan Li', 'Yuqing Li', 'Lemao Liu', 'Liyan Xu', 'Jiwei Li', 'Dit-Yan Yeung', 'Jie Zhou', 'Mo Yu'], 'affiliations': ['HKUST', 'IIE-CAS', 'WeChat AI, Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01959.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#rag', '#long_context', '#optimization', '#benchmark', '#training'], 'emoji': '🔍', 'ru': {'title': 'Контекст имеет значение: ситуативные эмбеддинги для точного информационного поиска', 'desc': 'Статья представляет новый подход к улучшению производительности информационного поиска путем контекстуализации коротких текстовых фрагментов в более широком контексте. Авторы предлагают новую парадигму обучения и модели ситуативных эмбеддингов (SitEmb), которые превосходят современные модели, имея меньше параметров. Метод особенно эффективен для задач, требующих понимания локального контекста в рамках более широкого документа. Результаты показывают значительное улучшение производительности на специально созданном наборе данных для оценки ситуативного поиска.'}, 'en': {'title': 'Enhancing Retrieval with Contextual Short Text Chunks', 'desc': 'This paper introduces a new training method and a model called situated embedding models (SitEmb) that improve the retrieval of information from text. By conditioning short text chunks on a broader context, the model captures the meaning of each chunk more effectively. The authors demonstrate that existing models struggle with this task due to their limitations in encoding context. Their SitEmb model outperforms leading models with significantly fewer parameters, showing better retrieval performance across various languages and applications.'}, 'zh': {'title': '情境嵌入，提升检索性能！', 'desc': '本文提出了一种新的训练范式和情境嵌入模型（SitEmb），通过将短文本片段与更广泛的上下文窗口相结合，提升了检索性能。传统的检索方法往往将长文档拆分为小块，但这些小块的理解需要上下文信息。我们的方法通过在更广泛的上下文中对短片段进行编码，来增强检索效果。实验结果表明，SitEmb模型在参数更少的情况下，显著超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.02276', 'title': 'CellForge: Agentic Design of Virtual Cell Models', 'url': 'https://huggingface.co/papers/2508.02276', 'abstract': "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.", 'score': 27, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '04238c0793ca08e5', 'authors': ['Xiangru Tang', 'Zhuoyun Yu', 'Jiapeng Chen', 'Yan Cui', 'Daniel Shao', 'Weixu Wang', 'Fang Wu', 'Yuchen Zhuang', 'Wenqi Shi', 'Zhi Huang', 'Arman Cohan', 'Xihong Lin', 'Fabian Theis', 'Smita Krishnaswamy', 'Mark Gerstein'], 'affiliations': ['Google DeepMind', 'Harvard University', 'Helmholtz Zentrum Munchen', 'Stanford University', 'University of Pennsylvania', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02276.jpg', 'data': {'categories': ['#dataset', '#training', '#science', '#open_source', '#architecture', '#agents', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'CellForge: ИИ-агенты создают виртуальные клетки из одноклеточных данных', 'desc': 'CellForge - это агентная система, использующая мультиагентный фреймворк для преобразования необработанных одноклеточных мультиомных данных в оптимизированные вычислительные модели виртуальных клеток. Система состоит из трех основных модулей: анализа задач, разработки методов и выполнения экспериментов. CellForge превосходит современные методы в прогнозировании одноклеточных возмущений на шести разнообразных наборах данных. Это демонстрирует, как итеративное взаимодействие между агентами с различными перспективами обеспечивает лучшие решения, чем прямой подход к задаче моделирования.'}, 'en': {'title': 'Transforming Biology with Collaborative AI Models', 'desc': 'CellForge is an innovative system that uses a multi-agent framework to create computational models for virtual cells from raw single-cell multi-omics data. It addresses the complexities of biological systems by employing specialized agents that collaborate to analyze tasks, design methods, and execute experiments. This approach allows CellForge to generate optimized model architectures and executable code, significantly improving predictions for single-cell perturbations. By integrating diverse perspectives from its agents, CellForge consistently outperforms existing state-of-the-art methods in the field.'}, 'zh': {'title': 'CellForge：优化虚拟细胞建模的智能系统', 'desc': 'CellForge 是一个基于多智能体框架的系统，能够将原始的单细胞多组学数据转化为优化的虚拟细胞计算模型。该系统通过分析任务和数据集，自动生成可执行的代码，显著提高了单细胞扰动预测的准确性。CellForge 的设计模块由不同专业的智能体协作开发建模策略，确保了模型的优化。实验结果表明，CellForge 在多种数据集上均优于现有的最先进方法，展示了多智能体协作的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.01059', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report', 'url': 'https://huggingface.co/papers/2508.01059', 'abstract': 'Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.', 'score': 21, 'issue_id': 5176, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '897c594bfa5630a6', 'authors': ['Sajana Weerawardhena', 'Paul Kassianik', 'Blaine Nelson', 'Baturay Saglam', 'Anu Vellore', 'Aman Priyanshu', 'Supriti Vijay', 'Massimo Aufiero', 'Arthur Goldblatt', 'Fraser Burch', 'Ed Li', 'Jianliang He', 'Dhruv Kedia', 'Kojin Oshiba', 'Zhouran Yang', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Carnegie Mellon University', 'Cisco Systems Inc.', 'Foundation AI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01059.jpg', 'data': {'categories': ['#dataset', '#alignment', '#security', '#training', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Интеллектуальный помощник по кибербезопасности на базе ИИ', 'desc': 'Foundation-Sec-8B-Instruct - это языковая модель, специализирующаяся на кибербезопасности и предназначенная для диалогового взаимодействия. Модель сочетает в себе специализированные знания в области кибербезопасности с возможностями следования инструкциям и ведения разговора. Оценки показывают, что Foundation-Sec-8B-Instruct превосходит другие модели в задачах кибербезопасности, сохраняя при этом способность следовать инструкциям. Авторы предполагают, что эта модель станет незаменимым помощником в повседневной работе специалистов по кибербезопасности.'}, 'en': {'title': 'Empowering Cybersecurity with Conversational AI', 'desc': 'Foundation-Sec-8B-Instruct is a large language model (LLM) specifically designed for cybersecurity applications, enhancing chat-style interactions and instruction-following capabilities. It builds upon the previous Foundation-Sec-8B model, which was tailored for fine-tuning on cybersecurity tasks but lacked conversational features. This new model integrates domain-specific knowledge with the ability to follow instructions and engage in dialogue, resulting in high-quality responses relevant to cybersecurity. Evaluations demonstrate that it surpasses other models like Llama 3.1-8B-Instruct in cybersecurity tasks while maintaining competitive performance in instruction-following.'}, 'zh': {'title': '网络安全对话的智能助手', 'desc': 'Foundation-Sec-8B-Instruct 是一个专注于网络安全的语言模型，旨在进行对话式交互和遵循指令。该模型在网络安全任务上表现优于其他模型，同时在遵循指令的能力上与之相匹配。它结合了特定领域的知识和人类偏好的对齐，能够生成高质量和相关的响应。我们希望 Foundation-Sec-8B-Instruct 能成为网络安全专业人员日常工作中不可或缺的助手。'}}}, {'id': 'https://huggingface.co/papers/2508.02150', 'title': "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following", 'url': 'https://huggingface.co/papers/2508.02150', 'abstract': "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if.", 'score': 20, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '37d1b608fde6bd5f', 'authors': ['Qingyu Ren', 'Qianyu He', 'Bowei Zhang', 'Jie Zeng', 'Jiaqing Liang', 'Yanghua Xiao', 'Weikang Zhou', 'Zeye Sun', 'Fei Yu'], 'affiliations': ['Ant Group', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02150.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ для лучшего выполнения инструкций', 'desc': 'Исследователи предложили новый подход к улучшению способности моделей машинного обучения следовать инструкциям без внешнего контроля. Они разработали систему самообучения с подкреплением, которая использует внутренние сигналы самой модели. Эксперименты показали, что этот метод значительно улучшает следование инструкциям, сохраняя при этом способности к рассуждению. Предложенный подход является масштабируемым и экономически эффективным решением для совершенствования моделей рассуждений.'}, 'en': {'title': 'Enhancing Instruction Following in Reasoning Models with Self-Supervised RL', 'desc': "This paper presents a self-supervised reinforcement learning (RL) framework designed to improve how reasoning models follow instructions. Traditional methods often depend on external models, which can be costly and limit accessibility. The proposed framework utilizes the internal signals of reasoning models to enhance their instruction-following abilities without needing external supervision. Experimental results show that this approach not only boosts instruction following but also preserves the models' reasoning performance, making it a scalable and cost-effective solution."}, 'zh': {'title': '自监督强化学习提升推理模型的指令遵循能力', 'desc': '本论文提出了一种自监督强化学习框架，旨在提升推理模型的指令遵循能力，而无需外部监督。传统方法通常依赖于更强大的外部模型，这导致了方法上的瓶颈和实际应用中的限制，如成本增加和可及性问题。我们的方法利用推理模型自身的内部信号来改善指令遵循能力，同时保持推理性能。实验结果表明，该框架在提升指令遵循能力的同时，提供了一种可扩展且具有成本效益的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.02317', 'title': 'VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo', 'url': 'https://huggingface.co/papers/2508.02317', 'abstract': 'A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.', 'score': 8, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '2e96724e612a0eb6', 'authors': ['Qianli Ma', 'Yaowei Zheng', 'Zhelun Shi', 'Zhongkai Zhao', 'Bin Jia', 'Ziyue Huang', 'Zhiqi Lin', 'Youjie Li', 'Jiacheng Yang', 'Yanghua Peng', 'Zhi Zhang', 'Xin Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.02317.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Ускоряем обучение омни-модальных LLM с помощью модульной архитектуры', 'desc': 'Эта статья представляет новую модульную систему обучения для омни-модальных языковых моделей (LLM). Система предлагает эффективное 3D-распараллеливание и гибкую конфигурацию, что ускоряет разработку омни-модальных LLM. Ключевые особенности включают разделение коммуникации и вычислений, а также простую интеграцию новых модальностей. Результаты показывают высокую эффективность и масштабируемость при обучении крупных омни-модальных моделей.'}, 'en': {'title': 'Accelerating Omni-Modal LLMs with Modular Training', 'desc': 'This paper introduces \textit{veomni}, a modular training framework designed to enhance the development of omni-modal large language models (LLMs). It addresses the challenges of training these models by separating model architecture from parallel processing logic, which allows for efficient 3D parallelism. The framework supports easy integration of new modalities, reducing the need for extensive code modifications. With \textit{veomni}, a mixture-of-experts model with 30 billion parameters can achieve high throughput and scalability, demonstrating its effectiveness in training large omni-modal LLMs.'}, 'zh': {'title': '模块化训练框架，提升全模态LLM开发效率', 'desc': '这篇论文介绍了一种名为\\veomni的模块化训练框架，旨在加速全模态大语言模型（LLM）的开发。该框架通过高效的三维并行处理和灵活的配置，解决了训练全模态LLM时面临的挑战。\\veomni将模型定义与并行逻辑解耦，使得在多种模态上进行大规模训练变得更加高效。使用\\veomni，研究人员能够以极高的速度训练具有30亿参数的全模态专家模型，展示了其在训练大型全模态LLM方面的优越效率和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2507.17520', 'title': 'InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation', 'url': 'https://huggingface.co/papers/2507.17520', 'abstract': "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.", 'score': 8, 'issue_id': 5176, 'pub_date': '2025-07-23', 'pub_date_card': {'ru': '23 июля', 'en': 'July 23', 'zh': '7月23日'}, 'hash': '13d868fd7ad8ea42', 'authors': ['Shuai Yang', 'Hao Li', 'Yilun Chen', 'Bin Wang', 'Yang Tian', 'Tai Wang', 'Hanqing Wang', 'Feng Zhao', 'Yiyi Liao', 'Jiangmiao Pang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.17520.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#reasoning', '#multimodal', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'InstructVLA: Мост между интуитивным управлением роботами и эффективным обучением', 'desc': 'InstructVLA - это модель обработки зрения, языка и действий, которая улучшает манипуляционные способности роботов, сохраняя при этом рассуждения на основе зрения и языка. Модель использует мультимодальное обучение и адаптацию на основе смеси экспертов. InstructVLA демонстрирует значительное улучшение производительности по сравнению с существующими моделями на различных задачах, включая задачи в симулированной и реальной среде. Эта модель открывает возможности для более интуитивного и управляемого взаимодействия человека с роботом при эффективном обучении политикам.'}, 'en': {'title': 'Bridging Vision, Language, and Action for Smarter Robots', 'desc': 'InstructVLA is a new model that combines vision, language, and action to improve how robots perform tasks. It uses a special training method called Vision-Language-Action Instruction Tuning (VLA-IT) to enhance both understanding and action capabilities without losing previous knowledge. This model outperforms existing systems in various tasks, showing significant improvements in manipulation and reasoning. By integrating multimodal training and expert adaptation, InstructVLA enables better human-robot interaction and efficient learning of tasks.'}, 'zh': {'title': '提升机器人操作与推理的完美结合', 'desc': 'InstructVLA是一种端到端的视觉-语言-动作模型，旨在提高机器人操作性能，同时保持视觉-语言推理能力。该模型通过多模态训练和专家混合适应，解决了现有模型在任务特定数据上的局限性和灾难性遗忘问题。InstructVLA引入了一种新的训练范式，称为视觉-语言-动作指令调优（VLA-IT），在标准视觉-语言模型数据集和一个包含65万样本的VLA-IT数据集上共同优化文本推理和动作生成。实验结果表明，InstructVLA在多个任务上表现优异，展示了其在高效政策学习和人机交互中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.01548', 'title': 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2508.01548', 'abstract': "A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce a dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining a similarly high pruning rate. Our work paves a new way for building more powerful and efficient LVLMs.", 'score': 7, 'issue_id': 5180, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '7611e04f9fa72eb7', 'authors': ['Quan-Sheng Zeng', 'Yunheng Li', 'Qilong Wang', 'Peng-Tao Jiang', 'Zuxuan Wu', 'Ming-Ming Cheng', 'Qibin Hou'], 'affiliations': ['Shanghai Innovation Institute', 'Tianjin University', 'VCIP, CS, Nankai University', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2508.01548.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка для эффективных визуально-языковых моделей', 'desc': 'GlimpsePrune - это динамическая система обрезки, которая повышает эффективность крупных визуально-языковых моделей (LVLM). Она адаптивно удаляет нерелевантные визуальные токены без ухудшения производительности модели. GlimpsePrune обрезает 92.6% визуальных токенов, сохраняя при этом базовую производительность на задачах визуальных вопросов и ответов (VQA). Улучшенная версия GlimpsePrune+ достигает 110% базовой производительности при сохранении высокого уровня обрезки.'}, 'en': {'title': 'Dynamic Pruning for Efficient Vision-Language Models', 'desc': 'GlimpsePrune is a dynamic pruning framework designed to enhance the efficiency of Large Vision-Language Models (LVLMs) by selectively removing irrelevant visual tokens. Unlike traditional methods that use fixed compression ratios, GlimpsePrune adapts to the complexity of different scenes, ensuring that important visual information is preserved. This framework prunes up to 92.6% of visual tokens while maintaining baseline performance on visual question answering tasks. Additionally, an improved version, GlimpsePrune+, not only retains high pruning rates but also boosts performance beyond the baseline, demonstrating a significant advancement in model efficiency.'}, 'zh': {'title': '动态剪枝，提升视觉语言模型效率', 'desc': 'GlimpsePrune是一个动态剪枝框架，旨在提高大型视觉语言模型的效率。它通过自适应地去除不相关的视觉标记，避免了固定压缩比带来的信息丢失问题。该方法在一次前向传播中进行剪枝，能够保留92.6%的视觉标记，同时在自由形式的视觉问答任务中保持基线性能。我们的研究为构建更强大和高效的视觉语言模型开辟了新路径。'}}}, {'id': 'https://huggingface.co/papers/2508.02137', 'title': 'Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind', 'url': 'https://huggingface.co/papers/2508.02137', 'abstract': 'AuroBind is a scalable virtual screening framework that fine-tunes atomic-level structural models to predict ligand-bound structures and binding fitness, achieving high hit rates in prospective screens across disease-relevant targets.  \t\t\t\t\tAI-generated summary \t\t\t\t Most human proteins remain undrugged, over 96% of human proteins remain unexploited by approved therapeutics. While structure-based virtual screening promises to expand the druggable proteome, existing methods lack atomic-level precision and fail to predict binding fitness, limiting translational impact. We present AuroBind, a scalable virtual screening framework that fine-tunes a custom atomic-level structural model on million-scale chemogenomic data. AuroBind integrates direct preference optimization, self-distillation from high-confidence complexes, and a teacher-student acceleration strategy to jointly predict ligand-bound structures and binding fitness. The proposed models outperform state-of-the-art models on structural and functional benchmarks while enabling 100,000-fold faster screening across ultra-large compound libraries. In a prospective screen across ten disease-relevant targets, AuroBind achieved experimental hit rates of 7-69%, with top compounds reaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and GPR160, AuroBind identified both agonists and antagonists with success rates of 16-30%, and functional assays confirmed GPR160 modulation in liver and prostate cancer models. AuroBind offers a generalizable framework for structure-function learning and high-throughput molecular screening, bridging the gap between structure prediction and therapeutic discovery.', 'score': 6, 'issue_id': 5190, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'f767500373dc2f12', 'authors': ['Zhongyue Zhang', 'Jiahua Rao', 'Jie Zhong', 'Weiqiang Bai', 'Dongxue Wang', 'Shaobo Ning', 'Lifeng Qiao', 'Sheng Xu', 'Runze Ma', 'Will Hua', 'Jack Xiaoyu Chen', 'Odin Zhang', 'Wei Lu', 'Hanyi Feng', 'He Yang', 'Xinchao Shi', 'Rui Li', 'Wanli Ouyang', 'Xinzhu Ma', 'Jiahao Wang', 'Jixian Zhang', 'Jia Duan', 'Siqi Sun', 'Jian Zhang', 'Shuangjia Zheng'], 'affiliations': ['Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China', 'Institute for Medical Engineering & Science, Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Key Laboratory of Protection, Development and Utilization of Medicinal Resources in Liupanshan Area, Ministry of Education, Peptide & Protein Drug Research Center, School of Pharmacy, Ningxia Medical University, Ningxia, China', 'Lingang Laboratory, Shanghai, China', 'Medicinal Chemistry and Bioinformatics Center, School of Medicine, Shanghai Jiao Tong University, Shanghai, China', 'Research Institute of Intelligent Complex Systems, Fudan University, Shanghai, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangdong, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Zhongshan Institute for Drug Discovery, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02137.jpg', 'data': {'categories': ['#healthcare', '#data', '#optimization', '#science', '#dataset', '#benchmark'], 'emoji': '🧬', 'ru': {'title': 'AuroBind: прорыв в виртуальном скрининге лекарств на атомарном уровне', 'desc': 'AuroBind - это масштабируемая система виртуального скрининга, которая оптимизирует структурные модели на атомарном уровне для предсказания комплексов лиганд-белок и оценки аффинности связывания. Система использует обучение на миллионах хемогеномных данных, включая прямую оптимизацию предпочтений и самодистилляцию. AuroBind превосходит существующие методы по точности и скорости, позволяя проводить скрининг сверхбольших библиотек соединений в 100 000 раз быстрее. В проспективных экспериментах система продемонстрировала высокую эффективность для различных мишеней, включая орфанные GPCR-рецепторы.'}, 'en': {'title': 'AuroBind: Revolutionizing Drug Discovery with Precision and Speed', 'desc': 'AuroBind is a new framework designed for virtual screening in drug discovery, focusing on predicting how small molecules (ligands) bind to proteins. It fine-tunes detailed structural models using large datasets to improve the accuracy of binding predictions and identify potential drug candidates. By employing techniques like direct preference optimization and self-distillation, AuroBind significantly enhances screening speed and hit rates across various disease targets. This approach not only outperforms existing methods but also helps in discovering new therapeutic options for previously undrugged human proteins.'}, 'zh': {'title': 'AuroBind：高效的虚拟筛选新框架', 'desc': 'AuroBind 是一个可扩展的虚拟筛选框架，能够微调原子级结构模型，以预测配体结合结构和结合适应性。该方法通过优化直接偏好、自我蒸馏和教师-学生加速策略，联合预测配体结合结构和结合适应性。AuroBind 在结构和功能基准测试中超越了现有的最先进模型，并在超大化合物库中实现了100,000倍的筛选速度提升。通过对十个与疾病相关的靶点进行前瞻性筛选，AuroBind 实现了7-69%的实验命中率，显示出其在药物发现中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.01691', 'title': 'Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and\n  Regional Languages Around the Globe', 'url': 'https://huggingface.co/papers/2508.01691', 'abstract': 'Voxlect is a benchmark for evaluating speech foundation models on dialect classification and downstream applications across multiple languages and dialects.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.', 'score': 6, 'issue_id': 5185, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'bdb8f5a75702298d', 'authors': ['Tiantian Feng', 'Kevin Huang', 'Anfeng Xu', 'Xuan Shi', 'Thanathai Lertpetchpun', 'Jihwan Lee', 'Yoonjeong Lee', 'Dani Byrd', 'Shrikanth Narayanan'], 'affiliations': ['University of Southern California, Los Angeles, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01691.jpg', 'data': {'categories': ['#multilingual', '#science', '#audio', '#benchmark', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Voxlect: Универсальный инструмент для анализа диалектов в речевых моделях', 'desc': 'Voxlect - это новый эталонный тест для оценки речевых моделей в задачах классификации диалектов и связанных приложениях для множества языков и диалектов. Исследование использует более 2 миллионов обучающих высказываний из 30 общедоступных речевых корпусов с диалектной информацией. Авторы оценивают производительность нескольких широко используемых речевых моделей в классификации диалектов, а также их устойчивость в условиях шума. Voxlect демонстрирует применимость для обогащения существующих наборов данных для распознавания речи диалектной информацией и оценки систем генерации речи.'}, 'en': {'title': 'Voxlect: Advancing Dialect Classification in Speech Models', 'desc': 'Voxlect is a benchmark designed to evaluate speech foundation models specifically for dialect classification across various languages. It utilizes over 2 million training utterances from 30 speech corpora that include dialectal information, allowing for comprehensive assessments of model performance. The study not only benchmarks dialect classification but also explores downstream applications, such as enhancing automatic speech recognition (ASR) datasets with dialectal data. Additionally, it evaluates the robustness of these models in noisy environments and provides insights into geographic continuity in dialectal variations.'}, 'zh': {'title': 'Voxlect：全球方言分类的基准测试', 'desc': 'Voxlect是一个用于评估语音基础模型在方言分类和下游应用中的基准测试。该研究涵盖了多种语言和方言，包括英语、阿拉伯语、普通话、粤语等，使用了超过200万个带有方言信息的语音样本。我们评估了多种广泛使用的语音基础模型在方言分类中的表现，并分析了模型在噪声条件下的鲁棒性。Voxlect不仅用于方言分类的基准测试，还可以增强现有的语音识别数据集，帮助分析不同方言的ASR性能。'}}}, {'id': 'https://huggingface.co/papers/2508.01151', 'title': 'Personalized Safety Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2508.01151', 'abstract': "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at https://torpedo2648.github.io/PSAlign/.", 'score': 6, 'issue_id': 5176, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '22d777cd71f123c6', 'authors': ['Yu Lei', 'Jinbin Bai', 'Qingyu Shi', 'Aosong Feng', 'Kaidong Yu'], 'affiliations': ['National University of Singapore', 'Peking University', 'TeleAI, China Telecom', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01151.jpg', 'data': {'categories': ['#cv', '#dataset', '#alignment', '#diffusion', '#open_source', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Персонализированная безопасность в генеративных моделях изображений', 'desc': 'Предложена новая система Personalized Safety Alignment (PSA) для настройки генеративных моделей изображений под индивидуальные предпочтения безопасности пользователей. PSA интегрирует персонализированные профили пользователей в процесс диффузии, адаптируя поведение модели к индивидуальным требованиям безопасности. Авторы представили новый датасет Sage, capturing пользовательские предпочтения безопасности. Эксперименты показали, что PSA превосходит существующие методы в подавлении вредного контента и лучше соответствует ограничениям пользователей.'}, 'en': {'title': 'Personalized Safety for Safer AI-Generated Images', 'desc': 'This paper presents a Personalized Safety Alignment (PSA) framework that enhances text-to-image diffusion models by incorporating individual user profiles. Current models apply a one-size-fits-all approach to safety, which does not consider the unique safety preferences shaped by personal factors. The PSA framework allows for user-specific adjustments in the generative process, ensuring that the generated images align with individual safety standards while maintaining high image quality. The authors also introduce a new dataset, Sage, to effectively capture and integrate these personalized safety preferences, demonstrating improved performance in harmful content suppression compared to existing methods.'}, 'zh': {'title': '个性化安全对齐：让生成内容更符合你的安全偏好', 'desc': '这篇论文提出了一种个性化安全对齐框架（PSA），旨在将用户特定的个人资料整合到文本到图像的扩散模型中，以更好地符合个体的安全偏好。当前的安全机制通常采用统一标准，无法考虑用户的多样化安全边界，如年龄、心理健康和个人信仰等因素。PSA通过在扩散过程中整合个性化用户资料，调整模型行为以匹配个体安全偏好，同时保持图像质量。实验结果表明，PSA在有害内容抑制和生成内容与用户约束的对齐方面优于现有方法，取得了更高的胜率和通过率。'}}}, {'id': 'https://huggingface.co/papers/2508.02271', 'title': 'Dynaword: From One-shot to Continuously Developed Datasets', 'url': 'https://huggingface.co/papers/2508.02271', 'abstract': 'A framework called Dynaword and its implementation Danish Dynaword enable community-driven, open, and continuously updated large-scale natural language datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.', 'score': 5, 'issue_id': 5193, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'efe4e9f51385893f', 'authors': ['Kenneth Enevoldsen', 'Kristian Nørgaard Jensen', 'Jan Kostkan', 'Balázs Szabó', 'Márton Kardos', 'Kirten Vad', 'Andrea Blasi Núñez', 'Gianluca Barmina', 'Jacob Nielsen', 'Rasmus Larsen', 'Peter Vahlstrup', 'Per Møldrup Dalum', 'Desmond Elliott', 'Lukas Galke', 'Peter Schneider-Kamp', 'Kristoffer Nielbo'], 'affiliations': ['Aarhus University', 'The Alexandra Institute', 'University of Copenhagen', 'University of Southern Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2508.02271.jpg', 'data': {'categories': ['#dataset', '#survey', '#open_source', '#data'], 'emoji': '🌱', 'ru': {'title': 'Открытые и развивающиеся датасеты для NLP силами сообщества', 'desc': 'Dynaword - это фреймворк для создания больших открытых наборов данных на естественном языке с возможностью постоянного обновления сообществом. Danish Dynaword - это конкретная реализация этого подхода, содержащая в 4 раза больше токенов, чем аналоги, и имеющая открытую лицензию. Фреймворк решает проблемы ограничений лицензирования, статичности датасетов и закрытости процессов контроля качества. Он включает легковесные тесты для проверки форматирования, качества и документации данных, обеспечивая устойчивую основу для вклада сообщества и эволюции датасета.'}, 'en': {'title': 'Empowering Community-Driven Natural Language Datasets', 'desc': 'The paper presents Dynaword, a framework designed to create large-scale natural language datasets that are open and continuously updated through community involvement. It addresses three main challenges in current dataset practices: licensing issues, static releases, and limited quality assurance. The implementation, Danish Dynaword, showcases this framework by providing a dataset with significantly more tokens than similar datasets, all under open licenses. It also includes mechanisms for community contributions and quality checks, promoting a sustainable model for dataset development and maintenance.'}, 'zh': {'title': '构建开放、可持续的大规模语言数据集', 'desc': 'Dynaword是一个框架，旨在创建可持续更新的大规模自然语言数据集。它解决了当前数据集面临的三个主要挑战，包括模糊的许可限制、静态数据集发布和质量保证过程的局限性。Danish Dynaword是该框架的具体实现，展示了其潜力，并包含了比类似发布多四倍的标记。该项目通过轻量级测试确保数据格式、质量和文档，促进了社区的持续贡献和数据集的演变。'}}}, {'id': 'https://huggingface.co/papers/2508.02558', 'title': 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction', 'url': 'https://huggingface.co/papers/2508.02558', 'abstract': 'Sparse-dLLM improves the efficiency of diffusion large language models by implementing dynamic cache eviction and sparse attention, enhancing throughput without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10times higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.', 'score': 4, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'abb2417fa4d42a82', 'authors': ['Yuerong Song', 'Xiaoran Liu', 'Ruixiao Li', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.02558.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#inference', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без потери качества', 'desc': 'Sparse-dLLM - это новый фреймворк для повышения эффективности диффузионных больших языковых моделей (dLLM). Он использует динамическое удаление кэша и разреженное внимание, что позволяет значительно увеличить пропускную способность без ущерба для производительности. Анализ паттернов внимания в dLLM показал устойчивую межслойную разреженность, где ключевые токены остаются важными на протяжении всего процесса декодирования. Эксперименты продемонстрировали, что Sparse-dLLM обеспечивает до 10 раз более высокую пропускную способность по сравнению с обычными dLLM при сопоставимой производительности и аналогичных затратах пиковой памяти.'}, 'en': {'title': 'Boosting Efficiency in Language Models with Sparse-dLLM', 'desc': 'Sparse-dLLM is a novel framework designed to enhance the efficiency of diffusion large language models (dLLMs) by utilizing dynamic cache eviction and sparse attention mechanisms. It addresses the high computational complexity and memory demands of traditional dLLMs during inference by selectively retaining important tokens while evicting less relevant ones. This approach leverages the consistent saliency of tokens across decoding steps, allowing for improved throughput without sacrificing performance. Experimental results show that Sparse-dLLM can achieve up to 10 times higher throughput compared to standard dLLMs, while maintaining similar performance levels and memory usage.'}, 'zh': {'title': 'Sparse-dLLM：提升扩散大语言模型效率的创新方案', 'desc': 'Sparse-dLLM通过动态缓存驱逐和稀疏注意力机制，提高了扩散大语言模型的效率。传统的缓存技术虽然加速了解码，但占用了大量内存，限制了长上下文的应用。我们的研究发现，扩散大语言模型中的注意力模式存在跨层稀疏性，关键的token在解码过程中始终保持重要性。Sparse-dLLM是首个不需要训练的框架，通过延迟双向稀疏缓存，动态保留重要token并驱逐不重要的前缀/后缀条目，从而实现了更高的解码吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2508.01415', 'title': 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems', 'url': 'https://huggingface.co/papers/2508.01415', 'abstract': 'RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.', 'score': 4, 'issue_id': 5177, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'ea0bbd88dcba8948', 'authors': ['Mingcong Lei', 'Honghao Cai', 'Zezhou Cui', 'Liangchen Tan', 'Junkun Hong', 'Gehan Hu', 'Shuangyu Zhu', 'Yimou Wu', 'Shaohan Jiang', 'Ge Wang', 'Zhen Li', 'Shuguang Cui', 'Yiming Zhao', 'Yatong Han'], 'affiliations': ['FNii-Shenzhen', 'Harbin Engineering University', 'Harbin Institute of Technology, Shenzhen', 'Infused Synapse AI', 'SSE', 'The Chinese University of Hong Kong, Shengzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.01415.jpg', 'data': {'categories': ['#agents', '#optimization', '#open_source', '#training', '#agi', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboMemory: Мозгоподобная память для непрерывного обучения роботов', 'desc': 'RoboMemory - это мультимодульная система памяти для непрерывного обучения роботов, вдохновленная принципами работы мозга. Она включает четыре основных модуля, имитирующих функции различных отделов мозга: препроцессор информации, систему долговременной памяти, модуль замкнутого планирования и исполнитель низкого уровня. Система показала значительное улучшение производительности по сравнению с существующими решениями на бенчмарке EmbodiedBench. RoboMemory решает проблемы высокой латентности и масштабируемости, что делает ее перспективной основой для интеграции мультимодальных систем памяти в физических роботах.'}, 'en': {'title': 'RoboMemory: Revolutionizing Lifelong Learning in Robots', 'desc': 'RoboMemory is a new framework designed to help robots learn continuously over time, inspired by how the human brain works. It uses four main components that mimic brain functions to improve memory and planning in robots, allowing them to handle complex tasks better. The framework addresses issues like slow memory access and the need for robots to remember different types of information effectively. Tests show that RoboMemory significantly outperforms existing systems in real-world scenarios, making it a promising advancement in robotic learning.'}, 'zh': {'title': 'RoboMemory：提升机器人终身学习的多记忆框架', 'desc': 'RoboMemory是一个受大脑启发的多记忆框架，旨在提高物理机器人在终身学习中的表现。它结合了认知神经科学的原理，解决了现实环境中的关键挑战，如持续学习和任务相关性捕捉。该框架包含四个核心模块，分别模拟大脑的不同部分，以实现长期规划和累积学习。通过在复杂记忆框架中并行更新和检索，RoboMemory显著提高了推理速度，并在实际任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.01408', 'title': 'Artificial Intelligence and Misinformation in Art: Can Vision Language\n  Models Judge the Hand or the Machine Behind the Canvas?', 'url': 'https://huggingface.co/papers/2508.01408', 'abstract': 'State-of-the-art vision language models struggle with accurately attributing artists and distinguishing AI-generated images, highlighting the need for improvement to prevent misinformation.  \t\t\t\t\tAI-generated summary \t\t\t\t The attribution of artworks in general and of paintings in particular has always been an issue in art. The advent of powerful artificial intelligence models that can generate and analyze images creates new challenges for painting attribution. On the one hand, AI models can create images that mimic the style of a painter, which can be incorrectly attributed, for example, by other AI models. On the other hand, AI models may not be able to correctly identify the artist for real paintings, inducing users to incorrectly attribute paintings. In this paper, both problems are experimentally studied using state-of-the-art AI models for image generation and analysis on a large dataset with close to 40,000 paintings from 128 artists. The results show that vision language models have limited capabilities to: 1) perform canvas attribution and 2) to identify AI generated images. As users increasingly rely on queries to AI models to get information, these results show the need to improve the capabilities of VLMs to reliably perform artist attribution and detection of AI generated images to prevent the spread of incorrect information.', 'score': 4, 'issue_id': 5185, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '31ac5fdf5a455427', 'authors': ['Tarian Fu', 'Javier Conde', 'Gonzalo Martínez', 'Pedro Reviriego', 'Elena Merino-Gómez', 'Fernando Moral'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Universidad Antonio de Nebrija', 'Universidad Politécnica de Madrid', 'Universidad de Valladolid'], 'pdf_title_img': 'assets/pdf/title_img/2508.01408.jpg', 'data': {'categories': ['#cv', '#dataset', '#ethics', '#hallucinations', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект vs Искусство: проблемы атрибуции в эпоху нейросетей', 'desc': 'Современные модели компьютерного зрения и обработки естественного языка (VLM) испытывают трудности с точной атрибуцией художников и распознаванием изображений, сгенерированных искусственным интеллектом. Исследование проводилось на большом наборе данных, включающем около 40 000 картин 128 художников. Результаты показывают ограниченные возможности VLM в атрибуции картин и идентификации изображений, созданных ИИ. Авторы подчеркивают необходимость улучшения этих моделей для предотвращения распространения неверной информации.'}, 'en': {'title': 'Enhancing AI for Accurate Art Attribution and Detection', 'desc': 'This paper investigates the limitations of state-of-the-art vision language models (VLMs) in accurately attributing artworks to their respective artists and distinguishing between real and AI-generated images. The study highlights that these models struggle with both canvas attribution and the identification of AI-generated content, which can lead to misinformation. Using a large dataset of nearly 40,000 paintings from 128 artists, the authors demonstrate the inadequacies of current AI models in these tasks. The findings emphasize the urgent need for advancements in VLMs to enhance their reliability in art attribution and image detection.'}, 'zh': {'title': '提升视觉语言模型以防止艺术作品错误归属', 'desc': '本论文探讨了当前视觉语言模型在艺术作品归属和区分AI生成图像方面的不足。研究发现，AI模型能够生成模仿画家风格的图像，导致错误归属的情况。与此同时，AI模型在识别真实画作的艺术家时也存在困难，可能导致用户错误地归属作品。通过对近40,000幅来自128位艺术家的画作进行实验，结果显示视觉语言模型在画布归属和AI生成图像识别方面的能力有限，强调了改进这些模型的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.01287', 'title': 'Exploitation Is All You Need... for Exploration', 'url': 'https://huggingface.co/papers/2508.01287', 'abstract': 'Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process.', 'score': 3, 'issue_id': 5179, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '8867dd3f084db81e', 'authors': ['Micah Rentschler', 'Jesse Roberts'], 'affiliations': ['Tennessee Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01287.jpg', 'data': {'categories': ['#agents', '#optimization', '#games', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'Исследование может возникнуть из чистой эксплуатации', 'desc': 'Это исследование показывает, что агенты мета-обучения с подкреплением могут демонстрировать исследовательское поведение при обучении с жадной целевой функцией. Для этого необходимы три условия: повторяющаяся структура среды, наличие памяти у агента и возможность долгосрочного назначения кредита. Эксперименты на многоруких бандитах и сетках подтверждают, что при наличии структуры и памяти политика, обученная только на максимизацию награды, проявляет поисковое поведение. Удивительно, но отсутствие долгосрочного назначения кредита не всегда препятствует возникновению исследования.'}, 'en': {'title': 'Exploration Emerges from Greedy Training with the Right Conditions', 'desc': 'This paper explores how meta-reinforcement learning agents can learn to explore their environments even when trained with a focus on maximizing immediate rewards. The authors propose that this exploratory behavior can emerge if the environment has recurring structures, the agent has memory to recall past experiences, and long-term credit assignment is possible. Through experiments, they show that when these conditions are met, agents can engage in information-seeking exploration without explicit incentives. The findings challenge the traditional view that exploration and exploitation are separate goals, suggesting they can coexist within a single reward-maximization framework.'}, 'zh': {'title': '探索与利用的统一：元强化学习的新视角', 'desc': '本研究探讨了元强化学习代理在特定条件下如何表现出探索行为。我们提出，当环境具有重复结构、代理具备记忆能力，并且能够进行长期信用分配时，即使代理仅以贪婪目标进行训练，也能自发地进行探索。实验结果表明，在随机多臂老虎机和时间扩展的网格世界中，满足这些条件的代理会表现出信息寻求的探索行为。我们的发现表明，探索和利用并非完全对立，而是可以通过统一的奖励最大化过程共同出现。'}}}, {'id': 'https://huggingface.co/papers/2508.00910', 'title': 'Cyber-Zero: Training Cybersecurity Agents without Runtime', 'url': 'https://huggingface.co/papers/2508.00910', 'abstract': 'Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.', 'score': 3, 'issue_id': 5177, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '181a31b28dfe8e6a', 'authors': ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang'], 'affiliations': ['Amazon', 'Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00910.jpg', 'data': {'categories': ['#agents', '#dataset', '#synthetic', '#benchmark', '#open_source'], 'emoji': '🛡️', 'ru': {'title': 'Синтез траекторий без среды выполнения для обучения передовых LLM в кибербезопасности', 'desc': 'Cyber-Zero - это первая система для синтеза траекторий агентов без использования среды выполнения для обучения языковых моделей в кибербезопасности. Она использует общедоступные отчеты CTF и симуляцию на основе LLM для создания реалистичных последовательностей взаимодействий. Обученные на синтезированных траекториях агенты на основе LLM достигают значительного улучшения производительности на трех ключевых бенчмарках CTF. Лучшая модель Cyber-Zero-32B устанавливает новый state-of-the-art среди открытых моделей, соответствуя возможностям проприетарных систем.'}, 'en': {'title': 'Revolutionizing Cybersecurity AI with Runtime-Free Trajectory Synthesis', 'desc': 'Cyber-Zero introduces a novel framework for training cybersecurity large language models (LLMs) without the need for executable runtime environments. It synthesizes agent trajectories from Capture The Flag (CTF) writeups, allowing the generation of realistic interaction sequences that mimic runtime behaviors. This approach enables the training of LLM-based agents that outperform existing models on key CTF benchmarks. By achieving state-of-the-art performance with a cost-effective solution, Cyber-Zero demonstrates the potential of runtime-free trajectory synthesis in advancing cybersecurity AI.'}, 'zh': {'title': 'Cyber-Zero：无运行时环境的网络安全代理训练新方法', 'desc': 'Cyber-Zero 是一个创新的框架，旨在通过合成高质量的代理轨迹来训练网络安全领域的语言模型（LLM），而无需实际的运行时环境。该框架利用公开的CTF（Capture The Flag）写作材料，采用基于角色的LLM模拟，逆向工程运行时行为，生成真实的长时间交互序列。通过使用Cyber-Zero合成的轨迹，我们训练的LLM代理在三个主要的CTF基准测试中，性能提升达13.1%。Cyber-Zero-32B模型在开放权重模型中创造了新的性能记录，展示了无运行时轨迹合成在网络安全代理开发中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.00890', 'title': 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks', 'url': 'https://huggingface.co/papers/2508.00890', 'abstract': 'AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.', 'score': 2, 'issue_id': 5178, 'pub_date': '2025-07-26', 'pub_date_card': {'ru': '26 июля', 'en': 'July 26', 'zh': '7月26日'}, 'hash': '359ff54230f7e0ba', 'authors': ['Fali Wang', 'Hui Liu', 'Zhenwei Dai', 'Jingying Zeng', 'Zhiwei Zhang', 'Zongyu Wu', 'Chen Luo', 'Zhen Li', 'Xianfeng Tang', 'Qi He', 'Suhang Wang'], 'affiliations': ['Amazon, Palo Alto, CA, USA', 'The Pennsylvania State University, University Park, PA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.00890.jpg', 'data': {'categories': ['#interpretability', '#rl', '#agents', '#optimization', '#inference'], 'emoji': '🤖', 'ru': {'title': 'Умное распределение ресурсов для сложных задач искусственного интеллекта', 'desc': 'AgentTTS - это фреймворк на основе LLM-агентов для оптимизации распределения вычислительных ресурсов в многоэтапных сложных задачах. Он использует итеративные взаимодействия с обратной связью для поиска оптимальных распределений моделей и бюджетов для каждой подзадачи. AgentTTS превосходит традиционные методы по эффективности поиска, устойчивости к размерам обучающих выборок и интерпретируемости. Фреймворк решает проблемы комбинаторного пространства поиска и взаимозависимости оптимальных распределений между подзадачами.'}, 'en': {'title': 'Optimizing Compute Allocation for Complex Multi-Stage Tasks with AgentTTS', 'desc': 'AgentTTS is a framework that optimizes how computing resources are allocated for complex tasks that involve multiple stages. It focuses on improving the performance of large language models (LLMs) by dynamically adjusting resource allocation during inference, especially for tasks that require different capabilities at each stage. The framework addresses challenges such as the combinatorial nature of model selection and budget allocation, which makes traditional search methods inefficient. Through extensive experiments, AgentTTS has shown to be more efficient and robust compared to existing methods, providing better performance across various datasets and tasks.'}, 'zh': {'title': 'AgentTTS：优化多阶段任务的计算分配', 'desc': 'AgentTTS是一个基于大语言模型（LLM）代理的框架，旨在优化多阶段复杂任务的计算资源分配。与传统方法相比，它在性能和鲁棒性上有显著提升。该框架通过迭代反馈与执行环境进行交互，自动搜索计算最优的分配方案。实验结果表明，AgentTTS在搜索效率上显著优于传统方法和其他基于LLM的基线，并且对训练集大小的变化表现出更好的鲁棒性和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2508.02605', 'title': 'ReMoMask: Retrieval-Augmented Masked Motion Generation', 'url': 'https://huggingface.co/papers/2508.02605', 'abstract': "ReMoMask, a unified framework, addresses limitations in text-to-motion generation by integrating a Bidirectional Momentum Text-Motion Model, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance on HumanML3D and KIT-ML benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.", 'score': 1, 'issue_id': 5185, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '8fa0e8fe5289831e', 'authors': ['Zhengdao Li', 'Siheng Wang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Jiangsu University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02605.jpg', 'data': {'categories': ['#diffusion', '#games', '#benchmark', '#rag', '#video', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'ReMoMask: Революция в генерации движений из текста', 'desc': 'ReMoMask - это унифицированная система для генерации движений по текстовому описанию, решающая ограничения существующих подходов. Она включает в себя три ключевые инновации: двунаправленную моментную текстово-моторную модель, семантическое пространственно-временное внимание и RAG-безклассификаторное управление. ReMoMask достигает наилучших результатов на стандартных бенчмарках HumanML3D и KIT-ML, значительно улучшая показатели FID. Система эффективно генерирует согласованные во времени движения за минимальное количество шагов.'}, 'en': {'title': 'ReMoMask: Revolutionizing Text-to-Motion Generation!', 'desc': "ReMoMask is a new framework designed to improve text-to-motion generation, which creates human motion sequences from text descriptions. It combines a Bidirectional Momentum Text-Motion Model, which enhances retrieval accuracy, with a Semantic Spatio-temporal Attention mechanism that ensures realistic motion by applying biomechanical constraints. Additionally, it uses RAG-Classier-Free Guidance to boost the model's ability to generalize from data. The framework has shown significant improvements in generating coherent motions, outperforming previous methods on key benchmarks."}, 'zh': {'title': 'ReMoMask：文本到运动生成的新突破', 'desc': 'ReMoMask是一个统一框架，旨在解决文本到运动生成中的局限性。它通过集成双向动量文本-运动模型、语义时空注意力机制和无分类器引导，显著提高了在HumanML3D和KIT-ML基准测试中的表现。该框架通过动量队列解耦负样本规模与批量大小，提升了跨模态检索的精度。同时，语义时空注意力机制在部分融合过程中施加生物力学约束，消除了异步伪影。'}}}, {'id': 'https://huggingface.co/papers/2508.02268', 'title': 'SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System', 'url': 'https://huggingface.co/papers/2508.02268', 'abstract': 'A bidirectional machine translation system, SHAMI-MT, bridges the gap between Modern Standard Arabic and the Syrian dialect using AraT5v2-base-1024 architecture, achieving high-quality translations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces SHAMI-MT, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of 4.01 out of 5.0 when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.', 'score': 1, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '0704696f67aca30f', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh - Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2508.02268.jpg', 'data': {'categories': ['#training', '#low_resource', '#multilingual', '#dataset', '#machine_translation'], 'emoji': '🌉', 'ru': {'title': 'Мост между арабским языком и диалектом: революция в машинном переводе', 'desc': 'SHAMI-MT - это система машинного перевода, разработанная для преодоления разрыва между современным стандартным арабским языком и сирийским диалектом. Она использует архитектуру AraT5v2-base-1024 и включает две специализированные модели для двунаправленного перевода. Система была обучена на наборе данных Nabra и оценена на корпусе MADAR. Модель перевода с арабского на сирийский диалект достигла высокого среднего балла качества 4.01 из 5.0 по оценке GPT-4.1.'}, 'en': {'title': 'Bridging Dialects: SHAMI-MT Translates Arabic with Precision', 'desc': 'The paper presents SHAMI-MT, a bidirectional machine translation system designed to translate between Modern Standard Arabic (MSA) and the Syrian dialect. Utilizing the AraT5v2-base-1024 architecture, the system includes two specialized models for MSA-to-Shami and Shami-to-MSA translations. These models were fine-tuned on the Nabra dataset and evaluated using the MADAR corpus, achieving a high quality score of 4.01 out of 5.0. This work addresses the challenges of diglossia in Arabic, providing a valuable tool for accurate and culturally relevant translations.'}, 'zh': {'title': '弥合阿拉伯语与叙利亚方言的翻译桥梁', 'desc': '本文介绍了一种双向机器翻译系统SHAMI-MT，旨在弥合现代标准阿拉伯语与叙利亚方言之间的差距。该系统基于先进的AraT5v2-base-1024架构，分别针对现代标准阿拉伯语到叙利亚方言和叙利亚方言到现代标准阿拉伯语的翻译进行了优化。通过在全面的Nabra数据集上进行微调，并在MADAR语料库的未见数据上进行严格评估，模型表现出色。SHAMI-MT为阿拉伯方言翻译领域提供了重要的高保真工具，促进了内容本地化和跨文化交流。'}}}, {'id': 'https://huggingface.co/papers/2508.01109', 'title': 'Platonic Representations for Poverty Mapping: Unified Vision-Language\n  Codes or Agent-Induced Novelty?', 'url': 'https://huggingface.co/papers/2508.01109', 'abstract': 'A multimodal framework using satellite imagery and text data outperforms vision-only models in predicting household wealth, with LLM-generated text proving more effective than agent-retrieved text.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.', 'score': 1, 'issue_id': 5188, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '411bd35601db2ebd', 'authors': ['Satiyabooshan Murugaboopathy', 'Connor T. Jerzak', 'Adel Daoud'], 'affiliations': ['Chalmers & Linköping University', 'Fraunhofer Center', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2508.01109.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#dataset', '#science'], 'emoji': '🛰️', 'ru': {'title': 'Мультимодальный подход превосходит визуальные модели в оценке благосостояния', 'desc': 'Исследование показывает, что комбинация спутниковых снимков и текстовых данных превосходит модели, основанные только на изображениях, в прогнозировании благосостояния домохозяйств. Разработана мультимодальная система, использующая спутниковые снимки Landsat и текстовые описания, сгенерированные большой языковой моделью (LLM). Система включает пять различных подходов, включая модель компьютерного зрения, LLM и ансамбль всех сигналов. Результаты демонстрируют, что объединение визуальной и текстовой информации значительно улучшает точность прогнозирования благосостояния по сравнению с использованием только изображений.'}, 'en': {'title': 'Unlocking Wealth Insights: The Power of Multimodal Learning', 'desc': "This paper presents a multimodal framework that combines satellite imagery and text data to predict household wealth more accurately than models using only visual data. By analyzing Demographic and Health Survey data from African neighborhoods, the authors demonstrate that integrating LLM-generated text with satellite images significantly improves prediction performance. The study reveals that the internal knowledge of large language models (LLMs) is more effective than text retrieved by AI agents, enhancing the model's robustness across different contexts. Additionally, the research contributes a large-scale dataset that links over 60,000 clusters of socio-economic data with corresponding satellite images and textual descriptions."}, 'zh': {'title': '多模态框架提升家庭财富预测精度', 'desc': '本研究探讨了家庭财富等社会经济指标是否可以通过卫星图像和互联网文本数据进行预测。我们开发了一个多模态框架，结合了卫星图像和基于位置/年份的LLM生成文本，以提高财富预测的准确性。研究结果表明，融合视觉和文本信息的模型在财富预测上优于仅使用视觉的模型，且LLM生成的文本效果更佳。我们还发布了一个包含超过60,000个DHS集群的大规模多模态数据集，以支持未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2508.00024', 'title': 'Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine\n  Learning', 'url': 'https://huggingface.co/papers/2508.00024', 'abstract': 'Combining Vision Transformer embeddings with quantum-classical pipelines achieves quantum advantage in classification tasks, demonstrating the importance of embedding choice in quantum machine learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.', 'score': 1, 'issue_id': 5185, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'c9825e1a6f4d2a1c', 'authors': ['Sebastián Andrés Cajas Ordóñez', 'Luis Fernando Torres Torres', 'Mario Bifulco', 'Carlos Andrés Durán', 'Cristian Bosch', 'Ricardo Simón Carbajo'], 'affiliations': ['Corporation for Aerospace Initiatives (CASIRI), University of Cauca, Popayán, Colombia', 'Department of Computer Science, University of Torino, Torino, Italy', 'National Irish Centre for AI (CeADAR), University College Dublin (UCD), Dublin, Ireland', 'SISTEMIC Research Group, University of Antioquia, Medellín, Colombia'], 'pdf_title_img': 'assets/pdf/title_img/2508.00024.jpg', 'data': {'categories': ['#cv', '#games', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Квантовое преимущество через синергию трансформеров и квантовых пространств признаков', 'desc': 'Статья представляет новый подход к квантовому машинному обучению, объединяющий эмбеддинги Vision Transformer с квантово-классическими пайплайнами. Авторы демонстрируют, что такой метод позволяет достичь квантового преимущества в задачах классификации, превосходя классические SVM на наборах данных Fashion-MNIST и MNIST. Ключевым открытием является то, что эмбеддинги ViT уникальным образом обеспечивают квантовое преимущество, в то время как признаки CNN показывают снижение производительности. Исследование подчеркивает важность выбора эмбеддингов в квантовом машинном обучении и открывает путь к масштабируемым квантовым алгоритмам, использующим современные нейронные архитектуры.'}, 'en': {'title': 'Unlocking Quantum Advantage with Vision Transformers', 'desc': 'This paper explores how combining Vision Transformer (ViT) embeddings with quantum-classical pipelines can enhance classification tasks in machine learning. It highlights the challenges faced by Quantum Support Vector Machines (QSVMs) due to high-dimensional quantum states and hardware limitations. The authors introduce a method that uses class-balanced k-means distillation alongside pretrained ViT embeddings, which significantly improves accuracy compared to classical SVMs. Their findings suggest that the choice of embedding is crucial for achieving quantum advantage, demonstrating a beneficial interaction between transformer attention mechanisms and quantum feature spaces.'}, 'zh': {'title': '量子机器学习中的嵌入选择与优势', 'desc': '本文探讨了将视觉变换器（Vision Transformer）嵌入与量子-经典管道结合的方式，以在分类任务中实现量子优势。研究表明，嵌入的选择对量子机器学习至关重要，使用ViT嵌入可以在Fashion-MNIST数据集上提高8.02%的准确率，而在MNIST数据集上提高4.42%。相比之下，卷积神经网络（CNN）特征的表现却有所下降。通过使用16量子比特的张量网络模拟，本文首次系统性地证明了量子核优势与嵌入选择之间的关键关系，揭示了变换器注意力与量子特征空间之间的基本协同作用。'}}}, {'id': 'https://huggingface.co/papers/2508.01773', 'title': 'Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2508.01773', 'abstract': "An uncertainty-driven framework for automated process reward data construction and aggregation methods improves the effectiveness and efficiency of Process-Level Reward Models in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.", 'score': 0, 'issue_id': 5186, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '872cba646c3b0c3d', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01773.jpg', 'data': {'categories': ['#math', '#training', '#dataset', '#reasoning', '#optimization', '#data'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических рассуждений ИИ через неопределенность', 'desc': 'Статья представляет новый подход к построению и агрегации данных для обучения моделей вознаграждения на уровне процесса (PRM) в задачах математических рассуждений. Авторы предлагают фреймворк, основанный на неопределенности, для автоматизированного создания данных о вознаграждениях процесса. Также введены два новых метода агрегации выходных данных: гибридное мажоритарное голосование по вознаграждениям и взвешенное частотное голосование по вознаграждениям. Эксперименты на нескольких наборах данных показывают эффективность предложенного подхода в улучшении способностей моделей к математическим рассуждениям.'}, 'en': {'title': 'Automating Reward Data for Smarter Math Reasoning', 'desc': 'This paper presents a new framework that automates the creation of process reward data, which is essential for training Process-Level Reward Models (PRMs) in mathematical reasoning tasks. The authors highlight the challenges of existing data construction methods, which are often time-consuming and inefficient. They introduce two innovative output aggregation techniques, Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, that enhance the performance of PRMs by effectively combining the strengths of traditional voting methods with PRM evaluations. Experimental results demonstrate that this uncertainty-driven approach significantly improves both the quality of the reward data and the reasoning capabilities of the models tested.'}, 'zh': {'title': '基于不确定性的自动化过程奖励数据构建框架', 'desc': '本文提出了一种基于不确定性的框架，用于自动化过程奖励数据的构建和聚合方法，以提高过程级奖励模型在数学推理任务中的有效性和效率。过程级奖励模型（PRMs）通过在每个中间步骤提供监督和评估，显著提升了模型的推理能力。然而，构建高质量的过程奖励数据通常需要耗费大量人力，现有方法效率低下。我们还提出了两种通用的不确定性感知输出聚合方法，进一步增强了PRMs的数学推理能力。'}}}, {'id': 'https://huggingface.co/papers/2507.16290', 'title': 'Dens3R: A Foundation Model for 3D Geometry Prediction', 'url': 'https://huggingface.co/papers/2507.16290', 'abstract': 'Dens3R is a 3D foundation model that jointly predicts multiple geometric quantities using a two-stage training framework, enhancing consistency and performance in dense 3D reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains a major challenge. Most existing methods are limited to predicting a single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore a unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, a 3D foundation model designed for joint geometric dense prediction and adaptable to a wide range of downstream tasks. Dens3R adopts a two-stage training framework to progressively build a pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design a lightweight shared encoder-decoder backbone and introduce position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose a post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various dense 3D prediction tasks and highlight its potential for broader applications.', 'score': 0, 'issue_id': 5189, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': 'd47ef3bd9b4560f6', 'authors': ['Xianze Fang', 'Jingnan Gao', 'Zhe Wang', 'Zhuo Chen', 'Xingyu Ren', 'Jiangjing Lyu', 'Qiaomu Ren', 'Zhonglei Yang', 'Xiaokang Yang', 'Yichao Yan', 'Chengfei Lyu'], 'affiliations': ['Alibaba Group, China', 'Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.16290.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧊', 'ru': {'title': 'Единая модель для точной 3D-реконструкции', 'desc': 'Dens3R - это модель для совместного предсказания нескольких геометрических характеристик в задачах плотной 3D-реконструкции. Она использует двухэтапную схему обучения для повышения согласованности и производительности. Dens3R применяет легковесную архитектуру энкодер-декодер и позиционно-интерполированное роторное позиционное кодирование. Модель способна точно регрессировать множество геометрических величин, таких как нормали поверхности и глубина, обеспечивая согласованное восприятие геометрии как для одного, так и для нескольких ракурсов.'}, 'en': {'title': 'Dens3R: Unified Predictions for Consistent 3D Geometry', 'desc': "Dens3R is a 3D foundation model that improves dense 3D reconstruction by predicting multiple geometric quantities together, such as depth and surface normals. It uses a two-stage training framework to enhance the consistency and accuracy of these predictions, addressing the limitations of existing methods that focus on single geometry predictions. By modeling the relationships between different geometric properties, Dens3R ensures that the predictions are coherent and reliable. The model's design includes a lightweight encoder-decoder and advanced encoding techniques, making it adaptable for various applications in 3D geometry tasks."}, 'zh': {'title': 'Dens3R：联合几何预测的3D基础模型', 'desc': 'Dens3R是一种3D基础模型，旨在通过两阶段训练框架联合预测多个几何量，从而提高密集3D重建任务中的一致性和性能。现有方法通常只能从输入图像中预测单一几何量，而Dens3R则通过建模不同几何属性之间的结构耦合，实现了联合回归。该模型采用轻量级共享编码器-解码器架构，并引入位置插值旋转位置编码，以增强对高分辨率输入的鲁棒性。实验结果表明，Dens3R在多种密集3D预测任务中表现优越，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.23726', 'title': 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2507.23726', 'abstract': 'Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.', 'score': 84, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'ab5bfbdad68eb6bf', 'authors': ['Luoxin Chen', 'Jinming Gu', 'Liankai Huang', 'Wenhao Huang', 'Zhicheng Jiang', 'Allan Jie', 'Xiaoran Jin', 'Xing Jin', 'Chenggang Li', 'Kaijing Ma', 'Cheng Ren', 'Jiawei Shen', 'Wenlei Shi', 'Tong Sun', 'He Sun', 'Jiahui Wang', 'Siran Wang', 'Zhihong Wang', 'Chenrui Wei', 'Shufa Wei', 'Yonghui Wu', 'Yuchen Wu', 'Yihang Xia', 'Huajian Xin', 'Fan Yang', 'Huaiyuan Ying', 'Hongyi Yuan', 'Zheng Yuan', 'Tianyang Zhan', 'Chi Zhang', 'Yue Zhang', 'Ge Zhang', 'Tianyun Zhao', 'Jianqiu Zhao', 'Yichi Zhou', 'Thomas Hanwen Zhu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2507.23726.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматическом доказательстве теорем с помощью ИИ', 'desc': 'Seed-Prover - это модель для автоматического доказательства теорем, использующая язык Lean. Она применяет итеративное уточнение доказательств и специализированную поддержку геометрии. Модель достигает высокой производительности в формальном доказательстве теорем и автоматизированных математических рассуждениях. Seed-Prover превосходит предыдущие системы на нескольких эталонных наборах задач, включая формализованные задачи Международной математической олимпиады.'}, 'en': {'title': 'Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement', 'desc': 'The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning.'}, 'zh': {'title': 'Seed-Prover：自动化数学推理的新突破', 'desc': 'Seed-Prover是一种基于Lean的引理风格推理模型，能够在形式定理证明和自动数学推理中实现高性能。该模型通过迭代优化和专门的几何支持，克服了传统自然语言推理的局限性。Seed-Prover利用Lean的反馈和自我总结来不断改进其证明过程，并设计了三种推理策略以应对国际数学奥林匹克（IMO）级别的问题。通过引入Seed-Geometry几何推理引擎，Seed-Prover在几何问题上也取得了显著进展，展示了形式验证与长链推理的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.23779', 'title': 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding', 'url': 'https://huggingface.co/papers/2507.23779', 'abstract': 'The Phi-Ground model family achieves state-of-the-art performance in GUI grounding for multimodal reasoning models, improving accuracy across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from "Iron Man", are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textbf{43.2} on ScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}', 'score': 35, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e6bd9c919aacc874', 'authors': ['Miaosen Zhang', 'Ziqiang Xu', 'Jialiang Zhu', 'Qi Dai', 'Kai Qiu', 'Yifan Yang', 'Chong Luo', 'Tianyi Chen', 'Justin Wagle', 'Tim Franklin', 'Baining Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.23779.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#reasoning', '#training', '#multimodal'], 'emoji': '🖥️', 'ru': {'title': 'Phi-Ground: прорыв в точности привязки GUI для ИИ-агентов', 'desc': 'Семейство моделей Phi-Ground достигает передовых результатов в задаче привязки графического интерфейса для мультимодальных моделей рассуждения. Эти модели улучшают точность на различных бенчмарках для компьютерных агентов, способных взаимодействовать с GUI. Авторы провели эмпирическое исследование процесса обучения моделей привязки, рассмотрев детали от сбора данных до тренировки. Результаты работы могут быть полезны не только для создания моделей привязки, но и для других задач восприятия.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Phi-Ground Models', 'desc': 'The Phi-Ground model family significantly enhances GUI grounding for multimodal reasoning models, achieving top performance on various benchmarks. This model is crucial for Computer Use Agents (CUAs) to perform tasks like clicking and typing accurately, which is essential for their effectiveness. Despite existing models struggling with accuracy below 65% on tough benchmarks, Phi-Ground demonstrates superior results, scoring 43.2 on ScreenSpot-pro and 27.2 on UI-Vision. The paper details the training process and insights gained, which can also aid in improving other perception tasks in machine learning.'}, 'zh': {'title': 'Phi-Ground：多模态推理的GUI定位新突破', 'desc': 'Phi-Ground模型系列在多模态推理模型的GUI定位方面达到了最先进的性能，提升了在多个基准测试中的准确性。GUI定位是计算机使用代理（CUA）执行实际操作的核心部分，直接影响系统的成功与否。当前的端到端定位模型在一些具有挑战性的基准测试中准确率仍低于65%，显示出其在实际应用中的不足。本文通过对定位模型的训练进行实证研究，最终开发出Phi-Ground模型系列，在代理设置下的五个定位基准测试中均取得了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.22879', 'title': 'RecGPT Technical Report', 'url': 'https://huggingface.co/papers/2507.22879', 'abstract': "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.", 'score': 22, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '2bd5536810f1694b', 'authors': ['Chao Yi', 'Dian Chen', 'Gaoyang Guo', 'Jiakai Tang', 'Jian Wu', 'Jing Yu', 'Mao Zhang', 'Sunhao Dai', 'Wen Chen', 'Wenjun Yang', 'Yuning Jiang', 'Zhujin Gao', 'Bo Zheng', 'Chi Li', 'Dimin Wang', 'Dixuan Wang', 'Fan Li', 'Fan Zhang', 'Haibin Chen', 'Haozhuang Liu', 'Jialin Zhu', 'Jiamang Wang', 'Jiawei Wu', 'Jin Cui', 'Ju Huang', 'Kai Zhang', 'Kan Liu', 'Lang Tian', 'Liang Rao', 'Longbin Li', 'Lulu Zhao', 'Na He', 'Peiyang Wang', 'Qiqi Huang', 'Tao Luo', 'Wenbo Su', 'Xiaoxiao He', 'Xin Tong', 'Xu Chen', 'Xunke Xi', 'Yang Li', 'Yaxuan Wu', 'Yeqiu Yang', 'Yi Hu', 'Yinnan Song', 'Yuchen Li', 'Yujie Luo', 'Yujin Yuan', 'Yuliang Yan', 'Zhengyang Wang', 'Zhibo Xiao', 'Zhixin Ma', 'Zile Zhou', 'Ziqi Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.22879.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#alignment', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'RecGPT: Рекомендации, ориентированные на намерения пользователей', 'desc': 'RecGPT - это новая система рекомендаций, интегрирующая большие языковые модели (LLM) для фокусировки на намерениях пользователей. Она улучшает разнообразие контента и удовлетворенность пользователей, а также повышает эффективность для продавцов и платформы. RecGPT использует многоэтапную парадигму обучения, включающую предварительное выравнивание с усиленным рассуждением и эволюцию самообучения. Система уже развернута в приложении Taobao и показывает стабильный рост производительности для всех заинтересованных сторон.'}, 'en': {'title': 'Empowering Recommendations with User Intent', 'desc': 'RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem.'}, 'zh': {'title': '以用户意图为中心的推荐系统新范式', 'desc': 'RecGPT 是一种将大型语言模型整合到推荐系统中的新框架，旨在关注用户意图。通过重新设计推荐流程，RecGPT 使推荐过程从单纯依赖历史数据转变为以用户意图为中心。该系统通过多阶段训练方法，结合推理增强的预对齐和自我训练，提升了推荐的准确性和多样性。实验结果表明，RecGPT 在用户满意度、商家曝光率和平台转化率等方面均取得了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2507.23682', 'title': 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models', 'url': 'https://huggingface.co/papers/2507.23682', 'abstract': 'The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.', 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'baa73e4730b01a97', 'authors': ['Xiaoyu Chen', 'Hangxing Wei', 'Pushi Zhang', 'Chuheng Zhang', 'Kaixin Wang', 'Yanjiang Guo', 'Rushuai Yang', 'Yucen Wang', 'Xinquan Xiao', 'Li Zhao', 'Jianyu Chen', 'Jiang Bian'], 'affiliations': ['Hong Kong University of Science and Technology', 'Microsoft Research', 'Nanjing University', 'Tsinghua University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23682.jpg', 'data': {'categories': ['#optimization', '#agents', '#agi', '#games', '#robotics', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'ViLLA: Улучшение роботизированных манипуляций с помощью латентных действий', 'desc': 'Фреймворк ViLLA улучшает модели визуально-языкового действия (VLA) путем включения латентных действий. Это позволяет повысить производительность как в симулированных, так и в реальных задачах роботизированных манипуляций. Предложенный подход villa-X совершенствует как обучение латентным действиям, так и их интеграцию в предобучение VLA. Модель демонстрирует превосходные результаты в симуляторах SIMPLER и LIBERO, а также на реальных роботах с захватами и ловкими руками.'}, 'en': {'title': 'Enhancing Robot Manipulation with Latent Actions in ViLLA Framework', 'desc': 'The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research.'}, 'zh': {'title': 'ViLLA框架：提升机器人操作的潜力', 'desc': 'ViLLA框架通过引入潜在动作来增强视觉-语言-动作（VLA）模型，从而提高机器人操作任务的性能。潜在动作是一种抽象表示，能够捕捉两个帧之间的视觉变化。本文介绍的villa-X是一个新颖的视觉-语言-潜在动作框架，旨在改进潜在动作建模，以学习可推广的机器人操作策略。我们的研究表明，villa-X在模拟环境和真实机器人设置中均表现出色，展示了ViLLA范式的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.22968', 'title': 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations', 'url': 'https://huggingface.co/papers/2507.22968', 'abstract': "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.", 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '3a2f5273d610d5d6', 'authors': ['Chengqian Ma', 'Wei Tao', 'Yiwen Guo'], 'affiliations': ['Independent Researcher', 'LIGHTSPEED', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22968.jpg', 'data': {'categories': ['#survey', '#long_context', '#benchmark', '#dataset', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'Бенчмарк для оценки разговорных ИИ-моделей в реальных условиях', 'desc': 'В статье представлен набор данных для оценки разговорных диалоговых моделей на английском и китайском языках. Этот бенчмарк позволяет оценить способность моделей понимать и имитировать человеческие разговоры, учитывая такие сложности как неоднозначность и контекстная зависимость. Набор данных содержит 1079 примеров и сопровождается методом оценки на основе больших языковых моделей, который хорошо коррелирует с человеческими оценками. Исследование направлено на комплексное изучение эффективности разговорных диалоговых моделей в решении практических задач.'}, 'en': {'title': 'Benchmarking Spoken Dialogue Models for Real-World Conversations', 'desc': "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."}, 'zh': {'title': '提升口语对话模型的评估标准', 'desc': '本文提出了一个用于评估口语对话模型（SDMs）性能的基准数据集，涵盖英语和中文，旨在理解和模拟人类口语对话。口语对话的复杂性体现在歧义性和上下文依赖性等挑战上，这些因素使得与文本基础的大型语言模型（LLMs）相比，SDMs的研究相对较少。数据集中包含1079个实例，并配备了一种基于LLM的评估方法，以更好地与人类判断相一致。通过这个数据集，研究者可以全面探讨SDMs在应对实际对话挑战中的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.23277', 'title': 'iLRM: An Iterative Large 3D Reconstruction Model', 'url': 'https://huggingface.co/papers/2507.23277', 'abstract': 'iLRM, an iterative Large 3D Reconstruction Model, improves scalability and efficiency in 3D reconstruction by decoupling scene representation, using a two-stage attention scheme, and injecting high-resolution information.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.', 'score': 19, 'issue_id': 5137, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'af6d37e5e25a6d73', 'authors': ['Gyeongjin Kang', 'Seungtae Nam', 'Xiangyu Sun', 'Sameh Khamis', 'Abdelrahman Mohamed', 'Eunbyung Park'], 'affiliations': ['Rembrand Project', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23277.jpg', 'data': {'categories': ['#optimization', '#3d'], 'emoji': '🏛️', 'ru': {'title': 'Эффективная 3D-реконструкция: итеративный подход с гауссовым представлением', 'desc': 'iLRM - это итеративная модель для масштабируемой и эффективной 3D-реконструкции. Она использует трехмерное гауссово представление сцены, двухэтапную схему внимания и внедрение высокоразрешающей информации. Модель решает проблемы масштабируемости, характерные для трансформерных архитектур при обработке множества ракурсов. Эксперименты показывают превосходство iLRM по качеству и скорости реконструкции по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with iLRM: Scalable and Efficient!', 'desc': 'The paper presents the iterative Large 3D Reconstruction Model (iLRM), which enhances the scalability and efficiency of 3D reconstruction processes. It achieves this by decoupling the scene representation from the input images, allowing for more compact 3D models. Additionally, iLRM employs a two-stage attention mechanism to minimize computational costs associated with multi-view interactions. By incorporating high-resolution information at each layer, the model ensures high-fidelity reconstructions while maintaining superior performance across various datasets.'}, 'zh': {'title': 'iLRM：高效可扩展的3D重建新模型', 'desc': 'iLRM（迭代大型3D重建模型）通过解耦场景表示、采用两阶段注意力机制和注入高分辨率信息，提升了3D重建的可扩展性和效率。该模型通过迭代优化生成3D高斯表示，能够在多个输入视图下实现快速且高质量的重建。与传统的全注意力机制相比，iLRM显著降低了计算成本，同时保持了重建的高保真度。实验结果表明，iLRM在重建质量和速度上均优于现有方法，尤其在处理更多输入视图时表现出更好的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2507.21509', 'title': 'Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models', 'url': 'https://huggingface.co/papers/2507.21509', 'abstract': "Persona vectors in large language models can monitor and control personality changes during training and deployment, enabling the identification and mitigation of undesirable traits.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.", 'score': 17, 'issue_id': 5127, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '8088854aaf027260', 'authors': ['Runjin Chen', 'Andy Arditi', 'Henry Sleight', 'Owain Evans', 'Jack Lindsey'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'Constellation', 'Truthful AI', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.21509.jpg', 'data': {'categories': ['#rlhf', '#hallucinations', '#data', '#ethics', '#training', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Векторы персоны: ключ к контролю личности ИИ-ассистентов', 'desc': 'Статья представляет концепцию векторов персоны в больших языковых моделях. Эти векторы позволяют отслеживать и контролировать изменения личности ассистента во время обучения и использования модели. Исследователи обнаружили, что векторы персоны могут предсказывать сдвиги в личности после дообучения и помогают выявлять нежелательные черты. Метод извлечения векторов персоны автоматизирован и может применяться к любой интересующей черте личности.'}, 'en': {'title': 'Controlling AI Personalities with Persona Vectors', 'desc': "This paper introduces the concept of persona vectors in large language models, which are used to track and manage personality traits during the model's training and deployment phases. The authors demonstrate that these vectors can identify undesirable traits like harmful behavior or excessive flattery by analyzing the model's activation space. They show that personality shifts can be predicted and controlled, allowing for interventions to mitigate negative changes. Additionally, the method can flag problematic training data that may lead to these undesirable personality traits, making it a valuable tool for improving AI behavior."}, 'zh': {'title': '利用人格向量控制语言模型的人格变化', 'desc': '本文探讨了在大型语言模型中使用人格向量来监控和控制模型在训练和部署过程中的人格变化。研究发现，模型的激活空间中存在与多种人格特征相关的人格向量，例如恶意、谄媚和幻觉倾向。通过这些向量，可以预测和控制在微调后可能出现的人格变化，并且可以通过后期干预来减轻这些变化。该方法是自动化的，可以应用于任何感兴趣的人格特征，只需提供自然语言描述。'}}}, {'id': 'https://huggingface.co/papers/2507.23698', 'title': 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents', 'url': 'https://huggingface.co/papers/2507.23698', 'abstract': "Reinforcement Learning enhances generalizable spatial reasoning and interaction in 3D environments through cross-view goal specification and automated task synthesis, achieving zero-shot generalization and improved interaction success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.", 'score': 7, 'issue_id': 5125, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '4cb697aecb943154', 'authors': ['Shaofei Cai', 'Zhancun Mu', 'Haiwen Xia', 'Bowei Zhang', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'School of Computing, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23698.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#games', '#3d', '#rl'], 'emoji': '🧠', 'ru': {'title': 'RL открывает новые горизонты пространственного мышления для ИИ', 'desc': 'Данная статья представляет метод улучшения пространственного мышления и взаимодействия агентов в 3D-средах с помощью обучения с подкреплением (RL). Авторы предлагают использовать кросс-видовую спецификацию целей и автоматизированный синтез задач для достижения обобщения без предварительного обучения. Эксперименты проводились в среде Minecraft и показали значительное улучшение успешности взаимодействия агентов. Результаты демонстрируют потенциал обучения с подкреплением для развития визуально-моторных навыков искусственных агентов.'}, 'en': {'title': 'Empowering 3D Agents with Reinforcement Learning for Generalized Interaction', 'desc': 'This paper discusses how Reinforcement Learning (RL) can improve the ability of agents to understand and interact in 3D environments, like Minecraft. It addresses the problem of RL models overfitting to specific tasks, which limits their ability to generalize to new situations. By using cross-view goal specification and automated task synthesis, the authors show that RL can help agents achieve zero-shot generalization, meaning they can perform well in unseen environments without prior training. The results indicate that RL can significantly enhance interaction success rates and spatial reasoning in diverse settings, including real-world applications.'}, 'zh': {'title': '强化学习：提升3D环境中的空间推理与交互能力', 'desc': '强化学习（RL）在3D环境中通过跨视角目标指定和自动化任务合成，增强了可推广的空间推理和交互能力，实现了零样本泛化和提高的交互成功率。本文探讨了RL在Minecraft中微调的视觉运动代理如何在未见过的世界中实现零样本泛化。我们分析并建立了跨视角目标指定作为视觉运动策略的统一多任务目标空间，以应对多任务RL表示中的挑战。此外，我们提出在高度可定制的Minecraft环境中进行自动化任务合成，以支持大规模多任务RL训练。'}}}, {'id': 'https://huggingface.co/papers/2507.23374', 'title': 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2507.23374', 'abstract': 'NeRF-GS combines Neural Radiance Fields and 3D Gaussian Splatting to enhance 3D scene representation and performance through joint optimization and shared spatial information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.', 'score': 6, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '3bc4d2b12fc82c0f', 'authors': ['Shuangkang Fang', 'I-Chao Shen', 'Takeo Igarashi', 'Yufeng Wang', 'ZeSheng Wang', 'Yi Yang', 'Wenrui Ding', 'Shuchang Zhou'], 'affiliations': ['Beihang University', 'StepFun', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2507.23374.jpg', 'data': {'categories': ['#3d', '#benchmark'], 'emoji': '🌟', 'ru': {'title': 'NeRF-GS: Синергия нейронных полей и гауссова сплаттинга для революционного 3D-моделирования', 'desc': 'NeRF-GS - это новая система, объединяющая нейронные радиационные поля (NeRF) и трехмерное гауссово сплаттинг (3DGS) для улучшения представления 3D-сцен. Она использует непрерывное пространственное представление NeRF для устранения ограничений 3DGS, таких как чувствительность к инициализации гауссианов и слабые межгауссовые корреляции. NeRF-GS оптимизирует обе модели, используя общую пространственную информацию, и вводит оптимизацию остаточных векторов для улучшения персонализированных возможностей 3DGS. Экспериментальные результаты показывают, что NeRF-GS превосходит существующие методы и достигает наилучших показателей в представлении 3D-сцен.'}, 'en': {'title': 'Enhancing 3D Scene Representation with NeRF-GS', 'desc': 'NeRF-GS is a new framework that combines Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) to improve how 3D scenes are represented. By optimizing both methods together, it addresses the weaknesses of 3DGS, such as its sensitivity to initial conditions and limited understanding of spatial relationships. The framework aligns the spatial features of 3DGS with those of NeRF, allowing for better performance through shared information. Experiments show that NeRF-GS outperforms existing techniques, highlighting the benefits of integrating these two approaches for enhanced 3D scene representation.'}, 'zh': {'title': 'NeRF-GS：融合神经辐射场与三维高斯点云的创新框架', 'desc': 'NeRF-GS是一个新颖的框架，它结合了神经辐射场（NeRF）和三维高斯点云（3DGS），通过联合优化和共享空间信息来增强三维场景的表示和性能。该框架利用NeRF的连续空间表示，克服了3DGS的一些局限性，如对高斯初始化的敏感性和空间意识的不足。通过逐步对齐3DGS的空间特征与NeRF，NeRF-GS使得两种表示能够在同一场景中共同优化。实验结果表明，NeRF-GS在基准数据集上超越了现有方法，达到了最先进的性能，证明了NeRF和3DGS是互补的，而非竞争的。'}}}, {'id': 'https://huggingface.co/papers/2507.21584', 'title': 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination\n  Reduction in MLLMs', 'url': 'https://huggingface.co/papers/2507.21584', 'abstract': 'TARS, a token-adaptive preference strategy, improves multimodal large language models by reducing hallucinations through min-max optimization under semantic constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics.', 'score': 6, 'issue_id': 5128, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': 'e9b8a4abec301022', 'authors': ['Kejia Zhang', 'Keda Tao', 'Zhiming Luo', 'Chang Liu', 'Jiasheng Tang', 'Huan Wang'], 'affiliations': ['AWS AI Lab, Amazon', 'DAMO Academy, Alibaba Group', 'Department of Artificial Intelligence, Xiamen University', 'Hupan Laboratory', 'School of Engineering, Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2507.21584.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#multimodal', '#benchmark', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'TARS: Адаптивная оптимизация для борьбы с галлюцинациями в мультимодальных ИИ', 'desc': 'TARS - это новая стратегия оптимизации предпочтений для мультимодальных больших языковых моделей. Она использует min-max оптимизацию с семантическими ограничениями для уменьшения галлюцинаций. TARS максимизирует сдвиги распределения на уровне токенов, одновременно минимизируя ожидаемые потери предпочтений. Это позволяет сохранить причинно-следственную связь и уменьшить переобучение на паттернах предпочтений, снижая уровень галлюцинаций в мультимодальных рассуждениях.'}, 'en': {'title': 'TARS: Reducing Hallucinations in MLLMs with Adaptive Preferences', 'desc': 'The paper introduces TARS, a novel token-adaptive preference strategy designed to enhance multimodal large language models (MLLMs) by minimizing hallucinations. TARS reformulates direct preference optimization (DPO) as a min-max optimization problem, allowing for dynamic adjustments to token-level distributions while adhering to semantic constraints. This approach helps prevent overfitting to fixed preference patterns, which can lead to misleading outputs, by introducing controlled perturbations that maintain causal grounding. The results demonstrate that TARS significantly reduces hallucination rates and improves performance on various benchmarks, outperforming traditional DPO methods.'}, 'zh': {'title': 'TARS：减少幻觉的智能偏好策略', 'desc': 'TARS是一种基于令牌自适应的偏好策略，旨在通过在语义约束下进行最小-最大优化来减少多模态大语言模型中的幻觉现象。传统的直接偏好优化（DPO）方法通常将幻觉相关的偏好视为固定目标，导致模型过拟合于表面语言线索。TARS通过重新构建DPO为最小-最大优化问题，最大化令牌级别的分布变化，同时最小化期望的偏好损失，从而保持因果基础并减少幻觉。实验表明，TARS在多个基准测试中表现优异，显著降低了幻觉率。'}}}, {'id': 'https://huggingface.co/papers/2507.20519', 'title': 'AgroBench: Vision-Language Model Benchmark in Agriculture', 'url': 'https://huggingface.co/papers/2507.20519', 'abstract': 'AgroBench evaluates vision-language models across agricultural tasks, revealing areas for improvement in fine-grained identification, particularly weed identification, with expert-annotated categories.  \t\t\t\t\tAI-generated summary \t\t\t\t Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), a benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers a state-of-the-art range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal that VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most open-source VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/AgroBenchPage/ .', 'score': 4, 'issue_id': 5125, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'efa19cc739cbe95e', 'authors': ['Risa Shinoda', 'Nakamasa Inoue', 'Hirokatsu Kataoka', 'Masaki Onishi', 'Yoshitaka Ushiku'], 'affiliations': ['Kyoto University', 'National Institute of Advanced Industrial Science and Technology (AIST)', 'OMRON SINIC', 'The University of Osaka', 'Tokyo Institute of Technology', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2507.20519.jpg', 'data': {'categories': ['#cv', '#open_source', '#science', '#dataset', '#benchmark'], 'emoji': '🌾', 'ru': {'title': 'AgroBench: экспертная оценка ИИ в сельском хозяйстве', 'desc': 'AgroBench - это новый эталонный тест для оценки моделей компьютерного зрения и обработки естественного языка в сельскохозяйственных задачах. Он охватывает семь сельскохозяйственных тем и включает 203 категории культур и 682 категории болезней, аннотированные экспертами-агрономами. Тестирование показало, что существующие модели имеют значительные возможности для улучшения в задачах точной идентификации, особенно при распознавании сорняков. Авторы анализируют типы ошибок моделей и предлагают пути для их дальнейшего развития.'}, 'en': {'title': 'Enhancing Agricultural AI: The AgroBench Benchmark', 'desc': 'AgroBench is a benchmark designed to evaluate vision-language models (VLMs) specifically in the context of agricultural tasks. It focuses on fine-grained identification, such as accurately recognizing different types of weeds and diseases in crops, using categories annotated by expert agronomists. The benchmark includes a comprehensive set of 203 crop categories and 682 disease categories, highlighting the current limitations of VLMs in these areas. The findings indicate that many existing VLMs struggle with precise identification, particularly in weed detection, suggesting significant opportunities for improvement in future model development.'}, 'zh': {'title': '提升农业任务中的视觉-语言模型表现', 'desc': 'AgroBench是一个用于评估视觉-语言模型（VLM）在农业任务中的表现的基准。它涵盖了七个农业主题，并由专家农学家进行标注，确保数据的准确性。研究发现，当前的VLM在细粒度识别任务，特别是杂草识别方面表现不佳，许多开源模型的表现接近随机。通过分析VLM的错误类型，AgroBench为未来的模型发展提供了改进的方向。'}}}, {'id': 'https://huggingface.co/papers/2507.23436', 'title': 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification', 'url': 'https://huggingface.co/papers/2507.23436', 'abstract': "Enhancing dual-teacher self-supervised frameworks with Kolmogorov-Arnold Networks improves art style classification by better modeling nonlinear feature correlations and disentangling complex style manifolds.  \t\t\t\t\tAI-generated summary \t\t\t\t Art style classification remains a formidable challenge in computational aesthetics due to the scarcity of expertly labeled datasets and the intricate, often nonlinear interplay of stylistic elements. While recent dual-teacher self-supervised frameworks reduce reliance on labeled data, their linear projection layers and localized focus struggle to model global compositional context and complex style-feature interactions. We enhance the dual-teacher knowledge distillation framework to address these limitations by replacing conventional MLP projection and prediction heads with Kolmogorov-Arnold Networks (KANs). Our approach retains complementary guidance from two teacher networks, one emphasizing localized texture and brushstroke patterns, the other capturing broader stylistic hierarchies while leveraging KANs' spline-based activations to model nonlinear feature correlations with mathematical precision. Experiments on WikiArt and Pandora18k demonstrate that our approach outperforms the base dual teacher architecture in Top-1 accuracy. Our findings highlight the importance of KANs in disentangling complex style manifolds, leading to better linear probe accuracy than MLP projections.", 'score': 3, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '43c3429e74f56b34', 'authors': ['Abdellah Zakaria Sellam', 'Salah Eddine Bekhouche', 'Cosimo Distante', 'Abdelmalik Taleb-Ahmed'], 'affiliations': ['Department of Innovation Engineering, University of Salento', 'Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, 73100 Lecce, Italy', 'UPV/EHU, University of the Basque Country, 20018 San Sebastian, Spain', 'Université Polytechnique Hauts-de-France, Université de Lille, CNRS, 59313 Valenciennes, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.23436.jpg', 'data': {'categories': ['#cv', '#math', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Сети Колмогорова-Арнольда улучшают самообучение в классификации стилей искусства', 'desc': 'Статья представляет усовершенствованный метод самообучения с двумя учителями для классификации стилей искусства. Авторы заменяют стандартные MLP-слои на сети Колмогорова-Арнольда (KAN) для лучшего моделирования нелинейных корреляций признаков. Этот подход сохраняет преимущества двух сетей-учителей, фокусируясь как на локальных текстурах, так и на глобальных стилистических иерархиях. Эксперименты показывают превосходство предложенного метода над базовой архитектурой с двумя учителями в точности классификации.'}, 'en': {'title': 'Harnessing KANs for Superior Art Style Classification', 'desc': 'This paper presents an enhancement to dual-teacher self-supervised frameworks for art style classification by integrating Kolmogorov-Arnold Networks (KANs). The authors argue that traditional linear projection layers fail to capture the complex, nonlinear relationships between stylistic features. By using KANs, which utilize spline-based activations, the model can better represent these intricate correlations and disentangle complex style manifolds. Experimental results show that this improved framework significantly increases classification accuracy on datasets like WikiArt and Pandora18k compared to the original dual-teacher architecture.'}, 'zh': {'title': '利用KANs提升艺术风格分类的准确性', 'desc': '本论文提出了一种增强的双教师自监督框架，通过引入Kolmogorov-Arnold网络（KANs）来改善艺术风格分类。传统的线性投影层无法有效建模复杂的风格特征交互，而KANs能够更好地捕捉非线性特征相关性。我们的方法结合了两个教师网络的互补指导，一个专注于局部纹理和笔触模式，另一个则关注更广泛的风格层次。实验结果表明，使用KANs的框架在WikiArt和Pandora18k数据集上显著提高了分类准确率。'}}}, {'id': 'https://huggingface.co/papers/2507.23632', 'title': 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective', 'url': 'https://huggingface.co/papers/2507.23632', 'abstract': 'Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.', 'score': 2, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'b07ddf6cb6b8bee8', 'authors': ['Gabriel Mongaras', 'Eric C. Larson'], 'affiliations': ['Lyle School of Engineering Southern Methodist University Dallas, TX 75205'], 'pdf_title_img': 'assets/pdf/title_img/2507.23632.jpg', 'data': {'categories': ['#optimization', '#architecture', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая силу софтмакс-внимания через призму RNN', 'desc': 'Статья исследует различия между софтмакс-вниманием и линейным вниманием в нейронных сетях. Авторы показывают, что софтмакс-внимание можно представить в рекуррентной форме, аналогичной рекуррентным нейронным сетям (RNN). Это позволяет провести анализ компонентов софтмакс-внимания и объяснить его большую выразительность по сравнению с линейными аналогами. Работа помогает понять, почему софтмакс-внимание остается основой современных трансформерных архитектур, несмотря на квадратичную сложность.'}, 'en': {'title': 'Unlocking the Power of Softmax Attention', 'desc': "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."}, 'zh': {'title': '软max注意力的优势解析', 'desc': '本文探讨了softmax注意力与线性注意力的区别。softmax注意力因其表达能力强而成为现代变换器架构的基础，但其在序列长度上的计算复杂度和内存需求是一个主要缺点。通过将softmax非线性替换为线性注意力，研究者们试图解决这一瓶颈。本文表明，线性注意力实际上是softmax注意力的一种近似，并通过递归神经网络的语言来描述softmax注意力的各个部分，从而揭示其更强表达能力的原因。'}}}, {'id': 'https://huggingface.co/papers/2507.14793', 'title': 'Flow Equivariant Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2507.14793', 'abstract': "Equivariant neural network architectures are extended to handle time-parameterized transformations, improving performance in sequence models like RNNs.  \t\t\t\t\tAI-generated summary \t\t\t\t Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.", 'score': 2, 'issue_id': 5126, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'eb29bf11c603c730', 'authors': ['T. Anderson Keller'], 'affiliations': ['The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA 15213'], 'pdf_title_img': 'assets/pdf/title_img/2507.14793.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '⏳', 'ru': {'title': 'Эквивариантность во времени: новый подход к обработке последовательностей', 'desc': "Статья расширяет концепцию эквивариантных нейронных сетей для обработки преобразований, параметризованных во времени. Это улучшает производительность рекуррентных нейронных сетей (RNN) и других последовательностных моделей. Авторы вводят понятие 'потоков' - однопараметрических подгрупп Ли, описывающих естественные трансформации во времени, такие как визуальное движение. Эксперименты показывают, что потоково-эквивариантные модели значительно превосходят стандартные RNN по скорости обучения и способности к обобщению."}, 'en': {'title': 'Enhancing RNNs with Time-Parameter Equivariance', 'desc': 'This paper extends equivariant neural network architectures to include time-parameterized transformations, which enhances their performance in sequence models like recurrent neural networks (RNNs). It highlights that traditional RNNs do not adequately account for the smooth, continuous changes in data over time, leading to inefficiencies. By introducing flow equivariance, the authors demonstrate that these new models can better handle temporal symmetries, resulting in improved training speed and generalization capabilities. This work aims to create sequence models that align more closely with the natural transformations observed in the real world.'}, 'zh': {'title': '提升序列模型性能的时间等变网络', 'desc': '本文扩展了等变神经网络架构，以处理时间参数化的变换，从而提高序列模型（如RNN）的性能。我们发现标准的RNN通常不具备流等变性，无法以几何结构的方式对移动刺激进行变换。通过引入流等变性，我们的模型在训练速度、长度泛化和速度泛化等方面显著优于非等变模型。此研究为构建尊重时间参数化对称性的序列模型奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2507.23404', 'title': 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring', 'url': 'https://huggingface.co/papers/2507.23404', 'abstract': 'An enhanced Dense Passage Retrieval framework for Arabic uses a novel Attentive Relevance Scoring mechanism to improve retrieval performance and ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at https://github.com/Bekhouche/APR{GitHub}.', 'score': 1, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '5e9a40999faf8be8', 'authors': ['Salah Eddine Bekhouche', 'Azeddine Benlamoudi', 'Yazid Bounab', 'Fadi Dornaika', 'Abdenour Hadid'], 'affiliations': ['Faculty of Pharmacy, Helsinki University, Helsinki, Finland', 'IKERBASQUE, Basque Foundation for Science, Bilbao, Spain', 'Lab. de Genie Electrique (LAGE), University of Ouargla, Ouargla, Algeria', 'Sorbonne University Abu Dhabi, Abu Dhabi, UAE', 'University of the Basque Country UPV/EHU, San Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.23404.jpg', 'data': {'categories': ['#architecture', '#open_source', '#low_resource', '#multilingual', '#dataset'], 'emoji': '🕌', 'ru': {'title': 'Улучшенный поиск по арабским текстам с помощью механизма внимания', 'desc': 'Статья представляет усовершенствованную систему плотного поиска пассажей (Dense Passage Retrieval) для арабского языка. Авторы разработали новый механизм оценки релевантности на основе внимания (Attentive Relevance Scoring), который более эффективно моделирует семантическую связь между вопросами и текстовыми фрагментами. Система интегрирует предобученные языковые модели для арабского языка и архитектурные улучшения для повышения точности ранжирования при ответах на вопросы. Код проекта доступен в открытом репозитории на GitHub.'}, 'en': {'title': 'Enhancing Arabic Retrieval with Attentive Relevance Scoring', 'desc': 'This paper introduces an improved Dense Passage Retrieval (DPR) framework tailored for the Arabic language, addressing its unique challenges in natural language processing. The key innovation is the Attentive Relevance Scoring (ARS) mechanism, which enhances the way relevance is assessed between questions and passages. By utilizing pre-trained Arabic language models and refining the architecture, the framework boosts both retrieval performance and ranking accuracy. This advancement aims to better support information retrieval tasks in Arabic, a language that has been underrepresented in NLP research.'}, 'zh': {'title': '提升阿拉伯语检索性能的新方法', 'desc': '本文提出了一种针对阿拉伯语的增强型密集段落检索框架，旨在提高检索性能和排名准确性。我们引入了一种新颖的注意力相关评分机制，替代了传统的交互机制，更有效地建模问题与段落之间的语义相关性。该方法结合了预训练的阿拉伯语语言模型和架构改进，显著提升了回答阿拉伯语问题时的检索效果。我们的代码已公开，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2507.23257', 'title': 'Efficient Machine Unlearning via Influence Approximation', 'url': 'https://huggingface.co/papers/2507.23257', 'abstract': 'The paper introduces the Influence Approximation Unlearning (IAU) algorithm, which leverages incremental learning principles to efficiently address the computational challenges of influence-based unlearning in machine learning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU.', 'score': 0, 'issue_id': 5131, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e1e0a29f18521e64', 'authors': ['Jiawei Liu', 'Chenwang Wu', 'Defu Lian', 'Enhong Chen'], 'affiliations': ['Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23257.jpg', 'data': {'categories': ['#optimization', '#security', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективное разобучение через призму инкрементного обучения', 'desc': 'Статья представляет алгоритм Influence Approximation Unlearning (IAU) для эффективного машинного разобучения. IAU использует принципы инкрементного обучения, чтобы преодолеть вычислительные сложности разобучения на основе влияния в моделях машинного обучения. Алгоритм устанавливает теоретическую связь между запоминанием (инкрементное обучение) и забыванием (разобучение), что позволяет решать задачу разобучения с точки зрения инкрементного обучения. Эмпирические исследования показывают, что IAU достигает превосходного баланса между гарантией удаления, эффективностью разобучения и сохранением полезности модели.'}, 'en': {'title': 'Efficient Unlearning through Incremental Learning: Introducing IAU', 'desc': 'The paper presents the Influence Approximation Unlearning (IAU) algorithm, which aims to improve the efficiency of influence-based unlearning in machine learning models. It addresses the high computational costs associated with traditional methods that require extensive calculations of the Hessian matrix for each training sample. By drawing parallels between the processes of memorizing and forgetting, the authors leverage incremental learning techniques to facilitate more efficient unlearning. Empirical results show that IAU not only ensures effective data removal but also maintains model performance, outperforming existing unlearning methods.'}, 'zh': {'title': '高效遗忘：影响近似遗忘算法的创新之路', 'desc': '本文介绍了一种名为影响近似遗忘（IAU）算法的新方法，该算法利用增量学习的原理来高效解决基于影响的遗忘在机器学习模型中的计算挑战。随着隐私问题的日益关注，机器遗忘成为一个重要的研究领域，尤其是影响基于遗忘的方法因其无需重新训练模型而受到青睐。然而，现有方法在计算海森矩阵及其逆矩阵时面临巨大的计算开销，限制了其在大规模模型中的应用。通过建立记忆（增量学习）与遗忘（遗忘学习）之间的理论联系，IAU算法实现了更高效的机器遗忘，且在多个数据集和模型架构上表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.04026', 'title': 'VeriGUI: Verifiable Long-Chain GUI Dataset', 'url': 'https://huggingface.co/papers/2508.04026', 'abstract': 'VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.', 'score': 31, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '2692487ee60e017e', 'authors': ['Shunyu Liu', 'Minghao Liu', 'Huichi Zhou', 'Zhenyu Cui', 'Yang Zhou', 'Yuhao Zhou', 'Wendong Fan', 'Ge Zhang', 'Jiajun Shi', 'Weihao Xuan', 'Jiaxing Huang', 'Shuang Luo', 'Fang Wu', 'Heli Qi', 'Qingcheng Zeng', 'Ziqi Ren', 'Jialiang Gao', 'Jindi Lv', 'Junjie Wang', 'Aosong Feng', 'Heng Zhou', 'Wangchunshu Zhou', 'Zhenfei Yin', 'Wenlong Zhang', 'Guohao Li', 'Wenhao Yu', 'Irene Li', 'Lei Ma', 'Lei Bai', 'Qunshu Lin', 'Mingli Song', 'Dacheng Tao'], 'affiliations': ['VeriGUI Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.04026.jpg', 'data': {'categories': ['#games', '#agents', '#dataset', '#long_context'], 'emoji': '🖥️', 'ru': {'title': 'VeriGUI: Новый стандарт для оценки долгосрочных GUI-агентов', 'desc': 'VeriGUI - это новый набор данных для оценки GUI-агентов в долгосрочных задачах, акцентирующий внимание на сложности длинных цепочек действий и верифицируемости на уровне подзадач. Датасет состоит из траекторий GUI-задач для десктопных и веб-приложений, аннотированных экспертами. Он позволяет разрабатывать и оценивать универсальные GUI-агенты, работающие в реалистичных компьютерных средах. Эксперименты на VeriGUI с использованием различных агентов и языковых моделей выявили значительные пробелы в производительности при работе с долгосрочными задачами.'}, 'en': {'title': 'Empowering GUI Agents for Complex Tasks with VeriGUI', 'desc': 'VeriGUI is a new dataset created to test GUI agents on complex tasks that require many steps. It focuses on breaking down tasks into smaller, manageable subtasks that can be verified individually. This approach allows agents to start from any subtask, making it easier to explore different strategies. The dataset includes detailed task examples from desktop and web environments, showing that current agents struggle with long-term planning and decision-making.'}, 'zh': {'title': 'VeriGUI：长时间任务中的智能体评估新标准', 'desc': 'VeriGUI是一个新颖的数据集，用于评估在长时间任务中执行图形用户界面（GUI）操作的智能体。该数据集强调了长链复杂性和子任务级可验证性，允许任务被分解为数百个相互依赖的子任务。通过这种方式，任何子任务都可以作为有效的起点，促进了多样化的探索策略。实验结果显示，现有智能体在处理长时间任务时存在显著性能差距，表明需要更强大的规划和决策能力。'}}}, {'id': 'https://huggingface.co/papers/2508.04700', 'title': 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience', 'url': 'https://huggingface.co/papers/2508.04700', 'abstract': "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.", 'score': 11, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6c7450255f7c28bc', 'authors': ['Zeyi Sun', 'Ziyu Liu', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Tong Wu', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.04700.jpg', 'data': {'categories': ['#agents', '#agi', '#open_source', '#optimization', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучающиеся агенты для освоения нового ПО', 'desc': 'SEAgent - это фреймворк для автономного обучения агентов использованию компьютерного программного обеспечения. Он позволяет агентам исследовать новое ПО через экспериментальное обучение и выполнение автоматически генерируемых заданий возрастающей сложности. SEAgent использует модель состояния мира для оценки траектории действий и генератор учебного плана для создания разнообразных задач. Обновление политики агента происходит через имитационное обучение на неудачных действиях и оптимизацию на успешных, что позволяет достичь значительного улучшения производительности по сравнению с существующими методами.'}, 'en': {'title': 'Empowering Agents to Learn and Evolve Autonomously', 'desc': 'SEAgent is a self-evolving framework designed for computer-use agents (CUAs) to autonomously learn and master new software through experiential learning. It allows agents to explore unfamiliar software environments and learn from their experiences by tackling tasks that increase in complexity. The framework includes a World State Model for assessing agent performance and a Curriculum Generator for creating diverse tasks. By integrating insights from specialist agents, SEAgent develops a robust generalist CUA that outperforms traditional methods, achieving a notable increase in success rates across various software environments.'}, 'zh': {'title': 'SEAgent：自主进化的智能体框架', 'desc': 'SEAgent是一种自我进化的智能体框架，能够使计算机使用代理通过体验学习自主掌握新软件。该框架通过逐步的任务课程，帮助代理在没有人类标注的情况下，探索和学习陌生的软件环境。SEAgent设计了世界状态模型和课程生成器，以便代理能够通过试错学习不断提高其能力。最终，SEAgent的表现超越了多个专门化代理的组合，显示出其在新软件环境中的优越性。'}}}, {'id': 'https://huggingface.co/papers/2508.03905', 'title': 'Sotopia-RL: Reward Design for Social Intelligence', 'url': 'https://huggingface.co/papers/2508.03905', 'abstract': 'Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.', 'score': 9, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '815208c1d75b8f05', 'authors': ['Haofei Yu', 'Zhengyang Qi', 'Yining Zhao', 'Kolby Nottingham', 'Keyang Xuan', 'Bodhisattwa Prasad Majumder', 'Hao Zhu', 'Paul Pu Liang', 'Jiaxuan You'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Stanford University', 'University of California Irvine', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.03905.jpg', 'data': {'categories': ['#games', '#alignment', '#rlhf', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Sotopia-RL: прорыв в обучении социальному интеллекту для ИИ', 'desc': 'Sotopia-RL - это новая система обучения с подкреплением для улучшения социального интеллекта больших языковых моделей. Она преобразует обратную связь в многомерные награды на уровне отдельных высказываний, что позволяет эффективнее обучать модели социальным задачам. Система решает проблемы частичной наблюдаемости и многомерности социальных взаимодействий, которые затрудняют применение классических методов обучения с подкреплением. Эксперименты показали, что Sotopia-RL значительно превосходит существующие подходы в решении социальных задач.'}, 'en': {'title': 'Enhancing Social Intelligence in LLMs with Sotopia-RL', 'desc': "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."}, 'zh': {'title': '提升社交智能的强化学习新框架', 'desc': 'Sotopia-RL是一种新颖的强化学习框架，旨在提升大型语言模型的社交智能。它通过将反馈细化为发言级别的多维奖励，来改善模型在社交任务中的表现。该框架解决了社交互动中的部分可观察性和多维性问题，使得模型能够更有效地学习复杂的社交策略。实验结果表明，Sotopia-RL在社交目标完成评分上达到了最先进的水平，显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2508.01191', 'title': 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens', 'url': 'https://huggingface.co/papers/2508.01191', 'abstract': 'CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.', 'score': 9, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '427ac75c7123b50a', 'authors': ['Chengshuai Zhao', 'Zhen Tan', 'Pingchuan Ma', 'Dawei Li', 'Bohan Jiang', 'Yancheng Wang', 'Yingzhen Yang', 'Huan Liu'], 'affiliations': ['Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01191.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Ограниченность CoT-рассуждений в LLM: мираж вне распределения обучающих данных', 'desc': 'Исследование показывает, что рассуждения по цепочке (CoT) в больших языковых моделях (LLM) ограничены расхождением между распределениями обучающих и тестовых данных. Авторы разработали среду DataAlchemy для изучения CoT-рассуждений по трем измерениям: задача, длина и формат. Результаты демонстрируют, что CoT-рассуждения неустойчивы и исчезают при выходе за пределы распределения обучающих данных. Это исследование подчеркивает сложность достижения подлинных и обобщаемых рассуждений в LLM.'}, 'en': {'title': 'Unmasking the Fragility of CoT Reasoning in LLMs', 'desc': 'This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs.'}, 'zh': {'title': '链式思维推理的局限性与挑战', 'desc': '本文探讨了链式思维（CoT）推理在大型语言模型（LLM）中的局限性，特别是训练数据与测试数据之间的分布差异对其影响。研究表明，CoT推理并不是一种稳健的推理形式，因为它的有效性受到训练数据和测试查询之间分布差异的限制。通过设计一个名为DataAlchemy的控制环境，作者系统性地分析了CoT推理在不同任务、长度和格式下的表现。结果显示，当CoT推理超出训练分布时，其效果会显著下降，揭示了实现真正可推广推理的持续挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.01858', 'title': 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents', 'url': 'https://huggingface.co/papers/2508.01858', 'abstract': 'A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent\'s capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent\'s processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent\'s conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner', 'score': 7, 'issue_id': 5220, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'ad30239b3abef884', 'authors': ['Yuhan Guo', 'Cong Guo', 'Aiwen Sun', 'Hongliang He', 'Xinyu Yang', 'Yue Lu', 'Yingji Zhang', 'Xuntao Guo', 'Dong Zhang', 'Jianzhuang Liu', 'Jiang Duan', 'Yijia Xiao', 'Liangjian Wen', 'Hai-Ming Xu', 'Yong Dai'], 'affiliations': ['Central South University', 'Fudan University', 'Harbin Institute of Technology', 'Hithink Research', 'Shanghai Jiao Tong University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Southwestern University of Finance and Economics', 'University of Adelaide', 'University of California, Los Angeles', 'University of Manchester', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01858.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#reasoning', '#open_source'], 'emoji': '🕸️', 'ru': {'title': 'Когнитивная структура для веб-агентов нового поколения', 'desc': 'Статья представляет новую структуру для веб-агентов, разделяющую их возможности на изучение контента знаний и когнитивные процессы. Авторы предлагают Web-CogKnowledge Framework, категоризирующий знания как фактические, концептуальные и процедурные. Для облегчения приобретения знаний создан структурированный набор данных Web-CogDataset из 14 реальных веб-сайтов. Разработан новый агент Web-CogReasoner, использующий знание-ориентированную цепочку рассуждений (Chain-of-Thought), показывающий превосходство над существующими моделями.'}, 'en': {'title': 'Empowering Web Agents through Structured Knowledge and Reasoning', 'desc': 'This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models.'}, 'zh': {'title': '智能体能力的双重分解：知识与认知', 'desc': '本文提出了一种网络智能体的框架，将其能力分解为知识内容学习和认知过程。我们定义了Web-CogKnowledge框架，将知识分为事实性、概念性和程序性三类，以支持智能体的学习和推理。通过构建Web-CogDataset，我们为智能体提供了系统化的知识基础，帮助其掌握必要的核心知识。最后，我们开发了Web-CogReasoner，并通过实验验证了其在处理新任务时的优越性，尤其是在结构化知识至关重要的情况下。'}}}, {'id': 'https://huggingface.co/papers/2508.04664', 'title': 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management', 'url': 'https://huggingface.co/papers/2508.04664', 'abstract': "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.", 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '1575b65bda95c5f9', 'authors': ['Mo Li', 'L. H. Xu', 'Qitai Tan', 'Ting Cao', 'Yunxin Liu'], 'affiliations': ['Independent Researcher', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04664.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Sculptor: Умное управление контекстом для улучшения работы языковых моделей', 'desc': 'Статья представляет фреймворк Sculptor для активного управления контекстом в больших языковых моделях (LLM). Sculptor позволяет LLM проактивно управлять вниманием и рабочей памятью, что снижает проактивную интерференцию и улучшает надежность рассуждений. Фреймворк включает инструменты для фрагментации контекста, суммирования и интеллектуального поиска. Эксперименты показывают, что Sculptor значительно улучшает производительность LLM на длинных контекстах без специального обучения.'}, 'en': {'title': 'Sculptor: Mastering Memory for Better Long-Context Reasoning', 'desc': 'The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies.'}, 'zh': {'title': '主动上下文管理，提升LLM性能！', 'desc': 'Sculptor是一个用于主动上下文管理的框架，旨在提高大型语言模型（LLM）在处理长上下文时的表现。该框架通过主动管理注意力和工作记忆，减少了前期信息的干扰，从而改善推理的可靠性。Sculptor提供了三种工具：上下文碎片化、摘要、隐藏与恢复，以及智能搜索，帮助LLM更有效地处理信息。实验结果表明，Sculptor在没有特定训练的情况下，显著提升了模型的性能，强调了主动上下文管理的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.03789', 'title': 'HPSv3: Towards Wide-Spectrum Human Preference Score', 'url': 'https://huggingface.co/papers/2508.03789', 'abstract': 'HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.', 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'a2a0678cfc88e0ef', 'authors': ['Yuhang Ma', 'Xiaoshi Wu', 'Keqiang Sun', 'Hongsheng Li'], 'affiliations': ['CPII, InnoHK', 'CUHK MMLab', 'Kings College London', 'Mizzen AI', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03789.jpg', 'data': {'categories': ['#alignment', '#data', '#benchmark', '#dataset', '#cv', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'HPSv3: Новый стандарт оценки качества генерации изображений по тексту', 'desc': 'HPSv3 - это новый метод оценки качества генерации изображений по тексту, основанный на широком спектре данных и учитывающий неопределенность при ранжировании. Авторы представили обширный датасет HPDv3, содержащий 1.08 миллиона пар текст-изображение и 1.17 миллиона аннотированных попарных сравнений. Метод использует модель на основе VLM, обученную с помощью функции потерь, учитывающей неопределенность при ранжировании. Также предложен итеративный подход Chain-of-Human-Preference для улучшения качества сгенерированных изображений.'}, 'en': {'title': 'Enhancing Image Generation with Human Preference Score v3', 'desc': 'The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment.'}, 'zh': {'title': '提升图像生成质量的HPSv3与CoHP方法', 'desc': 'HPSv3是一种人类偏好评分，利用广泛的数据集和考虑不确定性的排名损失，提升文本到图像生成的质量。我们发布了HPDv3，这是第一个包含108万对文本-图像和117万对标注比较的广泛人类偏好数据集。我们还引入了一种基于视觉语言模型的偏好模型，使用不确定性感知的排名损失进行细致排名。此外，我们提出了人类偏好链（CoHP），通过迭代图像优化，在每一步选择最佳图像，从而提高生成质量。'}}}, {'id': 'https://huggingface.co/papers/2507.23785', 'title': 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis', 'url': 'https://huggingface.co/papers/2507.23785', 'abstract': 'A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.', 'score': 6, 'issue_id': 5220, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '47c7686978b9c4dc', 'authors': ['Bowen Zhang', 'Sicheng Xu', 'Chuxin Wang', 'Jiaolong Yang', 'Feng Zhao', 'Dong Chen', 'Baining Guo'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23785.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического 3D-контента из видео с помощью диффузионных моделей', 'desc': 'Представлена новая система для создания динамического 3D-контента из одиночных видеовходов. Используется VAE для кодирования канонических гауссовых сплатов и их временных вариаций, а также диффузионная модель с учетом времени для генерации. Система обучена на тщательно отобранных анимируемых 3D-объектах из набора данных Objaverse. Демонстрирует превосходное качество генерации и обобщение на реальные видеовходы, несмотря на обучение только на синтетических данных.'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Worlds', 'desc': 'This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation.'}, 'zh': {'title': '从视频生成高质量动态3D内容的创新框架', 'desc': '本文提出了一种新颖的框架，用于从单个视频输入生成高质量的动态3D内容。我们引入了直接的4DMesh到高斯样条（GS）变分场变分自编码器（VAE），能够直接编码3D动画数据中的高斯样条及其时间变化。通过这种高效的表示方式，我们训练了一个基于时间感知的高斯变分场扩散模型，能够根据输入视频和高斯样条生成动态内容。实验结果表明，该模型在生成质量上优于现有方法，并且在处理真实视频输入时表现出良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.04586', 'title': 'Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference', 'url': 'https://huggingface.co/papers/2508.04586', 'abstract': 'The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.', 'score': 4, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '0e9d3ff69536a24d', 'authors': ['Nuo Chen', 'Moming Duan', 'Andre Huikai Lin', 'Qian Wang', 'Jiaying Wu', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.04586.jpg', 'data': {'categories': ['#ethics', '#survey'], 'emoji': '🌐', 'ru': {'title': 'Революция в формате конференций ИИ: от централизации к федерации', 'desc': 'Данная статья анализирует структурные проблемы конференций по искусственному интеллекту, включая рост числа публикаций, углеродный след, негативное настроение сообщества и логистические трудности. Авторы выделяют четыре ключевые области напряженности: научную, экологическую, психологическую и логистическую. Для решения этих проблем предлагается модель Community-Federated Conference (CFC), которая разделяет рецензирование, презентации и нетворкинг на глобально координируемые, но локально организованные компоненты. Эта модель предлагает более устойчивый, инклюзивный и гибкий подход к проведению исследований в области ИИ.'}, 'en': {'title': 'Towards Sustainable AI Conferences: A Community-Federated Approach', 'desc': 'This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research.'}, 'zh': {'title': '构建可持续的人工智能会议新模式', 'desc': '这篇论文诊断了人工智能会议的结构性问题，包括发表率、碳足迹、负面社区情绪和后勤挑战。随着会议数量的快速增长，传统的集中式会议模式变得越来越不可持续。论文提出了一种基于社区的联合会议模型，旨在解决这些问题，促进科学传播的公平性和社区的福祉。该模型将同行评审、展示和网络交流分开，提供了一种更可持续、包容和有韧性的人工智能研究发展路径。'}}}, {'id': 'https://huggingface.co/papers/2508.02215', 'title': 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding', 'url': 'https://huggingface.co/papers/2508.02215', 'abstract': 'LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.', 'score': 3, 'issue_id': 5220, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'caa8de613517d011', 'authors': ['Yike Zhang', 'Zhiyuan He', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yuqing Yang', 'Jianyong Wang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02215.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': '🔪', 'ru': {'title': 'Эффективное обрезание кэша для ускорения языковых моделей', 'desc': 'LeanK - это метод машинного обучения для оптимизации больших языковых моделей. Он уменьшает использование памяти и ускоряет декодирование, удаляя неважные каналы в кэше ключей. LeanK использует двухэтапный процесс обучения для создания маски каналов, удовлетворяющей требованиям разреженности и аппаратного выравнивания. Эксперименты показывают сокращение памяти кэша ключей до 70% и ускорение вычисления внимания в 1,3 раза.'}, 'en': {'title': 'LeanK: Pruning for Efficient Language Model Performance', 'desc': 'LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation.'}, 'zh': {'title': 'LeanK：高效解码的大型语言模型优化方案', 'desc': 'LeanK是一种基于学习的方法，旨在减少大型语言模型中的不重要的关键缓存通道，从而降低内存使用并加速解码，同时不影响准确性。该方法利用静态通道稀疏性，通过一种新颖的两阶段训练过程，学习满足特定稀疏比和硬件对齐要求的通道静态掩码。实验结果表明，LeanK可以减少高达70%的K缓存和16%-18%的V缓存内存，并且自定义解码内核使注意力计算速度提高了1.3倍。通过分析学习到的重要性分布，我们还提供了对长上下文推理过程中模型通道和注意力头的深入见解。'}}}, {'id': 'https://huggingface.co/papers/2508.01197', 'title': 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding', 'url': 'https://huggingface.co/papers/2508.01197', 'abstract': 'A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.', 'score': 3, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'd7be41190836a7cc', 'authors': ['Zhan Shi', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['College of Computer Science, Zhejiang University, Hangzhou 310027, China', 'College of Software Technology, Zhejiang University', 'Udeer.ai, Hangzhou 310000, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01197.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#3d', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Точное восприятие объектов для беспилотных автомобилей с помощью 3D грунтовки', 'desc': 'Статья представляет новый бенчмарк для задачи 3D грунтовки объектов с использованием естественного языка и воксельных аннотаций в контексте автономного вождения. Авторы предлагают модель GroundingOcc, которая объединяет визуальные, текстовые и облачные признаки для предсказания местоположения и занятости объектов. Модель включает мультимодальный энкодер, модули для предсказания воксельной занятости и уточнения локализации, а также дополнительные компоненты для 2D грунтовки и оценки глубины. Эксперименты показывают превосходство предложенного метода над существующими базовыми моделями в задаче 3D грунтовки занятости.'}, 'en': {'title': 'Enhancing Object Perception with 3D Occupancy Grounding', 'desc': 'This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': '提升自动驾驶物体感知的3D占用基础视觉定位', 'desc': '本论文提出了一种新的基准和模型，用于通过自然语言和体素级注释进行3D占用基础的视觉定位，旨在提高自动驾驶中的物体感知能力。现有的视觉定位任务通常依赖于边界框，这种方法无法捕捉到细粒度的细节，导致物体表示不准确。我们引入的GroundingOcc模型通过多模态学习，结合视觉、文本和点云特征，从粗到细地预测物体位置和占用信息。实验结果表明，我们的方法在3D占用基础的视觉定位任务中优于现有的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04295', 'title': 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation', 'url': 'https://huggingface.co/papers/2508.04295', 'abstract': "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.", 'score': 2, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '0dc09e7a8e2bad95', 'authors': ['Chaofan Wang', 'Tingrui Yu', 'Jie Wang', 'Dong Chen', 'Wenrui Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Beijun Shen'], 'affiliations': ['Huawei Technologies Co., Ltd', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04295.jpg', 'data': {'categories': ['#open_source', '#architecture', '#plp', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'EvoC2Rust: Эволюционный подход к автоматическому переводу C в Rust', 'desc': 'EvoC2Rust - это автоматизированный фреймворк для перевода проектов на C в Rust, использующий подход на основе скелета кода. Он сочетает методы на основе правил и языковых моделей для улучшения синтаксиса, семантики и безопасности кода. Фреймворк работает в три этапа: декомпозиция проекта, инкрементальный перевод функций и исправление ошибок компиляции. Оценка на открытых и промышленных проектах показала превосходство EvoC2Rust над существующими подходами в точности перевода и безопасности кода.'}, 'en': {'title': 'EvoC2Rust: Bridging C to Rust with Safety and Precision', 'desc': 'EvoC2Rust is a novel framework designed to automate the translation of entire C projects into Rust, leveraging a skeleton-guided approach that integrates both rule-based and LLM-based techniques. This method addresses the limitations of existing solutions by ensuring that the translated code maintains both safety and idiomatic Rust syntax. The framework operates in three stages: it first breaks down the C project into modules, then translates functions incrementally, and finally resolves any compilation errors using a combination of LLM and static analysis. Evaluation results show that EvoC2Rust significantly outperforms previous methods in terms of syntax, semantic accuracy, and code safety, making it a robust solution for converting legacy C code to Rust.'}, 'zh': {'title': 'EvoC2Rust：高效的C到Rust自动转换框架', 'desc': 'EvoC2Rust是一个自动化框架，旨在将整个C项目转换为Rust代码。它采用了骨架引导的方法，结合了基于规则和基于大语言模型（LLM）的方法，以提高代码的语法、语义和安全性。该框架通过三个进化阶段进行项目级翻译，首先将C项目分解为功能模块，然后逐步翻译函数，最后通过集成LLM和静态分析修复编译错误。评估结果显示，EvoC2Rust在C到Rust的翻译中表现优越，语法和语义准确性分别提高了17.24%和14.32%。'}}}, {'id': 'https://huggingface.co/papers/2508.03680', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03680', 'abstract': "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'score': 2, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '78a8398db0f71f63', 'authors': ['Xufang Luo', 'Yuge Zhang', 'Zhiyuan He', 'Zilong Wang', 'Siyun Zhao', 'Dongsheng Li', 'Luna K. Qiu', 'Yuqing Yang'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.03680.jpg', 'data': {'categories': ['#agents', '#rag', '#games', '#math', '#training', '#optimization', '#rl'], 'emoji': '⚡', 'ru': {'title': 'Agent Lightning: универсальный фреймворк для обучения ИИ-агентов', 'desc': 'Agent Lightning - это гибкий фреймворк для обучения с подкреплением больших языковых моделей в различных агентах. Он использует иерархический алгоритм обучения с подкреплением и отделяет выполнение от обучения для обработки сложных взаимодействий. Фреймворк позволяет интегрироваться с существующими агентами, разработанными различными способами, практически без изменения кода. Эксперименты показали стабильные улучшения в задачах text-to-SQL, генерации с использованием извлечения информации и использования математических инструментов.'}, 'en': {'title': 'Decoupling Training and Execution for Enhanced AI Agent Performance', 'desc': 'Agent Lightning is a versatile framework designed for training Large Language Models (LLMs) using Reinforcement Learning (RL) techniques. It separates the training process from agent execution, allowing for easier integration with various existing AI agents without significant code changes. By modeling agent execution as a Markov decision process, it introduces a hierarchical RL algorithm called LightningRL, which effectively manages complex interactions and multi-agent scenarios. The framework has shown promising results in tasks like text-to-SQL and retrieval-augmented generation, indicating its effectiveness for real-world applications.'}, 'zh': {'title': 'Agent Lightning：灵活的智能体训练框架', 'desc': 'Agent Lightning是一个灵活的强化学习框架，旨在为各种智能体训练大型语言模型（LLMs）。它通过将执行与训练解耦，使用层次化的强化学习算法，能够处理复杂的交互逻辑。该框架允许与现有智能体的无缝集成，几乎不需要代码修改。实验结果表明，Agent Lightning在文本到SQL、增强生成和数学工具使用等任务中表现出稳定的持续改进，展示了其在实际智能体训练和部署中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.03560', 'title': 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought', 'url': 'https://huggingface.co/papers/2508.03560', 'abstract': 'LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method.', 'score': 2, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '14b52fe9021b5b26', 'authors': ['Yi Gui', 'Zhen Li', 'Zhongyi Zhang', 'Guohao Wang', 'Tianpeng Lv', 'Gaoyang Jiang', 'Yi Liu', 'Dongping Chen', 'Yao Wan', 'Hongyu Zhang', 'Wenbin Jiang', 'Xuanhua Shi', 'Hai Jin'], 'affiliations': ['Chongqing University, Chongqing, China', 'Huazhong University of Science and Technology, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03560.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#architecture', '#multimodal', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'LaTCoder: улучшение сохранения макета при генерации кода из дизайна веб-страниц', 'desc': 'LaTCoder - это новый подход к задаче преобразования дизайна веб-страниц в код, который улучшает сохранение макета. Метод использует разделение дизайна на блоки и применяет рассуждения по цепочке мыслей (Chain-of-Thought) с помощью мультимодальных больших языковых моделей (MLLM). LaTCoder показывает значительные улучшения по автоматическим метрикам, таким как TreeBLEU и MAE. В ходе оценки предпочтений пользователей, веб-страницы, сгенерированные LaTCoder, были выбраны в более чем 60% случаев.'}, 'en': {'title': 'Enhancing Layout Preservation in Design-to-Code with LaTCoder', 'desc': "LaTCoder is a novel approach designed to improve the accuracy of converting webpage designs into code while preserving the layout. It utilizes Chain-of-Thought reasoning to enhance the performance of Multimodal Large Language Models (MLLMs) by breaking down designs into manageable blocks. The method employs two assembly strategies to optimize the final output, ensuring that the generated code closely matches the intended design. Experimental results show significant improvements in both automatic metrics and human preference, indicating LaTCoder's effectiveness in design-to-code tasks."}, 'zh': {'title': '提升网页设计布局保留的LaTCoder', 'desc': 'LaTCoder是一种新方法，旨在提高网页设计到代码生成过程中的布局保留能力。它通过将网页设计分割成多个图像块，并使用基于思维链的推理方法来生成每个块的代码。该方法结合了绝对定位和基于多模态大语言模型的组装策略，以选择最佳输出。实验结果表明，LaTCoder在多个基准测试中显著提高了自动评估指标和人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2507.23313', 'title': 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2507.23313', 'abstract': 'Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.', 'score': 1, 'issue_id': 5221, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'c584e9c932383ec6', 'authors': ['Alfio Ferrara', 'Sergio Picascia', 'Elisabetta Rocchetti'], 'affiliations': ['Department of Computer Science, Università degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2507.23313.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multimodal', '#interpretability', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Скрытое понимание искусства: как ИИ разделяет содержание и стиль', 'desc': 'Исследование показало, что трансформерные модели диффузии для генерации изображений по тексту демонстрируют различную степень разделения содержания и стиля в создаваемых произведениях искусства. Анализ проводился с использованием тепловых карт кросс-внимания, которые позволяют соотнести пиксели сгенерированных изображений с конкретными токенами промпта. Результаты выявили, что во многих случаях токены содержания влияют преимущественно на области, связанные с объектами, в то время как токены стиля воздействуют на фон и текстуры. Это указывает на то, что модели диффузии формируют некоторое внутреннее представление о различии между содержанием и стилем без явного обучения этому.'}, 'en': {'title': 'Decoding Art: Understanding Content and Style in AI-Generated Images', 'desc': 'This paper explores how transformer-based text-to-image diffusion models generate artworks by analyzing their internal representation of content and style. Using cross-attention heatmaps, the authors investigate how different prompt tokens influence specific regions of generated images, revealing a separation between content and style. The study finds that content tokens mainly affect object-related areas, while style tokens influence backgrounds and textures, indicating an emergent understanding of these concepts. This research enhances our knowledge of generative models and their ability to represent complex artistic ideas without direct supervision.'}, 'zh': {'title': '探索内容与风格的分离：扩散模型的艺术生成', 'desc': '本研究探讨了基于变换器的文本到图像扩散模型在生成艺术作品时如何编码内容和风格的概念。通过交叉注意力热图，我们能够将生成图像中的像素归因于特定的提示令牌，从而区分受内容描述和风格描述影响的图像区域。研究发现，扩散模型在不同艺术提示和风格请求下表现出不同程度的内容与风格分离。结果表明，内容令牌主要影响与物体相关的区域，而风格令牌则影响背景和纹理区域，显示出模型对内容与风格区分的理解。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (9)', '#agents (20)', '#agi (4)', '#alignment (10)', '#architecture (14)', '#audio (3)', '#benchmark (31)', '#cv (14)', '#data (11)', '#dataset (31)', '#diffusion (11)', '#ethics (4)', '#games (18)', '#graphs', '#hallucinations (5)', '#healthcare (2)', '#inference (6)', '#interpretability (6)', '#leakage', '#long_context (9)', '#low_resource (2)', '#machine_translation (2)', '#math (4)', '#multilingual (6)', '#multimodal (29)', '#open_source (26)', '#optimization (44)', '#plp (2)', '#rag (4)', '#reasoning (19)', '#rl (12)', '#rlhf (6)', '#robotics (5)', '#science (5)', '#security (3)', '#small_models (1)', '#story_generation', '#survey (5)', '#synthetic (5)', '#training (36)', '#transfer_learning', '#video (8)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-07 05:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-07 05:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-07 05:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    