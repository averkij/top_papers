
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
            <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}
                gtag('js', new Date());
                gtag('config', 'G-C1CRWDNJ1J');
            </script>
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap" rel="stylesheet">
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Chinese reading task about ML</title>
            <style>
                body {
                    font-family: Arial, sans-serif;
                    background-color: #f4f4f9;
                    color: #333;
                    margin: 0;
                    padding: 20px;
                }
                .container {
                    max-width: 800px;
                    margin: 0 auto;
                    background-color: #fff;
                    padding: 20px;
                    border-radius: 8px;
                    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                }
                h1 {
                    color: #0056b3;
                    text-align: center;
                }
                p {
                    line-height: 1.6;
                }
                .zh-text {
                    font-size: 1.3em;
                    font-family: 'Noto Sans SC';
                    font-weight: 300;
                    margin: 0 0 5px 0;
                }
                .pinyin {
                    padding-top: 5px;
                    padding-bottom: 5px;
                    font-style: italic;
                    color: #888;
                }
                table {
                    width: 100%;
                    border-collapse: collapse;
                    margin-top: 20px;
                }
                th, td {
                    padding: 12px;
                    border: 1px solid #ddd;
                    text-align: left;
                }
                th {
                    background-color: #0056b3;
                    color: #fff;
                }
                td {
                    background-color: #f9f9f9;
                }
                td.zh {
                    font-family: 'Noto Sans SC';
                    font-size: 1.2em;
                    font-weight: 400;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>Self-rewarding correction for mathematical reasoning</h1>
                <div><p class='zh-text'>1. 这篇文章研究了自我奖励推理的大语言模型（LLMs）。</p>
<p class='zh-text'>2. 这种模型可以在推理过程中同时生成逐步推理并评估其输出的正确性，无需外部反馈。</p>
<p class='zh-text'>3. 研究重点是自我纠正任务，模型可以自主检测和修正错误，决定何时终止迭代修正循环。</p>
<p class='zh-text'>4. 作者提出了一个两阶段的算法框架，使用自我生成的数据来构建这种模型。</p>
<p class='zh-text'>5. 实验结果显示，这种方法在Llama-3和Qwen-2.5上表现优异，超越了内在自我纠正能力，达到了与依赖外部奖励模型系统相当的性能。</p></div>
                <div class="pinyin">
                    <p>1. 这篇文章研究了自我奖励推理的大语言模型（LLMs）。
Zhè piān wénzhāng yánjiū le zìwǒ jiǎnglì tuīlǐ de dà yǔyán móxíng (LLMs)</p>
<p>2. 

这种模型可以在推理过程中同时生成逐步推理并评估其输出的正确性，无需外部反馈。
Zhè zhǒng móxíng kěyǐ zài tuīlǐ guòchéng zhōng tóngshí shēngchéng zhúbù tuīlǐ bìng píngjià qí shūchū de zhèngquèxìng, wúxū wàibù fǎnkuì</p>
<p>3. 

研究重点是自我纠正任务，模型可以自主检测和修正错误，决定何时终止迭代修正循环。
Yánjiū zhòngdiǎn shì zìwǒ jiūzhèng rènwù, móxíng kěyǐ zìzhǔ jiǎncè hé xiūzhèng cuòwù, juédìng héshí zhōngzhǐ diécì xiūzhèng xúnhuán</p>
<p>4. 

作者提出了一个两阶段的算法框架，使用自我生成的数据来构建这种模型。
Zuòzhě tíchū le yīgè liǎng jiēduàn de suànfǎ kuàngjià, shǐyòng zìwǒ shēngchéng de shùjù lái gòujiàn zhè zhǒng móxíng</p>
<p>5. 

实验结果显示，这种方法在Llama-3和Qwen-2</p>
<p>6. 5上表现优异，超越了内在自我纠正能力，达到了与依赖外部奖励模型系统相当的性能。
Shíyàn jiéguǒ xiǎnshì, zhè zhǒng fāngfǎ zài Llama-3 hé Qwen-2</p>
<p>7. 5 shàng biǎoxiàn yōuyì, chāoyuè le nèizài zìwǒ jiūzhèng nénglì, dádào le yǔ yīlài wàibù jiǎnglì móxíng xìtǒng xiāngdāng de xíngnéng</p>
                </div>
                <div><p>1. This article investigates large language models (LLMs) with self-rewarding reasoning capabilities.</p>
<p>2.  These models can generate step-by-step reasoning during the inference process and evaluate the correctness of their outputs without external feedback.</p>
<p>3.  The research focuses on self-correcting tasks, where the model can autonomously detect and correct errors, deciding when to terminate the iterative correction loop.</p>
<p>4.  The authors propose a two-stage algorithmic framework that uses self-generated data to build such models.</p>
<p>5.  Experimental results demonstrate that this method performs exceptionally well on Llama-3 and Qwen-2.</p>
<p>6. 5, surpassing inherent self-correcting capabilities and achieving performance comparable to systems that rely on external reward models.</p></div>
                <h2>Vocabulary</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Word</th>
                            <th>Pinyin</th>
                            <th>Translation</th>
                        </tr>
                    </thead>
                    <tbody>
        
                        <tr>
                            <td class="zh">自我奖励推理</td>
                            <td>zì wǒ jiǎng lì tuī lǐ</td>
                            <td>self-rewarding reasoning</td>
                        </tr>
            
                        <tr>
                            <td class="zh">大语言模型</td>
                            <td>dà yǔ yán mó xíng</td>
                            <td>large language model</td>
                        </tr>
            
                        <tr>
                            <td class="zh">逐步推理</td>
                            <td>zhú bù tuī lǐ</td>
                            <td>step-by-step reasoning</td>
                        </tr>
            
                        <tr>
                            <td class="zh">自我纠正</td>
                            <td>zì wǒ jiū zhèng</td>
                            <td>self-correction</td>
                        </tr>
            
                        <tr>
                            <td class="zh">自主检测</td>
                            <td>zì zhǔ jiǎn cè</td>
                            <td>autonomous detection</td>
                        </tr>
            
                        <tr>
                            <td class="zh">迭代修正</td>
                            <td>dié dài xiū zhèng</td>
                            <td>iterative correction</td>
                        </tr>
            
                        <tr>
                            <td class="zh">算法框架</td>
                            <td>suàn fǎ kuàng jià</td>
                            <td>algorithmic framework</td>
                        </tr>
            
                        <tr>
                            <td class="zh">自我生成</td>
                            <td>zì wǒ shēng chéng</td>
                            <td>self-generation</td>
                        </tr>
            
                        <tr>
                            <td class="zh">内在自我纠正</td>
                            <td>nèi zài zì wǒ jiū zhèng</td>
                            <td>intrinsic self-correction</td>
                        </tr>
            
                        <tr>
                            <td class="zh">依赖外部奖励</td>
                            <td>yī lài wài bù jiǎng lì</td>
                            <td>reliant on external rewards</td>
                        </tr>
            
                    </tbody>
                </table>
            </div>
        </body>
        </html>
        