
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
            <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}
                gtag('js', new Date());
                gtag('config', 'G-C1CRWDNJ1J');
            </script>
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap" rel="stylesheet">
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Chinese reading task about ML</title>
            <style>
                body {
                    font-family: Arial, sans-serif;
                    background-color: #f4f4f9;
                    color: #333;
                    margin: 0;
                    padding: 20px;
                }
                .container {
                    max-width: 800px;
                    margin: 0 auto;
                    background-color: #fff;
                    padding: 20px;
                    border-radius: 8px;
                    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                }
                h1 {
                    color: #0056b3;
                    text-align: center;
                }
                p {
                    line-height: 1.6;
                }
                .zh-text {
                    font-size: 1.3em;
                    font-family: 'Noto Sans SC';
                    font-weight: 300;
                    margin: 0 0 5px 0;
                }
                .pinyin {
                    padding-top: 5px;
                    padding-bottom: 5px;
                    font-style: italic;
                    color: #888;
                }
                table {
                    width: 100%;
                    border-collapse: collapse;
                    margin-top: 20px;
                }
                th, td {
                    padding: 12px;
                    border: 1px solid #ddd;
                    text-align: left;
                }
                th {
                    background-color: #0056b3;
                    color: #fff;
                }
                td {
                    background-color: #f9f9f9;
                }
                td.zh {
                    font-family: 'Noto Sans SC';
                    font-size: 1.2em;
                    font-weight: 400;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>Self-rewarding correction for mathematical reasoning</h1>
                <div><p class='zh-text'>1. 我们研究了一种自我奖励推理的大语言模型（LLMs），它可以在推理过程中同时生成逐步推理并评估其输出的正确性，而无需外部反馈。</p>
<p class='zh-text'>2. 这种集成方法使单个模型能够独立引导其推理过程，为模型部署提供计算优势。</p>
<p class='zh-text'>3. 我们特别关注自我修正任务，模型可以自主检测其响应中的错误，修正输出，并决定何时终止迭代改进循环。</p>
<p class='zh-text'>4. 为此，我们提出了一种两阶段的算法框架，使用仅自生成数据构建自我奖励推理模型。</p>
<p class='zh-text'>5. 在第一阶段，我们使用顺序拒绝采样来合成包含自我奖励和自我修正机制的长链推理轨迹。</p>
<p class='zh-text'>6. 在这些数据上微调模型，使其学习自我奖励和自我修正的模式。</p>
<p class='zh-text'>7. 在第二阶段，我们通过基于规则的信号的强化学习，进一步增强模型评估响应准确性和改进输出的能力。</p>
<p class='zh-text'>8. 实验表明，我们的方法在Llama-3和Qwen-2.5上超越了内在自我修正能力，并达到了与依赖外部奖励模型的系统相当的性能。</p></div>
                <div class="pinyin">
                    <p>1. Wǒmen yánjiū le yī zhǒng zìwǒ jiǎnglì tuīlǐ de dà yǔyán móxíng (LLMs), tā kěyǐ zài tuīlǐ guòchéng zhōng tóngshí shēngchéng zhúbù tuīlǐ ér pínggǔ qí shūchū de zhèngquèxìng, ér wúxū wàibù fǎnkuì</p>
<p>2.  Zhè zhǒng jíchéng fāngfǎ shǐ dān gè móxíng nénggòu dúlì yǐndǎo qí tuīlǐ guòchéng, wèi móxíng bùshǔ tígōng jìsuàn yòushì</p>
<p>3.  Wǒmen tèbié guānzhù zìwǒ xiūzhèng rènwù, móxíng kěyǐ zìzhǔ jiǎncè qí xiǎngyìng zhōng de cuòwù, xiūzhèng shūchū, bìng juédìng héshí zhōngzhǐ dìtí gǎijìn xúnhuán</p>
<p>4.  Wèi cǐ, wǒmen tíchū le yī zhǒng liǎng jiēduàn de suànfǎ kuàngjià, shǐyòng jǐn zì shēngchéng shùjù jiànzhù zìwǒ jiǎnglì tuīlǐ móxíng</p>
<p>5.  Zài dì-yī jiēduàn, wǒmen shǐyòng shùnxù jùjué cǎiyǎng lái héchéng bāohán zìwǒ jiǎnglì hé zìwǒ xiūzhèng jīzhì de chángliàn tuīlǐ guǐjī</p>
<p>6.  Zài zhèxiē shùjù shàng wēitiáo móxíng, shǐ qí xuéxí zìwǒ jiǎnglì hé zìwǒ xiūzhèng de móshì</p>
<p>7.  Zài dì-èr jiēduàn, wǒmen tōngguò jīyú guīzé de xìnhào de qiángzhì xuéxí, jìnfēng móxíng pínggǔ xiǎngyìng zhǔnquèxìng hé gǎijìn shūchū de nénglì</p>
<p>8.  Shíyàn biǎomíng, wǒmen de fāngfǎ zài Llama-3 hé Qwen-2</p>
<p>9. 5 shàng chāoyuè le nèixīn zìwǒ xiūzhèng nénglì, bìng dá le yǔ yīlài wàibù jiǎnglì móxíng de xìtǒng xiāngdāng de xiàonéng</p>
                </div>
                <div><p>1. We have studied a type of large language model (LLM) that employs self-rewarding reasoning, capable of simultaneously generating step-by-step reasoning and evaluating the correctness of its output without external feedback.</p>
<p>2.  This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment.</p>
<p>3.  We particularly focus on self-correction tasks, where the model can autonomously detect errors in its responses, correct the output, and decide when to terminate the iterative improvement loop.</p>
<p>4. 

To achieve this, we propose a two-stage algorithmic framework that uses only self-generated data to build a self-rewarding reasoning model.</p>
<p>5.  In the first stage, we use sequential rejection sampling to synthesize long-chain reasoning trajectories that include self-rewarding and self-correcting mechanisms.</p>
<p>6.  The model is then fine-tuned on this data to learn patterns of self-rewarding and self-correcting.</p>
<p>7.  In the second stage, we further enhance the model's ability to evaluate response accuracy and improve output through reinforcement learning based on rule-based signals.</p>
<p>8. 

Experiments show that our method outperforms the intrinsic self-correction capabilities of Llama-3 and Qwen-2.</p>
<p>9. 5 and achieves performance comparable to systems that rely on external reward models.</p></div>
                <h2>Vocabulary</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Word</th>
                            <th>Pinyin</th>
                            <th>Translation</th>
                        </tr>
                    </thead>
                    <tbody>
        
                        <tr>
                            <td class="zh">研究</td>
                            <td>yán jiū</td>
                            <td>research</td>
                        </tr>
            
                        <tr>
                            <td class="zh">自我奖励</td>
                            <td>zì wǒ jiǎng lì</td>
                            <td>self-reward</td>
                        </tr>
            
                        <tr>
                            <td class="zh">推理</td>
                            <td>tuī lǐ</td>
                            <td>reasoning</td>
                        </tr>
            
                        <tr>
                            <td class="zh">大语言模型</td>
                            <td>dà yǔ yán mó xíng</td>
                            <td>large language model</td>
                        </tr>
            
                        <tr>
                            <td class="zh">生成</td>
                            <td>shēng chéng</td>
                            <td>generate</td>
                        </tr>
            
                        <tr>
                            <td class="zh">逐步</td>
                            <td>zhú bù</td>
                            <td>step-by-step</td>
                        </tr>
            
                        <tr>
                            <td class="zh">评估</td>
                            <td>píng gū</td>
                            <td>evaluate</td>
                        </tr>
            
                        <tr>
                            <td class="zh">正确性</td>
                            <td>zhèng què xìng</td>
                            <td>correctness</td>
                        </tr>
            
                        <tr>
                            <td class="zh">外部反馈</td>
                            <td>wài bù fǎn kuì</td>
                            <td>external feedback</td>
                        </tr>
            
                        <tr>
                            <td class="zh">集成</td>
                            <td>jí chéng</td>
                            <td>integrate</td>
                        </tr>
            
                        <tr>
                            <td class="zh">独立</td>
                            <td>dú lì</td>
                            <td>independent</td>
                        </tr>
            
                        <tr>
                            <td class="zh">引导</td>
                            <td>yǐn dǎo</td>
                            <td>guide</td>
                        </tr>
            
                        <tr>
                            <td class="zh">计算优势</td>
                            <td>jì suàn yōu shì</td>
                            <td>computational advantage</td>
                        </tr>
            
                        <tr>
                            <td class="zh">部署</td>
                            <td>bù shǔ</td>
                            <td>deploy</td>
                        </tr>
            
                        <tr>
                            <td class="zh">自我修正</td>
                            <td>zì wǒ xiū zhèng</td>
                            <td>self-correction</td>
                        </tr>
            
                        <tr>
                            <td class="zh">检测</td>
                            <td>jiǎn cè</td>
                            <td>detect</td>
                        </tr>
            
                        <tr>
                            <td class="zh">响应</td>
                            <td>xiǎng yìng</td>
                            <td>response</td>
                        </tr>
            
                        <tr>
                            <td class="zh">终止</td>
                            <td>zhōng zhǐ</td>
                            <td>terminate</td>
                        </tr>
            
                        <tr>
                            <td class="zh">迭代</td>
                            <td>dié dài</td>
                            <td>iterate</td>
                        </tr>
            
                        <tr>
                            <td class="zh">改进</td>
                            <td>gǎi jìn</td>
                            <td>improve</td>
                        </tr>
            
                        <tr>
                            <td class="zh">循环</td>
                            <td>xún huán</td>
                            <td>loop</td>
                        </tr>
            
                        <tr>
                            <td class="zh">两阶段</td>
                            <td>liǎng jiē duàn</td>
                            <td>two-stage</td>
                        </tr>
            
                        <tr>
                            <td class="zh">算法框架</td>
                            <td>suàn fǎ kuàng jià</td>
                            <td>algorithmic framework</td>
                        </tr>
            
                        <tr>
                            <td class="zh">自生成</td>
                            <td>zì shēng chéng</td>
                            <td>self-generated</td>
                        </tr>
            
                        <tr>
                            <td class="zh">顺序拒绝采样</td>
                            <td>shùn xù jù jué cǎi yàng</td>
                            <td>sequential rejection sampling</td>
                        </tr>
            
                        <tr>
                            <td class="zh">合成</td>
                            <td>hé chéng</td>
                            <td>synthesize</td>
                        </tr>
            
                        <tr>
                            <td class="zh">长链推理</td>
                            <td>cháng lián tuī lǐ</td>
                            <td>long-chain reasoning</td>
                        </tr>
            
                        <tr>
                            <td class="zh">轨迹</td>
                            <td>guǐ jì</td>
                            <td>trajectory</td>
                        </tr>
            
                        <tr>
                            <td class="zh">微调</td>
                            <td>wēi tiáo</td>
                            <td>fine-tune</td>
                        </tr>
            
                        <tr>
                            <td class="zh">模式</td>
                            <td>mó shì</td>
                            <td>pattern</td>
                        </tr>
            
                        <tr>
                            <td class="zh">强化学习</td>
                            <td>qiáng huà xué xí</td>
                            <td>reinforcement learning</td>
                        </tr>
            
                        <tr>
                            <td class="zh">基于规则</td>
                            <td>jī yú guī zé</td>
                            <td>rule-based</td>
                        </tr>
            
                        <tr>
                            <td class="zh">信号</td>
                            <td>xìn hào</td>
                            <td>signal</td>
                        </tr>
            
                        <tr>
                            <td class="zh">增强</td>
                            <td>zēng qiáng</td>
                            <td>enhance</td>
                        </tr>
            
                        <tr>
                            <td class="zh">准确性</td>
                            <td>zhǔn què xìng</td>
                            <td>accuracy</td>
                        </tr>
            
                        <tr>
                            <td class="zh">实验</td>
                            <td>shí yàn</td>
                            <td>experiment</td>
                        </tr>
            
                        <tr>
                            <td class="zh">内在</td>
                            <td>nèi zài</td>
                            <td>intrinsic</td>
                        </tr>
            
                        <tr>
                            <td class="zh">依赖</td>
                            <td>yī lài</td>
                            <td>rely</td>
                        </tr>
            
                        <tr>
                            <td class="zh">外部奖励</td>
                            <td>wài bù jiǎng lì</td>
                            <td>external reward</td>
                        </tr>
            
                        <tr>
                            <td class="zh">系统</td>
                            <td>xì tǒng</td>
                            <td>system</td>
                        </tr>
            
                        <tr>
                            <td class="zh">性能</td>
                            <td>xìng néng</td>
                            <td>performance</td>
                        </tr>
            
                    </tbody>
                </table>
            </div>
        </body>
        </html>
        