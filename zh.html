
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
            <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}
                gtag('js', new Date());
                gtag('config', 'G-C1CRWDNJ1J');
            </script>
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap" rel="stylesheet">
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Chinese reading task about ML</title>
            <style>
                body {
                    font-family: Arial, sans-serif;
                    background-color: #f4f4f9;
                    color: #333;
                    margin: 0;
                    padding: 20px;
                }
                .container {
                    max-width: 800px;
                    margin: 0 auto;
                    background-color: #fff;
                    padding: 20px;
                    border-radius: 8px;
                    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                }
                h1 {
                    color: #0056b3;
                    text-align: center;
                }
                p {
                    line-height: 1.6;
                }
                .zh-text {
                    font-size: 1.3em;
                    font-family: 'Noto Sans SC';
                    font-weight: 300;
                    margin: 0 0 5px 0;
                }
                .pinyin {
                    padding-top: 5px;
                    padding-bottom: 5px;
                    font-style: italic;
                    color: #888;
                }
                table {
                    width: 100%;
                    border-collapse: collapse;
                    margin-top: 20px;
                }
                th, td {
                    padding: 12px;
                    border: 1px solid #ddd;
                    text-align: left;
                }
                th {
                    background-color: #0056b3;
                    color: #fff;
                }
                td {
                    background-color: #f9f9f9;
                }
                td.zh {
                    font-family: 'Noto Sans SC';
                    font-size: 1.2em;
                    font-weight: 400;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</h1>
                <div><p class='zh-text'>1. 这篇文章介绍了一种加速注意力计算的方法，称为 SageAttention2。</p>
<p class='zh-text'>2. 它使用 4-bit 矩阵乘法和精度增强技术。</p>
<p class='zh-text'>3. 首先，将矩阵 Q 和 K 量化为 INT4，将矩阵 P 和 V 量化为 FP8。</p>
<p class='zh-text'>4. 然后，提出平滑 Q 和 V 的方法，提高注意力精度。</p>
<p class='zh-text'>5. 实验证明，这种方法在多种模型上几乎没有性能损失，并且在 RTX4090 上的操作速度是 FlashAttention2 和 xformers 的 3 倍和 5 倍。</p>
<p class='zh-text'>6. 代码已在 GitHub 上公开。</p></div>
                <div class="pinyin">
                    <p>1. 这篇文章介绍了一种加速注意力计算的方法，称为 SageAttention2。它使用 4-bit 矩阵乘法和精度增强技术。首先，将矩阵 Q 和 K 量化为 INT4，将矩阵 P 和 V 量化为 FP8。然后，提出平滑 Q 和 V 的方法，提高注意力精度。实验证明，这种方法在多种模型上几乎没有性能损失，并且在 RTX4090 上的操作速度是 FlashAttention2 和 xformers 的 3 倍和 5 倍。代码已在 GitHub 上公开。

zhè piān wén zhāng jiè shào le yī zhǒng jiā sù zhù yì lì jì suàn de fāng fǎ, chēng wéi SageAttention2</p>
<p>2.  tā shǐ yòng 4-bit jǔ zhèn chéng fǎ hé jīng dù zēng qiáng jì shù</p>
<p>3.  shǒu xiān, jiāng jǔ zhèn Q hé K liàng huà wéi INT4, jiāng jǔ zhèn P hé V liàng huà wéi FP8</p>
<p>4.  rán hòu, tí chū píng huá Q hé V de fāng fǎ, tí gāo zhù yì lì jīng dù</p>
<p>5.  shí yàn zhèng míng, zhè zhǒng fāng fǎ zài duō zhǒng mó xíng shàng jī hū méi yǒu xìng néng sǔn shī, bìng qiě zài RTX4090 shàng de cāo zuò sù dù shì FlashAttention2 hé xformers de 3 bèi hé 5 bèi</p>
<p>6.  dài mǎ yǐ zài GitHub shàng gōng kāi</p>
                </div>
                <div><p>1. This article introduces a method for accelerating attention computation, called SageAttention2.</p>
<p>2.  It employs 4-bit matrix multiplication and precision enhancement techniques.</p>
<p>3.  First, matrices Q and K are quantized to INT4, while matrices P and V are quantized to FP8.</p>
<p>4.  Then, a method for smoothing Q and V is proposed to improve attention precision.</p>
<p>5.  Experiments demonstrate that this method incurs almost no performance loss across various models and operates at speeds that are 3 times and 5 times faster than FlashAttention2 and xformers, respectively, on the RTX4090.</p>
<p>6.  The code has been made publicly available on GitHub.</p></div>
                <h2>Vocabulary</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Word</th>
                            <th>Pinyin</th>
                            <th>Translation</th>
                        </tr>
                    </thead>
                    <tbody>
        
                        <tr>
                            <td class="zh">加速</td>
                            <td>jiā sù</td>
                            <td>accelerate</td>
                        </tr>
            
                        <tr>
                            <td class="zh">注意力</td>
                            <td>zhù yì lì</td>
                            <td>attention</td>
                        </tr>
            
                        <tr>
                            <td class="zh">计算</td>
                            <td>jì suàn</td>
                            <td>calculation</td>
                        </tr>
            
                        <tr>
                            <td class="zh">方法</td>
                            <td>fāng fǎ</td>
                            <td>method</td>
                        </tr>
            
                        <tr>
                            <td class="zh">称为</td>
                            <td>chēng wéi</td>
                            <td>called</td>
                        </tr>
            
                        <tr>
                            <td class="zh">矩阵</td>
                            <td>jǔ zhèn</td>
                            <td>matrix</td>
                        </tr>
            
                        <tr>
                            <td class="zh">乘法</td>
                            <td>chén fǎ</td>
                            <td>multiplication</td>
                        </tr>
            
                        <tr>
                            <td class="zh">精度</td>
                            <td>jīng dù</td>
                            <td>precision</td>
                        </tr>
            
                        <tr>
                            <td class="zh">增强</td>
                            <td>zēng qiáng</td>
                            <td>enhancement</td>
                        </tr>
            
                        <tr>
                            <td class="zh">技术</td>
                            <td>jì shù</td>
                            <td>technology</td>
                        </tr>
            
                        <tr>
                            <td class="zh">量化</td>
                            <td>liàng huà</td>
                            <td>quantization</td>
                        </tr>
            
                        <tr>
                            <td class="zh">平滑</td>
                            <td>píng huá</td>
                            <td>smooth</td>
                        </tr>
            
                        <tr>
                            <td class="zh">提高</td>
                            <td>tí gāo</td>
                            <td>improve</td>
                        </tr>
            
                        <tr>
                            <td class="zh">实验</td>
                            <td>shí yàn</td>
                            <td>experiment</td>
                        </tr>
            
                        <tr>
                            <td class="zh">证明</td>
                            <td>zhèng míng</td>
                            <td>prove</td>
                        </tr>
            
                        <tr>
                            <td class="zh">性能</td>
                            <td>xìng néng</td>
                            <td>performance</td>
                        </tr>
            
                        <tr>
                            <td class="zh">损失</td>
                            <td>sǔn shī</td>
                            <td>loss</td>
                        </tr>
            
                        <tr>
                            <td class="zh">操作</td>
                            <td>cāo zuò</td>
                            <td>operation</td>
                        </tr>
            
                        <tr>
                            <td class="zh">速度</td>
                            <td>sù dù</td>
                            <td>speed</td>
                        </tr>
            
                        <tr>
                            <td class="zh">公开</td>
                            <td>gōng kāi</td>
                            <td>public</td>
                        </tr>
            
                        <tr>
                            <td class="zh">代码</td>
                            <td>dài mǎ</td>
                            <td>code</td>
                        </tr>
            
                    </tbody>
                </table>
            </div>
        </body>
        </html>
        