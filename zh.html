
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-C1CRWDNJ1J');
        </script>
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap" rel="stylesheet">
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Chinese reading task about ML</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                background-color: #f4f4f9;
                color: #333;
                margin: 0;
                padding: 20px;
            }
            .container {
                max-width: 800px;
                margin: 0 auto;
                background-color: #fff;
                padding: 20px;
                border-radius: 8px;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            }
            h1 {
                color: #0056b3;
                text-align: center;
            }
            p {
                line-height: 1.6;
            }
            .zh-text {
                font-size: 1.3em;
                font-family: 'Noto Sans SC';
                font-weight: 300;
                margin: 0 0 5px 0;
            }
            .pinyin {
                padding-top: 5px;
                padding-bottom: 5px;
                font-style: italic;
                color: #888;
            }
            table {
                width: 100%;
                border-collapse: collapse;
                margin-top: 20px;
            }
            th, td {
                padding: 12px;
                border: 1px solid #ddd;
                text-align: left;
            }
            th {
                background-color: #0056b3;
                color: #fff;
            }
            td {
                background-color: #f9f9f9;
            }
            td.zh {
                font-family: 'Noto Sans SC';
                font-size: 1.2em;
                font-weight: 400;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models</h1>
            <div><p class='zh-text'>1. 这篇文章介绍了一种名为MIA-DPO的视觉偏好对齐方法，旨在处理多图像输入。</p>
<p class='zh-text'>2. 现有方法主要针对单图像场景，难以有效处理多图像任务。</p>
<p class='zh-text'>3. MIA-DPO通过扩展单图像数据并使用注意力值筛选错误响应，显著减少了多图像数据标注成本。</p>
<p class='zh-text'>4. 该方法在五个多图像基准测试中优于现有方法，平均性能提升3.0%至4.3%，且对单图像理解能力影响较小。</p></div>
            <div class="pinyin">
                <p>1. Zhè piān wénzhāng jièshào le yī zhǒng míng wèi MIA-DPO de shìjué piànhào duìqí fāngfǎ, zhǐ yú chǔlǐ duō túxiàng shūrù</p>
<p>2.  Xiànyǒu fāngfǎ zhǔyào zhēn duì dān túxiàng chǎngjīng, nán yǐ yǒuxiào chǔlǐ duō túxiàng rènwù</p>
<p>3.  MIA-DPO tōngguò kuòzhǎn dān túxiàng shùjù bìng shǐyòng zhùyìlì zhí xuǎn cuòwù xiǎngyìng, xiǎnzhù jiǎnshǎo le duō túxiàng shùjù biāozhù chéngběn</p>
<p>4.  Gāi fāngfǎ zài wǔ gè duō túxiàng jīzhǔn cèshì zhōng yōu yú xiànyǒu fāngfǎ, píngjūn xìngnéng tíshēng 3</p>
<p>5. 0% zhì 4</p>
<p>6. 3%, qiě duì dān túxiàng lǐjiě nénglì yǐngxiǎng jiào xiǎo</p>
            </div>
            <div><p>1. This article introduces a visual preference alignment method called MIA-DPO, designed to handle multiple image inputs.</p>
<p>2.  Existing methods primarily focus on single-image scenarios and struggle to effectively manage multi-image tasks.</p>
<p>3.  MIA-DPO extends single-image data and uses attention values to filter out incorrect responses, significantly reducing the cost of annotating multi-image data.</p>
<p>4.  This method outperforms existing approaches on five multi-image benchmark tests, with an average performance improvement of 3.</p>
<p>5. 0% to 4.</p>
<p>6. 3%, and has a minimal impact on single-image understanding capabilities.</p></div>
            <h2>Vocabulary</h2>
            <table>
                <thead>
                    <tr>
                        <th>Word</th>
                        <th>Pinyin</th>
                        <th>Translation</th>
                    </tr>
                </thead>
                <tbody>
    
                    <tr>
                        <td class="zh">视觉偏好对齐方法</td>
                        <td>shìjué piānhǎo duìqí fāngfǎ</td>
                        <td>visual preference alignment method</td>
                    </tr>
        
                    <tr>
                        <td class="zh">多图像输入</td>
                        <td>duō túxiàng shūrù</td>
                        <td>multi-image input</td>
                    </tr>
        
                    <tr>
                        <td class="zh">单图像场景</td>
                        <td>dān túxiàng chǎngjǐng</td>
                        <td>single-image scenario</td>
                    </tr>
        
                    <tr>
                        <td class="zh">难以有效处理</td>
                        <td>nányǐ yǒuxiào chǔlǐ</td>
                        <td>difficult to effectively handle</td>
                    </tr>
        
                    <tr>
                        <td class="zh">多图像任务</td>
                        <td>duō túxiàng rènwù</td>
                        <td>multi-image task</td>
                    </tr>
        
                    <tr>
                        <td class="zh">扩展单图像数据</td>
                        <td>kuòzhǎn dān túxiàng shùjù</td>
                        <td>extend single-image data</td>
                    </tr>
        
                    <tr>
                        <td class="zh">注意力值</td>
                        <td>zhùyìlì zhí</td>
                        <td>attention value</td>
                    </tr>
        
                    <tr>
                        <td class="zh">筛选错误响应</td>
                        <td>shāixuǎn cuòwù xiǎngyìng</td>
                        <td>screen out incorrect responses</td>
                    </tr>
        
                    <tr>
                        <td class="zh">显著减少</td>
                        <td>xiǎnzhù jiǎnshǎo</td>
                        <td>significantly reduce</td>
                    </tr>
        
                    <tr>
                        <td class="zh">多图像数据标注成本</td>
                        <td>duō túxiàng shùjù biāozhù chéngběn</td>
                        <td>multi-image data annotation cost</td>
                    </tr>
        
                    <tr>
                        <td class="zh">五个多图像基准测试</td>
                        <td>wǔ gè duō túxiàng jīzhǔn cèshì</td>
                        <td>five multi-image benchmark tests</td>
                    </tr>
        
                    <tr>
                        <td class="zh">优于现有方法</td>
                        <td>yōu yú xiànyǒu fāngfǎ</td>
                        <td>superior to existing methods</td>
                    </tr>
        
                    <tr>
                        <td class="zh">平均性能提升</td>
                        <td>píngjūn xìngnéng tíshēng</td>
                        <td>average performance improvement</td>
                    </tr>
        
                    <tr>
                        <td class="zh">对单图像理解能力影响较小</td>
                        <td>duì dān túxiàng lǐjiě nénglì yǐngxiǎng jiào xiǎo</td>
                        <td>minimal impact on single-image understanding capability</td>
                    </tr>
        
                </tbody>
            </table>
        </div>
    </body>
    </html>
    