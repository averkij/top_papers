[24.10.2024 12:24] [Experimental] Generating an image for paper MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models.
[24.10.2024 12:24] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models' Text: 'Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO). Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs. We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs. MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations. Our observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on. Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs. MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images.'
[24.10.2024 12:24] Response: **Prompt:** A surreal linear art composition on a white background featuring a chaotic grid of abstract images representing a variety of unrelated visual elements. In the foreground, an oversized golden key symbolizes 'unlocking potential,' surrounded by distorted human figures with exaggerated facial expressions, illustrating the complexity of visual preferences. Above the grid, a title label floats in the air, elegantly inscribed: **"MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models"**. The entire scene evokes a sense of disconnection and fluidity, reflecting the intricate interplay of images and preferences in the world of AI.
[24.10.2024 12:24] Generating image by prompt: **Prompt:** A surreal linear art composition on a white background featuring a chaotic grid of abstract images representing a variety of unrelated visual elements. In the foreground, an oversized golden key symbolizes 'unlocking potential,' surrounded by distorted human figures with exaggerated facial expressions, illustrating the complexity of visual preferences. Above the grid, a title label floats in the air, elegantly inscribed: **"MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models"**. The entire scene evokes a sense of disconnection and fluidity, reflecting the intricate interplay of images and preferences in the world of AI..
[24.10.2024 12:24] Saving generated image from https://fal.media/files/lion/t3ET77Vdem0Z3VUuqE6ru.png to 2dc1395b8aa096fc.jpg.
