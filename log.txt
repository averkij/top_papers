[28.10.2024 22:12] [Experimental] Generating an image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting.
[28.10.2024 22:12] [Experimental] Image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Continuous Speech Synthesis using per-token Latent Diffusion.
[28.10.2024 22:12] [Experimental] Image for paper Continuous Speech Synthesis using per-token Latent Diffusion already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Teach Multimodal LLMs to Comprehend Electrocardiographic Images.
[28.10.2024 22:12] [Experimental] Image for paper Teach Multimodal LLMs to Comprehend Electrocardiographic Images already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design.
[28.10.2024 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design' Text: 'The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.'
[28.10.2024 22:12] Response: **Prompt:** Create a surrealistic linear art piece on a white background depicting a large, intricate machine with flowing gears and cogs that resemble brains, symbolizing the complexity of large language models. Intertwine vibrant lines representing dynamic subnetworks, branching out into various pathways, each leading to a unique small expert figure, illustrating the concept of Mixture-of-Experts. Include abstract representations of memory management and batching, such as floating cubes and spirals that hint at inefficiency. Above the machine, place a translucent banner that reads: **"Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design"**. In the background, faintly show a landscape of cloud-like formations representing pre-trained models, with sparks of light indicating activation sparsity.
[28.10.2024 22:12] Generating image by prompt: **Prompt:** Create a surrealistic linear art piece on a white background depicting a large, intricate machine with flowing gears and cogs that resemble brains, symbolizing the complexity of large language models. Intertwine vibrant lines representing dynamic subnetworks, branching out into various pathways, each leading to a unique small expert figure, illustrating the concept of Mixture-of-Experts. Include abstract representations of memory management and batching, such as floating cubes and spirals that hint at inefficiency. Above the machine, place a translucent banner that reads: **"Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design"**. In the background, faintly show a landscape of cloud-like formations representing pre-trained models, with sparks of light indicating activation sparsity..
[28.10.2024 22:12] Saving generated image from https://fal.media/files/panda/JKpjO_b92j6DQ67H7iCEu.png to 94c8bf1991abe8b0.jpg.
[28.10.2024 22:12] [Experimental] Generating an image for paper FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality.
[28.10.2024 22:12] [Experimental] Image for paper FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data.
[28.10.2024 22:12] [Experimental] Image for paper Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance.
[28.10.2024 22:12] [Experimental] Image for paper Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark.
[28.10.2024 22:12] [Experimental] Image for paper MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark already exists.
