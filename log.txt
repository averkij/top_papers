[04.10.2024 13:56] Get feed.
[04.10.2024 13:56] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[04.10.2024 13:56] Abstract 1. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[04.10.2024 13:56] Abstract 2. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[04.10.2024 13:56] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[04.10.2024 13:56] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[04.10.2024 13:56] Abstract 5. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[04.10.2024 13:56] Abstract 6. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[04.10.2024 13:56] Abstract 7. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[04.10.2024 13:56] Abstract 8. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[04.10.2024 13:56] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[04.10.2024 13:56] Abstract 10. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[04.10.2024 13:56] Abstract 11. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[04.10.2024 13:56] Abstract 12. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[04.10.2024 13:56] Abstract 13. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[04.10.2024 13:56] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[04.10.2024 13:56] Abstract 15. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[04.10.2024 13:56] Abstract 16. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[04.10.2024 13:56] Abstract 17. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[04.10.2024 13:56] Abstract 18. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[04.10.2024 13:56] Read previous papers.
[04.10.2024 13:56] Generating reviews via LLM API.
[04.10.2024 13:56] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∏ –∏ AltText, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π, —É–ª—É—á—à–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º –ø–æ–¥–ø–∏—Å–µ–π.",
  "tags": ["#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ–ú–æ–¥–µ–ª–∏", "#—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ–ü–æ–¥–ø–∏—Å–∏", "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è"],
  "emoji": "üñºÔ∏è",
  "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
}
[04.10.2024 13:56] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–¥–µ–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ —Å –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –ò—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤—ã–≤–æ–¥–∞, –≤–∫–ª—é—á–∞—è –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫. –ú–æ–¥–µ–ª—å Loong, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 10-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –≤–∏–¥–µ–æ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ.",
  "tags": ["#–≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è", "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ–ú–æ–¥–µ–ª–∏", "#–¥–ª–∏–Ω–Ω—ã–µ–í–∏–¥–µ–æ"],
  "emoji": "üé¨",
  "title": "Loong: –ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[04.10.2024 13:56] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ-–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –ø—É—Ç–µ–º —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö LLaVA-Video-178K. –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è, –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª—å LLaVA-Video, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ-–±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –ø—Ä–æ—Ü–µ—Å—Å –µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–∏.",
  "tags": ["#–≤–∏–¥–µ–æLMM", "#—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ–î–∞–Ω–Ω—ã–µ", "#–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–∞—è–ù–∞—Å—Ç—Ä–æ–π–∫–∞"],
  "emoji": "üé¨",
  "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –≤–∏–¥–µ–æ-LMM"
}
[04.10.2024 13:56] Got response. {
  "desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω–∞—è –∫—Ä—É–ø–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –¥–≤—É—Ö –∫–ª—é—á–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö: –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏–∏ –ø–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–µ –∏ –æ—Ü–µ–Ω–∫–µ.",
  "tags": ["#multimodalEvaluation", "#preferenceLearning", "#LLMasJudge"],
  "emoji": "üßê",
  "title": "LLaVA-Critic: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–π –æ—Ü–µ–Ω–∫–∏"
}
[04.10.2024 13:56] Got response. {
  "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Contrastive Localized Language-Image Pre-training (CLOC). CLOC —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å CLIP –∫ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é promptable embeddings - –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–≥–∫–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –≤ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
  "tags": ["#CLOC", "#VisualLocalization", "#RegionEmbeddings"],
  "emoji": "üîç",
  "title": "CLOC: –£–ª—É—á—à–µ–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"
}
[04.10.2024 13:57] Got response. {
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é, –Ω–µ —Ç—Ä–µ–±—É—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª–∏–ª–æ –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–∞–∫ –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–∫–∞—Ö, —Ç–∞–∫ –∏ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≥—Ä–∞–Ω–∏—Ü –æ–±—ä–µ–∫—Ç–æ–≤.",
  "tags": ["#MonocularDepthEstimation", "#ZeroShotLearning", "#VisionTransformer"],
  "emoji": "üîç",
  "title": "Depth Pro: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –≥–ª—É–±–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ PPO –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. VinePPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ü–µ–Ω–∫–∏ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –≤–º–µ—Å—Ç–æ —Å–µ—Ç–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö MATH –∏ GSM8K, —Ç—Ä–µ–±—É—è –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.",
  "tags": ["#reinforcement-learning", "#credit-assignment", "#language-models"],
  "emoji": "üåø",
  "title": "VinePPO: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —Å–µ—Ç–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞ –Ω–∞ –∫–æ–Ω–µ—á–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –û–Ω–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ –≤–ª–∏—è–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≥—Ä–∞–Ω–∏—Ü –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ –æ–±–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞ —Ç–∞–∫–∂–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM.",
  "tags": ["#MarkovChains", "#LLMTheory", "#StatisticalLearning"],
  "emoji": "üß†",
  "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ü–µ–ø–µ–π –ú–∞—Ä–∫–æ–≤–∞"
}
[04.10.2024 13:57] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Diversified Multiplet Upcycling (DMU) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π CLIP. DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–æ–æ–±—É—á–∞—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑ –æ–¥–Ω–æ–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ CLIP. –≠—Ç–∏ –º–æ–¥–µ–ª–∏ –∑–∞—Ç–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –≤ CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ CLIP-MoE –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "tags": ["#CLIP", "#MoE", "#multimodal-learning"],
  "emoji": "üîÑ",
  "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ CLIP —Å –ø–æ–º–æ—â—å—é Diversified Multiplet Upcycling"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –º–∞—Å—à—Ç–∞–±—ã —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –ø–µ—Ä–µ—Å—ã—â–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å –≤—Ö–æ–¥–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏. –ú–µ—Ç–æ–¥ –ª–µ–≥–∫–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç, –ø–æ–∫–∞–∑—ã–≤–∞—è —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º FID, recall –∏ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏.",

  "tags": [
    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ–º–æ–¥–µ–ª–∏",
    "#–±–µ–∑–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–Ω–æ–µ—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
    "#–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ–ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–µ—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
  ],

  "emoji": "üé®",

  "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –ø–µ—Ä–µ—Å—ã—â–µ–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention2 –∏ xformers, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–æ–¥–µ–ª—è–º –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.",
  "tags": ["#–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ_–≤–Ω–∏–º–∞–Ω–∏—è", "#—É—Å–∫–æ—Ä–µ–Ω–∏–µ_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"],
  "emoji": "‚ö°",
  "title": "SageAttention: –ë—ã—Å—Ç—Ä–æ–µ –∏ —Ç–æ—á–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ 3D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–¥–Ω–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–∞—Ö. –û–Ω–∏ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç —Å—Ö–µ–º—É –º–µ–∂—Ä–∞–∫—É—Ä—Å–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞, —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫—Ä–æ—Å—Å-–ª—É—á–µ–≤–æ–≥–æ —É–ø–ª–æ—Ç–Ω–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø–ª–æ—Ç–Ω–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –≠—Ç–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ –æ–±—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "tags": ["#3DGaussianSplatting", "#MultiViewRendering", "#NovelViewSynthesis"],
  "emoji": "üé≠",
  "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ: –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è 3DGS"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –∑–∞–∫—Ä—ã—Ç—ã—Ö –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ RAG –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.",
  "tags": ["#LongContextModels", "#CitationEvaluation", "#ModelFaithfulness"],
  "emoji": "üìè",
  "title": "L-CiteEval: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
}
[04.10.2024 13:57] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. LintSeq –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–∫, –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏–Ω—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ –±–µ–∑–æ—à–∏–±–æ—á–Ω—ã—Ö –≤—Å—Ç–∞–≤–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –æ–±—É—á–∏–≤ –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–∏–≤ –∏—Ö —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –ø—Ä–∞–≤–æ–∫, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –∫–æ–¥ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ª—É—á—à—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ–¥–∞.",

  "tags": ["#CodeEditing", "#SyntheticDataGeneration", "#LintSeq"],

  "emoji": "‚úèÔ∏è",

  "title": "LintSeq: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∫–æ–¥–∞"
}
[04.10.2024 13:57] Got response. {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–ª–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–π –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ LLM –≤ —ç–Ω–∫–æ–¥–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision Transformer (ViT) –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –±–ª–æ–∫ –º—É–ª—å—Ç–∏–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ Dice —Å 0,74 –¥–æ 0,79.",

  "tags": [
    "#MedicalImageSegmentation",
    "#VisionTransformers",
    "#LLMIntegration"
  ],

  "emoji": "üè•",

  "title": "LLM-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø–æ–≤—ã—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
[04.10.2024 13:58] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫—Ä—É–ø–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LMM) –≤—Å–µ –µ—â–µ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å GPT-4o –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–ª—å–∫–æ ~50% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤ ~90%. –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–µ—Ä–µ—à–µ–Ω–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",

  "tags": ["#TemporalReasoning", "#MultimodalBenchmark", "#VideoUnderstanding"],

  "emoji": "‚è≥",

  "title": "–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ: –Ω–µ–ø–æ–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—à–∏–Ω–∞ –¥–ª—è –ò–ò"
}
[04.10.2024 13:58] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Distilled Voice Assistant) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —É—Å—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞. DiVA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ 100 —Ä–∞–∑ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.",
  "tags": ["#SpeechLLM", "#UnsupervisedLearning", "#VoiceAssistants"],
  "emoji": "üó£Ô∏è",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"
}
[04.10.2024 13:58] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è, —á—Ç–æ –º–∞–ª—ã–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–∑ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å 28 –∏ 125 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª —à–∞—Ö–º–∞—Ç, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ª–µ–≥–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏—è —à–∞—Ö–º–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –ò–∑—É—á–µ–Ω–æ –≤–ª–∏—è–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ø–æ—Ö –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å–Ω–∏–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.",
  "tags": ["#–º–∞–ª—ã–µ—è–∑—ã–∫–æ–≤—ã–µ–º–æ–¥–µ–ª–∏", "#–æ–±—É—á–µ–Ω–∏–µ—à–∞—Ö–º–∞—Ç–∞–º", "#–∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ"],
  "emoji": "‚ôüÔ∏è",
  "title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—Å–≤–∞–∏–≤–∞—é—Ç —à–∞—Ö–º–∞—Ç—ã —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
}
[04.10.2024 13:58] Got response. {
    "desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ—Å—ç–º–ø–ª–æ–≤. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç–µ—Ö–Ω–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
    "tags": ["#AudioAugmentation", "#T2ADiffusion", "#LimitedDataLearning"],
    "emoji": "üéµ",
    "title": "Synthio: –°–∏–Ω—Ç–µ–∑ –∞—É–¥–∏–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
}
[04.10.2024 13:58] Writing result.
[04.10.2024 13:58] Done.
[04.10.2024 14:44] Get feed.
[04.10.2024 14:44] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[04.10.2024 14:44] Abstract 1. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[04.10.2024 14:44] Abstract 2. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[04.10.2024 14:44] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[04.10.2024 14:44] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[04.10.2024 14:44] Abstract 5. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[04.10.2024 14:44] Abstract 6. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[04.10.2024 14:44] Abstract 7. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[04.10.2024 14:44] Abstract 8. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[04.10.2024 14:44] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[04.10.2024 14:44] Abstract 10. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[04.10.2024 14:44] Abstract 11. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[04.10.2024 14:44] Abstract 12. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[04.10.2024 14:44] Abstract 13. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[04.10.2024 14:44] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[04.10.2024 14:44] Abstract 15. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[04.10.2024 14:44] Abstract 16. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[04.10.2024 14:44] Abstract 17. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[04.10.2024 14:44] Abstract 18. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[04.10.2024 14:44] Read previous papers.
[04.10.2024 14:44] Generating reviews via LLM API.
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02740", "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models", "abstract": "Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02757", "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models", "abstract": "It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have ac
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02713", "title": "Video Instruction Tuning With Synthetic Data", "abstract": "The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we pr
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02712", "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models", "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal ta
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02746", "title": "Contrastive Localized Language-Image Pre-Training", "abstract": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applicati
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02724", "title": "Large Language Models as Markov Chains", "abstract": "Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoret
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02073", "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second", "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled shar
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.01679", "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment", "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receivi
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2409.19291", "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling", "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02416", "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models", "abstract": "Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output 
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02367", "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration", "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02103", "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis", "abstract": "Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02749", "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis", "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs i
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02115", "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?", "abstract": "Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02458", "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation", "abstract": "Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their po
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02763", "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos", "abstract": "There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short vi
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02678", "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data", "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased co
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02426", "title": "Learning the Latent Rules of a Game from Data: A Chess Story", "abstract": "We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associat
[04.10.2024 14:44] Using data from previous issue: {"id": "https://huggingface.co/papers/2410.02056", "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data", "abstract": "We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audi
[04.10.2024 14:44] Renaming data file.
[04.10.2024 14:44] Renaming previous data. hf_papers.json to 2024-10-03_hf_papers.json
[04.10.2024 14:44] Saving new data file.
[04.10.2024 14:44] Generating page.
[04.10.2024 14:44] Renaming previous page.
[04.10.2024 14:44] Renaming previous data. index.html to 2024-10-03_papers.html
[04.10.2024 14:44] Writing result.
[04.10.2024 14:44] Done.
