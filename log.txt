[04.10.2024 15:12] Get feed.
[04.10.2024 15:12] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[04.10.2024 15:12] Abstract 1. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[04.10.2024 15:12] Abstract 2. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[04.10.2024 15:12] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[04.10.2024 15:12] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[04.10.2024 15:12] Abstract 5. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[04.10.2024 15:12] Abstract 6. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[04.10.2024 15:12] Abstract 7. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[04.10.2024 15:12] Abstract 8. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[04.10.2024 15:12] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[04.10.2024 15:12] Abstract 10. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[04.10.2024 15:12] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[04.10.2024 15:12] Abstract 12. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[04.10.2024 15:12] Abstract 13. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[04.10.2024 15:12] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[04.10.2024 15:12] Abstract 15. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[04.10.2024 15:12] Abstract 16. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[04.10.2024 15:12] Abstract 17. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[04.10.2024 15:12] Abstract 18. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[04.10.2024 15:12] No previous papers found.
[04.10.2024 15:12] Generating reviews via LLM API.
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∏ –∏ AltText, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π. –í—ã—è–≤–ª–µ–Ω—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º –ø–æ–¥–ø–∏—Å–µ–π.",
  "tags": ["#multimodal_captioning", "#synthetic_captions", "#model_specific_optimization"],
  "emoji": "üñºÔ∏è",
  "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–ø–µ—Ö—É"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ –∏ –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ 10-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –≤–∏–¥–µ–æ, –Ω–æ –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ —Ä–æ–ª–∏–∫–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –≤—ã–≤–æ–¥–∞. Loong –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –≤–∏–¥–µ–æ-—Ç–æ–∫–µ–Ω—ã –≤ –µ–¥–∏–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è LLM.",
  "tags": ["#–≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è", "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µLLM", "#–¥–ª–∏–Ω–Ω—ã–µ–≤–∏–¥–µ–æ"],
  "emoji": "üé¨",
  "title": "Loong: –ø—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LLM"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ-–±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ, –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º.",
  "tags": ["#videoLLM", "#syntheticData", "#instructionFollowing"],
  "emoji": "üé¨",
  "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –≤–∏–¥–µ–æ-–ò–ò"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å—É–¥—å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–µ –∏ –æ—Ü–µ–Ω–∫–µ.",
  "tags": ["#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è_–æ—Ü–µ–Ω–∫–∞", "#–æ–±—É—á–µ–Ω–∏–µ_–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º", "#LLaVA"],
  "emoji": "üß†",
  "title": "LLaVA-Critic: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'. –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤. CLOC –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –æ–±–ª–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "tags": ["#CLOC", "#—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏", "#–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è_–æ–±—ä–µ–∫—Ç–æ–≤"],
  "emoji": "üîç",
  "title": "CLOC: –¢–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≥—Ä–∞–Ω–∏—Ü –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ –æ–±–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞ —Ç–∞–∫–∂–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–∏—Ä–æ–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "tags": [
    "#MarkovChains",
    "#LLMTheory",
    "#StatisticalLearning"
  ],
  "emoji": "üîó",
  "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ LLM —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ü–µ–ø–µ–π –ú–∞—Ä–∫–æ–≤–∞"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º vision transformer –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.",
  "tags": ["#MonocularDepthEstimation", "#ZeroShotLearning", "#VisionTransformer"],
  "emoji": "üîç",
  "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã: Depth Pro - –±—ã—Å—Ç—Ä—ã–π –∏ —Ç–æ—á–Ω—ã–π –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:12] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –≤–º–µ—Å—Ç–æ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π. VinePPO –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö MATH –∏ GSM8K –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å PPO –∏ –¥—Ä—É–≥–∏–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
  "tags": ["#creditAssignment", "#VinePPO", "#LLMFinetuning"],
  "emoji": "üåø",
  "title": "VinePPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞ –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM"
}
[04.10.2024 15:12] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å CLIP –≤ –∑–∞–¥–∞—á–∞—Ö zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. DMU –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫ –ª—é–±–æ–π –ø–ª–æ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ CLIP, –ø–æ–∑–≤–æ–ª—è—è –ª–µ–≥–∫–æ –∑–∞–º–µ–Ω–∏—Ç—å –µ–µ –Ω–∞ CLIP-MoE –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.",
  "tags": ["#CLIP-MoE", "#DiversifiedMultipletUpcycling", "#MultimodalLearning"],
  "emoji": "üîÑ",
  "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ CLIP —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –º–∞—Å—à—Ç–∞–±—ã —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –ø–µ—Ä–µ–Ω–∞—Å—ã—â–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ CFG –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ú–µ—Ç–æ–¥ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Å–ª–æ–≤–Ω—ã–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Å—ç–º–ø–ª–µ—Ä–∞–º–∏, —É–ª—É—á—à–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ FID, recall –∏ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.",
  "tags": ["#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ_–º–æ–¥–µ–ª–∏", "#–±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–µ_—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "#–≥–µ–Ω–µ—Ä–∞—Ü–∏—è_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"],
  "emoji": "üñºÔ∏è",
  "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention2 –∏ xformers, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–æ–¥–µ–ª—è–º –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.",
  "tags": ["#–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è_–≤–Ω–∏–º–∞–Ω–∏—è", "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "#—É—Å–∫–æ—Ä–µ–Ω–∏–µ_–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"],
  "emoji": "‚ö°",
  "title": "SageAttention: –ë—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ –≤ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ–¥–∞. –ù–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç —Å GPT-4 –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 20% –ø–æ –º–µ—Ç—Ä–∏–∫–µ pass@50 –Ω–∞ HumanEval.",
  "tags": ["#CodeEditing", "#SyntheticDataGeneration", "#LintSeq"],
  "emoji": "‚úèÔ∏è",
  "title": "LintSeq: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∫–æ–¥–∞"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫—Ä–æ—Å—Å-–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç –≥—Ä—É–±–æ–≥–æ –∫ —Ç–æ—á–Ω–æ–º—É –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–ø–ª–æ—Ç–Ω–µ–Ω–∏—è –∫—Ä–æ—Å—Å-–ª—É—á–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–≤—ã—à–∞–µ—Ç –æ–±—â—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "tags": ["#3DGaussianSplatting", "#NovelViewSynthesis", "#MultiViewOptimization"],
  "emoji": "üé≠",
  "title": "–ú–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D Gaussian Splatting"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –∑–∞–∫—Ä—ã—Ç—ã—Ö –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ RAG –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.",
  "tags": ["#LongContextModels", "#CitationEvaluation", "#ModelFaithfulness"],
  "emoji": "üìè",
  "title": "L-CiteEval: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –±–ª–æ–∫ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è Dice —Å 0.74 –¥–æ 0.79. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
  "tags": ["#–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è_—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è", "#vision_transformer", "#language_model_transfer"],
  "emoji": "üè•",
  "title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ –µ—â–µ –Ω–µ —Ä–µ—à–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π.",
  "tags": ["#TemporalReasoning", "#VideoUnderstanding", "#ModelEvaluation"],
  "emoji": "‚è≥",
  "title": "–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –≤–∏–¥–µ–æ: –Ω–µ–ø–æ–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—à–∏–Ω–∞ –¥–ª—è –ò–ò"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Distilled Voice Assistant) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —É—Å—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DiVA –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ 100 —Ä–∞–∑ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.",
  "tags": ["#SpeechLLM", "#UnsupervisedLearning", "#VoiceAssistant"],
  "emoji": "üó£Ô∏è",
  "title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª —à–∞—Ö–º–∞—Ç, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ª–µ–≥–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏—è —à–∞—Ö–º–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ø–æ—Ö –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –º–æ–¥–µ–ª–∏.",
  "tags": ["#–º–∞–ª—ã–µ–Ø–∑—ã–∫–æ–≤—ã–µ–ú–æ–¥–µ–ª–∏", "#–æ–±—É—á–µ–Ω–∏–µ–ü—Ä–∞–≤–∏–ª–∞–º–®–∞—Ö–º–∞—Ç", "#—É–º–µ–Ω—å—à–µ–Ω–∏–µ–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"],
  "emoji": "‚ôüÔ∏è",
  "title": "–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –∏–≥—Ä–∞—Ç—å –≤ –±–æ–ª—å—à–∏–µ —à–∞—Ö–º–∞—Ç—ã"
}
[04.10.2024 15:13] Querying the API.
[04.10.2024 15:13] Got response. {
  "desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç–µ—Ö–Ω–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "tags": ["#AudioAugmentation", "#TextToAudio", "#DiffusionModels"],
  "emoji": "üéµ",
  "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∞—É–¥–∏–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
}
[04.10.2024 15:13] Renaming data file.
[04.10.2024 15:13] No file to rename. hf_papers.json
[04.10.2024 15:13] Saving new data file.
[04.10.2024 15:13] Generating page.
[04.10.2024 15:13] Renaming previous page.
[04.10.2024 15:13] Renaming previous data. index.html to 2024-10-03_papers.html
[04.10.2024 15:13] Writing result.
[04.10.2024 15:13] Done.
[04.10.2024 15:15] Get feed.
[04.10.2024 15:16] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[04.10.2024 15:16] Abstract 1. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[04.10.2024 15:16] Abstract 2. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[04.10.2024 15:16] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[04.10.2024 15:16] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[04.10.2024 15:16] Abstract 5. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[04.10.2024 15:16] Abstract 6. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[04.10.2024 15:16] Abstract 7. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[04.10.2024 15:16] Abstract 8. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[04.10.2024 15:16] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[04.10.2024 15:16] Abstract 10. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[04.10.2024 15:16] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[04.10.2024 15:16] Abstract 12. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[04.10.2024 15:16] Abstract 13. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[04.10.2024 15:16] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[04.10.2024 15:16] Abstract 15. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[04.10.2024 15:16] Abstract 16. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[04.10.2024 15:16] Abstract 17. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[04.10.2024 15:16] Abstract 18. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[04.10.2024 15:16] Read previous papers.
[04.10.2024 15:16] Generating reviews via LLM API.
[04.10.2024 15:16] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É
[04.10.2024 15:16] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä
[04.10.2024 15:16] Using data from previous issue: {"desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ 
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'
[04.10.2024 15:16] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Fla
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã
[04.10.2024 15:16] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ
[04.10.2024 15:16] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ 
[04.10.2024 15:16] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Di
[04.10.2024 15:16] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö 
[04.10.2024 15:16] Using data from previous issue: {"desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥
[04.10.2024 15:16] Renaming data file.
[04.10.2024 15:16] Renaming previous data. hf_papers.json to 2024-10-03_hf_papers.json
[04.10.2024 15:16] Saving new data file.
[04.10.2024 15:16] Generating page.
[04.10.2024 15:16] Renaming previous page.
[04.10.2024 15:16] Renaming previous data. index.html to 2024-10-03_papers.html
[04.10.2024 15:16] Writing result.
[04.10.2024 15:16] Done.
[04.10.2024 15:27] Get feed.
[04.10.2024 15:27] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[04.10.2024 15:27] Abstract 1. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[04.10.2024 15:27] Abstract 2. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[04.10.2024 15:27] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[04.10.2024 15:27] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[04.10.2024 15:27] Abstract 5. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[04.10.2024 15:27] Abstract 6. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[04.10.2024 15:27] Abstract 7. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[04.10.2024 15:27] Abstract 8. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[04.10.2024 15:27] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[04.10.2024 15:27] Abstract 10. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[04.10.2024 15:27] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[04.10.2024 15:27] Abstract 12. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[04.10.2024 15:27] Abstract 13. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[04.10.2024 15:27] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[04.10.2024 15:27] Abstract 15. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[04.10.2024 15:27] Abstract 16. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[04.10.2024 15:27] Abstract 17. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[04.10.2024 15:27] Abstract 18. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[04.10.2024 15:27] Read previous papers.
[04.10.2024 15:27] Generating reviews via LLM API.
[04.10.2024 15:27] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É
[04.10.2024 15:27] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä
[04.10.2024 15:27] Using data from previous issue: {"desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ 
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'
[04.10.2024 15:27] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Fla
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã
[04.10.2024 15:27] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ
[04.10.2024 15:27] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ 
[04.10.2024 15:27] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Di
[04.10.2024 15:27] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö 
[04.10.2024 15:27] Using data from previous issue: {"desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥
[04.10.2024 15:27] Renaming data file.
[04.10.2024 15:27] Renaming previous data. hf_papers.json to 2024-10-03_hf_papers.json
[04.10.2024 15:27] Saving new data file.
[04.10.2024 15:27] Generating page.
[04.10.2024 15:27] Renaming previous page.
[04.10.2024 15:27] Renaming previous data. index.html to 2024-10-03_papers.html
[04.10.2024 15:27] Writing result.
[04.10.2024 15:27] Done.
[04.10.2024 18:24] Get feed.
[04.10.2024 18:24] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[04.10.2024 18:24] Abstract 1. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[04.10.2024 18:24] Abstract 2. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[04.10.2024 18:24] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[04.10.2024 18:24] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[04.10.2024 18:24] Abstract 5. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[04.10.2024 18:24] Abstract 6. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[04.10.2024 18:24] Abstract 7. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[04.10.2024 18:24] Abstract 8. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[04.10.2024 18:24] Abstract 9. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[04.10.2024 18:24] Abstract 10. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[04.10.2024 18:24] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[04.10.2024 18:24] Abstract 12. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[04.10.2024 18:24] Abstract 13. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[04.10.2024 18:24] Abstract 14. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[04.10.2024 18:24] Abstract 15. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[04.10.2024 18:24] Abstract 16. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[04.10.2024 18:24] Abstract 17. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon plan...
[04.10.2024 18:24] Abstract 18. We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems ...
[04.10.2024 18:24] Abstract 19. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[04.10.2024 18:24] Abstract 20. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[04.10.2024 18:24] Abstract 21. Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval...
[04.10.2024 18:24] Abstract 22. We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on ...
[04.10.2024 18:24] Abstract 23. Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance leve...
[04.10.2024 18:24] Abstract 24. Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3D...
[04.10.2024 18:24] Abstract 25. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-Eng...
[04.10.2024 18:24] Abstract 26. Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we i...
[04.10.2024 18:24] Read previous papers.
[04.10.2024 18:24] Generating reviews via LLM API.
[04.10.2024 18:24] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞
[04.10.2024 18:24] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É
[04.10.2024 18:24] Using data from previous issue: {"desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ 
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú
[04.10.2024 18:24] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ
[04.10.2024 18:24] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Di
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Fla
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã
[04.10.2024 18:24] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ
[04.10.2024 18:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ 
[04.10.2024 18:24] Querying the API.
[04.10.2024 18:24] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Reflective Monte Carlo Tree Search (R-MCTS) - –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π. R-MCTS —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π MCTS, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ –ø—É—Ç–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è GPT-4o —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö R-MCTS. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ VisualWebArena –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ GPT-4o —Å R-MCTS –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 6-30% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º SOTA.",

  "tags": ["#ReflectiveMCTS", "#VisualWebArena", "#GPT4Optimization"],

  "emoji": "üå≥",

  "title": "R-MCTS: –†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤"
}
[04.10.2024 18:24] Querying the API.
[04.10.2024 18:24] Got response. {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –≤–ª–∏—è–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–µ –∫–ª–µ—Ç–æ—á–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã (ECA) –∏ –æ–±—É—á–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö ECA. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–∞ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –º–æ–¥–µ–ª—è–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–º –±–æ–ª—å—à—É—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç—Å—è –≤ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ö–æ–¥–æ–≤ –≤ —à–∞—Ö–º–∞—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.",
  "tags": ["#–∫–ª–µ—Ç–æ—á–Ω—ã–µ_–∞–≤—Ç–æ–º–∞—Ç—ã", "#—Å–ª–æ–∂–Ω–æ—Å—Ç—å_—Å–∏—Å—Ç–µ–º", "#–≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ_–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"],
  "emoji": "üß†",
  "title": "–°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–∂–¥–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: —É—Ä–æ–∫–∏ –∫–ª–µ—Ç–æ—á–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–æ–≤"
}
[04.10.2024 18:24] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö 
[04.10.2024 18:24] Using data from previous issue: {"desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥
[04.10.2024 18:24] Querying the API.
[04.10.2024 18:25] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ complementary –º–µ—Ç–æ–¥–∏–∫–∏: –Ω–æ–≤—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é —Å–æ—Å–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–¥–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥—è—Ö –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –Ω–∞–¥ –±–∏—ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ out-of-domain –∑–∞–¥–∞—á–∞—Ö. –î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MTEB –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –æ–±—É—á–µ–Ω–∏—è.",
  "tags": ["#–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏", "#–Ω–µ–π—Ä–æ–Ω–Ω—ã–π_–ø–æ–∏—Å–∫", "#–∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ"],
  "emoji": "üß†",
  "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
}
[04.10.2024 18:25] Querying the API.
[04.10.2024 18:25] Got response. {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π VLM –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å –∏ –Ω–∞–±–ª—é–¥–∞—é—Ç –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–∏—Ä–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —É–¥–∞–ª—è–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø—É—Ç–µ–º –ª–∏–Ω–µ–π–Ω–æ–π –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–∞ 25.7% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö COCO2014.",
  "tags": ["#VLM", "#–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏", "#—Å—Ç–∏—Ä–∞–Ω–∏–µ–∑–Ω–∞–Ω–∏–π"],
  "emoji": "üß†",
  "title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ VLM —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π"
}
[04.10.2024 18:25] Querying the API.
[04.10.2024 18:25] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SciPrompt - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏, –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–∞ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. SciPrompt –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.",
  "tags": ["#prompt-based-fine-tuning", "#scientific-text-classification", "#low-resource-nlp"],
  "emoji": "üß™",
  "title": "SciPrompt: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –Ω–∞—É—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
}
[04.10.2024 18:25] Querying the API.
[04.10.2024 18:25] Got response. {
  "desc": "Robin3D - —ç—Ç–æ –º–æ—â–Ω–∞—è 3D –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö RIG, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. Robin3D –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è Relation-Augmented Projector –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ ID-Feature Bonding –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–Ω–µ—Å–µ–Ω–∏—è –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –ø—è—Ç–∏ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö 3D –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏.",
  "tags": ["#3DLLM", "#InstructionFollowing", "#3DMultimodalLearning"],
  "emoji": "ü§ñ",
  "title": "Robin3D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"
}
[04.10.2024 18:25] Querying the API.
[04.10.2024 18:25] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –æ–±—É—á–∞—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö '—ç–∫—Å–ø–µ—Ä—Ç–æ–≤' –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –æ–±—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ, –∞ –∑–∞—Ç–µ–º –∑–∞–º–µ–Ω—è—é—Ç –≤–µ—Ä—Ö–Ω–∏–µ –∏ –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞ —Å–ª–æ—è–º–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MGSM –Ω–∞ 10% –¥–ª—è —á–µ—Ç—ã—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤.",

  "tags": ["#modelMerging", "#crossLingualTransfer", "#mathematicalReasoning"],

  "emoji": "üß†",

  "title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–≤—ã–∫–æ–≤"
}
[04.10.2024 18:25] Querying the API.
[04.10.2024 18:25] Got response. {
  "desc": "Open-RAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Retrieval-Augmented Generation —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–ª–æ—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—É—é —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Open-RAG –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∑–∞–ø—É—Ç—ã–≤–∞—é—â–∏–º–∏ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Open-RAG –Ω–∞ –±–∞–∑–µ Llama2-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ RAG-–º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—à–∏—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π.",
  "tags": ["#RetrievalAugmentedGeneration", "#MixtureOfExperts", "#LatentLearning"],
  "emoji": "üß†",
  "title": "Open-RAG: –£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
[04.10.2024 18:25] Renaming data file.
[04.10.2024 18:25] Renaming previous data. hf_papers.json to 2024-10-03_hf_papers.json
[04.10.2024 18:25] Saving new data file.
[04.10.2024 18:25] Generating page.
[04.10.2024 18:25] Renaming previous page.
[04.10.2024 18:25] Renaming previous data. index.html to 2024-10-03_papers.html
[04.10.2024 18:25] Writing result.
[04.10.2024 18:25] Done.
[05.10.2024 01:21] Get feed.
[05.10.2024 01:21] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[05.10.2024 01:21] Abstract 1. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[05.10.2024 01:21] Abstract 2. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[05.10.2024 01:21] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[05.10.2024 01:21] Abstract 4. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[05.10.2024 01:21] Abstract 5. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[05.10.2024 01:21] Abstract 6. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[05.10.2024 01:21] Abstract 7. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[05.10.2024 01:21] Abstract 8. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[05.10.2024 01:21] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[05.10.2024 01:21] Abstract 10. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[05.10.2024 01:21] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[05.10.2024 01:21] Abstract 12. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[05.10.2024 01:21] Abstract 13. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[05.10.2024 01:21] Abstract 14. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[05.10.2024 01:21] Abstract 15. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[05.10.2024 01:21] Abstract 16. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[05.10.2024 01:21] Abstract 17. We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on ...
[05.10.2024 01:21] Abstract 18. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-Eng...
[05.10.2024 01:21] Abstract 19. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon plan...
[05.10.2024 01:21] Abstract 20. We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems ...
[05.10.2024 01:21] Abstract 21. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[05.10.2024 01:21] Abstract 22. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[05.10.2024 01:21] Abstract 23. Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval...
[05.10.2024 01:21] Abstract 24. Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance leve...
[05.10.2024 01:21] Abstract 25. Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3D...
[05.10.2024 01:21] Abstract 26. Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we i...
[05.10.2024 01:21] Read previous papers.
[05.10.2024 01:21] Generating reviews via LLM API.
[05.10.2024 01:21] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞
[05.10.2024 01:21] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É
[05.10.2024 01:21] Using data from previous issue: {"desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ 
[05.10.2024 01:21] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫
[05.10.2024 01:21] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Di
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Fla
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã
[05.10.2024 01:21] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ 
[05.10.2024 01:21] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π VLM –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å –∏ –Ω–∞–±–ª—é–¥–∞—é—Ç –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–∏—Ä–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –æ
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Reflective Monte Carlo Tree Search (R-MCTS) - –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π. R-MCTS —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π MCTS, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ
[05.10.2024 01:21] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –≤–ª–∏—è–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–µ –∫–ª–µ—Ç–æ—á–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã (ECA) –∏ –æ–±—É—á–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö ECA. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–∞ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫
[05.10.2024 01:21] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö 
[05.10.2024 01:21] Using data from previous issue: {"desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ complementary –º–µ—Ç–æ–¥–∏–∫–∏: –Ω–æ–≤—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é —Å–æ—Å–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–¥–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥—è—Ö –≤ –ø—Ä
[05.10.2024 01:21] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SciPrompt - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏, –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–∞ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞—É—á–Ω
[05.10.2024 01:21] Using data from previous issue: {"desc": "Robin3D - —ç—Ç–æ –º–æ—â–Ω–∞—è 3D –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö RIG, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. Robin3D –≤–∫–ª—é—á–∞–µ—Ç –≤ 
[05.10.2024 01:21] Using data from previous issue: {"desc": "Open-RAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Retrieval-Augmented Generation —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–ª–æ—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—É—é —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Open-RA
[05.10.2024 01:21] Renaming data file.
[05.10.2024 01:21] Renaming previous data. hf_papers.json to 2024-10-04_hf_papers.json
[05.10.2024 01:21] Saving new data file.
[05.10.2024 01:21] Generating page.
[05.10.2024 01:21] Renaming previous page.
[05.10.2024 01:21] Renaming previous data. index.html to 2024-10-04_papers.html
[05.10.2024 01:21] Writing result.
[05.10.2024 01:21] Done.
[05.10.2024 06:24] Get feed.
[05.10.2024 06:24] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[05.10.2024 06:24] Abstract 1. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[05.10.2024 06:24] Abstract 2. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[05.10.2024 06:24] Abstract 3. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[05.10.2024 06:24] Abstract 4. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[05.10.2024 06:24] Abstract 5. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[05.10.2024 06:24] Abstract 6. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[05.10.2024 06:24] Abstract 7. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[05.10.2024 06:24] Abstract 8. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[05.10.2024 06:24] Abstract 9. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[05.10.2024 06:24] Abstract 10. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[05.10.2024 06:24] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[05.10.2024 06:24] Abstract 12. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[05.10.2024 06:24] Abstract 13. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[05.10.2024 06:24] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[05.10.2024 06:24] Abstract 15. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[05.10.2024 06:24] Abstract 16. Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval...
[05.10.2024 06:24] Abstract 17. We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on ...
[05.10.2024 06:24] Abstract 18. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[05.10.2024 06:24] Abstract 19. Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance leve...
[05.10.2024 06:24] Abstract 20. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-Eng...
[05.10.2024 06:24] Abstract 21. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon plan...
[05.10.2024 06:24] Abstract 22. We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems ...
[05.10.2024 06:24] Abstract 23. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[05.10.2024 06:24] Abstract 24. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[05.10.2024 06:24] Abstract 25. Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3D...
[05.10.2024 06:24] Abstract 26. Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we i...
[05.10.2024 06:24] Read previous papers.
[05.10.2024 06:24] Generating reviews via LLM API.
[05.10.2024 06:24] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞
[05.10.2024 06:24] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É
[05.10.2024 06:24] Using data from previous issue: {"desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ 
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'
[05.10.2024 06:24] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫
[05.10.2024 06:24] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Di
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Fla
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã
[05.10.2024 06:24] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ complementary –º–µ—Ç–æ–¥–∏–∫–∏: –Ω–æ–≤—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é —Å–æ—Å–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–¥–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥—è—Ö –≤ –ø—Ä
[05.10.2024 06:24] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π VLM –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å –∏ –Ω–∞–±–ª—é–¥–∞—é—Ç –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–∏—Ä–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ 
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SciPrompt - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏, –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–∞ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞—É—á–Ω
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –æ
[05.10.2024 06:24] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Reflective Monte Carlo Tree Search (R-MCTS) - –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π. R-MCTS —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π MCTS, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ
[05.10.2024 06:24] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –≤–ª–∏—è–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–µ –∫–ª–µ—Ç–æ—á–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã (ECA) –∏ –æ–±—É—á–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö ECA. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–∞ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫
[05.10.2024 06:24] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö 
[05.10.2024 06:24] Using data from previous issue: {"desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥
[05.10.2024 06:24] Using data from previous issue: {"desc": "Robin3D - —ç—Ç–æ –º–æ—â–Ω–∞—è 3D –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö RIG, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. Robin3D –≤–∫–ª—é—á–∞–µ—Ç –≤ 
[05.10.2024 06:24] Using data from previous issue: {"desc": "Open-RAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Retrieval-Augmented Generation —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–ª–æ—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—É—é —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Open-RA
[05.10.2024 06:24] Renaming data file.
[05.10.2024 06:24] Renaming previous data. hf_papers.json to 2024-10-04_hf_papers.json
[05.10.2024 06:24] Saving new data file.
[05.10.2024 06:24] Generating page.
[05.10.2024 06:24] Renaming previous page.
[05.10.2024 06:24] Renaming previous data. index.html to 2024-10-04_papers.html
[05.10.2024 06:24] Writing result.
[05.10.2024 06:24] Done.
[05.10.2024 12:33] Get feed.
[05.10.2024 12:33] Abstract 0. Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of...
[05.10.2024 12:33] Abstract 1. The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, ...
[05.10.2024 12:33] Abstract 2. It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregress...
[05.10.2024 12:33] Abstract 3. Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect imag...
[05.10.2024 12:33] Abstract 4. We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation crite...
[05.10.2024 12:33] Abstract 5. We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as...
[05.10.2024 12:33] Abstract 6. Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this chall...
[05.10.2024 12:33] Abstract 7. Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art rei...
[05.10.2024 12:33] Abstract 8. Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to mod...
[05.10.2024 12:33] Abstract 9. Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this...
[05.10.2024 12:33] Abstract 10. In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. T...
[05.10.2024 12:33] Abstract 11. Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already sc...
[05.10.2024 12:33] Abstract 12. The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although qu...
[05.10.2024 12:33] Abstract 13. Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM...
[05.10.2024 12:33] Abstract 14. Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentatio...
[05.10.2024 12:33] Abstract 15. Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deli...
[05.10.2024 12:33] Abstract 16. Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval...
[05.10.2024 12:33] Abstract 17. We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on ...
[05.10.2024 12:33] Abstract 18. There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understandi...
[05.10.2024 12:33] Abstract 19. Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon plan...
[05.10.2024 12:33] Abstract 20. Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance leve...
[05.10.2024 12:33] Abstract 21. Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-Eng...
[05.10.2024 12:33] Abstract 22. We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems ...
[05.10.2024 12:33] Abstract 23. We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and ...
[05.10.2024 12:33] Abstract 24. We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noi...
[05.10.2024 12:33] Abstract 25. Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3D...
[05.10.2024 12:33] Abstract 26. Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we i...
[05.10.2024 12:33] Read previous papers.
[05.10.2024 12:33] Generating reviews via LLM API.
[05.10.2024 12:33] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞
[05.10.2024 12:33] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'
[05.10.2024 12:33] Using data from previous issue: {"desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ 
[05.10.2024 12:33] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú
[05.10.2024 12:33] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Di
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Fla
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã
[05.10.2024 12:33] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ complementary –º–µ—Ç–æ–¥–∏–∫–∏: –Ω–æ–≤—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é —Å–æ—Å–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–¥–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥—è—Ö –≤ –ø—Ä
[05.10.2024 12:33] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π VLM –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å –∏ –Ω–∞–±–ª—é–¥–∞—é—Ç –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–∏—Ä–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ 
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Reflective Monte Carlo Tree Search (R-MCTS) - –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π. R-MCTS —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π MCTS, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SciPrompt - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏, –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–∞ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞—É—á–Ω
[05.10.2024 12:33] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –æ
[05.10.2024 12:33] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –≤–ª–∏—è–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–µ –∫–ª–µ—Ç–æ—á–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã (ECA) –∏ –æ–±—É—á–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö ECA. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–∞ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫
[05.10.2024 12:33] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö 
[05.10.2024 12:33] Using data from previous issue: {"desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥
[05.10.2024 12:33] Using data from previous issue: {"desc": "Robin3D - —ç—Ç–æ –º–æ—â–Ω–∞—è 3D –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö RIG, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. Robin3D –≤–∫–ª—é—á–∞–µ—Ç –≤ 
[05.10.2024 12:33] Using data from previous issue: {"desc": "Open-RAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Retrieval-Augmented Generation —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–ª–æ—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—É—é —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Open-RA
[05.10.2024 12:33] Renaming data file.
[05.10.2024 12:33] Renaming previous data. hf_papers.json to 2024-10-04_hf_papers.json
[05.10.2024 12:33] Saving new data file.
[05.10.2024 12:33] Generating page.
[05.10.2024 12:33] Renaming previous page.
[05.10.2024 12:33] Renaming previous data. index.html to 2024-10-04_papers.html
[05.10.2024 12:33] Writing result.
[05.10.2024 12:33] Done.
