[25.10.2024 10:15] [Experimental] Generating an image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss.
[25.10.2024 10:15] [Experimental] Image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss already exists.
[25.10.2024 10:15] [Experimental] Generating an image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization.
[25.10.2024 10:15] [Experimental] Image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization already exists.
[25.10.2024 10:15] [Experimental] Generating an image for paper Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch.
[25.10.2024 10:15] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch' Text: 'The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up data synthesis from strong models (e.g., GPT-4) can further elicit reasoning performance. Though promising, the open-sourced community still lacks high-quality data at scale and scalable data synthesis methods with affordable costs. To address this, we introduce ScaleQuest, a scalable and novel data synthesis method that utilizes "small-size" (e.g., 7B) open-source models to generate questions from scratch without the need for seed data with complex augmentation constraints. With the efficient ScaleQuest, we automatically constructed a mathematical reasoning dataset consisting of 1 million problem-solution pairs, which are more effective than existing open-sourced datasets. It can universally increase the performance of mainstream open-source models (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2% to 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base model with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and well-aligned model on closed-source data, and proprietary models such as GPT-4-Turbo and Claude-3.5 Sonnet.'
[25.10.2024 10:15] Response: **Image Prompt:**

Create a linear art piece on a white background that visually represents the themes of scalable data synthesis, reasoning capability, and the transformative power of questions. Imagine a large, abstract question mark intertwined with gears and mathematical symbols, symbolizing the synthesis of data and reasoning processes. Surround the question mark with smaller, intricate question marks, each representing various high-quality data sources flowing into a central hub. In the background, include faint outlines of artificial intelligence motifs, like neural networks or circuit patterns, to indicate the technological aspect of LLMs. 

**Label Text:** "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch"
[25.10.2024 10:15] Generating image by prompt: **Image Prompt:**

Create a linear art piece on a white background that visually represents the themes of scalable data synthesis, reasoning capability, and the transformative power of questions. Imagine a large, abstract question mark intertwined with gears and mathematical symbols, symbolizing the synthesis of data and reasoning processes. Surround the question mark with smaller, intricate question marks, each representing various high-quality data sources flowing into a central hub. In the background, include faint outlines of artificial intelligence motifs, like neural networks or circuit patterns, to indicate the technological aspect of LLMs. 

**Label Text:** "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch".
[25.10.2024 10:15] Saving generated image from https://fal.media/files/zebra/8feAkv0OnmMZuSQrWHBP6.png to d898ba7b025b60e6.jpg.
