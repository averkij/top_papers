[22.10.2024 18:16] [Experimental] Generating an image for paper FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors.
[22.10.2024 18:16] [Experimental] Image for paper FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors already exists.
[22.10.2024 18:16] [Experimental] Generating an image for paper CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution.
[22.10.2024 18:16] [Experimental] Image for paper CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution already exists.
[22.10.2024 18:16] [Experimental] Generating an image for paper SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree.
[22.10.2024 18:16] [Experimental] Image for paper SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree already exists.
[22.10.2024 18:16] [Experimental] Generating an image for paper PUMA: Empowering Unified MLLM with Multi-granular Visual Generation.
[22.10.2024 18:16] [Experimental] Image for paper PUMA: Empowering Unified MLLM with Multi-granular Visual Generation already exists.
[22.10.2024 18:16] [Experimental] Generating an image for paper AutoTrain: No-code training for state-of-the-art models.
[22.10.2024 18:16] [Experimental] Image for paper AutoTrain: No-code training for state-of-the-art models already exists.
[22.10.2024 18:16] [Experimental] Generating an image for paper SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation.
[22.10.2024 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation' Text: 'Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SemiEvol for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios.'
[22.10.2024 18:16] Response: **Image Prompt:** A linear art composition on a white background depicting a large, open book with pages transforming into a swirling galaxy of abstract shapes and symbols representing data and knowledge. A semi-transparent figure stands at the center, reaching out towards a glowing orb of labeled and unlabeled data, with beams of light connecting the figure to the book and the galaxy. Surrounding the scene are fragmented phrases and symbols that evoke concepts of adaptation, evolution, and collaboration. 

**Label Text:** "SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation"
[22.10.2024 18:16] Generating image by prompt: **Image Prompt:** A linear art composition on a white background depicting a large, open book with pages transforming into a swirling galaxy of abstract shapes and symbols representing data and knowledge. A semi-transparent figure stands at the center, reaching out towards a glowing orb of labeled and unlabeled data, with beams of light connecting the figure to the book and the galaxy. Surrounding the scene are fragmented phrases and symbols that evoke concepts of adaptation, evolution, and collaboration. 

**Label Text:** "SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation".
[22.10.2024 18:16] Saving generated image from https://fal.media/files/elephant/UsApcD9NNHPeK-o5n-l6A.png to 863b95d8e45c3aa2.jpg.
[22.10.2024 18:17] [Experimental] Generating an image for paper Baichuan Alignment Technical Report.
[22.10.2024 18:17] [Experimental] Image for paper Baichuan Alignment Technical Report already exists.
[22.10.2024 18:17] [Experimental] Generating an image for paper Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages.
[22.10.2024 18:17] [Experimental] Image for paper Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages already exists.
[22.10.2024 18:17] [Experimental] Generating an image for paper RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style.
[22.10.2024 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style' Text: 'Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.'
[22.10.2024 18:17] Response: **Image Prompt:** Create a linear art piece on a white background depicting a surreal landscape where abstract representations of language models take the form of floating benches. Each bench is adorned with intricate designs symbolizing subtlety and style, such as delicate brush strokes and shifting patterns. Surrounding the benches are ethereal figures representing reward models, engaged in a dance that illustrates their struggle to discern the nuances of content and style. In the foreground, place a large, stylized title card that reads: "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style." The overall composition should evoke a sense of complexity and layered meaning, with interconnected elements that reflect the themes of evaluation, alignment, and the quest for improvement in language model performance.
[22.10.2024 18:17] Generating image by prompt: **Image Prompt:** Create a linear art piece on a white background depicting a surreal landscape where abstract representations of language models take the form of floating benches. Each bench is adorned with intricate designs symbolizing subtlety and style, such as delicate brush strokes and shifting patterns. Surrounding the benches are ethereal figures representing reward models, engaged in a dance that illustrates their struggle to discern the nuances of content and style. In the foreground, place a large, stylized title card that reads: "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style." The overall composition should evoke a sense of complexity and layered meaning, with interconnected elements that reflect the themes of evaluation, alignment, and the quest for improvement in language model performance..
[22.10.2024 18:17] Saving generated image from https://fal.media/files/penguin/rf1ACCmKSI62d_aehtfjy.png to 6fff842f967d7207.jpg.
