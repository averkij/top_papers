[19.10.2024 16:14] [Experimental] Generating an image for paper Movie Gen: A Cast of Media Foundation Models.
[19.10.2024 16:14] [Experimental] Image for paper Movie Gen: A Cast of Media Foundation Models already exists.
[19.10.2024 16:14] [Experimental] Generating an image for paper MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures.
[19.10.2024 16:14] [Experimental] Image for paper MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures already exists.
[19.10.2024 16:14] [Experimental] Generating an image for paper MobA: A Two-Level Agent System for Efficient Mobile Task Automation.
[19.10.2024 16:14] [Experimental] Image for paper MobA: A Two-Level Agent System for Efficient Mobile Task Automation already exists.
[19.10.2024 16:14] [Experimental] Generating an image for paper Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation.
[19.10.2024 16:14] [Experimental] Image for paper Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation already exists.
[19.10.2024 16:14] [Experimental] Generating an image for paper JudgeBench: A Benchmark for Evaluating LLM-based Judges.
[19.10.2024 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Return only prompt and nothing else. Title: 'JudgeBench: A Benchmark for Evaluating LLM-based Judges' Text: 'LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench .'
[19.10.2024 16:14] Response: **Prompt:** Create a surrealist artwork featuring a colossal, abstract judge's bench floating above a cityscape made of books and algorithms. The bench should be adorned with a surreal clock melting over its edge, symbolizing the passage of time and the evolution of LLM-based judges. Surround the bench with ethereal figures representing different LLM models, each with unique characteristics, such as glowing eyes and fragmented bodies, engaging in a chaotic debate. Below, a swirling vortex of text and code spirals into the ground, symbolizing the complexity of evaluating AI responses. In the foreground, a signpost reads, "JudgeBench: A Benchmark for Evaluating LLM-based Judges," its letters morphing into various programming symbols as they dissolve into the air.
[19.10.2024 16:14] Generating image by prompt: **Prompt:** Create a surrealist artwork featuring a colossal, abstract judge's bench floating above a cityscape made of books and algorithms. The bench should be adorned with a surreal clock melting over its edge, symbolizing the passage of time and the evolution of LLM-based judges. Surround the bench with ethereal figures representing different LLM models, each with unique characteristics, such as glowing eyes and fragmented bodies, engaging in a chaotic debate. Below, a swirling vortex of text and code spirals into the ground, symbolizing the complexity of evaluating AI responses. In the foreground, a signpost reads, "JudgeBench: A Benchmark for Evaluating LLM-based Judges," its letters morphing into various programming symbols as they dissolve into the air..
[19.10.2024 16:14] Saving generated image from https://fal.media/files/zebra/kYSn5mzdIFim2lInD9N0C.png to a81030e9f379736a.jpg.
[19.10.2024 16:14] [Experimental] Generating an image for paper Roadmap towards Superhuman Speech Understanding using Large Language Models.
[19.10.2024 16:14] [Experimental] Image for paper Roadmap towards Superhuman Speech Understanding using Large Language Models already exists.
[19.10.2024 16:14] [Experimental] Generating an image for paper Harnessing Webpage UIs for Text-Rich Visual Understanding.
[19.10.2024 16:14] [Experimental] Image for paper Harnessing Webpage UIs for Text-Rich Visual Understanding already exists.
