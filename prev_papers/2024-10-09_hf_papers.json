{
    "date": "10 –æ–∫—Ç—è–±—Ä—è",
    "issue_id": 38,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.05363",
            "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
            "url": "https://huggingface.co/papers/2410.05363",
            "abstract": "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench",
            "score": 19,
            "issue_id": 37,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PhyGenBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –≤ –º–æ–¥–µ–ª—è—Ö —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ (T2V). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 160 –ø—Ä–æ–º–ø—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 27 —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ –≤ —á–µ—Ç—ã—Ä–µ—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç PhyGenEval - –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ü–µ–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ T2V –º–æ–¥–µ–ª–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É.",
                "tags": [
                    "#PhyGenBench",
                    "#T2V",
                    "#–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–∞—è–§–∏–∑–∏–∫–∞"
                ],
                "emoji": "üß†",
                "title": "PhyGenBench: –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –≤ –º–æ–¥–µ–ª—è—Ö —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06961",
            "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
            "url": "https://huggingface.co/papers/2410.06961",
            "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.",
            "score": 9,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ SynPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. SynPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º, –≥–¥–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –∞ —É–ª—É—á—à–∞—Ç–µ–ª—å –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏. –ü–æ—Å–ª–µ —á–µ—Ç—ã—Ä–µ—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π SynPO, –º–æ–¥–µ–ª–∏ Llama3-8B –∏ Mistral-7B –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –Ω–∞–≥—Ä–∞–¥–∞–º –¥–ª—è —Å–≤–æ–∏—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
                "tags": [
                    "#—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ_–¥–∞–Ω–Ω—ã–µ",
                    "#—Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ_–Ø–ú",
                    "#–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ_—É–ª—É—á—à–µ–Ω–∏–µ"
                ],
                "emoji": "üîÑ",
                "title": "SynPO: —Å–∞–º–æ—É—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07171",
            "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2410.07171",
            "abstract": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp",
            "score": 6,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "IterComp - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –û–Ω –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. IterComp –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ —Ç—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —Å–ª–æ–∂–Ω–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏.",
                "tags": [
                    "#–∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è–ì–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ–û–±—É—á–µ–Ω–∏–µ",
                    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ–ú–æ–¥–µ–ª–∏"
                ],
                "emoji": "üß©",
                "title": "IterComp: –û–±—ä–µ–¥–∏–Ω—è—è —Å–∏–ª—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05643",
            "title": "TRACE: Temporal Grounding Video LLM via Causal Event Modeling",
            "url": "https://huggingface.co/papers/2410.05643",
            "abstract": "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at https://github.com/gyxxyg/TRACE.",
            "score": 6,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ–æ (Video Temporal Grounding). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è –≤–∏–¥–µ–æ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏–π. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å TRACE - –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—É—é –≤–∏–¥–µ–æ-LLM, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∫–∞–¥—Ä—ã, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏, –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∏ —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ TRACE –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ-LLM –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö VTG.",
                "tags": [
                    "#VideoTemporalGrounding",
                    "#CausalEventModeling",
                    "#MultitaskVideoLLM"
                ],
                "emoji": "üé¨",
                "title": "TRACE: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06244",
            "title": "Story-Adapter: A Training-free Iterative Framework for Long Story Visualization",
            "url": "https://huggingface.co/papers/2410.06244",
            "abstract": "Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter .",
            "score": 5,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Story-Adapter –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –º–æ–¥—É–ª–µ–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. Story-Adapter –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —Å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –≤ —É–ª—É—á—à–µ–Ω–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π.",
                "tags": [
                    "#StoryVisualization",
                    "#DiffusionModels",
                    "#CrossAttention"
                ],
                "emoji": "üéûÔ∏è",
                "title": "Story-Adapter: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07064",
            "title": "Data Selection via Optimal Control for Language Models",
            "url": "https://huggingface.co/papers/2410.07064",
            "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.",
            "score": 5,
            "issue_id": 37,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤—ã–±–æ—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏–∑ –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ —Å —Ü–µ–ª—å—é —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –≤—ã–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –æ–±–æ–±—â–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Ä–µ—à–∞–µ–º—É—é —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏–Ω—Ü–∏–ø–∞ –º–∞–∫—Å–∏–º—É–º–∞ –ü–æ–Ω—Ç—Ä—è–≥–∏–Ω–∞. –ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ PDS –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PDS —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–≤—ã—à–∞–µ—Ç –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö.",
                "tags": [
                    "#dataSelection",
                    "#optimalControl",
                    "#languageModelPretraining"
                ],
                "emoji": "üéØ",
                "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06166",
            "title": "Temporal Reasoning Transfer from Text to Video",
            "url": "https://huggingface.co/papers/2410.06166",
            "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.",
            "score": 4,
            "issue_id": 37,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (Video LLMs) –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –Ω–µ –∏–∑-–∑–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–∞–∑–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ Textual Temporal reasoning Transfer (T3), –∫–æ—Ç–æ—Ä—ã–π —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ T3 –ø–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LongVA-7B –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–¥–µ–æ –∑–∞–¥–∞—á–∞—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.",
                "tags": [
                    "#TemporalReasoning",
                    "#VideoLLM",
                    "#TextualTransfer"
                ],
                "emoji": "‚è≥",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ-LLM —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–µ—Ä–µ–Ω–æ—Å"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07177",
            "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
            "url": "https://huggingface.co/papers/2410.07177",
            "abstract": "This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel \"Memory Pointer Prompting\" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.",
            "score": 3,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞—é—â–∏–π 7 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∏ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –≤ –≤–∏–¥–µ–æ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞–≤—Ç–æ—Ä–∞–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º 'Memory Pointer Prompting', –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ.",
                "tags": [
                    "#EgocentricVideoUnderstanding",
                    "#MultimodalFoundationModel",
                    "#MemoryPointerPrompting"
                ],
                "emoji": "üëÄ",
                "title": "MM-Ego: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05954",
            "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
            "url": "https://huggingface.co/papers/2410.05954",
            "abstract": "Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at https://pyramid-flow.github.io.",
            "score": 2,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–∏—Ä–∞–º–∏–¥–∞–ª—å–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ —Å–µ—Ä–∏—é –ø–∏—Ä–∞–º–∏–¥–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤, –≥–¥–µ —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ø–æ–ª–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å end-to-end —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ–¥–∏–Ω–æ–≥–æ Diffusion Transformer (DiT). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —Å–ø–æ—Å–æ–±–µ–Ω –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é 5-10 —Å–µ–∫—É–Ω–¥ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 768p –∏ —á–∞—Å—Ç–æ—Ç–æ–π 24 –∫–∞–¥—Ä–∞ –≤ —Å–µ–∫—É–Ω–¥—É.",
                "tags": [
                    "#video-generation",
                    "#flow-matching",
                    "#diffusion-transformer"
                ],
                "emoji": "üéûÔ∏è",
                "title": "–ü–∏—Ä–∞–º–∏–¥–∞–ª—å–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05664",
            "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning",
            "url": "https://huggingface.co/papers/2410.05664",
            "abstract": "As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.",
            "score": 2,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—é –ø–æ –ø—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–º –∞—Å–ø–µ–∫—Ç–∞–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –≤—Å–µ –º–µ—Ç–æ–¥—ã –∏–º–µ—é—Ç –ø–æ–±–æ—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–ª—è —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.",
                "tags": [
                    "#—Ä–∞–∑–æ–±—É—á–µ–Ω–∏–µ_–º–æ–¥–µ–ª–µ–π",
                    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ_–º–æ–¥–µ–ª–∏",
                    "#—ç—Ç–∏–∫–∞_–ò–ò"
                ],
                "emoji": "üß†",
                "title": "–†–∞–∑–æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07167",
            "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
            "url": "https://huggingface.co/papers/2410.07167",
            "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: https://github.com/shikiw/Modality-Integration-Rate.",
            "score": 2,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å Modality Integration Rate (MIR) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Large Vision Language Models, LVLMs). MIR –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. –ú–µ—Ç—Ä–∏–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–∏ –º–µ–∂–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è. MIR –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º –æ–±—É—á–µ–Ω–∏—è –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ–º–æ–¥–µ–ª–∏",
                    "#–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ",
                    "#–º–µ—Ç—Ä–∏–∫–∏–∫–∞—á–µ—Å—Ç–≤–∞"
                ],
                "emoji": "üî¨",
                "title": "MIR: –Ω–æ–≤—ã–π —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07073",
            "title": "Pixtral 12B",
            "url": "https://huggingface.co/papers/2410.07073",
            "abstract": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",
            "score": 2,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Pixtral-12B - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 12 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω–∞—è –ø–æ–Ω–∏–º–∞—Ç—å –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç. –û–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ —É—Å—Ç—É–ø–∞—è –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. Pixtral –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—É—á–µ–Ω–Ω—ã–π —Å –Ω—É–ª—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∏—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ –∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —Å—Ç–æ—Ä–æ–Ω. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ª—é–±–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º –æ–∫–Ω–µ –∏–∑ 128 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ",
                    "#–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ_–∑—Ä–µ–Ω–∏–µ",
                    "#–æ–±—Ä–∞–±–æ—Ç–∫–∞_–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ_—è–∑—ã–∫–∞"
                ],
                "emoji": "üñºÔ∏è",
                "title": "Pixtral-12B: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05993",
            "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
            "url": "https://huggingface.co/papers/2410.05993",
            "abstract": "Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.",
            "score": 1,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Aria - –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –∏–º–µ–µ—Ç 3,9 –º–ª—Ä–¥ –∏ 3,5 –º–ª—Ä–¥ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. Aria –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Pixtral-12B –∏ Llama3.2-11B, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å –ª—É—á—à–∏–º–∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –ø–æ 4-—ç—Ç–∞–ø–Ω–æ–º—É –∫–æ–Ω–≤–µ–π–µ—Ä—É, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —è–∑—ã–∫–∞, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏, —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π_–ò–ò",
                    "#—Å–º–µ—Å—å_—ç–∫—Å–ø–µ—Ä—Ç–æ–≤",
                    "#–æ—Ç–∫—Ä—ã—Ç–∞—è_–º–æ–¥–µ–ª—å"
                ],
                "emoji": "üß†",
                "title": "Aria: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ò–ò —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–∏—Ä–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06373",
            "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning",
            "url": "https://huggingface.co/papers/2410.06373",
            "abstract": "This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.",
            "score": 1,
            "issue_id": 37,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏, –≤—ã—è–≤–ª—è—è —Ñ–µ–Ω–æ–º–µ–Ω '—Å–º–µ—â–µ–Ω–∏—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º' (BOCB). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ CNN –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Å–µ–º–µ–π—Å—Ç–≤–æ–º SGD-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤, –∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ ViT –∏ ConvNeXt, —Ç–µ—Å–Ω–æ —Å–≤—è–∑–∞–Ω—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ BOCB –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∞–≤—Ç–æ—Ä—ã –¥–∞—é—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏—é —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è.",
                "tags": [
                    "#backboneOptimizerCoupling",
                    "#visionArchitectures",
                    "#optimizerSelection"
                ],
                "emoji": "üîç",
                "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06949",
            "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
            "url": "https://huggingface.co/papers/2410.06949",
            "abstract": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.",
            "score": 1,
            "issue_id": 37,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ö—Ä—É–ø–∫–æ–≥–æ –∫–æ–¥–∞, –Ω–µ—Ç–æ—á–Ω—ã–π –∑–∞—Ö–≤–∞—Ç —Ç–∏–ø–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Seeker - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π. Seeker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥–µ–Ω—Ç—ã –¥–ª—è –ø–æ–º–æ—â–∏ LLM –≤ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏, –∑–∞—Ö–≤–∞—Ç–µ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π.",
                "tags": [
                    "#–æ–±—Ä–∞–±–æ—Ç–∫–∞_–∏—Å–∫–ª—é—á–µ–Ω–∏–π",
                    "#–º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ_—Å–∏—Å—Ç–µ–º—ã",
                    "#–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å_–∫–æ–¥–∞"
                ],
                "emoji": "üõ°Ô∏è",
                "title": "Seeker: –£–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é LLM –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05677",
            "title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",
            "url": "https://huggingface.co/papers/2410.05677",
            "abstract": "In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.",
            "score": 0,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ T2V-Turbo-v2 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π T2V –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ —É—Å–ª–æ–≤–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ T2V-Turbo-v2 —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ VBench —Å –æ–±—â–∏–º —Å—á–µ—Ç–æ–º 85.13.",
                "tags": [
                    "#—Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ",
                    "#–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è_—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏",
                    "#—É—Å–ª–æ–≤–Ω–æ–µ_—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
                ],
                "emoji": "üé¨",
                "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: T2V-Turbo-v2 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06524",
            "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
            "url": "https://huggingface.co/papers/2410.06524",
            "abstract": "Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.",
            "score": 0,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "CAIMIRA - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á —É –ª—é–¥–µ–π –∏ –ò–ò –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –±–æ–ª–µ–µ 300 000 –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç ~70 —Å–∏—Å—Ç–µ–º –ò–ò –∏ 155 –ª—é–¥–µ–π, –≤—ã—è–≤–∏–ª–æ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–∞—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∑–Ω–∞–Ω–∏–π. –õ—é–¥–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ò–ò –≤ –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–º –∏ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.",
                "tags": [
                    "#CAIMIRA",
                    "#ItemResponseTheory",
                    "#AIvsHuman"
                ],
                "emoji": "üß†",
                "title": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ –ò–ò: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05651",
            "title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler",
            "url": "https://huggingface.co/papers/2410.05651",
            "abstract": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.",
            "score": 0,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –≤—ã–±–æ—Ä–∫—É –≤–¥–æ–ª—å –ø—Ä—è–º–æ–≥–æ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—É—Ç–µ–π, –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—É—é –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ –∫–æ–Ω–µ—á–Ω—ã–º –∫–∞–¥—Ä–∞–º–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ CFG++ –∏ DDS, –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –º–µ–∂–¥—É –∫–ª—é—á–µ–≤—ã–º–∏ –∫–∞–¥—Ä–∞–º–∏.",
                "tags": [
                    "#keyframe-interpolation",
                    "#bidirectional-sampling",
                    "#image-to-video-diffusion"
                ],
                "emoji": "üéûÔ∏è",
                "title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.04223",
            "title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
            "url": "https://huggingface.co/papers/2410.04223",
            "abstract": "While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.",
            "score": 0,
            "issue_id": 38,
            "pub_date": "2024-10-05",
            "pub_date_ru": "5 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Llamole - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ –≥—Ä–∞—Ñ—ã –º–æ–ª–µ–∫—É–ª. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–∞–∑–æ–≤—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å Graph Diffusion Transformer –∏ –≥—Ä–∞—Ñ–æ–≤—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –¥–ª—è –º–Ω–æ–≥–æ—É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–ª–µ–∫—É–ª –∏ –≤—ã–≤–æ–¥–∞ —Ä–µ–∞–∫—Ü–∏–π. Llamole —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç A* –ø–æ–∏—Å–∫ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—Ç—Ä–æ—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç 14 –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ 12 –º–µ—Ç—Ä–∏–∫–∞–º –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞ –º–æ–ª–µ–∫—É–ª –∏ —Ä–µ—Ç—Ä–æ—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.",
                "tags": [
                    "#–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–π_–¥–∏–∑–∞–π–Ω",
                    "#—Ä–µ—Ç—Ä–æ—Å–∏–Ω—Ç–µ–∑",
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ_LLM"
                ],
                "emoji": "üß™",
                "title": "Llamole: –ø—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–ª–µ–∫—É–ª —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06241",
            "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
            "url": "https://huggingface.co/papers/2410.06241",
            "abstract": "The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost.",
            "score": 0,
            "issue_id": 38,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BroadWay –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–∏—è–º–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –≤ –≤–∏–¥–µ–æ. BroadWay –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: Temporal Self-Guidance –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, –∏ Fourier-based Motion Enhancement –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.",
                "tags": [
                    "#text2video",
                    "#attentionMaps",
                    "#motionEnhancement"
                ],
                "emoji": "üé¨",
                "title": "BroadWay: –ü–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06462",
            "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders",
            "url": "https://huggingface.co/papers/2410.06462",
            "abstract": "The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert's models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM's directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of \"living off the land\" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners' safety policies when posed directly without the accompanying coding support request.",
            "score": 0,
            "issue_id": 38,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–î–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–∏ —Ä–µ–∑–∫–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, LLM –º–æ–≥—É—Ç —Å–Ω–∏–∑–∏—Ç—å —Å–≤–æ—é –∑–∞—â–∏—Ç—É –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π –∫–æ–¥. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å API-—ç–Ω–¥–ø–æ–∏–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∞—Ç–∞–∫. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —ç—Ç–æ—Ç —Ç–∏–ø –∞—Ç–∞–∫–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏ –ø–æ —Å–º–µ–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –µ–≥–æ –∫–∞–∫ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –∞—Ç–∞–∫ —Ç–∏–ø–∞ 'living off the land' –≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ –æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–º –ü–û.",
                "tags": [
                    "#LLMVulnerabilities",
                    "#CodeInjection",
                    "#ContextShifting"
                ],
                "emoji": "üïµÔ∏è",
                "title": "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ LLM –ø—Ä–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∫–æ–¥–∞"
            }
        }
    ]
}