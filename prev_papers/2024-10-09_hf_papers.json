{
    "date": "10 –æ–∫—Ç—è–±—Ä—è",
    "issue_id": 37,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.05363",
            "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
            "url": "https://huggingface.co/papers/2410.05363",
            "abstract": "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench",
            "score": 14,
            "issue_id": 37,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PhyGenBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –≤ –º–æ–¥–µ–ª—è—Ö —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ (T2V). –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 160 –ø—Ä–æ–º–ø—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 27 —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ –≤ —á–µ—Ç—ã—Ä–µ—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç PhyGenEval - –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ü–µ–Ω–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ T2V –º–æ–¥–µ–ª–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É.",
                "tags": [
                    "#PhyGenBench",
                    "#T2V",
                    "#–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–∞—è–§–∏–∑–∏–∫–∞"
                ],
                "emoji": "üß†",
                "title": "PhyGenBench: –æ—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –≤ –º–æ–¥–µ–ª—è—Ö —Ç–µ–∫—Å—Ç-–≤-–≤–∏–¥–µ–æ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06166",
            "title": "Temporal Reasoning Transfer from Text to Video",
            "url": "https://huggingface.co/papers/2410.06166",
            "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.",
            "score": 3,
            "issue_id": 37,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (Video LLMs) –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –Ω–µ –∏–∑-–∑–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–∞–∑–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ Textual Temporal reasoning Transfer (T3), –∫–æ—Ç–æ—Ä—ã–π —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ T3 –ø–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LongVA-7B –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–¥–µ–æ –∑–∞–¥–∞—á–∞—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.",
                "tags": [
                    "#TemporalReasoning",
                    "#VideoLLM",
                    "#TextualTransfer"
                ],
                "emoji": "‚è≥",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ-LLM —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–µ—Ä–µ–Ω–æ—Å"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07064",
            "title": "Data Selection via Optimal Control for Language Models",
            "url": "https://huggingface.co/papers/2410.07064",
            "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.",
            "score": 2,
            "issue_id": 37,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤—ã–±–æ—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏–∑ –º–∞—Å—Å–∏–≤–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ —Å —Ü–µ–ª—å—é —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –≤—ã–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –æ–±–æ–±—â–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Ä–µ—à–∞–µ–º—É—é —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏–Ω—Ü–∏–ø–∞ –º–∞–∫—Å–∏–º—É–º–∞ –ü–æ–Ω—Ç—Ä—è–≥–∏–Ω–∞. –ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ PDS –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PDS —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–≤—ã—à–∞–µ—Ç –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö.",
                "tags": [
                    "#dataSelection",
                    "#optimalControl",
                    "#languageModelPretraining"
                ],
                "emoji": "üéØ",
                "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06373",
            "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning",
            "url": "https://huggingface.co/papers/2410.06373",
            "abstract": "This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.",
            "score": 1,
            "issue_id": 37,
            "pub_date": "2024-10-08",
            "pub_date_ru": "8 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏, –≤—ã—è–≤–ª—è—è —Ñ–µ–Ω–æ–º–µ–Ω '—Å–º–µ—â–µ–Ω–∏—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º' (BOCB). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ CNN –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Å–µ–º–µ–π—Å—Ç–≤–æ–º SGD-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤, –∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ ViT –∏ ConvNeXt, —Ç–µ—Å–Ω–æ —Å–≤—è–∑–∞–Ω—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ BOCB –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∞–≤—Ç–æ—Ä—ã –¥–∞—é—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏—é —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è.",
                "tags": [
                    "#backboneOptimizerCoupling",
                    "#visionArchitectures",
                    "#optimizerSelection"
                ],
                "emoji": "üîç",
                "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06949",
            "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
            "url": "https://huggingface.co/papers/2410.06949",
            "abstract": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.",
            "score": 1,
            "issue_id": 37,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –Ω–µ—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ö—Ä—É–ø–∫–æ–≥–æ –∫–æ–¥–∞, –Ω–µ—Ç–æ—á–Ω—ã–π –∑–∞—Ö–≤–∞—Ç —Ç–∏–ø–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Seeker - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π. Seeker –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥–µ–Ω—Ç—ã –¥–ª—è –ø–æ–º–æ—â–∏ LLM –≤ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏, –∑–∞—Ö–≤–∞—Ç–µ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π.",
                "tags": [
                    "#–æ–±—Ä–∞–±–æ—Ç–∫–∞_–∏—Å–∫–ª—é—á–µ–Ω–∏–π",
                    "#–º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ_—Å–∏—Å—Ç–µ–º—ã",
                    "#–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å_–∫–æ–¥–∞"
                ],
                "emoji": "üõ°Ô∏è",
                "title": "Seeker: –£–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é LLM –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞"
            }
        }
    ]
}