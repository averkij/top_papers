{
    "date": "7 –æ–∫—Ç—è–±—Ä—è",
    "issue_id": 6,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.00907",
            "title": "Addition is All You Need for Energy-efficient Language Models",
            "abstract": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.",
            "url": "https://huggingface.co/papers/2410.00907",
            "score": 70,
            "issue_id": 1,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º L-Mul –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —É–º–Ω–æ–∂–µ–Ω–∏—è —á–∏—Å–µ–ª —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π —Å –ø–æ–º–æ—â—å—é –æ–ø–µ—Ä–∞—Ü–∏–π —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —á–µ–º 8-–±–∏—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π, –Ω–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ L-Mul –≤ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —ç–Ω–µ—Ä–≥–æ–∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ 95% –¥–ª—è –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π –∏ –Ω–∞ 80% –¥–ª—è —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä—è–º–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ L-Mul –∫ –º–µ—Ö–∞–Ω–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä—è–º —Ç–æ—á–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#L-Mul",
                    "#—ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
                    "#–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è"
                ],
                "emoji": "üßÆ",
                "title": "L-Mul: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02703",
            "title": "Selective Attention Improves Transformer",
            "abstract": "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.",
            "url": "https://huggingface.co/papers/2410.02703",
            "score": 12,
            "issue_id": 1,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (Selective Attention) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –Ω–µ–Ω—É–∂–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–µ—Å—É—Ä—Å–∞–º –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏—á—å —Ç–∞–∫–æ–π –∂–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–∞–∫ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –≤–¥–≤–æ–µ –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
                "tags": [
                    "#SelectiveAttention",
                    "#MemoryEfficiency",
                    "#TransformerOptimization"
                ],
                "emoji": "üîç",
                "title": "–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02613",
            "title": "NL-Eye: Abductive NLI for Images",
            "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps - writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.",
            "url": "https://huggingface.co/papers/2410.02613",
            "score": 11,
            "issue_id": 4,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NL-Eye - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∫ –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. NL-Eye –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, —Ç—Ä–µ–±—É—è –æ—Ç –º–æ–¥–µ–ª–µ–π –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ VLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –ª—é–¥—è–º –≤ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ, —á–∞—Å—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è. NL-Eye –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤–∞–∂–Ω—ã–π —à–∞–≥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ VLM, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –Ω–∞–¥–µ–∂–Ω–æ–º—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.",
                "tags": [
                    "#–≤–∏–∑—É–∞–ª—å–Ω–æ–µ_–∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ_—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ",
                    "#–æ—Ü–µ–Ω–∫–∞_VLM",
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π_–≤—ã–≤–æ–¥"
                ],
                "emoji": "üß†",
                "title": "NL-Eye: —Ä–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03017",
            "title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise",
            "abstract": "Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.",
            "url": "https://huggingface.co/papers/2410.03017",
            "score": 8,
            "issue_id": 1,
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É Tutor CoPilot, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –í —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Å —É—á–∞—Å—Ç–∏–µ–º 900 —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–æ–≤ –∏ 1800 —É—á–µ–Ω–∏–∫–æ–≤ –∏–∑ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—Å–ª—É–∂–∏–≤–∞–µ–º—ã—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤ –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —É—á–µ–Ω–∏–∫–∏, —Ä–∞–±–æ—Ç–∞–≤—à–∏–µ —Å —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–º–∏ Tutor CoPilot, –Ω–∞ 4 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞ —á–∞—â–µ –æ—Å–≤–∞–∏–≤–∞–ª–∏ —Ç–µ–º—ã. –ê–Ω–∞–ª–∏–∑ 550 000+ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ Tutor CoPilot —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ —Å–∏—Å—Ç–µ–º—ã —á–µ–ª–æ–≤–µ–∫-–ò–ò –º–æ–≥—É—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∏ —Å–¥–µ–ª–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–ª—è –≤—Å–µ—Ö —É—á–∞—â–∏—Ö—Å—è.",
                "tags": [
                    "#–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π–†–µ–ø–µ—Ç–∏—Ç–æ—Ä",
                    "#–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–≠–∫—Å–ø–µ—Ä—Ç–∏–∑—ã",
                    "#–ü–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–µ–°—Ç—Ä–∞—Ç–µ–≥–∏–∏"
                ],
                "emoji": "ü§ñüë®‚Äçüè´",
                "title": "Tutor CoPilot: –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02362",
            "title": "A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond",
            "abstract": "Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github.",
            "url": "https://huggingface.co/papers/2410.02362",
            "score": 7,
            "issue_id": 3,
            "data": {
                "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Mamba –≤ –∞–Ω–∞–ª–∏–∑–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. Mamba, —è–≤–ª—è—è—Å—å —á–∞—Å—Ç–Ω—ã–º —Å–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (State Space Model), –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –±–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, Mamba —Å–ø–æ—Å–æ–±–Ω–∞ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°—Ç–∞—Ç—å—è –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mamba, –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.",
                "tags": [
                    "#MambaModel",
                    "#MedicalImageAnalysis",
                    "#StateSpaceModels"
                ],
                "emoji": "üß¨",
                "title": "Mamba: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.19989",
            "title": "RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models",
            "abstract": "Text-to-texture generation has recently attracted increasing attention, but existing methods often suffer from the problems of view inconsistencies, apparent seams, and misalignment between textures and the underlying mesh. In this paper, we propose a robust text-to-texture method for generating consistent and seamless textures that are well aligned with the mesh. Our method leverages state-of-the-art 2D diffusion models, including SDXL and multiple ControlNets, to capture structural features and intricate details in the generated textures. The method also employs a symmetrical view synthesis strategy combined with regional prompts for enhancing view consistency. Additionally, it introduces novel texture blending and soft-inpainting techniques, which significantly reduce the seam regions. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods.",
            "url": "https://huggingface.co/papers/2409.19989",
            "score": 7,
            "issue_id": 2,
            "data": {
                "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π, —Ä–µ—à–∞—é—â–∏–π –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤–∏–¥–æ–≤, –≤–∏–¥–∏–º—ã—Ö —à–≤–æ–≤ –∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç—É—Ä –∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ 2D –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è SDXL –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ ControlNet, –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∏ –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π —Ç–µ–∫—Å—Ç—É—Ä. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–æ–≤ —Å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –¢–∞–∫–∂–µ –≤–≤–µ–¥–µ–Ω—ã –Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç—É—Ä –∏ –º—è–≥–∫–æ–π –∏–Ω–ø–∞–π–Ω—Ç–∏–Ω–≥–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à–≤–æ–≤.",
                "tags": [
                    "#—Ç–µ–∫—Å—Ç—É—Ä–Ω—ã–π—Å–∏–Ω—Ç–µ–∑",
                    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ–º–æ–¥–µ–ª–∏",
                    "#3D–≥—Ä–∞—Ñ–∏–∫–∞"
                ],
                "emoji": "üé®",
                "title": "–ë–µ—Å—à–æ–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—É—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–µ–∞–ª–∏–∑–º–∞ –≤ 3D"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02760",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "url": "https://huggingface.co/papers/2410.02760",
            "score": 6,
            "issue_id": 1,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å—Ç–∏—Ä–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö - Erasure of Language Memory (ELM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ç–∏—Ä–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ç—Ä–µ—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö: –ø–æ–ª–Ω–æ—Ç–∞ —É–¥–∞–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å. ELM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å—Ç–∏—Ä–∞–µ–º—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –æ–±—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#conceptErasure",
                    "#languageModelEvaluation",
                    "#ELMtechnique"
                ],
                "emoji": "üßπ",
                "title": "ELM: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–º—É —Å—Ç–∏—Ä–∞–Ω–∏—é –∑–Ω–∞–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02241",
            "title": "MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction",
            "abstract": "Stock market prediction has remained an extremely challenging problem for many decades owing to its inherent high volatility and low information noisy ratio. Existing solutions based on machine learning or deep learning demonstrate superior performance by employing a single model trained on the entire stock dataset to generate predictions across all types of stocks. However, due to the significant variations in stock styles and market trends, a single end-to-end model struggles to fully capture the differences in these stylized stock features, leading to relatively inaccurate predictions for all types of stocks. In this paper, we present MIGA, a novel Mixture of Expert with Group Aggregation framework designed to generate specialized predictions for stocks with different styles by dynamically switching between distinct style experts. To promote collaboration among different experts in MIGA, we propose a novel inner group attention architecture, enabling experts within the same group to share information and thereby enhancing the overall performance of all experts. As a result, MIGA significantly outperforms other end-to-end models on three Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000. Notably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark, surpassing the previous state-of-the-art model by 8% absolute. Furthermore, we conduct a comprehensive analysis of mixture of experts for stock market prediction, providing valuable insights for future research.",
            "url": "https://huggingface.co/papers/2410.02241",
            "score": 4,
            "issue_id": 1,
            "data": {
                "desc": "MIGA - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–æ–Ω–¥–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å –≥—Ä—É–ø–ø–æ–≤–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π, MIGA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª–µ–π –∞–∫—Ü–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –≤–Ω—É—Ç—Ä–∏–≥—Ä—É–ø–ø–æ–≤–æ–π –∞—Ç—Ç–µ–Ω—Ü–∏–∏, —É–ª—É—á—à–∞—é—â—É—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏. MIGA –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ñ–æ–Ω–¥–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è 24% –≥–æ–¥–æ–≤–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–∞ –∏–Ω–¥–µ–∫—Å–µ CSI300.",
                "tags": [
                    "#MixtureOfExperts",
                    "#StockPrediction",
                    "#GroupAttention"
                ],
                "emoji": "üìà",
                "title": "MIGA: –ü—Ä–æ—Ä—ã–≤ –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ñ–æ–Ω–¥–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.03535",
            "title": "NRGBoost: Energy-Based Generative Boosted Trees",
            "abstract": "Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling.",
            "url": "https://huggingface.co/papers/2410.03535",
            "score": 3,
            "issue_id": 4,
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ Random Forests –∏ Gradient Boosted Decision Trees, –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –±—É—Å—Ç–∏–Ω–≥–∞, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥—É –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –≤ XGBoost. –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–ø–æ—Å–æ–±–µ–Ω –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤—ã–≤–æ–¥–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–µ–Ω –∫–∞–∫ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Ç–∞–∫ –∏ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–æ–∫.",
                "tags": [
                    "#GenerativeBoosting",
                    "#TabularData",
                    "#EnergyBasedModels"
                ],
                "emoji": "üå≥",
                "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥: –º–æ—â—å –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π –≤ –º–∏—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01273",
            "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
            "abstract": "Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.",
            "url": "https://huggingface.co/papers/2410.01273",
            "score": 3,
            "issue_id": 4,
            "data": {
                "desc": "CANVAS - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏ —è–∑—ã–∫–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –û–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –ø–æ–∑–≤–æ–ª—è—é—â–µ–º —Ä–æ–±–æ—Ç—É —É—á–∏—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç COMMAND —Å –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —á–µ–ª–æ–≤–µ–∫–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –≤ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. CANVAS –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É ROS NavStack –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.",
                "tags": [
                    "#–∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ",
                    "#–Ω–∞–≤–∏–≥–∞—Ü–∏—è_—Ä–æ–±–æ—Ç–æ–≤",
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"
                ],
                "emoji": "ü§ñ",
                "title": "CANVAS: –Ω–∞–≤–∏–≥–∞—Ü–∏—è —Ä–æ–±–æ—Ç–æ–≤ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –∑–¥—Ä–∞–≤—ã–º —Å–º—ã—Å–ª–æ–º"
            }
        }
    ]
}