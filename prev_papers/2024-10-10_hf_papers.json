{
    "date": "11 –æ–∫—Ç—è–±—Ä—è",
    "time_utc": "2024-10-11 02:43",
    "issue_id": 50,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.08207",
            "title": "DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models",
            "url": "https://huggingface.co/papers/2410.08207",
            "abstract": "Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces. For project webpage, see https://hexiaoxiao-cs.github.io/DICE/.",
            "score": 8,
            "issue_id": 50,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "DICE (Discrete Inversion for Controllable Editing) - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ—á–Ω–æ–π –∏–Ω–≤–µ—Ä—Å–∏–∏ –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å —Ç–æ—á–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –≥–∏–±–∫–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –º–∞—Å–∫–∞—Ö –∏–ª–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è—Ö —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º. DICE —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–æ–¥–µ–ª—è–º–∏ –∫–∞–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è VQ-Diffusion, Paella –∏ RoBERTa. –ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
                "tags": [
                    "#–¥–∏—Å–∫—Ä–µ—Ç–Ω–∞—è_–¥–∏—Ñ—Ñ—É–∑–∏—è",
                    "#–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ_—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
                    "#–∏–Ω–≤–µ—Ä—Å–∏—è_–º–æ–¥–µ–ª–µ–π"
                ],
                "categories": [
                    "#nlp",
                    "#cv",
                    "#generative",
                    "#editing",
                    "#diffusion"
                ],
                "emoji": "üéõÔ∏è",
                "title": "DICE: –¢–æ—á–Ω–∞—è –∏–Ω–≤–µ—Ä—Å–∏—è –¥–ª—è –≥–∏–±–∫–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08115",
            "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
            "url": "https://huggingface.co/papers/2410.08115",
            "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).",
            "score": 1,
            "issue_id": 50,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Optima - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Ç–±–æ—Ä–∞ –∏ –æ–±—É—á–µ–Ω–∏—è —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –±–∞–ª–∞–Ω—Å–∏—Ä—É—é—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Ç–æ–∫–µ–Ω-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏–∫–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ Monte Carlo Tree Search, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è. Optima –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–Ω–æ–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ª–∏–Ω–∏—è–º–∏ –∏ –æ–±—ã—á–Ω—ã–º–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Llama 3 8B.",
                "tags": [
                    "#–º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ_—Å–∏—Å—Ç–µ–º—ã",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏",
                    "#–º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å_LLM"
                ],
                "categories": [
                    "#nlp",
                    "#rl",
                    "#benchmark",
                    "#code",
                    "#rag"
                ],
                "emoji": "ü§ñ",
                "title": "Optima: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07041",
            "title": "Emergent properties with repeated examples",
            "url": "https://huggingface.co/papers/2410.07041",
            "abstract": "We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.",
            "score": 1,
            "issue_id": 50,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ª—É—á—à–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –º–µ–Ω—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è –ø—Ä–∏–º–µ—Ä–∞–º–∏, —á–µ–º –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ —Ç—Ä–µ—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö: –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –æ–±—â–µ–≥–æ –¥–µ–ª–∏—Ç–µ–ª—è, –º–æ–¥—É–ª—å–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–µ–≥–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞–ª–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –æ–±—ã—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–æ–π –∏–∑ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –º–æ–≥—É—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏.",
                "tags": [
                    "#–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ–¥–∞–Ω–Ω—ã—Ö",
                    "#–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ–Ω–∞–±–æ—Ä—ã–¥–∞–Ω–Ω—ã—Ö",
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ–∑–∞–¥–∞—á–∏"
                ],
                "categories": [
                    "#ml",
                    "#dataset",
                    "#benchmark",
                    "#mathematics",
                    "#transformers"
                ],
                "emoji": "üîÅ",
                "title": "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ - –º–∞—Ç—å —É—á–µ–Ω–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.05210",
            "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality",
            "url": "https://huggingface.co/papers/2410.05210",
            "abstract": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.",
            "score": 1,
            "issue_id": 50,
            "pub_date": "2024-10-07",
            "pub_date_ru": "7 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (VLM) –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Fine-grained Selective Calibrated CLIP (FSC-CLIP), –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –ø–æ—Ç–µ—Ä—é –∂–µ—Å—Ç–∫–∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—É—é –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é. FSC-CLIP –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –Ω–∞–¥–∑–æ—Ä, —Å–æ—Ö—Ä–∞–Ω—è—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FSC-CLIP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–∏–ª—å–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#CLIP",
                    "#compositionality",
                    "#fine-grained-learning"
                ],
                "categories": [
                    "#nlp",
                    "#cv",
                    "#multi-modal",
                    "#code",
                    "#benchmark"
                ],
                "emoji": "üß©",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"
            }
        }
    ]
}