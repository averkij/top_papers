{
    "date": "14 –æ–∫—Ç—è–±—Ä—è",
    "time_utc": "2024-10-14 04:15",
    "issue_id": 91,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.08565",
            "title": "Baichuan-Omni Technical Report",
            "url": "https://huggingface.co/papers/2410.08565",
            "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.",
            "score": 22,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Baichuan-Omni - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ö–µ–º—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–∞—è –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞. Baichuan-Omni –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è–ú–æ–¥–µ–ª—å",
                    "#Baichuan-Omni",
                    "#–æ—Ç–∫—Ä—ã—Ç—ã–π–ò—Å—Ö–æ–¥–Ω—ã–π–ö–æ–¥"
                ],
                "categories": [
                    "#multimodal",
                    "#nlp",
                    "#cv",
                    "#audio",
                    "#benchmark"
                ],
                "emoji": "üåê",
                "title": "Baichuan-Omni: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08261",
            "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
            "url": "https://huggingface.co/papers/2410.08261",
            "abstract": "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing 1024 times 1024 resolution images.",
            "score": 9,
            "issue_id": 91,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Meissonic - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –ø–æ–¥—Ö–æ–¥–µ masked image modeling (MIM). –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ MIM. Meissonic –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –º–∏–∫—Ä–æ-—É—Å–ª–æ–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–æ–∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Å–ª–æ–∏ —Å–∂–∞—Ç–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ SDXL.",
                "tags": [
                    "#masked_image_modeling",
                    "#text_to_image",
                    "#high_resolution_generation"
                ],
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#code"
                ],
                "emoji": "üé®",
                "title": "Meissonic: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å MIM"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07133",
            "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.07133",
            "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.",
            "score": 7,
            "issue_id": 90,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EvolveDirector - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç API —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Å—Ç–∞–ª–∞ –º–æ–¥–µ–ª—å Edgen, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∞—è –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#text2image",
                    "#modelDistillation",
                    "#datasetEvolution"
                ],
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#nlp",
                    "code",
                    "benchmark"
                ],
                "emoji": "üé®",
                "title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06456",
            "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
            "url": "https://huggingface.co/papers/2410.06456",
            "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.",
            "score": 5,
            "issue_id": 91,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "VITask - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∑–∞–¥–∞—á–∞–º. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –æ—Ç–≤–µ—Ç–æ–≤. VITask –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å VLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 12 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ VITask –Ω–∞–¥ –æ–±—ã—á–Ω—ã–º–∏ VLM –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.",
                "tags": [
                    "#VITask",
                    "#–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è_–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞",
                    "#–∞–¥–∞–ø—Ç–∞—Ü–∏—è_–º–æ–¥–µ–ª–µ–π"
                ],
                "categories": [
                    "#multimodal",
                    "#nlp",
                    "#cv",
                    "#benchmark",
                    "#code"
                ],
                "emoji": "üî¨",
                "title": "VITask: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09008",
            "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
            "url": "https://huggingface.co/papers/2410.09008",
            "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm",
            "score": 5,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "SuperCorrect - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª–µ–Ω—å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—è –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –º–µ–Ω—å—à–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã –º—ã—Å–ª–µ–π –∏–∑ –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫—Ä–æ—Å—Å-–º–æ–¥–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä—è–º—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞ –∫ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.",
                "tags": [
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "#–¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ–û–±—É—á–µ–Ω–∏–µ",
                    "#–∫—Ä–æ—Å—Å-–º–æ–¥–µ–ª—å–Ω–∞—è–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
                ],
                "categories": [
                    "#nlp",
                    "#rlhf",
                    "#benchmark",
                    "#code",
                    "#reasoning"
                ],
                "emoji": "üßÆ",
                "title": "SuperCorrect: –ü–æ–¥—Ö–æ–¥ '—É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09009",
            "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
            "url": "https://huggingface.co/papers/2410.09009",
            "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D",
            "score": 3,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π - Semantic Score Distillation Sampling (SemanticSDS). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤. SemanticSDS —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∫–∞—Ä—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ 3D-—Å—Ü–µ–Ω—ã –∏ –æ–±—ä–µ–∫—Ç—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.",
                "tags": [
                    "#3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏",
                    "#score_distillation_sampling"
                ],
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#text2image",
                    "#code"
                ],
                "emoji": "üé®",
                "title": "SemanticSDS: –¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–ª–æ–∂–Ω—ã—Ö 3D-—Å—Ü–µ–Ω"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07035",
            "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
            "url": "https://huggingface.co/papers/2410.07035",
            "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.",
            "score": 1,
            "issue_id": 91,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω–∏ –≤–≤–æ–¥—è—Ç PositionID Prompting –∏ PositionID Fine-Tuning –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ PositionID CP Prompting –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—Å—Ç–∞–≤–∫–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è-–≤—Å—Ç–∞–≤–∫–∏.",
                "tags": [
                    "#–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ_–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ",
                    "#–∫–æ–Ω—Ç—Ä–æ–ª—å_–¥–ª–∏–Ω—ã_—Ç–µ–∫—Å—Ç–∞",
                    "#–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ_–≤—Å—Ç–∞–≤–∫–∞_–≤_LLM"
                ],
                "categories": [
                    "#nlp",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "üìè",
                "title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –≤ LLM —Å –ø–æ–º–æ—â—å—é –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08391",
            "title": "KV Prediction for Improved Time to First Token",
            "url": "https://huggingface.co/papers/2410.08391",
            "abstract": "Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .",
            "score": 0,
            "issue_id": 91,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º KV Prediction –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT). –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 15-50% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö TriviaQA –∏ –¥–æ 30% –Ω–∞ HumanEval –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –±—é–¥–∂–µ—Ç–µ FLOPS –¥–ª—è TTFT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ CPU Apple M2 Pro –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ TTFT –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏.",
                "tags": [
                    "#KVPrediction",
                    "#TimeToFirstToken",
                    "#EdgeInference"
                ],
                "categories": [
                    "#nlp",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "‚ö°",
                "title": "KV Prediction: –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω—ã–π —Å—Ç–∞—Ä—Ç –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        }
    ]
}