{
    "date": "14 –æ–∫—Ç—è–±—Ä—è",
    "time_utc": "2024-10-14 02:49",
    "issue_id": 90,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.08565",
            "title": "Baichuan-Omni Technical Report",
            "url": "https://huggingface.co/papers/2410.08565",
            "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.",
            "score": 14,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Baichuan-Omni - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ö–µ–º—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–∞—è –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞. Baichuan-Omni –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è–ú–æ–¥–µ–ª—å",
                    "#Baichuan-Omni",
                    "#–æ—Ç–∫—Ä—ã—Ç—ã–π–ò—Å—Ö–æ–¥–Ω—ã–π–ö–æ–¥"
                ],
                "categories": [
                    "#multimodal",
                    "#nlp",
                    "#cv",
                    "#audio",
                    "#benchmark"
                ],
                "emoji": "üåê",
                "title": "Baichuan-Omni: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09008",
            "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
            "url": "https://huggingface.co/papers/2410.09008",
            "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm",
            "score": 4,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "SuperCorrect - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª–µ–Ω—å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—è –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –º–µ–Ω—å—à–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã –º—ã—Å–ª–µ–π –∏–∑ –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫—Ä–æ—Å—Å-–º–æ–¥–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä—è–º—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞ –∫ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.",
                "tags": [
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "#–¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ–û–±—É—á–µ–Ω–∏–µ",
                    "#–∫—Ä–æ—Å—Å-–º–æ–¥–µ–ª—å–Ω–∞—è–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
                ],
                "categories": [
                    "#nlp",
                    "#rlhf",
                    "#benchmark",
                    "#code",
                    "#reasoning"
                ],
                "emoji": "üßÆ",
                "title": "SuperCorrect: –ü–æ–¥—Ö–æ–¥ '—É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07133",
            "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.07133",
            "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.",
            "score": 4,
            "issue_id": 90,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EvolveDirector - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç API —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Å—Ç–∞–ª–∞ –º–æ–¥–µ–ª—å Edgen, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∞—è –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#text2image",
                    "#modelDistillation",
                    "#datasetEvolution"
                ],
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#nlp",
                    "code",
                    "benchmark"
                ],
                "emoji": "üé®",
                "title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09009",
            "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
            "url": "https://huggingface.co/papers/2410.09009",
            "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D",
            "score": 3,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π - Semantic Score Distillation Sampling (SemanticSDS). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤. SemanticSDS —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∫–∞—Ä—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ 3D-—Å—Ü–µ–Ω—ã –∏ –æ–±—ä–µ–∫—Ç—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.",
                "tags": [
                    "#3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏",
                    "#score_distillation_sampling"
                ],
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#text2image",
                    "#code"
                ],
                "emoji": "üé®",
                "title": "SemanticSDS: –¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–ª–æ–∂–Ω—ã—Ö 3D-—Å—Ü–µ–Ω"
            }
        }
    ]
}