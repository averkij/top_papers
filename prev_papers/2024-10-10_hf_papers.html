
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (10 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #03dac6;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            padding: 2em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 175px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 60px;
            height: 34px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 34px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 26px;
            width: 26px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(26px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container, .sort-container {
            flex: 1;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: center;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: center;
                width: 100%;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">хф дэйли</h1>
            <p>11 октября | 10 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф дэйли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');

            const sortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            console.log(sortBy);
            sortDropdown.value = sortBy;
            sortArticles(sortBy);
        }
        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        window.addEventListener('load', () => {
            loadSettings();
        });

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.05265', 'title': 'PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs', 'url': 'https://huggingface.co/papers/2410.05265', 'abstract': 'Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at https://github.com/ChenMnZ/PrefixQuant.', 'score': 14, 'issue_id': 51, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'PrefixQuant - это новый метод квантования больших языковых моделей (LLM), который изолирует выбросы на уровне токенов офлайн без переобучения. Он префиксирует часто встречающиеся токены-выбросы в KV-кэше, что позволяет применять эффективное статическое квантование на уровне тензоров. PrefixQuant превосходит существующие методы динамического квантования по производительности и скорости вывода. Метод демонстрирует значительные улучшения в перплексии и точности на различных задачах по сравнению с предыдущими подходами.', 'tags': ['#квантование_активаций', '#оптимизация_LLM', '#токены_выбросы'], 'categories': ['#nlp', '#benchmark', '#code', '#efficiency', '#deployment'], 'emoji': '🗜️', 'title': 'PrefixQuant: Эффективное статическое квантование LLM без потери качества'}}, {'id': 'https://huggingface.co/papers/2410.08207', 'title': 'DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models', 'url': 'https://huggingface.co/papers/2410.08207', 'abstract': 'Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces. For project webpage, see https://hexiaoxiao-cs.github.io/DICE/.', 'score': 9, 'issue_id': 50, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'DICE (Discrete Inversion for Controllable Editing) - это новый подход к точной инверсии для дискретных диффузионных моделей. Он позволяет осуществлять точную реконструкцию и гибкое редактирование дискретных данных без необходимости в предопределенных масках или манипуляциях с вниманием. DICE работает с моделями как в области изображений, так и текста, включая VQ-Diffusion, Paella и RoBERTa. Метод сохраняет высокую точность данных при улучшении возможностей редактирования.', 'tags': ['#дискретная_диффузия', '#контролируемое_редактирование', '#инверсия_моделей'], 'categories': ['#nlp', '#cv', '#generative', '#editing', '#diffusion'], 'emoji': '🎛️', 'title': 'DICE: Точная инверсия для гибкого редактирования дискретных данных'}}, {'id': 'https://huggingface.co/papers/2410.03450', 'title': 'MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents', 'url': 'https://huggingface.co/papers/2410.03450', 'abstract': "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM as ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released.", 'score': 6, 'issue_id': 51, 'pub_date': '2024-10-04', 'pub_date_ru': '4 октября', 'data': {'desc': 'Статья представляет новый метод MART для улучшения работы воплощенных агентов с использованием мультимодальных языковых моделей (MLLM). MART использует данные взаимодействия для тонкой настройки MLLM-ретривера на основе обучения с предпочтениями, что позволяет эффективно оценивать и приоритезировать траектории для новых задач. Также вводится механизм абстракции траекторий, использующий возможности MLLM по суммаризации для лучшего понимания ключевых этапов. Эксперименты показывают значительное улучшение успешности выполнения задач в новых сценах по сравнению с базовыми методами.', 'tags': ['#воплощенныеАгенты', '#мультимодальныйРетривер', '#абстракцияТраекторий'], 'categories': ['#rl', '#nlp', '#cv', '#multimodal', '#embodiedAI'], 'emoji': '🤖', 'title': 'MART: Улучшение воплощенных агентов через тонкую настройку мультимодальных ретриверов'}}, {'id': 'https://huggingface.co/papers/2410.08115', 'title': 'Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System', 'url': 'https://huggingface.co/papers/2410.08115', 'abstract': "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).", 'score': 3, 'issue_id': 50, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Optima - новая система, улучшающая эффективность коммуникации и решения задач в многоагентных системах на основе больших языковых моделей. Она использует итеративный подход генерации, ранжирования, отбора и обучения с функцией вознаграждения, балансирующей производительность, токен-эффективность и читаемость. Система интегрирует техники, вдохновленные Monte Carlo Tree Search, для генерации данных обучения. Optima демонстрирует значительные улучшения по сравнению с одноагентными базовыми линиями и обычными многоагентными системами на основе Llama 3 8B.', 'tags': ['#многоагентные_системы', '#оптимизация_коммуникации', '#масштабируемость_LLM'], 'categories': ['#nlp', '#rl', '#benchmark', '#code', '#rag'], 'emoji': '🤖', 'title': 'Optima: революция в эффективности многоагентных систем на основе LLM'}}, {'id': 'https://huggingface.co/papers/2410.05210', 'title': 'Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality', 'url': 'https://huggingface.co/papers/2410.05210', 'abstract': "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.", 'score': 3, 'issue_id': 50, 'pub_date': '2024-10-07', 'pub_date_ru': '7 октября', 'data': {'desc': 'В статье предлагается новый метод улучшения композиционного понимания в предобученных моделях зрения и языка (VLM) без ущерба для производительности в мультимодальных задачах с нулевым обучением. Авторы представляют Fine-grained Selective Calibrated CLIP (FSC-CLIP), который интегрирует локальную потерю жестких негативных примеров и селективную калиброванную регуляризацию. FSC-CLIP обеспечивает детальный негативный надзор, сохраняя целостность представлений модели. Результаты оценки на различных бенчмарках показывают, что FSC-CLIP достигает композиционности на уровне современных моделей, сохраняя при этом сильные мультимодальные возможности.', 'tags': ['#CLIP', '#compositionality', '#fine-grained-learning'], 'categories': ['#nlp', '#cv', '#multi-modal', '#code', '#benchmark'], 'emoji': '🧩', 'title': 'Улучшение композиционного понимания без потери мультимодальных возможностей'}}, {'id': 'https://huggingface.co/papers/2410.06508', 'title': 'Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning', 'url': 'https://huggingface.co/papers/2410.06508', 'abstract': 'Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.', 'score': 2, 'issue_id': 51, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'AlphaLLM-CPL - это новый метод обучения языковых моделей (LLM) с использованием поиска по дереву Монте-Карло (MCTS). Он улучшает рассуждения LLM, эффективно извлекая информацию из траекторий MCTS. AlphaLLM-CPL использует попарное обучение на основе узлов дерева поиска и куррикулярное обучение предпочтениям. Эксперименты показывают значительное улучшение способностей LLM к математическим рассуждениям по сравнению с предыдущими методами.', 'tags': ['#MCTS', '#AlphaLLM', '#CurriculumLearning'], 'categories': ['#nlp', '#rl', '#reasoning', '#distillation', '#training'], 'emoji': '🧠', 'title': 'AlphaLLM-CPL: Усиление рассуждений LLM через дистилляцию MCTS'}}, {'id': 'https://huggingface.co/papers/2410.07303', 'title': 'Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow', 'url': 'https://huggingface.co/papers/2410.07303', 'abstract': 'Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing boldsymbol v-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion.', 'score': 2, 'issue_id': 51, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет новый метод под названием Rectified Diffusion, который улучшает скорость генерации изображений в диффузионных моделях. Авторы утверждают, что успех ректификации основан на использовании предобученной диффузионной модели для получения сопоставленных пар шума и образцов. Метод обобщает пространство проектирования и область применения ректификации на более широкую категорию диффузионных моделей. Rectified Diffusion упрощает процедуру обучения и достигает лучшей производительности при меньших затратах на обучение.', 'tags': ['#RectifiedDiffusion', '#ГенерацияИзображений', '#ОптимизацияОДУ'], 'categories': ['#cv', '#generative', '#diffusion', '#optimization', '#code'], 'emoji': '🖼️', 'title': 'Ускорение диффузионных моделей: новый взгляд на ректификацию'}}, {'id': 'https://huggingface.co/papers/2410.07041', 'title': 'Emergent properties with repeated examples', 'url': 'https://huggingface.co/papers/2410.07041', 'abstract': 'We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.', 'score': 2, 'issue_id': 50, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Исследование показывает, что трансформеры лучше обучаются на меньших наборах данных с повторяющимися примерами, чем на больших наборах с уникальными примерами. Эксперименты проводились на трех математических задачах: нахождение наибольшего общего делителя, модульное умножение и собственные значения матриц. Авторы демонстрируют эффективность метода двухэтапного обучения, сочетающего повторное использование малого подмножества примеров с обычной выборкой из остального набора данных. Результаты подчеркивают, что преимущества повторения могут превосходить преимущества разнообразия данных в глубоком обучении.', 'tags': ['#повторениеданных', '#алгоритмическиенаборыданных', '#математическиезадачи'], 'categories': ['#ml', '#dataset', '#benchmark', '#mathematics', '#transformers'], 'emoji': '🔁', 'title': 'Повторение - мать учения для трансформеров'}}, {'id': 'https://huggingface.co/papers/2410.08151', 'title': 'Progressive Autoregressive Video Diffusion Models', 'url': 'https://huggingface.co/papers/2410.08151', 'abstract': 'Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/.', 'score': 1, 'issue_id': 51, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья представляет новый подход к генерации длинных видео с использованием авторегрессивных моделей диффузии. Авторы предлагают применять прогрессивно увеличивающиеся уровни шума к латентным кадрам, что позволяет создавать более тонкие связи между ними. Этот метод позволяет генерировать видео длительностью до 1 минуты (1440 кадров при 24 FPS) без ухудшения качества или резких изменений сцены. Результаты демонстрируют передовой уровень в области генерации длинных видео.', 'tags': ['#VideoGeneration', '#DiffusionModels', '#LongVideoSynthesis'], 'categories': ['#cv', '#video', '#generative', '#diffusion', '#autoregressive'], 'emoji': '🎬', 'title': 'Прорыв в генерации длинных видео: минута качественного контента'}}, {'id': 'https://huggingface.co/papers/2410.08049', 'title': 'Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations', 'url': 'https://huggingface.co/papers/2410.08049', 'abstract': 'This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AILab-CVC/UniRepLKNet promoting further research and development in the community.', 'score': 1, 'issue_id': 51, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья предлагает новую парадигму использования больших сверточных ядер в современных сверточных нейронных сетях (ConvNets). Авторы утверждают, что применение нескольких больших ядер вместо стека из множества мелких может быть более эффективной стратегией проектирования. Они представляют архитектуру UniRepLKNet, которая обеспечивает систематические принципы проектирования для ConvNets с большими ядрами, подчеркивая их уникальную способность захватывать обширную пространственную информацию без глубокого наслоения. Результаты показывают превосходство этого подхода в различных задачах компьютерного зрения и других модальностях, демонстрируя универсальные возможности моделирования ConvNets с большими ядрами при более высокой скорости вывода по сравнению с vision transformers.', 'tags': ['#LargeConvolutionalKernels', '#UniRepLKNet', '#EffectiveReceptiveField'], 'categories': ['#cv', '#architecture', '#cnn', '#benchmark', '#code'], 'emoji': '🔍', 'title': 'Большие сверточные ядра - ключ к универсальным и эффективным ConvNets'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        
        function renderArticles(articles) {
            console.log(articles)
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const tags = item["data"]["tags"].join(" ");
                const articleHTML = `
                    <article>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${tags}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles(sortBy) {
            let sortedArticles = [...articlesData];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortArticles(event.target.value);
        });
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-11 04:14');
        }

        // Initial render
        updateTimeDiffs();
        renderArticles(articlesData);        
    </script>
</body>
</html>
    