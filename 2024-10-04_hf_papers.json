{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hugging Face Papers",
    "home_page_url": "https://huggingface.co/papers",
    "feed_url": "https://example.org/feed.json",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.02740",
            "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
            "abstract": "Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This comprehensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models.",
            "url": "https://huggingface.co/papers/2410.02740",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ AltText –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π CLIP, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∏ –∏ AltText, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∏—Å–µ–π. –í—ã—è–≤–ª–µ–Ω—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º –ø–æ–¥–ø–∏—Å–µ–π.",
                "tags": [
                    "#multimodal_captioning",
                    "#synthetic_captions",
                    "#model_specific_optimization"
                ],
                "emoji": "üñºÔ∏è",
                "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–ø–µ—Ö—É"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02713",
            "title": "Video Instruction Tuning With Synthetic Data",
            "abstract": "The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.",
            "url": "https://huggingface.co/papers/2410.02713",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç LLaVA-Video-178K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–∏–¥–µ–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å LLaVA-Video, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ-–±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ, –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º.",
                "tags": [
                    "#videoLLM",
                    "#syntheticData",
                    "#instructionFollowing"
                ],
                "emoji": "üé¨",
                "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –≤–∏–¥–µ–æ-–ò–ò"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02757",
            "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
            "abstract": "It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct a deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as a unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with a loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: https://epiphqny.github.io/Loong-video.",
            "url": "https://huggingface.co/papers/2410.02757",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loong - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–ø–æ—Å–æ–±–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ –∏ –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ 10-—Å–µ–∫—É–Ω–¥–Ω—ã—Ö –≤–∏–¥–µ–æ, –Ω–æ –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Ç–Ω—ã–µ —Ä–æ–ª–∏–∫–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –≤—ã–≤–æ–¥–∞. Loong –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –≤–∏–¥–µ–æ-—Ç–æ–∫–µ–Ω—ã –≤ –µ–¥–∏–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è LLM.",
                "tags": [
                    "#–≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µLLM",
                    "#–¥–ª–∏–Ω–Ω—ã–µ–≤–∏–¥–µ–æ"
                ],
                "emoji": "üé¨",
                "title": "Loong: –ø—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LLM"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02712",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and (2) Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.",
            "url": "https://huggingface.co/papers/2410.02712",
            "data": {
                "desc": "LLaVA-Critic - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏. LLaVA-Critic —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å—É–¥—å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–µ –∏ –æ—Ü–µ–Ω–∫–µ.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è_–æ—Ü–µ–Ω–∫–∞",
                    "#–æ–±—É—á–µ–Ω–∏–µ_–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º",
                    "#LLaVA"
                ],
                "emoji": "üß†",
                "title": "LLaVA-Critic: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02746",
            "title": "Contrastive Localized Language-Image Pre-Training",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks.",
            "url": "https://huggingface.co/papers/2410.02746",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CLOC (Contrastive Localized Language-Image Pre-training), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ CLIP. CLOC –¥–æ–ø–æ–ª–Ω—è–µ—Ç CLIP –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é 'promptable embeddings'. –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–æ–≤. CLOC –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –æ–±–ª–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#CLOC",
                    "#—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏",
                    "#–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è_–æ–±—ä–µ–∫—Ç–æ–≤"
                ],
                "emoji": "üîç",
                "title": "CLOC: –¢–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02073",
            "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
            "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
            "url": "https://huggingface.co/papers/2410.02073",
            "data": {
                "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Depth Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –≥–ª—É–±–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã —Å –Ω–µ–ø—Ä–µ–≤–∑–æ–π–¥–µ–Ω–Ω–æ–π —á–µ—Ç–∫–æ—Å—Ç—å—é –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã. Depth Pro –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º vision transformer –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.",
                "tags": [
                    "#MonocularDepthEstimation",
                    "#ZeroShotLearning",
                    "#VisionTransformer"
                ],
                "emoji": "üîç",
                "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≥–ª—É–±–∏–Ω—ã: Depth Pro - –±—ã—Å—Ç—Ä—ã–π –∏ —Ç–æ—á–Ω—ã–π –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01679",
            "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
            "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.",
            "url": "https://huggingface.co/papers/2410.01679",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ Proximal Policy Optimization (PPO) –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VinePPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–µ—Å–º–µ—â–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –≤–º–µ—Å—Ç–æ —Ü–µ–Ω–Ω–æ—Å—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π. VinePPO –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö MATH –∏ GSM8K –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å PPO –∏ –¥—Ä—É–≥–∏–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
                "tags": [
                    "#creditAssignment",
                    "#VinePPO",
                    "#LLMFinetuning"
                ],
                "emoji": "üåø",
                "title": "VinePPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞ –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02724",
            "title": "Large Language Models as Markov Chains",
            "abstract": "Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.",
            "url": "https://huggingface.co/papers/2410.02724",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø—Ä–æ–≤–æ–¥—è –∞–Ω–∞–ª–æ–≥–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏ –∏ —Ü–µ–ø—è–º–∏ –ú–∞—Ä–∫–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —ç—Ç–∏—Ö —Ü–µ–ø–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≥—Ä–∞–Ω–∏—Ü –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ –æ–±–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞ —Ç–∞–∫–∂–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–∏—Ä–æ–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#MarkovChains",
                    "#LLMTheory",
                    "#StatisticalLearning"
                ],
                "emoji": "üîó",
                "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ LLM —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ü–µ–ø–µ–π –ú–∞—Ä–∫–æ–≤–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02678",
            "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
            "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to models ``forgetting\" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute.",
            "url": "https://huggingface.co/papers/2410.02678",
            "data": {
                "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–µ—á–µ–≤—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Speech LLMs) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–≤–µ—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —Ä–µ—á–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DiVA (Distilled Voice Assistant) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —É—Å—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DiVA –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ 100 —Ä–∞–∑ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.",
                "tags": [
                    "#SpeechLLM",
                    "#UnsupervisedLearning",
                    "#VoiceAssistant"
                ],
                "emoji": "üó£Ô∏è",
                "title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.19291",
            "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
            "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU efficiently fine-tunes a series of CLIP models that capture different feature spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for the Feed-Forward Network (FFN). These models can then be transformed into a CLIP-MoE with a larger model capacity, leading to significantly enhanced performance with minimal computational overhead. To the best of our knowledge, Diversified Multiplet Upcycling is the first approach to introduce sparsely activated MoE into CLIP foundation models. Extensive experiments demonstrate the significant performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks by serving as a vision encoder. Furthermore, Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks. Through Diversified Multiplet Upcycling, we aim to provide valuable insights for future research on developing more efficient and effective multimodal learning systems.",
            "url": "https://huggingface.co/papers/2409.19291",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diversified Multiplet Upcycling (DMU). DMU –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π CLIP, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å CLIP-MoE —Å –±–æ–ª—å—à–µ–π –µ–º–∫–æ—Å—Ç—å—é. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å CLIP –≤ –∑–∞–¥–∞—á–∞—Ö zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. DMU –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫ –ª—é–±–æ–π –ø–ª–æ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ CLIP, –ø–æ–∑–≤–æ–ª—è—è –ª–µ–≥–∫–æ –∑–∞–º–µ–Ω–∏—Ç—å –µ–µ –Ω–∞ CLIP-MoE –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.",
                "tags": [
                    "#CLIP-MoE",
                    "#DiversifiedMultipletUpcycling",
                    "#MultimodalLearning"
                ],
                "emoji": "üîÑ",
                "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ CLIP —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02416",
            "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
            "abstract": "Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance.",
            "url": "https://huggingface.co/papers/2410.02416",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (APG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ (CFG), —Ä–∞–∑–¥–µ–ª—è—è –µ–≥–æ –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. APG –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –º–∞—Å—à—Ç–∞–±—ã —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –±–µ–∑ –ø–µ—Ä–µ–Ω–∞—Å—ã—â–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ CFG –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ú–µ—Ç–æ–¥ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Å–ª–æ–≤–Ω—ã–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ —Å—ç–º–ø–ª–µ—Ä–∞–º–∏, —É–ª—É—á—à–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ FID, recall –∏ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ_–º–æ–¥–µ–ª–∏",
                    "#–±–µ—Å–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–µ_—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                    "#–≥–µ–Ω–µ—Ä–∞—Ü–∏—è_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
                ],
                "emoji": "üñºÔ∏è",
                "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ–º–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02749",
            "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
            "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop a synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into a sequence of code edits by using a linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor a dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune a series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as a function of samples, i.e. the fraction of problems \"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode.",
            "url": "https://huggingface.co/papers/2410.02749",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º LintSeq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ–¥–∞. –ù–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç —Å GPT-4 –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 20% –ø–æ –º–µ—Ç—Ä–∏–∫–µ pass@50 –Ω–∞ HumanEval.",
                "tags": [
                    "#CodeEditing",
                    "#SyntheticDataGeneration",
                    "#LintSeq"
                ],
                "emoji": "‚úèÔ∏è",
                "title": "LintSeq: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∫–æ–¥–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02367",
            "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
            "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation.",
            "url": "https://huggingface.co/papers/2410.02367",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SageAttention - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. SageAttention –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FlashAttention2 –∏ xformers, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–æ–¥–µ–ª—è–º –±–µ–∑ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.",
                "tags": [
                    "#–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è_–≤–Ω–∏–º–∞–Ω–∏—è",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
                    "#—É—Å–∫–æ—Ä–µ–Ω–∏–µ_–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"
                ],
                "emoji": "‚ö°",
                "title": "SageAttention: –ë—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ –≤ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02115",
            "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
            "abstract": "Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM outputs is insufficient, as it is quite challenging for humans to verify the results from the extremely lengthy context. Yet, although some efforts have been made to assess whether LCMs respond truly based on the context, these works either are limited to specific tasks or heavily rely on external evaluation resources like GPT-4.In this work, we introduce L-CiteEval, a comprehensive multi-task benchmark for long-context understanding with citations, aiming to evaluate both the understanding capability and faithfulness of LCMs. L-CiteEval covers 11 tasks from diverse domains, spanning context lengths from 8K to 48K, and provides a fully automated evaluation suite. Through testing with 11 cutting-edge closed-source and open-source LCMs, we find that although these models show minor differences in their generated results, open-source models substantially trail behind their closed-source counterparts in terms of citation accuracy and recall. This suggests that current open-source LCMs are prone to responding based on their inherent knowledge rather than the given context, posing a significant risk to the user experience in practical applications. We also evaluate the RAG approach and observe that RAG can significantly improve the faithfulness of LCMs, albeit with a slight decrease in the generation quality. Furthermore, we discover a correlation between the attention mechanisms of LCMs and the citation generation process.",
            "url": "https://huggingface.co/papers/2410.02115",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L-CiteEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 11 –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 8K –¥–æ 48K –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 11 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –∑–∞–∫—Ä—ã—Ç—ã—Ö –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ RAG –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.",
                "tags": [
                    "#LongContextModels",
                    "#CitationEvaluation",
                    "#ModelFaithfulness"
                ],
                "emoji": "üìè",
                "title": "L-CiteEval: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02458",
            "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation",
            "abstract": "Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: https://bit.ly/3zf2CVs",
            "url": "https://huggingface.co/papers/2410.02458",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision Transformer (ViT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏–π –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –±–ª–æ–∫ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è Dice —Å 0.74 –¥–æ 0.79. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
                "tags": [
                    "#–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è_—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è",
                    "#vision_transformer",
                    "#language_model_transfer"
                ],
                "emoji": "üè•",
                "title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02103",
            "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
            "abstract": "Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.",
            "url": "https://huggingface.co/papers/2410.02103",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ 3D Gaussian Splatting (3DGS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–¥–Ω–∏–º —Ä–∞–∫—É—Ä—Å–æ–º, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –∫—Ä–æ—Å—Å-–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç –≥—Ä—É–±–æ–≥–æ –∫ —Ç–æ—á–Ω–æ–º—É –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–ø–ª–æ—Ç–Ω–µ–Ω–∏—è –∫—Ä–æ—Å—Å-–ª—É—á–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–≤—ã—à–∞–µ—Ç –æ–±—â—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö –≥–∞—É—Å—Å–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#3DGaussianSplatting",
                    "#NovelViewSynthesis",
                    "#MultiViewOptimization"
                ],
                "emoji": "üé≠",
                "title": "–ú–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 3D Gaussian Splatting"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02525",
            "title": "Contextual Document Embeddings",
            "abstract": "Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. Our method can be applied to improve performance on any contrastive learning dataset and any biencoder.",
            "url": "https://huggingface.co/papers/2410.02525",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–µ complementary –º–µ—Ç–æ–¥–∏–∫–∏: –Ω–æ–≤—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —É—á–∏—Ç—ã–≤–∞—é—â—É—é —Å–æ—Å–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–¥–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å–µ–¥—è—Ö –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –Ω–∞–¥ –±–∏—ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ out-of-domain –∑–∞–¥–∞—á–∞—Ö. –î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MTEB –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –æ–±—É—á–µ–Ω–∏—è.",
                "tags": [
                    "#–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏",
                    "#–Ω–µ–π—Ä–æ–Ω–Ω—ã–π_–ø–æ–∏—Å–∫",
                    "#–∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ_–æ–±—É—á–µ–Ω–∏–µ"
                ],
                "emoji": "üß†",
                "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02762",
            "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
            "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.",
            "url": "https://huggingface.co/papers/2410.02762",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π VLM –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å–ª–æ–≤–∞—Ä—å –∏ –Ω–∞–±–ª—é–¥–∞—é—Ç –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–∏—Ä–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —É–¥–∞–ª—è–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø—É—Ç–µ–º –ª–∏–Ω–µ–π–Ω–æ–π –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–∞ 25.7% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö COCO2014.",
                "tags": [
                    "#VLM",
                    "#–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏",
                    "#—Å—Ç–∏—Ä–∞–Ω–∏–µ–∑–Ω–∞–Ω–∏–π"
                ],
                "emoji": "üß†",
                "title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ VLM —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02763",
            "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
            "abstract": "There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understanding long-form videos. However, is this really the case? Our studies indicate that LMMs still lack many fundamental reasoning capabilities even when dealing with short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation benchmark encompassing 1000 short and natural video-caption pairs. We demonstrate that existing LMMs severely struggle to distinguish temporal differences between different actions and object transformations. For example, the best model GPT-4o only obtains ~50% on our text and video scores, showing a large gap compared to the human baseline of ~90%. All open-source multimodal models and CLIP-based models perform much worse, producing mostly random chance performance. Through this work, we shed light onto the fact that temporal reasoning in short videos is a problem yet to be fully solved. The dataset and evaluation code are available at https://vinoground.github.io.",
            "url": "https://huggingface.co/papers/2410.02763",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Vinoground –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è GPT-4o, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ –µ—â–µ –Ω–µ —Ä–µ—à–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏ –∫–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#TemporalReasoning",
                    "#VideoUnderstanding",
                    "#ModelEvaluation"
                ],
                "emoji": "‚è≥",
                "title": "–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –≤–∏–¥–µ–æ: –Ω–µ–ø–æ–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—à–∏–Ω–∞ –¥–ª—è –ò–ò"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01946",
            "title": "SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics",
            "abstract": "Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.",
            "url": "https://huggingface.co/papers/2410.01946",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SciPrompt - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏, –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –¥–æ–º–µ–Ω–∞ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. SciPrompt –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.",
                "tags": [
                    "#prompt-based-fine-tuning",
                    "#scientific-text-classification",
                    "#low-resource-nlp"
                ],
                "emoji": "üß™",
                "title": "SciPrompt: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –Ω–∞—É—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01335",
            "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
            "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.",
            "url": "https://huggingface.co/papers/2410.01335",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö, –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –æ–±—É—á–∞—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö '—ç–∫—Å–ø–µ—Ä—Ç–æ–≤' –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –æ–±—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ, –∞ –∑–∞—Ç–µ–º –∑–∞–º–µ–Ω—è—é—Ç –≤–µ—Ä—Ö–Ω–∏–µ –∏ –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞ —Å–ª–æ—è–º–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞. –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MGSM –Ω–∞ 10% –¥–ª—è —á–µ—Ç—ã—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤.",
                "tags": [
                    "#modelMerging",
                    "#crossLingualTransfer",
                    "#mathematicalReasoning"
                ],
                "emoji": "üß†",
                "title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–≤—ã–∫–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02052",
            "title": "Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning",
            "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning.",
            "url": "https://huggingface.co/papers/2410.02052",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Reflective Monte Carlo Tree Search (R-MCTS) - –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π. R-MCTS —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π MCTS, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ –ø—É—Ç–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è GPT-4o —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö R-MCTS. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ VisualWebArena –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ GPT-4o —Å R-MCTS –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 6-30% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º SOTA.",
                "tags": [
                    "#ReflectiveMCTS",
                    "#VisualWebArena",
                    "#GPT4Optimization"
                ],
                "emoji": "üå≥",
                "title": "R-MCTS: –†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02536",
            "title": "Intelligence at the Edge of Chaos",
            "abstract": "We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, we evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. Our findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. We conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity.",
            "url": "https://huggingface.co/papers/2410.02536",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –≤–ª–∏—è–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–µ –∫–ª–µ—Ç–æ—á–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã (ECA) –∏ –æ–±—É—á–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö ECA. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–∞–≤–∏–ª–∞ —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –º–æ–¥–µ–ª—è–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–º –±–æ–ª—å—à—É—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç—Å—è –≤ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ö–æ–¥–æ–≤ –≤ —à–∞—Ö–º–∞—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.",
                "tags": [
                    "#–∫–ª–µ—Ç–æ—á–Ω—ã–µ_–∞–≤—Ç–æ–º–∞—Ç—ã",
                    "#—Å–ª–æ–∂–Ω–æ—Å—Ç—å_—Å–∏—Å—Ç–µ–º",
                    "#–≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ_–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
                ],
                "emoji": "üß†",
                "title": "–°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–∂–¥–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: —É—Ä–æ–∫–∏ –∫–ª–µ—Ç–æ—á–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–æ–≤"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02426",
            "title": "Learning the Latent Rules of a Game from Data: A Chess Story",
            "abstract": "We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig's novella \"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples.",
            "url": "https://huggingface.co/papers/2410.02426",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–æ—Å–æ–±–Ω—ã –∏–∑—É—á–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∏–º –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º 28M –∏ 125M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ 1000-1000000 –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª —à–∞—Ö–º–∞—Ç, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ª–µ–≥–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏—è —à–∞—Ö–º–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ø–æ—Ö –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#–º–∞–ª—ã–µ–Ø–∑—ã–∫–æ–≤—ã–µ–ú–æ–¥–µ–ª–∏",
                    "#–æ–±—É—á–µ–Ω–∏–µ–ü—Ä–∞–≤–∏–ª–∞–º–®–∞—Ö–º–∞—Ç",
                    "#—É–º–µ–Ω—å—à–µ–Ω–∏–µ–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"
                ],
                "emoji": "‚ôüÔ∏è",
                "title": "–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –∏–≥—Ä–∞—Ç—å –≤ –±–æ–ª—å—à–∏–µ —à–∞—Ö–º–∞—Ç—ã"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.02056",
            "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
            "abstract": "We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose a novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using a T2A model trained only on weakly-captioned AudioSet.",
            "url": "https://huggingface.co/papers/2410.02056",
            "data": {
                "desc": "Synthio - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥–∏–æ (T2A) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç–µ—Ö–Ω–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#AudioAugmentation",
                    "#TextToAudio",
                    "#DiffusionModels"
                ],
                "emoji": "üéµ",
                "title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∞—É–¥–∏–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.00255",
            "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
            "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\% improvement in the captioning task (Scan2Cap).",
            "url": "https://huggingface.co/papers/2410.00255",
            "data": {
                "desc": "Robin3D - —ç—Ç–æ –º–æ—â–Ω–∞—è 3D –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö RIG, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. Robin3D –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è Relation-Augmented Projector –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ ID-Feature Bonding –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–Ω–µ—Å–µ–Ω–∏—è –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –ø—è—Ç–∏ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö 3D –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏.",
                "tags": [
                    "#3DLLM",
                    "#InstructionFollowing",
                    "#3DMultimodalLearning"
                ],
                "emoji": "ü§ñ",
                "title": "Robin3D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.01782",
            "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
            "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
            "url": "https://huggingface.co/papers/2410.01782",
            "data": {
                "desc": "Open-RAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Retrieval-Augmented Generation —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–ª–æ—Ç–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Å–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—É—é —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Open-RAG –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∑–∞–ø—É—Ç—ã–≤–∞—é—â–∏–º–∏ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Open-RAG –Ω–∞ –±–∞–∑–µ Llama2-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ RAG-–º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—à–∏—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π.",
                "tags": [
                    "#RetrievalAugmentedGeneration",
                    "#MixtureOfExperts",
                    "#LatentLearning"
                ],
                "emoji": "üß†",
                "title": "Open-RAG: –£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            }
        }
    ],
    "date": "5 –æ–∫—Ç—è–±—Ä—è"
}