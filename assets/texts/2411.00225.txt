Title: Fashion-VDM: Video Diffusion Model for Virtual Try-On

Authors: Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, Ira Kemelmacher-Shlizerman


================================================================================

Abstract
========

We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person's identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM.

Start
=====

4 2 0 2 4 ] . [ 2 5 2 2 0 0 . 1 1 4 2 : r Fashion-VDM: Video Diffusion Model for Virtual Try-On JOHANNA KARRAS, Google Research, University of Washington, USA YINGWEI LI, Google Research, USA NAN LIU, Google Research, USA LUYANG ZHU, Google Research, University of Washington, USA INNFARN YOO, Google Research, USA ANDREAS LUGMAYR, Google Research, USA CHRIS LEE, Google Research, USA IRA KEMELMACHER-SHLIZERMAN, Google Research, University of Washington, USA Fig. 1. Fashion-VDM. Given an input garment image and person video, Fashion-VDM generates video of the person virtually trying on the given garment, while preserving their original identity and motion. We present Fashion-VDM, video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate high-quality try-on video of the person wearing the given garment, while preserving the persons identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose diffusionbased architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SA Conference Papers 24, December 36, 2024, Tokyo, Japan 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1131-2/24/12. https://doi.org/10.1145/3680528.3687623 video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-theart for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM. CCS Concepts: Computing methodologies Computer graphics; Computer vision. Additional Key Words and Phrases: Virtual Try-On, Video Synthesis, Diffusion Models ACM Reference Format: Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, and Ira Kemelmacher-Shlizerman. 2024. Fashion-VDM: Video Diffusion Model for Virtual Try-On. In SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers 24), December 36, 2024, Tokyo, Japan. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3680528.

1
With the popularity of online clothing shopping and social media
marketing, there is a strong demand for virtual try-on methods.
==================================================================================================================================

1 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al Given garment image and person image, virtual try-on aims to show how the person would look wearing the given garment. In this paper, we explore video virtual try-on, where the input is garment image and person video. The benefit of video virtual try-on (VVT) experience is that it would depict how garment looks at different angles and how it drapes and flows in motion. VVT is challenging task, as it requires synthesizing realistic try-on frames from different viewpoints, while generating realistic fabric dynamics (e.g. folds and wrinkles) and maintaining temporal consistency between frames. Additional difficulty arises if the person and garment poses vary significantly, as this creates occluded garment and person regions that need to be hallucinated. Another challenge is the scarcity of try-on video data. Perfect ground truth data (i.e. two videos of different people wearing the same garment and moving in the exact same way) is difficult and expensive to acquire. In general, available human video data, such as UBC Fashion [Zablotskaia et al. 2019], are much more scarce and less diverse than image data, such as LAION 5B [Schuhmann et al. 2022]. Past approaches to virtual try-on typically leverage dense flow fields to explicitly warp the source garment pixels onto the target person frames [Dong et al. 2022; Haoye Dong and Yin 2019; Jiang et al. 2022; Wen-Jiin Tsai 2023; Zhong et al. 2021]. However, these flow-based approaches can introduce artifacts due to occlusions in the source frame, large pose deformations, and inaccurate flow estimates. Moreover, these methods are incapable of producing realistic and fine-grained fabric dynamics, such as wrinkling, folding, and flowing, as these details are not captured by appearance flows. recent breakthrough in image-based virtual try-on uses diffusion model [Zhu et al. 2023], which implicitly warps the input garment under large pose gaps and heavy occlusion using spatial cross-attention. However, directly applying [Zhu et al. 2023] or other image-based try-on methods for VVT in frame-by-frame manner creates severe flickering artifacts and temporal inconsistencies. Diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; SohlDickstein et al. 2015; Song et al. 2020; Song and Ermon 2019] have shown promising results on various video synthesis tasks, such as text-to-video generation [Ho et al. 2022b] and image-to-video generation [Guo et al. 2023; Hu et al. 2023; Karras et al. 2023]. However, key challenge is generating longer videos, while maintaining temporal consistency and adhering to computational and memory constraints. Previous works use cascaded approaches [Ho et al. 2022a], sliding windows inference [Ho et al. 2022b; Xu et al. 2023], pastframe conditioning [Harvey et al. 2022; Lee et al. 2023; Mei and Patel 2023], and transitions or interpolation [Chen et al. 2023a; Wang et al. 2023b]. Yet, even with such schemes, longer videos are temporally inconsistent, contain artifacts, and lack realistic textures and details. We argue that, similar to context modeling for LLMs [Chen et al. 2023b], short-video generation models can be naturally extended for long-video generation by temporally progressive finetuning scheme, without introducing additional inference passes or multiple networks. potential option for diffusion-based VVT is to apply an animation model to single try-on image generated by an image try-on model. However, as this is not an end-to-end trained system, any image try-on errors will accumulate throughout the video. We argue that single VVT model would overcome this issue by 1) injecting explicit person and garment conditioning information into the model and 2) having an end-to-end training objective. We present Fashion-VDM, the first VVT method to synthesize temporally consistent, high-quality try-on videos, even on diverse poses and difficult garments. Fashion-VDM is single-network, diffusionbased approach. To maintain temporal smoothness, we inflate the M&M VTO [Zhu et al. 2024] architecture with 3D-convolution and temporal attention blocks. We maintain temporal consistency in videos up to 64-frames long with single network by training in temporally progressive manner. To address input person and garment fidelity, we introduce split classifier-free guidance (split-CFG) that enables increased control over each input signal. In our experiments, we also show that split-CFG increases realism, temporal consistency, and garment fidelity, compared to ordinary or dual CFG. Additionally, we increase garment fidelity and realism by training jointly with image and video data. Our results show that Fashion-VDM surpasses benchmark methods by large margin and synthesizes state-of-the-art try-on videos.

2 RELATED WORKS
2.1 Video Diffusion Models
Many early video diffusion models [Ho et al. 2022b] (VDMs) adapt
text-to-image diffusion models to generate batches of consecutive
video frames, often employing temporal blocks within the denoising
UNet architecture to learn temporal consistency [Ho et al. 2022a,b].
Latent VDMâ€™s [Andreas Blattmann 2023; Blattmann et al. 2023; Gu
et al. 2023; Guo et al. 2023; He et al. 2022b; Karras et al. 2023; Mei and
Patel 2023; Wang et al. 2023a] reduce the computational complexity
of standard VDMâ€™s by performing diffusion in the latent space.
==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

To achieve longer videos and increased spatial resolution, [Ho et al. 2022a] proposes cascade of temporal and spatial upsampling UNets. Other methods employ similar schemes of cascaded models for long video generation [Wang et al. 2023a]. However, cascaded strategies require multiple networks and inference runs. Another strategy is to synthesize sparse keyframes, then use frame interpolation [Mei and Patel 2023], past-frame conditioning [He et al. 2022b], temporally overlapping frames [Xu et al. 2023], and predicting transitions between frames [Chen et al. 2023a; Wang et al. 2023b] to achieve longer, temporal-consistent videos. Unlike past long-video VDMs, Fashion-VDM is unified (non-cascaded) diffusion model that generates long video up to 64 frames long in single inference run, thereby reducing memory requirements and inference time.

2.2
Many VDMâ€™s are text-conditioned [Andreas Blattmann 2023; Blattmann
et al. 2023; Ho et al. 2022a; Mei and Patel 2023] and there is in-
creasing interest in image-conditioned VDMâ€™s [Guo et al. 2023; Hu
et al. 2023; Karras et al. 2023]. To maintain the exact details of
input images, some methods require inference-time finetuning [An-
dreas Blattmann 2023; Guo et al. 2023; Karras et al. 2023]. In contrast,
Fashion-VDM requires no additional finetuning during test time to
maintain high-quality details of the input person and garment.
==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

Some recent diffusion-based animation methods are both imageand pose-conditioned [Girdhar et al. 2023; Guo et al. 2023; Hu et al. 2 Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 2. Fashion-VDM Architecture. Given noisy video ğ‘§ğ‘¡ at diffusion timestep ğ‘¡ , forward pass of Fashion-VDM computes single denoising step to get the denoised video ğ‘§ ğ‘¡ 1. Noisy video ğ‘§ğ‘¡ is preprocessed into person poses ğ½ğ‘ and clothing-agnostic frames ğ¼ğ‘, while the garment image ğ¼ğ‘” is preprocessed into the garment segmentation ğ‘†ğ‘” and garment poses ğ½ğ‘” (Section 3.3). The architecture follows [Zhu et al. 2024], except the main UNet contains 3D-Conv and temporal attention blocks to maintain temporal consistency. Additionally, we inject temporal down/upsampling blocks during 64-frame temporal training. Noisy video ğ‘§ğ‘¡ is encoded by the main UNet and the conditioning signals, ğ‘†ğ‘” and ğ¼ğ‘, are encoded by separate UNet encoders. In the 8 DiT blocks at the lowest resolution of the UNet, the garment conditioning features are cross-attended with the noisy video features and the spatially-aligned clothing-agnostic features ğ‘§ğ‘ and noisy video features are directly concatenated. ğ½ğ‘” and ğ½ğ‘ are encoded by single linear layers, then concatenated to the noisy features in all UNet 2D spatial layers. 2023; Karras et al. 2023; Xu et al. 2023]. DreamPose uses pretrained (latent) Stable Diffusion model without temporal layers to generate videos in frame-by-frame manner [Karras et al. 2023]. More recently, Animate Anyone [Hu et al. 2023] encodes the image using ReferenceNet and their diffusion model incorporates spatial, cross, and temporal attention layers to maintain consistency and preserve details, while MagicAnimate [Xu et al. 2023] introduces an appearance encoder to maintain the fidelity across the frames and generates long video using temporally overlapping segments. In contrast, Fashion-VDM is non-latent, temporally-aware video diffusion model, capable of synthesizing up to 64 consecutive frames in single inference pass.

2.3 Virtual Try-On
Traditional image virtual try-on approaches first warp the target
garment onto the input person, then refine the resulting image [Bai
et al. 2022; Choi et al. 2021; Cui et al. 2023; Han et al. 2018; He
et al. 2022a; Lee et al. 2022; Men et al. 2020; Ren et al. 2022; Yang
et al. 2020; Yu et al. 2019; Zhang et al. 2021]. Similarly, for video
virtual try-on (VVT), past methods often rely on multiple networks
to predict intermediate values, such as optical flow, background
masks, and occlusion masks, to warp the target garment to the
person in each frame of the video [Dong et al. 2022; Haoye Dong
and Yin 2019; Jiang et al. 2022; Wen-Jiin Tsai 2023; Zhong et al.
2021]. However, inaccuracies in these intermediate values lead to
artifacts and misalignment. Some image try-on approaches incor-
porate optical flow estimation to alleviate this misalignment [Bai
et al. 2022; Lee et al. 2022; Lewis et al. 2021; Xintong Han and Scott
========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

2020]. For VVT, MV-TON [Zhong et al. 2021] proposes memory refinement module to correct inaccurate details in the generated frames by encoding past frames into latent space, then using this as external memory to generate new frames. ClothFormer [Jiang et al. 2022] estimates an occlusion mask to correct for flow inaccuracies. Current state-of-the-art VVT methods achieve improved results by utilizing attention modules in the warping and fusing phases [Jiang et al. 2022; Wen-Jiin Tsai 2023]. In contrast to earlier flow-based methods, TryOnDiffusion [Zhu et al. 2023] leverages diffusion-based method conditioned with pose and garment for image virtual try-on. WarpDiffusion [Zhang et al. 2023] tries to reduce the computational cost and data requirements by bridging warping and diffusion-based virtual try-on methods. StableVITON [Kim et al. 2023] avoids warping by finetuning pre-trained latent diffusion [Rombach et al. 2022] encoders for input person and garment conditioning via cross-attention blocks. Mix-and-match (M&M) VTO [Zhu et al. 2023] extends single tryon task for mixmatch tryon application with novel person embedding finetuning strategy.

2.4
Video datasets are often smaller and less diverse, compared to image
datasets, as images are more abundant online. To alleviate this prob-
lem, [Ho et al. 2022a,b; Xu et al. 2023] propose jointly leveraging
image and video data for training. VDM [Ho et al. 2022b] and Ima-
gen Video [Ho et al. 2022a] implement joint training by applying
a temporal mask to image batches. MagicAnimate [Xu et al. 2023]
=====================================================================================================================================================================================================================================================================================================================================================================================================================

3 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al Fig. 3. Split-CFG Ablation. We compare different split-cfg weights, where (ğ‘¤, ğ‘¤p, ğ‘¤g, ğ‘¤full ) correspond to the unconditional guidance, person-only guidance, person and cloth guidance, and full guidance terms, respectively. applies joint training during the pretraining stage of their appearance encoder and pose ControlNet. We improve upon existing joint training schemes (see Section 3.7), ultimately demonstrating the benefit of joint image and video training for video try-on.

3.1 Problem Formulation
In video virtual try-on, the input is a video {ğ¼ 0
} of a
person ğ‘ consisting of ğ‘ frames and a single garment image ğ¼ğ‘” of
another person wearing garment ğ‘”. The goal is to synthesize a video
ğ‘¡ğ‘Ÿ denotes the ğ‘–-th try-on video frame
{ğ¼ 0
that preserves the identity and motion of the person ğ‘ wearing the
garment ğ‘”.
================================================================================================================================================================================================================================================================================================================================================

ğ‘¡ğ‘Ÿ , ..., ğ¼ ğ‘ 1 ğ‘¡ğ‘Ÿ }, where ğ¼ğ‘– ğ‘, ..., ğ¼ ğ‘ 1 ğ‘ ğ‘¡ğ‘Ÿ , ğ¼ 1 ğ‘, ğ¼

3.2 Preliminary: M&M VTO
Our VTO-UDiT network architecture is inspired by [Zhu et al. 2024],
a state-of-the-art multi-garment image try-on diffusion model that
also enables text-based control of garment layout. VTO-UDiT is
represented by
=============================================================================================================================================================================================================================================

Ë†ğ‘¥0 = ğ‘¥ğœƒ (ğ‘§ğ‘¡ , ğ‘¡, ğ‘ğ‘¡ğ‘Ÿ ) (1) where Ë†ğ‘¥0 is the predicted try-on image by the network ğ‘¥ğœƒ , parameterized by ğœƒ , at diffusion timestep ğ‘¡, ğ‘§ğ‘¡ is the noisy image, and ğ‘ğ‘¡ğ‘Ÿ is the conditioning inputs. VTO-UDiT is parameterized in v-space, following [Salimans and Ho 2022]. Each conditioning input is encoded separately by fully convolutional encoders and processed at the lowest resolution of the main UNet via DiT blocks [Peebles and Xie 2022], where conditioning features are processed with self-attention or cross-attention modules. However, while it shows impressive results for image try-on, VTO-UDiT cannot reason about temporal consistency when applied to video inputs.

3.3
From the input video frames, we compute the clothing-agnostic
ğ‘ , ..., ğ½ ğ‘ âˆ’1
frames ğ¼ğ‘ = {ğ¼ 0
},
ğ‘
=======================================================================================================

}, person poses ğ½ğ‘ = {ğ½ 0 ğ‘, ..., ğ¼ ğ‘ 1 ğ‘ ğ‘ , ğ½ 1 ğ‘, ğ¼ 1 ğ‘, ..., ğ‘€ ğ‘ 1 ğ‘, ğ‘€ and person masks {ğ‘€0 }. The clothing-agnostic frames ğ‘ mask out the entire bounding box area of the person in the frame, except for the visible body regions (head, hands, legs, and shoes), following TryOnDiffusion [Zhu et al. 2023]. Optionally, the clothingagnostic frames can keep the original bottoms, if doing top try-on only. From the input garment image ğ¼ğ‘”, we extract the garment segmentation image ğ‘†ğ‘”, garment pose ğ½ğ‘”, and garment mask ğ‘€ğ‘”. The garment pose refers to the pose keypoints of the person wearing the garment before segmentation. We channel-wise concatenate ğ‘€ğ‘– ğ‘ to ğ¼ğ‘– ğ‘ and ğ‘€ğ‘” to ğ¼ğ‘”. Poses, masks, and segmentations are computed using an in-house equivalent of Graphonomy [Gong et al. 2019]. Both person and garment pose keypoints are preprocessed to be spatially aligned with the person frames and garment image, respectively.

3.4 Architecture
Our overall architecture is depicted in Figure 2. We adapt the VTO-
UDiT architecture [Zhu et al. 2023] by inflating the two lowest-
resolution downsampling and upsampling blocks with temporal
attention and 3D-Conv blocks, as shown in Figure 2. To be specific,
after the 2D-Conv layers, we add a 3D-Conv block, a temporal
attention block, and a temporal mixing block to linearly combine
spatial and temporal features, as proposed in [Blattmann et al. 2023].
In the temporal mixing blocks, processed features after the spatial
attention layer ğ‘§ğ‘  are linearly combined with processed features after
the temporal attention layer ğ‘§ğ‘¡ via learned weighting parameter ğ›¼:
========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

ğ‘§ ğ‘¡ = ğ›¼ ğ‘§ğ‘  + (1 ğ›¼) ğ‘§ğ‘¡ (2) During 64-frame training (see Section 3.6), we further inflate the model with temporal downsampling and upsampling blocks with factor 2, to reduce the memory footprint of the model. These blocks are added before and after the lowest-resolution spatial blocks, respectively. The person and garment poses are encoded and used to condition all 2D spatial layers in the UNet. The 8 Diffusion Transformer (DiT) blocks [Peebles and Xie 2022] between the UNet encoder and decoder condition our model on the segmented garment and clothing-agnostic image features, as proposed by [Zhu et al. 2024]. In each block, the garment images are cross-attended with the noisy target features, while the agnostic input images are concatenated to the noisy target features.

3.5 Split Classifier-Free Guidance
Standard classifier-free guidance (CFG) [Ho and Salimans 2022] is a
sampling technique that pushes the distribution of inference results
towards the input conditioning signal(s); however, it does not allow
for disentangled guidance towards separate conditioning signals.
Instruct-Pix2Pix [Brooks et al. 2023] introduces dual-CFG, which
separates the CFG weights for text and image conditioning signals,
drawing inspiration from Composable Diffusion [Liu et al. 2022].
======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

We introduce split-CFG, generalization of dual-CFG which allows independent control over multiple conditioning signals. See Algorithm 1. The inputs to Split-CFG are the trained denoising UNet ğœ–ğœƒ , the list of all conditioning signal sets ğ¶, and the respective conditioning weights ğ‘Š . For each subset of conditioning signals ğ‘ğ‘– ğ¶, containing one or more conditional inputs, the algorithm 4 Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan computes the conditional result ğœ–ğ‘– given ğ‘ğ‘– . Then, the weighted difference between the conditional result ğœ–ğ‘– from the past conditional result ğœ–ğ‘– 1 is added to the prediction. In this way, the prediction is pushed in the direction of ğ‘ğ‘– . Algorithm 1: Split Classifier-Free Guidance Split-CFG(ğœ–ğœƒ , ğ¶,ğ‘Š ) ğ‘ Ë†ğœ–ğœƒ (ğ‘§ğ‘¡ , ğ¶ ) ğ‘¤0ğœ–ğœƒ (ğ‘§ğ‘¡ , ) Ë†ğœ–0 Ë†ğœ–ğœƒ (ğ‘§ğ‘¡ , ğ¶ ) for ğ‘ğ‘– in do current conditioning signals; initialize prediction; store past prediction; ğ‘ ğ‘ {ğ‘ğ‘– } Ë†ğœ–ğ‘– ğœ–ğœƒ (ğ‘§ğ‘¡ , ğ‘ ) Ë†ğœ–ğœƒ (ğ‘§ğ‘¡ , ğ¶ ) Ë†ğœ–ğœƒ (ğ‘§ğ‘¡ , ğ¶ ) + ğ‘¤ğ‘– ( Ë†ğœ–ğ‘– Ë†ğœ–ğ‘– 1 ) ; Ë†ğœ–ğ‘– 1 Ë†ğœ–ğ‘– update ğ‘; store new prediction; update Ë†ğœ–ğ‘– 1 end return Ë†ğœ–ğœƒ (ğ‘§ğ‘¡ , ğ¶ ) Split-CFG is naturally dependent on the order of the conditioning signals. Intuitively, the first conditional output will have the largest distance from the null output, thus most affecting the final result. In our implementation, our conditioning groups ğ¶ consist of (1) the empty set (unconditional inference), (2) the clothing-agnostic }), (3) all clothing-related inputs (ğ‘†ğ‘”, ğ½ğ‘”, ğ‘€ğ‘”), images ({ğ¼ 0 and (4) lastly, all remaining conditioning inputs ({ğ½ 0 }. We denote the respective weights of each term as (ğ‘¤ , ğ‘¤ğ‘, ğ‘¤ğ‘”, ğ‘¤full). Empirically, we find this ordering yields the best results. ğ‘ , ..., ğ½ ğ‘ 1 ğ‘ ğ‘, ..., ğ¼ ğ‘ 1 ğ‘ Overall, we find that controlling sampling via split-CFG not only enhances the frame-wise garment fidelity, but also increases photorealism (FID) the inter-frame consistency of video (FVD), compared to ordinary CFG.

3.7
Training the temporal phases solely with video data, which is much
more limited in scale compared to image data, would disregard
the image dataset entirely after the pretraining phase. We observe
that video-only training in the temporal phases sacrifices image
quality and fidelity for temporal smoothness. To combat this issue,
we train the temporal phases jointly with 50% image batches and
============================================================================================================================================================================================================================================================================================================================================================================================================

5 Fig. 4. Joint Training Ablation. Joint image and video training improves the realism of occluded views. 50% video batches. We implement joint training via conditional network branching [Huang et al. 2016], i.e. for image batches, we skip updating the temporal blocks in the network. Unlike temporal masking strategies[Ho et al. 2022a,b], using conditional network branching allows us to include other temporal blocks (Conv-3D, temporal mixing) in addition to temporal attention. Critically, we also train with either image-only or video-only batches, rather than batches of video with appended images [Ho et al. 2022a,b]. This improves data diversity and training stability by not constraining the possible batches by the number of available video batches. We observe that improved garment fidelity and multi-view realism, especially for synthesized details in occluded garment regions with joint image-video training compared to video-only training (see Figure 4).

4.1.1 Reproducibility. To promote future work in this area and
allow fair comparisons with our method, we plan to release a bench-
mark dataset, including sample paired person videos, garment im-
ages, and corresponding preprocessed inputs. We also analyze a
version of our model trained and tested exclusively on publicly-
available UBC video data [Zablotskaia et al. 2019] in Section 4.3.1.
========================================================================================================================================================================================================================================================================================================================================================================================================

SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al Table 1. Quantitative Ablation Studies. For each ablated version of our model, we compute FID, FVD, and CLIP scores using both UBC and our test videos with randomly paired garments. Bolded values indicate the best score in each column.

Our Test Dataset
================

FID FVD CLIP FID FVD CLIP ğ‘¤/ğ‘œ Split-CFG ğ‘¤/ğ‘œ Joint Training ğ‘¤/ğ‘œ Prog. Training ğ‘¤/ğ‘œ Temporal Blocks Ours (Full) 145 106 102 94 687 579 631 1019 515 0.745 0.744 0.736 0.739 0.752 78 96 87 95 71 450 565 824 565 377 0.663 0.651 0.651 0.642 0.

5.1 Split Classifier-Free Guidance
Split-CFG improves per-frame person and garment fidelity, thereby
improving overall inter-frame temporal consistency and photore-
alism. In Figure 3, we compare results generated with different
split-CFG weights at inference time. By increasing the person guid-
ance weight ğ‘¤ğ‘ from 0 to 1, the realism and identity of the input
person are improved. Increasing the full-conditional weight ğ‘¤full
============================================================================================================================================================================================================================================================================================================================================================================================================================================

6 Fig. 5. Garment Fidelity Ablations. We compare our full model with ablated versions without split-CFG and without joint image-video training in terms of garment fidelity. Both split-CFG and joint image-video training improve fine-grain garment details (top row) and novel view generation (bottom row). improves the garment fidelity, but not as much as by increasing the garment weight ğ‘¤ğ‘” alone, as in the last column. We provide quantitative split-CFG ablation results in the Supplementary. In the Supplementary, we demonstrate that increasing ğ‘¤ğ‘” also increases fine-grain garment details when using version of our model trained on limited video data. This suggests split-CFG does not require extensive training to be useful and can be impactful in low-resource settings. Joint Image-Video Training

5.3 Temporal Blocks
As seen in prior works [Ho et al. 2022a,b], interleaving 3D-convolution
and temporal attention blocks into the 2D UNet greatly improves
temporal consistency. Removing temporal blocks entirely causes
large temporal inconsistencies. For instance, in the top row of Fig-
ure 6, the ablated model without temporal blocks swaps the pants
and body shape in each frame.
==============================================================================================================================================================================================================================================================================================================================================================================================

Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan Table 2. Quantitative Comparisons. We compare Fashion-VDM to the baseline methods using the UBC test dataset [Zablotskaia et al. 2019] and our test dataset of internet videos. Fashion-VDM quantitatively outperforms other methods on all metrics.

Our Test Dataset
================

FID FVD CLIP FID FVD CLIP TryOn Diffusion Magic Animate Animate Anyone Ours (Full) 94 155 118 1019 1861 819 515 0.739 0.702 0.727 0.752 Ours (UBC-Only) 39 172 0. 95 97 112 71 129 960 694 468 377 949 0.663 0.642 0.629 0.669 0. single try-on image from the first input frame and garment image using TryOn Diffusion, then use the extracted poses from the input frames to animate the result. In addition, we provide user survey results in the Supplementary.

6.1 Qualitative Results:
We qualitatively compare Fashion-VDM to the baseline methods
in Figure 8. In the top and bottom rows, we show how other meth-
ods exhibit large artifacts with large pose changes. In these exam-
ples, baseline methods struggle to preserve garment details and
hallucinate plausible occluded views. Plus, both MagicAnimate and
Animate Anyone create an overall cartoon-like appearance.
======================================================================================================================================================================================================================================================================================================================================================================================================================

In our supplementary video results, we observe that frame-byframe TryOn Diffusion results exhibit lots of flickering and garment inconsistencies. MagicAnimate fails to preserve the correct background and also does not maintain consistent garment appearance througout the video. Animate Anyone also exhibits garment temporal inconsistency, especially with large viewpoint changes, and the human motion has an unrealistic, warping effect. Overall, Fashion-VDM synthesizes more natural-looking garment motion, such as folding, wrinkling, and flow, and better preserves garment appearance.

7 LIMITATIONS AND FUTURE WORK
The main limitations of Fashion-VDM include inaccurate body
shape, artifacts, and incorrect details in occluded garment regions.
See examples and further discussion in the Supplementary. Improb-
able details may be hallucinated in unseen garment regions, because
the input image only shows one view of the garment. Future work
might consider multi-view conditioning and individual person cus-
tomization for improved garment and person fidelity. Other errors
include minor aliasing for fine-grained patterns. Finally, our method
==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

Fig. 6. Temporal Smoothness Ablations. We compare video frames generated by our ablated model without temporal blocks (top row) and without progressive training (middle row) to our full model (bottom row). Both ablated versions exhibit large frame-to-frame inconsistencies and artifacts.

6 COMPARISONS TO STATE-OF-THE-ART
We qualitatively and quantitatively compare our method to the state-
of-the-art in diffusion-based try-on and animation, as no previous
diffusion-based video try-on baselines with publicly-available code
currently exist: (1) TryOn Diffusion [Zhu et al. 2023] (2) MagicAni-
mate [Xu et al. 2023], and (3) Animate Anyone [Hu et al. 2023]. For
(1), we generate try-on results in a frame-by-frame manner for each
input frame to generate a video. For (2) and (3), we first generate a
================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

7 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al does not simulate exact physical cloth dynamics, but rather realistic video try-on visualization. Establishing physics could be great next step.

9 ETHICS STATEMENT
While we believe our research creates a positive contribution to the
research community by advancing the state-of-the-art in generative
video diffusion, we also condemn its potential for misuse, including
any spreading misinformation or manipulating human content for
malicious purposes. While our method is trained on public data
containing identifiable humans, we will not release any images or
videos containing personally identifiable features, such as faces,
tattoos, or logos to protect the privacy of these individuals.
=================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

REFERENCES Sumith Kulal Daniel Mendelevitch Maciej Kilian Dominik Lorenz Yam Levi Zion English Vikram Voleti Adam Letts Varun Jampani Robin Rombach Andreas Blattmann, Tim Dockhorn. 2023. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. Shuai Bai, Huiling Zhou, Zhikang Li, Chang Zhou, and Hongxia Yang. 2022. Single Stage Virtual Try-On Via Deformable Attention Flows. In Computer Vision ECCV 2022, Shai Avidan, Gabriel Brostow, Moustapha CissÃ©, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer Nature Switzerland, Cham, 409425. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2256322575. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. InstructPix2Pix: Learning To Follow Image Editing Instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1839218402. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending Context Window of Large Language Models via Positional Interpolation. arXiv:arXiv:2306.15595 Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. 2023a. SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction. arXiv:2310. Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. 2021. VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1413114140. Aiyu Cui, Jay Mahajan, Viraj Shah, Preeti Gomathinayagam, and Svetlana Lazebnik. 2023. Street TryOn: Learning In-the-Wild Virtual Try-On from Unpaired Person Images. arXiv:2311.16094 Prafulla Dhariwal and Alex Nichol. 2021. Diffusion Models Beat GANs on Image Synthesis. arXiv:arXiv:2105.05233 Xin Dong, Fuwei Zhao, Zhenyu Xie, Xijin Zhang, Daniel K. Du, Min Zheng, Xiang Long, Xiaodan Liang, and Jianchao Yang. 2022. Dressing in the Wild by Watching Dance Videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 34803489. Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. 2023. Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. arXiv:arXiv:2311. Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, and Liang Lin. 2019. Graphonomy: Universal Human Parsing via Graph Transfer Learning. arXiv:arXiv:1904.04536 Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang Jiang, and Hang Xu. 2023. Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. arXiv:arXiv:2309.03549 Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. 2023. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. arXiv:arXiv:2307.04725 Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S. Davis. 2018. VITON: An Image-Based Virtual Try-On Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Xiaohui Shen B. Wu Bing cheng Chen Haoye Dong, Xiaodan Liang and J. Yin. 2019. FWGAN: Flow-Navigated Warping GAN for Video Virtual Try-On. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Yunlin, Taiwan, 11611170. William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. 2022. Flexible Diffusion Modeling of Long Videos. arXiv:arXiv:2205.11495 Sen He, Yi-Zhe Song, and Tao Xiang. 2022a. Style-Based Global Appearance Flow for Virtual Try-On. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 34703479. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022b. Latent Video Diffusion Models for High-Fidelity Long Video Generation. arXiv:arXiv:2211.13221 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by Two Time-Scale Update Rule Converge to Local Nash Equilibrium. In Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/ paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. 2022a. Imagen Video: High Definition Video Generation with Diffusion Models. arXiv:arXiv:2210.02303 Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. arXiv:arXiv:2006.11239 Jonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guidance. arXiv:arXiv:2207.12598 Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. 2022b. Video Diffusion Models. arXiv:arXiv:2204.03458 Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. 2023. Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation. arXiv:arXiv:2311.17117 Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. 2016. Deep Networks with Stochastic Depth. arXiv:arXiv:1603.09382 Jianbin Jiang, Tan Wang, He Yan, and Junhui Liu. 2022. ClothFormer: Taming Video Virtual Try-On in All Module. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1079910808. Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira KemelmacherShlizerman. 2023. DreamPose: Fashion Video Synthesis with Stable Diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2268022690. Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. 2023. StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On. arXiv:2312.01725 Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. 2022. High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions. In Computer Vision ECCV 2022, Shai Avidan, Gabriel Brostow, Moustapha CissÃ©, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer Nature Switzerland, Cham, 204219. Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang, Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu Kim, and Sangpil Kim. 2023. Soundini: Sound-Guided Diffusion for Natural Video Editing. arXiv:2304. Kathleen Lewis, Srivatsan Varadharajan, and Ira Kemelmacher-Shlizerman. 2021. TryOnGAN: body-aware try-on via layered interpolation. ACM Trans. Graph. 40, 4, Article 115 (jul 2021), 10 pages. https://doi.org/10.1145/3450626.3459884 Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. 2022. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision. Springer, 423439. Kangfu Mei and Vishal Patel. 2023. VIDM: Video Implicit Diffusion Models. Proceedings of the AAAI Conference on Artificial Intelligence 37, 8 (Jun. 2023), 91179125. Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, and Zhouhui Lian. 2020. Controllable Person Image Synthesis With Attribute-Decomposed GAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). William Peebles and Saining Xie. 2022. Scalable Diffusion Models with Transformers. arXiv:arXiv:2212. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:arXiv:2103.00020 Yurui Ren, Xiaoqing Fan, Ge Li, Shan Liu, and Thomas H. Li. 2022. Neural Texture Extraction and Distribution for Controllable Person Image Synthesis. In Proceedings 8 Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1353513544. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1068410695. Tim Salimans and Jonathan Ho. 2022. Progressive Distillation for Fast Sampling of Diffusion Models. arXiv:arXiv:2202.00512 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. arXiv:arXiv:2210.08402 Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. arXiv:arXiv:1503.03585 Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit Models. arXiv:arXiv:2010. Yang Song and Stefano Ermon. 2019. Generative Modeling by Estimating Gradients of the Data Distribution. arXiv:arXiv:1907.05600 Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards Accurate Generative Models of Video: New Metric & Challenges. arXiv:arXiv:1812.01717 Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. 2023b. Gen-L-Video: Multi-Text to Long Video Generation via Temporal CoDenoising. arXiv:2305.18264 Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. 2023a. LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models. arXiv:arXiv:2309.15103 Yi-Cheng Tien Wen-Jiin Tsai. 2023. Attention-based Video Virtual Try-On. ACM, Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, 209216. Weilin Huang Xintong Han, Xiaojun Hu and Matthew Scott. 2020. Clothflow: flow-based model for clothed person generation. Proceedings of the IEEE/CVF international conference on computer vision, 139144,. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. 2023. MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model. arXiv:arXiv:2311.16498 Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, and Ping Luo. 2020. Towards Photo-Realistic Virtual Try-On by Adaptively Generating-Preserving Image Content. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Ruiyun Yu, Xiaoqi Wang, and Xiaohui Xie. 2019. Vtnfp: An image-based virtual try-on network with body and clothing feature preservation. Proceedings of the IEEE/CVF international conference on computer vision, 1051110520. Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. 2019. DwNet: Dense warp-based network for pose-guided human video generation. arXiv:arXiv:1910.09139 Jinsong Zhang, Kun Li, Yu-Kun Lai, and Jingyu Yang. 2021. PISE: Person Image Synthesis and Editing With Decoupled GAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 79827990. Xujie Zhang, Xiu Li, Michael Kampffmeyer, Xin Dong, Zhenyu Xie, Feida Zhu, Haoye Dong, and Xiaodan Liang. 2023. WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on. arXiv:2312.03667 Xiaojing Zhong, Zhonghua Wu, Taizhe Tan, Guosheng Lin, and Qingyao Wu. 2021. MVTON: Memory-based Video Virtual Try-on network. (2021). arXiv:arXiv:2108.07502 Luyang Zhu, Yingwei Li, Nan Liu, Hao Peng, Dawei Yang, and Ira KemelmacherShlizerman. 2024. M&M VTO: Multi-Garment Virtual Try-On and Editing. Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. 2023. TryOnDiffusion: Tale of Two UNets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 46064615. 9 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al Fig. 7. Qualitative Results. We showcase video try-on results generated by Fashion-VDM using randomly paired person-garment test videos from the UBC dataset [Zablotskaia et al. 2019] and our own collected test dataset. Note that the input garment image and input person frames come from different videos. 10 Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 8. Qualitative Comparisons. Fashion-VDM outperforms past methods in garment fidelity and realism. Especially in cases of large disocclusion, our method synthesizes more realistic novel views. 11 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al

Supplementary Material
======================

A PROGRESSIVE TRAINING DETAILS The overall progressive temporal training strategy is depicted in Figure 9. We first train base image model from scratch on image data at 512px resolution and batch size 8 for 1M iterations. Then, we inflate the base architecture with temporal blocks and continue training the model using our joint image-video training strategy. In these temporal training phases, half of the batches are from the image dataset and the other half are batches of consecutive video frames from the video dataset. When training with an image batch, we skip the temporal blocks entirely in the forward and backward passes. At each successive phase of temporal training, we initialize the model from the previous phases checkpoint and double the training video length: 8 16 32 64. We train each temporal training phase for 150K iterations. Once the video length becomes prohibitively large in memory at 64-frames, we introduce temporal downsampling and upsampling layers to the model. At test time, our model generates 512 384px videos up to 64-frames in one inference pass with single network. A.1 Training and Inference Details We train our model on 16 TPU-v4s for approximately 2 weeks, including all training phases. Our image baseline model is trained for 1M iterations with batch size of 8 and resolution 512 384px using the Adam optimizer with linearly decaying learning rate of 1ğ‘’ 4 to 1ğ‘’5 over 1M steps and 10K warm-up steps. Each phase of progressive temporal training is initialized from the previous checkpoint and trained for 150K iterations, following the order of phases described in the Section A. For all phases, we incorporate dropout for each conditional input independently 10% of the time. We train with an L2 loss on ğœ–. During inference, we use the DDPM sampler [Ho et al. 2020] with 1000 refinement steps. Each video takes approximately 8 minutes to synthesize with split-CFG and 5 minutes without split-CFG. EXAMPLES OF FAILURE CASES We show two examples of failure cases of our method in Figure 10. In row 1, we show artifacts that appear in the body/garment boundary, due to an imperfect person segmentation in the clothing-agnostic image. Imperfect segmentation is common cause of such artifacts, and may also incorrectly leak regions from the original garment. In our human evaluation (Section C.1), 10/17 videos that were failed had agnostic errors. In general, although our preprocessing methods are state-of-the-art, other types of preprocessing errors occur limit the quality of Fashion-VDM. In total, 70% of videos not chosen by human raters had errors in one or more inputs. As shown in row 2, body shape misrepresentation (e.g. slimming) occurs, because the clothing-agnostic images remove all body parts, besides hands, feet, and head, thus they do not include detailed information about body size. (ğ‘¤ , ğ‘¤ğ‘, ğ‘¤ğ‘”, ğ‘¤ğ‘ ) (1, 0, 1, 1) (1, 1, 1, 1) (1, 1, 1, 3) (1, 1, 3, 3) (1, 3, 3, 3) (1, 1, 3, 1)

Our Test Dataset
================

FID FVD CLIP FID FVD CLIP 136 95 100 99 104 62 1053 644 653 454 481 0.712 0.748 0.745 0.756 0.763 0.770 126 52 84 55 63 76 779 242 588 262 284 385 0.632 0.667 0.664 0.662 0.671 0.661 Table 3. Quantitative Ablation of Split-CFG Weights. We compute FID, FVD, and CLIP scores of our full model using different split-CFG weights. SPLIT-CFG WEIGHTS ABLATIONS We quantitatively evaluate our choice of split-CFG weights for both datasets on held-out validation set. The results are shown in Table 3. Calibrating these weights correctly is not only beneficial to preserving garment fidelity, as shown by the FID score, but also increasing temporal consistency, as shown by the FVD score. Intuitively, by increasing the similarity of the output garment to the input garment, there is less allowed variability in the appearance of each frame, thus increased temporal smoothness. Based on these results, we employ weights (1, 1, 3, 1) for UBC and weights (1, 1, 1, 1) for our test dataset. C.1 User Study In addition to qualitative and quantitative evaluations, we perform user studies for our state-of-the-art comparisons. The results are shown in Table 4. Our user studies are conducted by 5 human raters who are unfamiliar with the method. For each sample, the raters were asked to select which video performs best in each category: temporal smoothness, garment fidelity to the input garment image, and person fidelity to the input person video. The scores on both UBC test dataset and our test dataset reported are fraction of total votes divided by the total number of videos. Fashion-VDM exceeds other methods on all three user preference categories for both datasets. C.2 UBC-Only Model We initialize this model from our pretrained image model, which is comparable to an open source image diffusion model, like Stable Diffusion [?], which are trained on even larger image datasets, including LAION 5B [Schuhmann et al. 2022]. We then train progressively using both image data and UBC video data, following the same progressive training scheme as the full model. The UBC-only model exceeds all baselines on the UBC test dataset quantitatively, but is qualitatively worse at preserving intricate garment details and patterns. This is expected, given the limited size and lack of diversity of UBC training dataset. However, we discovered that increasing the split-CFG garment weight significantly improves lost garment details, even more so than with the full model. We qualitatively show this in Figure 12. This implies that when training with limited data, split-CFG becomes even more crucial to preserving the conditioning image details. We provide qualitative examples generated by our model trained only on the UBC dataset [Zablotskaia et al. 2019] in Figure 12. While 12 Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 9. Progressive Training Strategy. Fashion-VDM is trained in multiple phases of increasing frame length. We first pretrain an image model, by training only the spatial layers on our image dataset. In subsequent phases, we train temporal and spatial layers on increasingly long batches of consecutive frames from our video dataset. Video Smoothness Person Fidelity Garment Fidelity Video Smoothness Person Fidelity Garment Fidelity

Our Test Dataset
================

TryOn Diffusion Magic Animate Animate Anyone Ours (Full) 0.01 0.03 0.03 0.93 0.00 0.00 0.03 0.97 0.00 0.03 0.03 0.94 0.03 0.02 0.04 0.91 0.01 0.01 0.01 0. 0.00 0.00 0.05 0.95 Table 4. User Study. Our study indicates that users overwhelmingly prefer Fashion-VDM to other baselines in terms of video smoothness, person fidelity, and garment fidelity on both test datasets. Fig. 11. Split-CFG Ablation with UBC-Only Model. When FashionVDM is trained on the limited UBC dataset only, we observe overfitting to the largely plain garments in the UBC train dataset. However, we find that increasing garment image guidance (ğ‘¤ğ‘”) in split-CFG significantly increases garment details. Fig. 10. Failure Cases. Errors in the person segmentation may lead to artifacts (top row). Fashion-VDM may incorrectly represent body shape (bottom row). the results are still smooth and temporally consistent, the model struggles to maintain complex patterns and garment shape details. This is likely due to overfitting to the limited size and scope of the UBC training dataset, consisting of 500 videos of women in dresses. SA Conference Papers 24, December 36, 2024, Tokyo, Japan Karras, J. et al Fig. 12. Qualitative Results for UBC-Only Model. Our model trained only on UBC data generates temporally consistent, smooth try-on videos for plain and simple patterned garments, but struggles to preserve intricate patterns and complex garment shapes. 14 Fashion-VDM: Video Diffusion Model for Virtual Try-On SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 13. Additional Qualitative Results. We showcase video try-on results generated by Fashion-VDM using swapped test videos from the UBC dataset [Zablotskaia et al. 2019] and our own collected test dataset. Note that the input garment image and input person frames come from different videos.

