Title: Zipfian Whitening

Authors: Sho Yokoi, Han Bao, Hiroto Kurita, Hidetoshi Shimodaira


================================================================================

Abstract
========

The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow a highly non-uniform distribution, known as Zipf's law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling, WhiteningBERT, and headless language models, work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.

Start
=====

4 2 0 2 1 ] . [ 1 0 8 6 0 0 . 1 1 4 2 : r a

Zipfian Whitening
=================

Sho Yokoi Tohoku University / RIKEN yokoi@tohoku.ac.jp Han Bao Kyoto University bao@i.kyoto-u.ac.jp Hiroto Kurita Tohoku University hiroto.kurita@dc.tohoku.ac.jp Hidetoshi Shimodaira Kyoto University / RIKEN shimo@i.kyoto-u.ac.jp

Abstract
========

The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow highly non-uniform distribution, known as Zipfs law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipfs law significantly improves task performance, surpassing established baselines. From theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective [42], and in terms of the loss functions for imbalanced classification [36]. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling [37], WhiteningBERT [26], and headless language models [23], work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model. (cid:135) https://github.com/cl-tohoku/zipfian-whitening

Introduction
============

Representing discrete words by continuous vectors is fundamental and powerful framework of modern deep-learning-based natural language processing (NLP). Static word embeddings [43, 37], dynamic word embeddings [18, 33], and causal language models [45, 12, 54] have caused paradigm shiftthey have greatly improved the performance of virtually all kinds of NLP applications and have been actively used in relevant areas as well. While the embedded units may be characters or subwords instead of words, we simply refer to them collectively as word. Recently, the machine learning and NLP communities have discovered that the word embedding space is skewed and that correcting this can lead to better performance in downstream tasks [39, 21, 16, 56]. The isotropy of the embedding space would be one factor: vectors dispersing more evenly should be more discriminative than those clustered in the same direction [38, 21, 51]. Typically, such spatial symmetry in the embedding space is enhanced through centering/whitening [39, 16, 26]. Nevertheless, we would like to point out that most existing approaches implicitly assume uniform word frequency to formalize spatial symmetry. Consider the classical centering operation as an example: we first calculate the mean of the word vectors, and then subtract it to ensure they are zero-meaned. This method, however, has an unexpected pitfall. Recall that the definition of the p, assuming it has finite set of distinct realizations, centroid or barycenter of random vector 38th Conference on Neural Information Processing Systems (NeurIPS 2024). is given by Exp[x] = (cid:80) p(xi)xi. The classical centering, based on the standard (unweighted) mean, implicitly assumes that all words occur uniformly p(w1) = = p(wn). In reality, however, word frequencies are known to follow highly non-uniform distribution1, creating significant gap between the methodology and the actual usage of words. This seemingly obvious issue does not arise when addressing classical statistical estimation problems, as data vectors in our hands are usually representations of observations or instances. In contrast, word vectors used in NLP are representations of types or classes; each of them (such as the vector for the) abstracts the numerous instances (such as the tokens of the) appearing in the data. This problem of hidden frequencies becomes apparent in the cases where the type-token distinction [58] is crucial, such as when dealing with natural language data ( 2). The take-home message of this paper can be summarized as follows: use empirical word frequencies when calculating expected values. Following this very simple guideline leads to strong empirical outcomes ( 3.2, 3.3) and opens rich theoretical landscape ( 4, 5). Notation Let wi = w1, . . . , wn { } Rd denotes the row vector of each word type wi, and p(wi) denote the vocabulary, i.e., the set of words in interest. Bold-face [0, 1] denotes its frequency.

2 Motivation: type-token distinction and expected values
========================================================

Why have word frequencies been overlooked when considering the geometric properties of embedding spaces? This can be explained through the concept of type-token distinction [58], which is fundamental concept in linguistics and related fields but generally not required in statistical machine learning. Here, type represents class and token represents an instance. For example, the phrase perform natural language processing in natural way contains eight tokens and seven types. The instances natural appear twice, but as word type, it is counted only once. With the type-token distinction in mind, let us take fresh look at data matrices and their expected values. Typically, each row in data matrix represents one observation, i.e., one instance token. If we want to centralize set of data vectors, computing the unweighted mean is natural way in the machine learning pipeline. On the other hand, each row of word embedding matrix, i.e., word vector, is type embedding. Each word vector abstracts the numerous instances appearing repeatedly in corpus, though information on the frequency of instances for each word type is not encoded in it. The unweighted mean of word vectors treats type vectors as token vectors, resulting in the complete omission of word frequency information. { i= i=1 xi [x] = (cid:80)n Rnd or the set of word vectors Let us describe the above idea formally. The data maRnd or the set of data vectors trix i=1 } Rd represents collection of instances, observations, or tokens; then the empirical distribution is µX = (cid:80)n 1 δ(xi), where δ is the Dirac delta function. Here, the unweighted mean can be seen as the expectation (cid:98)ExµX 1 xi with the empirical distribution. On the other hand, the word embedding matrix Rd represents collection of types. When describing the empirical distribution, the hidden frequency of tokens is necessary. Given p, the empirical distribution is µW = p(wi)δ(wi). From this perspective, the centroid of the word vectors should be written as the expectation (cid:98)EwµW The distinction is not just theoretical. First, refer to Fig. 1. Word vectors are known to cluster by frequency [39, 24, 44, 10]. In this situation, the centroid (cid:56) weighted by the word frequencies is located near the narrow region where high-frequent words are concentrated (a region with light blue background), and thus differs from the unweighted mean (cid:56). Second, see Table 1, which shows Figure 1: Low-frequent words {} and high-frequent words {} are unevenly distributed in the embedding space [39, 24, 44, 10]. Consequently, the apparent mean calculated by unweighted averaging (cid:56) often differs from the actual centroid (cid:56). p(wi)wi over p. i=1 } [w] = (cid:80) wi { 1As known as Zipfs law. If we count the frequencies of words in huge English corpora, we find that the has frequency of about 5.89 102 and isotropy has frequency of about 3.47 108, difference of million times greater. 50 words sampled from each of types and tokens. Uniform sampling from types, corresponding to an unweighted mean, tends to select mostly rare words from the heavy tail. Sampling from tokens clearly captures more natural representation of language as it typically appears in text. Table 1: The difference between type-based sampling and token-based sampling. Words sampled from types Words sampled from tokens abrah, gerwen, klausen, scintillation, fanon, rubato, upstanding, collard, creeks, skookum, unbelievers, monocyte, nishikawa, hangman, crusher, unitary, bridgnorth, arousal, mildred, porton, aquasox, wylie, hipaa, krimuk, hexahedron, gilding, visakhapatnam, tatsuo, tarascon, bajram, scholes, hadad, incidental, theodosius, reichskommissariat, boeheim, amsl, buencamino, thrasyvoulos, insulated, discourtesy, nisra, ycko, luen, dooku silverchair, heat, barbera, dalvi, kuei, for, the, station, nine, ranked, zero, the, garcia, rank, four, williams, drunken, a, one, eight, of, were, zero, debate, orchestra, of, wrist, points, fractured, the, to, redirect, adnan, white, car, fond, concluded, under, two, by, five, his, infection, the, the, pop, in, one, in, one, one, fram, handled, battle, mutual

3 Embedding symmetry
====================

3.1 Definition of embedding symmetry In mathematical science fields, such as high-dimensional probability theory [55] and the volume of convex bodies [27], there are numerous intriguing definitions of spatial symmetry. Among them, we begin with the definition of the symmetry of random vectors with their frequencies [48, 55]. This is suited for dealing with word vectors because they entail word frequencies, unlike usual data instances. Definition 1 (A random vector on Rd has zero mean; the 1st moment of symmetric random vector). := Evp[v] = 0 (1) on Rd is in Definition 2 (A random vector isotropic position around its barycenter; the 2nd moment of symmetric random vector). Cov[v] := Evp[(v Evp[v])(v Evp[v])] (2) From these definitions, we will develop methods to adjust given word vectors to be symmetric in 3.2, and to evaluate the symmetry of given word vectors in 3.3. In machine learning and NLP, the spatial symmetry of embedding spaces is hot topic, and numerous theories and algorithms have been proposed [41, 21, 38, 56]. However, the approach in many researches implicitly treats all vectors equally, ignoring word frequency information. In the following sections, we will detail both the empirical and theoretical issues that uniform approach can cause, especially when applied to NLP tasks. Furthermore, when embeddings correspond to tokens rather than typessuch as in the internal representations of masked or causal language modelsa uniform approach tends to be effective. This point will be discussed in 5.1. 3.2 Enhancement of embedding symmetry This section proposes Zipfian whitening2, which symmetrizes given set of word vectors with word frequency. At glance, the most natural method to achieve Def. 1 and Def. 2 would be PCA whitening, also known as sphering. Notably, each step of whiteningcentering, decorrelation, and standardizationimplicitly involves calculating expected values. Our approach is simple: each time we calculate an expected value, we should weight it by the empirical word frequency. The specific algorithm is as shown in Algorithm 1. The only difference from general whitening is that it uses word frequency in the part highlighted in blue . Please refer to Appendix for formal explanation showing that the word vectors obtained by the proposed algorithm actually satisfy Def. 1 and Def. 2. 2In this paper, Zipfian is simply used to denote highly non-uniform distribution. Our focus is on the mismatch between actual word frequencies and uniform distribution, and we have not constructed arguments or experiments that rely on specific properties of power laws. Refining experiments and theory based on the degree of tail heaviness is an interesting direction for future work. 3 Algorithm 1 Zipfian whitening; post-processing algorithm on word embeddings. The part highlighted in blue shows the difference from the typical centering and whitening. wi wi { , word frequency : are centered, { (cid:101)wi } V} Rd Rd [0, 1]. are further whitened. } Input: Word embeddings Output: Processed word embeddings. wi { Rd (cid:80) Zipfian centering (1st moment): wiV p(wi)wi do (cid:98)µ 1: (cid:98)µ 2: for all wi wi 3: 4: end for wi Rd Rd Zipfian decorrelation and standardization (2nd moment): (cid:20) (cid:112)p(w1) 1 , . . . , (cid:112)p(wV) V (cid:21) RVd 5: 6: ΣV 7: for all wi 8: (cid:101)wi 9: end for SVD(Wp) Σ = diag(σ1, . . . , σd) do wiV Σ1 Σ1 := diag(1/σ1, . . . , 1/σd) Rdd. Rdd consists of the singular values of p. Table 2: The empirical performance of Zipfian whitening, which exploits the empirical frequency of 100. By carefully words during expectation calculations. Each cell shows the STS-B [15] score performing the simple operation of whitening, it consistently outperforms powerful baseline methods. GloVe 46.17 Word2Vec 56.98 + Centering + Whitening + ABTT [39] + SIF + CCR [7] Uniform 45.17 52.21 Zipfian 52.25 66.92 54.28 58. + Centering + Whitening + ABTT [39] + SIF + CCR [7] Uniform 55.85 56.03 Zipfian 58.84 66.50 56.98 63.04 Empirical evaluation: We confirm the effectiveness of Zipfian whitening (Algorithm 1) by measuring performance on standard sentence-level downstream tasks using post-processed word vectors. We employed the most standard word embeddingsGloVe [43], word2vec [37], and fastText [11]and utilized the widely adopted evaluation tasks, including STS-B [15] and related benchmarks. Detailed experimental settings can be found in Appendix B. Table 2 shows the results on the STS-B task. Remarkably, the proposed Zipfian whitening shows significant advantages not only over standard (uniform) centering and whitening but also over the strong baseline method [7] specifically designed to create powerful sentence vectors. Consistent results were obtained with various benchmark datasets, multiple empirical word probabilities, and language other than English (Appendix C)3. In 4.2, one reason for this remarkable performance is clarified from the perspective of information geometry. 3.3 Evaluation of embedding symmetry The community is greatly interested not only in making word vector spaces symmetric but also in evaluating how symmetric or asymmetric space is [21, 49]. Here, we return to Def. 1 and Def. 2 and describe metrics for evaluating the symmetry of word embedding spaces with word frequency. Degree of centralitythe 1st moment of symmetry: Recall that, if the barycenter E[v] is close to 0, then the random vector can be considered symmetric in terms of the first moment (Def. 1). 3Notably, we observed improved scores when using word frequencies from the evaluation dataset itself as p(w). In general, for NLP tasks, p(w) refers to word frequencies derived from the embedding training data or from standard large corpus. However, to optimize downstream task performance, it is preferable to base p(w) on word frequencies within the evaluation dataset itself used for those tasks. This adjustment exemplifies covariate shift [52] in machine learning, where the distribution of training data differs from that of test data. 4 E[v] 0 appears to be reasonable way to measure the Thue, examining the value of R>0) should be considered symmetry of the first moment. However, random vectors and αv (α equivalent in terms of spatial symmetry. Thus, we define the scale-invariant metric (Def. 3), obtained ]. by dividing by the average length E[ E[v] := E[v] Definition 3 (Degree of centrality for the random vector p; the 1st moment of symmetry). Sym1(v) := 1 (cid:13) (cid:13) (cid:13) (cid:13) vp [v] (cid:13) (cid:13) / (cid:13) (cid:13) vp ] [ (3) By definition, Sym1(v) takes values in [0, 1], and Sym1(v) = 1 if and only if is zero mean. Degree of isotropythe 2nd moment of symmetry: If the covariance matrix E[(v E[v])] is constant multiple of the identity matrix d, i.e., if the random vector has an equal spread in all directions, is symmetric in terms of the second moment (Def. 2). Following convention, this degree can be confirmed by examining the flatness of the eigenspectrum. E[v])(v Definition 4 (Degree of isotropy around the barycenter for the random vector moment of symmetry). Sym2(v) := 1 log λ1 λj (cid:80) , . . . , λd λj (cid:80) (cid:32) (cid:33) p; the 2nd (4) λ1, . . . , λd { H(p1, . . . , pd) := } (cid:80) are the eigenvalues of the covariance matrix vp [(v vp [v])(v vp [v])]. pi log pi is the Shannon entropy. Proposition 1. Sym2(v) takes values in [0, 1], and Sym2(v) = 1 if and only if is isotropic around Proof. Please refer to Appendix D. its barycenter (Def. 2). Note that the approach of measuring the entropy of the spectrum to evaluate the flatness of signal can be found in many fields. For example, similar definitions are seen in probability processes [14] and signal processing [17, 47]. We also follow this standard and powerful line. Algorithm: To compute the evaluation metrics of symmetry (Def. 3, Def. 4) for given word vectors, again, one should just use the empirical word frequency when calculating the expectations. pseudocode for measuring symmetry is provided in Appendix E. Empirical evaluation: To what extent does our symmetry score (an intrinsic evaluation of embedding spaces) correlate with downstream task performance (an extrinsic evaluation of those)? As baselines, we use versions of our symmetry score that do not account for word frequency, calculated in uniform manner. We also compare with popular symmetry scores in NLP, the average of cosine similarity (Ave. Cos.) [21] and the recently proposed IsoScore [49]. Note that all these baselines implicitly assume uniform word frequency. Additional experimental settings can be found in Appendix B. Fig. 2 shows the results. The right side of Fig. 2 demonstrates the superiority of the Zipfian approach. Moving from the bottom-left to the top-right of the figurei.e. as both the 1st (x-axis) and 2nd moments (y-axis) of the symmetry score increaseit is clearly visible that the downstream task performance increases (the color becomes more red). In contrast, in the left-hand plot, which assumes uniform word frequency, there is no observed relationship between the symmetry score (x and y-axis) and the downstream task performance (color). Table 3 lists the correlation coefficients between the symmetry scores and downstream task performance in more detail. It can be seen that the symmetry scores considering word frequency can predict downstream task performance with remarkably high correlation. On the other hand, the prediction performance of other metrics, including Ave. Cos. and IsoScore that implicitly assume uniform word frequency, is unsatisfactory. Surprisingly, when the most popular Ave. Cos. metric shows almost no correlation (0.04) with downstream task performance (STS-B), Zipfian symmetry metric has strong positive correlation (0.83) with it.

4 Why is Zipfian whitening better than uniform whitening?
=========================================================

A natural question is why the Zipfian approach empirically dramatically outperforms the uniform approach. We provide theoretical explanation using Table 4. In nutshell, significant difference arises depending on whether the base measure of an exponential family is uniform or Zipfian. 5 Figure 2: The relationship between the 1st-order symmetry (Def. 3, x-axis), the 2nd-order symmetry (Def. 4, y-axis), and task performance (color). Each point represents either pre-trained or postprocessed word embeddings (GloVe, word2Vec, and fastText). The Zipfian measure well captures the downstream task performance (right), while the uniform isotropic measure cannot (left). Table 3: Spearmans ρ 100 (each cell) between the symmetry scores (each column) and downstream STS-B performance (each row), on pre-trained and post-processed embeddings (GloVe, word2Vec, and fastText). The scores based on the Zipfian prior show significantly higher correlation with task performance compared to those based on the uniform prior including Ave. Cos. and IsoScore. Ave. Cos. [21] IsoScore [49] Uniform Zipfian 1st moment 2nd moment 1st moment 2nd moment STS-B SICK-R 6.95 20.09 0.07 18.41 21.91 13.26 21.21 8.71 62.13 60.04 89.55 64.60 4.1 Characterization through generative model, partition function, and whitening Exponential families: Hereafter, we interpret two salient generative models from the viewpoint of exponential families: one given by Arora et al. [6] and the other generalizing the LevyGoldberg formula [32, Eq. (7)]. Details of these models will be provided shortly. An exponential family is class of probability distributions of random variable parametrized by parameter θ, written in the following (canonical) form: p(x θ) = π(x) exp ( x, θ ψ(θ)) , ψ(θ) := log Z(θ) = log π(x) exp( x, θ , (18) (cid:32) (cid:88) (cid:33) ) where is sufficient statistic, θ is called natural parameter, π is the base measure (or prior), and ψ is the log-partition function. Once we specify the base measure π and the canonical pair (x, θ), the log-partition function is determined. That being said, the base measure π is the design choice of an exponential family left for us. In the following, we specifically examine an exponential family of c), where word is predicted given context c. Specifically, the context distributions in the form p(w represents co-occurring word (in static word embeddings), cloze sentence (in masked language models), or sentence prefix (in causal language models). In all of these cases, we predict word with the logit , making the exponential family natural probabilistic model. Here, the vector represents the vector expression of the context c, known as the context vector. Note that, even for the same word t, the predicted word vector w(t) and the predicting context vector c(t) are distinct. w, Uniform prior: Arora et al. firstly considered log-linear generative model of word embeddings given context (6) and demonstrated that when the generative model is adopted with normalized context vectors and huge vocabulary, the partition function asymptotically becomes constant (8) [6, Lemma 2.1]. Here, we can regard that this model belongs to the exponential family with the uniform base measure π(w) = π(c) = 1/ 4. 4Although Arora et al. [6]s generative model treats context vector as model parameter drifting by random walk, we can cast their model into an exponential family because they did not specify how the initial context vector is generated. Hence, by regarding as an observed token with the uniform prior π(c) = 1/V, 6 Table 4: Through the differences in the underlying generative models, the empirical superiority of Zipfian whitening over uniform whitening can be understood. Generative models behind the (whitened) embeddings π(w) exp(w, c) Z(c) , Z(c) = p(w c) = (cid:88) π(w) exp(w, c); p(w, c) = p(w c)π(c) (5) with Uniform prior with Zipfian prior (w c) = 1 exp(w, c) (c) , π(c) 1 (6) (w c) = p(w) exp(w, c) (c) , π(c) = p(c) (7) Partition functions become constant under certain conditions Assume 1, , then At the optimal solution of the corresponding loss, (c) := (cid:88) exp(w, c) = const. (8) [6] (c) := (cid:88) p(w) exp(w, c) = const. (9) [32] Whitening coarsely achieves constant partition function (c) = V+ (cid:18)(cid:88) (cid:19) + V+ 0c + 1 2 1 2 (cid:18)(cid:88) (cid:19) ww + . . . z (c) = V+ (cid:18)(cid:88) (cid:19) p(w)w + cI . . . const. (10)[39] V+ 0c + (cid:19) p(w)ww + . . . (cid:18)(cid:88) cI . . . const. (11)[ours] 1 1 2 Vector norm under generative models w2 2 2d log p(w) 2Z (12) [6] G(w) 2KL(p()p( w)) (13) [42] [Thm. 1] long vector frequent (uninformative) word long vector informative word Loss and error corresponding to generative models p(w c) softmax cross-entropy loss logit-adjusted softmax cross-entropy loss (w,c) log exp(w, c) (c) misclassification error w, c] P(w,c)[w arg max (14) (16) (w,c) log p(w) exp(w, c) (c) balanced error 1 (cid:88) wV Pcw[w arg max w, c] (15) [36] (17) [36] Zipfian prior: An exponential family adopted with the Zipfian measure can be written as (7). This generative model can be naturally derived from the skip-gram model with negative sampling (SGNS) [37]. By assuming that the linear model is sufficiently capable of discriminating cooccurring words and negative samples (as in the realizable case), we can see that the generative model of the word embeddings must comply with the following formula: w, (cid:55) log p(w, c) p(w)p(c) log = w, , (19) where is the number of negative samples. This optimality formula owes to Levy and Goldberg [32], and we call (19) the LevyGoldberg formula. more concise derivation is later given by Oyama et al. [42]. We can regard the LevyGoldberg formula as an exponential family with the Zipfian k1. base measure, π(w) = p(w) and π(c) = p(c), and the constant log-partition function (c) The generative model (7) is relaxation of the LevyGoldberg formula since we do not impose the realizability assumption necessary for the derivation of (19). their model is reduced to (6). The static context prior does not contradict Arora et al. [6]s model with sufficiently large d, where the random walk drifts extremely slowly. 7 What does whitening do? Mu and Viswanath [39] proposed method to approximately make the partition function of the uniform prior model constant by centering the word vectors and removing the top principal components (10). Our Zipfian whitening corresponds to Mu and Viswanaths post-processing method, in the sense that ours and theirs make the partition function constant up to the second moment (11) and (10), respectively. In summary, Zipfian whitening (11) transforms probabilistic model into an exponential family adopted with the Zipfian base measure (7), making it closer to the LevyGoldberg formula (19). 4.2 Emphasis on rare words by Zipfian prior Let us explore further why the Zipfian prior results in good performance in downstream tasks ( 3.2). In summary, the Zipfian prior approach emphasizes low-frequency words, while the uniform prior approach emphasizes high-frequency words, both from perspectives of vector norms and errors/losses. So far in this paper, we have repeatedly discussed weighting each word according to frequency, so it may seem contradictory that Zipfian approach emphasizes low-frequency words as result. To illustrate, let us reconsider centering. In centering, the mean vector is subtracted from each vector. Weighting each word vector by frequency when constructing the mean vector means that signals corresponding to high-frequency words are removed more substantially from each vector. The emphasis on low-frequency words has been repeatedly supported throughout the history of NLP and information retrieval, such as Luhns hypothesis [34], inverse document frequency (IDF) [53], and smooth inverse frequency (SIF) [7]. For instance, it is reasonable to emphasize the word isotropy when creating sentence embedding containing both words the and isotropy. From the perspective of vector norm Under the Zipfian prior model, words with larger information content have longer (emphasized) vector representations. Conversely, under the uniform prior model, words with smaller information content have longer (emphasized) vector representations. As representative example of uniform prior models, the norms of word vectors learned by random walk language models are theoretically and empirically proportional to word frequency (12) (see Eq. (2.4) and Fig. 2 in Arora et al. [6]). That is, in such embedding space, words with less information (e.g., the) are emphasized. This tendency is consistently observed in dynamic language models and causal language models that adopt the softmax cross-entropy loss, another typical example of the uniform prior family [28]. By contrast, when training word embeddings with skip-gram negative sampling [37], the word embeddings follow the Zipfian prior family, and their norms become larger with greater information, which we show subsequently [50, 60, 42]. Based on the formulation of the exponential family and following Eq. (12) of Oyama et al. [42], we formally describe the norm properties of the word vectors obtained from the Zipfian prior model. Theorem 1 (The norm of word vector learned with empirical Zipfian prior models reflect the information amount of the word; refined version of [42] Eq. (12)). Assume that word embeddings follow the Zipfian prior model (7), For the same word t, the vector on the predicted side and wi { } the vector on the predicting side are shared: w(t) tV p(t)w(t) = 0 (centered w.r.t. Zipfian prior), then each word vector w(t) satisfy (cid:88) c(t) (weight tying), and (cid:80) t)), G(t) := p(t t)c(t)c(t), (20) w(t) 2 G(t) 2KL(p( p( ) tV where Proof. Refer to Appendix F. with positive definite matrix denote norm based on quadratic form wAw5. In Fig. 3, we experimentally confirmed that the norms of informative words become larger with Zipfian whitening (shown from center to the right in Fig. 3), bringing them closer to the ideal Zipfian prior model6. 5The matrix G(w) takes the form (cid:80) 6Given these results, some readers may be interested in the experimental outcomes for baseline where uniform whitening is applied, followed by rescaling norms based on information content through Zipfian whitening. For these results, please refer to Appendix G. is indeed positive definite, similar to covariance matrix. xix 8 Figure 3: Relationships between the information content 2 for top 500 frequent words w. The figure in the center represents the pre-trained GloVe model. By using Zipfian whitening, the information content gets encoded in the norm (center to right). Conversely, with uniform whitening, this phenomenon does not occur (center to left). log p(w) and the vector norms From the perspective of error and loss The error and loss functions associated with the Zipfian prior model emphasize low-frequency words. In contrast, the error and loss functions of the uniform prior model focus on the average loss across the entire dataset, resulting in greater emphasis on high-frequency words. { (w, c) } The standard classification loss is the softmax cross-entropy loss (14). By taking its expectation over the dataset , embeddings associated with higher-frequency words receive more updates because the softmax is the uniform inverse link, corresponding to the uniform prior model. By contrast, the logit-adjusted loss (15) has been proposed to tackle class imbalance [36]. From our viewpoint, the logit adjustment term p(w) makes the inverse link belong to the Zipfian prior model. The softmax and logit-adjusted losses are Fisher consistent to the misclassification (16) and balanced (17) error rates, respectively. As the latter tends to stress minor classes, the logit-adjusted loss and Zipfian prior model are suitable for emphasizing low-frequency words during the learning process. Another prominent loss function for representation learning is contrastive loss, with the SGNS loss (word2vec) [37] as representative example in the context of word representation learning. This loss similarly uses loss aligned with the Zipfian prior: (cid:34) (w,c) log σ( w, ) + i=1,...,k (cid:88) log σ( i p(w) (cid:35) , ) (21) i, where σ is sigmoid function, and is the number of negative samples. Since high-frequency words are more likely to be sampled as negative examples, the loss has less impact on high-frequency words in positive examples. Consequently, low-frequency positive words are relatively emphasized in representation learning. The LevyGoldberg formula in the previous section describes the properties of an ideally trained word2vec model, which are essentially the properties of Zipfian prior models.

5 Unified explanation of the efficacy of existing methods
=========================================================

Distinguishing the distribution that the base measure follows helps us understand why some existing NLP methods are effective. 5.1 Uniform whitening of token embeddings Zipfian whitening of type embeddings Masked language models like BERT [18] and RoBERTa [33] produce dynamic (contextualized) token embeddings. Adding up such token embeddings of constituent tokens to create sentence embeddings often leads to poor empirical performance [46]. However, symmetrizing significantly improves their performance; such methods including batch centering, WhiteningBERT, and contrastive learning methods [16, 46, 59, 22, 26, 57]. This improvement can also be explained from the perspective of the Zipfian prior. dataset or corpus is first fed into the model to obtain token embeddings7. Centering/whitening is then applied to this entire set of embeddings. As this token embedding (multi)set has the multiplicity asymptotically proportional to the word frequency, 7Here, the computation of the additive composition := 1/s (cid:80) ws can be ignored without major issues in formal discussions of spatial symmetry. This is because the words in sentence are generated based on word 9 Table 5: The empirical performance difference between uniformenforced centering and whitening with uniform prior for dynamic embeddings, and Zipfianconventional uniform centering and whitening over tokens with an implicit Zipfian prior over types. Each cell shows the STS-B [15] 100. This comparison reveals that token-level uniform centering/whitening, corresponding to score type-level Zipfian centering/whitening, leads to empirically better performance. BERT-base 63.75 RoBERTa-base 60.75 + Centering + Whitening Uniform 64.04 60.53 Zipfian 64.82 64.91 + Centering + Whitening Uniform 60.34 61. Zipfian 61.30 65.59 this uniform centering/whitening of token embeddings corresponds to the word-frequency-weighted (Zipfian) centering/whitening of type embeddings. For more formal description of the above explanations, please refer to Appendix H. Additionally, recent work has found that contrastive additive sentence encoders implicitly weight words by their information content [30]. This finding is consistent with the previous discussion on vector norms ( 4.2), and can be seen as indirect evidence supporting the idea that these models belong to the Zipfian prior family. This idea can also be supported by empirical evidence. This idea is also supported by empirical evidence. To establish baseline for centering and whitening token embeddings under uniform prior, we scale each embedding by the reciprocal of its type frequency, ensuring uniform treatment across types. Refer to the Appendix for the detailed computation of this pseudo uniform approach and formal explanation of how it achieves type uniformity. Table 5 shows the results. Comparing the pseudo-uniform centering/whitening (which assumes uniform prior over types) with the conventional token-level uniform centering/whitening (which implicitly assumes Zipfian prior over types) reveals that the latter approach based on Zipfian prior empirically achieves better performance. Additional experimental settings and results can be found in Appendix and Appendix I. 5.2 Headless causal language model roughly belongs to Zipfian prior family The recently proposed headless language model [23] uses only words within the same batch to predict next tokens with pseudo-softmax function. This method originally aimed to reduce the direction, but an interesting side effect is the computational cost of the softmax function in the improvement in the performance. This success can also be explained from the perspective of Zipfian priors. If we repeatedly sample small batches, the sampling frequency of each word will increasingly reflect its true frequency as the batch size approaches 1. V

6 Conclusion
============

Standard methods for adjusting and measuring symmetries in word embedding spacessuch as centering and whiteningimplicitly assume uniformly distributed word frequencies, which is unrealistic. We hypothesize that, based on the type-token distinction, using empirical Zipfian word frequencies is essential when calculating the expectation ( 2). Based on the idea and the definitions of firstand second-order symmetry in random vectors, we derived Zipfian whitening, which enhances the symmetry of the word embedding space. Even though it is nearly identical to standard PCA whitening, Zipfian whitening significantly outperforms existing methods ( 3.2). Similarly, we derived metric to evaluate the symmetry of word embedding spaces. Our intrinsic metrics showed strong correlation with extrinsic task performance, even when popular metrics show almost none ( 3.3). We then presented framework explaining the differences in effect between whitening based on uniform and Zipfian approaches, by attributing them to differences in the base measure of the exponential family ( 4.1). By further exploring this viewpoint through information geometry and loss functions, we showed how the Zipfian approach emphasizes the informativeness of low-frequency words ( 4.2). Lastly, through our proposed viewpoint, we found that popular NLP methods perform well because their word embeddings end up encoding Zipfian prior; such models include word2vec [37] (Fig. 4.2), WhiteningBERT [26] ( 5.1), and headless language models [23] ( 5.2). frequency distribution, resulting in the first and second moments (Def. 1, Def. 2) of sentence vectors closely matching those of word vectors.

Acknowledgements
================

This work is supported by JST ACT-X Grant Number JPMJAX200S and JSPS KAKENHI Grant Number 22H05106. We received numerous constructive and valuable comments from the anonymous reviewers of NeurIPS 2024, which have significantly contributed to the quality improvements from the submission version to the camera-ready version. We would also like to thank Hayato Tsukagoshi of Nagoya University for his insightful comments on the handling of dynamic embeddings and on the experimental setup of the SimCSE paper [22], including minor discrepancies between the papers description and its actual implementation. We also extend our gratitude to the organizers and participants of MLSS 2024, the Tohoku NLP group, the Shimodaira lab at Kyoto University, and many members of the Japanese NLP and machine learning community, for their constructive feedback and motivating encouragement throughout our research discussions.

Limitations
===========

How these assumptions might be violated in practice In our theoretical analysis concerning norms, and in the discussion on the relationship between whitening and normalization constants, we have proceeded by ignoring the residual terms beyond the second order. Empirically, focusing only on the first and second order has yielded significant results. However, to accurately identify cases where the proposed method might fail, detailed theoretical and empirical examination of the asymptotic behavior of higher-order moments might be crucial. This remains an important future work. The condition that the partition function is constant is only necessary condition from the perspective of both the generative models optimal solution and whitening. The true logical relationship between whitening and the generative model has not been clarified. In particular, verifying whether the projection through whitening allows us to transition between the two model families (the uniform family and the Zipfian family) is an intriguing and valuable direction for both theoretical exploration and practical application. The scope of the empirical claims made Our experiments primarily focused on static and dynamic word embeddings, as many of their theoretical properties have been understood and they have been central to the rise of isotropization. Admittedly, this paper also advances our understanding of causal language models. However, to make more significant practical impact in the era of large language models, employing the proposed method as regularization term for next-token prediction holds great promise for future work. The experiments utilized typical downstream NLP tasks, particularly popular datasets for sentencelevel semantic tasks. By scaling up the task set to include word-level tasks or leveraging broader range of multilingual data, we can more robustly demonstrate the practical utility of the proposed framework. The factors that influence the performance of our approach The proposed method inherently involves numerically unstable calculations, such as multiplying by the inverse of small singular values. Embeddings for low-frequency words are often far from converged even after extensive pre-training, and the eigenvalues of the embedding space are known to decay. Given these situations, the adverse effects of small singular values are plausible. Considering recent advancements in whitening techniques, developing more numerically stable algorithm is an important direction for future work.

Broader Impacts
===============

Potential impacts to AI alignment Dohmatob et al. [19] reported that repeated sampling from generative AIs may shift word frequency distributions toward lighter-tailed distributions. This may reduce linguistic diversity and lead to cultural homogenization by diminishing region-specific or 11 culturally unique expressions. Our Zipfian whitening and similar regularization methods could enhance output diversity, enriching the linguistic landscape. Potential negative societal impacts The sentence similarity tasks used in our evaluation experiments are now considered core technologies for RAG (retrieval-augmented generation), which is essential when large language models leverage external resources. If chatbots generate responses tailored to user ideologies or preferred information sources, it may result in negative societal impacts, including political agitation.

References
==========

[1] E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre. SemEval-2012 Task 6: Pilot on Semantic Textual Similarity. In E. Agirre, J. Bos, M. Diab, S. Manandhar, Y. Marton, and D. Yuret, editors, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385393, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1051. [2] E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. *SEM 2013 shared task: Semantic Textual Similarity. In M. Diab, T. Baldwin, and M. Baroni, editors, Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 3243, Atlanta, Georgia, USA, June 2013. Association for Computational Linguistics. URL https://aclanthology.org/S13-1004. [3] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, R. Mihalcea, G. Rigau, and J. Wiebe. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In P. Nakov and T. Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 8191, Dublin, Ireland, Aug. 2014. Association for Computational Linguistics. doi: 10.3115/v1/S14-2010. URL https://aclanthology.org/S14-2010. [4] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Maritxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In P. Nakov, T. Zesch, D. Cer, and D. Jurgens, editors, Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252263, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045. URL https://aclanthology.org/S15-2045. [5] E. Agirre, C. Banea, D. Cer, M. Diab, A. Gonzalez-Agirre, R. Mihalcea, G. Rigau, and J. Wiebe. SemEval2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In S. Bethard, M. Carpuat, D. Cer, D. Jurgens, P. Nakov, and T. Zesch, editors, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 497511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL https://aclanthology. org/S16-1081. [6] S. Arora, Y. Li, Y. Liang, T. Ma, and A. Risteski. Latent Variable Model Approach to PMI-based Word Embeddings. TACL, 4:385399, 12 2016. ISSN 2307-387X. doi: 10.1162/tacl{_}a{_}00106. URL https://aclweb.org/anthology/papers/Q/Q16/Q16-1028/. [7] S. Arora, Y. Liang, and T. Ma. Simple but Tough-to-Beat Baseline for Sentence Embeddings. In ICLR, 2017. URL https://openreview.net/forum?id=SyK00v5xx. [8] A. Bakarov. survey of word embeddings evaluation methods. arXiv [cs.CL], Jan. 2018. [9] S. Bird and E. Loper. NLTK: The Natural Language Toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214217, 7 2004. URL https://www.aclweb.org/anthology/ P04-3031. [10] D. Bis, M. Podkorytov, and X. Liu. Too much in common: Shifting of embeddings in transformer language models and its implications. pages 51175130, June 2021. [11] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching Word Vectors with Subword Information. TACL, 5:135146, 2017. doi: 10.1162/tacl_a_00051. URL https://www.aclweb.org/anthology/ papers/Q/Q17/Q17-1010/. [12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language Models are Few-Shot Learners. In NeurIPS, volume 33, pages 18771901, 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. 12 [13] E. Bruni, G. Boleda, M. Baroni, and N.-K. Tran. Distributional Semantics in Technicolor. In ACL, pages 136145, 7 2012. URL https://www.aclweb.org/anthology/P12-1015. [14] L. L. Campbell. Minimum coefficient rate for stationary random processes. Information and Control, 3(4): 360371, Dec. 1960. [15] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. In SemEval, pages 114, 8 2017. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/papers/S/S17/S17-2001/. [16] X. Chen, N. Ding, T. Levinboim, and R. Soricut. Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance. In First Workshop on Evaluation and Comparison of NLP Systems, pages 5159, Online, 11 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.eval4nlp-1.6. URL https://www.aclweb.org/anthology/2020.eval4nlp-1.6. [17] R. R. Coifman and M. V. Wickerhauser. Entropy-based algorithms for best basis selection. IEEE Trans. Inf. Theory, 38(2):713718, Mar. 1992. [18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL, pages 41714186, 2019. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423/. [19] E. Dohmatob, Y. Feng, P. Yang, F. Charton, and J. Kempe. tale of tails: Model collapse as change of scaling laws. arXiv [cs.LG], Feb. 2024. [20] K. Ethayarajh. Unsupervised Random Walk Sentence Embeddings: Strong but Simple Baseline. In Rep4NLP, pages 91100, 7 2018. URL https://aclweb.org/anthology/papers/W/W18/ W18-3012/. [21] K. Ethayarajh. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. In EMNLP, pages 5565, 2019. URL http://arxiv.org/abs/ 1909.00512. [22] T. Gao, X. Yao, and D. Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. [23] N. Godey, É. V. de La Clergerie, and B. Sagot. Headless language models: Learning without predicting with contrastive weight tying. arXiv [cs.CL], Sept. 2023. [24] C. Gong, D. He, X. Tan, T. Qin, L. Wang, and T.-Y. Liu. FRAGE: Frequency-agnostic word representation. Adv. Neural Inf. Process. Syst., 31, 2018. [25] P. He, X. Liu, J. Gao, and W. Chen. {DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentangled} {attention}. In ICLR, 2021. URL https://openreview.net/forum?id=XPZIaotutsD. [26] J. Huang, D. Tang, W. Zhong, S. Lu, L. Shou, M. Gong, D. Jiang, and N. Duan. WhiteningBERT: An easy unsupervised sentence embedding approach. In M.-F. Moens, X. Huang, L. Specia, and S. W.-T. Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 238244, Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. [27] R. Kannan, L. Lovász, and M. Simonovits. Random walks and an o*(n5) volume algorithm for convex bodies. Random Struct. Algorithms, 11(1):150, Aug. 1997. [28] G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Transformer language models handle word frequency in prediction head. pages 45234535, July 2023. [29] K. Kurihara, D. Kawahara, and T. Shibata. JGLUE: Japanese general language understanding evaluation. In N. Calzolari, F. Béchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, J. Odijk, and S. Piperidis, editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 29572966, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.317. [30] H. Kurita, G. Kobayashi, S. Yokoi, and K. Inui. Contrastive learning-based sentence encoders implicitly weight informative words. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1093210947, Singapore, Dec. 2023. Association for Computational Linguistics. [31] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, Eytan Ruppin. Placing search in context: the concept revisited. ACM Trans. Inf. Syst. Secur., 20(1):116131, Jan. 2002. [32] O. Levy and Y. Goldberg. Implicit Matrix FactorizaIn Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weineditors, NIPS, pages 21772185, 2014. URL http://papers.nips.cc/paper/ tion. berger, 5477-neural-word-embedding-as-implicit-matrix-factorization. Neural Word Embedding as 13 [33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: robustly optimized BERT pretraining approach. arXiv [cs.CL], July 2019. [34] H. P. Luhn. The automatic creation of literature abstracts. IBM J. Res. Dev., 2(2):159165, Apr. 1958. [35] M. Marelli, S. Menini, M. Baroni, L. Bentivogli, R. Bernardi, and R. Zamparelli. SICK cure for the evaluation of compositional distributional semantic models. In LREC, pages 216223, Reykjavik, Iceland, 5 2014. URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf. [36] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar. Long-tail learning via logit adjustment. Sept. 2020. [37] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 31113119, 2013. URL http://papers.nips.cc/paper/ 5021-distributed-representations-of-words-and-phrases-and-their-compositionality. [38] D. Mimno and L. Thompson. The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 28732878, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics. [39] J. Mu and P. Viswanath. All-but-the-Top: Simple and Effective Postprocessing for Word Representations. In ICLR, 2018. URL https://openreview.net/forum?id=HkuGJ3kCb. [40] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. MTEB: Massive text embedding benchmark. In EACL, pages 20142037, Dubrovnik, Croatia, 5 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.148. URL https://aclanthology.org/2023.eacl-main.148. [41] K. Oono and T. Suzuki. Graph neural networks exponentially lose expressive power for node classification. Sept. 2019. [42] M. Oyama, S. Yokoi, and H. Shimodaira. Norm of word embedding encodes information gain. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 21082130, Singapore, Dec. 2023. Association for Computational Linguistics. [43] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global Vectors for Word Representation. In EMNLP, pages 15321543, 2014. doi: 10.3115/v1/D14-1162. URL https://aclweb.org/anthology/papers/ D/D14/D14-1162/. [44] I. Provilkov, D. Emelianenko, and E. Voita. BPE-dropout: Simple and effective subword regularization. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 18821892, Online, July 2020. Association for Computational Linguistics. [45] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language Models are Unsupervised Multitask Learners. Technical report, 2019. URL https://d4mucfpksywv.cloudfront.net/ better-language-models/language_models_are_unsupervised_multitask_learners.pdf. [46] N. Reimers and I. Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In EMNLP, pages 39803990, 11 2019. doi: 10.18653/v1/D19-1410. URL https://www.aclweb.org/ anthology/D19-1410. [47] O. Roy and M. Vetterli. The effective rank: measure of effective dimensionality. pages 606610, Sept. 2007. [48] M. Rudelson. Random Vectors in the Isotropic Position. Journal of Functional Analysis, 164(1):6072, 1999. ISSN 0022-1236. doi: https://doi.org/10.1006/jfan.1998.3384. URL http://www.sciencedirect. com/science/article/pii/S0022123698933845. [49] W. Rudman, N. Gillman, T. Rayne, and C. Eickhoff. IsoScore: Measuring the uniformity of embedding space utilization. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 33253339, Dublin, Ireland, May 2022. Association for Computational Linguistics. [50] A. M. J. Schakel and B. J. Wilson. Measuring word significance using distributed representations of words. arXiv [cs.CL], Aug. 2015. [51] H. Shi, J. Gao, H. Xu, X. Liang, Z. Li, L. Kong, S. M. S. Lee, and J. Kwok. Revisiting over-smoothing in BERT from the perspective of graph. Sept. 2021. [52] H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227244, 2000. [53] K. Sparck Jones. statistical interpretation of term specificity and its application in retrieval. In Document retrieval systems, pages 132142. Taylor Graham Publishing, GBR, Dec. 1988. 14 [54] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971v1, 2023. URL https://arxiv.org/abs/2302.13971v1. [55] R. Vershynin. High-Dimensional Probability. Cambridge University Press, 2018. [56] L. Wang, J. Huang, K. Huang, Z. Hu, G. Wang, and Q. Gu. Improving Neural Language Generation with Spectrum Control. In ICLR, 2020. URL https://openreview.net/forum?id=ByxY8CNtvr. [57] L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv:2212.03533v2, 2024. URL https://arxiv.org/ abs/2212.03533v2. [58] L. Wetzel. Types and tokens. The Stanford Encyclopedia of Philosophy (Fall 2018 Edition), Apr. 2006. [59] Y. Yan, R. Li, S. Wang, F. Zhang, W. Wu, and W. Xu. ConSERT: contrastive framework for selfsupervised sentence representation transfer. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, ACL-IJCNLP, pages 50655075, Online, 8 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-long.393. URL https://aclanthology.org/2021.acl-long.393. [60] S. Yokoi, R. Takahashi, R. Akama, J. Suzuki, and K. Inui. Word Rotators Distance. In EMNLP, pages 2944 2960, Online, 11 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.236. URL https://www.aclweb.org/anthology/2020.emnlp-main.236.

A Explanation fo the Zipfian whitened word vectors will have a zero mean
========================================================================

and be in an isotropic position in terms of expectation The second step of PCA whitening involves decorrelating the dimensions and then normalizing them, which is achieved by transforming the centered random vector as follows: (cid:101)w := E[w w]1/2w. (22) Actually, E[ (cid:101)w (cid:101)w] = holds; (cid:101)w satisfies Def. 2. Computationally, the estimation of E[w w]1/2 can be performed efficiently via singular value decomposition (SVD) of the centered data matrix := [w ]. Note again that, this standard method assumes that the frequency of each word (i.e., each row) is uniform, which presents the issues discussed in 2. To account for word frequency, SVD should be performed on the matrix (cid:113) 1 , . . . , (cid:105) := (cid:104)(cid:112)p(w1)w 1 , . . . , p(wV)w (23) RVd, where each word frequency is multiplied by its square root. In fact, for wp serves as an estimator [w w]. This can be confirmed by comparing the (j, k)th elements of each matrix (or matrix-valued random variable): wp [w w][j, k] = wp (cid:88) p)[j, k] = (W [w[j] w[k]], p(wi)wi[j] wi[k], (24) (25) where A[j, k] denotes the (j, k)th element of A, and v[j] denotes the jth element of v. Finally, the estimator for the desired E[w w]1/2 can be computed as = ΣV (via SVD) [w w]1/2 = (W (26) p)1/2 = ((U ΣV )(U ΣV ))1/2 = (V Σ2V )1/2 = Σ1. (27) (cid:98)E wp Table 6 shows the correspondence between uniform (normal) whitening and Zipfian whitening. This may be useful for readers familiar with matrix notation.

B Experimental settings
=======================

To ensure the reproducibility of the experiments conducted in this paper, we provide detailed configurations below. Additionally, the source code has been made publicly available at https: //github.com/cl-tohoku/zipfian-whitening. B.1 Word embeddings For static embeddings, we used the most standard ones, 300-dim GloVe[43] model trained on Common Crawl8, 300-dim word2vec[37] model trained on Google News9, and 300-dim fastText[11] subword/non-subword models trained on Common Crawl10. For the multilingual experiment, we used fastText-ja [11], fastText model trained on Japanese Wikipedia and Common Crawl 11. 8https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B. 300d from Sentence-Transformers implementation [46]. 9https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/ from https: //code.google.com/archive/p/word2vec/ 10https://fasttext.cc/docs/en/english-vectors.html 11https://fasttext.cc/docs/en/crawl-vectors.html 16 Table 6: The correspondence between uniform (normal) whitening and Zipfian whitening. data matrix, original vectors probability = Uniform whitening for general table data ... xi ... Uniform dist. 1 p(xi) = mean vector (cid:88) (cid:98)µx = 1 xi centered matrix, centered vectors = ... xi = xi (cid:98)µx ... centered matrix to create cov. mat. = (cid:113) ... 1/(n 1) xi ... covariance matrix = (cid:98)S = X 1 SVD of centered matrix = ΣV = eigendecomposition of covariance matrix whitened matrix whitened vector (cid:19) , . . . (cid:19) Σ = diag(σ1, . . . , σd) 1 (cid:98)S = Λ = diag(λ1, . . . , λd) (cid:18) σ2 = ΛV = diag := Σ2 1 1 1 (cid:18) 1 λ1 , . . . (cid:33) Λ1/2 := diag (cid:32)(cid:115) = diag 1 σ2 1 (cid:102)X = XV Λ1/2 = ... (cid:101)xi = xiV Λ1/2 ... Zipfian whitening for word embeddings ... wi ... Power-law dist. = p(wi) = p(wi) (word freq.) (cid:98)µw = (cid:88) p(wi)wi = ... wi = wi (cid:98)µw ... = (cid:113) ... p(wi) wi ... Rnd Rd Rnd Rnd (cid:98)S = Rdd = ΣV Σ = diag(σ1, . . . , σd) = ΛV (cid:98)S = Λ = diag(λ1, . . . , λd) 1, . . . (cid:1) := Σ2 = diag (cid:0)σ Λ1/2 := diag (cid:19) , . . . (cid:18) 1 λ1 (cid:19) (cid:18) 1 σ (cid:102)W = Λ1/2 = ... (cid:101)wi = wiV Λ1/2 ... , . . . = diag , . . . For dynamic embeddings, we used three most standard masked language models, BERT[18]12, RoBERTa[33]13, and DeBERTa [25]14. All three models are base size. To aggregate the dynamic word embeddings to create sentence embeddings, we follow the first-last average pooling from the prior work [22]. In this setting, we first average the hidden states of first and last dynamic layer of 12https://huggingface.co/google-bert/bert-base-uncased 13https://huggingface.co/FacebookAI/roberta-base 14https://huggingface.co/microsoft/deberta-base the model to get the averaged token embeddings15, then average the token embeddings to get final sentence embeddings16. B.2 Empirical word frequency and vocabulary As the empirical word probability p(w) of English words, we used the enwiki dataset preprocessed by Arora et al. [7]17. For the Japanese word probability, we used Japanese Wikipedia word frequency from Wiktionary, denoted as jawiki 18. Furthermore, we also used the frequency of words in the evaluation data itself (test set probability). The word frequency in the test set is implicitly utilized in [7]s sentence embedding method and is also natural approach in the context of covariate shift [52]. As vocabulary word embedding models vocabulary across all settings, including baseline methods. , we used the overlapping entries between the word frequency list and the pre-trained B.3 Baseline methods As baselines for post-processing of word vectors, we used ABTT (all-but-the-top) [39], which established the trend of post-processing word vectors; and the strong baseline method by [7], the combination of SIF (smoothed inverse frequency) weighting and CCR (common component removal)19. We followed the hyperparameter choices of the original papers, with the dimensionality reduction parameter for ABTT set to := 3, and the weighting parameter for SIF set to := 103. B.4 Extrinsic tasks As downstream tasks, we used the most commonly utilized ones in the community, STS12-16 [1, 2, 3, 4, 5], STS-B [15] and SICK-R [35]. For the multilingual experiments, we used JSTS (Japanese version of the STS) from JGLUE benchmark [29]. They are sentence-level similarity tasks and are standard for empirically evaluating the performance of word vectors20 21. These datasets consist of pairs of sentences and their semantic similarity rated by annotators. We first tokenized the dataset 15Note that, we apply centering/whitening operations to such token embeddings, not to the final sentence embeddings, in order to match the setting in the theoretical analysis and the static word embedding experiments. 16Though we followed the experimental setting from the prior work [22], there is slight discrepancy in the experimental results of the baseline setting. We found that this was due to prior work inadvertently taking the average of the hidden states of the zero-th layer (i.e., static word embedding layer) and the final dynamic layer. See the discussion at https://github.com/princeton-nlp/SimCSE/issues/285 for more details. 17https://github.com/PrincetonML/SIF/raw/master/auxiliary_data/enwiki_vocab_min200. txt 18https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Japanese2015_10000 and https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Japanese2015_10001-20000 19CCR is process applied to sentence vectors, but due to its linearity, it can be adapted to word vectors. For more details, please refer to Yokoi et al. [60]. 20For those outside the field of NLP research or practice, the question, Why not run word-level evaluation metrics? is natural and valid one. Our language has property known as compositionality, allowing infinite semantic content to be conveyed through finite vocabulary as building blocks. This principle underlies models like word2vec [37], BERT [18], and the GPT series [12], where the fundamental unit of representation is the word; these models are then applied to solve tasks with larger components, such as sentences. Our research adheres to this foundational principle of NLP. Also, existing word-level similarity datasets have significant issues that make them less suitable for our work (see Bakarov [8, Section 4.1.1]). Given that whitening reflects word information content in vector norms, classic tasks like keyword extractionwhich selects words with high information contentcould be good candidates; results from prior study using methods similar to ours would also be informative [42, Section 7.1]. 21Setting aside the criticisms from previous studies for now, we conducted an evaluation using the two most well-known lexical similarity datasets. Table 7 shows the results. We found that the process of raw Zipfian centering Zipfian whitening consistently improves lexical properties. However, note that the finding direction: uniform whitening > direction: Zipfian whitening contradicts the experimental results in Appendix G, which showed direction: uniform whitening, norm: Zipfian whitening < direction: Zipfian whitening, norm: Zipfian whitening. Here, lexical similarity tasks rely solely on vector direction and do not reference vector norms, as only the cosine similarity between word vectors is used to predict similarity. This discrepancy likely arises because these datasets are not representative of natural language, as discussed in Bakarov [8, Section 4.1.1]. For example, the widely used dataset WordSim353 [31] includes only about 200 subjective ratings on 18 by NLTK [9] with some post-processing following [20]22, then lowercased all tokens. The typical experimental protocol we followed is to sum the word vectors to form sentence vector and then check if the angles (cosine similarity) between them correlate well with the gold scores. We reported Spearmans rank correlation between the predictions (cosine scores) and human-annotated gold scores23. B.5 Computational resources for experiments We conducted all experiments using single NVIDIA RTX 6000 Ada GPU with 48GB VRAM. Each STS task required 10 seconds per model and whitening method, totaling approximately 10 minutes for the entire experiment, excluding the embedding loading time to the GPU. For the calculation of the symmetry scores, each setting took one minute, resulting in total of 5 minutes, again excluding the embedding loading time and the average cosine similarity (Ave. Cos.) setting. The Ave. Cos. score computation took 10 minutes per model, totaling 20 minutes for the two models.

Zipfian whitening
=================

In 3.2, we evaluated the empirical performance of Zipfian whitening on the STS-B dataset. In this section, we present experimental results using more comprehensive datasets. Detailed experimental settings can be found in Appendix B. Table 8, Table 9 and Table 10 show the results. Across all datasets, the method incorporating Zipfian prior consistently outperforms the method employing uniform prior. Proof of Prop. 1 Proof. We will show the following. E[(v E[v])(v E[v])] λ1 = λ2 = = λd Sym2(v) = 1 log (cid:32) λ1 λj (cid:80) , . . . , λd λj (cid:80) (cid:33) = 1 1 (28) (29) (30) ( 1 ) When E[(v = 1 = ) Since E[(v ( ΛU using diagonal matrix with eigenvalues Λ = diag(λ1, . . . , λd) E[v])] = kI holds, its eigenvalues are E[v])] is symmetric positive definite, it can be represented as Rdd and an orthogonal E[v])(v E[v])(v k, . . . , { } . common nouns, such as (tiger, cat, 7.35) or (king, cabbage, 0.23), which may or may not co-occur in the same document. Table 7: Each cell shows the correlation coefficients 100 between the cosine similarity of (corrected) GloVe embeddings and the human-annotated gold score on lexical similarity tasks. WordSim353 [31] MEN [13] GloVe 78.70 GloVe + Centering + Whitening Uniform 75.39 82.31 Zipfian 79.66 80.90 + Centering + Whitening 80.49 Uniform 78.07 84. Zipfian 80.55 83.97 22https://github.com/kawine/usif/blob/71ffef5b6d7295c36354136bfc6728a10bd25d32/ usif.py#L113-L126 23We used the MTEB [40] implementation: https://github.com/embeddings-benchmark/mteb, for the evaluation of the static word embeddings in Table 2,Table 8, and Table 9. For the evaluation of the dynamic word embeddings in Table 5 and Table 12, we used the implementation in SimCSE paper [22]: https://github.com/princeton-nlp/SimCSE, to match the experimental setting. 19 matrix . Now we have λ1 = λ2 = . . . λd =: k, then E[(v kI dU = kI d. E[v])(v E[v])] = ΛU = 2 ) The Shannon entropy H(p) of random variable taking possible values attains its ( maximum value log if and only if follows uniform distribution.

E Pseudocode for the evaluation metrics of symmetry
===================================================

See Algorithm 2 to measure the degree of symmetry of word embeddings. Algorithm 2 Measure the degree of symmetry of word embeddings Input: Word embeddings wi { Output: Degree of centrality (the 1st moment) (cid:100)Sym1( { , word frequency : } [0, 1]. wi (cid:100)Sym2( { Measure the degree of centrality (the 1st moment of symmetry): , p). } wi , p) and isotropy (the 2nd moment) } 1: (cid:98)µ 2: ℓ (cid:88) wiV (cid:88) wiV wi p(wi)wi Rd p(wi) wi (cid:98)µ /ℓ 3: (cid:100)Sym1( { , p) } 1 4: 5: ΣV SVD(W) Σ = diag(σ1, . . . , σd) 1, . . . , σ2 (σ2 d) 1 (cid:88) log , p) } wi 6: (λ1, . . . , λd) 7: (cid:100)Sym2( { Measure the degree of isotropy (the 2nd moment of symmetry): (cid:20) (cid:112)p(w1)(w1 (cid:98)µ), . . . , (cid:112)p(wV)(wV (cid:98)µ) (cid:21) Rdd consists the singular values of . λi λi (cid:80) log λi λi (cid:80) Proof of Thm. 1 Proof. By the assumption, the word and context vectors for the same word , w(t) and c(t), are obtained through the linear embedding layer with weight tying, namely, w(t) = 1t and RV is the one-hot vector RdV is the embedding matrix and 1t c(t) = 1t, where w), we need to begin with the indicating the token t. To derive the KL divergence for the model p(c w) belongs to an exponential family. generative model p(w c) (7) and confirm that p(c w) = p(c p(w c)p(c) p(w) = w, exp( (c) ) p(c) = w, 1c p(c) exp( ) , where we used the constancy of the partition function ( p(c is given as follows: w) is an exponential family parametrized by w(:= θ ) from (9) at the last identity. Hence, RV), and its log-partition function ψcw(θ) = log (cid:18)(cid:88) cV p(c) exp( θ, 1c ) (cid:19) . The second-order expansion of the KL divergence can be derived based on the second moment of w), which is given by the Hessian of ψcw in the case of exponential families. First, let us derive p(c 20 the first moment. ψcw θ = (cid:80) (cid:80) )1c p(c) p(c) θ, 1c exp( θ, 1c exp( ) = (cid:88) cV p(c) exp( θ, 1c ) 1c = (cid:88) cV p(c w)1c = w) . p(c ... ... Then, the second moment is derived. (cid:26) θ p(c) (cid:88) cV ψcw θθ = =

1
Z z
(cid:88)
==============

cV w)1c1 = diag[. . . p(c p(c w) . . . ], exp( θ, 1c ) (cid:27) 1 =

1
Z z
=====

(cid:88) cV θ, 1c p(c) exp( )1c1 V diagonal matrix with p(c w) being the (c, c)-th diagonal entry. Now, we which is the are ready to derive the KL divergence. For two tokens w, , if we write θ := 1w and θ := 1w, the KL divergence of the exponential family can be expanded as the following quadratic form in their parameters θ and θ: 2KL(p( w) p( w)) (θ θ) w)(U 1c)(U 1c) p(c (cid:27) (w w) (cid:19) (cid:18) ψcw θθ (cid:18) ψcw θθ w)U (cid:26) (cid:88) w) cV (cid:26)(cid:88) p(c w) cV w)G(w)(w 2 G(w). ) = p( = (w = (w = (w = (w = (θ θ) (cid:19) (w w) (cid:27) w)cc (w w) w) We can consider word w0 such that p( w0), that is, an uninformative word w0 whose presence does not change the marginal distribution at all. Noting from Equation (22) of Oyama et al. [42] that := (cid:80) w0, we have wV p(w)w KL(p( p( ) w)) = KL(p( = w0 Assump. = p( w0) 2 G(w) 2 G(w) 2 G(w). w)) (31) (32) (33) (34)

G Experiments with a mix of uniform and Zipfian settings
========================================================

Based on the findings that Zipfian whitening positively impacts word vector norms ( 4.2), we present experimental results for baseline: first, uniform whitening is applied, followed by rescaling norms according to information content through Zipfian whitening. Table 11 presents the results, with the basic settings identical to those in Table 2, but uses Pearsons as the evaluation metric. Here, Uniform +α refers to the process of correcting word vectors using uniform prior, then replacing only the norm with that obtained from Zipfian whitening. We found that appropriately weighting by norm has critical effect on task performance. Notably, pure Zipfian centering/whitening performs even better, suggesting that Zipfian correction has two effects: (i) the norm becomes representative of information content ( 4.2), and (ii) vectors are more evenly dispersed (isotropic), resulting in appropriate positioning in terms of direction as well. 21 Formal Explanation of Uniform whitening of token embeddings Zipfian whitening of type embeddings In this section, we provide more formal explanation of Uniform whitening of token embeddings Zipfian whitening of type embeddings, as described in 5.1. For intuitive explanations and related discussions, please refer to 5.1. H.1 Uniform whitening of token embeddings Zipfian whitening of type embeddings Assume that, when the type word of token is w, the token embedding aligns with the shared type embedding w. Assumption 1. If type(t) = w, then = w. Note that this is rough approximation, as token embeddings are dynamic and vary with context. Under this assumption, the unweighted mean of token embeddings (cid:98)E is asymptotically equivalent to word-frequency-weighted (Zipfian) average of type embeddings [w], as [t] obtained from dataset : 1 (cid:88) tD Assump. 1 1 (cid:88) cD(w)w wV : cD(w) := # { wV : type(t) = . } (cid:88) p(w)w =: [w], (35) where cD denotes the count of type in (cid:98)E [t] := H.2 Pseudo-uniform whitening of token embeddings embeddings uniform whitening of type To establish baseline for centering/whitening token embeddings under uniform prior, we can apply 1/cD(type(t)) to each token embedding t, for removing type frequencies that are coefficient 1/VD implicitly referenced. Here, : : VD denotes the vocabulary contained in VD := { , type(t) = . The pseudo-uniform average (cid:98)E } equivalent to the uniform average of type embeddings (Assump. 1) that ignores the dynamic nature of token embeddings: (cid:101)u [t] calculated in this way is asymptotically [w], under the previous assumption (cid:98)E (cid:101)u [t] := (cid:88) 1 tD VD 1 cD(type(t)) Assump. 1 (cid:88) 1 wVD VD (cid:24)(cid:24)(cid:24) cD(w) (cid:24)(cid:24)(cid:24) cD(w) (cid:88) 1 wV =: [w]. (36)

I Experimental results on all benchmark datasets to evaluate the effects of
===========================================================================

uniform whitening on token embeddings In 5.1, we evaluated the empirical performance of uniform whitening of dynamic token embeddings on the STS-B dataset. In this section, we present experimental results using more comprehensive datasets. Table 12 shows the results. Across all datasets, the methods implicitly incorporating Zipfian prior consistently outperforms the method employing uniform prior. 22 Table 8: Full results of the empirical performance of Zipfian whitening. Each cell shows the STS 100. As empirical word frequency p(w), we used enwiki. Across all models and tasks, score Zipfian whitening outperforms powerful baseline methods. Method STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg. Averaging Uniform Zipfian ABTT SIF + CCR Averaging Uniform Zipfian ABTT SIF + CCR Averaging Uniform Zipfian ABTT SIF + CCR Averaging Uniform Zipfian ABTT SIF + CCR + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening 56.46 55.54 53.31 54.52 57.76 52.67 60.23 58.57 58.17 56.53 56.89 56.16 55.53 60.05 57.94 59.73 52.47 58.30 58.86 58.35 61. 49.10 49.21 45.12 48.68 61.22 49.64 57.28 GloVe 51.13 49.67 57.93 60.87 67.04 59.40 62.39 Word2Vec 63.65 62.19 62.77 65.08 67.20 63.13 66.87 fastText 62.37 55.16 53.90 64.57 68.43 60.82 68.39 58.60 56.03 68.68 69.82 76.80 69.53 67.26 71.73 70.15 72.42 73.91 76.60 72.25 74.32 72.26 64.22 65.33 74.10 78.07 71.99 76.98 50.41 46.32 62.45 69.20 72.22 67.38 68.78 68.64 67.34 66.95 69.95 70.33 69.32 73. 68.97 55.02 59.01 71.69 73.85 69.09 76.95 fastText-subword 47.34 43.13 41.00 55.03 60.68 41.79 54.50 51.94 49.89 47.30 54.07 63.18 48.81 60.77 61.99 62.03 62.08 60.23 73.59 60.84 68.82 49.03 46.90 58.69 62.61 71.72 60.71 61. 61.79 59.60 61.05 65.71 70.99 60.98 67.64 63.59 53.39 52.61 67.59 74.00 60.76 70.27 51.54 49.70 48.85 58.41 69.87 47.57 61.63 57.01 56.44 57.92 58.01 61.80 58.56 56.91 61.77 61.39 62.74 62.18 62.52 62.02 59.22 59.99 58.85 58.34 60.75 62.85 60.34 59. 53.60 54.56 54.80 54.64 59.82 55.09 56.83 46.17 45.17 52.21 52.25 66.92 54.28 58.70 56.98 55.85 56.03 58.84 66.50 56.98 63.04 59.82 52.46 48.60 59.40 69.55 57.02 67.08 50.43 46.91 43.55 50.38 68.20 44.23 60.36 52.69 50.87 58.74 61.04 67.75 60.36 62. 63.30 62.10 62.64 64.65 67.19 62.89 66.34 63.56 56.98 55.75 65.20 69.37 62.62 68.67 52.28 50.78 48.96 54.49 65.22 49.71 60.03 23 Table 9: Full results of the empirical performance of Zipfian whitening, test set frequency setting. 100. As empirical word frequency p(w), we used test set frequency. Each cell shows the STS score Across all models and tasks, Zipfian whitening outperforms powerful baseline methods. Besides, the test set frequency setting consistently outperforms the enwiki setting in Table 8, demonstrating that the models benefit from using task-specific statistics in line with covariate shift approach [52]. Method STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg. Averaging Uniform Zipfian ABTT Averaging Uniform Zipfian ABTT Averaging Uniform Zipfian ABTT Averaging Uniform Zipfian ABTT + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening + Centering + Whitening 57.71 56.32 51.67 50.69 61.63 52.93 59.00 57.88 58.45 55.02 59.37 56. 58.23 60.60 55.56 55.92 62.20 59.13 51.37 51.31 51.52 43.15 60.87 49.06 GloVe 50.61 52.68 57.14 61.59 69.48 60.10 Word2Vec 63.99 64.24 65.46 65.81 69.48 64. fastText 62.89 61.09 57.73 65.72 71.03 63.30 58.38 64.80 70.09 70.19 76.83 71.93 72.51 74.71 76.43 74.36 76.42 74.74 73.09 73.92 70.68 74.12 77.95 74.80 50.29 61.17 60.94 70.66 78.36 66. 68.92 70.34 69.42 71.47 76.92 70.42 69.36 69.51 63.51 73.36 79.35 71.00 fastText-subword 51.49 44.80 49.33 53.40 72.21 45.16 54.57 49.66 53.51 53.67 67.79 49.57 62.75 62.27 68.28 63.05 75.86 62. 48.76 55.80 63.08 68.25 74.08 63.12 62.25 65.57 67.78 69.52 73.56 65.19 64.25 64.49 62.40 72.18 76.28 65.96 52.97 47.43 58.34 59.09 73.88 50.75 56.76 57.98 55.14 60.03 60.11 58.23 61.87 62.47 62.87 62.92 60.07 62. 60.22 61.14 57.93 62.30 60.66 61.69 53.53 54.86 56.94 56.57 60.52 55.49 46.22 47.94 53.16 56.64 71.60 53.72 57.15 58.09 60.85 61.02 70.42 58.21 60.27 57.42 54.65 62.95 73.56 58.23 52.41 43.12 51.69 47.16 70.99 44. 52.67 56.67 58.74 62.58 70.30 60.99 63.67 64.76 65.89 65.73 69.46 64.59 64.04 64.02 60.35 66.65 71.58 64.87 54.16 50.49 55.66 53.73 68.87 50.96 Table 10: Evaluation results using Japanese fastText. Each cell shows the JSTS [29] score 100. Even in the multilingual setting, Zipfian whitening outperforms powerful baseline methods. (a) With jawiki as p(w) (b) With test set frequency as p(w) fastText-ja 55. fastText-ja 59.94 + Centering + Whitening + ABTT + SIF + CCR Uniform Zipfian 57.55 65.56 56.05 55. 57.14 61.03 + Centering + Whitening + ABTT Uniform Zipfian 63.05 69.86 59.89 61.75 63. Table 11: Each cell shows the STS-B [15] score 100. GloVe 43.65 + Centering + Whitening Uniform 41.27 53.22 Zipfian +α 53.66 64.83 Zipfian 55.15 70.22 24 Table 12: Full results of the whitening on dynamic embeddings. Each cell shows the STS score 100. Token-level uniform centering/whitening ("Zipfian" settings), which corresponds to centering/whitening at the word type level under Zipfian prior, consistently outperforms the "Uniform" setting across all STS tasks. Method STS12 STS13 STS14 STS15 STS16 SICK-R STS-B Avg. First-last avg. "Uniform" "Zipfian" + Centering + Whitening + Centering + Whitening First-last avg. "Uniform" "Zipfian" + Centering + Whitening + Centering + Whitening First-last avg. "Uniform" "Zipfian" + Centering + Whitening + Centering + Whitening 45.09 47.51 40.31 47.58 53.75 44.00 46.07 37.67 44.97 52.80 45.03 45.20 38.12 45.87 52.97 BERT-base uncased 64.30 64.53 56.11 66.26 74. 54.56 54.68 47.02 57.32 64.21 70.52 72.19 68.35 73.18 73.88 RoBERTa-base 59.02 55.50 54.64 61.19 73.39 49.31 46.27 47.71 53.73 64.18 66.63 66.06 66.31 69.57 72. DeBERTa-base 61.94 61.25 50.46 63.24 73.54 52.39 50.84 45.30 55.07 63.25 68.90 68.56 63.52 70.53 72.60 67.87 69.28 64.53 71.09 72.83 59.62 60.06 62.85 67.88 72. 64.83 63.87 62.29 68.88 71.97 59.05 59.77 48.59 63.27 69.71 57.56 51.33 50.13 58.60 71.07 56.54 53.18 46.99 58.50 69.79 63.75 64.04 60.53 64.82 64.91 60.75 60.34 61.31 61.30 65. 61.66 62.01 58.19 63.18 64.63 60.73 61.71 55.06 63.36 67.62 56.70 55.09 54.37 59.61 67.40 58.76 57.84 52.12 60.75 66.

