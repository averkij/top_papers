Title: Language Models can Self-Lengthen to Generate Long Texts

Authors: Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin


================================================================================

Abstract
========

Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. This limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. Current approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage. In this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. The framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in a new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. Experiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at https://github.com/QwenLM/Self-Lengthen.

Start
=====

4 2 0 2 1 3 ] . [ 1 3 3 9 3 2 . 0 1 4 2 : r LANGUAGE MODELS CAN SELF-LENGTHEN TO GENERATE LONG TEXTS Shanghaoran Quan, Tianyi Tang , Bowen Yu, An Yang, Dayiheng Liu , Bofei Gao Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin Qwen Team, Alibaba Inc. {quanshanghaoran,yubowen.ybw,ya235025}@alibaba-inc.com

ABSTRACT
========

Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet notable gap remains in generating long, aligned outputs. This limitation stems from training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. Current approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage. In this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. The framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. Experiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at https://github.com/QwenLM/Self-Lengthen.

INTRODUCTION
============

Recently we have witnessed significant breakthroughs in Large Language Models (LLMs), accompanied by extensive research aimed at improving their alignment with human demands (Cao et al., 2024; Gao et al., 2024; Quan, 2024a;b). One key research area is enhancing LLMs abilities to manage increasingly long contexts (Han et al., 2023; Pawar et al., 2024; Bai et al., 2023). While much of the existing research on long context LLM alignment emphasizes processing lengthy inputs, and several open-source models like Qwen2 (Yang et al., 2024) and LLaMA3 (Dubey et al., 2024) herds of models already demonstrate impressive capabilities in long context understanding tasks, such as Needle in Haystack (Kamradt, 2023), notable gap remains in the ability of LLMs to generate long aligned outputs effectively, as illustrated in Figure 1, and this issue cannot be easily addressed by manipulating decoding strategies. We analyze that this is due to training gap: during the pre-training stage, despite having access to vast array of long text sources, there is lack of effective instructions to cultivate this capability. Conversely, in the post-training stage, the majority of human-conducted or AI-augmented query-response pairs are short, which leads to the trained LLMs facing challenges in generating long, aligned outputs. To address this issue, current efforts employ two strategies: instruction backtranslation (Li et al., 2023) and behavior imitation (Xu et al., 2024), to construct post-training data with long responses. Instruction backtranslation leverages existing long-form, high-quality human-written texts, such as those found in magazines or books, as responses, and uses them to generate corresponding queries. However, obtaining high-quality data that spans various long-generation tasks and domains is challenging, and much of the available data presents risks of copyright infringement (Maini et al., 2024). Work done during internships at Qwen team, Alibaba Inc. Co-corresponding authors 1 Figure 1: Existing LLMs are struggling to generate long, aligned outputs. (Left) The actual output lengths when prompting LLMs for generation tasks with specific length constraints under default parameters. All four tested LLMs struggle to exceed 2,000 words. (Right) We attempted to get longer outputs by adjusting the decoding strategies (sampling and beam-search) and parameters (min tokens, length penalty, etc.); see Appendix for details. However, the quality evaluated by humans significantly deteriorated after reaching 2,000 words. By employing our proposed SelfLengthen method, we significantly enhanced the output length of the Qwen2-7B-Instruct backbone model while preserving the quality of the generated content. On the other hand, behavior imitation aims to cost-effectively leverage proprietary models like GPT4 to generate lengthy responses. This approach, which assumes the existence of more proficient model, is constrained by OpenAIs terms of use1 (Dong et al., 2024). Additionally, we currently lack clear understanding of how to build model capable of generating long texts from scratch. In this paper, for the first time, we demonstrate how we can stimulate the long-generation ability using only the LLMs intrinsic knowledge and skills, without the need for any auxiliary data or proprietary models. At the core of our approach is an iterative training framework called SelfLengthen, which consists of two roles: the Generator and the Extender. The Generator is responsible for generating the initial response, while the Extenders task is to lengthen the response. Specifically, at the beginning, both the Generator and the Extender are initialized using an existing instruct model. Given query, the Generator first outputs response. Then, we split the response into two parts based on punctuation. The Extender is prompted to expand the first part, after which it combines the expanded version of the first part with the original response to revise and expand the second part. This procedure results in new response that is approximately twice as long as the original. Next, the Generator is trained to directly produce this longer response based on the query, while the Extender is fine-tuned to extend the original response into this longer one. This results in updated versions of the Generator and Extender after each round of training. By iteratively applying this process, the Generator and Extender are progressively trained to handle increasingly longer responses. Once the Generator is able to produce outputs of the desired length, it can be used to generate long queryresponse pairs, effectively creating the post-training data needed for long-form tasks. Experiments conducted on both benchmarks and human evaluations demonstrate that Self-Lengthen consistently achieves better long-text generation capabilities compared to instruction backtranslation and behavior imitation, even without the need for additional long-form text data or proprietary models. This holds true across two open-source LLM backbones: Qwen2 and LLaMA3. Importantly, this improvement in long-text output does not negatively impact performance on general tasks measured by objective benchmarks like MMLU (Hendrycks et al., 2020), or subjective evaluations, including AlignBench (Liu et al., 2023). more detailed analysis reveals that with each iteration of Self-Lengthen training, both the Generator and the Extender are able to produce content that is approximately twice as long as the output from the previous round, while maintaining the same level of quality. This feature has been integrated into the Qwen2.5 series of models, allowing for an increased output length from 1,000 words to 8,000 words. 1https://openai.com/policies/terms-of-use

2 RELATED WORK
==============

Long Text Generation Most previous works explore long text generation in hierarchical manner. Fan et al. (2018) initially propose to create story by first generating short summary and then improve this method by introducing an intermediate step of producing the predicate-argument structure of the story as an outline (Fan et al., 2019). Tan et al. (2020), Sun et al. (2020) and Yao et al. (2019) further refine this hierarchical long text generation technique, followed by more advanced methods like Re3 (Yang et al., 2022b) and its variant DOC (Yang et al., 2022a), which introduced recursive prompting method for LLMs for long story generation in plan-and-write fashion. Additionally, STORM (Shao et al., 2024) organizes long-form articles by first using retrieval and multi-perspective questions asking to develop an outline and then writing each section in parallel. Another series of studies employ human-in-the-loop approaches to interactively create long articles (Coenen et al., 2021; Chung et al., 2022; Goldfarb-Tarrant et al., 2019; Zhou et al., 2023). Long Output Alignment While the above long-generation methods mainly target specific text types (e.g., stories), recently there has been growing interest in aligning LLMs to follow long output instructions (Que et al., 2024). The latter is often more challenging due to the greater diversity in generation types. Weaver (Wang et al., 2024) series is set of close-source commercial models designed specifically for creative writing. Suri (Pham et al., 2024) gathers high-quality long-form human-written text to employ instruction backtranslation and demonstrates that fine-tuned models can effectively extend LLM output lengths. LongWriter (Bai et al., 2024) broadens query types by adopting plan-and-write pipeline and applying behavior imitation on GPT-4o to follow more diverse instructions. While existing methods primarily achieve long-form output based on existing texts or more powerful LLMs, we explore new paradigm that stimulates long-generation ability from scratch using only the LLMs intrinsic knowledge and skills by iteratively self-lengthen the output length and inductively self-align to generate increasingly longer outputs.

3 PREREQUISITION AND PRELIMINARIES
==================================

Method Resource Prerequisition Compared with existing long output methods, our approach only requires set of seed instructions on long output tasks and an open-source instruction model to automatically selfimprove the models ability to output long texts. comparison of resource requirements is listed in Table 1. Our highlevel idea is to use the existing models to lengthen its output in each round, and in turn fine-tune the existing models to have longer output capabilities. Then, we iterate this process to continuously construct longer data and more powerful long output models. To achieve this goal, two critical models: iteratively used and trained. Output LLM Suri (Backtranslation) High-quality Human-written text Open-source instruct model Only cover small range of tasks LongWriter (Behavior Imitation) Self-Lengthen (Self-Alignment) Seed instructions Seed instructions High-capability long-context LLM (GPT-4 in their paper) Open-source instruct model Rigidly structured All styles and types Table 1: Comparison of resources, LLM, and output for different long output data sourcing methods. the Generator and the Extender, will be Generator Given an instruction guiding long output, our Generator can generate long output within certain length range. We will iteratively train the Generator Geni in the i-th iteration, with each initialized from the previous iteration. In particular, at the beginning, Gen0 is directly initialized from well-aligned instruct model. During the iteration process, the output length of the Generator will continuously increase. Formally, we use Genθ to denote the Generator with parameters θ, respectively. Then, the likelihood Genθ(yx) for long output is obtained by cumulatively multiplying the conditional probability of each next token p(ytx, y<t) as shown in Equation (1). Genθ(yx) = len(y) (cid:89) i=1 3 pθ(yix, y<i) (1) Figure 2: high-level overview of the proposed Self-Lengthen. (Micro-Iteration) We begin by using the Generator to produce an initial response. Then, we employ the Extender to expand this response in two-stage process, resulting in much longer output. This extension process can be repeated iteratively to create increasingly lengthy responses. (Macro-Iteration) Once we have generated long responses, we use them to fine-tune both the Generator and Extender. These improved models are then utilized in subsequent iterations to generate and extend even longer responses. Extender Our Extender can extend long response to longer response y+ under its instruction x. Similar to the Generator series, we will train an Extender Exti that can extend responses to longer length based on the previous iteration, and the first round Extender Ext0 is initialized from the seed instruction model with appropriate extend prompt promptEXT . The likelihood y+ Extθ(y+promptEXT (x, y)) for extended response y+ can be represented as Equation (2). Extθ(y+x, y) = len(y+) (cid:89) i= pEXT θ (y+ ix, y, y+ <i) (2) Within each iteration, len(y+) > len(y) will always hold, and we will increase len(y) and len(y+) during iterations.

4 METHODOLOGY: SELF-LENGTHEN
============================

Based on user instructions, our algorithm empowers the seed instruct model to autonomously iterate to self-improve the ability to produce longer outputs. This involves process of gradually expanding the models output while training it in reverse, effectively increasing the length and content of its generated output step by step. We show our high-level methodology in Figure 2. 4.1 INDUCTIVELY EXTENDING LONG OUTPUT ABILITY The workflow of our Self-Lengthen can be split into four essential steps: 1) instruction augmentation, 2) initial response generation, 3) response extension, and 4) fine-tuning the next iterations models. These four steps will be iteratively conducted and inductively extend the LLMs output length. Instruction Augmentation We employ self-instruct (Wang et al., 2022) to bootstrap our instruction set. Specifically, we maintain an instruction pool and randomly select two instructions at time as few-shot learning examples to generate new instruction. This approach fosters deep exploration and diverse array of instructions. Additionally, we employ the seed LLM to validate the generated instructions, filtering out any unsuitable instructions for producing long responses. Initial Response Generation In the i-th iteration, we employ the Generator Geni, which has been fine-tuned from the previous iteration, to produce initial responses based on the augmented 4 instructions x. As the Generators ability to generate long outputs improves with each iteration, the length of these initial responses will also grow progressively. Response Extension After generating the initial response y, the next step is to extend its length to create longer response y+. The vanilla extension method, namely directly using prompt to generate an extended response, is limited to the models upper length constraint and cannot yield continuous increases during iterations. To overcome this, we propose two-stage extension method that allows us to reach and exceed the models length limits. In Stage 1, we use the first half of the initial response, denoted as y[: 1 2 ], as the input of Exti for the extension process. This yields an extended version y+ 1 for the first half of the content. Because the input is shorter, the model can produce significantly longer output than the inputpotentially expanding y[: 1 2 ] to almost twice its original length, matching about len(y). In Stage 2, we provide the entire initial response to Exti for further expansion. Here, the extension of the first half y+ 1 serves as the existing extended content for completion of the second half, just like the in-context-learning for illustrating the structure and content of this extension: since y1 is derived from the first half of y, we anticipate that this approach will facilitate further expansion of the subsequent content in similar manner. 1 [: 2 key technique used in this process is the removal of the last one-third of y+ 1 , and we only use the remaining part denoted as y+ 3 ] for completion. This ensures that the previous extension doesnt conclude abruptly, allowing the model to smoothly link the preceding and subsequent segments, thereby enhancing overall coherence and consistency. We dont need to pinpoint the specific endpoint in the original y1 that corresponds to y+ 3 ]; we let the model identify it and begin with the completion of the removal part before extending the second half. An illustration of this process is shown in Figure 3. 1 [: 2 Figure 3: An illustration of the response extension process. Given an instruction, we first employ the Generator to produce an initial response. We then use the Extender to take the first half of this initial response to create the extension of the first part (Stage 1). Following this, we proceed to extend the entire response, using the previous two-thirds of the extended first half for demonstration to complete the extension of the remaining part (Stage 2). Ultimately, the model delivers cohesive and consistent extended response. Note that this example is just for illustrative purposes since in real scenarios the responses would be much longer. This extension step can be repeated multiple times within single iteration, which we refer to as an micro-iteration. Specifically, after we apply two-stage extension to transform the initial response into y+, we can perform another two-stage extension by substituting the input with y+ to obtain further extended version y++. We will apply this process three times within each iteration, and retain only those responses that are genuinely longer after the extensions. 5 Fine-tuning the Next Iterations Models Once we obtain the final extended responses y+ (we use y+ for simplicity, but it could also be y++ or y+++ depending on our micro-iteration process), we will utilize these to fine-tune the Generators Geni+1 and Extenders Exti+1 for enhancing their capability to produce longer outputs. At this stage, we will also apply rule-based methods to filter out invalid responses to ensure their quality. Specifically, we will eliminate responses exhibiting any of the following issues: 1. Inadequate length: The length of the extended response does not exceed 120% of the initial response length, hindering the depth of the content. 2. Frequent repetition: The response excessively reiterates the same phrases or sentences. 3. Endless: The extended response does not end with normal punctuation. 4. Code-switching: The response incorporates unintended languages, e.g., an English article containing inappropriate Chinese characters. To accelerate the process of length increase during iterations, we implement length-bias sampling technique. This involves randomly down-sampling responses that have shorter lengths. Specifically, we denote the set of extended responses as and define r(len(y+)) as the length percentile of y+ (the shortest response will be 0 and the longest will be 1). We then sample to create our length-biased set as follows, where the shorter responses are assigned much higher dropping rate: = {y+ : (0, 1) > 2 (1 r(len(y+)))3} (3) This process will drop about half of the responses but significantly increase the average length. After obtaining the pruned extended responses, we will perform supervised fine-tuning to get Geni+1 on (x, y+) pairs, with the parameters initializing from Geni as warm-up. For the Extender, we will first randomly remove 15% of the lines from the initial responses to create manipulated responses y. We will then use (x, y, y+) triplets, incorporated into promptEXT , to perform supervised fine-tuning on Exti to obtain Exti+1. The random dropping of lines encourages the fine-tuned Extender to complete critical missing information within paragraphs, thereby enhancing its extending capabilities. The entire process will be conducted multiple times, which we refer to as macro-iteration. 4.2 FINAL ALIGNMENT Our goal isnt just to acquire specific Generator or Extender; we aim to enhance the long text generation capabilities of any seed model while maintaining its overall performance. To accomplish this, after multiple macro-iterations of our method, we will gather query-response pairs and implement length controls in the queries to create SFT data. Data Collecting We will gather data from the initial responses and extended responses y+, along with their corresponding prompts in each iteration to create more extensive dataset. This dataset will encompass variety of prompt lengths and types. Additionally, we will use the Generators in each iteration to generate more data, further enriching our dataset. Query Rephrasing for Length Control In our earlier steps, we did not sufficiently highlight the importance of length control in the input instructions. As result, the Generators produce responses of varying lengths for the same instruction across different iterations. To develop general model, we will now incorporate length constraints in the input instructions. Specifically, we will first count the length of each response, denoted as len(y), and then utilize our seed LLM to integrate this constraint into the input instructions. We will strategically design prompts to create diverse range of output instructions that encompass various types of length restrictions.

5 EXPERIMENT AND EVALUATION
===========================

To demonstrate the effectiveness of our method, we chose two other mainstream methods as baselines for comparison: 6 Backtranslation: We compare our method with instruction backtranslation from high-quality human-written text, which has been demonstrated to be effective in extending models output length, according to Suri (Pham et al., 2024). We randomly selected 3,000 samples from the Suri SFT dataset as our English text source and collected 3,000 high-quality Chinese SFT data ranging from 2,000 to 8,000 in length from the web to create our Chinese dataset. Plan-and-Write: We compare our method with the hierarchical plan-and-write methods, which have been widely studied in previous research. Among them, we select LongWriter (Bai et al., 2024) as our basic implementation since it can handle broader range of query types and is more suitable for our experiment settings. We source the data through 1) behavior imitation using GPT4o, and 2) self-alignment using Qwen2-7B-Instruct. We will evaluate our method against these baselines in two key aspects: 1) generated data and 2) fine-tuned models.

5.1 DATA EVALUATION
===================

To conduct comprehensive assessment, we carried out human evaluation of the generated data. In this evaluation, we focused mainly on comparing our method with backtranslation and hierarchical plan-and-write approaches. To control for variables, we centered our analysis on the generated data rather than the finely-tuned model. We randomly selected 15 query-response pairs from each method, ensuring that their lengths were approximately uniformly distributed between 2000 and 8000 words. For the queries, evaluators scored them based on how well they addressed user needs, their diversity, and their naturalness. When assessing responses, evaluators considered several factors to determine an overall satisfaction score, including relevance, coherence, accuracy, consistency, clarity, creativity, and engagement. Due to the extraordinarily long length of the responses, we utilized GPT-4o to summarize and analyze them, aiding evaluators in their assessments for improved readability and increased annotation efficiency. Both queries and responses were rated on scale from 1 to 10. We recruited multiple individuals to annotate the data and record the average scores. Detailed evaluation guidelines can be found in the Appendix E.1. Figure 4: The results of data evaluation. Additionally, we calculate the Distinct scores (Li et al., 2015) of the generated responses and record the average, which measures response diversity. This metric is crucial for our long output generation tasks, especially those requiring creative writing. We have summarized all results in Figure 4 and identified two noteworthy findings. Firstly, unlike Suri, which relies on instruction-backtranslations from various human-written texts, we utilize user logs, resulting in significantly better query diversity compared to backtranslation methods. We also found that the quality of our responses surpasses that of the backtranslation method created by humans and the behavior imitation method using GPT-4o. Both using Qwen27B-Instruct for data generation, our approach yields substantially higher response scores than Planand-Write-Qwen, highlighting the advantages of our method. Secondly, we observed notable improvement over other methods in terms of Distinct score, indicating wider variety of expressions and content. This finding provides an intuitive explanation of our superior performance. 5.2 FINE-TUNED MODEL EVALUATION LonGen Benchmark and Metrics In our evaluation, we collect and assemble set of test prompts from our online logs. These prompts are meticulously tested to ensure they do not appear in our training set and do not contain any personally identifiable information (PII). The queries are very diverse and cover wide range of real user needs across different long-form generation tasks. To protect user privacy when using these prompts, we also utilize GPT-4o to rewrite them. The rewritten 7 Table 2: The main results on the LonGen. SL and SQ stand for the length-following score and response quality score, respectively. The highest score among different fine-tuning methods on the backbone model is highlighted in green . The P&W method uses GPT-4o for data generation. Model Overall [2k, 4k) [4k, 6k) [6k, 8k] MMLU AlignBench SL SQ SL SQ SL SQ SL SQ Open-source LLM Qwen2-72B-Instruct Llama-3.1-70B-Instruct Mistral-Large-Instruct Proprietary LLM GPT-4o Claude-3.5-Sonnet Qwen Backbone Qwen2-7B-Instruct w/ Backtranslation w/ Plan-and-Write w/ Self-Lengthen LLaMA Backbone Llama3.1-8B-Instruct w/ Backtranslation w/ Plan-and-Write w/ Self-Lengthen 6.08 2.08 15.30 82.26 84.82 86. 15.12 6.24 39.07 83.93 84.84 87.02 0.25 0.00 6.83 82.45 85.05 85.70 2.86 0.00 0.00 80.41 84.55 85. 14.98 46.01 85.75 85.34 41.00 71.14 85.41 85.68 3.62 43.81 85.59 85. 0.31 23.08 86.25 85.04 - - - - - - - - - - 3.38 58.29 58.51 60.48 2.36 18.50 49.36 51.61 81.74 85.72 85.52 85.82 79.40 43.03 82.86 83.43 10.15 68.26 68.40 71.65 7.08 9.56 60.70 62. 83.21 85.93 85.14 86.55 81.70 39.29 81.43 80.00 0.00 62.90 60.42 62.71 0.00 10.32 59.33 60.61 81.98 85.91 85.30 85.27 78.68 44.76 87.14 84. 0.00 43.70 46.72 47.07 0.00 35.63 28.05 32.15 80.02 85.32 86.12 85.63 77.85 45.43 79.76 84.90 72.9 72.6-0.3 72.3-0.6 72.5-0.4 69.7 64.7-5.0 66.5-3.2 67.0-2. 7.06 7.04-0.02 7.05-0.01 6.99-0.07 4.61 3.42-1.19 5.15+0.54 5.04+0.43 prompts adhere to strict length constraints, sourcing the LonGen benchmark, with detailed statics shown in Table 3. We then assess the responses based on two criteria: 1. Length Following Score: We categorize the length constraints into four groups: 1) the length is about specific length, 2) the length falls within specific range, 3) the length is above specific length, and 4) the length is below specific length. Based on the ground truth length defined in the queries and the models actual output length, we calculate scalar length-following score ranging from 0 to 1, where higher score indicates greater adherence to the desired length. The detailed calculation method is provided in Appendix E.2.1. 2. Output Quality Score: We employ LLM-as-a-judge to evaluate the quality of the generated responses. Specifically, we use GPT-4o to assess the responses on seven key aspects pertinent to long-form generation tasks: relevance, coherence, accuracy, consistency, clarity, creativity, and engagement. Each aspect will be assigned scalar score ranging from 1 to 10. We then calculate the average score across all aspects to arrive at the final quality score. The judging template is included in our repository. In our evaluation, we begin by testing the performance of several widely-used open-source and proprietary LLMs. We then use Qwen2-7B-Instruct and Llama3.1-8B-Instruct as our backbone models and evaluate the performance of fine-tuned models using baseline methods and ours. Notably, the original versions of Suri and LongWriter datasets either lack appropriate length requirements in their queries or have unsuitable length constraints, resulting in results that fall significantly short of expectations. To ensure fair comparison, we incorporate length constraints by applying the same workflow and prompts to their original queries. Finally, we compare the improvements achieved against the backbone LLMs. The results are presented in Table 2. We observe that most existing open-source and proprietary LLMs struggle to generate outputs longer than 4,000 words, which is consistent with our preliminary study. However, after fine-tuning on our sourced dataset, we achieved significant increase in output length compared to general models. This improvement is also advantageous when compared to backtranslation and the plan-and-write approaches. We also observe significant differences in the performance of the backtranslation method between the two backbone models. Upon closer inspection, we find that the fine-tuned LLaMA model tends to generate repetitive content at relatively high rate. We believe this may be related to the differing proportions of human-written text 8 # Data in each subset Language Chinese English Type about range above below 10 10 10 10 [2k, 4k) [4k, 6k) [6k, 8k] Business Communication Output Category Literature and Creative Writing 120 120 Academic Research 66 64 37 10 26 10 Miscellaneous Professional Writing 18 10 15 14 10 Personal Expression Technical Documentation Journalism and Media 10 10 10 Table 3: Key statistics of LonGen, which contains 2 languages * 3 length ranges * 4 constraint types * 10 = 240 pieces of data in total. Figure 5: length scatters on LonGen. Required-actual in the pretraining data of the backbone models, or it could stem from variations in the properties or training frameworks of those models. Furthermore, we find that the quality of our outputs outperforms the other two methods overall and surpasses that of the backbone models by considerable margin. The only aspect that is reduced is clarity, which is understandable, as longer outputs tend to increase the read complexity, especially when the content is much richer. These results demonstrate our effectiveness of employing relatively small LLM that self-aligns for competitive long-output capabilities, even exceeding those of much larger LLMs and other long-output methods that require human-written text or high-capability long-context LLMs. Additionally, we evaluate the performance of the fine-tuned models on general tasks which are most in the short form responses measured by objective benchmarks like MMLU (Hendrycks et al., 2020), or subjective evaluations, including AlignBench (Liu et al., 2023). We found that the performance of the fine-tuned models is nearly equivalent to that of the backbone models, with only negligible decreases. Meanwhile, we observed notable improvements on AlignBench for the LLaMA model. This finding indicates the improvement in long-text output does not negatively impact general abilities and demonstrates the advantages and feasibility of our method. Self-Lengthen Exhibits Strong Performance in GPT-4o Pair Comparisons We leveraged GPT-4o to evaluate pairs of responses generated by different fine-tuned models on Qwen2-7B-Instruct and the backbone model in response to the same queries from our evaluation set. To ensure fair comparison, we will swap positions in each response pair and compare twice to avoid the LLM positional bias. The averaged win rates for each pair of models are recorded in the heat map presented in Figure 6. The results indicate that all specialized models exhibited significant improvements over the backbone model in longgeneration tasks. Moreover, our approach outperformed both the instruction backtranslation and planand-write methods, highlighting the great advantages of our method. Figure 6: The win-rate heat map. Self-Lengthen Enables Steady Increase in Output Length across Macro-Iterations At each macro-iteration, we record the generated output length and the associated overall quality scores as evaluated by GPT-4o, and display their distribution in Figure 7. We document both the initial responses and the final extended responses, designating these as results for the Generator and Extender, respectively. Additionally, we track the performance of the seed model as iteration 0. The figure demonstrates that both the Generator and Extender experience consistent increase in average output length with each macro-iteration, from the initial average length of 1,000 words reaching approximately 8,000 words after three macro-iterations, and we will extend the length to approximately twice its original size in each iteration. We also observe that while longer outputs may be accompanied by slight decrease in quality scores, this drop is negligible and can be disregarded throughout the iterations. 9 Figure 7: The length and score distribution in each macro-iteration. Compared with not using length-bias sampling technique (Lower Row), we find that using length-bias sampling (Upper Row) will significantly speed up length extending process, and does not have any sign to aggravate the score-dropping. The horizontal axis represents the number of macro-iterations, while the vertical axis indicates the length (word) or score. The Output Styles can be Simply Adjusted via Changing the Extend Prompt We have discovered that slightly modifying the extended prompt allows us to alter the final output style, and the results are sensitive to these adjustments. This finding highlights the flexibility and significant potential of our extending method. For instance, if we wish to make the generated content more narrative rather than technical, we can simply emphasize that point in the extending prompt. This approach enables us to customize the output according to our preferences or to achieve various text styles by tweaking the prompt. To illustrate this finding, we enhanced the prompts by attaching: 1) You should elaborate on the details to make each paragraph richer and 2) You should add more examples and subparagraphs while keeping all other components unchanged. We then conducted three macro-iterations for both extending prompts and selected 1,000 samples of length between 4,000 and 6,000 words to analyze the number of paragraphs produced. We present the results in Figure 8. We found that extending prompt 2 exhibits significant rightward shift in paragraph count distribution compared to extending prompt 1, with an average increase of 10% in the number of paragraphs. This suggests change in its output style.

6 CONCLUSION
============

Figure 8: The distributions of paragraph counts using two different extending prompts. In this work, we introduced Self-Lengthen, which demonstrates for the first time how to elicit highquality, long length-following responses from existing LLMs without relying on additional data or proprietary models. Self-Lengthen alternately trains Generator and an Extender, where the Extender incrementally expands the Generators responses in segments, serving as training objective to enhance both the length of the Generators subsequent response and the Extenders subsequent extension. Through three macro-iterations, we can produce responses that are about eight times longer, thereby constructing high-quality (query, long response) training data. Our method has been rigorously evaluated using both benchmark metrics and human assessments, which have consistently confirmed the quality of the training data produced by Self-Lengthenand the effectiveness of the trained models.

REFERENCES
==========

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Anthropic: Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic. com/news/claude-3-5-sonnet. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, et al. Towards scalable automated alignment of llms: survey. arXiv preprint arXiv:2406.01252, 2024. John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, and Minsuk Chang. Talebrush: Sketching stories with generative pretrained language models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pp. 119, 2022. Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, and Ann Yuan. Wordcraft: human-ai collaborative editor for story writing. arXiv preprint arXiv:2107.07430, 2021. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. arXiv preprint arXiv:2406.13542, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models, 2024. Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018. Angela Fan, Mike Lewis, and Yann Dauphin. Strategies for structuring story generation. arXiv preprint arXiv:1902.01109, 2019. Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, et al. Towards unified view of preference learning for large language models: survey. arXiv preprint arXiv:2409.02795, 2024. Seraphina Goldfarb-Tarrant, Haining Feng, and Nanyun Peng. Plan, write, and revise: an interactive system for open-domain story generation. arXiv preprint arXiv:1904.02357, 2019. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Gregory Kamradt. Needle in haystack - pressure testing llms, 2023. URL https://github. com/gkamradt/LLMTest_NeedleInAHaystack. 11 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. RephrasarXiv preprint ing the web: recipe for compute and data-efficient language modeling. arXiv:2401.16380, 2024. Saurav Pawar, SM Tonmoy, SM Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The what, why, and how of context length extension techniques in large language modelsa detailed survey. arXiv preprint arXiv:2401.07872, 2024. Chau Minh Pham, Simeng Sun, and Mohit Iyyer. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371, 2024. Shanghaoran Quan. Automatically generating numerous context-driven sft data for llms across diverse granularity. arXiv preprint arXiv:2405.16579, 2024a. Shanghaoran Quan. Dmoerm: Recipes of mixture-of-experts for effective reward modeling. arXiv preprint arXiv:2403.01197, 2024b. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. Assisting in writing wikipedia-like articles from scratch with large language models. arXiv preprint arXiv:2402.14207, 2024. Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, and Chun Fan. Summarize, outline, and elaborate: Long-text generation via hierarchical supervision from extractive summaries. arXiv preprint arXiv:2010.07074, 2020. Bowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric Xing, and Zhiting Hu. Progressive generation of long text with pretrained language models. arXiv preprint arXiv:2006.15720, 2020. Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, et al. Weaver: Foundation models for creative writing. arXiv preprint arXiv:2401.17268, 2024. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. 12 An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. Doc: Improving long story coherence with detailed outline control. arXiv preprint arXiv:2212.10077, 2022a. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. arXiv preprint arXiv:2210.06774, 2022b. Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-andwrite: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 73787385, 2019. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text. arXiv preprint arXiv:2305.13304, 2023. MODEL CARDS & GENERAL BENCHMARKS We list the details of our evaluated models in Table 4. Model name Model version Context window Max output tokens Claude 3.5 Sonnet (Anthropic, 2024) GPT-4o (Achiam et al., 2023) Qwen2-7B-Instruct (Yang et al., 2024) Qwen2-72B-Instruct (Yang et al., 2024) Llama-3.1-8B-Instruct (Dubey et al., 2024) Llama-3.1-70B-Instruct (Dubey et al., 2024) Mistral-Large-Instruct (Jiang et al., 2023) claude-3-5-sonnet-20240620 gpt-4o-2024-08-06 - - - - Mistral-Large-Instruct-2407 200,000 tokens 128,000 tokens 128,000 tokens 128,000 tokens 128,000 tokens 128,000 tokens 128,000 tokens 4,096 tokens 4,096 tokens - - - - - Table 4: Model cards.

B MORE RESULTS
==============

We show the detailed length following evaluation results in Table 5 and the detailed response quality evaluation results in Table 6.

C IMPLEMENTATION DETAILS
========================

We conduct experiments using Qwen2-7B-Instruct and Llama3.1-8B-Instruct as the backbone models across multiple nodes equipped with Nvidia H100 GPUs and Intel Xeon Processors. For inference, we utilize FastChat (Zheng et al., 2023) to control vLLM (Kwon et al., 2023) workers for high throughput. For training, we use an internal training framework to train the Qwen model, and use Llama-Factory (Zheng et al., 2024) as the high-level API and implement DeepSpeed ZeRO3 (Rajbhandari et al., 2020) and CPU offloading to train the LLaMA model. 13 Model Open-source LLM Qwen2-72B-Instruct Llama-3.1-70B-Instruct Table 5: The detailed results of the length following score on the LonGen. The four different length constraints about, range, above, and below are represented by , [], >, and <, respectively. The highest score among different fine-tuning methods on the backbone model is highlighted in green . Overall [2k, 4k) [4k, 6k) [6k, 8k] [] > < [] > < [] > < [] > < 3.75 1.14 4. 0.65 4.02 1.86 11.76 11.24 4.67 3. 9.73 1.95 10.04 29.45 5.59 14.01 0. 0.00 8.16 0.00 0.00 3.88 0. 0.00 9.74 1.00 0.00 5.56 0. 0.00 0.00 4.60 0.00 0.00 2. 0.00 0.00 4.82 0.00 0.00 Mistral-Large-Instruct 14.44 12.50 14.64 19.63 35.15 33.63 34.19 53.33 Proprietary LLM GPT-4o 13.19 12.78 16.23 17.71 36.79 36.44 42.10 48.67 2.77 1. 6.15 3.70 0.00 0.05 0.43 0. Claude-3.5-Sonnet 47.02 42.09 54.43 40.50 69.80 69.22 76.02 69.52 46.77 31.88 61.42 35.16 24.49 25.17 25.84 16.84 Qwen Backbone Qwen2-7B-Instruct 1.54 0. 0.46 10.59 4.63 2.84 1.37 31. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 w/ Backtranslation 62.08 45.48 52.71 72.89 68.24 66.09 73.96 64.76 74.16 38.38 57.74 81.31 43.85 31.96 26.41 72.59 w/ Plan-and-Write 60.94 44.32 54.93 73.87 78.81 64.65 69.37 60.77 67.37 37.26 53.33 83.71 36.63 31.04 42.10 77. w/ Self-Lengthen 61.63 51.90 52.07 76.32 73.45 86.32 69.13 57.72 67.42 33.36 56.30 93.75 44.01 36.04 30.77 77.48 LLaMA Backbone Llama3.1-8B-Instruct 1.68 0. 0.82 6.94 w/ Backtranslation 13.23 12.91 26.88 20.98 5.03 0. 0.00 6.53 2.45 4.20 20.83 26. 0.00 5.37 0.00 4.28 0.00 0. 0.00 0.00 0.00 0.00 20.73 10.88 33.75 27.92 55.71 25.13 w/ Plan-and-Write 55.82 34.60 44.48 62.55 70.80 61.61 63.15 47.26 72.00 27.65 54.26 83.38 24.65 14.53 16.02 57.01 w/ Self-Lengthen 44.48 44.28 44.59 73.09 51.22 71.24 56.30 69.52 59.11 46.85 57.30 79.19 23.10 14.74 20.17 70.58 Table 6: The detailed results of the response quality score on the LonGen. The highest score among different fine-tuning methods on the backbone model is highlighted in green . Model Overall Relevance Coherence Accuracy Consistency Clarity Creativity Engagement Open-source LLM Qwen2-72B-Instruct Llama-3.1-70B-Instruct Mistral-Large-Instruct Proprietary LLM GPT-4o Claude-3.5-Sonnet Qwen Backbone Qwen2-7B-Instruct w/ Backtranslation w/ Plan-and-Write w/ Self-Lengthen LLaMA Backbone Llama3.1-8B-Instruct w/ Backtranslation w/ Plan-and-Write w/ Self-Lengthen 82.26 84.82 86.05 85.75 85. 90.67 95.38 96.54 97.21 95.79 86.12 88.62 89.38 89.00 89.00 87.00 88.92 91.29 90.71 90. 89.92 92.54 93.42 93.29 93.08 87.33 88.21 88.25 88.79 86.04 65.29 67.50 69.42 68.17 70. 69.50 72.54 74.04 73.08 72.75 81.74 85.72+3.98 85.52+3.78 85.82+4.08 90.88 96.67+5.79 96.25+5.37 96.21+5.33 85.42 88.67+3.25 88.04+2.62 88.54+3.12 86.25 90.67+4.42 89.62+3.37 89.88+3. 89.75 92.50+2.75 92.29+2.54 92.71+2.96 87.00 85.08-1.92 84.96-2.04 84.71-2.29 64.25 72.46+8.21 73.08+8.83 73.21+8.96 68.62 74.00+5.38 74.42+5.80 75.46+6.84 79.40 43.03-36.37 82.86+3.46 83.43+4.03 88.02 54.12-33.90 93.53+5.51 95.33+7. 83.63 38.24-45.39 87.06+3.43 87.33+3.70 84.51 68.24-16.27 86.47+1.96 90.67+6.16 87.17 50.00-37.17 90.59+2.54 94.00+8.63 84.64 61.69 43.53-41.11 23.53-38.16 67.65+5.96 82.94-1.70 68.00+6.31 79.33-5. 66.12 23.53-42.59 71.76+5.64 69.33+3.

D PILOT STUDY
=============

In our pilot study, we aim to investigate the long-output capabilities of existing models. We select ten queries suitable for eliciting long responses and adjusted the length requirements within these queries to generate outputs of varying lengths, all while using default decoding parameters. We then document both the actual output lengths and the specified length demands. To further ascertain whether adjusting decoding strategies can produce valid long outputs, we randomly sample various key decoding parameters within their permissible ranges and attempt to gen14 erate longer responses. Specifically, we test both sampling and beam-search decoding strategies, using randomly selected parameters outlined in Table 7. We also set the min tokens to sufficiently large number to force the models to generate adequately lengthy outputs. We show our findings in Figure 1. Sampling Beam-Search length penalty repetition penalty temperature top best of [0, 2] [0, 2] [0, 1] 1 [0, 2] [0, 2] 0 1 [2, 10] Table 7: Sampling range for decoding parameters across two strategies. We will randomly choose decoding strategies and sample the appropriate parameters to generate extended outputs. Subsequently, we will evaluate the quality of responses of suitable lengths using GPT-4o, selecting only the top 1% for human evaluation.

E EVALUATION DETAILS
====================

E.1 HUMAN EVALUATION GUIDANCE For each method, we provide several random grouped queries from the selected data for the annotator to assess the query scores. The evaluation instructions are displayed in the text box below. Instruction of evaluating queries for human Please assess the following user queries for generating long responses. Evaluate how well these queries meet user needs, their diversity, and overall naturalness. Then, assign an overall score from 1 to 10, with 1 being the lowest and 10 the highest. To enhance readability and improve annotation efficiency given the lengthy nature of the responses, we first utilize GPT-4o to summarize the responses and provide brief analysis to aid evaluators in their assessments. We present the prompt used for this process below. Prompt to summarize and analyze the long responses for GPT-4o You are meticulous and impartial analysis assistant. Given user prompt and the corresponding lengthy response, please first summarize the response in approximately 400 words. Then, provide straightforward analysis of the quality of the response, including both strengths and weaknesses. User prompt: query Long response: response After that, we direct the evaluators to access each response using the following instructions. Instruction of evaluating responses for human Please assess your satisfaction with the models response under the users query. Consider factors such as relevance, logic, accuracy, consistency, clarity, originality, and engagement, and give score ranging from low to high [1, 10]. In your evaluation, you may refer to summaries and brief analyses provided by third-party models, but please note that this information is for reference only and should not influence your independent judgment. For the evaluation of queries and responses, we also provide more detailed instructions for each score and several cases with analysis to guide the evaluators. However, we do not plan to display these instructions on the paper to conserve space. E.2 AUTOMATIC EVALUATION E.2.1 LENGTH FOLLOWING SCORE For length constraints, we identify four types: 1) about, 2) range, 3) above, and 4) below. Based on these different types, we will calculate the corresponding target lower bound length targetmin and the target upper bound length targetmax as Table 8: type about range above below constraint target min target max [x1, x2] > < 0.8x x1 0.5x 1.2x x2 1.5x Table 8: The target minimum and maximum lengths for the four length constraint types. Next, let represent the actual output length, and the length following score SL will be: SL = 1 max max (cid:16) 0, (cid:16) targetmin targetmax 0, 3 2 1 (cid:17) if targetmin targetmax if < targetmin (cid:17) 2 if > targetmax

F PROMPT TEMPLATE
=================

We display the prompt templates used in our experiments. For self-instructing, we use the following template. Prompt template for self-instructing Your task is to construct user instructions that can guide the generation of long text. Instruction 1: {prompt1} Instruction 2: {prompt2} Instruction 3: The prompt1 and prompt2 will be replaced with two randomly selected examples. For response extending, we use the following template. Prompt template for response extending You are an expert in writing polishing. Based on the provided instruction and original outline, enhance the content by carefully expanding the original outline to increase its informational content, incorporate specific details, and improve its descriptive quality. Remember that your task is not to continue writing content, but to enrich the current content. You can achieve this by enriching existing paragraphs with more detailed descriptions or creating more subheadings and sections. You should try to be creative and informative as much as possible and do NOT include any repetitive content or lengthy narratives in your response. You need to generate text that is as long, rich, and diverse as possible. Instruction: {prompt} Original outline: {initial response}

G CASE DEMONSTRATION
====================

We present case study to clarify our workflow. To start, we employ augmented instructions and use the generator to create an initial response. Instruction Write story about young boy who discovers magical book in his attic and learns how to harness the power of magic within himself. Initial response (553 words) The Boy Who Unlocked His Inner Magic Twelve-year-old Max had always been fascinated by the dusty, cobweb-covered trunks and boxes that filled his familys attic. He loved exploring the hidden treasures he found there, from old toys and clothes to mysterious artifacts and forgotten heirlooms. But nothing could have prepared him for what he was about to discover. One sunny Saturday morning, while rummaging through an old trunk, Max stumbled upon peculiar-looking book bound in worn leather. The cover was adorned with intricate symbols and patterns that seemed to shimmer in the dim light of the attic. As he opened the book, faint hum filled the air, and the pages crackled with an otherworldly energy. Intrigued, Max carefully flipped through the yellowed pages, revealing illustrations of fantastical creatures, magical spells, and ancient incantations. note on the first page caught his eye: For those who seek the spark within, To unlock the secrets of the heart, Read these words, and listen well, For the magic lies within you, not in this spell. As Max read the words, he felt an unusual tingling sensation in his fingers, as if the book was awakening something deep within him. Suddenly, visions began to flood his mind vivid images of enchanted forests, shimmering stars, and majestic dragons soaring through the skies. Over the next few days, Max returned to the attic again and again, devouring the knowledge contained within the book. He practiced the simple spells and incantations, feeling the power coursing through his veins like electricity. At first, it was just gentle buzzing, but soon the sensations grew stronger, and Max started to notice changes within himself. He discovered he could make objects move with mere thought, levitate small items, and even conjure tiny sparks of light from thin air. His friends at school noticed the change in him too; theyd see him effortlessly solve complex math problems or create beautiful, intricate drawings without any apparent effort. But most remarkable of all, Max realized that the true source of his newfound abilities lay not in the book, but within himself. The book had simply unlocked door to his own inner potential, allowing him to tap into reservoir of untapped magic that had been slumbering inside him all along. As Max continued to hone his skills, he learned to control his powers more effectively, mastering techniques to heal injuries, communicate with animals, and even bend time and space to his will. His confidence soared, and he knew that he was no longer just ordinary boy; he was wizard, capable of achieving incredible feats. With great power comes great responsibility, however. Max understood that he must use his gifts wisely, helping others and protecting the world from those who would misuse such powerful forces. And so, with the magical book safely tucked away, Max embarked on journey to explore the limits of his abilities, knowing that the true magic lay not in the pages of book, but in the depths of his own heart and soul. From that day forward, Max lived life of wonder, using his powers to inspire and protect those around him, becoming beacon of hope and shining example of the magic that lay within every individual, waiting to be unleashed. Next, we expand on the first half of the initial response and obtain the following content. Extended content on the first half response The Boy Who Unlocked His Inner Magic World of Discovery Twelve-year-old Max had always been captivated by the dusty, cobweb-covered trunks and boxes that filled his familys attic. For years, he had spent countless hours exploring the hidden treasures he found there, from vintage toys and antique clothes to mysterious artifacts and forgotten heirlooms. Each new discovery sparked sense of wonder and curiosity within him, fueling his imagination and inspiring his love for adventure. His parents often joked that he had knack for uncovering the most unlikely treasures, and they would smile knowingly whenever he emerged from the attic with some newfound gem. But nothing could have prepared Max for what he was about to discover on that fateful Saturday morning. As he climbed the creaky stairs leading up to the attic, the warm sunlight streaming through the windows illuminated the space, casting golden glow over the familiar landscape of trunks, boxes, and forgotten relics. With spring in his step, Max set out to explore the attic once again, determined to uncover treasure that would surpass all others. The Mysterious Book While rummaging through an old trunk, Maxs fingers brushed against peculiar-looking book bound in worn leather. The cover was adorned with intricate symbols and patterns that seemed to shimmer in the dim light of the attic, like constellation of tiny stars. As he opened the book, faint hum filled the air, and the pages crackled with an otherworldly energy. The musty scent of aged paper wafted up, carrying with it whispers of secrets and mysteries long forgotten. Intrigued, Max carefully turned the pages, revealing illustrations of fantastical creatures, magical spells, and ancient incantations that seemed to leap off the page and dance before his eyes. note on the first page caught his eye, written in elegant script that seemed to shimmer like moonlight: For those who seek the spark within, To unlock the secrets of the heart, Read these words, and listen well, For the magic lies within you, not in this spell. As Max read the words, he felt an unusual tingling sensation in his fingers, as if the book was awakening something deep within him. It was as if the very essence of the book was resonating with his own heartbeat, calling forth part of him that he never knew existed. Suddenly, visions began to flood his mind vivid images of enchanted forests, shimmering stars, and majestic dragons soaring through the skies. The world around him melted away, leaving only the thrumming energy of the book and the promise of untold wonders waiting to be discovered. Unlocking the Secrets Within As Max delved deeper into the book, he began to realize that the words were not just mere ink on paper, but key to unlocking the secrets of his own heart. The illustrations seemed to come alive, swirling with colors and textures that danced across the page like kaleidoscope of dreams. The magical spells and incantations whispered secrets in his ear, guiding him toward path of selfdiscovery and empowerment. With each passing moment, Max felt his connection to the book growing stronger, as if the very fabric of reality was weaving itself together to reveal the hidden truths of his own soul. And so, Max embarked on journey of self-discovery, guided by the mystical energies of the book and the wisdom of his own heart. As he explored the depths of his own inner world, he began to tap into reservoir of creativity, courage, and compassion that he never knew he possessed. The boundaries between fantasy and reality blurred, and Max found himself walking the thin line between the two, where the magic of the book merged with the magic within him to create world of limitless possibility. Embracing the Power Within As the days passed, Max grew more confident in his ability to harness the power of magic within himself. He practiced the spells and incantations, feeling the energy coursing through his veins like liquid fire. The world around him transformed, taking on vibrant hue that reflected the colors of his own imagination. Friends and family marveled at the change in him, noticing the way he moved with newfound sense of purpose and confidence. With every step forward, Max felt himself becoming one with the universe, connected to the threads of fate that wove together the tapestry of existence. He understood that the true magic lay not in the book or its words, but in the depths of his own heart, where the spark of creation burned bright and true. And as he looked back on the day he discovered the magical book, Max smiled, knowing that he had unlocked secret that would stay with him forever the secret of embracing the power within himself, and unleashing world of magic upon the world. Next, we utilize the first two-thirds of the extended first half for ICL to extend the initial response. We use gray color here to indicate the ICL segment. Extended response (1071 words) The Boy Who Unlocked His Inner Magic World of Discovery Twelve-year-old Max had always been captivated by the dusty, cobweb-covered trunks and boxes that filled his familys attic. For years, he had spent countless hours exploring the hidden treasures he found there, from vintage toys and antique clothes to mysterious artifacts and forgotten heirlooms. Each new discovery sparked sense of wonder and curiosity within him, fueling his imagination and inspiring his love for adventure. His parents often joked that he had knack for uncovering the most unlikely treasures, and they would smile knowingly whenever he emerged from the attic with some newfound gem. But nothing could have prepared Max for what he was about to discover on that fateful Saturday morning. As he climbed the creaky stairs leading up to the attic, the warm sunlight streaming through the windows illuminated the space, casting golden glow over the familiar landscape of trunks, boxes, and forgotten relics. With spring in his step, Max set out to explore the attic once again, determined to uncover treasure that would surpass all others. The Mysterious Book While rummaging through an old trunk, Maxs fingers brushed against peculiar-looking book bound in worn leather. The cover was adorned with intricate symbols and patterns that seemed to shimmer in the dim light of the attic, like constellation of tiny stars. As he opened the book, faint hum filled the air, and the pages crackled with an otherworldly energy. The musty scent of aged paper wafted up, carrying with it whispers of secrets and mysteries long forgotten. Intrigued, Max carefully turned the pages, revealing illustrations of fantastical creatures, magical spells, and ancient incantations that seemed to leap off the page and dance before his eyes. note on the first page caught his eye, written in elegant script that seemed to shimmer like moonlight: For those who seek the spark within, To unlock the secrets of the heart, Read these words, and listen well, For the magic lies within you, not in this spell. As Max read the words, he felt an unusual tingling sensation in his fingers, as if the book was awakening something deep within him. It was as if the very essence of the book was resonating with his own heartbeat, calling forth part of him that he never knew existed. Suddenly, visions began to flood his mind vivid images of enchanted forests, shimmering stars, and majestic dragons soaring through the skies. The world around him melted away, leaving only the thrumming energy of the book and the promise of untold wonders waiting to be discovered. Unlocking the Secrets Within As Max delved deeper into the book, he began to realize that the words were not just mere ink on paper, but key to unlocking the secrets of his own heart. The illustrations seemed to come alive, swirling with colors and textures that danced across the page like kaleidoscope of dreams. The magical spells and incantations whispered secrets in his ear, guiding him toward path of selfdiscovery and empowerment. With each passing moment, Max felt his connection to the book growing stronger, as if the very fabric of reality was weaving itself together to reveal the hidden truths of his own potential. Over the next few days, Max returned to the attic again and again, devouring the knowledge contained within the book. He practiced the simple spells and incantations, feeling the power coursing through his veins like electricity. At first, it was just gentle buzzing, but soon the sensations grew stronger, and Max started to notice changes within himself. Objects moved with mere thought, levitated small items, and tiny sparks of light flickered to life in his fingertips. His friends at school took notice of the transformation in him, too. They saw him effortlessly solving complex math problems or creating intricate drawings that seemed to come alive on the page. Some whispered among themselves about the magic that had taken hold of their classmate, but Max knew better. He understood that the true source of his newfound abilities lay not in the book, but within himself. The book had merely unlocked door to his own inner potential, allowing him to tap into reservoir of untapped magic that had been slumbering inside him all along. Mastering the Art of Magic As Max continued to hone his skills, he learned to control his powers more effectively, mastering techniques to heal injuries, communicate with animals, and even bend time and space to his will. His confidence soared, and he knew that he was no longer just an ordinary boy; he was wizard, capable of achieving incredible feats. With each passing day, Max became more adept at wielding 18 his powers, channeling them toward the greater good. He used his abilities to help those in need, whether it was calming frightened animal or shielding his family from harm. But with great power comes great responsibility, and Max understood that he must use his gifts wisely. He recognized that the line between good and evil was ever-shifting, and that even the most well-intentioned actions could have unintended consequences. With the weight of this understanding, Max vowed to walk path of compassion and wisdom, using his powers to uplift and protect those around him. Beacon of Hope As Max looked back on the journey that had brought him to this point, he knew that the true magic lay not in the pages of book, but in the depths of his own heart and soul. The experience had taught him that the greatest magic of all was not the ability to cast spells or wield elemental forces, but the capacity to connect with others, to understand their struggles and triumphs, and to offer helping hand when needed. And so, with the magical book safely tucked away, Max embarked on journey to explore the limits of his abilities, to learn from his mistakes, and to grow into the person he was meant to be. He knew that the road ahead would be fraught with challenges and uncertainties, but he also knew that he was ready. For Max had discovered truth that would stay with him forever: the magic within him was not gift, but reflection of the infinite potential that lay within every individual, waiting to be unleashed. After completing one inner iteration, we successfully extended the initial response from 553 to 1071 words. We then replaced the initial response with this extended version and conducted two more iterations. As result, the response grew to 1325 words in the second iteration and reached 1538 words in the final extension. Final extended response (1538 words) The Boy Who Unlocked His Inner Magic World of Discovery In the quaint town of Willowdale, where sun-kissed streets and charming homes blended seamlessly into the lush countryside, twelve-year-old Max had grown up surrounded by the nostalgic charm of his familys ancestral home. Nestled at the top of gentle hill, the house boasted an expansive attic that seemed to stretch on forever, its wooden beams groaning under the weight of generations worth of memories. For Max, the attic was realm of endless fascination, treasure trove of dusty relics and forgotten heirlooms waiting to be unearthed. Over the years, he had developed an uncanny ability to sniff out the most unlikely treasures, earning him the affectionate nickname Treasure Hunter among his friends and family. His parents often marveled at his innate talent for discovery, watching with pride as he delved deeper into the attics labyrinthine corridors, unearthing everything from antique dolls and vintage clothing to rare coins and mysterious artifacts. The thrill of the hunt was intoxicating, and Max reveled in the rush of adrenaline that accompanied each new find. Yet, despite the countless hours hed spent exploring the attic, Max couldnt shake the feeling that there remained one final secret waiting to be uncovered hidden treasure that would surpass all others and reveal piece of the familys history hed never known existed. This sense of longing had become an integral part of Maxs identity, driving him to explore every nook and cranny of the attic with fervor that bordered on obsession. He had scoured the shelves, rummaged through trunks, and even crawled through narrow crawlspaces in search of the elusive treasure. And yet, despite his unwavering dedication, the attic continued to yield its secrets slowly, teasing him with tantalizing glimpses of what might lie just beyond his grasp. That fateful Saturday morning dawned bright and clear, with sunlight spilling through the attic windows like liquid gold. The air was alive with the sweet scent of blooming flowers and the distant chirping of birds, beckoning Max to begin his latest expedition. He climbed the creaky stairs, his footsteps echoing through the silence, and stepped into the attic with sense of anticipation that bordered on excitement. The Attics Hidden Depths As he ventured deeper into the attic, Max found himself navigating maze of narrow pathways and dimly lit alcoves. Cobwebs clung to the corners, casting eerie shadows on the walls, while the faint scent of decay wafted through the air, hinting at the presence of long-forgotten relics. Despite the eerie atmosphere, Max felt an overwhelming sense of belonging, as if the attic had been waiting patiently for him to uncover its secrets. He pushed aside tattered curtain, revealing hidden compartment filled with dusty vases, tarnished silverware, and faded photographs. nearby trunk, adorned with intricate carvings, seemed to whisper secrets in the flickering light, drawing Max closer with an otherworldly allure. The trunks lid creaked open with soft sigh, releasing puff of dust that carried the whispers of the past. The Mysterious Book Amidst the layers of yellowed tissue paper and faded lace, small, leather-bound book lay nestled, its cover exuding an aura of mystique. The book itself was work of art, crafted from supple leather that seemed to glow with an inner light. Delicate patterns etched into the surface shimmered like constellations on clear night sky, drawing Max in with an otherworldly allure. As he lifted the cover, the pages rustled softly, releasing fragrance that was both earthy and ethereal the scent of aged parchment infused with hints of vanilla and sandalwood. The musty aroma transported Max to realm of ancient knowledge, where forgotten wisdom waited to be rediscovered. The pages themselves were marvel, illustrated with fantastical creatures that seemed to spring to life beneath his fingertips. Dragons soared through the skies, their scales glinting like diamonds; unicorns pranced across moonlit meadows, their manes flowing like silk; and phoenixes burst forth from fiery ashes, reborn in blaze of glory. Each illustration was window into world beyond the mundane, realm where magic was woven into the very fabric of existence. note on the first page, penned in calligraphy that shone like moonlight, spoke directly to Maxs heart: For those who seek the spark within, To unlock the secrets of the heart, Read these words, and listen well, For the magic lies within you, not in this spell. As Max read the words, strange tingling sensation coursed through his fingers, as if the book was awakening part of him that had lain dormant for centuries. The room around him dissolved, replaced by visions of enchanted realms and mystical landscapes that unfolded like tapestry before his minds eye. The thrumming energy of the book synchronized with his heartbeat, drawing him deeper into the mystery. Unlocking the Secrets Within With each passing moment, Max became increasingly enthralled by the books secrets, pouring over the pages as if devouring feast. The words on the pages began to take on life of their own, speaking directly to his soul in language that transcended words. The illustrations pulsed with color and texture, transforming into living, breathing entities that swirled around him like maelstrom of creativity. 19 As he delved deeper into the books mysteries, Max realized that the words were not mere ink on paper, but key to unlocking the deepest recesses of his own heart. The book was teaching him that the true source of magic lay not in external sources, but within himself reservoir of untapped potential waiting to be tapped. With each passing day, Max felt the boundaries between reality and fantasy blurring, as if the very fabric of existence was being rewritten to accommodate his growth. Over the course of several days, Max returned to the attic again and again, lost in the world of the book. He practiced the simple spells and incantations, feeling the power coursing through his veins like electric currents. At first, it was gentle buzzing, but soon the sensations intensified, and Max noticed subtle yet profound changes within himself. Objects moved with mere thought, levitating above the floorboards like delicate balloons; small sparks of light flickered to life in his fingertips, illuminating the darkness; and the very air around him seemed to vibrate with an otherworldly energy. The Awakening of Potential As Max harnessed the power of the book, he began to realize that he was not alone in his newfound abilities. His classmates at school took notice of the transformation, sensing change in the way he approached challenges and interacted with the world around him. Some whispered about the magic that had taken hold of their classmate, while others looked on with mixture of awe and trepidation. Max, however, understood that the true nature of his powers lay far beyond the realm of magic tricks and illusions. He comprehended that the energies coursing through his body were manifestation of his own potential, testament to the boundless possibilities that lay within every individual. As he navigated the complexities of adolescence, Max learned to wield his abilities with care and precision, channeling them toward the greater good. Mastering the Art of Magic As Max honed his craft, he mastered techniques to heal wounds, communicate with animals, and even manipulate time and space to his will. His confidence soared, and he realized that he was no longer an ordinary boy; he was wizard, capable of achieving wondrous feats. With each passing day, Max became more skilled at wielding his powers, directing them toward the greater good. He shielded his loved ones from harm, calmed frightened animals, and brought solace to those in need. Yet, with great power comes great responsibility, and Max understood that the line between good and evil was ever-shifting, like the tides of restless sea. Even the most benevolent intentions could lead to unforeseen consequences, and Max vowed to tread path of compassion and wisdom, using his gifts to uplift and protect those around him. Beacon of Hope Looking back on his journey, Max came to comprehend that the true magic lay not in the pages of book, nor in the powers he wielded, but in the depths of his own heart and soul. The experience had taught him that the greatest magic of all was not the ability to conjure spells or command elemental forces, but the capacity to connect with others, to empathize with their struggles and triumphs, and to extend helping hand when needed. With the magical book safely stored away, Max embarked on journey to explore the limits of his abilities, to learn from his mistakes, and to grow into the person he was destined to become. He knew that the path ahead would be strewn with challenges and uncertainties, but he also knew that he was prepared. For Max had discovered truth that would remain with him forever: the magic within him was not gift, but reflection of the boundless potential that resides within every individual, waiting to be unleashed.

