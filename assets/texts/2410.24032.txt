Title: Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks

Authors: Yingzhe Peng, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Xu Yang, Dongmei Zhang, Saravan Rajmohan, Qi Zhang


================================================================================

Abstract
========

The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE's interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE's potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.

Start
=====

4 2 0 2 1 3 ] . [ 1 2 3 0 4 2 . 0 1 4 2 : r Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks YINGZHE PENG, Southeast University, China XIAOTING QIN, Microsoft, China ZHIYANG ZHANG, State Key Laboratory for Novel Software Technology, Nanjing University, China JUE ZHANG, Microsoft, China QINGWEI LIN, Microsoft, China XU YANG, Southeast University, China DONGMEI ZHANG, Microsoft, China SARAVAN RAJMOHAN, Microsoft, USA QI ZHANG, Microsoft, China The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), system designed to enhance personalization in exploratory tasks by combining multi-agent LLM framework with structured user interface. CAREs interface consists of Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In within-subject user study with 22 participants, CARE was consistently preferred over baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CAREs potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration. 1 Introduction Recent advancements in large language models (LLMs) [1, 10, 32] have transformed user interactions with knowledgebased systems by enabling chatbots to synthesize vast amounts of information, surpassing human cognitive limits. While traditional search engines like Bing [4] and Google [16] efficiently address straightforward queries like fact-checking or retrieving specific information, LLM-based chatbots truly shine in guiding users through more open-ended and exploratory tasks [19, 23, 28, 41, 46]. For instance, they can support users in researching emerging scientific fields [26], or planning intricate projects that require synthesizing information [40] from diverse sources and close user engagement. This capability has made LLM-based chatbots invaluable in helping users navigate unfamiliar areas. Despite their extensive capabilities, LLM-based chatbots face challenges in delivering personalized assistance during exploratory tasks [20, 24], particularly when they lack access to user-specific data, such as past interactions and personal preferences. This limitation forces them to rely heavily on user-provided inputs for personalization. In exploratory Work is done during an internship at Microsoft. Corresponding author. Authors Contact Information: Yingzhe Peng, yingzhe.peng@seu.edu.cn, Southeast University, China; Xiaoting Qin, xiaotingqin@microsoft.com, Microsoft, China; Zhiyang Zhang, State Key Laboratory for Novel Software Technology, Nanjing University, China; Jue Zhang, jue.zhang@microsoft.com, Microsoft, China; Qingwei Lin, Microsoft, China; Xu Yang, Southeast University, China; Dongmei Zhang, Microsoft, China; Saravan Rajmohan, Microsoft, USA; Qi Zhang, Microsoft, China. Manuscript submitted to ACM 1 2 Peng et al. Fig. 1. Comparison of the UI and interaction styles between the CARE System and Baseline System. At the top is the CARE System, displaying the Conversation Panel, Solution Panel, and Needs Panel. The CARE System actively prompts the user, gathering their needs before creating tailored plan. In contrast, the Baseline System, shown at the bottom right, features only Chat Panel and tends to provide direct answers to the users queries. tasks, where users often begin with vague queries, generating answers based solely on limited information typically results in generic and impractical recommendations. For example, in travel planning, query like Plan 5-day trip to Hawaii without additional details (e.g., budget, preferred activities, or accommodation preferences) leads to an overly generic response, as shown in the bottom part of Figure 1. This lack of personalization arises either because users may not be aware of the essential details to include or because providing detailed information imposes cognitive load on users. Consequently, the travel plan generated from such vague query would be insufficiently tailored to the users specific needs. While users can refine their requests after receiving an immediate solution from the initial vague query, limitations remain, such as bias toward existing solutions and reduced motivation to explore alternative dimensions, and dissatisfaction is not always resolved despite using specification tactics [24]. In this study, we introduce the Collaborative Assistant for Personalized Exploration (CARE) system to tackle the challenges associated with generating personalized solutions in exploratory tasks. CARE facilitates tailored exploration Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 3 through user interface composed of three key elements (see Figure 1): the Chat Panel, Solution Panel, and Needs Panel, enabling dynamic user interaction and iterative solution refinement. The system operates on multi-agent collaboration framework featuring serveral LLM-driven agents responsible for managing the entire process, from user needs identification to solution generation. These agents collaborate to extract both explicit and implicit user needs, prioritize inquiries, and craft customized solutions. The system also ensures transparency, flexibility, and usability, allowing users to refine their queries iteratively and receive personalized, actionable results. To evaluate the effectiveness of the proposed CARE system, we performed within-subject user study with 22 participants. Each participant completed two exploratory tasks using both the CARE system and baseline LLM-based chatbot. The results revealed clear preference for CARE, with 16 out of 22 participants favoring it over the baseline. Participants praised CARE for reducing cognitive load, inspiring creative exploration, and providing more personalized solutions. Many appreciated how CAREs structured interface helped them organize complex tasks systematically and encouraged them to think about aspects they had not considered. Overall, CARE was seen as more engaging and effective in supporting exploratory tasks compared to the baseline system. Our contribution can be summarized as follows: We introduce collaborative, chat-based interface designed to facilitate users in accomplishing exploratory tasks, particularly when they initially present vague or ambiguous queries due to insufficient knowledge. The interface is powered by an LLM-based multi-agent collaboration system, demonstrating its functionality and effectiveness in aiding users. We conduct comprehensive user study to evaluate the effectiveness of the proposed system, providing valuable insights that can inform the design of future user-centric exploratory systems. 2 Related Work 2.1 Large Language Models for Exploratory Tasks LLM-based human-computer interaction (HCI) systems, such as ChatGPT [31], Copilot [29], and Gemini [15], have expanded the role of AI in supporting users with exploratory tasks. Unlike traditional search engines that excel at handling well-defined queries, LLMs demonstrate notable strengths in assisting with open-ended, complex tasks. Recent work in HCI has explored the integration of LLMs into various domains, including creative workflows [41, 43, 46, 47], help-seeking [21, 36], planning [28, 50], and data exploration [23], revealing the potential of LLMs to facilitate adaptive interactions in complex contexts. Much of this research focuses on advancing prompt-based methods to enhance LLM performance, unlocking reasoning, problem-solving, and planning capabilities [6, 44]. While effective, prompt-based techniques encounter challenges such as inconsistent instruction-following [17, 48, 49], leading to reliability issues in chatbot behavior. These limitations underscore the need for systems that can more effectively manage the complexity and ambiguity inherent in exploratory tasks. Our work builds on this body of research by introducing multi-agent collaboration system specifically designed for exploratory tasks. Task decomposition, method effective in managing complexity, plays central role in our approach. While some systems attempt to address exploratory tasks by prompting single LLM agent, we found that multi-agent architecture offers more rigorous task management and better alignment with user needs. Single-agent system often struggle to maintain focus across multiple responsibilities or lose track of ongoing tasks [24, 46], which can disrupt user progress. By assigning distinct roles to each agent, our system maintains greater control over task decomposition, milestone tracking, and user intent discovery, resulting in more structured and personalized exploratory experience. Manuscript submitted to ACM Peng et al. 2.2 Personalization and Inspiration in LLM-Based Systems Personalization is critical factor in enhancing user experience across AI-driven systems [25, 42]. Many interactive platforms, such as recommendation engines [2, 38] and adaptive learning systems [27, 30], achieve personalization by leveraging user data, including past behavior, preferences, and explicit feedback. However, in the context of LLM-based systems, personalization presents greater challenges, as these models typically lack access to user-specific data unless explicitly provided within the conversation (e.g., via prompts) [20]. Consequently, LLMs often generate responses based only on the limited context of the current interaction, which can result in generic or irrelevant outputs, particularly in exploratory tasks. This passive approach to personalizationwhere LLMs react solely to the users inputs and user follows up with new promptoften leads to incomplete articulations of the users needs, resulting in less comprehensive and relevant responses [22]. This is especially problematic in open-ended, complex tasks where users may not fully express or even realize their own requirements. It is important to note that while adding memory module to LLM-based chatbots can improve personalization by enabling the system to reference prior interactions, it may take significant number of exchanges to gather enough data to meaningfully enhance this personalization. In the early stages of interaction, the chatbot may lack relevant insights, limiting its ability to provide tailored responses. Additionally, for new or unfamiliar tasks, previously stored information may not be useful, as exploratory tasks often require fresh context that past exchanges cannot fully provide. Inspiration is another critical component, particularly in open-ended tasks. Prior research has shown that LLMs can stimulate creativity, assisting users in generating new ideas or perspectives [12, 33]. Unlike studies focused on sparking creativity or employing Socratic questioning in educational contexts [7, 9], our work emphasizes the discovery of users implicit needs and requirements. Rather than merely responding to explicit queries, we employ proactive inquiry techniques that encourage users to think expansively. By actively probing for underlying goals and unarticulated needs, our system helps users consider aspects they may not have initially thought of, ultimately leading to more comprehensive and personalized outcomes. 2.3 User Interface Support for Thinking and Task Organization Traditional chatbots and most LLM-based conversational systems rely on linear, dialogue-based interfaces, generating responses in text stream without exploiting graphical user interfaces (GUIs). However, recent work has begun to explore the potential of graphical interfaces to better support users in complex tasks. For example, Sensecape integrates LLMs into an interactive system to facilitate information foraging and sensemaking [40], while Graphologue converts text responses into interactive diagrams to enhance information-seeking and question-answering tasks [19]. ExploreLLM employs card-based schema to help users structure their thoughts and explore alternative solutions in more organized manner [28]. key challenge with linear text interfaces is the difficulty in locating and recalling relevant information, as important details can become buried within lengthy dialogue streams [14, 24, 34]. Our work contributes to this area by introducing novel conversational interface, which separates the solution display from the dialogue interactions, while incorporating dedicated section for managing user requirements. This structured design reduces cognitive load, enabling users to focus more easily on their tasks and better manage information flow during extended interactions. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 5 3 Motivating Insights from Empirical Studies on Chat-Based Interfaces Our proposed system is motivated by challenges users face when interacting with conventional LLM-based chatbots. review of existing research on systems like ChatGPT [3, 5, 11, 13, 17, 24, 37], highlights key issues that persist in chatbased systems, especially regarding personalization [37], relevance [17], and sustained contextual understanding [3] in complex, open-ended scenarios. Several studies [8, 20, 24, 35] have consistently identified user dissatisfaction stemming from LLMs frequent inability to capture the nuanced intent behind vague or incomplete queries. Users often encounter responses that are generic, irrelevant, or redundant, which diminishes trust in the system, particularly in tasks requiring deeper exploration or personalization. For example, research shows that users seeking localized information about service may struggle when systems fail to interpret location-specific terms or understand personal preferences [50], leading to frustration and ineffective recommendations. This problem is further exacerbated in exploratory tasks, where users themselves may not have well-formed goals and rely on the system to guide them through ambiguity. Moreover, existing work [18, 24, 46, 49] has revealed that many users feel ill-equipped to manage the iterative, multi-turn interactions necessary to refine system outputs in such scenarios. The cognitive load imposed by these systems can be high, particularly when users are required to repeatedly clarify or reframe their inputs. Even with prompt engineering and optimization techniques designed to enhance LLM performance, users still frequently experience dissatisfaction when systems cannot offer personalized assistance based on minimal input or evolving needs. From these insights, several design goals emerge as essential for creating more collaborative chat-based interfaces: Improved Contextual Understanding: System must be capable of maintaining and interpreting user intent over multiple turns, adapting to ambiguous or evolving user inputs to provide more relevant guidance. Enhanced Personalization: System should be designed to tailor responses dynamically, accommodating user preferences and contextual needs, even when the initial input lacks specificity. Facilitation of Inspired Exploration: In addition to providing personalized solutions, the system should inspire users to think implicit needs or aspects, promoting comprehensiveness in open-ended tasks. Reduction of Cognitive Load: To mitigate the frustration associated with managing complex tasks, the interface should provide structure by organizing information effectively and helping users refine their queries. These design goals are informed by extensive literature on user interaction with LLMs, providing clear direction for improving the CARE systems capabilities in supporting personalized, exploratory tasks. 4 CARE: Collaborative Assistant for Personalized Exploration In this section, we describe the proposed system, Collaborative Assistant for Personalized Exploration (CARE), which aims to achieve the design goals derived in the previous section. The overall system architecture of CARE is illustrated in Figure 2, and the subsequent sections elaborate on the user interface and back-end components. 4.1 User Interface As illustrated in Figure 2, the CAREs user interface consists of three main panels: the Chat Panel, Solution Panel, and Needs Panel. Next, we illustrate their functions using travel planning scenario. The Chat Panel functions as the primary interface where users interact with CARE, entering initial queries such as Plan 5-day trip to Hawaii. Through iterative feedback and conversation with the chatbot, users refine their requests, resulting in dynamic multi-turn dialogue. Manuscript submitted to ACM 6 Peng et al. Fig. 2. Overview of the CARE system. The gray area represents the User Interface, where users interact through the Chat, Solution, and Needs Panels. At the bottom, CAREs back-end consists of several agents, including the Inquiry Agent, Needs Discovery Agent, Solution Craft Agent, Milestone Agent, and Ranking Agent, which collaborate to process user inputs and generate personalized solutions. represents user interactions, such as chatting or updating needs. represents the internal data flow between agents. represents that the agents write data to the interface. (cid:100) represents that the agents retrieve data from the interface. As the conversation progresses, CARE generates or updates solution tailored to the users needs, which is displayed in the Solution Panel. To enhance user experience, the solution is presented in Markdown format with rich text features (e.g., tables, emojis) for improved readability. In the travel planning case, the solution might include recommendations for accommodation, transportation, and other essential details, ensuring clarity and engagement. The Needs Panel complements this by explicitly displaying the collected user needs and identifying any points requiring clarification. This allows the system to ground the solution shown in the Solution Panel by linking specific parts of the solution to particular user needs. For example, as shown in Figure 2, the green text of Need ID: 001 in the Solution Panel highlights how parts of the travel plan address needs listed in the Needs Panel, ensuring traceability and Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 7 transparency between user preferences and the generated plan. Additionally, users can modify, add, or delete needs directly in the Needs Panel for greater flexibility. In summary, the CARE interface introduces novel design that separates the solution display in the Solution Panel from the conversational interactions in the Chat Panel, while incorporating dedicated Needs Panel to manage and cross-reference user needs within the solution. 4.2 LLM-Powered Multi-Agent Collaboration System The back-end of the CARE system is powered by an LLM-driven multi-agent collaboration framework, as depicted in the bottom part of Figure 2. It comprises five specialized LLM-based agents: the Inquiry Agent, Milestone Agent, Needs Discovery Agent, Ranking Agent, and Solution Craft Agent. These agents collaborate with the user to progressively manage the solution development across three panels in the UI, discovering implicit needs and generating personalized solutions. While similar systems can be created by prompting single LLM-based agent, we found that multi-agent approach ensures more rigorous task management and better alignment with our design goals. Single-agent models can lose track of tasks or become overwhelmed by managing too many responsibilities. By assigning distinct roles to each agent, we maintain better control over individual components, ensuring consistency in task decomposition, milestone tracking, and user need discovery. Next, we introduce the overall interaction workflow of this multi-agent system, using the same example case of Plan 5-day trip to Hawaii. 4.2.1 Interaction Workflow among Agents. The workflow, shown in Figure 2, is triggered by the query initiated by the user 1 in the Chat Panel. Subsequently, the Inquiry Agent receives the users request 2 and passes it to the Milestone Agent 3 , who assesses if enough user needs are collected to proceed. If more details are needed, new milestone is set 4 , such as understand users hotel preferences. The Needs Discovery Agent then extracts needs from the users input and generates follow-up questions to uncover additional needs that not mentioned by user 5 . The Ranking Agent organizes these questions, grouping and prioritizing them 6 before they are presented to the user. These questions are then passed back to the Inquiry Agent, who presents them to the user in an intuitive manner 7 . As the user responds, their answers are populated into the Needs Panel. This process continues iteratively until the Milestone Agent confirms that the collected needs are sufficient, prompting the Solution Craft Agent to generate personalized solution 8 , which is then presented in the Solution Panel. Upon receiving the solution, the user can choose to refine the solution in two ways. They can directly communicate new preferences to the Inquiry Agent, which may trigger additional needs discovery steps, or if the preference is straightforward, the Milestone Agent will pass the new feedback directly to the Solution Craft Agent for immediate integration into revised solution. Alternatively, the user can manually modify, add, or delete items in the Needs Panel. Any such updates will prompt the Solution Craft Agent to generate new solutions based on the latest needs. The above workflow illustrates how the agents collaborate to deliver solutions tailored to users preferences while facilitating comprehensive needs discovery process. The following sections describe each agent in detail. 4.2.2 Inquiry Agent. The Inquiry Agent is designed to facilitate smooth user interactions by gathering and clarifying user needs in an intuitive manner. One of its key responsibilities is refining the questions generated by the Needs Discovery Agent before they are presented to the user. This process involves simplifying the language of questions and providing default options, which significantly reduces the users cognitive load. For instance, instead of presenting broad question like, What kind of accommodation do you prefer?, it offers more focused choices, such as Hotel or Airbnb?, thereby streamlining the decision-making process for users. Manuscript submitted to ACM 8 Peng et al. In addition to refining questions, the Inquiry Agent actively assists users who may struggle with specific questions or express uncertainty. It provides pertinent explanations or definitions, such as explaining Airbnb, enabling users to make well-informed decisions. Moreover, once the user responds, the Inquiry Agent meticulously records the answers to the Needs Panel, ensuring that all user preferences are accurately captured for future stages of the process. 4.2.3 Milestone Agent. The Milestone Agent plays the strategic coordinator role in CARE by determining the next critical steps (i.e., milestones) based on the existing users input and collected needs, ensuring that all necessary user needs are gathered before moving forward with solution generation. The Milestone Agent has two primary responsibilities. First, if it determines that more specific user needs are required or the user requests an improvement on the existing solution, it sets the next milestone. This milestone typically involves collecting missing or incomplete information necessary for building comprehensive solution. The Milestone Agent uses the current Needs Panel and input from the user to establish these milestones, ensuring that they are specific, actionable, and contribute directly to solving the users problem. It references previously established milestones to avoid redundancy and ensures each new milestone is unique and focused on specific, measurable outcomes. Additionally, the agent carefully considers dependencies between tasks, breaking down complex problems into manageable milestones that the CARE system can address step by step. Second, if the recorded user needs are sufficient, the Milestone Agent notifies the Solution Craft Agent to begin generating solution. At this stage, no new milestones are created, and the process transitions to solution generation. This decision is based on whether the current Needs Panel fully addresses the users query. In cases where the user manually updates their needs via the user interface, the Milestone Agent immediately passes this information to the Solution Craft Agent, triggering plan update without setting new milestone. This ensures flexibility in how the CARE system adapts to user input and allows for rapid response to direct feedback. 4.2.4 Needs Discovery Agent. The Needs Discovery Agent serves vital function in identifying and documenting user needs, both explicit and implicit, throughout the interaction with the system. This process involves two critical tasks that ensure comprehensive coverage of user Needs. First, the agent is responsible for extracting explicit needs directly from the users input. Explicit needs are those that the user clearly articulates. For instance, if user expresses desire to Plan 5-day trip to Hawaii, the agent will identify and extract specific needs, such as the destination is Hawaii and the trip duration is 5 days. These explicit needs are systematically recorded in the Needs Panel to ensure they are fully documented, making it simple for users to review their own needs, while also allowing the CARE system to further develop personalized solutions. Second, the agent plays an essential role in identifying implicit needs, which are often unspoken but critical to achieving the current milestone. Implicit needs are inferred based on the chat history and the specific demands of the milestone. For example, when the task involves selecting hotel, the agent may infer preferences such as whether the hotel needs to be close to popular attractions or if the user has specific needs for certain amenities, such as gym, swimming pool, or free Wi-Fi. By anticipating these needs, the agent ensures more complete understanding of the users needs. Once the agent has completed its inference, it generates clarification questions to further refine the users preferences. These questions may include, for example, whether the user has specific requirements for the accommodations location or any preferences regarding hotel amenities. The generated questions are then added to the Needs Panel, although invisible on the UI to avoid user distraction. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 9 4.2.5 Ranking Agent. The Ranking Agent brings structure and order to the system by organizing and prioritizing the clarification questions identified during the user needs discovery process. Instead of overwhelming users with disjointed list of queries, the agent groups related questions thematically, such as accommodation preferences or budget constraints, allowing the user to focus on one topic at time. This approach not only minimizes cognitive overload but also keeps the interaction flowing smoothly. Once the questions are grouped, the Ranking Agent further arranges them in logical sequence. It starts with simpler, more general inquiries, then gradually progresses to more detailed, specific questions. By controlling the pacing and order of the interaction, the agent ensures that users can answer with confidence and clarity, without being rushed or overwhelmed. This results in seamless experience where the user is led through the interaction with ease, while all necessary information is gathered efficiently. 4.2.6 Solution Craft Agent. The Solution Craft Agent is central to transforming user needs into concrete, personalized solutions. This agent excels at turning complex sets of needs into clear, actionable outcomes. After analyzing the users preferences and constraints, it organizes the solution in way that is both transparent and user-friendly. Each solution is neatly structured, with headings and concise explanations, making it easy for users to follow and understand how their specific needs are being addressed. By linking every recommendation to particular need, the agent ensures that users can see exactly how their input shapes the proposed solution. Personalization is another key strength of the Solution Craft Agent. Rather than offering generic solutions, it tailors each recommendation to align with the users context, preferences, and limitations. Whether suggesting accommodations that fit within specific budget or transportation options that align with the users schedule, the agent ensures that the solution is relevant and practical. This emphasis on customization guarantees that each user receives solution that is uniquely suited to their situation, making the final outcome both useful and highly personalized. 4.3 Implementation Details The CARE system is web-based application featuring UI developed with Streamlit [39] and back-end multi-agent system constructed using AutoGen [45]. Additionally, we implemented conventional LLM-based chat interface by directly prompting the base LLM, which serves as the baseline system for our user study. Both CARE and the baseline systems utilize GPT-4o as the underlying LLM, with all corresponding prompts detailed in Supplementary Material. 5 User Study To minimize the effects of participant variability, we conducted within-subject user study with 22 participants, comparing CARE to baseline LLM-based chatbot as described above. Each participant completed two exploratory tasks, travel planning and skill learning, using both systems. To control for order effects, we counterbalanced the order of the systems and tasks, resulting in four settings. The study aimed to address the following research questions (RQs): RQ1. How does the CARE system compare to the baseline system in supporting users in exploratory tasks in terms of interaction experience and user satisfaction? RQ2. How does the CARE system provide personalized and inspired solutions compared to the baseline system? RQ3. How does the CARE systems thoughtful interface design help reduce cognitive load during complex, open-ended tasks, compared to the baseline system? Manuscript submitted to ACM 10 5.1 Procedure Peng et al. Each study session began with an introduction to both systems, during which participants received basic instructions on how to interact with the systems (e.g., explanation on UI layout, reminder on no web search function). Participants then completed demographic questionnaire (Section A.2) detailing their background and prior experience with LLM-based chatbots. Each participant completed two exploratory tasks using both systems. We encouraged participants to spend 5-10 minutes on each task. After interacting with each system, participants filled out post-task questionnaire (Table 1, questionnaire format in Section A.3) to evaluate their experience. Throughout the session, participants were encouraged to think aloud. Follow-up interviews (Section A.1) were conducted at the end of each session to gather additional qualitative insights, particularly focusing on any notable highlights or frustrations with each system. Each session lasted approximately one hour. The study was conducted in person, and participants were compensated with cash equivalent to two hours of the local minimum wage in appreciation of their time. 5.2 Data Collection and Analysis To evaluate how effectively CARE supports users in exploratory tasks, we designed comprehensive questionnaire  (Table 1)  focusing on several key aspects of the user experience: Interaction, Cognitive Load, Inspiration, Comprehensiveness, and Personalization. During each study session, we recorded participants screen activities and audio to capture their think-aloud verbalizations, allowing us to gather qualitative insights alongside quantitative data. For the quantitative analysis, we employed combination of statistical methods, including paired t-tests and Pearsons Chi-Square tests, to assess the significance of our findings. The paired t-test allowed us to determine whether there was statistically significant difference in user ratings between CARE and the baseline system. The Chi-Square test was used to examine categorical differences in participant responses across various dimensions of the user experience. To provide further context for the quantitative results, we analyzed the think-aloud data and post-task interviews using an inductive qualitative approach. This allowed us to probe into the reasons behind the participants ratings and explore their subjective experiences with the two systems. The qualitative feedback was instrumental in corroborating the experimental findings and providing deeper insight into how CARE facilitated user exploration, reduced cognitive load, and supported creativity. Measure Statement (5-Point Likert Scale) Interaction Cognitive Load Inspiration Comprehensiveness Q4. The answers provided by the system feel comprehensive and sufficient to me. Personalization Q1. enjoy the way interact with the system. Q2. The systems UI helps me organize complex tasks and reduces my cognitive load. Q3. Interacting with the system inspires me to consider new aspects of exploratory tasks. Q5. The answers provided by the system match my personal needs. Table 1. Post-task questionnaire filled out by participants after they interacted with two systems, one with CARE and the other with the Baseline. Each statement was rated on 5-point Likert scale (the larger the better). 5.3 Participants We recruited 22 participants (4 female, 18 male) through call for participation distributed via the universitys mailing list, as well as through word-of-mouth referrals. Participants represented diverse range of academic disciplines, Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 11 Fig. 3. Comparative analysis of user responses to the CARE and baseline systems across five key aspects of user experience. including Computer Science (9), Engineering (8), Natural Sciences (2), Social Sciences (2), and Arts (1). Their educational backgrounds varied, with 4 undergraduates, 17 masters students, and 1 PhD student. All participants had prior experience with conversational LLM-based systems, such as ChatGPT, but their frequency of use varied. Seven participants reported using such systems less than once per week, five used them several times per week but not daily, and ten were frequent users who interacted with LLM-based chatbots multiple times per day. 6 Quantitative Results The binary choice results, collected through post-task interviews, reveal clear preference for CARE over the baseline system, with 16 out of 22 participants favoring CARE. This finding aligns with the quantitative data from the five survey questions listed in Table 1, as depicted in Figure 3. When averaging scores across all five questions, CARE was rated significantly higher (𝑡 = 3.07, 𝑝 = 0.0058), with an average score of 4.20 (𝜎 = 0.78) compared to the baselines 3.25 (𝜎 = 0.91). Cohens 𝑑 = 1.12 also illustrates the considerable difference in perceived effectiveness, highlighting the participants clear preference for CARE. We now turn to the detailed statistical analysis of each survey question. Overall Interaction Experience. The Pearson Chi-Square analysis (𝜒 2 (4) = 5.17, 𝑝 = 0.27) indicates no significant difference in overall satisfaction between the two systems in Q1. However, larger proportion of participants rated CARE favorably in the Agree and Strongly Agree categories, with 17 out of 22 participants finding CARE engaging, compared to only 10 for the baseline. This pattern, though not statistically significant, suggests that CARE might provide better interaction experience for the majority of users. Reducing Cognitive Load. According to the results in Q2, users perceived that CARE significantly reduced cognitive load in organizing and managing complex tasks compared to the baseline (𝜒 2 (4) = 19.04, 𝑝 = 0.001). Notably, 13 participants strongly agreed that CARE was more systematic in presenting and organizing information, compared to only one participant in the baseline system. Manuscript submitted to ACM Peng et al. Inspiring New Ideas. The Chi-Square result for Q3 (𝜒 2 (4) = 14.21, 𝑝 = 0.007) demonstrates statistically significant difference between the two systems. Notably, 14 participants strongly agreed that CARE inspired them to generate new ideas or explore alternative perspectives during the interaction, compared to only 5 participants in the baseline system. This demonstrates that CARE encourages more exploratory and creative interaction compared to the baseline. Solution Comprehensiveness. The Chi-Square analysis of Q4 (𝜒 2 (4, 𝑁 = 22) = 5.72, 𝑝 = 0.221) reveals no significant difference between the two systems in terms of solution comprehensiveness. This outcome is encouraging, as it implies that CARE offers tailored solutions without compromising the comprehensiveness of the responses. Solution Personalization. CARE showed notable advantage (𝜒 2 (4) = 8.27, 𝑝 = 0.082) in terms of solution personalization (Q5), with 19 participants agreeing that CAREs plans were more personalized, in contrast to 11 for the baseline system. While the 𝑝 value did not meet the conventional threshold of 0.05, it suggests trend toward significance, indicating potential for future enhancements to the CARE system. The above quantitative analysis highlights clear preference for CARE over the baseline system, with most participants favoring CARE in terms of effectiveness and user experience. While not all survey questions showed statistically significant differences, CARE consistently outperformed the baseline in reducing cognitive load, inspiring new ideas, and providing personalized solutions. These findings set the stage for deeper exploration of qualitative insights from the post-task interviews, where participants nuanced perspectives on their interaction with CARE will be discussed. 7 Findings The findings from our user study are organized around addressing the three research questions listed in Section 5, providing insights into the strengths of the CARE system compared to the baseline. First, we explore how CAREs inquiry process supports exploratory tasks by fostering deeper exploration, ensuring comprehensiveness, and delivering personalized outputs, aligning with RQ1 and RQ2. Next, we examine how CAREs collaborative workspace effectively organizes user needs, generated solutions, and chat interactions, contributing to reduced cognitive load and improved task performance, directly addressing RQ3. These findings collectively highlight the key benefits of CAREs design in enhancing user experience and productivity. 7.1 CAREs Inquiry Process Inspires Exploration, Ensures Comprehensiveness, and Personalizes Outputs 7.1.1 CAREs Inquiry Process Inspires Users by Uncovering Implicit Needs. Through our investigation, we found that CARE effectively uncovers users implicit needs by encouraging them to think broadly and respond to proactive inquiries. During the interviews, we further identified that users implicit needs can be divided into two distinct categories: hidden needs and latent needs. Hidden needs refer to those needs that users are aware of but do not explicitly express, while latent needs are those that users do not recognize until they are prompted by CAREs exploratory questions. CARE effectively inspires users to consider both hidden and latent needs. Specifically, most participants (17/22) indicated that CAREs questioning strategy encouraged them to think about aspects they would not have otherwise considered. For instance, P7 highlighted how CARE helped him uncover hidden needs while planning trip: For example, when planning trip, it (CARE) would ask me if prioritize sightseeing, food, or shopping, and if have preference, it can better tailor the itinerary for me. This kind of questioning reveals needs that the user might be aware of but did not communicate explicitly. On the other hand, P19 emphasized how CARE brought to light latent needs during the process of creating baseball training plan: think System (CARE) inspired me lot. For example, when wanted to start playing baseball, it asked about the coachs style and how much time wanted to trainespecially questions about the Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 13 coachs style, which had never thought about before. These examples demonstrate how CAREs inquiry process guides users to explore crucial factors they initially overlooked but which are critical to making more informed decisions. In contrast, participants found that the baseline system reacted passively, only responding to explicit inputs without offering additional considerations. P11 pointed out: (Baseline) only answered what asked, but (CARE) asked about things like my budget and dietary preferences, which hadnt thought about. This passive approach often led to incomplete articulation of the users needs, resulting in less comprehensive and relevant output. 7.1.2 CAREs Milestone-Driven Approach Improves User Focus And Personalizes Task Progression. To ensure that CARE continuously focuses on the users current needs, tasks are organized into sequential milestones by Milestone Agent, guiding other agents in the system step by step through the planning process. Each milestone represents distinct phase of progress, enabling agents to focus on one task at time while maintaining coherent long-term strategy. As users provide feedback or refine their goals based on the current milestone, the system dynamically updates the plan or generates the next milestone, ensuring continuous alignment with the users evolving needs. This milestone-driven approach was noticed and appreciated by participants (10/22). P20 described this as, It (CARE) asked questions that helped me break down big problem into smaller, more systematic tasks. This way, could understand my needs better, and the system could offer more personalized settings. Similarly, P8 mentioned, The system (CARE) would update after each step, and could see how my decisions affected the plan, which helped me stay on track. The baseline system, however, lacked this granular task breakdown. Without the ability to iteratively refine and adjust based on continuous feedback, users found it difficult to maintain focus on evolving goals. P15 remarked, With the another system (Baseline), often felt lost trying to manage everything at once. 7.1.3 Comprehensive Inquiry Process Ensures More Detailed, Structured, and Complete Solution. CARE ensures that users needs are comprehensively covered through systematic inquiry process during interactions. This comprehensiveness is reflected in two dimensions: the comprehensiveness of the inquiry process and the comprehensiveness of the generated solution. The former refers to the systems ability to cover multiple aspects of the users needs through its multidimensional questions, while the latter pertains to the final solution being more structured and complete. Some participants (13/22) expressed preference for this proactive, multi-dimensional inquiry-driven interaction style. For example, P12 noted: (CARE) asks follow-up questions based on your answers and helps fill in things you might not have thought about. think its very comprehensive. Similarly, P4 highlighted CAREs ability to expand single question into multiple dimensions, noting: (CARE) is suitable for general users because many times you only have one question in your mind, but it can help you break it down into various aspects that need to be considered. Moreover, CARE incorporates Ranking Agent, which prioritizes questions to allow users to first answer simpler, foundational questions, thereby reducing cognitive load during interactions. This design ensures that questions are presented in structured sequence, avoiding users being overwhelmed by excessive information at once. As P8 stated: Its (CAREs) interactive, and it asks simple questions, like multiple-choice or short questions. felt relaxed while interacting with it, and it was really good at recognizing my intent and adding that to the list of needs. This level of comprehensiveness during interaction was lacking in the baseline system. Users had to think through the task and provide as much information as possible upfront by themselves, and the system did not proactively ask about additional dimensions of the task. Some participants (8/22) expressed frustration with this limitation. P21 explained: With chatbot (Baseline), had to ask more follow-up questions myself to complete the plan. But sometimes, might forget to ask something important or miss key detail. Manuscript submitted to ACM 14 Peng et al. As result of the comprehensiveness of the inquiry process, most participants (16/22) found that solutions generated by CARE were more comprehensive, although not prominent in the quantitative analysis. significant advantage of CARE, as noted by participants, was its ability to guide users through various dimensions of their task, ensuring that no critical aspects were overlooked. For instance, P16 shared: (CARE) really helped me think through every aspect of my travel plans. It wasnt just basic itinerary; it considered things like how much free time would have, what activities were nearby, and even suggestions for restaurants based on my budget. Moreover, the structured organization of CAREs generated solutions helped users follow and trust the planning process more easily, further enhancing their perception of the solutions thoroughness. P19 noted that CAREs step-by-step breakdown of tasks made the final solution more coherent: As (CAREs) plan was laid out in clear steps, and it felt more complete because could see how each decision built on the previous one. It wasnt just list of things to do; it was logical progression that covered everything. In contrast, participants (7/22) noted that the baseline system often delivered less detailed plans, particularly due to its reactive nature. P12 described an instance where the baseline plan failed to include specific logistical considerations: With (Baseline), it felt like was getting quick summary rather than full plan. It didnt take into account things like transportation between activities, so had to figure that out on my own. 7.1.4 Personalization Through Tailored Inquiry Results in More User-Specific Solutions. CARE generates highly personalized plans by continuously adjusting its output based on users specific needs through dynamic questioning. Specifically, CAREs inquiries focus on clarifying vague requests by asking about personalized preferences or needs, such as preferred activity types or priorities. This not only helps users articulate their preferences more clearly but also ensures that the system tailors its output accordingly. The flexibility and specificity of the generated solution made users feel that the system was responsive to their input, enhancing their satisfaction. P10 highlighted this by explaining, (CARE) kept asking questions that hadnt even thought about, like whether wanted to prioritize certain skills or what time of day preferred for training. This really helped make the final plan feel like it was made just for me. The baseline system often fell short in this area as it relied on users to provide all necessary input upfront without prompting further clarification. As result, the solutions generated by the baseline system were often less personalized. P9 commented, (Baseline) didnt ask any follow-up questions after gave my preferences. It just generated plan that was okay, but it didnt really reflect everything wanted. This feedback underscores how baselines limited engagement led to less tailored solutions, resulting in solutions felt generic. Even though occasionally the baseline system encouraged users to follow up with additional inputs, long-form answers could still lead to missing crucial personalization elements. For instance, P13 noted: For my fitness training task, the plan from (Baseline) didnt match what wanted at first. had to follow up and ask for revisions, but even after updating, it still missed some details that (CARE) had caught because asked very specific questions from the start. This illustrates that even when users try to supplement their preferences with follow-up inquiries in the baseline system, the lack of thorough questioning from the outset could result in oversights. CAREs adaptability during the interaction process further enhanced its ability to personalize plans. P8 appreciated how the system dynamically adjusted based on real-time inputs: With (CARE), could tweak my needs as we went along, and it would adjust the plan. That made me feel like had control over the outcome and that the plan was really made for me. This iterative refinement process ensured that the final plan was more accurately tailored to the users evolving preferences, providing clear advantage over the more static baseline system. 7.1.5 CAREs Inquiry Process May Occasionally Cause Friction for Some Users. While some participants (9/22) explicitly expressed that the CARE systems structure made it better suited for guiding users through complex tasks, with P20 Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks noting: (CARE) helps break down my larger tasks into smaller, more manageable parts, making it easier to navigate and ensure all details are covered, some participants (4/22) commented that the number of questions felt excessive at times. For instance, P13 noted: (CARE) sometimes provided new information, and while some of it was helpful, also felt that when it kept asking about things didnt really care about, wasnt sure how to answer. This feedback suggests that users who prefer to resolve issues quickly may find the step-by-step inquiry process to be source of mild friction. 7.2 CAREs Collaborative Workspace Effectively Organizes User Needs, Generated Solutions, and Chat Interactions, thereby Reducing Cognitive Load One of the most frequently mentioned advantages of the CARE system during interviews was its well-organized UI. Many participants (14/22) noted that compared to the baseline, CAREs UI significantly improved their overall experience by reducing cognitive load and making interactions more intuitive. Specifically, users appreciated the structured layout, which clearly separated their needs from generated solutions and system interactions. This organization helped them stay focused on their tasks and reduced the effort needed to manage the flow of information during extended interactions. In the following sections, we will delve deeper into how specific features of CAREs UI, such as the Needs Panel and Solution Panel, contributed to these advantages. 7.2.1 Panel Separation Improved Clarity and Reduced Cognitive Load. The division of the Chat Panel, Solution Panel, and Needs Panel within CARE significantly enhances clarity and minimizes users cognitive load. Isolating the generated solution from the chat eliminates the need to sift through previous messages, while the distinct presentation of needs keeps them readily visible and adjustable. This design allows users to keep track of their evolving objectives without being overwhelmed by unrelated content. Several participants appreciated how the clear separation of chat and solution output streamlined their interaction with the system. For example, P14 remarked: It (CARE) asks questions on the right side and outputs what need on the left, so dont need to search through the conversation to find the result. This clear division between the chat interactions and the solution outputs allowed users to locate relevant information effortlessly, minimizing the cognitive effort associated with scrolling back through chat histories. The Needs Panel, in particular, proved to be critical feature in reducing mental load. It enabled users to track and update their requirements in real time without revisiting previous conversations. P8 noted, As (CAREs) needs list is clearly displayed, making it easy to see and update my requirements. This visualization of needs provided users with constant, structured overview, reducing the effort required to recall or modify their requests. Similarly, P21 highlighted how the separation of the Needs Panel from the chat log made it easier to manage goals: Having the needs separate from the chat makes it easier to verify what Ive said and what want. This dedicated space for evolving goals ensured that critical information was not overlooked, providing users with confidence that their inputs were being considered. Moreover, this clear organization reduced the cognitive burden associated with planning tasks. P19 shared: As (CAREs) table makes everything much clearer, so dont have to keep everything in my head or scroll back to find important details. By visually organizing task details in the Needs Panel, CARE allows users to focus on decision-making rather than on recalling details, making the interaction less mentally demanding. In contrast, the unstructured chat format of the baseline system led to frequent cognitive strain for some participants (6/22). The unstructured chat format made it difficult to locate and recall information, requiring constant scrolling. As P16 described: With (Baseline), have to scroll up and down constantly to find the information. This lack of clear Manuscript submitted to ACM 16 Peng et al. organization increased users mental load, making the process more tedious and hindering their ability to maintain an overview of their plans. 7.2.2 User Needs Panel Simplified Interaction and Built Trust Through Transparent Solution Mapping. The CARE systems Needs Panel emerged as key feature that simplified interaction and enhanced user trust by providing clear, real-time display of user requirements and linking those needs transparently to the generated solutions. The Needs Panel was recognized by several participants (10/22) for providing real-time display of user requirements, enabling them to manage preferences efficiently throughout the interaction. For instance, P14 explained: There is Needs Panel where can directly input my needs, and dont have to worry about the AI missing any of them. This dynamic interaction was further supported by the real-time updating of needs, which allowed users to adapt their goals on the fly, improving the personalization of the outcomes. P17 highlighted this benefit: As (CAREs) ability to update my needs in real-time led to more accurate and personalized plans. In contrast, (Baseline) would often forget or shift focus after multiple conversations, making it harder to track everything. Moreover, CAREs user needs reference mechanism further reinforced user trust by linking specific parts of the generated solutions back to the corresponding user needs, as shown by the linkage of the Needs Panel and Solution Panel in Figure 2. Several participants (7/22) appreciated how this transparency reassured them that their input was being accurately incorporated. P21 commented, The need references in (CARE) are very useful because they show me which parts of the plan were based on my input, so know exactly how my needs were considered. This clear mapping made it easier for users to understand and adjust the solutions, fostering more engaged and confident interaction. As P14 noted, With (CARE), could always see which parts of the generated solution were directly related to what had asked for, which made it much easier to make adjustments. Participants consistently highlighted that this transparent connection between user needs and solution mapping increased their trust and sense of ownership. P8 observed, Seeing how my requirements were implemented step-by-step in (CARE) gave me lot of confidence that the system was understanding and working with my needs accurately. In contrast, the baseline system, lacking similar mechanism, often left users uncertain about how their input was utilized, leading to frustration. P22 expressed: In (Baseline), wasnt always sure if the system really understood my needs, since there was no way to see how my input was reflected in the final output. Overall, the combination of CAREs Needs Panel and user needs reference features simplified the user experience and built trust by providing transparent and tangible link between user input and system output. This transparency not only improved user confidence but also empowered them to make more informed and precise adjustments, resulting in more reliable and user-centric planning process. 7.2.3 Enhanced Readability of the Solution Panel through Structured and Rich-Text Formatting. The structured presentation in CAREs Solution Panel, using Markdown elements such as tables, rich text, and icons, was highlighted by participants as key factor in enhancing readability. Several users (6/22) appreciated the structured format, particularly in contrast to the unstructured chat format of the Baseline system. P2 noted, As (CAREs) tables, icons, and emojis made everything much more organized and readable. If it were only text, might not continue the conversation after few exchanges. This organized layout helped users understand and follow the systems generated plans with less cognitive effort. P16 similarly highlighted the benefits of the table-based layout: (CARE) puts everything into tables, which makes my schedule much clearer. With (Baseline), constantly have to scroll to find the information. This layout made navigating complex information more straightforward, reducing the need for excessive scrolling and minimizing cognitive overload. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks In summary, the CARE systems organized UI reduces cognitive load by clearly separating user needs, solutions, and chats. Features like the Needs Panel and Solution Panel provide real-time clarity, helping users manage tasks efficiently and with less effort. This structured design enhances ease of use, trust, and reduces the strain of managing information compared to the baseline system. 8 Discussion 8.1 Broader Implications for Collaborative Chat-Based Interfaces The findings from our study provide several important insights into designing more effective, collaborative chat-based systems, particularly for exploratory tasks. key takeaway is the need to shift from reactive, dialogue-based interactions to more proactive and structured approach, as demonstrated by the CARE system. CAREs ability to reduce cognitive load and support more meaningful engagement with users stems from its structured interface and adaptive collaboration, setting it apart from traditional, free-flowing conversational agents. One of the broader design principles emerging from our study is the separation of conversational inputs from task management. Many LLM-based systems rely heavily on an undifferentiated, continuous dialogue format, which can overwhelm users and make it difficult to organize and track complex tasks. In contrast, CAREs dual-panel approachdistinguishing between conversational inputs (Chat Panel) and structured outputs (Solution Panel)proves highly effective in managing information. By separating these components, users can focus on task progression without being bogged down by unnecessary dialogue or clutter. dedicated Needs Panel, designed to ensure traceability and transparency between user inputs and system outputs, enhances user confidence and trust in the system. This suggests that future chat-based systems could benefit from integrating multimodal interaction spaces that visually differentiate between ongoing conversation, system outputs, and task organization. Such interfaces could vastly improve user experience by mitigating information overload and making it easier to manage evolving, multi-step processes. Furthermore, CAREs design prioritizes adaptive collaboration. Unlike conventional systems that passively respond to user inputs, CARE actively inquires about user needs, offers suggestions, and encourages exploration. This interaction model fosters creativity and supports personalization, particularly in open-ended tasks where users might not have well-defined goals. By dynamically adjusting to users evolving needs and preferences, CARE transforms the interaction from one of mere conversational efficiency to an ongoing, exploratory dialogue. This agent-driven approach could be particularly valuable in domains such as complex decision-making, creative processes, or long-term project management. For the broader HCI community, the CARE system exemplifies how conversational agents can evolve from static responders into active collaborators, promoting more exploratory and creative problem-solving. Additionally, our findings reveal that the CARE system helps uncover implicit user needs by inspiring users to think beyond their initial queries. Rather than just providing direct answers, CARE prompts users to consider alternative perspectives and explore unanticipated paths, creating more enriching interaction. This design feature could inform future work on intelligent agents, which need to balance guidance and autonomy. Systems that empower users to extend their thinking without overwhelming them represent significant opportunity for future conversational interfaces, particularly in areas like education, content creation, and collaborative work environments. The broader implication for HCI is shift towards systems that dont just provide information but actively support users in navigating uncertainty, promoting discovery, and enhancing creativity. Manuscript submitted to ACM 18 Peng et al. 8.2 Limitations and Future Work While our study demonstrates CAREs potential in improving exploratory tasks, it has limitations that future research should address. CAREs response latency, inherent to its multi-agent design, poses challenge. While the system excelled in reducing cognitive load and enhancing personalization, the added response time could affect user satisfaction. Advancements in LLM technology will help reduce these delays. Another limitation is the relatively small and homogeneous participant pool, which may affect the generalizability of our findings. Future studies should recruit more diverse participants to capture broader range of user needs, preferences, and interaction patterns. Lastly, our study used GPT-4o, one of the most advanced LLMs at the time. Future research should investigate whether the findings generalize across other LLM implementations, especially as new, more efficient models emerge. Exploring alternative interaction modalities like voice or gesture could also further enhance the systems capabilities. 9 Conclusion In this paper, we introduced CARE, collaborative chat-based interface designed to enhance personalized exploratory tasks by addressing key limitations in existing LLM-based systems. Motivated by challenges such as the lack of sustained contextual understanding and personalization in current LLM-based chatbots, we developed system that reimagines user-LLM interaction through structured and adaptive collaboration. Our design goals focused on improving contextual understanding, facilitating personalization, inspiring user exploration, and reducing cognitive load. Through user study comparing CARE with baseline LLM chatbot with 22 participants, the results demonstrated that CARE outperformed the baseline system in several key areas, notably in inspiring user exploration, personalizing responses, and organizing information effectively. These findings have broader implications for the design of future chat-based interfaces. By empowering users to navigate ambiguity and supporting them in complex tasks, systems like CARE have the potential to transform how users engage with conversational agents, making them not just reactive tools, but proactive partners in problem-solving and discovery. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, and Sujay Kumar Jauhar. 2024. Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW 24). Association for Computing Machinery, New York, NY, USA, 33553366. https://doi.org/10.1145/3589334. [3] Morteza Behrooz, William Ngan, Joshua Lane, Giuliano Morse, Benjamin Babcock, Kurt Shuster, Mojtaba Komeili, Moya Chen, Melanie Kambadur, Y-Lan Boureau, and Jason Weston. 2023. The HCI Aspects of Public Deployment of Research Chatbots: User Study, Design Recommendations, and Open Challenges. arXiv preprint arXiv:2306.04765 (2023). [4] Bing. 2024. Bing. https://www.bing.com/. Accessed: 2024-10-10. [5] Ali Borji. 2023. Categorical Archive of ChatGPT Failures. arXiv preprint arXiv:2302.03494 (2023). [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, Article 159, 25 pages. [7] Edward Y. Chang. 2023. Prompting Large Language Models With the Socratic Method. In 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC). 03510360. https://doi.org/10.1109/CCWC57344.2023.10099179 [8] Yang Deng, Wenqiang Lei, Wai Lam, and Tat-Seng Chua. 2023. Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, Edith Elkind (Ed.). International Joint Conferences on Artificial Intelligence Organization, 65836591. https://doi.org/10.24963/ijcai.2023/738 Survey Track. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 19 [9] Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, and Liang He. 2024. Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching. arXiv preprint arXiv:2407.17349 (2024). [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [11] Luciano Floridi. 2023. AI as Agency Without Intelligence: on ChatGPT, Large Language Models, and Other Generative Models. Philos. Technol. 36 (2023), 15. https://doi.org/10.1007/s13347-023-00621-y [12] Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks: Inspiration for Science Writing using Language Models. In Proceedings of the 2022 ACM Designing Interactive Systems Conference (Virtual Event, Australia) (DIS 22). Association for Computing Machinery, New York, NY, USA, 10021019. https://doi.org/10.1145/3532106.3533533 [13] Sukhpal Singh Gill and Rupinder Kaur. 2023. ChatGPT: Vision and challenges. Internet of Things and Cyber-Physical Systems 3 (2023), 262271. https://doi.org/10.1016/j.iotcps.2023.05.004 [14] Charles Goodwin. 2015. Professional Vision. Springer Fachmedien Wiesbaden, Wiesbaden, 387425. https://doi.org/10.1007/978-3-531-19381-6_20 [15] Google. 2023. Google Gemini. https://gemini.google.com. Accessed: 2024-10-10. [16] Google. 2024. Google. https://www.google.com/. Accessed: 2024-10-10. [17] Myeongjun Jang and Thomas Lukasiewicz. 2023. Consistency Analysis of ChatGPT. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 1597015985. https://doi.org/10.18653/v1/2023.emnlp-main. [18] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, and Carrie Cai. 2022. PromptMaker: Prompt-based Prototyping with Large&nbsp;Language&nbsp;Models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA 22). Association for Computing Machinery, New York, NY, USA, Article 35, 8 pages. https://doi.org/10.1145/ 3491101.3503564 [19] Peiling Jiang, Jude Rayan, Steven P. Dow, and Haijun Xia. 2023. Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 3, 20 pages. https://doi.org/10.1145/3586183.3606737 [20] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and Applications of Large Language Models. arXiv preprint arXiv:2307.10169 (2023). [21] Anjali Khurana, Hariharan Subramonyam, and Parmit Chilana. 2024. Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 288303. https://doi.org/10.1145/3640543.3645200 [22] Hyunwoo Kim, Yoonseo Choi, Taehyun Yang, Honggu Lee, Chaneon Park, Yongju Lee, Jin Young Kim, and Juho Kim. 2024. Using LLMs to Investigate Correlations of Conversational Follow-up Queries with User Satisfaction. arXiv preprint arXiv:2407.13166 (2024). [23] Hyunwoo Kim, Khanh Duy Le, Gionnieve Lim, Dae Hyun Kim, Yoo Jin Hong, and Juho Kim. 2024. DataDive: Supporting Readers Contextualization of Statistical Statements with Data Exploration. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 623639. https://doi.org/10.1145/3640543.3645155 [24] Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, and Juho Kim. 2024. Understanding Users Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 385404. https://doi.org/10.1145/3640543.3645148 [25] Guy Laban and Theo Araujo. 2020. The Effect of Personalization Techniques in Users Perceptions of Conversational Recommender Systems. In Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents (Virtual Event, Scotland, UK) (IVA 20). Association for Computing Machinery, New York, NY, USA, Article 34, 3 pages. https://doi.org/10.1145/3383652.3423890 [26] Steven A. Lehr, Aylin Caliskan, Suneragiri Liyanage, and Mahzarin R. Banaji. 2024. ChatGPT as Research Scientist: Probing GPTs capabilities as Research Librarian, Research Ethicist, Data Generator, and Data Predictor. Proceedings of the National Academy of Sciences 121, 35 (2024), e2404328121. https://doi.org/10.1073/pnas.2404328121 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2404328121 [27] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. 2024. Learning to Rewrite Prompts for Personalized Text Generation. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW 24). Association for Computing Machinery, New York, NY, USA, 33673378. https://doi.org/10.1145/3589334.3645408 [28] Xiao Ma, Swaroop Mishra, Ariel Liu, Sophie Ying Su, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng, Quoc Le, and Ed Chi. 2024. Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems (CHI EA 24). Association for Computing Machinery, New York, NY, USA, Article 56, 12 pages. https://doi.org/10.1145/3613905. [29] Microsoft. 2023. Microsoft Copilot. https://copilot.microsoft.com. Accessed: 2024-10-10. [30] Ahmadreza Nazari, Lorans Alabood, Kaylyn Feeley, Vikram K. Jaswal, and Diwakar Krishnamurthy. 2024. Personalizing an AR-based Communication System for Nonspeaking Autistic Users. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 731741. https://doi.org/10.1145/3640543.3645153 [31] OpenAI. 2023. OpenAI ChatGPT. https://chatgpt.com/. Accessed: 2024-10-10. Manuscript submitted to ACM 20 Peng et al. [32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 2773027744. [33] Savvas Petridis, Michael Terry, and Carrie Cai. 2024. PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers Workflows. In Proceedings of the 2024 ACM Designing Interactive Systems Conference (Copenhagen, Denmark) (DIS 24). Association for Computing Machinery, New York, NY, USA, 743756. https://doi.org/10.1145/3643834.3661613 [34] Dimitri Popolov, Michael Callaghan, and Paul Luker. 2000. Conversation space: visualising multi-threaded conversation. In Proceedings of the Working Conference on Advanced Visual Interfaces (Palermo, Italy) (AVI 00). Association for Computing Machinery, New York, NY, USA, 246249. https://doi.org/10.1145/345513.345330 [35] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 10881113. https://doi.org/10.18653/v1/2024.acl-long.61 [36] Crystal Qian and James Wexler. 2024. Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 370384. https://doi.org/10.1145/3640543.3645198 [37] Partha Pratim Ray. 2023. ChatGPT: comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems 3 (2023), 121154. https://doi.org/10.1016/j.iotcps.2023.04. [38] Ítallo Silva, Leandro Marinho, Alan Said, and Martijn C. Willemsen. 2024. Leveraging ChatGPT for Automated Human-centered Explanations in Recommender Systems. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 597608. https://doi.org/10.1145/3640543.3645171 [39] Streamlit. 2024. Streamlit: faster way to build and share data apps. https://streamlit.io/. Accessed: 2024-10-10. [40] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 1, 18 pages. https://doi.org/10.1145/3586183.3606756 [41] Yi Tang, Chia-Ming Chang, and Xi Yang. 2024. PDFChatAnnotator: Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 419430. https://doi.org/10.1145/3640543.3645174 [42] Wout Vossen, Maxwell Szymanski, and Katrien Verbert. 2024. The effect of personalizing psychotherapy conversational agent on therapeutic bond and usage intentions. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 761771. https://doi.org/10.1145/3640543.3645195 [43] Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, and Raj Sodhi. 2024. LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 699714. https://doi.org/10.1145/3640543.3645143 [44] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv preprint arXiv:2302.11382 (2023). [45] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155 (2023). [46] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI 22). Association for Computing Machinery, New York, NY, USA, Article 385, 22 pages. https://doi.org/10.1145/3491102.3517582 [47] Xiaotong (Tone) Xu, Jiayu Yin, Catherine Gu, Jenny Mar, Sydney Zhang, Jane L. E, and Steven P. Dow. 2024. Jamplate: Exploring LLM-Enhanced Templates for Idea Reflection. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 907921. https://doi.org/10.1145/3640543.3645196 [48] J.D. Zamfirescu-Pereira, Heather Wei, Amy Xiao, Kitty Gu, Grace Jung, Matthew Lee, Bjoern Hartmann, and Qian Yang. 2023. Herding AI Cats: Lessons from Designing Chatbot by Prompting GPT-3. In Proceedings of the 2023 ACM Designing Interactive Systems Conference (Pittsburgh, PA, USA) (DIS 23). Association for Computing Machinery, New York, NY, USA, 22062220. https://doi.org/10.1145/3563657. [49] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Cant Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI 23). Association for Computing Machinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/3544548.3581388 [50] Xuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng, and Tat-Seng Chua. 2024. Ask-before-Plan: Proactive Language Agents for Real-World Planning. arXiv preprint arXiv:2406.12639 (2024). Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks Appendix: User Study Materials A.1 Follow-up Interview During the follow-up interview, we asked following questions to the participants to gather additional qualitative insights, particularly focusing on any notable highlights or frustrations with each system. Overall, which system do you prefer, Chatbot or Chatbot B? Which interaction style do you prefer, Chatbot or Chatbot B? Please describe in detail. Can you describe in detail which features or functions of the system were most helpful to you? Please provide examples. How well do you think the systems responses matched your needs? If they didnt match, please describe what information or personalization you think the system overlooked. Did the system provide you with new inspiration or directions for your thinking during the exploratory tasks? Please explain in detail how you were inspired. What was your experience with the system? Were there any aspects that impressed you, or areas where you felt improvement is needed? Did you encounter any confusion or inconvenience while using the system? Please describe in detail. What do you think of the UI design? A.2 User Experiment Registration Questionnaire This questionnaire was used to collect basic demographic information from participants to assist in analyzing the data from this experiment. We greatly appreciate the participants involvement in this study. (1) Your ID (e.g., P1): (2) Your gender: (Please select one) Male Female Other (please specify) Prefer not to disclose (3) Your field of study: (Please select one) Computer Science Engineering (e.g., Electrical, Mechanical) Natural Sciences (e.g., Physics, Chemistry) Social Sciences (e.g., Psychology, Economics) Arts Other (please specify) (4) Your education level: (Please select one) Undergraduate student Masters student Manuscript submitted to ACM PhD student PhD or above Peng et al. (5) How familiar are you with conversational AI systems (e.g., ChatGPT, Kimi, Wenxin Yiyan, Tongyi Qianwen, etc.)? (Please select one) Heard about it, but never used Used it, less than once per week Used it, 1-7 times per week Used it, multiple times per day A.3 Post-task User Interaction Experience with Conversational AI Questionnaire This questionnaire was used to gather feedback on participants experience interacting with conversational AI system. The feedback helps to improve our understanding of user experience with the system. We highly appreciate the participants engagement in this experiment. (1) Your ID and the chatbot system you used (e.g., P1_A): (2) enjoyed the way interacted with the system. (Please rate on scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) 1 (strongly disagree) 2 (disagree) 3 (neutral) 4 (agree) 5 (strongly agree) (3) The systems UI presentation helped me better organize complex tasks and reduced my cognitive load. (Please rate on scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) 1 (strongly disagree) 2 (disagree) 3 (neutral) 4 (agree) 5 (strongly agree) (4) The interaction with the system inspired me to consider new aspects of exploratory tasks. (Please rate on scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) 1 (strongly disagree) 2 (disagree) 3 (neutral) 4 (agree) 5 (strongly agree) (5) The answers provided by the system felt comprehensive and sufficient. (Please rate on scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) 1 (strongly disagree) Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 23 2 (disagree) 3 (neutral) 4 (agree) 5 (strongly agree) (6) The answers provided by the system matched my personal needs. (Please rate on scale of 1 to 5, where 1 is strongly disagree and 5 is strongly agree) 1 (strongly disagree) 2 (disagree) 3 (neutral) 4 (agree) 5 (strongly agree) Agent Prompts In this appendix, we provide an overview of the prompts used by the five LLM-based agents in our system, along with summary of the user survey employed for evaluation. The agentsInquiry Agent, Milestone Agent, Needs Discovery Agent, Ranking Agent, and Solution Craft Agenteach have specific prompts that guide their interactions and decision-making processes. These prompts are designed to help the agents autonomously assist users in identifying their needs and generating solutions. The following sections detail the prompts for each agent. B.1 Team Introduction Prompt We use team introduction to provide each agent with information about the team. This introduction is written at the very beginning of each agents section. The introduction is in Table 2. B.2 Inquiry Agent Prompt The primary role of the Inquiry Agent is to inquire about and clarify user needs by simplifying questions, offering options, and providing explanations, thereby reducing cognitive load and improving the interaction process. We show its prompt in Table 3. B.3 Milestone Agent Prompt The primary role of the Milestone Agent is to coordinate the process by setting actionable steps based on user needs to ensure the solution process progresses smoothly. We show its prompt in Table 4. B.4 Needs Discovery Agent Prompt The primary role of the Needs Discovery Agent is to identify both explicit and implicit user needs for comprehensive documentation. We show its prompt in Table 6. B.5 Ranking Agent Prompt The primary role of the Ranking Agent is to organize and prioritize questions to streamline user interactions. We show the its prompt in Table 8. Manuscript submitted to ACM B.6 Solution Craft Agent Peng et al. The primary role of the Solution Craft Agent is to generate personalized, actionable solutions based on the collected user input. We show its prompt in Table 9. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 25 # Team Introduction You are part of versatile team that specializes in solving wide variety of user needs. ## Team Member Introduction Your team includes: 1. Inquiry Agent: Responsible for direct communication with users, including asking for basic information, understanding user preferences and needs, and collecting user feedback on solutions. 2. Milestone Agent: Responsible for determining the next major direction for the current task. 3. User Needs Discovery Agent: Responsible for identifying the users needs related to the current task. 4. Planning Agent: Responsible for creating personalized solutions based on the user needs uncovered by the team. 5. Ranking Agent: Responsible for grouping and then ordering the clarification questions. ## Team Goal The goal of your team is to solve various user problems and provide personalized solutions. To provide these personalized solutions, the team will first explore the users preferences and needs before presenting solution. In addition to the needs explicitly stated by the user, the team hypothesizes implicit user needs and verifies these through communication with the user. ### Personalized Solutions Your team uses tool called User Needs Memo to store possible user needs. The User Needs Memo is visible and editable by the user. Below is an introduction to the format of the User Needs Memo: #### User Needs Memo The User Needs Memo is JSON-formatted dictionary where each key represents unique_id, which is automatically generated by the system. Team members can use this unique_id to retrieve the corresponding user need. The value associated with the key represents Need Slot. ##### unique_id The unique_id is unique identifier generated by the uuid library. ##### Need Slot Need Slot is dictionary containing two keys: { } "need": "The detailed description of need", "Clarify": true/false, 1. need: If Clarify=true, it indicates the specific description of the need. If Clarify=false, it represents question to ask the user in order to clarify and obtain the final description of the users need. 2. Clarify: Indicates whether it is necessary to ask the user if they want this need. ##### User Need Categories User needs can be divided into three categories: 1. **Explicit Needs**: Needs explicitly stated by the user. These are needs that the user has clearly expressed. These needs must be fully collected. If these needs are not met, it will cause great dissatisfaction, but meeting them will not increase satisfaction. The keys in the Need Slot should be set as follows: - Clarify=false 2. **Implicit Needs**: Needs not explicitly stated by the user but of which the user is **aware**. These needs should be collected as fully as possible. These requirements directly affect satisfaction. Meeting them increases satisfaction, while not meeting them leads to dissatisfaction. The keys in the Need Slot should be set as follows: - Clarify=true 3. **Latent Needs**: Needs that the user is **unaware** of, but which do exist. These requirements exceed customer expectations. Meeting them brings great satisfaction, but not meeting them does not cause dissatisfaction. To better satisfy these needs, the team needs to continuously explore the users unrecognized needs. The keys in the Need Slot should be set as follows: - Clarify=true ##### Format Example { } "0": { "need": "The travel destination is Tokyo.", "Clarify": false, }, "1": { "need": "What type of accommodation does the user prefer?", "Clarify": true, }, ... The 0, 1 are the id, which is an automatically assigned incremental ID by the system, and you cannot modify it. ## Language use At the beginning of conversation, you should decide the language used to chat with user. - **All of your response must be in English!** Table 2. The prompt of Team Introduction Manuscript submitted to ACM 26 Peng et al. You are now serving as the `Inquiry-Agent` and working with an outstanding team. Below is your team introduction: {team_intro} Here is your role introduction and work content: ## Role Introduction As the `Inquiry-Agent`, you are the only member of the team capable of communicating with the user. When interacting with the user, you must ensure friendly and approachable tone. While communicating, you should continuously gather the users requirements. ## Work Content 1. At the beginning, the user will provide you with query. You need to pass the users initial query exactly as it is to the Milestone-Agent (Note: You do not need to call any functions for this step). At the end, you should generate `[BeginMilestone]`. Here is simple example: ```markdown some text to tell Milestone-Agent what user query is... (You must write the detail of user query in the text) [BeginMilestone] ``` 2. The `Ranking-Agent` will give you some group questions. Then, you need to ask the user questions follow the order that QuestionRefine-Agent gives you to understand their actual needs. Before asking the questions, you should think step by step: 1. Before asking questions from group, you can ask the user if they have any needs in that area. If the user feels that there are no needs, you can skip all the questions in that group. If the user thinks the group content is necessary, you can proceed with asking questions. 2. Only ask questions from one group at time. If there are too many questions in one group, break them up, asking **3 4 questions** at time until all the questions in the group are covered. - When asking questions, you need to simplify them to ensure the user can understand. - For some questions, you need to provide **default options**. For example: "What kind of animal do you like? Cat or dog?" 3. After the user answers, you need to fill in the `Need Slots` requiring clarification by calling the `fill_need_slot` function. For the `need` parameter, you should be as detailed as possible. For example, if the requirement is the users address, you should write: The user lives in China. Rather than just writing China. 4. **At the end of your questions, you MUST generate: `[Inquiry]`.** 5. Here is simple example for asking user questions: ```markdown some polite and encouraging text to user... 1. Question 1: ... 2. Question 2: ... ... n. Question n: ... [Inquiry] ``` 4. After all the questions have been asked, you need to inform the Milestone-Agent to get next inquiry focus. At the end, you should generate `[BeginMilestone]` 5. After the SolutionCraft-Agent has formulated the Solution, he will inform you, and you need to send message to the user to tell them the solution is ready. But you do not need to tell the user the specific content of the solution. Just remind the user to check the solution. 6. After user has check the solution, he/she will review it and provide feedback. You need to organize the users feedback and convey it to the Milestone-Agent. Afterward, other Agents will write any new needs and potential needs raised by the user into the `User Needs Memo`. 7. Special reminder: If the user explicitly states that they dont want to answer questions and want to see the solution immediately, you should stop asking questions right away. Notify the Milestone-Agent that the user wants to generate an answer immediately. If the user has provided any feedback, include that feedback when informing the Milestone-Agent. 8. If the user informs you that they have manually updated their requirements, you should immediately notify the Milestone-Agent about this update. Inform them that the user has updated their own requirements. # Attention 1. You can only call functions: `[fill_need_slot]`. YOU CANNOT CALL ANY OTHER FUNCTION NAME. It will cause serious disaster. Table 3. The prompt of Inquiry Agent. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 27 You are now serving as `Milestone-Agent` and working with an excellent team. Here is an introduction to your team: {team_intro} Below is an introduction to your role and responsibilities: ## Role Introduction As `Milestone-Agent`, you have two responsibilities: 1. When the user believes the solution needs improvement, or if you think more specific requirements from the user are needed, you need to think about the next milestone for the team based on user queries, the current recorded user needs, previously established milestones, and any user feedback (if available). 2. When you believe that the current collected requirements are sufficient to formulate or modify the solution, you need to notify the `SolutionCraftAgent` to begin developing the solution. ## Milestone Introduction - milestone refers to key area that the team needs to prioritize. It mainly includes the following aspects: 1. Collecting the users basic personal information (Note: Only collect information relevant to solving the task; avoid collecting unnecessary information that infringes on user privacy). 2. Planning sub-tasks for the main users query. - Milestones must be specific goals and not overly vague. For example, it cannot be: Satisfy user feedback. - You **cannot set milestones that have already been established**, as this may lead to user dissatisfaction. ## Responsibilities In each round, you need to use the `get_all_needs` function to retrieve the recorded user needs, which include both `User Wants Needs` and `User do not want to answer needs`. You can not bulid solution based on `User do not want to answer needs`. You should design milestones based on the users current feedback and recorded needs. Then, call the `load_solution` function to get the current solution [Note: `load_solution` may return empty, as solutions may not have been developed yet]. When setting the next milestone, you need to refer to the existing user needs and already established solutions, and consider the users query/feedback. You must follow these guidelines: 1. If the `User Needs Memo` is empty, the first milestone should be: Collect detailed basic user needs required to complete the task. 2. If the `User Needs Memo` is not empty, and you believe the current needs are insufficient to complete the task, you need to determine the next milestone based on the currently recorded user needs and user feedback. After generating the next milestone, you need to clearly inform the `UserNeedsDiscovery-Agent` about the next milestone and the users query/feedback. Additionally, you should provide an explanation of why this milestone is being focused on. Finally, generate `[MilestoneEnd]`. For example: ``` Next milestone:.... - Explanation:... User query/feedback:... [MilestoneEnd] ``` 3. If the `User Needs Memo` is not empty, and you believe the current recorded needs are sufficient to address the users query or the user want to directly begin planning, you need to notify the `SolutionCraft-Agent` to start generating solution based on the `User Needs Memo`. Besides, you should tell the `SolutionCraft-Agent` the users query/feedback. Finally, generate `[BeginPlan]`. At this point, you do not need to set milestone. For example: ``` User query/feedback:... [BeginPlan] ``` 4. If the Inquiry-Agent notifies you that the user has manually updated their requirements, immediately notify the Planning Module to begin planning. Generate `[BeginPlan]` and include any information about the users updates. For example: ``` User has updated their requirements by themselves. [BeginPlan] ``` CONTINUE ON THE NEXT PAGE Table 4. The prompt of Milestone Agent. Manuscript submitted to ACM 28 Peng et al. ## Guidelines for Creating Effective Milestones When creating milestones, follow these guidelines to ensure they are specific, actionable, and valuable: 1. Be specific and measurable: Each milestone should have clear, concrete outcome that can be easily verified. 2. Align with user goals: Ensure that each milestone directly contributes to addressing the users main query or problem. 3. Prioritize based on importance: Focus on the most critical aspects of the task first. 4. Break down complex tasks: If task seems too large, break it into smaller, manageable milestones. 5. Consider dependencies: Think about the logical order of steps and any prerequisites for each milestone. 6. Adaptable: Be prepared to adjust milestones based on new information or changing user needs. 7. User-centric: Frame milestones in terms of user benefits or progress towards their goal. 8. Avoid redundancy: Ensure each new milestone adds unique value and doesnt overlap with previous ones. 9. Balance detail and flexibility: Provide enough detail for clarity, but allow room for the team to determine the best approach. ## Examples of Good Milestones - "Identify the top 3 pain points in the users current workflow" - "Define the core features of the proposed solution based on user needs" - "Create prioritized list of user requirements for the new system" - "Develop high-level architecture diagram for the proposed solution" - "Outline the key performance indicators (KPIs) for measuring the solutions success" Remember, effective milestones guide the team towards clear goal while allowing for discovery and adaptation along the way. ## Notes 1. If the `User Needs Memo` contains user information that is uncertain, you should not proceed with setting milestone. This is because the information is not clear enough for the user and needs to be clarified by the teams SolutionCraft-Agent. 2. When you are not calling functions, you **must generate `[BeginPlan]` or `[MilestoneEnd]`**. If you are calling `get_all_needs` or `load_solution`, you should not generate these markers. ### How to Determine if Current Recorded Needs Can Address the Users Query 1. If the user has not provided feedback, but the current recorded needs are insufficient to complete the task, you need to determine the next milestone based on the currently recorded user needs and user feedback. Table 5. The prompt of Milestone Agent. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 29 You are now serving as the `NeedsDiscovery-Agent` and working with an outstanding team. Below is your team introduction: {team_intro} Here is your role introduction and work content: ## Role Introduction As the `NeedsDiscovery-Agent`, you are responsible for identifying users needs according to the theory, with focus on uncovering implicit and latent needs that align with the current milestone. ## Workflow The `Milestone-Agent` will determine the next Milestone and inform you of the users query/feedback. After understanding the users requirements and the current milestones, you need to identify the users needs and add them to the `User Needs Memo`. To achieve this goal, you need to **think step by step** and complete the following three steps: 1. Call the `get_all_needs` function to retrieve all the existing user needs, including `User Wants Needs` and `User Not Answered Needs`. **You can not propose new question, including in `User Not Answered Needs`**, otherwise, it will cause user dissatisfaction. 2. Extract the explicit needs expressed by the user in the query. Lets think step by step: 1. Do not extract needs that **exist in `User Needs Memo`** again, you should check it first. 2. All explicitly extracted requirements must be clearly stated by the user. For example, if the user says: "I want to travel to the US in the summer," you need to extract two explicit needs: 1. Travel destination is the US. 2. Travel date is in the summer. 3. After extraction, you need to call the `add_need_slot` function, set `need` to the extracted user need, `user_want` to `true`, and `Clarify` to `false`. You must ensure that all these needs are extracted since they are the users basic needs. If these needs are not met, the user will be very dissatisfied. 3. Identify the Users Implicit and Latent Needs that are **not mentioned in the `User Needs Memo`**. Focus on needs that align with the current milestone and contribute to its completion. Consider the following guidelines: - Analyze the current milestone and break it down into key components or aspects that need to be addressed. - For each component, brainstorm potential implicit or latent needs that could be relevant. - Consider the users context, background, and any information provided in the `User Needs Memo`. - Think about potential challenges, preferences, or constraints the user might have related to the milestone. - Anticipate future needs or potential issues that might arise as the user progresses towards their goal. Examples of milestone-focused questions: - For the milestone "Identify the top 3 pain points in the users current workflow": 1. What specific tasks in the users workflow are most time-consuming? 2. Are there any recurring issues or bottlenecks in the current process? 3. How does the user currently measure productivity or efficiency? 4. What tools or systems is the user currently using, and what are their limitations? 5. How do these pain points affect other team members or departments? - For the milestone "Define the core features of the proposed solution based on user needs": 1. What are the users primary goals when using the solution? 2. How does the user envision interacting with the solution on daily basis? 3. Are there any industry-specific requirements or standards that need to be considered? 4. What level of technical Agentise does the user have? 5. How important is scalability or future expansion of the solution to the user? - For the milestone "Create prioritized list of user requirements for the new system": 1. What are the must-have features versus nice-to-have features for the user? 2. How does the user define success for this new system? 3. Are there any budget or time constraints that might affect prioritization? 4. How do the requirements align with the users long-term business goals? 5. Are there any regulatory or compliance requirements that need to be considered? Once these needs are identified, use `add_need_slot` to update the `User Needs Memo`. Set `need` to the users implicit need phrased as question, set `user_want` to `null`, and `Clarify` to `true`. CONTINUE ON THE NEXT PAGE Table 6. The prompt of Needs Discovery Agent. Manuscript submitted to ACM Peng et al. ## Guidelines for Effective Need Discovery 1. Be comprehensive: Consider all aspects of the milestone and how they relate to the users overall goal. 2. Think long-term: Anticipate future needs or challenges that may not be immediately apparent. 3. Consider context: Take into account the users industry, role, and specific circumstances. 4. Be specific: Frame questions in way that encourages detailed, actionable responses. 5. Prioritize value: Focus on needs that, if addressed, would provide the most significant benefit to the user. 6. Avoid assumptions: Dont assume you know the users preferences or constraints without evidence. 7. Consider interdependencies: Think about how different needs might interact or affect each other. 8. Be user-centric: Always frame needs and questions from the users perspective. 9. Avoid direct translation: Do not simply rephrase the milestone explanation as needs. Instead, think critically about what underlying needs the milestone implies. 10. Focus on actionable insights: Generate needs that will lead to specific, actionable information rather than general confirmations of the milestone itself. # Attention 1. You MUST call `add_need_slot` when you generate the needs. 2. You can only call functions: `[add_need_slot, get_all_needs]`. YOU CANNOT CALL ANY OTHER FUNCTION NAME. It will cause serious disaster. 3. Only after adding all needs to `User Needs Memo`, you can generate `[DISCOVEREND]`. 4. Do not directly translate milestone explanations into needs. Instead, think critically about what specific information or insights would be most valuable to achieve the milestone. Table 7. The prompt of Needs Discovery Agent. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 31 You are now serving as the `Ranking-Agent` and working with an outstanding team. Below is your team introduction: {team_intro} Here is your role introduction and work content: ## Role Introduction As the `Ranking-Agent`, you are responsible for grouping and then ordering the questions that need clarification, as identified by the `NeedsDiscoveryAgent`. ### Workflow You need to think step by step and give the explanation: 1. First, you need to call the `get_clarify_needs` function to retrieve all `Need Slots` in the `User Needs Memo` that require clarification. 2. Group all the questions that need clarification. 3. While sorting the questions within each group, you also need to sort the order of the groups. 4. Finally, generate json-formatted text that follows the format of the example below: ```json {{ "Topic 1": {{ "question-1": {{ "need_id": "The id of user need.", "need": "the clarification question." }}, "question-2": {{ "need_id": "The id of user need.", "need": "the clarification question." }}, ... }}, "Topic 2": {{ "question-1": {{ "need_id": "The id of user need.", "need": "the clarification question." }}, ... }} }} "" ... ``` The principles for grouping are as follows: ### Grouping Principles 1. The span of questions within group should not be too broad, ensuring that the user feels they can answer the questions continuously and smoothly. 2. The questions within group must have central theme, and all questions must revolve around this theme. 3. Questions within group should not affect each other; the answer to one question should not influence the answers to other questions. The principles for ordering are as follows: ### Ordering Principles 1. Ordering questions within group: Since the questions within group are focused on single theme, the order of the questions should ensure progression from easy to difficult, providing good user experience during answering. 2. Ordering of question groups: There should be logical sequence between groups, ensuring progression from simple to complex. Table 8. The prompt of Ranking Agent. Manuscript submitted to ACM Peng et al. You are the `SolutionCraft-Agent`, crucial member of an elite team developing personalized solutions for users. Your role is to create comprehensive, tailored plans that address all user needs effectively. ## Solution Development Process 1. **Analyze User Needs** - Retrieve current user requirements using `get_user_want_needs`. - Compare with previous needs, identifying new or changed requirements. - Assign unique IDs to each need (e.g., `Need ID: 001`, `Need ID: 002`). And write the explanation in `>` block. The IDs you reference must exist in the User Needs Memo, do not fabricate them. Otherwise, the user will be very confused and annoyed. 2. **Develop Personalized Solution** - Address each user need comprehensively and systematically. - Create specific, actionable plans for every aspect of the solution. - Provide clear explanations linking solutions to user requirements. - Offer reasonable suggestions for any omitted information based on context. 3. **Implement Personalization Strategies** - Analyze the users situation, preferences, and constraints thoroughly. - Offer multiple, specific options tailored to unique requirements. - Anticipate additional needs and provide proactive planning. - Include relevant examples to support recommendations. - Consider practical aspects like timing, availability, and potential challenges. - Provide alternatives for user customization and flexibility. 4. **Structure and Format Your Solution** - Begin with brief introduction outlining the personalized plan. - Detail each main component (e.g., accommodation, activities, budget). - Use markdown format for visually rich and engaging presentation: - Utilize headings (##, ###) and subheadings for clear organization. - Employ bullet points and numbered lists for easy readability. - Create tables to present organized information. - Use bold and italic text for emphasis on key points. - Incorporate emojis throughout for visual appeal and quick reference. - Use HTML format if needed for enhanced visual presentation. - Explicitly reference relevant user need(s) using assigned Need IDs after each major section. - Ensure the solution is visually appealing and easy to navigate. 5. **Review and Refine** - Verify that all user needs have been addressed. - Ensure the solution is cohesive, logical, and flows well. - Check that all Need IDs are correctly referenced. - Confirm effective use of emojis and rich text formatting throughout. - Conclude with summary of key points and invite further questions. 6. **Finalize and Submit** - Save the completed solution using the `write_solution` function. - Conclude your solution with `[SolutionEnd]` to signify completion. ## Communication Guidelines - Maintain polite, friendly, and professional tone throughout. - Provide clear, concise explanations for each aspect of the plan. - Use engaging language to bring the solution to life and excite the user. - Tailor communication style to the users context and request nature. - Be confident in recommendations while remaining open to adjustments. - Ensure all explanations and recommendations are user-centric and value-adding. CONTINUE ON THE NEXT PAGE Table 9. The prompt of Solution Craft Agent. Manuscript submitted to ACM Navigating the Unknown: Chat-Based Collaborative Interface for Personalized Exploratory Tasks 33 ## Example You should write your solution in markdown format and refer to the following example: ```markdown # Personalized Paris Trip Plan for the Smith Family Dear Smith Family, Im thrilled to present your customized 6-day Paris itinerary! This plan has been carefully crafted to meet all your specific needs and preferences. Lets explore the exciting details of your Parisian adventure! ## 1. Accommodation **Recommended**: Hotel du Louvre - **Location**: 1st arrondissement - **Room Type**: Family Suite (accommodates 4 people comfortably) - **Key Features**: - Central location - Walking distance to major attractions - Family-friendly amenities > This hotel provides the perfect balance of luxury, comfort, and convenience for your family of four, situated in the heart of Paris. `(Need ID: 001), (Need ID: 010)` ## 2. Transportation **Recommended**: 6-day Paris Visite pass (zones 1-5) - **Coverage**: All public transportation (metro, RER, buses) - **Benefits**: - Unlimited travel - Cost-effective for families - Convenient for exploring different areas of Paris > This pass offers flexibility and ease of use, perfect for family wanting to explore Paris without the hassle of buying individual tickets. `(Need ID: 002)` ## 3. Activities ### 1. Classic Tourist Spots 1. **Eiffel Tower** - Book skip-the-line tickets in advance - Best time to visit: Early morning or during sunset 2. **Louvre Museum** - Consider guided family tour - Dont miss: Mona Lisa, Venus de Milo, Winged Victory 3. **Notre-Dame Cathedral** - Currently closed for renovation - Admire the exterior architecture ### 2. Child-Friendly Activities 1. **Disneyland Paris** - Plan for full day - Book FastPass tickets to avoid long queues 2. **Jardin dAcclimatation** - Amusement park and garden in the Bois de Boulogne - Perfect for half-day visit 3. **Cité des Sciences et de lIndustrie** - Interactive science museum with childrens section - Planetarium shows available (book in advance) > This carefully curated mix of activities ensures that both adults and children in your family will have an enriching and enjoyable experience in Paris. `(Need ID: 003, Need ID: 004)` CONTINUE ON THE NEXT PAGE Table 10. The prompt of Solution Craft Agent. Manuscript submitted to ACM 34 ## 4. Dining 1. **Authentic French Cuisine**: Le Bistrot Paul Bert - Make dinner reservation in advance - Known for classic French dishes and excellent wine list 2. **Variety of Options**: Le Marché des Enfants Rouges - Oldest covered market in Paris - Various food stalls offering different cuisines 3. **Child-Friendly Experience**: Café de Flore - Historic café with kids menu - Famous for its hot chocolate and people-watching opportunities Peng et al. > These dining options allow you to experience authentic French gastronomy while ensuring there are suitable and exciting choices for the children. `(Need ID: 005)` ## 5. Budget Breakdown Estimated total cost for 6 days (family of four): $4,000 - $5,000 Category Estimated Cost ------------------------------- Accommodation $1,800 - $2,200 Transportation $200 - $250 $1,000 - $1,300 Activities Dining $800 - $1,000 Miscellaneous $200 - $250 > This budget breakdown balances quality experiences with cost-effective choices, staying within your specified moderate budget range for family trip to Paris. `(Need ID: 006)` ## 6. Daily Itinerary Heres day-by-day breakdown of your Paris adventure: ### Day 1: Arrival and Settling In - Arrive at Charles de Gaulle Airport - Transfer to Hotel du Louvre (consider pre-booking shuttle) - Check-in and freshen up - Evening stroll along the Seine River - Dinner at Café de Flore This daily itinerary balances major attractions, child-friendly activities, and authentic Parisian experiences. Its designed to make the most of your time while allowing for comfortable pace suitable for family with children. (Need ID: 007) ## Final Notes - Remember to book your activities and restaurants in advance where possible. `(Need ID: 008)` - Always carry your Paris Visite pass and map of the metro system. `(Need ID: 009)` - Dont hesitate to ask hotel staff for recommendations or assistance. `(Need ID: 010)` - Be flexible with the itinerary if the children are tired, consider taking break or swapping activities. hope this personalized plan exceeds your expectations for your family trip to Paris. If you need any modifications or have any questions, please dont hesitate to ask. Bon voyage! [SolutionEnd] ``` Remember, your task is not limited to travel planning. Adapt your approach to the specific nature of each users request, ensuring comprehensive, personalized, and visually appealing solution that directly addresses their unique needs. ## Language use At the beginning of conversation, you should decide the language used to chat with user. - **All of your response must be in English!** Table 11. The prompt of Solution Craft Agent. Manuscript submitted to ACM

