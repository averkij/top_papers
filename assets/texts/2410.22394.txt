Title: AAAR-1.0: Assessing AI's Potential to Assist Research

Authors: Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, Wenpeng Yin


================================================================================

Abstract
========

Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions.

Start
=====

4 2 0 2 9 2 ] . [ 1 4 9 3 2 2 . 0 1 4 2 : r AAAR-1.0: ASSESSING AIS POTENTIAL TO

ASSIST RESEARCH
===============

Renze Lou1, Hanzi Xu2, Sijia Wang3, Jiangshu Du4, Ryo Kamoi1, Xiaoxin Lu1, Jian Xie5, Yuxuan Sun6, Yusen Zhang1, Jihyun Janice Ahn1, Hongchao Fang1, Zhuoyang Zou1, Wenchao Ma1, Xi Li7, Kai Zhang8, Congying Xia9, Lifu Huang3, Wenpeng Yin1 1Pennsylvania State University; 2Netflix; 3University of California, Davis; 4University of Illinois Chicago; 5Fudan University; 6Zhejiang University; 7University of Alabama at Birmingham; 8Ohio State University; 9Salesforce Research Correspondence to: {renze.lou, wenpeng}@psu.edu

ABSTRACT
========

Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, benchmark dataset designed to evaluate LLM performance in four fundamental, expertise-intensive research tasks: (i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; (ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; (iii) PAPERWEAKNESS, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on daily basis. An evaluation of both open-source and closed-source LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will release the AAAR-1.0 and keep iterating it to new versions. Project Webpage: https://renzelou.github.io/AAAR-1.0/

INTRODUCTION
============

Although AI has brought transformative changes to various aspects of life, its impact on researchers unfolds in nuanced manner. On the one hand, AI assists in various research disciplines, such as Social Science (Neuman et al., 2023), Finance (Gu et al., 2024), Medicine (Rakhimov et al., 2022), GeoScience (Praskievicz, 2018), etc., significantly expediting academic processes. However, many of these applications are superficial, often limited to data-driven clustering or classification. On the flip side, the AI era poses challenges for researchers. Despite its ability to streamline some activities, researchers still face demanding, cognitively intensive tasks such as staying current through extensive paper reading, rapidly generating ideas in response to fast-paced advancements, conducting rigorous experiments to substantiate claims, and managing an increasing volume of peer reviews. Then question looms: How effectively can AI assist researchers in tasks that are domain-specific, expertise-demanding, and reasoning-intensive? Existing works proved the promising potential for using LLMs in assisting AI research. Si et al. (2024) conducted large-scale human study and found that LLMs can generate creative research ideas. Lu et al. (2024) proposed an autonomous agent to handle complicated research workflow and write whole research paper. However, most of these works focus on addressing highly subjective problems that require high degree of expertise, making evaluation laborious and hard to reproduce. This 1 Figure 1: The input-output illustration of four tasks in the proposed AAAR-1.0 benchmark. underscores the need for comprehensive benchmark that rigorously assesses LLMs capabilities in expertise-intensive research activities. To this end, in this work, we introduce AAAR-1.0, novel benchmark that aims to comprehensively assess the LLMs capacity on expert-level research tasks. As illustrated in Figure 1, AAAR-1.0 decomposes four distinct expert-level AI research tasks from the researchers daily activities, including i) EQUATIONINFERENCE, investigating whether the LLMs can infer the equation correctness based on the paper context; ii) EXPERIMENTDESIGN, validating LLMs ability on designing reliable experiments for research idea; iii) PAPERWEAKNESS, testing the quality of weaknesses discovered by LLMs from paper drafts; and iv) REVIEWCRITIQUE, investigating whether LLMs can identify and explain the deficient/unreliable human-written paper reviews. To ensure data quality, senior AI researchers with extensive domain expertise perform data annotation for AAAR-1.0, followed by rigorous multi-round data examination and filtering. All four tasks require models to possess strong domain knowledge covering various cutting-edge research findings, as well as expert-level research experience, to the extent that even humans need substantial research accumulation to tackle the tasks we designed. Crucially, tasks here are singular, stand-alone challenges (with clear input and output expectations) rather than complicated task chain (Li et al., 2024; Lu et al., 2024), providing more transparent assessment of the models intermediate output. Benefiting from the proposed automatic metrics, we conduct extensive experiments across numerous mainstream LLMs, where we find that: With random guess baseline of 25%, the performance of most LLMs on EQINFER hovers just slightly above chance, with the top models reaching around 60%. This highlights the difficulty of the task, despite its reliance only on local context reasoning. In EXPDESIGN, LLM-designed experiments are innovative and more diverse than those by humans; however, many are trivial, lack feasibility, and stray from the original research objectives. In PAPERWEAKNESS, LLM-identified weaknesses often lack depth and specificity, making them broadly applicable and less useful for providing feedback on paper drafts. In REVIEWCRITIQUE, LLMs struggle to effectively identify deficient human reviews, indicating limited usefulness in assisting meta-reviewers in evaluating the quality of individual paper reviews. Figure 2: Data construction workflows of the three tasks in AAAR-1.0.

2 RELATED WORK
==============

LLMs for AI Research. With the rapid evolution of pertaining techniques, LLMs are found to be useful in assisting various research disciplines (Yu et al., 2024; Labrak et al., 2024), particularly in AI research, such as generating novel research ideas (Kumar et al., 2024), reviewing research draft (Gao et al., 2024; Du et al., 2024; Liang et al., 2024), and writing scientific papers (Chamoun et al., 2024; Lu et al., 2024). For example, Si et al. (2024) conducted large-scale human investigation on LLM-generated research ideas and found that LLMs can generate novel ideas compared with humans while lacking feasibility. Du et al. (2024) found that while LLMs are effective at summarizing papers, they tend to overly trust the authors claimed strengths and struggle to identify weaknesses specific to the paper. Furthermore, some works try to employ LLMs to solve more complicated research tasks that are composed of multiple steps (Li et al., 2024; 2023; Tang et al., 2023). Notably, Lu et al. (2024) proposed AI-SCIENTIST, an autonomous agent framework that can handle series of challenging research tasks consecutively, including generating research ideas, coming up with the corresponding experiments along with the implementations, and then writing the final research paper exactly how human conduct whole research pipeline. However, there is still lack of systematic evaluations and quantitative analyses on the LLMs (intermediate) output of each single-step research task. Our work focuses on building benchmark that has individual research steps with clear input-output expectations, thus making it suitable for comprehensive LLMs evaluation. Benchmarks for AI Research Tasks. Existing LLM assists research benchmarks mainly focus on the implementation and execution part of the research pipeline (Lu et al., 2024; Chen et al., 2024b; Li et al., 2024). For instance, Huang et al. (2024) proposed MLAgentBench to test the LLMs capacity for writing project code and training the ML models, where the evaluation metric is the test performance of the models trained by LLMs. However, real-world AI research activities are diverse and some of them are hard to assess for quality, such as generating research ideas, which requires intensive manual assessment (Si et al., 2024; Liang et al., 2024), or LLM-based estimation (Lu et al., 2024). Our work centers on tasks that emphasize comprehensive mastery of the scientific research field and core elements of researchers daily workload, and we try to build curated task-specific metrics for every single task for more efficient and accurate LLMs appraisal.

3 AAAR-1.0
==========

Figure 2 provides an overview of the construction process of AAAR-1.0. In the following sections, we elaborate on the data collection details of the aforementioned four tasks, including 3.1 EQUATIONINFERENCE ( EQINFER ), 3.2 EXPERIMENTDESIGN ( EXPDESIGN ), 3.3 PAPERWEAKNESS ( WEAKNESS ), and 3.4 REVIEWCRITIQUE .

3.1 EQUATIONINFERENCE
=====================

Crafting correct scientific equation in paper writing or validating an equation in paper reviewing is challenging, as it requires thorough understanding of an algorithm or the intricate relationships among numerous variables. Directly prompting LLMs to generate equations proves overly demanding. Therefore, this work formulates EQINFER (Figure 1) as as multiple-choice equation inference task based on contextual cues, curated as follows. ① Data crawling and cleaning. For the data source, we adopt the pre-compilation LaTeX code for two reasons: i) existing PDF parsing tools, such as PyMuPDF and PaperMage (Lo et al., 2023), can introduce considerable noise to the parsed equation text; ii) considering most of exiting LLMs are capable with processing LaTeX code, using LaTeX source instead of parsed text can be more accurate and provide LLMs with richer information. Meanwhile, to avoid using any low-quality human-written equations, we only crawl those peer-reviewed papers accepted by top-tier conferences. Accordingly, we first obtain the accepted paper list from ACL Anthology, from year 2019 to 2023. Next, we search each paper on arXiv to crawl its LaTeX source (if it exists). Finally, we get total of 1,762 papers source LaTeX packages. We then clean the LaTeX sources by deleting all the comments and combining multiple cross-referred .tex files into main file. Afterwards, we use regex to randomly extract (at most) 3 equations code snippets per paper, finally resulting in 3,877 human-written equations are extracted. ② LLM-based equation synthesis. As we formulate this task as classification, for each humanwritten positive equation, we have to craft at least three counterpart negative equations. To this end, we prompt GPT-4 to synthesize more equations based on the paper context. For each positive equation, we repeat this prompting (with high decoding temperature) until three different negative equations are synthesized. ③ LLM-based filtering. However, the LLM-synthetic equations can sometimes be contextunaligned, i.e., some synthesized equations contain notations that are never defined in the paper context, which is superficial shortcut for the classification tasks. To improve the data quality, we prompt GPT-4 to identify those context-unaligned negative equations. We then discard those instances where all three negative equations are identified as contextually unaligned. This filtering leads to final of 1,449 classification instances (62.3% instances are filtered). ④ Expert-based examination. Furthermore, its also possible that synthesized negative equations are actually correct (i.e., false negative options) even if the negative and positive equations are written differently, the final compiled results might be the same. To filter out the false negative equations and to have final check on the classification instances, we then employ human experts to conduct further data review. We asked 5 senior Ph.D. students who are experienced in AI research to manually check all the instances. For each classification instance, we ask those human experts to consider the following criteria: i) Are all four equations (both positive and negative) grammatically correct? ii) After compilation, is there only one correct answer? We ask every human expert to use external LaTeX compilation tools (e.g., TeXlive), and identify the instances that cannot meet the criteria. Each instance is examined by at least two experts, and we only keep instances that all experts decide to keep. After this strict examination, total of 1,049 instances are eventually kept (27.6% instances are filtered) Final data. We finally shuffle the four equations for each classification instance and randomly assign letters (A, B, C, and D) to the equations. We show the data statistics of the final EQINFER in Table 11 and the sample data cases in Figure 7. 3.2 EXPERIMENTDESIGN Given research topic, such as novel ML algorithm, qualified researcher can design solid experiment plan for it, and clarify underlying motivation to ensure the reliability of the designed experiment. Unlike the concurrent works that focus on the experiment implementation (Lu et al., 4 2024; Huang et al., 2024), we emphasize the importance of assessing the high-level experiment design of LLMs before the subsequent implementation to avoid any expensive execution iteration. Therefore, as shown in Figure 1, we formulate EXPDESIGN as text-generation task that takes pre-experiment paper context as input, and then generates the experiment and explanation list. ① Data crawling. As for the data source, we first collect 10k papers data from arXiv, including LaTeX sources and PDFs, which cover broad AI categories, including cs.AI, cs.CL, and cs.CV, from year 2018 to 2023. Similarly, to ensure the source data quality, we only use papers that have appeared at well-known conferences. ② Domain-expert annotation. Making reliable and executable experiment plan requires solid foundation knowledge of specific research area. Consequently, we set high standard for choosing annotators: i) be senior Ph.D. student with at least one peer-reviewed publication in leading AI venues; ii) have more than 4 years of AI research experience; iii) frequently serve as conference reviewers. Finally, we invite total of 10 qualified experts to participate in our data collection procedure. Given the 10k crawled papers, we first ask every annotator to bid on the papers that they are interested in. After bidding, each of them is assigned 10 papers, i.e., total of 100 papers to be annotated. During annotation, we post each paper PDF on online Google Drive and ask the annotator to first carefully read the whole paper. Then, we ask them to identify and locate the key experiments in each paper (i.e., highlighting the relevant paragraphs of each experiment). We dont consider some trivial experiments, such as those supplemental analyses in the appendix section. For each identified experiment, the annotator has to concisely answer two questions: i) What did this experiment do? ii) Why did the paper authors conduct this experiment? In other words, we ask the annotator to summarize all the key experiments in this paper and explain the underlying motivations based on their rich domain experience. ③ Multi-round peer discussion. Intuitively, different experts might have different opinions on the same research topic. Particularly, when explaining the underlying motivation of an experiment, adopting only single experts opinion might introduce bias to our annotation. Hence, we conduct further multi-round peer discussion. For each online paper PDF, where all the key experiments are identified, summarized, and explained, we ask different expert (reviewer) to review the annotation by considering the following three criteria: i) Are the identified experiments all the key experiments? ii) Does each experiment summarization covers all key information? iii) Does each explanation sound reasonable and reliable? Each reviewer has to leave comments to the online PDF regarding the above criteria, and then the annotator has to respond to each comment either accept the suggestion and revise the previous annotation, or provide rebuttal to the reviewer to uphold the annotation. This discussion iterates until both opinions align with each other. Eventually, for each paper, we collect two lists: i) the experiment list, summarizing each experiment step of the paper; ii) the explanation list, the underlying motivations that are one-one corresponding to the experiment. Final data. After annotation, we use the pre-experiment context of each paper (according to the first-experiment location identified by the annotator) as the input. Furthermore, we use GPT-4 to delete any sentence that potentially leaks the experiment from the input.1 Similar to the EQINFER, we utilize the source LaTeX as the input text to avoid PDF paring noise. As for the image input, we collect those figures within each papers source LaTeX package and only keep figures that are used in the pre-experiment context. Overall, total of 100 instances are collected. As shown in Figure 1, the input of each instance is the pre-experiment context (including the figures), and the ground-truth output is the expert-annotated experiment plan and the explanations. Table 12 shows data statistics and Figure 8 illustrates the sample case in EXPDESIGN. 3.3 PAPERWEAKNESS Another critical research task is paper review. Previous works have demonstrated the usefulness of the LLM-based review feedback (Gao et al., 2024; Jin et al., 2024; Lu et al., 2024). However, as indicated by Du et al. (2024); Liang et al. (2024), LLMs only excel at summarizing the research strengths while falling significantly short on weakness criticism. Hence, we build WEAKNESS for particularly investigating the LLM-generated weaknesses. 1About 9.8% sentences are deleted. 5 ① Data crawling. We first crawl total of 3,779 anonymous submissions of ICLR 2023 from OpenReview,2 including PDF and other meta information (e.g., scores, decisions, and tracks). As the ICLR 2023 has 13 distinct tracks while the paper distribution across different tracks is highly biased, we then uniformly sample papers from different research tracks to improve the domain diversity. Meanwhile, during sampling, we also keep the accept/reject papers distributed equally to avoid data bias. In word, we finally collect total of 1,000 papers (500 accepted; 500 rejected), uniformly covering all 13 tracks. Please refer to Figure 5 for the track and score distribution of the 1,000 papers. ② Extraction of human-written weaknesses. Since the raw comments crawled from ICLR 2023 are mixed with both strengths and weaknesses, we further employ GPT-4 to extract all the weaknesses from each reviewers comments and compose multiple weaknesses into list. Notably, we force GPT-4 to keep the original text of the reviewer, i.e., all weaknesses in our dataset are those original sentences written by the reviewer without any modifications.3 Whats more, sometimes one reviewer might repeatedly mention the same weakness throughout the comment. In this case, we simply keep all the repeated weaknesses because, if one weakness is repeatedly mentioned by the reviewer, its intuitively an important weakness that the reviewer wants to emphasise; accordingly, keeping the repeat items can penalize LLMs more on missing this weakness. For each paper, we can finally get multiple weakness lists (one weakness list per reviewer, one paper can have multiple reviewers). We further delete few papers without any weaknesses found in the raw comments, resulting in total of 993 instances, i.e., 993 {paper, weakness lists} pairs. ③ Input data processing. As we mentioned before, we crawl papers from OpenReview instead of arXiv because the under-review paper draft is required for this task. However, not every paper from OpenReview can be found on arXiv, i.e., the source LaTeX code and figures of most under-review papers are unavailable. Therefore, we utilize VILA (Lin et al., 2023) to parse text data out from the PDF; we also employ PDFFigures-2.0 (Clark & Divvala, 2016) to extract all the figures and tables (in image) from the paper, as Vila is not good at processing the table data. Final data. Our final data is composed of 993 instances, each input is paper text along with figure/table images, and each output is peer reviewers weakness lists. Table 13 shows data statistics; Figure 9 presents an example of the data instances. We show the data diversity (score and track distribution) in Figure 5. 3.4 REVIEWCRITIQUE In addition to identifying weaknesses in paper drafts, more challenging research task that requires more senior research experience is conducting meta-reviewing. Given paper submission, along with individual reviews and author rebuttals, meta-reviewing is not to summarize individual reviews. Instead, meta-reviewer must go through all the information and make final recommendation. This requires the meta-reviewer to identify deficient/unreliable review segments (e.g., if viewpoint is too subjective or contains factual errors) in each individual review and make decision based on the non-deficient ones. This task demands years of experience in the relevant domain; even for human experts, only senior researchers are typically qualified for meta-reviewing. Therefore, as illustrated in Figure 1, we also investigate how LLMs assist meta-reviewers, specifically in identifying deficient review points. We reuse the REVIEWCRITIQUE dataset from our recent work (Du et al., 2024), where we crawled papers initial submissions along with their reviews from OpenReview and employed more than 40 AI research experts to label each review segment (i.e., deficient or not), with detailed human explanations. In total, there were 100 papers with 380 human reviews. Each review was divided into sentence-level segments, resulting in 11,376 review segments (viewpoints). 2We adopt ICLR because it releases full submissions, while some other conferences only release accepted papers. 3We manually checked GPT-4s extraction results of 200 cases GPT-4 only missed 1% of reviewerwritten weaknesses and maintained almost all the original text.

4 EVALUATION CRITERIA
=====================

For EQINFER , we adopt accuracy as the classification criterion. For EXPDESIGN and WEAKNESS, since both tasks have natural language outputs, semantic-based metrics are necessary. Hence, in addition to the conventional ROUGE (Lin, 2004), we also develop several novel similarity-based metrics for each specific task. When evaluating the experiment plan list of EXPDESIGN , we hope the LLMs can mention as many similar experiment steps as the experts plan. Nevertheless, we also dont expect LLMs to generate too many irrelevant or redundant steps in the plan. This intuition covers both the recall and precision aspects. Therefore, we develop semantic similarity-based F1 score, denoted as S-F1, which is the harmonic mean of S-Precision and S-Recall: S-Precision = S-Recall = 1 1 m (cid:88) i=1 (cid:88) j=1 max sim(pi, gj) max sim(gj, pi) (1) (2) where the and represent the LLMs prediction plan and the ground-truth plan, respectively. The and are the list length of and (e.g., experiment steps in p). We use SentenceBERT (Reimers, 2019) to measure the semantic similarity between the pi step and the gj step. Meanwhile, S-F1 omits the item order difference of two lists, but when giving same-length lists (items have one-one correspondence), we can utilize the following similarity-based matching score: S-Match = 1 m (cid:88) i=1 sim(pi, gi) (3) Unlike EXPDESIGN, the output of WEAKNESS is multiple reviewers weakness lists, which means we have to measure LLMs single prediction list with nested list. Hence, we rewrite S-Precision , S-Recall to SN-Precision , SN-Recall: SN-Precision = 1 (cid:32) (cid:88) i=1 1 (cid:88) k=1 max sim(pi, gk ) SN-Recall = 1 (cid:32) (cid:88) k=1 1 nk nk(cid:88) j=1 max sim(gk , pi) (cid:33) (cid:33) (4) (5) where is the number of reviewers of the given paper, nk means the length of k-th reviewers weakness list, and gk indicates the j-th item in k-th reviewers weakness list. Additionally, in the real world, we would think review weakness is reliable if it is specific to paper. Meanwhile, we also hope the review is informative, i.e., no excessive similar weaknesses in one review. Therefore, we adopt the following ITF-IDF metric proposed by Du et al. (2024), which is inspired by the classic TF-IDF: ITF-IDF = 1 (cid:32) (cid:88) j=1 1 mj mj (cid:88) i=1 log (cid:19) (cid:18) mj Oj log (cid:19)(cid:33) (cid:18) Rj Oj = Rj = mj (cid:88) k=1 (cid:88) l=1 sim(pj , pj k) max sim(pj , pl s) (6) (7) (8) where the is the total number of papers in the dataset, pj is j-th papers prediction weakness list, pj is the i-th weakness in pj. Moreover, Oj is the soft number of papers that also contain the pj , which is computed by summing the maximum calculates the intra-paper occurrence frequency of pj ; Rj 7 Table 1: Various LLMs performances on the 1,049 instances of EQINFER task. Methods Random Guess Open-source LLMs Gemma 2-27B (Gemma Team, 2024) Falcon-40B (Almazrouei et al., 2023) OLMo-7B (Groeneveld et al., 2024) Mistral-7B (Jiang et al., 2023) Qwen 2.5-72B (Qwen Team, 2024) Mixtral-8x22B-MoE (Jiang et al., 2024) Llama 3.1-70B (MetaAI, 2024) Closed-source LLMs Gemini 1.5 Pro (Anil et al., 2023) GPT-4o (OpenAI, 2024a) GPT-4 (OpenAI et al., 2023) o1-preview (OpenAI, 2024b) Claude 3.5 sonnet (Anthropic, 2024a) Accuracy (%) 25.00 3.24 4.39 19.00 22.21 35.93 37.08 38.13 34.31 43.18 49.85 59.49 61.10 measures informativeness, measures specificity. The complete ITF-IDF consider both aspects and reflects the overall and other papers weaknesses. In word, Oj similarity scores between pj and Rj weakness diversity. For REVIEWCRITIQUE , we use F1 score as the classification metric; while for the deficiency explanation, we use ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020) to reflect how well the model-generated explanation aligns with the experts annotation.

5 EXPERIMENTS AND ANALYSES
==========================

In this section, we conduct extensive experiments on AAAR-1.0, across various mainstream LLMs, to quantify the current LLMs capacity to tackle high-level research tasks. Specifically, 5.1 for EQINFER , 5.2 for EXPDESIGN , 5.3 for WEAKNESS , and 5.4 for REVIEWCRITIQUE . Please refer to Appendix B.2 for running details of the LLMs. 5.1 EQUATIONINFERENCE Settings. As different LLMs have distinct context windows, to ensure fair comparison, we fix the maximum input length for all models. According to the data statistics of Table 11, we empirically use 1,000 words for both contexts before and after equations, i.e., 2,000 surrounded words. Main results. Table 1 shows the main results. Firstly, the open-source LLMs, especially the Falcon and Gemma, perform unexpectedly disappointing (even worse than random guesses). These screwed scores are mainly due to the poor long-context instruction following ability, where we find some open-source LLMs are confused with the massive input and often copy the LaTeX code from the input. In contrast, closed-source LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge from the larger model parameters. However, considering the conventional multi-choice QA formulation of EQINFER, the recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks (Song et al., 2023). Notably, with the help Figure 3: The input context length scaling trend on the EQINFER task. Table 2: Various LLMs performances on the 100 instances of EXPDESIGN . The explanation generation is based on the oracle experiments to prevent error propagation. Copy Input is random baseline: for experiment design, randomly select 5 sentences from the input paper; for experiment explanation, directly copy each experiment idea. Methods Copy Input OLMo-7B (Groeneveld et al., 2024) Falcon-40B (Almazrouei et al., 2023) Gemma 2-27B (Gemma Team, 2024) Mistral-7B (Jiang et al., 2023) Mixtral-8x22B-MoE (Jiang et al., 2024) Llama 3.1-70B (MetaAI, 2024) Qwen 2.5-72B (Qwen Team, 2024) Gemini 1.5 Pro (Anil et al., 2023) Claude 3.5 sonnet (Anthropic, 2024a) GPT-4 (OpenAI et al., 2023) GPT-4o (OpenAI, 2024a) o1-preview (OpenAI, 2024b) S-F 21.13 33.94 17.87 34.33 37.62 42.21 40.57 43.24 51.87 48.74 43.89 53.00 46.67 Experiment Design Experiment Explanation S-Precision S-Recall S-Match ROUGE-L ROUGE-1 17.94 Open-source LLMs 37.25 21.78 39.71 43.09 50.13 48.43 51.73 Closed-source LLMs 50.77 46.49 42.34 51.24 45.04 26. 31.79 15.35 30.51 34.19 36.82 35.43 37.55 53.37 51.53 45.82 55.12 48.70 40.32 45.78 17.03 42.77 50.18 49.07 50.05 51.12 52.87 53.03 55.03 54.79 58.55 22. 26.30 12.10 26.20 30.20 29.96 29.33 29.46 28.52 18.75 22.82 27.54 29.11 25.28 30.38 12.72 29.63 34.69 34.53 34.11 34.68 33.80 26.15 30.01 34.31 36.70 of internal CoT, o1 gains stronger performances than GPT-4/GPT-4o, indicating the potential benefits of adopting reasoning for this task. Q: Do more contexts boost performance? Table 1 unifies the input context lengths to 1,000 tokens for various LLMs. To answer this question, we experiment with long-context LLMs to investigate the impact of the input context lengths. Particularly, we scale the input length (per side) from 100 to 1,500 words. As shown in Figure 3, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesnt help the performance and even significantly drops Qwens scores. While for the closed-source GPT-4-Turbo and GPT-4o, scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards. This is in line with human intuition, i.e., surrounding context is required for the equation inference, as the adjacent context usually provides important information, such as the target algorithm description or the notation definition. However, after exceeding specific threshold, more context information is not beneficial anymore and even confuses those LLMs with poor long-context handling capacity (Wang et al., 2024; Liu et al., 2024). 5.2 EXPERIMENTDESIGN Settings. Similarly, we unify the input context length of different LLMs to ensure fair comparison. According to Table 12, we set 2,000 and 3,000 input words for openand closed-source LLMs, respectively. Meanwhile, as experiment explanation is the subsequent task of experiment design, using model-generated experiments can propagate errors in explanation, leading to inferior results for most LLMs. To this end, we provide LLMs with the oracle experiments when generating explanations. Main results. Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the Copy Input baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs (10%). We find that closed-source LLMs are more creative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall. As for the experiment explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant. Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior. We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in 9 Table 3: The impact on S-Match scores of maintaining the experiments self-containment for EXPDESIGN .

Models
======

One-by-One Whole-List Llama 3.1-70B Qwen 2.5-72B Gemini 1.5 Pro Claude 3.5 sonnet GPT-4 GPT-4o o1-preview 50.05 51.12 52.87 53.03 55.03 54.79 58. 49.36 ( 0.7) 48.56 ( 2.6) 57.48 ( 4.6) 59.11 ( 6.1) 56.95 ( 1.9) 58.54 ( 3.8) 61.58 ( 3.0) high superficial overlap with the ground-truth explanation. This observation highlights the importance of adopting the proposed S-Match to avoid evaluation bias of traditional generation metrics. Q1: Can self-contained experiment design enhance the experiment explanation? When generating the explanation in Table 2, we provide LLMs with each individual experiment and let them explain one by one, because we find that, when providing the whole experiment list, those opensource models only explain partial experiments because of their poor instruction-following capacity. However, there are intuitively some semantic or logical relations between different experiments, e.g., some experiments are prerequisites to others. Therefore, this one-by-one prompting might break the self-containment of an experiment plan. Consequently, we test with the whole-list prompting, where the LLMs are given the complete experiment list and are asked to explain all experiment steps together. As shown in Table 3, unlike the open-source LLMs, the explanation performances of those closedsource LLMs are generally improved after adopting whole-list prompting. According to further manual checking, after maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment. Q2: Do human evaluation results align with automatic metrics for explanation? As the explanation can be open-ended, in this paragraph, we provide the human evaluation results on different LLMs experiment explanation outputs. In detail, we randomly select 20 out of 100 papers and ask 5 annotators to read the experiments along with each models explanations; we then let the annotator decide whether each models explanation is acceptable (see Appendix C.1 for more details). Table 4 illustrates the results, where the score variance is higher than Table 2. However, the performance ranking of both tables is perfectly correlated with each other (Spearmans rank correlation coefficient = 1), demonstrating the effectiveness of S-Match. Table 4: The human evaluation results on LLMs output explanations of EXPDESIGN . Acc. ratio means how many model outputs are accepted by the annotator. Models Acc. ratio Llama 3.1-70B Gemini 1.5 Pro Claude 3.5 sonnet GPT-4o o1-preview 22.93 55.07 61.46 69.72 76.14 Q3: Do more contexts boost performance? We also investigate the impact of input context length for EXPDESIGN. As shown in Figure 4, we scale up the input pre-experiment context length from 0.1k to 10k tokens (10k tokens is the maximum paper context length in the dataset). For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k tokens, which is similar to EQINFERs scaling results after the necessary information has been covered, scaling more up doesnt boost the performance. Meanwhile, the results of the experiment explanation demonstrate that explaining motivations almost doesnt require any paper context, i.e., the LLMs solely rely on the given experiments. However, we do not 10 Figure 4: The input context length scaling trend of different LLMs on the EXPDESIGN task. Table 5: The figure inputs ablation of EXPDESIGN . For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for openand closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window sizes, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper. Models GPT-4o w/ figures GPT-4 w/ figures InternVL2-26B w/ figures Experiment Design Experiment Explanation S-F S-Precision S-Recall S-Match ROUGE-L ROUGE-1 53.00 50. 43.89 43.54 40.52 38.83 51.24 48.94 42.34 42.56 48.95 46.91 55.12 51. 45.82 44.85 35.20 33.70 58.54 58.53 56.95 55.03 50.03 50.29 29.25 27. 25.98 22.82 29.13 29.29 35.50 34.30 33.37 30.01 34.26 34.06 expect this because we hope LLMs can explain the motivation based on thorough understanding of the paper, just like how human experts do. Hence, there is still considerable gap between the LLMs and humans in terms of grasping research motivations. Q4: Does multi-modal input boost performance? Intuitively, besides the text, when designing experiments for given research topic, the figures can provide rich supplementary information, such as an algorithm illustration that can help better understand this research topic and underlying motivations. Hence, we test the performance of different LMMs (Large Multimodal Models), including GPT4-o, GPT-4, and InternVL2 (Chen et al., 2024a). Table 5 shows the ablation results on the figure data. To our surprise, the figure data doesnt improve the LMMs results in this task, even harming the performances. This might be due to the low informativeness of the figures, as figures usually consume more input tokens but act only as supplementary information to the text, indicating future work on developing LMMs that can effectively leverage the scientific figures. 5.3 PAPERWEAKNESS Settings. Intuitively, the full paper content is necessary for providing feedback to that paper. Therefore, instead of setting maximum input length, in WEAKNESS, we try to feed all the paper context into the LLMs. As the input length of WEAKNESS is extremely long (see Table 13), we adopt split-combine method we first split the whole paper into several smaller pieces and let LLMs predict the weaknesses of each piece separately; after that, we combine all pieces weaknesses as final complete prediction. In practice, for the length of each small piece, we set 2,000 and 3,000 words for openand closed-source LLMs, respectively. Additionally, in this task, we also examine the performance of recent agent framework, namely AI-SCI (Lu et al., 2024), which enhances GPT11 Table 6: Various LLMs performances on the 993 instances of WEAKNESS . Methods Human Review OLMo-7B (Groeneveld et al., 2024) Falcon-40B (Almazrouei et al., 2023) Gemma 2-27B (Gemma Team, 2024) Mistral-7B (Jiang et al., 2023) Mixtral-8x22B-MoE (Jiang et al., 2024) Llama 3.1-70B (MetaAI, 2024) Qwen 2.5-72B (Qwen Team, 2024) Gemini 1.5 Pro (Anil et al., 2023) Claude 3.5 sonnet (Anthropic, 2024a) GPT-4 (OpenAI et al., 2023) GPT-4o (OpenAI, 2024a) o1-preview (OpenAI, 2024b) AI-SCI (GPT-4o) (Lu et al., 2024) SN-F1 (%) SN-Precision (%) SN-Recall (%) Weakness Diversity ITF-IDF () Open-source LLMs 43.25 27.34 35.85 42.03 43.23 42.78 42.74 Closed-source LLMs 40.38 25.13 34.68 43.80 44.59 43.19 43.80 48.75 47.85 47.66 47.73 48.62 LLM Agent Framework 45.05 43.97 41.97 42.15 42.09 42.54 40.02 47.04 30.88 37.91 40.77 42.23 42.70 42.05 55.08 56.00 55.19 55.48 57.08 51.91 7.69 2.45 1.06 1.43 1.17 0.98 2.60 1.21 5.88 3.91 5.31 5.95 5. 2.23 Table 7: The performance comparison of different input processing methods for WEAKNESS . We use GPT-4o and GPT-4-Turbo because both accept maximum of 128k tokens input. We also put the results of AI-SCI in the table for reference. Here, split-combine splits the input paper into several pieces, where each pieces length is denoted as window size; no-split means the conventional input cutting, for example, if the window size is 3,000, then only the first 3,000 words in the paper are used. According to the data statistics, 20,000 words can cover maximum lengths of more than 95% of the papers in our dataset. Models Input Context Processing Window Size (in words) SN-F SN-Precision SN-Recall ITF-IDF GPT-4-Turbo GPT-4o AI-SCI split-combine no-split no-split split-combine no-split no-split split-combine no-split no-split 3,000 3,000 20,000 3,000 3,000 20,000 3,000 3,000 20, 47.66 45.80 44.99 47.73 45.74 45.47 45.05 42.56 42.53 42.15 43.66 42.64 42.09 43.45 42.97 40.02 40.90 40. 55.19 48.39 47.82 55.48 48.54 48.51 51.91 44.65 44.78 5.31 5.58 5.58 5.95 5.92 6.02 2.23 2.53 2. 4os paper review ability by leveraging advanced prompting techniques, e.g., self-reflection (Shinn et al., 2024) and response ensembling (Wang et al., 2023).4 Main results. Table 6 shows the main results, where the closed-source LLMs overall performances are generally superior to the results of open-source LLMs. Similarly, closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses. However, there is still considerable gap in the weakness diversity between the LLMs and human experts.5 Compared with human review, most LLM-generated weaknesses are vague and lack the necessary knowledge about some frontier research works. Surprisingly, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task. 4We dont run AI-SCI on EXPDESIGN, because AI-SCI takes model-generated ideas as the inputs, which are incompatible with our task setting. 5Note that the humans ITF-IDF score in Table 6 can be slightly underestimated. This is because we keep the repeated weaknesses in the human review, which affects the human reviews informativeness (lower ITF) but is useful when calculating the SN-Recall for LLMs. Table 8: The ablation study about the paper tables and figures of WEAKNESS . Based on the conclusion in Table 7, we use the split-combine to process the text input here (2,000 and 3,000 words context window size for openand closed-source models). For GPT-4o, we use all the table/figure images; while for InternVL2, we randomly select two images per paper, i.e., two random figures, two random tables, or one random figure + table. Models GPT-4o w/ tables w/ figures w/ tables & figures InternVL2-26B w/ tables w/ figures w/ tables & figures SN-F1 SN-Precision SN-Recall ITF-IDF 47.73 46.76 46.62 46.58 41.91 40.55 42.88 42. 42.09 41.32 41.20 41.17 41.02 40.37 42.10 42.00 55.48 54.17 54.04 53.98 43.28 42.91 43.76 43.31 5.95 5.53 5.48 5.36 1.48 1.46 1.46 1. Q1: Is the split-combine effective? Ideally, if the LLM has sufficient context window size, it is unnecessary to split the input papers for separate processing. Consequently, in this paragraph, we utilize the LLMs accepting long context input to compare split-combine with no-split, i.e., letting LLMs write weaknesses by giving the full paper. In practice, we set the maximum number of input words to 20k, which ensures 95% papers in the WEAKNESS can be fully processed. As shown in Table 7, compared with giving the full paper contexts, split-combine generally brings about superior performances. During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece. Surprisingly, the LLMs performances with full paper context can be even worse than just remaining the first 3,000 words. This implies that even the current powerful long-context LLMs still fall short when processing long scientific documents. Q2: Does multi-modal input boost performance? Our dataset covers both tables and figure illustrations extracted from the paper PDF as inputs. Intuitively, when reviewing paper, both figures and tables are critical, not only for better understanding, but also because some weaknesses are related to tables/figures.6 Therefore, in Table 8, we adopt two LMMs to investigate the effectiveness of image inputs. Overall, image information, including both figures and tables, doesnt bring significant performance improvement, i.e., only InternVL2 gains performance boost after incorporating figures; while tables slightly drop both models results. This is probably because the LMMs cannot reason well over the information-intensive images, especially the table images. 5.4 REVIEWCRITIQUE Settings. As individual review comments are split into multiple smaller segments (sentences), in order to avoid the performance variance that comes from the prompting, we follow Du et al. (2024) to utilize two prompting strategies. i) Labeling-All: given everything necessary including list of indexed review segments, require the LLM to output list of triples, like {id, reliable or not, explanation}. ii) Select-Deficient: Given everything necessary including list of indexed review segments, require the LLM to output list of tuples, {id, explanation}, when it believes the id corresponds to deficient segment. To further enhance evaluation robustness, we ensemble the results obtained from the two prompting strategies using two methods. i) Both No: if both prompts classify segment as deficient, we consider it to be deficient. ii) Either No: if either of the prompts labels segment as Deficient, we consider it to be deficient. 6We find that there is approximately one human-written weakness related to figures or tables in each paper. 13 Table 9: From (Du et al., 2024), various LLMs performances on the 11,376 instances of REVIEWCRITIQUE . The best F1 score among different prompt methods for single model is underlined. The best F1 score across all models is also bold. Models Precision / Recall / F1 Labeling-All Select-Deficient Both No Either No Open-source LLMs Llama3-8B (AI@Meta, 2024) Llama3-70B (AI@Meta, 2024) Qwen2-72B (Bai et al., 2023) 7.73 / 45.95 / 12.22 13.63 / 42.49 / 18.19 9.97 / 26.60 / 12.96 11.47 / 30.29 / 14.88 13.95 / 31.16 / 17.46 11.35 / 34.61 / 14. 11.37 / 21.27 / 12.46 16.16 / 23.51 / 16.67 9.07 / 15.13 / 9.62 8.19 / 53.61 / 13.35 12.46 / 50.02 / 18.43 10.49 / 43.00 / 15.16 Closed-source LLMs Gemini 1.5 (Anil et al., 2023) GPT-4 (OpenAI et al., 2023) Claude Opus (Anthropic, 2024b) 16.58 / 34.13 / 19.76 14.91 / 34.49 / 18.38 16.86 / 34.26 / 20.35 14.71 / 43.60 / 19.72 17.18 / 34.59 / 20.30 17.69 / 26.61 / 18. 17.01 / 27.05 / 18.28 18.71 / 21.40 / 16.85 17.14 / 18.70 / 15.78 14.46 / 50.37 / 20.34 14.72 / 47.68 / 20.66 16.94 / 42.12 / 21.99 Main results. We put the results of Du et al. (2024) in Table 9. Closed-source models (GPT-4, Claude Opus, and Gemini 1.5) generally outperform open-source models (Llama3-8B and 70B, Qwen2-72B) in F1 score. Claude Opus achieves the highest F1 scores, with GPT-4 and Gemini 1.5 performing slightly worse. Notably, recall scores are consistently higher than precision scores across all LLMs and prompting strategies, suggesting that LLMs tend to incorrectly identify segments as deficient. Despite the superior performance of the closed-source models, their F1 scores remain relatively low even with different prompt strategies, highlighting the challenges LLMs face in such expertise-intensive tasks and emphasizing the importance of human expertise in the meta-reviewing process. Q: How about the LLMs explanation quality regarding the deficient review? As we also prompt the LLMs to generate the corresponding explanation on why they think each review segment is deficient, we report how well the model-generated deficiency explanation aligns with the human explanation. Model Table 10: Evaluation of LLMs explanations for correctly identified deficient segments. We put the results of Du et al. (2024) in Table 10. The results in Table 10 show that overall scores for all LLMs are relatively low, indicating they can identify some Deficient segments but struggle to articulate their reasoning. Among the LLMs, Claude Opus achieves the highest scores across all metrics, suggesting its explanations align best with human annotators. Claude Opus also excels in identifying Deficient segments, as shown previously. GPT-4 and Gemini 1.5 show similar performance to Claude Opus. The open-source models, Llama3 (8B and 70B) and Qwen2-72B, generally score lower than the closed-source models. 17.13 / 2.71 / 14.64 / 55.63 20.18 / 3.69 / 17.52 / 57.28 18.47 / 2.98 / 16.38 / 56.46 16.49 / 2.22 / 13.65 / 55.23 15.94 / 1.95 / 13.78 / 57.09 17.07 / 3.00 / 14.69 / 56.88 GPT-4 Claude Opus Gemini 1.5 Llama3-8B Llama3-70B Qwen2-72B ROUGE-1/2/L/BERTScore

6 CONCLUSION
============

In this work, we propose AAAR-1.0, novel benchmark targeting comprehensive evaluation of the current LLMs AI research capacity. AAAR-1.0 consists of distinct expertise-intensive tasks along with the curated evaluation metrics, and collect high-quality data by employing senior AI researchers. Extensive experiments highlight the challenges and values of AAAR-1.0.

LIMITATIONS
===========

We shed light on two limitations of this work: i) As we gather data from open-source platforms such as arXiv and OpenReview, there is possibility that current or future LLMs may be trained on the same source data utilized in our benchmark. This situation could influence the fairness of LLM comparisons and the conclusions drawn from this paper. While we acknowledge this potential data leakage, we maintain that our work can provide valuable insights and serve as an upper bound for some LLMs, particularly if they are indeed pretrained on those scientific papers. At present, this research can inform the design of current LLMs and agents, enhancing the communitys understanding 14 of the strengths and limitations of using LLMs in scientific research. ii) Meanwhile, due to the high consumption of employing senior researchers in conducting data annotation, the data size for some tasks, such as EXPDESIGN, is relatively small. This might introduce variability in LLM performance, even with repeated runs. As this work marks the beginning of series, we plan to release larger-scale datasets that will cover more diverse research tasks in AAAR-2.0.

ETHICS STATEMENT
================

Our study highlights that LLMs can be used to assist humans in AI research. This doesnt mean we encourage the use of AI to replace human researchers. In contrast, we emphasize that the main responsibility for conducting scientific research should always lie with humans to prevent any societal risks, while the LLMs are only tools for making human research more efficient. To this end, we commit to careful distribution of the data collected in our research, ensuring it serves strictly for research purposes. Our goal is to mitigate the risks while maximizing the benefits offered by LLMs.

ACKNOWLEDGMENTS
===============

We would like to thank arXiv and OpenReview for releasing the paper source packages and the review comments. The authors would also like to thank Ibraheem Moosa and Sarkar Snigdha Sarathi Das for assisting in the data collection.

REFERENCES
==========

AI@Meta. Llama 3 model card, 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance, 2023. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Gemini Team, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/ claude-3-5-sonnet, June 2024a. Anthropic. Introducing the next generation of claude anthropic. https://www.anthropic.com/ news/claude-3-family, March 2024b. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Eric Chamoun, Michael Schlichktrull, and Andreas Vlachos. Automated focused feedback generation for scientific writing assistance. arXiv preprint arXiv:2405.20477, 2024. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024a. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, et al. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. arXiv preprint arXiv:2410.05080, 2024b. Christopher Clark and Santosh Divvala. Pdffigures 2.0: Mining figures from research papers. In Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, pp. 143152, 2016. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, and Wenpeng Yin. Llms assist NLP researchers: Critique paper (meta-)reviewing. In The 2024 Conference on Empirical Methods in Natural Language Processing, 2024. doi: 10.48550/ARXIV.2406.16253. URL https://doi.org/10.48550/arXiv.2406.16253. Zhaolin Gao, Kianté Brantley, and Thorsten Joachims. Reviewer2: Optimizing review generation through prompt generation. arXiv preprint arXiv:2402.10886, 2024. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models. Preprint, 2024. Jingyi Gu, Junyi Ye, Wenpeng Yin, and Guiling Wang. Adaptive and explainable margin trading via large language models on portfolio management. In Proceedings of the 5th ACM International Conference on AI in Finance (ICAIF24), 2024. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. Agentreview: Exploring peer review dynamics with llm agents. arXiv preprint arXiv:2406.12708, 2024. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, and Asif Ekbal. Can large language models unlock novel scientific research ideas? arXiv preprint arXiv:2409.06185, 2024. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. Biomistral: collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373, 2024. Haoyuan Li, Hao Jiang, Tianke Zhang, Zhelun Yu, Aoxiong Yin, Hao Cheng, Siming Fu, Yuhao Zhang, and Wanggui He. Traineragent: Customizable and efficient model training through llmpowered multi-agent system. arXiv preprint arXiv:2311.06622, 2023. Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196, 2024. Chin-Yew Lin. Rouge: Package for Automatic Evaluation of Summaries. In Text summarization branches out, pp. 7481, 2004. 16 Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Kyle Lo, Zejiang Shen, Benjamin Newman, Joseph Chang, Russell Authur, Erin Bransom, Stefan Candra, Yoganand Chandrasekhar, Regan Huff, Bailey Kuehl, et al. Papermage: unified In toolkit for processing, representing, and manipulating visually-rich scientific documents. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 495507, 2023. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. MetaAI. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/blog/ meta-llama-3-1/, July 2024. Yair Neuman, Yochai Cohen, and Wenpeng Yin. Identifying social norm violation in movie plots: from borat to american pie. Digit. Scholarsh. Humanit., 38(4):16361645, 2023. doi: 10.1093/ LLC/FQAD052. URL https://doi.org/10.1093/llc/fqad052. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024a. OpenAI. Introducing openai o1. introducing-openai-o1-preview/, September 2024b. https://openai.com/index/ OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sarah Praskievicz. River classification as geographic tool in the age of big data and global change. Geographical Review, 108(1):120137, 2018. Mekhriddin Rakhimov, Ravshanjon Akhmadjonov, and Shahzod Javliev. Artificial intelligence in medicine for chronic disease classification using machine learning. In 2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT), pp. 16. IEEE, 2022. Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. Linxin Song, Jieyu Zhang, Lechao Cheng, Pengyuan Zhou, Tianyi Zhou, and Irene Li. Nlpbench: Evaluating large language models on solving nlp problems. arXiv preprint arXiv:2309.15630, 2023. Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, et al. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. arXiv e-prints, pp. arXiv2311, 2023. Gemma Team. Google launches gemma 2, its next generation of open models. https://blog. google/technology/developers/google-gemma-2/, Jun 2024a. Qwen Team. Qwen2.5: party of foundation models, September 2024b. URL https://qwenlm. github.io/blog/qwen2.5/. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. arXiv preprint arXiv:2406.17419, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf? id=1PL1NIMMrw. Botao Yu, Frazier Baker, Ziqi Chen, Xia Ning, and Huan Sun. Llasmol: Advancing large language models for chemistry with large-scale, comprehensive, high-quality instruction tuning dataset. arXiv preprint arXiv:2402.09391, 2024. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.

APPENDICES
==========

Within this supplementary material, we elaborate on the following aspects: Appendix A: Data Statistics and Diversity Appendix B: Implementation Details Appendix C: More Experiment Results and Details Appendix D: Data Cases and Annotation Platform Illustration Appendix E: Prompt Templates

A DATA STATISTICS AND DIVERSITY
===============================

We provide the detailed data statistics of three datasets in our benchmark, as shown in Table 11, 12, and 13. We use the NLTK package7 to tokenize words and count the length. When calculating the length of equations, we use the pylatexenc tool8 to simplify the equations first. Meanwhile, for the WEAKNESS, we also plot the review scores distribution of the papers used in the dataset, as well as the track distribution. As can be found in Figure 5, our dataset has decent distribution, where the papers are uniformly distributed across 13 tracks, and most papers scores ranged from 5 to 8 (i.e., most papers are weakly rejected or accepted). Table 11: The statistics of EQINFER . Here, the left and right input context indicates the paper contexts before and after the missed equation; pos. means the ground-truth equations (written by the source paper authors), while neg. is the GPT4-synthetic wrong equations. # of classification instances # of source papers ave. left input context length (in words) ave. right input context length (in words) max left input context length (in words) max right input context length (in words) min left input context length (in words) min right input context length (in words) ave. pos. output equation length (in character) ave. neg. output equation length (in character) max pos. output equation length (in character) max neg. output equation length (in character) min pos. output equation length (in character) min neg. output equation length (in character) 1,049 869 4,377 6,362 24,849 32,948 711 8 55 48 1,039 306

B IMPLEMENTATION DETAILS
========================

B.1 METRIC DETAILS When calculating the metrics, specifically for the similarity-based scores, we utilize SentenceBERT (Reimers, 2019) to encode each segment (e.g., each experiment idea in the list) into dense vector, and then calculate the cosine similarity,9 which takes about 1GB of memory when running on single A100 GPU. 7https://www.nltk.org/ 8https://github.com/phfaist/pylatexenc 9https://huggingface.co/sentence-transformers/all-mpnet-base-v2 19 Table 12: The statistics of EXPDESIGN . # of instances # of source papers ave. input context length (in words) max input context length (in words) min input context length (in words) ave. # of input figures max # of input figures min # of input figures ave. length of Experiment&Explanation list ave. length per experiment (in words) ave. length per explanation (in words) max length of Experiment&Explanation list max length per experiment (in words) max length per explanation (in words) min length of Experiment&Explanation list min length per experiment (in words) min length per explanation (in words) 100 100 4,288 9,799 698 2.6 16.0 0.0 5.7 34.3 27.1 13 135 89 2 9 9 Table 13: The statistics of WEAKNESS . # of instances # of source papers ave. input context length (in words) max input context length (in words) min input context length (in words) ave. # of input figures max # of input figures min # of input figures ave. # of input tables max # of input tables min # of input tables ave. # of reviewers per paper max # of reviewers per paper min # of reviewers per paper ave. # of weaknesses per reviewer max # of weaknesses per reviewer min # of weaknesses per reviewer ave. length of weakness (in words) max length of weakness (in words) min length of weakness (in words) 993 993 9,811 49,195 24 7.0 37.0 0.0 4.3 53.0 0.0 3.8 9.0 3.0 4.8 39.0 1.0 39.1 371.0 2. B.2 LLMS RUNNING DETAILS In our experiments, we utilize various LLMs, including both closed and open-sourced. We list the model weight sources for the open-source LLMs: OLMo-7B: https://huggingface.co/allenai/OLMo-7B Falcon-40B: https://huggingface.co/tiiuae/falcon-40b Gemma 2-27B: https://huggingface.co/google/gemma-2-27b Mistral-7B: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 20 (a) The review score distribution of the papers used in WEAKNESS . (b) The track distribution of the papers used in WEAKNESS . Figure 5: The data diversity illustration of WEAKNESS , including the score distribution and track distribution of the papers used in our dataset. Mixtral-8x22B-MoE: Mixtral-8x22B-Instruct-v0.1 https://huggingface.co/mistralai/ Llama 3.1-70B: https://huggingface.co/meta-llama/Llama-3.1-70B Qwen 2.5-72B: https://huggingface.co/Qwen/Qwen2.5-72B 21 We use VLLM to unify the inference endpoints of all the above models.10 We use Pytorch 2.4.0 with CUDA 12.1, and use 8 NVIDIA A100 GPUs for the LLMs inference. Meanwhile, we use the gpt-4o-2024-08-06, gpt-4-1106-preview, o1-preview-2024-09-12, gemini-1.5pro-002, and claude-3-5-sonnet-20240620 for the closed-source LLMs. We use LiteLLM to unify the API calling for all these LLMs.11 Given the unstable performance of LLMs, particularly closed-source ones, we run each model thrice during our experiments, selecting the median result from these repeated runs.

C MORE EXPERIMENT RESULTS AND DETAILS
=====================================

C.1 HUMAN EVALUATION ON LLM-GENERATED EXPLANATION We ask 5 annotators to evaluate the LLM-generated explanations. Specifically, each of them is assigned 4 or 5 papers, along with the corresponding experiment lists. For each paper, the annotator is given 5 different models outputs (model names are anonymized), and the annotator has to decide if each LLM-generated explanation is acceptable according to the experiment. We show the human evaluation results in Table 4,

D DATA CASES AND ANNOTATION PLATFORM ILLUSTRATION
=================================================

As shown in Figure 7, 8, and 9, we show the sample cases of the three tasks in AAAR-1.0. Meanwhile, we illustrate the screenshot of our annotation platform in Figure 6. Figure 6: The annotation platform for collecting the annotation of EXPDESIGN . We ask annotators to first make comments on the Google Drive PDF, then move all the annotations to the online Google Doc (for further verification and discussion).

E PROMPT TEMPLATES
==================

In this appendix, we attach all the prompts used in this work, including prompts in data collection and model prediction, as shown in Figure 10, 11, and 12. 10https://github.com/vllm-project/vllm 11https://github.com/BerriAI/litellm 22 Figure 7: sample case of EQINFER . Figure 8: sample case of EXPDESIGN . Figure 9: sample case of WEAKNESS . 23 Figure 10: The prompts used in EQINFER , including both data collection and model prediction. Figure 11: The prompts used in EXPDESIGN , including both data collection and model prediction. Figure 12: The prompts used in WEAKNESS .

