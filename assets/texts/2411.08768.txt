Title: Sharingan: Extract User Action Sequence from Desktop Recordings

Authors: Yanting Chen, Yi Ren, Xiaoting Qin, Jue Zhang, Kehong Yuan, Lu Han, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang


================================================================================

Abstract
========

Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.

Start
=====

Sharingan: Extract User Action Sequence from Desktop Recordings Yanting Chen1, Yi Ren2, Xiaoting Qin2, Jue Zhang2, Kehong Yuan1, Lu Han2, Qingwei Lin2, Dongmei Zhang2, Saravan Rajmohan2 and Qi Zhang2 4 2 0 2 3 1 ] . [ 1 8 6 7 8 0 . 1 1 4 2 : r Abstract Video recordings of user activities, particularly desktop recordings, offer rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research. I. INTRODUCTION Video recordings are increasingly favored for capturing user activities due to their ease of implementation and broad applicability. Moreover, videos universal compatibility across platforms and devices, combined with its ability to capture detailed and context-rich data, ensures minimal information loss and facilitates thorough analysis.

Recent
======

advancements in Vision-Language Models (VLMs) [21], [20], [8], [24], [5], [14], [1] have significantly improved the utility of video recordings. These AI-driven models automate the interpretation and extraction of insights from video data, enhancing the identification of user behaviors and patterns. Combined with the increasing prevalence of AI-integrated hardware [9], these technological innovations are accelerating the adoption of video as an essential tool for documenting and analyzing user activities. [22], Despite extensive research into understanding user actions from various types of videos [7], [12], there remains lack of focus on desktop recordings. Addressing this gap is crucial, as extracting user actions from desktop videos offers numerous benefits. For instance, it can enhance Robotic Process Automation (RPA) by utilizing demo videos as input, increasing productivity through automation [10]. Moreover, 1 Tsinghua University 2 Microsoft *Equal Contribution. Work is done during an internship at Microsoft. Corresponding author. Contact: jue.zhang@microsoft.com desktop video analysis facilitates the automatic creation of tutorials and guidelines, while also enabling the extraction of personalized interaction patterns, which can be leveraged elsewhere to create more personalized user experience. We propose two VLM-based methods for extracting user action sequences from desktop recordings. In the Direct Frame-Based Approach (DF), sampled video frames are directly input into VLMs, while the Differential FrameBased Approach (DiffF) first detects frame changes using computer vision techniques before interpreting them with VLMs. The key difference lies in whether explicit frame differences are incorporated to aid action inference. We evaluate both methods using two benchmark datasets: one crafted by us, focusing on individual action types, and the other adapted from GUI-World [3] which better reflects real-world scenarios. Experimental results reveal that current VLMs show great potential in extracting user actions from desktop recordings. For instance, using the DF approach, we achieve an accuracy of 70% 80% in identifying operation types (e.g., click), and the extracted action sequences are replayable through RPA-like processes. Moreover, comparing the two approaches reveals that VLMs struggle to utilize UI changes derived explicitly, sometimes leading to performance degradation. Thus, we recommend the Direct Frame-Based Approach, relying on VLMs inherent ability to infer actions, as the current best practice for action extraction. Our contributions can be summarized as follows: We introduce two VLM-based methods to address the gap in existing research on extracting user action sequences from desktop recordings. To the best of our knowledge, this is the first attempt to leverage VLMs for this task. We develop two benchmark datasets to assess the performance of methods on this task. All evaluation source codes and benchmarks will be made publicly available. We perform comprehensive evaluation of the proposed methods using the developed benchmarks. II. RELATED WORK VLMs in Robotics and Automation VLMs have been increasingly applied across various robotics and automation tasks. They have proven valuable for robot navigation [16], [34], [32], robot action recognition [31], [4], and task planning [25], contributing significantly to enhancing robotic visual perception capabilities [11], [17]. These works primarily focus on physical environments where VLMs help robots interpret and interact with the world around them, and (a) Direct Frame-Based Approach (b) Differential Frame-Based Approach Fig. 1: Architectures of Direct Frame-Based Approach (left) and Differential Frame-Based Approach (right). specific applications like autonomous driving [28] exemplify their utility in high-stakes scenarios. Our work focuses on robotic process automation in desktop environments. Recent studies, such as GUI Agent-related works [18], [33], [26], [15], demonstrate the potential of VLMs in automating user interface tasks by processing natural language inputs. These approaches largely ignore the vast potential of video as data modality. Our study is orthogonal but distinctby focusing on extracting user action sequences directly from desktop video recordings, we enable new downstream tasks such as task automation, personalized software tutorials, and workflow optimization in desktop RPA from demo videos. VLM-based Video Understanding The integration of large language models (LLMs) and VLMs into video understanding frameworks has seen rapid advancements. Existing approaches typically address video understanding by querying sampled frames and generating captions to retrieve visual information or answer questions [30], [6], [29], [13]. However, these works often remain focused on image-based tasks, failing to fully exploit the temporal dynamics of videos or tackle action-related tasks in complex settings. Our focus diverges from these existing works by emphasizing action recognition in dynamic video sequences, particularly within desktop environments where user activities are rich in temporal complexity. Recent efforts like Video-of-Thought [7] and Wolf [12] have begun to explore spatial-temporal understanding, but their experiments center on human action recognition in daily life or physical activities. By introducing dataset tailored for desktop environments, our framework and rigorous evaluation not only bridges the gap in extracting actionable insights from desktop recordings but also serves as stepping stone for future research in automating usercentric desktop tasks with video as primary input modality. III. METHODOLOGY This work focuses on extracting sequences of user actions from desktop video recordings by leveraging VLMs that the frame level.1 To achieve this, we analyze videos at uniformly sample frames = {Ft} from the video V, where the frame Ft is selected at time t. The extracted action sequence, represented as = {Ai}, consists of individual actions Ai described textually at step i. Each action Ai is characterized by tuple (Oi, Di, Ci), where Oi represents the operation type (e.g., click), Di details the operation specifics (e.g., the UI element interacted with), and Ci provides the context (e.g., the application in use). This study considers five operation types: click, select, scroll, drag, and type. An illustrative example of an action is (click, Styles dropdown menu, Microsoft Word). We explore two methodologies for extracting actions from sampled video frames. The first, the Direct Frame-Based Approach (DF), directly inputs sampled frames into VLMs to generate an action sequence. This approach necessitates that VLMs adhere to specific instructions embedded in the prompts, which are crafted using prompt tuning techniques such as in-context learning and chain-of-thought reasoning. In contrast, the Differential Frame-Based Approach (DiffF) first utilizes computer vision techniques to explicitly identify regions of change between consecutive frames. VLMs are then used to interpret these changes and generate the action sequence. The primary distinction between these methods lies in whether the frame differences are explicitly extracted, enabling an investigation into whether this extraction enhances action inference by VLMs. The following sections provide detailed description of each approach, with their architectures depicted in Figure 1. A. Method I: Direct Frame-Based Approach As shown in Figure 1a, DF consists of three modules: Action Proposer, Action Corrector and Action Merger. Since current VLMs often only handle limited number of images in one call, we use sliding window to process frames at time, allowing overlapping frames. The Action Merger combines results from all windows. If frames can fit within the VLMs context, the sliding window and Action Merger are unnecessary. Each module is described in detail below. Action Proposer. This module proposes candidate action sequence for sampled frames within each time window. The prompt2 used for this task, detailed in Table VIII in the Appendix, directs the VLM to focus on changes potentially related to user actions. For instance, when inferring click, the VLM utilizes several features: i) mouse shape change, 1The inclusion of audio modality is beyond the scope of this work. 2Due to space limit, this and following prompts are not included in current draft. We intend to open source them in supplementary materials. such as an arrow-like pointer shifting to hand when hovering over clickable element; ii) changes in the UI elements state, like background color adjustments when the mouse hovers; and iii) new events triggered, such as opening new window or expanding menu. By synthesizing these visual cues across frames, the VLM determines the users action type as click, identifies the clicked UI element as the action detail D, and specifies the application name as context C. Additionally, the VLM generates intermediate outputs, including mouse position, preand post-action element states, and its reasoning process. These outputs support the Action Corrector in performing error corrections. Action Corrector. This module corrects several types of errors presented in Action Proposers output, including: i) redundant actions, such as an unnecessary click action accompanying drag operation; ii) invalid actions, where the operation type, details, or context are inconsistent with other supporting information; and iii) missing information, such as incomplete operation details or context, which might be inferred from supplementary data. The prompt for this module is provided in Table XI in the Appendix. Action Merger. This module merges the generated action sequences across all sliding time windows, addressing issues related to fragmented operations from video slicing and redundant actions caused by overlapping windows. For example, consecutive select actions on text are consolidated into single select action by merging the operation details from each individual action. The prompt for this module is detailed in Table XII in the Appendix. B. Method II: Differential Frame-Based Approach The DiffF approach, illustrated in Figure 1b, consists of four components: Frame Difference Localizer, Frame Difference Descriptor, Action Proposer and Action Corrector. In contrast to the DF approach, DiffF introduces the Frame Difference Localizer and Descriptor to explicitly capture frame differences. The Action Proposer and Corrector modules, while playing similar roles to those in DF, are adapted to the DiffF framework. Note that DiffF bypasses previous input image limits for VLMs as frame difference generation requires only two consecutive frames. Additionally, since action proposing and correction are handled in text and VLMs typically support extensive textual context, Action Merger is generally unnecessary in DiffF, except for exceptionally long videos, which are not addressed in this work. Frame Difference Localizer. This module identifies and outputs screen regions that have changed between two consecutive frames. An example of the detected regions with annotated changes is illustrated in Fig. 2 in the Appendix. The localization process involves the following steps with the tools [2], [27]: Normalize the RGB pixel values of input frames to [0, 1]. Apply Gaussian blur with (5, 5) kernel size and standard deviation of 2 to reduce false-positive UI changes caused by high-frequency noise, commonly introduced by lossy video compression. This configuration is found to be effective at minimizing false positives while preserving essential, albeit minor, UI changes. Calculate the L2 norm of the difference in pixel values between the two frames and threshold the resulting difference with tuned value of 0.15 to create binary mask. Remove objects smaller than 10 pixels from the binary mask (as changes smaller than this threshold are generally imperceptible to the human eye) and identify connected components with their corresponding bounding boxes. Expand each bounding box by 100 pixels on all sides and merge any overlapping boxes to provide more visual context while reducing the number of regions to compare. Frame Difference Descriptor. This module utilizes VLM to generate detailed textual descriptions of UI changes based on current frame and detected changed regions as compared to previous frame in the above step. The prompt for this module is provided in Table XIII in the Appendix, along with sample output corresponding to the changes illustrated in Figure 2. The output includes overall frame context and specific details about the changed UI elements. However, since Descriptor is only provided with localized information (i.e., current frame and their frame differences), it does not generate action sequences directly. Action Proposer. After aggregating textual descriptions of UI changes across all frames, this module prompts the VLM to propose candidate actions, forming an action sequence for the entire video. The prompt used in this process is analogous to that in the DF approach and differs mainly in the supporting information part, which will be utilized in the next Action Corrector module. detailed description of the prompt is provided in Table XV in the Appendix. Action Corrector. Given that DiffF is prone to generate false positive actions due to extraneous information in the textual descriptions of UI changes, the Action Corrector incorporates an additional rule-based component not present in DF. While the VLM-based corrector in DiffF employs similar prompt to the Action Corrector in DF, the rule-based component specifically targets the elimination of scroll actions without corresponding UI movement and click actions where the cursor is absent from the evidence. IV. EVALUATION In this section, we first introduce two benchmark datasets ACTONE and ACTREAL, followed by description of the evaluation methods and metrics used in the experiments. A. Benchmark Datasets ACTONE. Using OBS Studio [19] for screen recording and manual annotation of action sequences, we built the ACTONE dataset, specifically designed to evaluate VLMs fundamental abilities in action sequence extraction tasks. This dataset includes five operation types of (click, select, Dataset ACTONE ACTREAL Case Domain click select scroll drag type All Software Website Multi All Total Videos 14 11 6 5 4 40 23 15 3 41 Frame Count 276 242 292 285 255 277 684 985 836 805 Action Count 1.7 1.0 1.2 1.4 1.5 1.3 7.5 8.3 6.0 7. Unique Action Type Count 1.1 1.0 1.2 1.4 1.3 1.2 3.0 3.1 3.3 3.1 TABLE I: Statistics of benchmark datasets ACTONE and ACTREAL. The numbers in the last three columns are the average values computed within each respective case domain. scroll, drag, and type), covering common usage scenarios in desktops, web browsers and popular applications. Since the goal is to assess fundamental capabilities, the videos do not contain overly complex actions; most of them only include single action type. Additionally, as current VLMs often allows no more than ten images per call, to ensure fair comparison across different models, all videos in this dataset are limited to maximum duration of 10 seconds. Details on video and action statistics is provided in Table I. ACTREAL. To evaluate real-world user activities, we construct the ACTREAL dataset adapted from GUI-World [3], collection of videos sourced from YouTube. We select videos from the Software, Website, and Multi categories to align with our focus on desktop recordings. Videos containing hover actions or non-input keyboard actions are filtered out, and we limit the dataset to videos with 6-10 actions and at least 3 unique action types to ensure diversity and complexity. After filtering, 36 videos remain in the Website category, 12 in Multi, and 186 in Software. Upon manual review, many videos exhibit quality issues like cropped displays, low resolution, obstructions from watermarking, or incomplete/wrong annotations. We discard these videos and manually re-annotate the action sequences for the remaining ones. The final dataset consists of 41 videos. As shown in Table I, compared to ACTONE, the average number of actions per video increases approximately five-fold, and the number of unique actions per video is tripled, making this dataset significantly more complex and diverse. B. Evaluation Methods and Metrics We primarily evaluate the predicted action sequences by comparing them with the ground-truth sequences in the benchmark datasets. Although the comparison is performed in semantic space rather than through exact word matching, errors still arise due to the variability in how the same UI object can be described in different ways. To further validate the proposed semantic comparison metrics, we introduce another functional metric derived by replaying the predicted action sequences using VLM-based UI automation tool in the same environment as the benchmark dataset. If the final outcome matches the ground-truth video, it confirms the accuracy of the action sequence. This replay approach mirrors the Robotic Process Automation (RPA) process, key potential application of this work. Below, we provide detailed description of both evaluation methods. Semantic Comparison. The comparison is performed at the individual video level by analyzing two sequences of text strings: the predicted action sequence, Ap = {(Op ) Lp}, and the ground-truth action sequence, Ag = {(Og ) Lg}. These sequences may have differing lengths, as denoted by Lp and Lg. , Dp , Dg , , ij , SD ij , and SC Our comparison process begins by computing three similarity matrices, SO ij , for each action element, where Lp and Lg. Since the operation type is discrete and limited to five categories, SO ij is computed via exact matching, producing binary (0-1) matrix. For SD ij and SC ij , semantic matching is performed by: i) generating BERT embeddings [23] for the operation detail and context elements from both the prediction and groundtruth sets; ii) computing pairwise cosine similarity between the embeddings; iii) applying manually tuned threshold of 0.7 to convert similarity scores into binary matrix. Once the binary similarity matrices for all three action components are obtained, the overall similarity matrix Sij is calculated through element-wise multiplication. We next perform matching between Ap and Ag and count the matched pairs for metrics computation. The matching algorithm involves three steps: i) iterating over groundtruth actions in chronological order; ii) for each ground-truth action, identifying the first unmatched predicted action that aligns with it (indicated by 1 in Sij), and treating them as match; iii) counting the number of matched pairs m. Algorithm 1 Matching algorithm Input: Sij Output: number of matched pairs has been matched previously [False for = 1 . . . Lp]: [i] represents whether Ap 0 for 1 to Lg do for 1 to Lp do if [i] = False and Sij = 1 then + 1 [i] True break end if end for end for return Finally, with the obtained similarity matching result list S, we compute the Precision (P ) and Recall (R) metrics. Recall is defined as the ratio of correctly predicted actions to the total number of actions in the ground-truth, i.e., = m/Lg. Similarly, Precision is the ratio of correctly matched actions to the total number of predicted actions, calculated as = m/Lp. To gain deeper insight into the matching performance, we compute two sets of Precision and Recall metrics: one considering the full matching of all three action elements, and the other focusing solely on operation type. This allows us to assess both the overall accuracy and the accuracy specifically at the operation-type level. RPA-like Replay. To validate the semantic comparison metrics, we conducted replay tests of predicted action sequences in the same environment as the original video recordings. Due to the lack of replay environment for the ACTREAL dataset as it is collected elsewhere, our analysis focuses on the ACTONE dataset. We employed modified version of the VLM-based UI navigation tool [33], which accepts an action sequence in natural language as input. Despite extensive customization efforts, current implementation supports only nine cases, primarily involving click and type actions. The replay results (summarized in Table VII in the Appendix) reveal that out of the nine test cases, six were successfully replayed with the predicted action sequences. Two of the three unsuccessful cases exhibited lower Precision and Recall values, whereas the six successful cases all achieved one for both Precision and Recall. This validation experiment demonstrates that the Precision and Recall metrics derived from semantic matching are consistent with actual success rates of RPA replay, thus validating their effectiveness as indicators of action extraction performance. V. EXPERIMENT RESULTS This section presents the experimental results of evaluating the DF and DiffF methods using the ACTONE and ACTREAL datasets. We also undertake thorough error analysis to identify potential root causes of failures. Additionally, we perform several ablation studies to gain deeper insights into the effectiveness of our proposed methods. A. Setup In our experiments, we implement both DF and DiffF approaches in Python and utilize two prominent series of VLMs: the GPT series (GPT-4o/4o-mini) [20] and the Gemini series (Gemini1.5-Pro/Flash) [8]. Note that we intentionally select large and small models for each series to study if the model size plays crucial role in our task. For all VLMs, we set the temperature to 0 and use the default API settings for other parameters. Given that the GPT series permits maximum of 10 images, we configure the window size to 10 frames, with an overlap of 5 frames in the DF approach. The frame sampling rate is set to one frame per second for ACTONE, whereas ACTREAL employs rate of 2 frames per second due to the faster pace of user actions in ACTREAL. B. Results for the ACTONE Dataset The evaluation results for ACTONE is given in Table II. It includes the Precision and Recall metrics for the assessment of all three action elements (denoted as All) and restricted evaluation focusing solely on the operation type (denoted as Operation). Key observations include: noitemsep, left=0pt Model Comparison: GPT-4o outperformed all other models across the four metrics for both DF and DiffF Method Model DF DiffF Gemini1.5-Pro Gemini1.5-Flash GPT-4o GPT-4o-mini Gemini1.5-Pro Gemini1.5-Flash GPT-4o GPT-4o-mini Recall (Operation) 0.71 0.69 0.83 0.63 0.75 0.74 0.87 0.59 Precision (Operation) 0.73 0.59 0.81 0.33 0.48 0.37 0.66 0.26 Recall (All) 0.49 0.30 0.71 0.38 0.45 0.54 0.76 0.45 Precision (All) 0.51 0.26 0.68 0.17 0.24 0.27 0.59 0.19 TABLE II: Evaluation results for the ACTONE dataset. methods, followed by Gemini1.5-Pro. The Precision and Recall values, ranging from 0.6 to 0.85, highlight the potential of VLMs for extracting user actions from desktop recordings. Conversely, the smaller models, GPT4o-mini and Gemini1.5-Flash, showed marked decline in performance, underscoring the inherent difficulty of the task. DF vs. DiffF: DF and DiffF exhibit comparable perthough DiffF shows slightly lower Preciformance, sion. This observation implies that incorporating explicit frame differences might not be essential for the current VLMs. Operation vs. All: Performance degradation from evaluating only operation type to full evaluation is more significant in smaller models than in larger ones. We also study the breakdown analysis by case domain (i.e., operation type) for both methods with the results for GPT4o and Gemini1.5-Pro given in Table VI in the Appendix. The results reveal significant performance variation across operation types for different models and methods, with no clear indication of which operation type is consistently easier to extract. This suggests that these operation types may present comparable levels of difficulty for current VLMs. C. Results for the ACTREAL Dataset The evaluation results for ACTREAL is depicted in Table III, with same column settings as ACTONE. We observe that: noitemsep, left=0pt ACTREAL vs. ACTONE: The performance of both methods declined on ACTREAL compared to ACTONE, particularly in the All type metrics. This suggests notable domain shift between the datasets, with ACTREAL presenting greater challenges for VLMs. DF vs. DiffF: DF shows less decline compared to DiffF, suggesting that DF is better suited for real-world scenarios. Model Comparison: For DF, GPT-4o outperforms all other models; as for DiffF, Gemini1.5-Pro has the best operation Recall and Precision, whereas GPT-4o has best overall Precision and Recall. Operation vs. All: For both methods and all models, Precision and Recall under All conditions significantly decrease compared to Operation counterpart, unlike in ACTONE. This suggests that extracting details and context is more challenging in ACTREAL. closer includes examination of ACTREAL reveals that it Method Model DF DiffF Gemini1.5-Pro Gemini1.5-Flash GPT-4o GPT-4o-mini Gemini1.5-Pro Gemini1.5-Flash GPT-4o GPT-4o-mini Recall (Operation) 0.73 0.77 0.82 0.73 0.64 0.59 0.38 0.30 Precision (Operation) 0.72 0.39 0.70 0.46 0.79 0.43 0.78 0.59 Recall (All) 0.37 0.47 0.53 0.41 0.22 0.26 0.27 0.13 Precision (All) 0.32 0.22 0.45 0.27 0.26 0.16 0.54 0. TABLE III: Evaluation results for the ACTREAL dataset. Method DF DiffF Visual Hallucination 7 8 Visual Blindness 5 7 Inadequate Reasoning 2 Poor Instruction-Following 1 4 TABLE IV: Count of failed cases when applying GPT-4o to the ACTONE dataset. diverse range of UI elements that are difficult to describe unambiguously, even for humans. Additionally, there are more frequent screen changes unrelated to user actions, such as sudden pop-ups, which introduce additional noise into VLM inputs. Consequently, the task of capturing accurate details and context in ACTREAL is notably more complex. D. Error Analysis We proceed with detailed analysis of the failure cases in the DF and DiffF methods. Specifically, we focus on cases exhibiting errors in Precision and Recall metrics when applying GPT-4o to the ACTONE dataset. Upon examination, they can be categorized into the following four error types: Visual Hallucination: VLMs sometimes generates hallucinated content when interpreting visual inputs. For instance, the Frame Difference Descriptor in DiffF may incorrectly detect slight scroll bar movement when it appears or disappears, leading the downstream Action Proposer to falsely suggest scroll action. Inadequate Reasoning: VLMs may exhibit Visual Blindness: VLMs occasionally fail to detect critical UI changes. For example, in failed case involving the drag of browser tab, the Frame Difference Descriptor in DiffF fails to capture the tab movement. insufficient reasoning over contextual information. It is often observed in the Action Proposer/Corrector when inferring/correcting actions based on prior outputs. Inadequate reasoning typically involves the inability to apply or recognize domain knowledge. For instance, the Action Proposer in DiffF suggests an incorrect click based on style change in drop-down menu during mouse hover, which could have been avoided by considering the lack of menu expansion. Poor Instruction-Following: VLMs can fail to follow complex or lengthy instructions. For example, the Action Corrector may not update the type action details, even when explicitly prompted to use supplementary data. Table IV summarizes the count of failed cases when applying GPT-4o to the ACTONE dataset using both DF and DiffF. single failure may encompass multiple error types, particularly in the DiffF method. The majority of failures in DF are due to visual issues, while DiffFs failures mainly stem from challenges in reasoning. This can be explained by the fact that DF relies heavily on visual capabilities to detect UI changes and determine actions. In contrast, DiffF struggles with inferring actions by reasoning over complex UI change descriptions, which often include irrelevant details. The results in Table IV suggest that enhancing the visual capabilities of VLMs would significantly improve DF performance. However, for DiffF, reasoning and instructionfollowing capabilities are equally essential. E. Ablation Study Effectiveness of Action Corrector. We evaluated the Action Correctors importance in DF and DiffF by removing it when using GPT-4o. The results (Row 2 and 3 in Table V) show significant performance drops across almost all metrics, confirming its essential role in both methods. Impact of Sliding Window. The sliding window in DF addresses GPT-series input limits, but Gemini-series models have more relaxed limit, allowing us to test performance without it. As shown in Row 4 of Table V, removing the sliding window reduces Recall but increases Precision. similar trend occurs without window overlap (not shown), suggesting that without these mechanisms, VLMs produce fewer actions, missing some but reducing irrelevant ones. Role of Explicit UI Change Extraction. Comparing DF and DiffF reveals that DF relies solely on VLMs capabilities, while DiffF depends on explicit UI change descriptions. This raises the question of whether combining original frames with explicitly extracted UI changes could enhance performance. To explore this, we conduct two ablation studies. In the first study, we add all video frames as extra visual input to the Action Proposer in DiffF. As shown in Row 5 of Table V, while some metrics improve, the gains are marginal. In the second study, we introduce bounding boxes around changed regions in DFs input frames. The results (last row of Table V) show no improvement and even some decline. Detailed analysis reveals that the decline in Precision can be attributed to an overemphasis on localized screen areas, neglecting the broader context.

These studies suggest
=====================

that augmenting VLM attention through explicit UI changes may misguide the model, leading to narrow focus at the expense of critical global information. Given DFs strong performance on both ACTONE and ACTREAL datasets, the most effective method for action extraction with current VLMs may be to rely on their intrinsic capabilities, as attempts to enhance them with explicit UI change extraction may introduce unnecessary confusion. VI. DISCUSSION The ability of VLMs to extract user actions from desktop recordings opens up significant opportunities. One notable Method + Model + Dataset (Variation) DF + GPT-4o + AO (w/o Action Corrector) DiffF + GPT-4o + AO (w/o Action Corrector) DF + Gemini1.5-Pro + AR (w/o Sliding Window) DiffF + GPT-4o + AO (add frames to Proposer) DF + GPT-4o + AO (w/ region diff annotation) Recall (Operation) 0.83 (0.76) 0.87 (0.89) 0.73 (0.51) 0.87 (0.88) 0.83 (0.84) Precision (Operation) 0.81 (0.63) 0.66 (0.46) 0.72 (0.91) 0.66 (0.69) 0.81 (0.61) Recall (All) 0.71 (0.48) 0.76 (0.35) 0.37 (0.22) 0.76 (0.76) 0.71 (0.60) Precision (All) 0.68 (0.40) 0.59 (0.21) 0.32 (0.36) 0.59 (0.61) 0.68 (0.42) TABLE V: Ablation study results for several method variations on ACTONE (AO) and ACTREAL (AR). Default method results are shown outside brackets (from Tables II and III), with corresponding variation results in brackets. application is Robotic Process Automation, as evidenced by our metric validation test, showing that user actions extracted using our methods can be replayed through VLM-based UI automation tools. Additionally, this technology encourages the adoption of video as another modality for logging user activities (i.e., vlogs). For example, documenting best practices or preparing guidelines traditionally involves timeconsuming written documentation. With VLMs, we now can generate guidelines more efficiently by recording demonstration videos from which VLMs can derive the necessary information. Furthermore, vlogs can be used for capturing personalized activities, with VLMs analyzing this data to identify individual interaction patterns and integrate them into other tools for ubiquitous personalization. VII. CONCLUSIONS In this paper, we proposed two novel VLM-based methods for extracting user actions from desktop recordings: the Direct Frame-Based Approach (DF) and the Differential FrameBased Approach (DiffF). Our evaluation shows that the DF approach is more effective, achieving higher accuracy in identifying actions. These methods have significant potential for applications in Robotic Process Automation, video-based tutorials and guideline generation, and user personalization. This work serves as foundation for future research in desktop video action extraction, with opportunities for refining VLM capabilities and exploring broader applications.

REFERENCES
==========

[1] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Manas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. An introduction to vision-language modeling, 2024. [2] G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. [3] Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, and Lichao Sun. Gui-world: dataset for gui-oriented multimodal llm-based agents, 2024. [4] Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, and Mengyuan Liu. Vg4d: Vision-language model goes 4d video recogIn 2024 IEEE International Conference on Robotics and nition. Automation (ICRA), pages 50145020, 2024. [5] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd, 2024. [6] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding, 2024. [7] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1310913125. PMLR, 2024. [8] Google. Gemini models. https://deepmind.google/ technologies/gemini/, 2024. Accessed: 2024-09-15. [9] Humane. Ai pin. https://humane.com/aipin, 2024. Accessed: 2024-09-15. [10] Arushi Jain, Shubham Paliwal, Monika Sharma, Lovekesh Vig, and Gautam Shroff. Smartflow: Robotic process automation using llms, 2024. [11] Ivan Kapelyukh, Yifei Ren, Ignacio Alzugaray, and Edward Johns. Dream2real: Zero-shot 3d object rearrangement with vision-language In 2024 IEEE International Conference on Robotics and models. Automation (ICRA), pages 47964803, 2024. [12] Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, Xinshuo Weng, Fuzhao Xue, Andrew Tao, Ming-Yu Liu, Sanja Fidler, Boris Ivanovic, Trevor Darrell, Jitendra Malik, Song Han, and Marco Pavone. Wolf: Captioning everything with world summarization framework, 2024. [13] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2023. [14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [15] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?, 2024. [16] Kangcheng Liu. Online robot navigation and manipulation with distilled vision-language models, 2024. [17] Zijun Long, George Killick, Richard McCreadie, and Gerardo AragonCamarasa. Robollm: Robotic vision tasks grounded on multimodal In 2024 IEEE International Conference on large language models. Robotics and Automation (ICRA), pages 1242812435, 2024. [18] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. [19] OBS. Obs. https://obsproject.com/, 2024. Accessed: 2024-09-15. [20] OpenAI. Gpt-4o. https://platform.openai.com/docs/models/ gpt-4o, 2024. Accessed: 2024-09-15. [21] OpenAI. Gpt-4v. https://platform.openai.com/docs/models/ gpt-4-turbo-and-gpt-4, 2024. Accessed: 2024-09-15. - always-on wearable ai. Owl [22] Owl. https://github.com/ OwlAIProject/Owl, 2024. Accessed: 2024-09-15. [23] pypi. bert-embedding. bert-embedding/, 2024. Accessed: 2024-09-15. https://pypi.org/project/ [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [25] Keisuke Shirai, Cristian C. Beltran-Hernandez, Masashi Hamaya, Atsushi Hashimoto, Shohei Tanaka, Kento Kawaharazuka, Kazutoshi Tanaka, Yoshitaka Ushiku, and Shinsuke Mori. Vision-language performance in type and click actions is relatively weaker. Overall, for the Diff approach, GPT-4o and Gemini show different strengths: GPT-4o performs better in click and select actions but worse in scroll, whereas Gemini excels in scroll actions. Then, we describe the replay results on selected samples from the ACTONE dataset, where we reproduced 9 cases from the dataset, with the results summarized in the Table VII. In the result, two-thirds of the cases can be successfully reproduced. C. Prompt We also list all relevant prompts. First, we present the prompt for the action proposer in the DF method, as shown in the Table VIII. Then, we provide the prompt for the corrector in the DF method, followed by the prompt of the Frame Difference Descriptor, as shown in the corresponding Table XI and XIII. Finally, we present the prompts for the Action Proposer and the Action Corrector, as summarized in the Table XV and Table XVIII. In 2024 IEEE International interpreter for robot Conference on Robotics and Automation (ICRA), pages 20512058, 2024. task planning. [26] Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, Ruyi An, Molei Qin, Chuqiao Zong, Longtao Zheng, Yujie Wu, Xiaoqiang Chai, Yifei Bi, Tianbao Xie, Pengjie Gu, Xiyun Li, Ceyao Zhang, Long Tian, Chaojie Wang, Xinrun Wang, Borje F. Karlsson, Bo An, Shuicheng Yan, and Zongqing Lu. Cradle: Empowering foundation agents towards general computer control, 2024. [27] Stefan van der Walt, Johannes L. Schonberger, Juan Nunez-Iglesias, Francois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-image: image processing in Python. PeerJ, 2:e453, 2014. [28] Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, Yutong Ban, Alexander Amini, Guy Rosman, Sertac Karaman, and Daniela Rus. Drive anywhere: Generalizable end-to-end autonomous driving with multimodal foundation models, 2023. [29] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent, 2024. [30] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive treebased video representation for llm reasoning on long videos, 2024. [31] Chao Wei and Zhidong Deng. Incorporating scene graphs into pretrained vision-language models for multimodal open-vocabulary action recognition. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 440447, 2024. [32] Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, and Bernadette Bucher. Vlfm: Vision-language frontier maps for zeroshot semantic navigation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 4248, 2024. [33] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qin Zhang. Ufo: ui-focused agent for windows os interaction, 2024. [34] Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, and K. W. Samuel Au. Interactive navigation in environments with traversable obstacles using large language and vision-language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 78677873, 2024. VIII. APPENDIX A. Details of Methodology We provide further details regarding the methodology employed in this work. First, we demonstrate the implementation of the Frame Difference Localizer, as illustrated in the Figure 2. Subsequently, we present an example output from the Frame Difference Descriptor, as depicted in the corresponding Figure 3. B. Details of Experimental Results We provide additional experimental details. We first present the case domain-level experimental results on the ACTONE dataset, as shown in the Table VI. The key observations of the results are as follows: For the Direct Frame-Based (DF) approach, from the perspective of assessing all three action elements (ALL), GPT-4o performs best in clicktype cases, followed by scroll-type, and performs worst in drag-type cases. Gemini performs best in type-type cases, followed by click-type, and performs worst in select-type cases. Overall, for the DF approach, both models perform better in click, scroll, and type actions, but perform worse in drag and select actions. For the Differential Frame-Based (Diff) approach, GPT-4o performs best in click-type cases, followed by select-type, but performs worst in scroll-type cases. Gemini performs best in scroll-type cases, while its

APPENDIX I
==========

Fig. 2: An example output from Frame Difference Localizer. Left and right are the changed regions with red bounding boxes in the previous and current frames, respectively. { } "global_description": "The whole screenshot mainly contains desktop with several icons, including Recycle Bin, folder named 'demo', Google Chrome, and OBS Studio. The taskbar is visible at the bottom.", (cid:44) "description": "The region contains folder named 'remain' and part of the desktop background. The folder is selected in the new image.", (cid:44) "changed": true, "old_cursor_shape": "normal", "new_cursor_shape": "normal", "changes": [ { } "subject": "folder", "type": "style_change", "old": "The folder 'remain' is not selected.", "new": "The folder 'remain' is selected with blue checkmark and blue selection box around it.", "message": "The folder 'remain' has been selected." ], "frame": 2, "index": 0, "id": "2_0" Fig. 3: An example output of Frame Difference Descriptor. It is derived from the identified UI changes in Figure 2. Method Model DF Diff GPT-4o Gemini1.5-Pro GPT-4o Gemini1.5-Pro Case Domain (Video Count) click (14) select (11) scroll (6) drag (5) type (4) click (14) select (11) scroll (6) drag (5) type (4) click (14) select (11) scroll (6) drag (5) type (4) click (14) select (11) scroll (6) drag (5) type (4) Recall (Operation) 0.94 1.00 0.75 0.40 0.67 0.85 0.64 0.58 0.70 0.67 0.82 1.00 0.83 0.70 0.92 0.69 0.73 0.92 0.80 0.67 Precision (Operation) 0.88 0.95 0.75 0.50 0.63 0.87 0.64 0.67 0.63 0.75 0.71 0.77 0.41 0.37 0.88 0.49 0.44 0.60 0.33 0.55 Recall (All) 0.90 0.64 0.75 0.30 0.67 0.63 0.18 0.58 0.50 0.67 0.82 0.82 0.67 0.70 0.58 0.36 0.45 0.75 0.60 0. Precision (All) 0.81 0.64 0.75 0.40 0.63 0.65 0.18 0.67 0.43 0.75 0.71 0.64 0.37 0.37 0.63 0.18 0.21 0.57 0.22 0.13 TABLE VI: Evaluation results for the ACTONE dataset categorized by case domain. Test Case in ACTONE click/icon/taskbar click/text/checkbox click/text/dropdown click/text/link click/text/text field click/text icon/menu click/text icon/tab type/number type/word Semantic Matching Precision (All) 1 1 0.5 1 1 1 1 0.5 Recall (All) 1 1 0.5 1 1 1 1 1 1 Successful Replay? yes yes no yes yes yes no no yes TABLE VII: Replay results of test cases in ACTONE. Precision and Recall metrics from semantic matching are shown for comparison. - You are screen recording video analysis agent. - Your task is to first identify the user's <Operation> from the provided <Video> and then output the (cid:44) full user's <Operation Sequence>. # <Video> description: - <Video> is sperated into screenshot frames. Each frame is snapshot of the application interface, and the user may interact with the interface elements. (cid:44) - The computer environment can be Windows, Mac, or Linux operating systems. - The application consists of Office 365, desktop, web browser, and other applications. - The application interface element is composed of multiple elements, e.g., buttons, dropdowns, icons, text fields, and other interactive elements. (cid:44) - The user's <Operation> on the application interface element consists of click, drag, scroll, select, and type operations, e.g., click on button, drag an icon, scroll page, select text, or type in text field. (cid:44) (cid:44) # <Operation> description and identification methods: ## click Description: - The user clicks on an interface element(e.g., button, link, or icon), activating the (cid:44) element(e.g., button press effect), and triggering various events(e.g., opening new window, expanding menu, changing the state of an element). (cid:44) - If you think the user's operation is "click", you need to keep observing several frames to see if (cid:44) the operation is "drag" or "select", which contains the "click" operation. Identification: - By the change of mouse: - shape change: The mouse may change shape (e.g., pointing hand when clicking link). - By the change of interface element: - press effective: Buttons or other clickable elements display press effect (e.g., changing (cid:44) color, showing shadow, slightly changing shape, checkbox gets checked). - By the change of display: - feedback message: The interface displays feedback messages or changes(e.g., new window opens, (cid:44) ## select menu expands). Description: - Text selection: The user selects text in document, highlighting the selected text with different background color. For example, select two sentences in Microsoft Word. (cid:44) - Icon selection: The user selects icons on desktop or in an application, the selected icons should be enclosed within blue rectangular box. For example, select two icons on the desktop. (cid:44) - Cell selection: The user selects cells in spreadsheet or table, highlighting the selected cells (cid:44) with different background color. For example, select three cells in Excel. Identification: - By the change of mouse: - Text selection: - mouse position: The position of the mouse indicates the start and end of the selection range. (cid:44) (cid:44) You should observe the mouse movement over the text to identify the selection.(e.g., from the beginning of the first sentence to the end of the second sentence) - Icon selection: - Mouse position: mouse move over the icons, and the selected icons should be enclosed within blue rectangular box. You should observe the blue rectangular box region to identify the number of selected icons. (cid:44) (cid:44) - By the change of interface element: - color change: The background color of the selected text or selected icons changes to indicate (cid:44) ## type the selection.(e.g., from white to blue) Description: - Text input: The user types text into text field or document, entering characters, words, or sentences. For example, typing in search bar, filling out form, or writing an email. (cid:44) - Command input: The user types commands or inputs specific text strings to perform actions or (cid:44) trigger events. For example, typing commands in terminal. Identification: - By the change of interface element: - text change: The content of the text field changes as the user types, updating the displayed (cid:44) text. ## scroll Description: - Vertical scroll: The user scrolls vertically through document or interface, moving the content up or down to view more information. (cid:44) - Horizontal scroll: The user scrolls horizontally through document or interface, moving the (cid:44) content left or right to view more information. Identification: - By the change of mouse: - mouse position: The mouse may move to the scroll bar or scroll area, indicating the intention to (cid:44) scroll. CONTINUE ON THE NEXT PAGE TABLE VIII: Prompt of action proposer of DF method: 1/3 - By the change of interface element: - scroll bar movement: Observe the movement of the scroll bar. The vertical scroll bar is usually located on the right side of the interface, and the horizontal scroll bar is usually located at the bottom of the interface. (cid:44) (cid:44) - By the change of display: - Content movement: The content of the document or interface moves up or down (vertical scroll) or (cid:44) ## drag left or right (horizontal scroll) as the user scrolls. Description: - Icon drag: The user drags an icon, moving it from one location to another within the interface. For example, dragging file to folder, moving an icon on the desktop. (cid:44) - Text drag: The user drags selected text, moving it to different location within document or (cid:44) interface. For example, rearranging paragraphs in document. Identification: - By the change of position of the icon or text: - object position change: The object or text moves with the mouse, indicating the drag operation. - By the change of mouse or mouse: - Mouse position: The mouse moves with the dragged object or text. - By the change of interface element: - drag indicator: The object or text being dragged shows visual indicator of movement, such as (cid:44) shadow, outline, or placeholder. # The <Difference> between operations: - Drag vs. click: Drag operation include click operation, but drag operation involves moving an object from one location to another, while click operation involves triggering actions or changing the state of elements. (cid:44) (cid:44) - Drag vs. select: - Drag operation involves moving an object from one location to another, while select operation involves highlighting text or selecting items, don't involve moving objects. (cid:44) - Icon selection is type of select operation, not drag operation. - Select vs. click: Select operation include click operation, but select operation involves (cid:44) highlighting text or selecting items, while click operation involves triggering actions or changing the state of elements. (cid:44) - Click vs. scroll: The click operation involves triggering actions or changing the state of elements, (cid:44) while the scroll operation involves moving the content up or down to view more information. # General guidelines for extracting <Operation Sequence>: - The <Operation Sequence> should be ordered by the timestamp of the operations, starting from the earliest operation to the latest operation. (cid:44) - For each frame, you should do as follows: - First focus on the position of the mouse and the elements near the mouse. - Identify the user's operation <Operation> and the associated information. - If the operation is continuous(e.g., drag, select, scroll), you should observe several frames to identify the complete operation. E.g., if the user is selecting multiple sentences, you should observe the selection process until the user finishes the selection. (cid:44) - If the operation is compound, keep observing several frames to identify the complete operation. - If the operation is not clear, you can skip it. (cid:44) - For each operation, extract the following information: - "frame_total": The total number of frames in the video. - "frame_idx": The start and end frame index of the operation, e.g., [1, 3] means the operation is from the 1st frame to the 3rd frame. (cid:44) - "mouse_position": The position of the mouse or pointer on the screen, described in relation to the interface elements or text near it. (cid:44) (cid:44) - "element_state_pre_interaction": The state of the element before the interaction. If the operation is scroll, you should include the content before the scroll. If operation is drag, click, or select, you should include the state of the element near the mouse. (cid:44) - "element_state_after_interaction": The state of the element after the interaction. If the operation is scroll, you should include the content after the scroll. If the operation is drag, click,or select, you should include the state of the element near the mouse. (cid:44) - "Thoughts": Why you think the operation is the specific category . (e.g., "The mouse clicks the icon (cid:44) and move it to another location, which indicates drag operation.") (cid:44) - "operation_category": The category of operation. - It can only one of the following: "Click", "Drag", "Scroll", "Select", "Type". - Choose the one based on principle as follows: - Your choice should be based on thought, mouse position, element_state_pre_interaction, element_state_after_interaction. (cid:44) - You should distinguish between click, select, drag, and click operations according to the <Difference> between operations. (cid:44) - If the operation is "drag" or "select", you should choose "drag" or "select" instead of "click". - "application": The name of the application that the user is interacting with, including the (cid:44) application category and identifier. - category: The application category can include but not limit to one of the following: Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Web Browser, Windows OS, etc. (cid:44) - identifier: The application identifier should be more specific description of the application CONTINUE ON THE NEXT PAGE TABLE IX: Prompt of action proposer of DF method: 2/3 - If category is "Web Browser", the identifier can be the name of the website (e.g., "Google", "YouTube", "Apple Music"). (cid:44) - if category is "Windows OS", the identifier can be the name of the window or the desktop (e.g., "Desktop", "Taskbar", "File Explorer", "Menu"). (cid:44) - If there is no specific identifier, leave it empty(e.g., "")." - "target_object": The object in "application" that the user interacted with, including the object (cid:44) category and identifier. - category: The object category can only be one of the following: "button", "text field", "icon", (cid:44) (cid:44) "dropdown", "list", "scroll bar", "document", "webpage", "dialog box", "menu", "file", "folder", "checkbox", "radio button", "search bar", "form", "email", "paragraph", "sentence", "word". (cid:44) - identifier: The identifier should be description or name of the object (e.g., "Bold button", (cid:44) "Main Text Area", "File icon"). - If there is no specific identifier, leave it empty(e.g., "")." - For the "scroll bar" category, the identifier should be the direction of the scroll bar and the (cid:44) subject that the scroll bar control.(e.g. horizontal scroll bar of sheet1) - "additional_info": Any additional information related to the operation, such as the direction of scroll, the amount of scroll, the content typed, or the location of selected text or icons. - "additional_info" is optional and should be included only for the operation category "scroll", (cid:44) "type", and "select", for other categories, you can leave it empty(e.g., ""). (cid:44) - For the "scroll" operation, include the direction of scroll ("up" or "down", "left" or "right") (cid:44) and the amount of scroll (e.g., "half page", "two lines", "until that you can see the yellow icon in the dropdown list"). (cid:44) - For the "select" operation, include the content selected. (cid:44) - For text selection, include the specific text selected. The granularity of the selection can be at the paragraph, sentence, or word level. e.g., "hello" in world level, "Hello World" in sentence level, "Hello World, How are you?" in paragraph level. (cid:44) - For icon selection, include the selected icons. The granularity of the selection can be at the icon level. e.g., "google icon", "apple icon". (cid:44) - For cell selection, include the selected cells. The granularity of the selection can be at the (cid:44) cell level. e.g., "A1", "B2". - For the "type" operation, include the content typed. - If the user types character by character, concatenate multiple characters into one word.(e.g., concatenate "H", "e", "l", "l", "o" into "Hello"). (cid:44) - If the word typed is too long, only output the number of sentences or the first few words. For (cid:44) example, "the first sentence", "the first three words". - For the "drag" operation, include the initial and destination position of the object. For example, (cid:44) "from the right to left". - "abstract": The abstract description based on the above information, formatted as (cid:44) (cid:44) "operation_category" + "target_object" + "application" + "additional_info"(if needed), you should make it more fluent and readable. # Output format of <Operation Sequence>: - The output should be in JSON format. - The JSON object should contain an array of user operations, each represented as JSON object containing the extracted information for that operation. (cid:44) - You should avoid redundancy and repetition in the response. - Example output format: { "user_operations": [ { "timestamp": "[1, 3]", "mouse_position": "near the text 'Hello World'", "element_state_pre_interaction": "Application window with text 'Hello World'", "element_state_after_interaction": "Application window with the text 'Hello World' selected", "thoughts": "The mouse is near the 'hello world' and the background the text changed", "operation_category": "select", "target_object": { "category": "text field", "identifier": "Main Text Area" }, "application": { "category": "Microsoft Word", "identifier": "" }, "additional_info": "Hello World", "abstract": "User selected 'Hello World' in the Main Text Area in Microsoft Word" } ] } CONTINUE ON THE NEXT PAGE TABLE X: Prompt of action proposer of DF method: 3/3 - You are post-processing agent. - Your input is sequence of user operations extracted from the interface images, which may contain errors or redundancies. (cid:44) - Your task is to post-process the operations according to the <GUIDELINE> of the post-processing. - You should output the chain of thoughts according to the <GUIDELINE> of the chain of thoughts before the final result. (cid:44) - Output the final result in structured format according to the <FORMAT> of the output. # <GUIDELINE> of the post-processing ## Check the redundant operations - First, you should check the adjacent operations, and if the adjacent operations are the same (cid:44) (cid:44) operation, and their target_object is the same, you should keep the first operation and remove the redundant operation. E.g., if there are two "click" operations on the same button, you should keep the first "click" operation and remove the second "click" operation. (cid:44) - Second, you should check the adjacent operations, and if the one operation is the sub-operation of (cid:44) (cid:44) (cid:44) the other operation, you should remove the sub-operation. E.g., if there is "click" operation followed by "drag" operation, you should remove the "click" operation and keep the "drag" operation. ## Check the reasonableness of the operations - First, you should check the operation category according to Your thoughts. If the operation category is not reasonable, you should correct it. (cid:44) - Second, you should check the target_object according to the thoughts and abstract. If the target_object is not reasonable, you should correct it. (cid:44) - Third, you should check the application according to the thoughts and abstract. If the application (cid:44) is not reasonable, you should correct it. ## Check the completeness of the operations - First, you should check the additional_info according to the operation category. If the additional_info is missing, you should complete it. (cid:44) - Second, you should check the abstract according to the operation category, target_object, (cid:44) (cid:44) application, and additional_info. If the abstract is missing or not fluent, you should complete or correct it. # <GUIDELINE> of the chain of thoughts - First, check the redundant operations according to the <GUIDELINE> of the post-processing. And give the reason why you think the operation is redundant. (cid:44) - Second, check the reasonableness of the operations according to the <GUIDELINE> of the post-processing. And give the reason why you think the operation is not reasonable. (cid:44) - Last, check the completeness of the operations according to the <GUIDELINE> of the post-processing. (cid:44) And give the reason why you think the operation is not complete. # <GUIDELINE> of the output - The output should be in strictly valid JSON format, with no extra text or characters before or after the JSON. (cid:44) - If there are no user operations, you must return the 'user_operations' key with an empty list as its (cid:44) value. Example output format: { "user_operations": [ { "thoughts": "The mouse is near the 'hello world' and the background of the text changed", "operation_category": "select", "target_object": { "category": "text field", "identifier": "Main Text Area" }, "application": { "category": "Microsoft Word", "identifier": "" }, "additional_info": "Hello World", "abstract": "User selected 'Hello World' in the Main Text Area in Microsoft Word" } ] } TABLE XI: Prompt of corrector of DF method - You are an operation merge agent. - You will receive list of user operations that are extracted from video frames using the sliding window method; the definition of the input is in <INPUT DETAIL>. (cid:44) - Your task is to delete the repeated operations caused by the overlapping of the adjacent windows and (cid:44) merge the entire operation sequence, you can refer to the <GUIDELINE FOR MERGING> for the merging rules. (cid:44) - You should output the merged operation sequence in the same format of <OUTPUT FORMAT> # <INPUT DETAIL>: - The input is list of user operations extracted from the video frames. - Each item in the list is JSON object containing the start and end frame of the sliding window,with (cid:44) list of user operations extracted from the window. # <GUIDELINE FOR MERGING>: - For each pair of neighboring sliding windows, pay attention to actions at the end of the previous window and those at the beginning of the next window, check if certain actions match one of the following merging criteria: (cid:44) (cid:44) - Are there "drag" actions on the same element (or different element description referring to the same (cid:44) element) in the same app? - Write these actions down, which should be merged into one "drag" later. - Are there scroll actions on the same element (or different element descriptions referring to the same (cid:44) element) in the same app? - Write these actions down, which should be merged into one "scroll" later. - Are there "type" actions in the same app? (cid:44) - For such "type" actions, is earlier action "additional_info" (the typed text) prefix of later action "additional_info"? If so, they actually belong to the same sequence of character typing and should be merged into one "type" action. (cid:44) - Write the actions satisfying the conditions, which should be merged to one "type" later. - Are there "select" actions in the same app? - For such "select" actions, is earlier action "additional_info" (selected items) subset/superset (cid:44) of later action "additional_info"? If so, they actually belong to the same sequence of selecting and should be merged into one "select" action. (cid:44) - Write the actions satisfying the conditions, which should be merged to one "select" later. - Is there "select" (or "click") action followed by "drag" action on the same element (or different (cid:44) element description referring to the same element) in the same app? - Confirm if "select" is followed by "drag". - Write the actions satisfying the conditions, which should be merged into one "drag" later. - Is there "select" (or "click") action followed by "drag" action on same element (or different (cid:44) element description referring to the same element) in the same app? - Confirm if "select" is followed by "drag"". - Write the actions satisfying the conditions, which should be merged into one "drag" later. # <OUTPUT FORMAT>: - The output should be in JSON format. - The JSON object should contain an array of user operations, each represented as JSON object (cid:44) containing the extracted information for that operation. Example output format: { "user_operations": [ { "thoughts": "The mouse is near the 'hello world' and the background of the text changed", "operation_category": "select", "target_object": { "category": "text field", "identifier": "Main Text Area" }, "application": { "category": "Microsoft Word", "identifier": "" }, "additional_info": "Hello World", "abstract": "User selected 'Hello World' in the Main Text Area in Microsoft Word" } ] } TABLE XII: Prompt of merger of DF method <<<CONTEXT>>> user is operating with mouse and/or keyboard in Windows, which is recorded and requires analysis. <<<INPUT>>> The image input contains 2 images: - one image is full screenshot of the current screen, with yellow box drawn around the region where UI changes might have happened. (cid:44) - the other image represents the side-by-side comparison of the region bounded by yellow box before and (cid:44) after the likely UI change. The image is pair of sub-images separated by vertical black line, where left is the old looking of UI (1 second ago) and right is the new looking (now). (cid:44) Note that the image may or may not contain UI changes. Red boxes are drawn around changed pixels for you to better identify them. The text input includes 2 fields for you to have rough idea of the screen location of the region, and what the region would likely contain at this location: (cid:44) - "screen_resolution": the screen resolution of the complete screenshot, in the format of [height, width]. - "bbox": the location and size of the yellow box in the screenshot provided to you, in the format of (cid:44) [minr, minc, maxr, maxc]. <<<TASK>>> Your goal is to describe the differences between the old and new looking of UI in the comparison image (cid:44) (second one). You should focus on the second image while the first image is for you to understand the context. (cid:44) You need to output JSON object with the following fields: - "global_description" (string): brief overall description of what is contained in the full screenshot (first image). (cid:44) (cid:44) - "description" (string): brief description of what is contained in visible region (second image), and what UI changes have happened if any. If you recognize well-known icons or logos, you should also mention them here. (cid:44) - "changed" (bool): indicating whether the UI has changed in the image. - "old_cursor_shape" (string): The shape of the cursor in the old looking, in common terms like "normal", "crosshair", "I-beam" etc. Leave it null if cursor is not present in old image. (cid:44) - "new_cursor_shape" (string): The shape of the cursor in the new looking, in common terms like "normal", "crosshair", "I-beam" etc. Leave it null if cursor is not present in new image. (cid:44) - "changes" (list): list of JSON objects, each describing UI change. Leave it empty if "changed" is (cid:44) false. Each object should have the following fields: - "subject" (string): The subject of the UI change. - "type" (string): One of "appear", "disappear", "move", "rotate", "text_content_change", "style_change". (cid:44) - "old" (string): Description of the old looking of the subject in the image. - "new" (string): Description of the new looking of the subject in the image. - "message" (string): Explanation of what the UI change indicates. Do not suggest UI actions, but just (cid:44) (cid:44) what information the changes are intended to convey. Each change may have more than one explanations, use your critical thinking to list as many sound explanations as possible. Examples are: "the UI element is focused", "the UI element is defocused", "the UI element is (cid:44) selected", "The app has notifications", "a new window is open". Note that there may be more than one UI change in the image. You should describe all the changes you can (cid:44) observe in the above fields. Following are example outputs. <<<EXAMPLE OUTPUT 1>>> { "global_description": "The whole screenshot mainly contains an open Excel window, with worksheet displayed.", (cid:44) "description": "The region contains an open Excel window, with worksheet displayed.", "changed": true, "old_cursor_shape": null, "new_cursor_shape": null, "changes": [ { } "subject": "window", "type": "appear", "old": "the region is part of the desktop background", "new": "the region contains an open Excel window", "message": "The Excel window has been opened or moved to the region, or changed from minimized (cid:44) to opened state" ] } CONTINUE ON THE NEXT PAGE TABLE XIII: Prompt of Frame Difference Descriptor : 1/2 <<<EXAMPLE OUTPUT 2>>> { "global_description": "The whole screenshot mainly contains Word document, with paragraph of text displayed.", (cid:44) "description": "The region contains part of the blank area of the document.", "changed": true, "old_cursor_shape": null, "new_cursor_shape": "normal", "changes": [ { } "subject": "cursor", "type": "move", "old": "the cursor is at the left of the region", "new": "the cursor is at the right of the region", "message": "The cursor has been moved from left to right" ] } <<<EXAMPLE OUTPUT 3>>> { "global_description": "The whole screenshot mainly contains desktop with few icons displayed.", "description": "The region contains part of the desktop background without any icons.", "changed": true, "old_cursor_shape": "I-beam", "new_cursor_shape": null, "changes": [ { } "subject": "cursor", "type": "disappear", "old": "the cursor is present in the region", "new": "the cursor is absent in the region", "message": "The cursor has been hidden or moved out of the region" ] } <<<EXAMPLE OUTPUT 4>>> { "global_description": "The whole screenshot mainly contains browser window, with search bar displayed.", (cid:44) "description": "The region contains search bar with the text 'hello world' displayed.", "changed": true, "old_cursor_shape": "I-beam", "new_cursor_shape": null, "changes": [ { } "subject": "text", "type": "text_content_change", "old": "the text in the visible area is 'hello'", "new": "the text in the visible area is 'hello world'", "message": "More text has been added to the input field" ] } <<<EXAMPLE OUTPUT 5>>> { "global_description": "The whole screenshot mainly contains text editor window, with some text displayed.", (cid:44) "description": "The region contains line of text in yellow.", "changed": true, "old_cursor_shape": null, "new_cursor_shape": null, "changes": [ { } "subject": "text", "type": "style_change", "old": "the text in the region is black", "new": "the text in the region is yellow", "message": "the text color has changed" ] } <<<EXAMPLE OUTPUT 6>>> { "changed": false, "changes: [] } TABLE XIV: Prompt of Frame Difference Descriptor : 2/2 (continued) <<<CONTEXT>>> user is operating with mouse and/or keyboard in Windows, which is recorded and requires analysis. <<<TASK>>> Note that due to random processing errors, some of the input events may be inaccurate or even contradictory, hence it may be impossible to generate list of actions coherent to all input events. (cid:44) Your goal, in this case, is to aggregate the input events into list of coherent and most likely actions. Specifically, you should prefer short list of actions, each of which explains many of events, rather than long list of (cid:44) (cid:44) actions, each of which explains only 1 or 2 events. <<<INPUT>>> The user input is list of JSON objects, each representing UI change in region of the screen. <<<OUTPUT>>> Your output should be list of JSON objects, each representing coherent and most likely action. Each object should have the following fields: (cid:44) - "app" (string): The name of the application where the actions are performed. - "element" (string): The UI element where the actions are performed. - "action" (string): The action performed on the UI element. One of "click", "drag", "scroll", "select", "type". (cid:44) - "region" (string): The id of the region in which the UI change happened. For mouse actions, it should be the region where the cursor locates; for keyboard actions, it should be where the text changes happen. The id is in the format "<frame>_<index>" (cid:44) - "evidences" (list): The list of (id string, reason string) paris of the input events that support this (cid:44) (cid:44) (cid:44) action. Each id is in the format "<frame>_<index>". The "region" id should be included in this list for consistency. Each evidence can only be used at most once in the output. # <Operation> description and identification methods: ## click Description: - The user clicks on an interface element(e.g., button, link, or icon), activating the (cid:44) element(e.g., button press effect) and triggering various events(e.g., opening new window, expanding menu, changing the state of an element). (cid:44) - If you think user's operation is "click", you need to keep observing several frames to see if the (cid:44) operation is "drag" or "select", which contains the "click" operation. Identification: - By the change of mouse: - shape change: The mouse may change shape (e.g., pointing hand when clicking link). - By the change of interface element: - press effective: Buttons or other clickable elements display press effect (e.g., changing (cid:44) color, showing shadow, slightly changing shape, checkbox gets checked). - By the change of display: - feedback message: The interface displays feedback messages or changes(e.g., new window opens, (cid:44) ## select menu expands). Description: - Text selection: The user selects text in document, highlighting the selected text with different background color. For example, select two sentences in Microsoft Word. (cid:44) - Icon selection: The user selects icons on desktop or in an application, the selected icons should be enclosed within blue rectangular box. For example, select two icons on the desktop. (cid:44) - Cell selection: The user selects cells in spreadsheet or table, highlighting the selected cells (cid:44) with different background color. For example, select three cells in Excel. Identification: - By the change of mouse: - Text selection: - mouse position: The position of the mouse indicates the start and end of the selection range. (cid:44) (cid:44) You should observe the mouse movement over the text to identify the selection.(e.g., from the beginning of the first sentence to the end of the second sentence) - Icon selection: - mouse position: mouse move over the icons, and the selected icons should be enclosed within blue rectangular box. You should observe the blue rectangular box region to identify the number of selected icons. (cid:44) (cid:44) - By the change of interface element: - color change: The background color of the selected text or selected icons changes to indicate (cid:44) the selection.(e.g., from white to blue) CONTINUE ON THE NEXT PAGE TABLE XV: Prompt of Action Proposer : 1/3 ## type Description: - Text input: The user types text into text field or document, entering characters, words, or sentences. For example, typing in search bar, filling out form, or writing an email. (cid:44) - Command input: The user types commands or inputs specific text strings to perform actions or (cid:44) trigger events. For example, typing commands in terminal. Identification: - By the change of interface element: - text change: The content of the text field changes as the user types, updating the displayed (cid:44) text. ## scroll Description: - Vertical scroll: The user scrolls vertically through document or interface, moving the content up or down to view more information. (cid:44) - Horizontal scroll: The user scrolls horizontally through document or interface, moving the (cid:44) content left or right to view more information. Identification: - By the change of mouse: - mouse position: The mouse may move to the scroll bar or scroll area, indicating the intention to (cid:44) scroll. - By the change of interface element: - scroll bar movement: Observe the movement of the scroll bar. The vertical scroll bar is usually located on the right side of the interface, and the horizontal scroll bar is usually located at the bottom of the interface. (cid:44) (cid:44) - By the change of display: - content movement: The content of the document or interface moves up or down (vertical scroll) or (cid:44) ## drag left or right (horizontal scroll) as the user scrolls. Description: - Icon drag: The user drags an icon, moving it from one location to another within the interface. For example, dragging file to folder, moving an icon on the desktop. (cid:44) - Text drag: The user drags selected text, moving it to different location within document or (cid:44) interface. For example, rearranging paragraphs in document. Identification: - By the change of position of the icon or text: - object position change: The object or text moves with the mouse, indicating the drag operation. - By the change of mouse or mouse: - mouse position: The mouse moves with the dragged object or text. - By the change of interface element: - drag indicator: The object or text being dragged shows visual indicator of movement, such as (cid:44) shadow, outline, or placeholder. # The <Difference> between operations: - Drag vs. click: Drag operation include click operation, but drag operation involves moving an object from one location to another, while click operation involves triggering actions or changing the state of elements. (cid:44) (cid:44) - Drag vs. select: - Drag operation involves moving an object from one location to another, while select operation involves highlighting text or selecting items, don't involve moving objects. (cid:44) - Icon selection is type of select operation, not drag operation. - Select vs. click: Select operation include click operation, but select operation involves (cid:44) highlighting text or selecting items, while click operation involves triggering actions or changing the state of elements. (cid:44) - Click vs. scroll: Click operation involves triggering actions or changing the state of elements, (cid:44) while scroll operation involves moving the content up or down to view more information. In following section is an example output. Note that these events are unlikely to happen in the real world, (cid:44) but they are as such merely to illustrate the format of the output. Remember: - all actions are one of the 5 types: click, drag, scroll, select, type. For 'select' action, make sure to name all the UI items selected. (cid:44) - The output should be in JSON format. Output one single JSON object instead of multiple ones. - The JSON object should contain an array of user operations, each represented as JSON object (cid:44) containing the extracted information for that operation. (cid:44) - There may be multiple click actions in the recording. In this case, you should output multiple click actions, each with its own evidences. Keep in mind that the UI changes associated with click happen in general within few frames after the click event itself, so your evidences should not span over too many different frames. (cid:44) - If you find "click" action, prefer "smaller" UI elements for the "element" field like "button", (cid:44) (cid:44) "cell", "option", avoid too vague descriptions, e.g. say "'refresh' option in context menu" instead of "context menu". (cid:44) - for "select", "click", "drag: actions, cursor must be in the new looking in the region where it (cid:44) happens. CONTINUE ON THE NEXT PAGE TABLE XVI: Prompt of Action Proposer : 2/3 (continued) (cid:44) (cid:44) - "drag" and "scroll" actions must happen and be supported by regions where "move" events happen ("move" of anything but the cursor). To distinguish between "drag" and "scroll": if cursor is present, then cursor moving with ui element in motion indicates drag, and still cursor indicates scroll; If no cursor, then it's scroll (cid:44) - For "drag" to happen, cursor must be in the new looking in the region where it happens. - "type" action can happen only if there is at least one "text_change" in the region where it happens. (cid:44) On the other hand, text change can also happen without "type" action, e.g. caused indirectly by click/select action else where. (cid:44) - If scrollbar appears, it does not necessarily mean that scroll action has happened. It may be result of click or drag action. An area of moving ui elements is necessary condition of scroll action. (cid:44) (cid:44) <<<EXAMPLE OUTPUT>>> [ { "app": "Microsoft Excel", "element": "cell A1", "action": "click", "region": "1_0", "evidences": [ ["1_0", "cursor is in this region, and bounding box appears around cell A1"], ["1_1", "The row is highlighted"] ["1_2", "The column 1 is highlighted"] ["1_3", "The cell reference changed to A1"] ] }, { "app": "Microsoft Excel", "element": "cell A2", "action": "type", "region": "3_1", "evidences": [ ["3_1", "cursor is in this region, and text in it changed"], ["3_2", "text in this region changed"] ] "app": "Microsoft Word", "element": "main text area", "action": "type", "region": "5_1", "evidences": [ ["5_1", "text changed from 'Hello' to 'Hello, World!'"] ] } { } ] TABLE XVII: Prompt of Action Proposer : 3/3 (continued) Now you have to perform sequence of tasks one by one, to improve your prediction. The input of <TASK 1> is the JSON object containing the action sequence you just wrote. Each of the successive tasks takes the output of the previous task as input. (cid:44) Make sure to follow the instructions of each task, by writing down your thoughts, writing the output, (cid:44) before going to the next task. Do not skip the thinking process or writing output of any of the tasks. The tasks are to be performed in the following order: <TASK 1>: correct "action" field <TASK 2>: correct "app" field <TASK 3>: correct "element" field for "select" actions <TASK 4>: correct "element" field for "scroll" actions <TASK 5>: correct "element" field for "type" actions <TASK 6>: correct "element" field for "drag" actions <TASK 7>: correct the "click" actions by analyzing their "evidences" <TASK 8>: merge actions Instructions of the tasks: <TASK 1>: correct "action" field Now you have to revise the "action" field of the action triples you just wrote, and correct all the actions that are not one of the five types ['click', 'type', 'scroll', 'select', 'drag']. This is usually because you used mistakenly one term from the events in your initial input. In this case, you should pick one of the above 5 verbs that best describes this action. For example: - If your wrong action is 'move', think twice: what UI elements were moved? Is it caused by 'drag' or (cid:44) (cid:44) (cid:44) 'scroll'? The other 3 actions would unlikely to cause move in common UI interactions. (cid:44) - If your wrong action is 'hover': recall that while hover may indeed have happened, but you should not output it as an action. Since valid cursor actions ('click', 'select', 'drag') are always followed by hover, think step by step which of the 3 actions actually happened, based on the current region and evideces you are trying to explain in this action. (cid:44) (cid:44) (cid:44) Also don't duplicate actions, that is, output the same action on same region twice. If so, you should remove the duplicates. (cid:44) Remember to keep the actions without error intact. Fix the errors in the actions. To complete your task, first write down your thoughts on how to correct these errors. Then, generate valid and complete JSON object that contains the actions with "action" field corrected, and all correct actions intact. (cid:44) (cid:44) <TASK 2>: correct "app" field Now you have to revise the "app" field of the action triples you just wrote, and - Avoid writing the "app" field too vaguely like "Windows". Instead, mention it in format of (cid:44) "<componenet> of Windows" for clarity. Examples are: - If the action happens on the desktop, output the "app" field as "Desktop of Windows". - If the action happens on the taskbar, output the "app" field as "Taskbar of Windows". - If the action happens on the start menu, output the "app" field as "Menu of Windows". - If the action happens on web browser, output the "app" field as "<visible tab title> of Web Browser". For example, "Bing of Web Browser" or "The New York Times of Web Browser". One special case, is when the tab is google search result, you should output the "app" field as "Google in Web Browser". (cid:44) (cid:44) Rememeber to leave the correct "app" values and all other fields intact in your output JSON object. If no correction is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. To complete your task, first write down your step-by-step thoughts on how to correct the "app" fields if they have these issues. (cid:44) (cid:44) If one "app" field needs correction, but the current action item does not include enough information to correct it, make sure to go back to its supporting UI change events (find them by ids indicated in "evidences" and "region" fields) to review the original UI change events and correct the "app" field accordingly. (cid:44) After your thinking, output the complete and valid JSON object of actions with each of the "app" fields (cid:44) (cid:44) corrected. <TASK 3>: correct "element" field for "select" actions Now you have to revise the "element" field of the action triples you just wrote. Consider the "select" action triples only, leaving all other actions intact. (cid:44) - If some text is selected, the "element" field should contain the selected text in single quotes (cid:44) (cid:44) (WITHOUT extra explanatory words like "text"), instead of the UI element including the text like "textbox". Rememeber to leave the correct "element" values and all other fields intact in your output JSON object. (cid:44) If no correction is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. CONTINUE ON THE NEXT PAGE TABLE XVIII: Prompt of Action Corrector : 1/4 To complete your task, first write down your step-by-step thoughts on how to correct/remove the "element" fields of the "select" actions if they have these issues. (cid:44) If one "element" field needs correction, but the current action item does not include enough (cid:44) (cid:44) information to correct it, make sure to go back to its supporting UI change events (find them by ids indicated in "evidences" and "region" fields) to review the original UI change events and correct the "element" field accordingly. (cid:44) After your thinking, output the complete and valid JSON object of actions with each of the "element" (cid:44) fields corrected. <TASK 4>: correct "element" field for "scroll" actions Now you have to revise the "element" field of the action triples you just wrote. Consider the "scroll" (cid:44) action triples only, leaving all other actions intact. (cid:44) - For each scroll action in web browser tab, output "element" as "<vertical/horizontal> scroll bar of <visible tab title>". Confirm whether the scroll bar is vertical or horizontal by looking at the UI change events that support the scroll action. (cid:44) - For every other scroll action, output "element" as "<vertical/horizontal> scroll bar of <UI element being scrolled>". Confirm whether the scroll bar is vertical or horizontal by looking at the UI change events that support the scroll action. (cid:44) (cid:44) Rememeber to leave the correct "element" values and all other fields intact in your output JSON object. (cid:44) If no correction is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. To complete your task, first write down your step-by-step thoughts on how to correct/remove the "element" fields of the "scroll" actions if they have these issues. (cid:44) If one "element" field needs correction, but the current action item does not include enough (cid:44) (cid:44) information to correct it, make sure to go back to its supporting UI change events (find them by ids indicated in "evidences" and "region" fields) to review the original UI change events and correct the "element" field accordingly. (cid:44) After your thinking, output the complete and valid JSON object of actions with each of the "element" (cid:44) fields corrected. <TASK 5>: correct "element" field for "type" actions (cid:44) Now you have to revise the "element" field of the action triples you just wrote. Consider the "type" action triples only, leaving all other actions intact. The typed content might be either text or other content: (cid:44) - If text was typed: - the "element" field should contain the typed text in single quotes (WITHOUT extra explanatory words like "text"), instead of the UI element like "textbox"; (cid:44) - Make sure to summarize the complete typed text, by reviewing the "region" and "evidences" events that support the "type" action; (cid:44) (cid:44) - It may happen that the text you found from the UI events are still incomplete due to missing keyframes, so you should also take look of the regions AFTER the "region" and "evidences" events (region ids are in form of "<frame>_<index>", so you should look at regions of greater frame numbers, not smaller) to infer the complete typed text. If such later regions exist, add the region ids to the "evidences" field of the current action. (cid:44) (cid:44) (cid:44) - If Something else is typed, observe among the the events: what content has appeared before I-beam (cid:44) (cid:44) (cid:44) (cid:44) cursor locations in the changed regions close-in-time to the "region" and "evidences" regions (that is, frame id should not differ with more than 2). Describe the new content briefly in the "element" field (without quotes). If new content has appeared in UI event region, add its id to the "evidences" field of the current action. Rememeber to leave the correct "element" values and all other fields (except when you need to add more region ids to "evidences" of "type" action) intact in your output JSON object. (cid:44) If no correction is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. To complete your task, first write down your step-by-step thoughts on how to correct/remove the "element" fields of the "type" actions if they have these issues. (cid:44) If one "element" field needs correction, but the current action item does not include enough (cid:44) (cid:44) information to correct it, make sure to go back to its supporting UI change events (find them by ids indicated in "evidences" and "region" fields) to review the original UI change events and correct the "element" field accordingly. (cid:44) After your thinking, output the complete and valid JSON object of actions with each of the "element" (cid:44) fields corrected. <TASK 6>: correct "element" field for "drag" actions (cid:44) Now you have to revise the "element" field of the action triples you just wrote. Consider the "drag" action triples only, leaving all other actions intact. The typed content might be either text or other content: CONTINUE ON THE NEXT PAGE (cid:44) TABLE XIX: Prompt of Action Corrector : 2/4 (continued) - If items were dragged: - Does the "app" in which the drag happend allow dragging multiple items? If so, one or more items (cid:44) may be dragged; otherwise, only one item may be dragged. Some common apps: - Desktop: allow dragging multiple selected icons. - Taskbar: does not allow dragging multiple icons. - File Explorer: allow dragging multiple selected files. - Microsoft Word: allow dragging multiple selected lines, which must be contiguous. - List (in general): if items are selected, they can be dragged at the same time. - Name the items(s) being dragged specifically in "element", avoiding vague descriptions like "icons" or "items". (cid:44) - To identify the items being dragged, revisit the "region" and "evidences" events that support the (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) "drag" action. In particular, what items kept moving in every supporting "region" and "evidences"? Were they moving with the cursor? During drag, some elements may move when their place was occupied by the dragged item. These items did not keep in all events following the cursor, and their movement was not caused directly by the drag, therefore they should not be included as dragged items. - If text was dragged: - the "element" field should contain the dragged text in single quotes (WITHOUT extra explanatory words like "text"), instead of the UI element like "textbox"; (cid:44) - Observe selected text during the drag action, as text must be in selected state to be dragged; - Revisit "text_content_change" events that happened between start and end of drag actions. Does the selected text appear or disappear in these events? If so, it is likely that the text was dragged and moved, instead of being changed. The reason is that due to low frame sampling rate (1FPS), dragged/moved text may look like changed, while the intermediate dragging animation may be missing in low FPS keyframes. (cid:44) (cid:44) (cid:44) (cid:44) Rememeber to leave the correct "element" values and all other fields (except when you need to add more region ids to "evidences" of "drag" action) intact in your output JSON object. (cid:44) If no correction is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. To complete your task, first write down your step-by-step thoughts on how to correct/remove the "element" fields of the "drag" actions if they have these issues. (cid:44) Your thoughts must include the following steps: - Find out the "app" in which the drag happend. Does the app allow dragging multiple items? If so, one or more items may be dragged; otherwise, only one item may be dragged. (cid:44) - In "region" and "evidences" events that support the "drag" action, which items have moved? List them. (cid:44) (cid:44) - Among these items, which items kept moving in every supporting "region" and "evidences"? Were they moving with the cursor? During drag, some elements may move when their place was occupied by the dragged item. These items did not keep in all events following the cursor, and their movement was not caused directly by the drag, therefore they should not be included as dragged items. (cid:44) - List the items that kept moving in all supporting events with the cursor. - Review all UI events: in which event has one of the above specifaclly been selected? List them (cid:44) here for latet steps (cid:44) (cid:44) - If you found multiple items in the previous step, review the supporting events in "region" and "evidences", to see if they were moving in translation manner, that is, in parallel. Items that have exchanged location or reordered cannot possibly have been dragged at the same time (since it is NOT translation motion), and hence should not be included in the "element" field. List the items that were BOTH selected and moving in translation manner. (cid:44) - If you found multiple items in the previous step, list all of them here for later output of (cid:44) (cid:44) (cid:44) (cid:44) "element" IF the app allows dragging multiple items; otherwise, list only one item here, which seems to be the most likely dragged item. If one "element" field needs correction, but the current action item does not include enough (cid:44) (cid:44) information to correct it, make sure to go back to its supporting UI change events (find them by ids indicated in "evidences" and "region" fields) to review the original UI change events and correct the "element" field accordingly. (cid:44) After your thinking, output the complete and valid JSON object of actions with each of the "element" (cid:44) fields corrected. <TASK 7>: correct the "click" actions by analyzing their "evidences" Now you have to revise the "evidences" field of the "click" action triples you just wrote. Consider the (cid:44) "click" action triples only, leaving all other actions intact. Rememeber to leave the correct "evidences" values and all other fields intact in your output JSON object. If no correction is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. To complete your task, first write down your step-by-step thoughts for each "click" action: - Review all the supporting UI events in the "evidences". If all evidences indicate that the curosor just (cid:44) happened to move over/beside the element, and no style change of the action elementhappend at all CONTINUE ON THE NEXT PAGE TABLE XX: Prompt of Action Corrector : 3/4 (continued) - Write down what is expected to happen if this click action was actually performed and Walk through (cid:44) (cid:44) ALL UI events after the frame where the click supposedly happend (not only the evidences of the action) to check if the expected UI change actually happened. For example: - Clicking on taskbar icon should open the corresponding app window, or display it if already open. - Clicking on dropdown menu should open the dropdown menu, or close it if it was already open. - Distinguish between "click" and "hover": Some style changes happen on "hover", and UI events woul (cid:44) have been different than if it was "click". For example, most slight text/background color change would likely by caused (cid:44) - Indicator of "hover": the cursor moved over the element and then quickly moved out. After moving out, (cid:44) (cid:44) the element's looking changes back to the original state (the looking before cursor coming acrossing the element). Make sure to check few frames before and after the evidence of this action to see if it's the case (cid:44) - After the above steps, write down the updated list of evidences for this action, taking your above (cid:44) (cid:44) thoughts into account. Keep the evidences that support the "click" action, and remove the evidences that indeicate "hover" action or another action type or no action at all. (cid:44) After your thinking, output the complete and valid JSON object of actions with each of "click" action corrected and all other actions intact. For each "click" action, if the new "evidences" list is non-empty, then keep the action item with updated "evidences" and all other fileds unchanged. If the new "evidences" list is empty, then remove the action item from the list of actions. (cid:44) (cid:44) <TASK 8>: merge actions Now you have to revise the actions of type "type", "select", "drag", "click" you just wrote to see any of the same type actions are mergeable, leaving all the other actions intact. (cid:44) Indenfity all groups of actions that should have been merged, output one single merged action where: - "action" is the common action type of the actions being merged - "element" is summary of "element" fields of all actions being merged, which contains all (cid:44) information of each individual "element". If "element" is in form of set (e.g. set of icons, text as set of characters), use the "element" of the latest action (latest means greatest frame id) (cid:44) - "app" is the common app shared by all actions being merged - "evidences" is the union of "evidences" fields of all actions being merged - "region" is the region of the latest action (latest means greatest frame id) - all other fields are the same as the latest action (latest means greatest frame id) Indicators of actions of same type: - The actions happen in same app (necessary but not sufficient condition) - The actions have overlapping evidences or evidences that interleave or are close in time (in terms of id; evidences close in time must differ at most by 2 in frame id) (cid:44) - For "type" actions, ealier action "element" (in terms of frame id) is prefix of later action "element", since text is typed in sequence. (cid:44) - For "select" actions, earlier action "element" (in terms of frame id) is subset OR superset of (cid:44) later action "element", since more or less text/items are selected during these frames Rememeber to leave the correct actions intact in your output JSON object. If no merging is needed, simply output your prvious JSON object EXACTLY as it is, so that the (cid:44) downstream system can notice that no correction is made. To complete your task, first write down your step-by-step thoughts on how to merge the actions. For each of the groups of actions that should have been merged, based on the above indicators of actions of same type, your thoughts must include the following steps: (cid:44) - Find all groups of actions that should have been merged, based on the above indicators of actions of same type. (cid:44) - make sure their action type is the same and one of "type", "select", "drag", "click". Otherwise they should not be merged. (cid:44) - make sure they are in same app, otherwise they should not be merged. - If the action type to merge is "type": is earlier action "element" (in terms of frame id) prefix of immediate later action "element"? (cid:44) - If the action type to merge is "select": is earlier action "element" (in terms of frame id) subset OR superset of immediate later action "element"? (cid:44) - Does this group of actions needs further correction by removing some actions and/or adding more (cid:44) (cid:44) actions? Make sure to go back to the supporting UI change events (find them by ids indicated in "evidences" and "region" fields) of each action under consideration to see if it's to be added or removed from the group. (cid:44) - Avoid "over-merging": do not merge actions into one while they actually represent more than one (cid:44) action. Check the supporting UI events of each action in the group to see if they are one or more actions in reality. (cid:44) - If the group leads to an "over-merging" the number of true actions is still less than the number of actions in the group, then remember to reason in the same step-by-setp fashion later on each of these subgroups, each corresponding to true action. (cid:44) - write down conclusion: is this group indeed to merge? (cid:44) After your thinking, output the complete and valid JSON object of actions with each of identified group (cid:44) merged, and all other action intact. TABLE XXI: Prompt of Action Corrector : 4/4 (continued)

