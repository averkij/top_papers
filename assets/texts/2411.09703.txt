Title: MagicQuill: An Intelligent Interactive Image Editing System

Authors: Zichen Liu, Yue Yu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Wen Wang, Zhiheng Liu, Qifeng Chen, Yujun Shen


================================================================================

Abstract
========

Image editing involves a variety of complex tasks and requires efficient and precise manipulation techniques. In this paper, we present MagicQuill, an integrated image editing system that enables swift actualization of creative ideas. Our system features a streamlined yet functionally robust interface, allowing for the articulation of editing operations (e.g., inserting elements, erasing objects, altering color) with minimal input. These interactions are monitored by a multimodal large language model (MLLM) to anticipate editing intentions in real time, bypassing the need for explicit prompt entry. Finally, we apply a powerful diffusion prior, enhanced by a carefully learned two-branch plug-in module, to process editing requests with precise control. Experimental results demonstrate the effectiveness of MagicQuill in achieving high-quality image edits. Please visit https://magic-quill.github.io to try out our system.

Start
=====

MagicQuill: An Intelligent Interactive Image Editing System Zichen Liu,1,2 Yue Yu,1,2 Hao Ouyang2 Qiuyu Wang2, Ka Leong Cheng1,2 Wen Wang3,2 Zhiheng Liu4 Qifeng Chen,1 Yujun Shen,2 1HKUST 2Ant Group 3ZJU 4HKU 4 2 0 2 4 1 ] . [ 1 3 0 7 9 0 . 1 1 4 2 : r Figure 1. MagicQuill is an intelligent and interactive image editing system built upon diffusion models. Users seamlessly edit images using three intuitive brushstrokes: add, subtract, and color (A). MLLM dynamically predicts user intentions from their brush strokes and suggests contextual prompts (B1-B4). The examples demonstrate diverse editing operations: to generate jacket from clothing contour (B1), add flower crown from head sketches (B2), remove background (B3), and apply color changes to the hair and flowers(B4).

Abstract
========

1. Introduction As highly practical application, image editing encounters variety of user demands and thus prioritizes excellent In this paper, we unveil MagicQuill, an ease of use. integrated image editing system designed to support users in swiftly actualizing their creativity. Our system starts with streamlined yet functionally robust interface, enabling users to articulate their ideas (e.g., inserting elements, erasing objects, altering color, etc.) with just few strokes. These interactions are then monitored by multimodal large language model (MLLM) to anticipate user intentions in real time, bypassing the need for prompt entry. Finally, we apply the powerful diffusion prior, enhanced by carefully learned two-branch plug-in module, to process the editing request with precise control. Please visit https://magicquill.github.io to try out our system. Equal contribution. Corresponding author. Performing precise and efficient edits on digital photographs remains significant challenge, especially when aiming for nuanced modifications. As shown in Fig. 1, consider the task of editing portrait of lady where specific alterations are desired: converting shirt to custom-designed jacket, adding flower crown at an exact position with well-designed shape, dyeing portions of her hair in particular colors, and removing certain parts of the background to refine her appearance. Despite the rapid advancements in diffusion models [6, 10, 14, 19, 35 38, 47, 62, 68] and recent attempts to enhance control [20, 23, 48, 69], achieving such fine-grained and precise edits continues to pose difficulties, typically due to lack of intuitive interfaces and models for fine-grained control. The challenges highlight the critical need for interactive editing systems that facilitate precise and efficient modifications. An ideal solution would empower users to specify what they want to edit, where to apply the changes, and how the modifications should appear, all within user-friendly interface that streamlines the editing process. We aim to develop the first robust, open-source, interactive precise image editing system to make image editing easy and efficient. Our system seamlessly integrates three core modules: the Editing Processor, the Painting Assistor, and the Idea Collector. The Editing Processor ensures high-quality, controllable generation of edits, accurately reflecting users editing intentions in color and edge adjustments. The Painting Assistor enhances the ability of the system to predict and interpret the users editing intent. The Idea Collector serves as an intuitive interface, allowing users to input their ideas quickly and effortlessly, significantly boosting the editing efficiency.

The Editing Processor
=====================

two kinds of implements brushstroke-based guidance mechanisms: scribble guidance for structural modifications (e.g., adding, detailing, or removing elements) and color guidance for modification of color attributes. Inspired by ControlNet [66] and BrushNet [23], our control architecture ensures precise adherence to user guidance while preserving unmodified regions. Our Painting Assistor reduces the repetitive process of typing text prompts, which disrupts the editing workflow and creates cumbersome transition between prompt input and image manipulation. It employs an MLLM to interpret brushstrokes and automatically predicts prompts based on image context. We call this novel task Draw&Guess. We construct dataset simulating real editing scenarios for fine-tuning to ensure the effectiveness of the MLLM in understanding user intentions. This enables continuous editing workflow, allowing users to iteratively edit images without manual prompt input. The Idea Collector provides an intuitive interface compatible with various platforms including Gradio and ComfyUI, allowing users to draw with different brushes, manipulate strokes, and perform continuous editing with ease. We present comprehensive evaluation of our interactive editing framework. Through qualitative and quantitative analyses, we demonstrate that our system significantly improves both the precision and efficiency of performing detailed image edits compared to existing methods. Our Editing Processor achieves superior edge alignment and color fidelity compared to baselines like SmartEdit [20] and BrushNet [23]. The Painting Assistor exhibits superior user intent interpretation capabilities compared to state-of-theart MLLMs, including LLaVA-1.5 [31], LLaVA-Next [30], and GPT-4o [21]. User studies indicate that the Idea Collector significantly outperforms baseline interfaces in all aspects of system usability. By leveraging advanced generative models and usercentric design, our interactive editing framework significantly reduces the time and expertise required to perform detailed image edits. By addressing the limitations of current image editing tools and providing an innovative solution that enhances both precision and efficiency, our work advances the field of digital image manipulation. Our framework opens possibilities for users to engage creatively with image editing, achieving their goals easily and effectively. 2. Related Works 2.1. Image Editing Image editing involves modifying the visual appearance, structure, or elements of an existing image [19]. Recent breakthroughs in diffusion models [17, 44, 49] have significantly advanced visual generation tasks, outperforming GAN-based models [15] in terms of image editing capabilities. To enable control and guidance in image editing, variety of approaches have emerged, leveraging different modalities such as textual instructions [6, 11, 14, 32, 47, 65], masks [20, 23, 48, 69], layouts [10, 33, 68], segmentation maps [35, 62], and point-dragging interfaces [36 38]. Despite these advances, these methods often fall short when precise modifications at the regional level are required, such as alterations to object shape, color, and other details. Among the various methods, sketch-based editing approaches [22, 25, 34, 42, 59, 61, 64] offer users more intuitive and precise means of interaction. However, the current methods remain limited by the accuracy of the text signals input alongside the sketches, making it challenging to precisely control the information of the editing areas, such as color. To achieve precise control, we introduce two types of local guidance based on brushstrokes: scribble and color, thereby enabling fine-grained control over shape and color at the regional level. 2.2. MLLMs for Image Editing Multi-modal large language models (MLLMs) extend LLMs to process both text and image content [16], enabling text-to-image generation [9, 28, 52, 53, 58], promptrefinement [60, 63], and image quality evaluation [51]. In the area of image editing, MLLMs have demonstrated significant potential. MGIE [13] enhances instructionbased image editing by using MLLMs to generate more expressive, detailed instructions. SmartEdit [20] leverages MLLM for better understanding and reasoning towards complex instruction. FlexEdit [55] integrates MLLM to understand image content, masks, and textual instructions. GenArtist [57] uses an MLLM agent to decompose complex tasks, guide tool selection, and enable systematic image generation, editing, and self-correction with step-by-step verification. Our system extends this line of research by introducing more intuitive approach, utilizing MLLM to simplify the editing process. Specifically, it directly integrates the image context with user-input strokes to 2 Figure 2. System framework consisting of three integrated components: an Editing Processor with dual-branch architecture for controllable image inpainting, Painting Assistor for real-time intent prediction, and an Idea Collector offering versatile brush tools. This design enables intuitive and precise image editing through brushstroke-based interactions. infer and translate the editing intentions, thereby automatically generating the necessary prompts without requiring repeated user input. This innovative task, which we term Draw&Guess, facilitates continuous editing workflow, enabling users to iteratively refine images with minimal manual intervention. 2.3. Interactive Support for Image Generation Interactive support enhances the performance and usability of generative models through human-in-the-loop collaboration [27]. Recent works have focused on making prompt engineering more user-friendly through techniques like image clustering [4, 12] and attention visualization [56]. Despite advancements in interactive support, key challenge remains in bridging the gap between verbal prompts and visual output. While systems like PromptCharm [56] and DesignPrompt [39] use inpainting for interactive image editing, these tools typically offer only coarse-grained control over element addition and removal, requiring users to brush over areas before generating objects within those regions. Furthermore, users must manually input prompts to specify the objects they wish to generate. Our approach addresses these limitations by introducing fine-grained image editing through the use of brushstrokes. Additionally, we incorporate multimodal large language model (MLLM) that provides on-the-fly assistance by interpreting user intentions and suggesting prompts in real-time, thereby reducing cognitive load and enhancing overall usability. 3. System Design Our system is structured around three key aspects: Editing Processor with strong generative prior, Painting Assistor with instant intent prediction, and Idea Collector with user-friendly interface. An overview of our system design is presented in Fig. 2. Our system introduces brushstroke-based control signals to give intuitive and precise control. These signals allow users to express their editing intentions by simply drawing what they envision. We designed two types of brushes, scribble and color, to accurately manipulate the edited image. The scribble brushes, add brush and subtract brush, aim to provide precise structural control by operating on the edge map of the original image. The color brush works with downsampled color blocks to enable fine-grained color manipulation of specific regions. Fig. 3 illustrates the workflow to convert the user hand-drawn input signal into control condition for faithfully inpainting Inspired by Ju et al. [23], Zhang the target editing area. et al. [66], we employ two additional branches to the latent diffusion framework [44], with the inpainting branch giving content-aware per-pixel guidance for the re-generation of the editing area, and the control branch providing structural guidance. The model architecture is illustrated in Fig. 4. Further details will be discussed in Sec. 3.1. To reduce the cognitive load for users to input appropriate prompts at every stage of editing, our system integrates MLLM [29] as the Painting Assistor. This component analyzes user brushstrokes to deduce the editing intention 3 based on the image context, thereby automatically suggesting contextually relevant prompts for editing. We have named this innovative task Draw&Guess. To effectively prepare the MLLM for Draw&Guess, we designed dataset construction method to simulate user hand-drawn editing scenarios and acquire ground truth for Draw&Guess. We fine-tuned dedicated LLaVA [31] model, achieving instant prompt guessing with satisfactory accuracy. More specifics will be covered in Sec. 3.2. Additionally, to provide users with streamlined, intuitive interface that empowers them to express their ideas for complex image editing tasks with ease, we designed an Idea Collector with user-friendly interface. The key features of the interface will be outlined in Sec. 3.3. 3.1. Editing Processor Control Condition from Brushstroke Signal. Let Madd and Msub denote the binary masks corresponding to add and subtract brush respectively. These masks share the same dimensions as the original image I, where values are set to 1 in regions corresponding to user brush strokes and 0 elsewhere. The subtract brush masks out the edges from the edge map E, which is initially extracted from the original image using pre-trained CNN fCN . Conversely, the add brush introduces new edges by setting designated regions to white in the edge map. The resulting modified edge map Econd serves as the control condition for manipulating geometric structure in the editing processor. This can be formally expressed as = fCN (I), Esub = (1 Msub), Econd = Esub + Madd (1 Esub). (1) For precise region-specific colorization, we represent each color brush stroke as tuple (Mcolor, c, α), where Mcolor denotes binary mask indicating the user-defined stroke region, specifies the stroke color, and α [0, 1] represents the stroke opacity. The colorization operation can be formally expressed as Ic = (1 α Mcolor) + α Mcolor c, (2) where the color with an alpha blending factor α is applied over specific region of the image defined by the binary mask Mcolor. To generate the color condition Ccond, we first downscale the image Ic by factor of 16 using cubic interpolation, followed by upscaling to the original resolution using nearest-neighbor interpolation. This process generated color block preserving the global color structure while simplifying local details. The edge condition Econd and color condition Ccond jointly guide the inpainting process for precise editing control. The editing region, represented by mask M, is Figure 3. Data processing pipeline. The input image undergoes edge extraction via CNN and color simplification through downscaling. Three editing conditions are then generated based on brush signals: editing mask, edge condition, and color condition, which together provide control for image editing. obtained by dilating the union of brush regions by pixels. The masked image Imasked can then be formulated as = Growp(Madd Msub Mcolor), Imasked = (1 M). (3) This expansion accounts for the fact that editing can affect areas surrounding the mask, such as shadows or other adjacent details. By growing the mask, we ensure that these peripheral regions are properly generated, resulting in more seamless and realistic edit. Controllable Image Inpainting. The inpainting branch adopts the UNet [23, 45] architecture, incorporating the masked image feature into the pre-trained diffusion network. This branch inputs the concatenated noisy latent at t-th step zt, masked image latent zmasked extracted using VAE [26] from Imasked, and downsampled mask by cubic interpolation from M. The inpainting branch processes these features, utilizing trainable clone of the diffusion model, stripped of cross-attention layers to focus solely on the image feature. The extracted features carrying pixel-level information are inserted into each layer of the frozen diffusion model through zero-convolution layers [66]. Given text condition τ , let (zt, t, c; Θ)i represents the feature of the i-th layer in the total layers of the diffusion UNet with parameter Θ. Similarly, let ([zt, zmasked, m], t; ΘI )i represents the output of the i-th layer in the inpainting UNet, where [] denotes the concatenation operation. This feature insertion can be represented by timestep t, (zt, τ, t; Θ)i + = wI Z(F ([zt, zmasked, m], t; ΘI )i), (4) where wI is an adjustable hyperparameter that determines the inpainting strength. Equipped with the inpainting branch, the diffusion UNet can fill the masked area in content-aware manner based on the text prompt. The control branch aims to introduce conditional generation ability to the diffusion UNet based on condition 4 bypass the Q&A process, as the results demonstrate that prompt-free generation achieves satisfactory results. For the color brush, the Q&A setup is similar: The user will upload an image containing some contours in red color. To help you locate the contour, ... You need to identify what is inside the contours using single word or phrase., (the repetitive part is omitted). The system extracts contour information from the color brush stroke boundaries. The final predicted prompt is generated by combining the strokes color information with Q&A outputs. To optimize response time, we constrain Q&A responses to concise, single-word or short-phrase formats. For the color brush Q&A task, accurate object recognition within contours is essential. LLaVA [31] inherently excels in object recognition tasks, making it adept at identifying the content within color brush stroke boundaries. However, the interpretation of add brush strokes poses significant challenge due to the inherent abstraction of human hand-drawn strokes or sketches. To address this, we find it necessary to construct specialized dataset to fine-tune LLaVA to better understand and interpret human hand-drawn brush strokes. Dataset Construction. We selected the Densely Captioned Images (DCI) dataset [54] as our primary source. Each image within the DCI dataset has detailed, multi-granular masks, accompanied by open-vocabulary labels and rich descriptions. This rich annotation structure enables the capture of diverse visual features and semantic contexts. Step 1: Answer Generation for Q&A. The initial stage involves generating edge maps using PiDiNet [50] from images in the DCI dataset, as shown in Fig. 5b. We calculate the edge density within the masked regions and select the top 5 masks with the highest edge densities, as illustrated in Fig. 5c. The labels corresponding to these selected masks serve as the ground truths for the Q&A. To ensure the model focuses on guessing user intent rather than parsing irrelevant details, we clean the label to keep only noun components, streamlining to emphasize essential elements. Step 2: Simulating Brushstroke with Edge Overlay. In the second part of the dataset construction, we focus on the five masks identified in the first step. Each mask undergoes random shape expansion to introduce variability. We use the BrushNet [23] model based on the SDXL [41] to perform inpainting on these augmented masks with empty prompt, as shown in Fig. 5d. Subsequently, the edge maps generated earlier are overlaid onto the inpainted areas as in Fig. 5e. These overlay images simulate practical examples of how user hand-drawn strokes might alter an image. MLLM Fine-Tuning. Our dataset construction method effectively prepares the model to understand and predict user edits, which contains total of 24, 315 images, categorized under 4, 412 different labels, ensuring broad spectrum of data for training. To optimize the performance of the MLLM over Draw&Guess, we fine-tuned the LLaVA Figure 4. Overview of our Editing Processor. The proposed architecture extends the latent diffusion UNet with two specialized branches: an inpainting branch for content-aware per-pixel inpainting guidance and control branch for structural guidance, enabling precise brush-based image editing. = {Econd, Ccond}. We adopt ControlNet [66] to insert conditional control into the middle and decoder blocks of the diffusion UNet. Let C(zt, C, t; ΘC)i represent the output of the i-th layer in the ControlNet, the control feature insertion can be formulated as (zt, τ, t; Θ) 2 +i + = wC Z(F (zt, C, t; ΘC )i), (5) where wC is an adjustable hyperparameter that determines the control strength. Both the inpainting and control branches dont alter the weights of the pre-trained diffusion models, enabling it to be plug-and-play component applicable to any community fine-tuned diffusion models. The control branch is trained using the denoising score matching objective, which can be written as = Ezt,t,ϵN (0,I) (cid:104)(cid:13) (cid:13)ϵ ϵc (cid:0)zt, C, t; {Θ, ΘC}(cid:1)(cid:13) (cid:13) 2(cid:105) , (6) where ϵc is the combination of the denoising U-Net and the ControlNet model. 3.2. Painting Assistor Prompt formatting. In our system, we implement two types of question answering (Q&A) [3] tasks to facilitate the Draw&Guess. For the add brush, we utilize prompt structured as follows: This is draw and guess game. will upload an image containing some strokes. To help you locate the strokes, will give you the normalized bounding box coordinates of the stokes where their original coordinates are divided by the padded image width and height. The top-left corner of the bounding box is at (x1, y1), and the bottom-right corner is at (x2, y2). Now tell me in single word phrase, what am trying to draw with these strokes in the image? The Q&A output directly serves as the predicted prompt. For the subtract brush, we 5 (a) Original Image (b) Edge Map (c) Chosen Mask (d) Inpainting Result (e) Edge Overlay Figure 5. Illustration of dataset construction process. (a) Original images from the DCI dataset; (b) Edge maps extracted from original images; (c) Selected masks (highlighted in purple) with highest edge density; (d) Results after BrushNet inpainting on augmented masked regions; (e) Final results with edge map overlay on selected areas. By overlaying edge maps on inpainted results, we simulate scenarios where users edit images with brush strokes, as the edge maps resemble hand-drawn sketches. The bounding box coordinates of the mask and labels are inherited from the DCI dataset. model, leveraging the Low-Rank Adaptation (LoRA) [18] technique, allowing the efficient fine-tuning without extensively large dataset. Consistent with the original LLaVA training objectives, our approach aims to maximize the likelihood of the correct labels given the input corpora u, which is defined as max Θlora (cid:88) i=1 log (cid:16) ui u1, . . . , ui1; {Θpt, Θlora} (cid:17) , (7) where Θpt and Θlora are parameters in the pre-trained MLLM and the LoRA respectively. 3.3. Idea Collector Interface Design. The user interface of MagicQuill is designed for an intuitive and streamlined image editing experience, as depicted in Figure 2. The interface is divided into several interactive sections, emphasizing ease of use while providing flexible control over the editing process. The interface comprises several key areas: Prompt Area (A) displaying MLLM-suggested prompts, Toolbar (B) with essential editing tools, Layer Management (C) for organizing brush strokes, the main Canvas (D) for editing, Generated Images area (E) for previewing results, Execute Button (F), and Parameter Adjustment (G). Cross-Platform Support. We implement the Idea Collector as modular ReactJS component library, designed for cross-platform compatibility with various generative AI frameworks, such as Gradio [1] and ComfyUI [7]. The architecture separates client-side user interactions from server-side model computations through HTTP protocols, enabling platform-independent deployment via standard HTML rendering. 4. Experiment In evaluating our system, we focused on three primary the Editing Processor, the Painting Assistor, modules: and the Idea Collector. First, we assessed the quality of controllable generation provided by the Editing Processor, with particular attention to edge alignment and color fidelity. This evaluation involved analyzing how effectively users could manipulate and achieve desired visual outputs, which ensures the system responds accurately to users control signal, detailed in Sec. 4.1. Second, We evaluated the Painting Assistors semantic prediction accuracy using simulated hand-drawn inputs. This assessment was critical for validating the capability of the MLLM in interpreting user intentions, ensuring contextually appropriate suggestions that align with the image semantics. Additionally, we conducted user studies to gather feedback on the systems efficiency improvements and prediction accuracy in realworld scenario, presented in Sec. 4.2. Third, we assessed the usability of the user interfaces across all modules. We decomposes the assessment into four distinct dimensions spanning from operational efficiency to user satisfaction. This multi-dimensional assessment framework enabled systematic comparison with baseline systems while ensuring thorough evaluation of the interface, as shown in Sec. 4.3. 4.1. Controllable Generation To thoroughly evaluate the controllable generation capabilities of our editing processor, we compared it with four representative baselines from different categories: (1) SmartEdit [20], an instruction-based editing method. We utilize LLaVA-Next [30] to generate the editing instruction; (2) SketchEdit [64], GAN-based sketch-conditioned method; (3) BrushNet [23], the mask and prompt-guided inpainting method; and (4) composite baseline combining 6 Figure 6. Visual result comparison. The first two columns present the edge and color conditions for editing, while the last column shows the ground truth image that the models aim to recreate. SmartEdit [20] utilizes natural language for guidance, but lacks precision in controlling shape and color, often affecting non-target regions. SketchEdit [64], GAN-based approach [15], struggles with open-domain image generation, falling short compared to models with diffusion-based generative priors. Although BrushNet [23] delivers seamless image inpainting, it struggles to align edges and colors simultaneously, even with ControlNet [66] enhancement. In contrast, our Editing Processor strictly adheres to both edge and color conditions, achieving high-fidelity conditional image editing. BrushNet [23] and ControNet [66]. As illustrated in Fig. 6, the instruction-based method, SmartEdit, tends to produce outputs that are too random, lacking the precision required for accurate editing purposes. Similarly, while BrushNet enables region-specific modifications, it struggles with maintaining predictable detail generation even with ControlNet enhancement, making precise manipulation challenging. In contrast, our model achieves more accurate edge alignment and color fidelity, which we attribute to our specialized design of the inpainting and control branch that emphasizes these aspects. Table 1. Quantitative results and input condition comparisons between the baselines and ours. Our Editing Processor performs better than the baselines across all metrics, indicating its superiority in controllable generation over edge and color. Method Input Condition Text Edge Color SmartEdit SketchEdit BrushNet Brush.+Cont. Ours LPIPS[67] PSNR SSIM 0.339 0.138 0.0817 0.0748 0.0667 16.695 0.561 23.288 0.835 25.455 0.893 25.770 0.894 27.282 0.902 We further conducted quantitative analysis of our constructed test dataset in Sec. 3.2, which contains 490 images. Our model outperformed the baselines across all key metrics as in Tab. 1. These results demonstrate significant improvements in controllable generation. 4.2. Prediction Accuracy To evaluate the prediction accuracy of the Painting Assistor, we compared it with three state-of-the-art MLLMs: LLaVA-1.5 [31], LLaVA-Next [30], and GPT-4o [21] on our test dataset of 490 images from Sec. 3.2. Each model was prompted with images containing sketches and bounding box coordinates to generate semantic interpretations. The semantic outputs were assessed using three metrics: BERT [8], CLIP [43], and GPT-4 [2] similarity scores, which measure the closeness of the generated descriptions to the ground truth. For GPT-4 similarity, we ask GPT4 to rate the semantic and visual similarity between the predicted response and the ground truth on 5-point scale, where 1 means completely different, 3 means somewhat related, and 5 means exactly same. The evaluation results are presented in Tab. 2, illustrating that our model achieves the highest prediction accuracy Table 2. Performance comparison between our Painting Assistor and other MLLMs, demonstrating superior visual and semantic consistency in predictions.

Method
======

LLaVA-1.5 LLaVA-Next GPT-4o Ours GPT-4 [2] BERT [8] CLIP [43] Similarity Similarity Similarity 0.721 0.716 0.684 0.749 0.795 0.794 0.790 0.824 1.894 1.941 1.976 2.712 Figure 8. Comparative user ratings between our system and the baseline in four dimensions, with standard deviation shown as error bars. This setup enables the focus on the value provided with our Idea Collector by controlling other variables. Procedure. The study lasted approximately 30 minutes for each participant with two systems (our system and the baseline). Each session began with brief introduction to the system using the case illustrated in Fig. 1. Participants then had 5 minutes to freely explore and edit images. After using both systems, participants completed questionnaire with 22 questions (10 questions per system covering all four dimensions and 2 questions regarding the Painting Assistor detailed in Sec. 4.2). We employed the System Usability Scale (SUS) [5] for scoring, using Likert scale from 1 (strongly disagree) to 5 (strongly agree), to capture global view of subjective usability for each system. As shown in Fig. 8, our system demonstrated significantly higher scores across all dimensions compared to the baseline. Indicating the effectiveness of our Idea Collector. Further details can be found in the supplementary. 5. Conclusion In conclusion, our interactive image editing system MagicQuill effectively addresses the challenges of performing precise and efficient edits by combining the strengths of the Editing Processor, Painting Assistor, and Idea Collector. Our comprehensive evaluations demonstrate significant improvements over existing methods in terms of controllable generation quality, editing intent prediction accuracy, and user interface efficiency. For future work, we aim to expand the capabilities of our system by incorporating additional editing types, such as reference-based editing, which would allow users to guide modifications using external images. We also plan to implement layered image generation to provide better editing flexibility and support for complex compositions. Moreover, enhancing typography support will enable more robust manipulation of textual elements within images. These developments will further enrich our framework, offering users more versatile and powerful tool for creative expression in digital image editing. Figure 7. User ratings for the Painting Assistor, focusing on its prediction accuracy and efficiency enhancement capabilities. among all tested MLLMs. This superior performance indicates that our Painting Assistor more accurately captures and predicts the semantic meanings of user drawings. To further qualitatively evaluate the Painting Assistor, we conducted user study with 30 participants who freely edited images using our system. Participants rated the Painting Assistor on 5-point scale for prediction accuracy (1: very poor, 5: excellent) and efficiency facilitation (1: significantly reduced, 5: significantly enhanced). As shown in Fig. 7, 86.67% of users rated prediction accuracy at least 4, validating the ability of our fine-tuned MLLM to interpret user intentions. Similarly, 90% rated efficiency facilitation 4 or above, confirming that Draw&Guess effectively streamlines the editing process by reducing manual prompt inputs. The average scores for accuracy and efficiency were 4.07 and 4.37. 4.3. Idea Collection Effectiveness and Efficiency Collecting user ideas effectively and efficiently is critical for the usability and adoption of interactive systems, especially in creative applications where user engagement is crucial. To evaluate the Idea Collector, we conducted user study with 30 participants, comparing our system against baseline system on the following dimensions: Complexity and Efficiency measures how streamlined and intuitive the user finds the system for creative editing. Consistency and Integration assesses whether the system maintains cohesive interface and interaction design. Ease of Use captures the learnability of the system, especially for users with varying backgrounds. Overall Satisfaction reflects users general satisfaction with the design, features, and usability of the system. Baseline. The baseline system was implemented as customized ComfyUI workflow, replacing our Idea Collector interface with an open-source canvas, Painter Node [40].

References
==========

[1] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569, 2019. 6 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 7, 8 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 5 [4] Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, and Tovi Grossman. Promptify: Text-to-image generation through interactive prompt exploration with large language models. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 114, 2023. 3 [5] John Brooke et al. Sus-a quick and dirty usability scale. Usability evaluation in industry, 189(194):47, 1996. 8, 3 [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 1, 2 [7] ComfyUI. The most powerful and modular diffusion model gui, api and backend with graph/nodes interface. https: //github.com/comfyanonymous/ComfyUI, 2024. [8] Jacob Devlin. Bert: Pre-training of deep bidirectional arXiv preprint transformers for language understanding. arXiv:1810.04805, 2018. 7, 8 [9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 2 [10] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:1622216239, 2023. 1, 2 [11] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: arXiv preprint Diffusion transformer for image editing. arXiv:2411.03286, 2024. 2 [12] Yingchaojie Feng, Xingbo Wang, Kam Kwai Wong, Sijia Wang, Yuhong Lu, Minfeng Zhu, Baicheng Wang, and Wei Chen. Promptmagician: Interactive prompt engineering for text-to-image creation. IEEE Transactions on Visualization and Computer Graphics, 30(1):295305, 2024. [13] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. 2 [14] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1270912720, 2024. 1, 2 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 2, 7 [16] Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, et al. Llms meet multimodal generation and editing: survey. arXiv preprint arXiv:2405.19334, 2024. 2 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6, 1, [19] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525, 2024. 1, 2 [20] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. 1, 2, 6, 7 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 7 [22] Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing generative adversarial network with users sketch and color. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17451753, 2019. 2 [23] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. arXiv preprint arXiv:2403.06976, 2024. 1, 2, 3, 4, 5, 6, 7 [24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 1 [25] Kangyeol Kim, Sunghyun Park, Junsoo Lee, and Jaegul Choo. Reference-based image composition with sketch arXiv preprint via structure-aware diffusion model. arXiv:2304.09748, 2023. 2 [26] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [27] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook Seo. Large-scale text-to-image 9 generation models for visual artists creative works. In Proceedings of the 28th International Conference on Intelligent User Interfaces, page 919933, New York, NY, USA, 2023. Association for Computing Machinery. 3 [28] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36, 2024. 2 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 3 [30] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 6, 7 [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 4, 5, [32] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint arXiv:2303.05125, 2023. 2 [33] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple In Proceedings of the 37th International Consubjects. ference on Neural Information Processing Systems, pages 5750057519, 2023. 2 [34] Weihang Mao, Bo Han, and Zihao Wang. Sketchffusion: Sketch-guided image editing with diffusion model. In 2023 IEEE International Conference on Image Processing (ICIP), pages 790794. IEEE, 2023. 2 [35] Naoki Matsunaga, Masato Ishii, Akio Hayakawa, Kenji Suzuki, and Takuya Narihira. Fine-grained image editing by pixel-wise guidance using diffusion models. arXiv preprint arXiv:2212.02024, 2022. 1, 2 [36] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. 2 [37] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general diffusion-based image editing. arXiv preprint arXiv:2311.01410, 2023. [38] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative In ACM SIGGRAPH 2023 Conference image manifold. Proceedings, pages 111, 2023. 1, [39] Xiaohan Peng, Janin Koch, and Wendy E. Mackay. Designprompt: Using multimodal interaction for design exploration the 2024 ACM with generative ai. Designing Interactive Systems Conference, page 804818, New York, NY, USA, 2024. Association for Computing Machinery. 3 In Proceedings of [40] Aleksey Petrov. Comfyui custom nodes alekpet. https:// github.com/AlekPet/ComfyUI_Custom_Nodes_ AlekPet, 2024. 8 [41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 5 [42] Tiziano Portenier, Qiyang Hu, Attila Szabo, Siavash Arjomand Bigdeli, Paolo Favaro, and Matthias Zwicker. Faceshop: Deep sketch-based face image editing. arXiv preprint arXiv:1804.08972, 2018. 2 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7, [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 1 [45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 4 [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1 [47] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 1, 2 [48] Jaskirat Singh, Jianming Zhang, Qing Liu, Cameron Smith, Zhe Lin, and Liang Zheng. Smartmask: Context aware highfidelity mask generation for fine-grained object insertion and layout control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6497 6506, 2024. 1, 2 [49] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [50] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietikainen, and Li Liu. Pixel difference In Proceedings of networks for efficient edge detection. the IEEE/CVF international conference on computer vision, pages 51175127, 2021. 5 [51] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning textto-image generation with image understanding feedback. In Synthetic Data for Computer Vision Workshop@ CVPR 2024, 2023. 2 10 ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59515961, 2022. 2, 6, 7 [65] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024. 2 [66] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 3, 4, 5, 7 [67] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [68] Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, and Yusuke Iwasawa. Paste, inpaint and harmonize via denoising: Subject-driven image editing with pre-trained diffusion model. arXiv preprint arXiv:2306.07596, 2023. 1, 2 [69] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023. 1, 2 [52] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 2 [53] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models In Proceedings of the IEEE/CVF are in-context learners. Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 2 [54] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano. picture is worth more than 77 text tokens: Evaluating In Proceedings of clip-style models on dense captions. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2670026709, 2024. 5 [55] Jue Wang, Yuxiang Lin, Tianshuo Yuan, Zhi-Qi Cheng, Xiaolong Wang, Jiao GH, Wei Chen, and Xiaojiang Peng. Flexedit: Marrying free-shape masks to vllm for flexible image editing. arXiv preprint arXiv:2408.12429, 2024. 2 [56] Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, and Tianyi Zhang. Promptcharm: Text-to-image generation through In Proceedings multi-modal prompting and refinement. of the CHI Conference on Human Factors in Computing Systems, New York, NY, USA, 2024. [57] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. arXiv preprint arXiv:2407.05600, 2024. 2 [58] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 2 [59] Chufeng Xiao and Hongbo Fu. Customsketching: Sketch concept extraction for sketch-based image synthesis and editing. arXiv preprint arXiv:2402.17624, 2024. 2 [60] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. 2 [61] Shuai Yang, Zhangyang Wang, Jiaying Liu, and Zongming Guo. Deep plastic surgery: Robust and controllable image In Computer Vision editing with human-drawn sketches. ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XV 16, pages 601 617. Springer, 2020. 2 [62] Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, Hideki Koike, et al. Imagebrush: Learning visual in-context instructions for exemplar-based image manipulation. Advances in Neural Information Processing Systems, 36, 2024. 1, [63] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v (ision) for autoarXiv preprint matic image design and generation. arXiv:2310.08541, 2023. 2 [64] Yu Zeng, Zhe Lin, and Vishal Patel. Sketchedit: Maskfree local image manipulation with partial sketches. In Pro11 MagicQuill: An Intelligent Interactive Image Editing System

Supplementary Material
======================

A. Implementation Details A.1. Editing Processor Our Editing Processor is built upon Stable Diffusion v1.5 [44] and is compatible with all customized fine-tuned weights. We set the control parameters with inpainting strength wI = 1.0 and control strength wC = 0.5, while expanding the mask region by 15 pixels during controllable inpainting. The generation process employs the Euler ancestral sampler with Karras scheduler [24], requiring 20 steps per generation. On standard hardware, generating 512 512 resolution image takes approximately 2 seconds with 15 GB VRAM consumption. For the control branch, we conduct fine-tuning on the LAION-Aesthetics dataset [46], specifically selecting images with aesthetic scores above 6.5. The training process spans 3 epochs with learning rate of 5e 6 and batch size of 8. A.2. Painting Assistor We fine-tune LLaVA-1.5 model with 7B parameters for Draw&Guess task on our own constructed dataset in leveraging LoRA [18]. The LoRA rank and Sec. 3.2, alpha are 64 and 16 respectively. The model is trained for 3 epochs with learning rate of 2e 5 and batch size of 8. Under 4-bit quantization, the model achieves real-time prompt inference within 0.3 seconds using only 5 GB VRAM, enabling efficient on-the-fly prompt generation with satisfactory accuracy. A.3. Idea Collector Cross-platform Support. Besides Gradio, MagicQuill can also be integrated into ComfyUI as custom node, as shown in Fig. 9. It is designed with customizable widgets for parameter settings and extensible architecture for future platform integrations. cutting slice out of it, as shown in Fig 2. The user begins by uploading the image through the toolbar, which provides access to range of tools (Fig. 2-B). Using the add brush, the user outlines the slice to be cut directly on the canvas (Fig. 2-D). Meanwhile, the Draw & Guess feature introduced in Sec. 3.2 predicts that the user intends to manipulate cake and suggests the relevant prompt automatically in the prompt area (Fig. 2-A). Afterward, the user switches to the subtract brush to fill in the outlined slice, visually marking the area to be removed from the cake. For additional precision, the eraser tool is available to refine the cut. Once the adjustments are made, the user generates the image by clicking the Run button (Fig. 2-F), which runs the model detailed in Sec. 3.1. The resulting image appears in the generated image area (Fig. 2-E). Users can confirm changes via the tick icon to update the canvas, or click the cross icon to revert modifications. This workflow enables iterative refinement of edits, providing flexible control throughout the process. B. Failure Case B.1. Failure Case of Editing Processor Scribble-Prompt Trade-Off. We observe quality degradation when user-provided add brush strokes deviate from the semantic content specified in the prompt, common occurrence among users with limited artistic skills. This creates fundamental strictly following the trade-off: scribble structure may compromise the generation quality with respect to the text prompt. To address this issue, we propose adjusting the edge control strength. Figure 9. MagicQuill as custom node in ComfyUI. (a) Users Input (b) Edge Strength: 0.6 (c) Edge Strength: 0.2 Figure 10. Illustration of the Scribble-Prompt Trade-Off. Given user-provided brush strokes (a) with the text prompt man, we show generation results with different edge control strengths: (b) with strength of 0.6 and (c) with strength of 0.2. Usage Scenario. To demonstrate the user-friendly workflow of MagicQuill, we present an illustrative scenario: user wants to modify an image of complete cake, As demonstrated in Fig. 10, when presented with an oversimplified sketch that substantially deviates from the prompt man, high edge strength of 0.6 produces results 1 that, while faithful to the sketch, appear inharmonious. By reducing the edge strength to 0.2, we achieve notably improved generation quality. Colorization-Details Trade-Off. We observe tradeoff between colorization accuracy and detail preservation. Since our conditional image inpainting pipeline relies on downsampled color blocks and CNN-extracted edge maps as input, structrual details in the edited regions may be compromised during the generation process. (a) Original Image (b) Color brush, α 1.0 (c) Result for α 1.0 (d) Color brush, α 0.8 (e) Result for α 0. Illustration of the Colorization-Detail Trade-Off. Figure 11. Results of color brush strokes with different alpha values: (b, c) using alpha value 1.0, and (d, e) using alpha value 0.8, where the latter better preserves more structural details of the original image. As illustrated in Fig. 11, this limitation can be partially mitigated by reducing the alpha value of the color brush trokes, which preserves more information from the original image when downsampled to color blocks. Future work could explore using grayscale images as the control condition to achieve colorization while maintaining fine-grained structural details. B.2. Failure Case of Painting Assistor Ambiguity of the Brush Strokes. Our system enables users to express their editing intentions through brush strokes, which are then interpreted by the Painting Assistor via Draw&Guess. However, this approach faces inherent limitations due to the ambiguous nature of user-provided sketches. For instance, simple circular sketch could represent various objects like strawberry, raspberry, or candy, making it challenging for the model to accurately infer the users intended modification, as shown in Fig. 12. This ambiguity in sketch interpretation can lead to 2 (a) Users Input (b) Prompt: Candy (c) Prompt: Raspberry Figure 12. Demonstration of semantic ambiguity in sketch interpretation. (A) Users sketch intended to represent raspberry; (B) Our Draw&Guess model incorrectly interprets the sketch as candy, leading to misaligned generation; (C) The expected generation result with correct raspberry interpretation. misaligned generations that deviate from the users expectations. Fortunately, our user study reveals that participants were generally understanding of such interpretation errors and considered the models predictions to be reasonable attempts at disambiguating their sketches. C. Generalizability of Editing Processor Our Editing Processor demonstrates generalization capabilities across various fine-tuned Stable Diffusion v1.5 models. Since both the inpainting and control branches preserve the weights of pre-trained diffusion models, our method seamlessly integrates with any community fine-tuned model as plug-and-play component. We validate this versatility by testing on several popular fine-tuned models including RealisticVision, GhostMix, and DreamShaper, achieving consistent editing performance while inheriting the unique stylistic characteristics of each model, as shown in Fig. 13. This compatibility highlights the practical value of our Editing Processor, as users can leverage their preferred finetuned models or LoRA [18] weight while maintaining the editing capabilities provided by our framework. D. In-Context Editing Intent Interpretation The MLLM in Painting Assistor, fine-tuned on our own constructed dataset in Sec. 3.2, demonstrates sophisticated in-context reasoning capabilities for editing intent interpretation. The model effectively leverages contextual visual information to interpret user brush strokes based on their surrounding environment. For instance, simple vertical line is interpreted differently based on its context: as candle on cake, column on ruins, or an antenna on robot, as illustrated in Fig. 14. These context-aware interpretations validate the effectiveness of our dataset construction approach and highlight the models ability to incorporate environmental cues in its reasoning process. Figure 15. The baseline system implemented in ComfyUI. editing experience, with varying skill levels, providing realistic range of user proficiency. To control for learning effects, we randomly divided participants into two groups: Group used MagicQuill before the baseline  (Fig. 15)  , while Group followed the reverse order. Each participant completed comprehensive evaluation consisting of 10 questions per system, modified from the System Usability Scale (SUS) [5], spanning four key categories: Complexity and Efficiency, Consistency and Integration, Ease of Use, and Overall Satisfaction. . The detailed evaluation results are presented in Fig. 16. Additionally, participants responded to 2 specific questions addressing the Painting Assistors accuracy and efficiency detailed in Sec. 4.2. In the Ease of Use category, all participants rated the easiness (Q1) with score of 3 or above, and most reported learning our system more quickly (Q3, Q4) and independently (Q2) compared to the baseline. These findings indicate lower barrier to entry for creative tasks with our system. For Complexity and Efficiency, 80% of participants found our systems complexity appropriate (Q5), contrasting with perceptions of excessive complexity in the baseline. Additionally, 83.3% felt our system was smooth to use (Q6), suggesting that our design lowered cognitive load and supported efficient task completion. In Consistency and Integration, 80% agreed on effective feature integration (Q7), and 90% of participants agreed that our system was consistent and coherent (Q8). This feedback suggests our system provided cohesive and intuitive user experience. Lastly, for Overall Satisfaction, 93% expressed willingness to use our system (Q9), and 83% reported confidence in using it (Q10). This high satisfaction rate reflects positive user reception and highlights the systems overall effectiveness in meeting user expectations in editing. The systems ability to maintain user engagement was evidenced by users voluntarily extending their editing sessions beyond the allocated time. After minimal training, users were able to create compelling edits, demonstrating the systems accessibility and ease of use. gallery of useredited images is presented in Fig. 17. (a) Original Image (b) Users Input (c) Editing Result Figure 13. Demonstration of our methods generalization capability across different fine-tuned Stable Diffusion models. Results shown using RealisticVision (top row), GhostMix (middle row), and DreamShaper (bottom row) as base models, all achieving consistent editing performance. (a) Guess: Antenna (b) Guess: Candle (c) Guess: Column Figure 14. Examples of context-aware editing intention interpretation. The MLLM interprets the same vertical line sketch differently based on surrounding context: (a) as an antenna on robots head, (b) as candle on birthday cake, and (c) as column among ancient ruins. E. User Study Details and Questionnaires To assess the effectiveness and usability of the Painting Assistor and Idea Collector, we recruited 30 participants from diverse backgrounds, including postgraduate students, artists, and computer vision researchers. All had image 3 Figure 16. The questionnaire and user ratings comparing MagicQuill to the baseline system (1=strongly disagree, 5=strongly agree). Figure 17. gallery of creative image editing achieved by the participants of the user study using MagicQuill. Each pair shows the original image and its edited version, demonstrating diverse user-driven modifications.

