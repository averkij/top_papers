Title: A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents

Authors: Ankan Mullick, Sombit Bose, Abhilash Nandy, Gajula Sai Chaitanya, Pawan Goyal


================================================================================

Abstract
========

In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with a single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is a notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing a multi-lingual multi-label intent dataset. We introduce a novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose a pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network-based system over baseline approaches in terms of accuracy and F1-score across various datasets.

Start
=====

A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents 1Ankan Mullick 1Sombit Bose 2Gajula Sai Chaitanya and 1Pawan Goyal {ankanm, sbcs.sombit.24, nandyabhilash}@kgpian.iitkgp.ac.in 1Abhilash Nandy 4 2 0 2 9 ] . [ 1 6 7 4 2 2 . 0 1 4 2 : r gsaichai@qti.qualcomm.com pawang@cse.iitkgp.ac.in 1Computer Science and Engineering Department, IIT Kharagpur, India 2Qualcomm, India

Abstract
========

In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing multilingual multi-label intent dataset. We introduce novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network based system over baseline approaches in terms of accuracy and F1-score across various datasets.

Introduction
============

Task-oriented dialogue systems have become major field of study in recent years, significantly advancing the capabilities of Natural Language Understanding (NLU). These systems execute command-based tasks, demonstrating versatility in handling diverse user queries through set of predefined skills, known as intents. Users interact with dialogue systems to fulfill their needs, and intent detection plays pivotal role in comprehending user queries and generating appropriate responses in task-oriented conversations, thereby maintaining user engagement. The task of intent detection involves identifying the intent(s) within given statement or query, which represents the underlying meaning conveyed by the user. For example, the query How is the weather today?" would be associated with the GetW eather intent. Dialogue systems rely on detecting these intents to understand user queries and provide suitable answers. However, in real-world conversation, query or statement often contain multiple different intents. For instance, as shown in Fig. 1, for the query (from Facebook English dataset): remind me to pick up contact lenses tomorrow, set the alarm for 5 mins and 30 seconds", contains two distinct intent categories with following spans: remind me to pick up contact lenses tomorrow (set reminder intent) and set the alarm for 5 mins and 30 seconds (set alarm intent). Both of these are fine intent categories. Multiple similar fine intents can be merged to create one coarse intent as explained in Table 1. Thus, the above query contains reminder_service and change_alarm_content coarse intents as shown In case of multiple intents in senin Fig. 1. tence, one intent which is dominant and most important in that sentence can be termed as Primary intent while the other intents can be considered Non-Primary. For example, in the query (From Mix-SNIPS dataset) How is the weather today? It would be lovely to go for movie" is combination of two simple sentences How is the weather today? and It would be lovely to go for movie, whose intents are GetW eather and BookM ovieT icket respectively. Out of the two possible intents, BookM ovieT icket is primary (primary and main focus of the sentence) and GetW eather becomes non-primary. It would require an intent span extraction algorithm to extract multiple intent spans and multi-label, multi-class classifier to detect different fine and coarse intents. Over the past few years, researchers concentrate on intent identification across different domains. Flexible and adaptive intent class detection models have been developed for dynamic and evolving realworld applications. (Liao et al., 2023; Kuzborskij Figure 1: Examples of multi-label multi intent datasets (SNIPS, Facebook and BANKING) et al., 2013; Scheirer et al., 2012; Degirmenci and Karal, 2022) focus on streaming data to identify evolving new classes using incremental learning. SENNE Cai et al. (2019), IFSTC (Xia et al., 2021), SENC-MaS (Mu et al., 2017b), SENCForest (Mu et al., 2017a), ECSMiner (Masud et al., 2010) aim at SENC (streaming emerging new class) problem on intents on streams. (Sun et al., 2016) work on emergence and disappearance of intents. (Wang et al., 2020) uses high dimensional data for streaming classification. (Mullick et al., 2022d) identifies multiple novel intents using clustering framework. (Na et al., 2018; Zhan et al., 2021; Larson et al., 2019; Yan et al., 2020; Zhou et al., 2022; Firdaus et al., 2023) detect new intents in the form of outlier detection. Unlike the previous single-intent detection models, which can easily utilize the utterances sole intent to guide slot prediction, multi-intent SLU (Spoken Language Understanding) encounters the challenge of multiple intents, presenting unique and worthwhile area of research. (Mullick et al., 2023, 2022b; Mullick, 2023b,a; Mullick et al., 2022a) explore intent detection in different directions. AGIF (Qin et al., 2020), GL-GIN (Qin et al., 2021), (Gangadharaiah, 2019), (Song et al., 2022) work on multiple intent identification problem but these approaches do not detect the sentence spans related to different intents and also do not distinguish the primary and non-primary intents. Based on Convert (Henderson et al., 2019) backed framework, (Coope et al., 2020) extract spans for different slots but does not extract and identify multiple intents. (Mullick et al., 2024; Guha et al., 2021; Mullick et al., 2022c) focus on entity extraction in different forms. Previous research also includes both pipeline-based approaches (Jiang et al., 2023) and end-to-end methods (Ma et al., 2021; Cui et al., 2019; Ma et al., 2022). However, our work is different from the fact that we identify multiple intent spans along with their corresponding fine and coarse labels. Our work differs from the fact that, we extract multiple intent spans from given sentence and detect its coarse and fine intent labels. In this paper, we seek to address the following research questions in the field of multi-label multi-class intent detection with span extraction: 1. We introduce novel multi-label multi-class intent detection dataset (MLMCID-dataset) utilizing diverse set of existing datasets with various intent sizes in multilingual settings (English and non-English languages), including coarse and finegrained intent labeling along with primary and nonprimary intent marking. 2. We thereafter, build pointer network based encoder-decoder framework to extract multiple intent spans from the given query. 3. We propose feed-forward network based intent detection module (MLMCID - Multi-Label Multi-Class Intent Detection) to automatically detect multiple primary and non-primary intents for coarse and fine categories in sextuplet form. We evaluate the performance of MLMCID for full and few shot-settings across several MLMCID datasets. 4. We experiment with different LLMs (Llama2, GPT) to assess their efficacy, comparing them with our approach, and providing detailed qualitative analysis along with specialized loss function for multi-label multi-class intent detection. Empirical findings on various MLMCID datasets demonstrate that our pointer network based RoBERTa model surpasses other baselines methods including LLMs, achieving higher accuracy with an improvement in macro-F1.

2 Dataset
=========

We conduct different experiments to evaluate our framework on various datasets - all of which are benchmark datasets in NLU domain. We consider three different sizes of the datasets (as per intent class count - mentioned within bracket) - (i) Small: a) SNIPS (10 intents) (Coucke et al., 2018), b) ATIS (21 intents) (Tur et al., 2010), c) Facebook Multi-lingual (12 intents) (Schuster et al., 2018) (consisting of the comparable corpus of English, Spanish and Thai data), abbreviated as Fb. (ii) Medium: a) HWU (64 intents) (Liu et al., 2019a), b) BANKING (77 intents) (Casanueva et al., 2020). (iii) Large: a) CLINC (150 intents) (Larson et al., 2019). Intents of similar domains which convey similar broader meaning and are manually grouped together to make coarse-grained labels from original fine-grained labels 1. Table 1 shows an example of Facebook-English (Fb-en) combining multiple fine intents (like - cancel reminder, set reminder, show reminders) which are closely similar and convey similar broader meaning of reminder_service so these are grouped together to form one single broad coarse grained intent label - reminder_service and an example of SNIPS combining multiple fine intents (like - GetTrafficInformation, ShareETA) are merged into one single course intent class (Traffic_update). Finally, we end up with course intent class of 4 for SNIPS, 5 for Facebook, 18 for HWU, 12 for Banking and 120 for CLINC2. Due to space shortage, the details are in Appendix Table 12 and 13. Fine Intents Combined cancel reminder, set reminder, show reminders GetTrafficInformation, reETA ShaCoarse Intent reminder_service Traffic_update Table 1: Fine-Course Intent for Fb-en and SNIPS All the above datasets are of single intent. In order to validate the broad applicability of the model, we follow the MixAtis and MixSNIPS datageneration guidelines (Qin et al., 2020) to prepare multi-intent datasets for Fb, HWU, BANKING and CLINC. We also use MixATIS and MixSNIPS datasets (Qin et al., 2020). All datasets are in English except for Facebook - which contains Spanish and Thai also along with English. Three annotators are selected after several discussions and conditions of fulfilling criteria like annotators should have domain knowledge expertise along with good working proficiency in English. Each formed sentence instance is manually checked for correctness, coherence, grammatically meaningful and filter out many sentences which do not qualify. Annotators mark Multiple intents and their respective spans within the specified sentence. Annotators 1Course intent is combination of multiple similar meaning or closely matching finer intents of higher hierarchy. One coarse-grained intent is cluster of multiple closely matching fine-grained labels. 2For ATIS we keep fine intents as it is, without coarse intents due to high dis-similarity among intents also point out which intent is Primary3 and which one is non-Primary. If Primary and non-Primary intents can not be distinguished then both of the intents are considered as Primary.

Dataset
=======

Mix-SNIPS 11000 13161 Mix-ATIS 800 FB-EN 800 FB-ES 800 FB-TH 780 HWU64 BANKING 1156 1353 498 284 Train Dev 2197 600 100 100 100 97 144 169

CLINC
Yahoo
MPQA
================

Test 2198 829 100 100 100 97 144 169 162 136 Table 2: MLMCID-dataset statistics To show the real world applicability of our framework, we also experiment on two different practical datasets: a) MPQA4 (Multi Perspective Question Answering) (Mullick et al., 2016, 2017), b) Yahoo News article (Mullick et al., 2016, 2017). Intent can be broadly categorised as opinionated or factual. Each sentence from MPQA and Yahoo news articles is marked as opinion and fact. Further, opinions can be of four different subcategory (Asher et al., 2009) - Report, Judgment, Advise and Sentiment and facts can be subcategorised into five types (Soni et al., 2014) - Report, Knowledge, Belief, Doubt and Perception. So coarse intent can be sub-categorized in four opinionated fine-intents and five factual fine-intents. In MPQA and Yahoo news article, annotators are told to identify different clauses of compound and complex sentences and mark the fine label intent categories for opinion and fact. In all the annotation tasks - initial labeling is done by two annotators and any annotation discrepancy is checked and resolved by the third annotator after discussing with others. Overall inter-annotator agreement is 0.89 which is considered good as per (Landis and Koch, 1977). The detail statistics of train-dev-test divisions of different dataset intent dataset are shown in Table 2. We term this dataset as MLMCID-dataset. We use the Facebook data from MLMCIDdataset comprising 1000 text instances and corresponding intent labels are annotated for its 3 vari3Between two intents, we define one as primary which is more important than others and main focus of the sentence 4https://mpqa.cs.pitt.edu/ Figure 2: Pointer Network Based multi-label, multi-class intent detection (MLMCID) architecture ations - English, Spanish and Thai. The text instances of English, Spanish and Thai languages are termed as Facebook (English), Facebook (Spanish) and Facebook (Thai) dataset respectively. coarse intent label, fine intent label) even if there is an overlap with other intents. c1 and c2 mark the course labels. f1 and f2 indicates fine labels. outi is the ith output sextuple.

4 Solution Approach
===================

i , ep2 , inf2 , inf1 ), inc2 ), inc and ep2 and stp2 To formally describe the multi-label, multi-class intent detection (MLMCID) problem setting, let there be an input sentence Si = {w1, w2, ..., wn} contains words. The model aims to extract multiple intent spans along with their coarse and fine classes in the form of sextuple, ST = {outiouti = ]}ST [(stp1 , ep1 , (stp2 i=1 ; where ti denotes the ith triplet and ST denotes the length of the sextuple set. stp1 represents the beginning position of first intent span and second intent span respectively for the ith sextuple. Similarly, ep1 denotes the end position of first intent span and second intent span for the ith sextuple. So (stp1 and ep1 ) mark the first intent and ep2 span for the ith sextuple. Similarly, (stp2 ) mark the second intent span for the ith sextuple. inc1 represents the possible coarse and fine intent class of the first intent span. Similarly, inc2 represents the possible coarse and fine intent class of the second intent span. p1 and p2 denote the two pointer network models. Pointer Network Model has the following advantages: it is joint model for entity extraction and relation classification. Pointer network model can detect an intent in sentence in form of triplet (intent span, and inf1 and inf2 i For the task of multi-label, multi-class intent detection (MLMCID), our goal is to jointly extract the intent spans along with detecting multiple coarse and fine intents. Our MLMCID output representation is sextuple format. We employ pointer network based architecture for joint extraction of the sextuple. Following are the different components of solution framework approach:

4.1 Encoder
===========

We use four different embeddings in the encoder block (for English language datasets): a) BERT (bert-base-uncased) (Devlin et al., 2019), b) RoBERTa (roberta-base-uncased) (Liu et al., 2019b), c) DistilBERT (Sanh et al., 2019) and d) Electra (Clark et al., 2020). For non-English language datasets (Facebook Thai and Spanish), we utilise mBERT (multilingual BERT) (Pires et al., 2019), XLM-R (XLM-RoBERTa) (Conneau et al., 2020) and mDistilBERT (Sanh et al., 2019). mBERT architecture pre-trained on Wikipedia articles from 104 languages. XLM-RoBERTa is large multi-lingual language model based on RoBERTa, trained on 2.5TB of filtered CommonCrawl data. mDistilBERT is distilled version of mBERT containing 134 million parameters. Let, Si be the ith sentence containing w1, w2 ... wn words. After sentence encoding, the encoder generates vector (VE ) from the ith sentence Si. It is shown in the Encoder Block in Fig 2.

4.2 Decoder
===========

We apply Pointer Network-based approach along with LSTM-based sequence generator, attention model and FFN (Feed-Forward Network) architecture (Similar to (Nayak and Ng, 2020)) to identify intent spans and predict the coarse and fine intent labels. Different blocks are as following: LSTM-based Sequence Generator: The sequence generator structure is based on an LSTM layer with hidden dimension Dh to produce the sequence of two intent spans. Using the attention layer sentence encoding (aE ), pointer network based previous tuple (tupi) and hidden vectors (hD i1) as input to generate the hidden representation of the current token (hD 0 ) denotes the dummy tuple. Following are LSTM outcomes: ). The tup0 = ( tupi = i1 (cid:88) tupj j=0 = LSTM(aE hD tupi1, hD i1) (1) (2) (3) ˆst stp 1 + b1 sthm = w1 st, 1 = softmax( ˆst ), hm = w1 ˆe1 + b1 ep1 = softmax(ˆe1 ) (4) Attention Modeling: Utilizing Bahdanau et al. (2014) attention algorithm we use previous tuple (tupi1) and hidden vector (hD i1) as input at timestamp to produce the attention weighted context vector (aE ) for the current input sentence. Pointer Network: Bi-LSTM layer with hidden dimension DH , followed by two FFN (Feed Forward Networks), constitutes pointer network. Here we use two-pointer networks for extracting two intent spans. We concatenate hD and VE (obtained from the encoding layer) to provide the input of Bi-LSTM model (forward and backward LSTM), which provides hidden representation to be fed to FFN models. Two FFNs with softmax provide scores between 0 and 1, the start (st) and end (e) index of one intent span. where w1 FFN. b1 forward layers (FFN). ˆst are the weight parameters of are the bias parameters of the feedi are normalized st and w1 1 and ˆe1 st and b1 probabilities of the ith source sentence. stp1 and ep1 denotes the begin and end token of the first intent span in the first pointer network model of the ith source sentence. Then, the second pointer network model extracts the second entity. After concatenating the first Bi-LSTM output vector (hm ) with decoder sequence generator output (hD ) and sentence encoding (VE ), we feed them to the second pointer network to obtain the position of the begin and end tokens of the second intent span. Together, these two pointer networks produce the feature vectors tupi containing intent span 1 (span1 ) and span 2 (span2 Intent Detector: We concatenate tupi with hD and pass it through feed-forward network (FFN) with softmax to produce the normalized probabilities over intent sets and thereby predict the coarse (inc1 ) intent labels for first and second spans. ) and fine (inf1 , inc , inf2 ). i

4.3 Baselines
=============

We employ different open-source LLMs with prompt based fine-tuning on the training set to generate the two different intent spans and detect coarse and fine intents. Llama2:5 We apply Llama2-7b ((Touvron et al., 2023)) using Quantized Low-Rank Adaptation (QLoRA) (Dettmers et al., 2023) (to optimize training efficiency) for supervised fine-tuning using MLMCID-Datasets. GPT: We also use state-of-the-art large-size LLMs, developed by OpenAI: GPT-3.5 (gpt) 6 and GPT4 (OpenAI, 2023)7 with example based prompting to extract intent spans and identify coarse and fine intents (Computed on April, 2024).

5 Experiments
=============

To validate our proposed framework, we compare the Pointer Network Model (PNM) of MLMCID while taking various embeddings as input: BERT, RoBERTa, DistilBERT, and Electra on all datasets. We also explore different large language models (Llama2-7b, GPT-3.5 and GPT-4) to check how effectively they can extract multiple intent spans and detect different intents. After that, we experiment with different variations of overall best performing RoBERTa model - varying the training data 5https://ai.meta.com/llama/ 6https://chat.openai.com/ 7https://openai.com/gpt-4 Dataset MIX_SNIPS FACEBOOK (English) MIX_ATIS HWU64 BANKING CLINC Overall Average BERT (p, av) 89.2,80.2 89.0,80.1 98.0,80.8 98.2,88.2 71.3,64.6 51.7,38.6 83.5,68.0 81.9,65.9 84.0,76.9 82.7,71.4 86.3,72.7 77.1,64.1 84.1,75.7 80.8,73.9 RoBERTa (p, av) 90.0,81.9 89.7,82.1 98.5,81.2 92.8,82.8 70.2,63.5 53.4,38.8 85.5,70.0 80.0,63.7 85.4,78.5 85.2,75.2 92.3,81.3 88.3,75.5 88.2,78.5 85.2,75.8 DistilBERT (p, av) 89.2,80.2 88.5,79.4 97.2,80.2 92.8,82.2 72.2,63.6 50.3,35.8 82.5,66.2 79.9,64.1 78.8,70.9 79.2,67.9 79.8,68.0 71.7,60.0 82.2,73.2 81.4,70.6 Electra (p, av) 89.8,80.7 89.5,80.5 97.4,80.5 92.8,83.1 70.6,59.7 46.3,35.5 83.0,66.2 79.4,62.5 79.9,71.8 79.4,68.1 88.7,71.7 81.3,63.0 85.7,72.2 80.9,71.3 Llama2 (p, av) 48.3,41.2 42.6,40.5 21.0,19.2 20.6,19.6 16.9,15.0 15.7,14.0 35.8,38.1 32.9,30.5 31.5,31.6 28.2,29.1 57.5,55.9 51.2,50.3 34.1,37.0 30.5,32.8 GPT-3.5 (p, av) 60.4,55.8 60.2,56.2 70.7,62.1 65.3,60.8 29.5,32.5 27.2,31.5 56.0,52.3 50.6,51.2 25.4,20.5 20.2,20.3 58.7,57.2 56.3,55.3 49.2,38.1 44.9,41. GPT-4 (p, av) 64.7,61.1 62.5,60.3 75.6,76.5 72.6,70.5 38.7,32.8 36.8,32.6 59.1,53.1 57.3,56.4 47.9,47.4 45.2,43.6 64.3,56.6 63.7,54.3 60.6,53.3 58.7,53.6 F1 F1 F1 F1 F1 F1 F1 Table 3: Overall Accuracy (A) and Macro F1-score (F1) in (%) of different models in MLMCID and LLMs for coarse labels (on English Datasets) - primary intent (p) and average(av). (The best outcomes are marked in Bold) size to understand how much training data is required for decent performance. We also perform zero-shot and few-shot experiments to check the approachs usefulness in the presence of minimal data. Tables 3, 4 and 5 show the overall performances of different models for the English (MixSNIPS, Mix-ATIS, Facebook, HWU, BANKING and CLINC) and Non-English (Facebook Thai and Spanish) datasets. We use prediction accuracy and macro F1-score as evaluation metrics. Table 3 and 4 infer performances on primary and overall average of coarse and fine intent labels on English datasets. Following are the details of our findings: Findings 1: For coarse label intent detection, as shown in Table 3, RoBERTa (with PNM) in MLMCID achieves superior performances in terms of accuracy and F1-score across all datasets of different intent sizes (Mix-SNIPS, Mix-ATIS, HWU, BANKING, CLINC) for both primary intent detection and overall average except for Facebook English where BERT is more effective in terms of F1-score for both primary and overall average. Findings 2: Similar to coarse intent detection, for fine label intent detection, RoBERTa (with PNM) in MLMCID also produce better results than others in terms of accuracy and F1-score for most of the cases across all English datasets except for Facebook English dataset, where Electra provides better outcome in terms of accuracy and F1-score for both primary and overall intent detection. It is shown in Table 4. Findings 3: For all English datasets, BERT, RoBERTa, DistilBERT and Electra performs almost similar with decent accuracy and F1-score which signifies the utility of pointer network model based MLMCID architecture. Findings 4: We observe that the LLMs (Llama-27b, GPT-3.5, GPT-4) fall behind in performance from Pointer Network based approaches with different encoders, even though they are much larger than our proposed framework, thus strengthening the need for such specialized MLMCID architecture. Llama2-7b performs poorly among three LLMs - this may be due to the fact of less contextual understanding in this specific task. More details in Appendix A. Findings 5: RoBERTa with PNM in MLMCID performs better than any other models for overall average accuracy and F1-score across all English datasets for both primary and average course and Dataset MIX_SNIPS FACEBOOK (English) MIX_ATIS HWU64 BANKING CLINC Overall Average BERT (p, av) 85.4,80.9 83.5,80.1 96.5,81.3 87.5,79.5 71.3,64.6 51.7,38.6 74.1,57.2 57.9,43.6 78.5,61.2 73.5,57.0 88.1,73.9 81.7,66.9 82.3,69.9 72.7,60.9 RoBERTa (p, av) 89.6,85.0 89.0,85.9 97.5,80.7 94.5,82.0 70.2,63.5 53.4,38.8 83.0,67.1 68.3,52.8 82.3,71.2 80.0,68.4 89.3,81.2 85.3,74.2 85.3,74.8 78.4,66.9 DistilBERT (p, av) 87.5,81.9 86.6,81.7 96.5,79.7 78.4,73.1 72.2,63.6 50.3,35.8 75.1,57.7 61.0,44.6 69.5,54.3 64.1,51.4 81.6,68.1 75.2,60.8 80.4,67.5 69.3,57.9 Electra (p, av) 86.3,80.9 86.2,82.1 98.5,81.7 95.4,82.7 70.6,59.7 46.3,35.5 70.1,53.9 54.5,41.6 73.3,57.2 67.8,52.4 84.9,70.8 79.4,63.4 80.6,67.4 71.6,59.7 Llama2 (p, av) 35.0,20.1 27.5,22.1 11.1,12.1 9.2,9.7 16.9,15.0 15.7,14.0 29.8,20.3 25.6,19.6 19.0,17.7 15.6,16.2 43.0,37.8 39.6,35.7 25.8,20.5 22.2,19.6 GPT-3.5 (p, av) 64.2,60.5 55.6,51.2 44.4,46.4 40.2,41.3 29.5,32.5 27.2,31.5 41.8,33.2 31.6,30.5 21.0,20.5 18.1,19.4 47.0,40.9 45.4,39.5 41.3,39.0 36.4,35. GPT-4 (p, av) 64.7,61.1 57.3,54.9 73.4,77.6 69.5,69.8 38.7,32.8 36.8,32.6 52.5,48.2 48.9,46.3 27.3,25.7 25.6,24.3 55.7,48.1 51.2,45.3 52.1,48.9 48.2,45.5 F1 F1 F1 F1 F1 F1 F1 Table 4: Overall Accuracy (A) and Macro F1-score (F1) in (%) of different models in MLMCID and LLMs for fine labels (on English Datasets) - primary intent (p) and average(av). (The best outcomes are marked in Bold) Dataset FACEBOOK (Spanish) FACEBOOK (Thai) Average Coarse Fine Coarse Fine Coarse Fine mBERT (p, av) 98.0,80.7 91.3,82.2 96.7,80.0 84.6,80.0 96.5,79.8 88.4,75.8 96.0,79.5 84.1,74.2 97.2,80.3 89.8,79.0 97.3,79.7 84.3,77.1 XLM-R (p, av) 98.5,81.5 92.5,82.7 97.5,81.0 86.0,81.7 97.0,80.0 96.6,78.8 96.5,79.7 82.5,75.5 97.8,80.8 94.6,80.8 97.0,80.8 84.3,78.6 mDistilBERT (p, av) 98.0,80.2 91.1,82.9 96.5,80.2 84.3,76.8 96.8,79.0 94.2,73.4 95.5,77.2 68.8,62.7 97.4,79.6 92.6,78.2 96.0,78.7 76.5,69.8 Llama-2 (p, av) 51.2,39.9 47.2,39.6 38.3,27.2 36.2,30.6 28.0,24.2 25.6,24.8 16.3,15.2 15.7,14.9 39.6,32.1 36.4,32.2 27.3,21.2 25.9,22.8 GPT-3.5 (p, av) 64.6,61.6 62.6,61.3 57.6,56.6 55.4,55.0 69.7,58.6 67.8,57.2 18.2,18.7 17.9,16.8 67.2,60.1 65.2,59.3 37.9,37.7 36.7,35. GPT-4 (p, av) 70.7,75.6 69.4,69.3 69.7,74.2 66.2,65.6 73.4,71.5 71.6,69.3 68.7,64.9 59.2,58.7 72.1,73.6 70.5,69.3 69.2,69.6 62.7,62.2 F1 F1 F1 F1 F1 F1 Table 5: Overall Accuracy (A) and Macro F1 (F1) in (%) of different models in MLMCID and LLMs for coarse and fine grained labels of Facebook Spanish and Thai datasets - primary intent (p) and overall average(av). (The best outcomes are marked in Bold) fine intent detection after intent spans extraction.

Ablation Studies
================

Findings 6: For non-English languages like Spanish (Facebook) and Thai (Facebook) datasets , we observe that for both fine and coarse grained intent labels, XLM-R and mBERT both produce good results but XLM-R outperforms mBERT in all aspects across all datasets and overall for both primary intent detection and overall average intent detection with intent span extraction. Findings 7: To check the effectivity of span extraction by pointer network, we vary the similarity (extracted intent span vs actual intent span) threshold utilise that extracted span to check the overall accuracy. We check for 50% - 90% similarity threshold range and overall framework (RoBERTA with PNM) accuracies (for both primary and average intent) across all datasets for coarse and fine intent labels are shown in Table 6 and 7. It is seen good performance even with 50% similarity which shows the efficacy of the system. 1. K-shot setting: To evaluate the RoBERTa based PNM model of MLMCID architecture, we utilize samples for all English datasets where = 5 (5-shot) and 10 (10-shot) for coarse and fine intent labels. The accuracy and F1-score of primary and average intents are shown in Table 9. This shows even with very limited number of data-points (like in 5-shot), the system is able to achieve decent performance across different datasets. Practical Datasets: We test 2. the trained RoBERTa models with PNM (using SNIPS, BANKING and CLINC dataset) in MLMCID to evaluate on external MPQA and Yahoo datasets. We also check LLMs - Llama2-7b (vanilla and finetuned), GPT-3.5 and GPT-4 on MPQA and Yahoo but RoBERTa based PNM in MLMCID outperfomrs LLMs in most of the cases and show decent performance as shown in Table 8. It is seen that, for Llama2-7b vanilla model performs poorly and Th 50 % 60 % 70 % 80 % 90 % MIX_SNIPS 89.2,80.9 87.7,78.9 79.4,70.8 70.4,63.5 59.2,54.2 FB_en 96.0,78.5 95.0,77.9 91.0,74.6 83.0,68.8 75.0,63.2 Dataset (primary (p) and average (av) intent) in % HWU64 85.5,70.0 85.5,68.9 84.6,68.1 81.9,66.6 77.5,62.6 MIX_ATIS 95.1,90.2 91.9,90.2 85.1,89.2 83.8,88.2 80.9,86.2 FB_es 94.5,77.4 86.5,71.2 75.6,63.1 72.6,59.4 61.6,50. FB_th 89.9,82.4 77.4,70.3 75.2,67.7 71.4,62.9 69.4,59.6 BANKING 81.8,74.7 79.4,72.0 75.9,68.3 69.9,62.8 63.4,56.0 CLINC 90.1,79.2 88.4,77.5 84.0,73.0 79.1,67.6 67.5,58.2 Table 6: Overall Accuracy (A) in (%) of RoBERTa model in MLMCID for coarse grained labels (on English Datasets) - primary (p) and average (av) intents. (Th indicates threshold value) Th 50 % 60 % 70 % 80 % 90 % MIX_SNIPS 83.6,80.7 82.1,78.9 76.1,72.3 68.6,64.8 55.2,52.4 FB_en 93.5,78.1 92.5,77.0 87.6,71.9 78.7,65.9 72.8,61.0 Dataset (primary (p) and average (av) intent) in % HWU64 83.0,67.1 80.4,65.0 79.5,64.3 75.2,62.4 67.5,55.1 MIX_ATIS 95.1,90.2 91.9,90.2 85.1,89.2 83.8,88.2 80.9,86.2 FB_es 91.5,75.9 85.6,70.2 78.7,63.8 74.8,60.6 63.0,50.7 FB_th 89.6,81.1 82.4,79.6 75.9,67.2 68.4,61.0 65.4,57. BANKING 77.1,69.8 74.8,67.5 69.1,62.2 64.5,56.0 57.7,49.4 CLINC 86.6,78.9 86.1,77.4 82.9,70.9 77.0,68.0 66.4,62.8 Table 7: Overall Accuracy (A) in (%) of RoBERTa model in MLMCID for fine grained labels (on English Datasets) - primary (p) and average (av) intents. (Th indicates threshold value) , Dataset MPQA YAHOO Fine Coarse Fine Coarse Llama2-7b Finetune (p,av) 42.8,27.1 65.7,64.2 48.3,37.5 61.2,49.9 Llama2-7b Vanilla (p, av) 18.8,16.9 51.9,50.0 18.8,15.8 52.8,50.0 GPT-3.5 (p, av) GPT-4 (p, av) 20.0,14.2 62.8,59.9 11.4,10.6 50.0,50.0 48.5,37.1 68.5,45.6 58.0,56.2 61.2,49.1 RoBERTaSNIPS(p, av) 45.0,42.5 75.6,43.7 55.3,54.9 66.3,65.7 RoBERTaBANKING(p,av) 44.5,42.0 73.0,41.9 54.0,53.8 64.5,62.9 RoBERTaCLINC(p, av) 43.9,41.5 72.8,42.6 52.9,54.2 63.2,60.8 Table 8: Overall Accuracy (A) in (%) of RoBERTa model in MLMCID (trained on SNIPS, BANKING and CLINC) and LLMs for fine and course grained labels - primary (p) and average (av) intent. fine-tune version perform better but does not outperform GPT and RoBERTa based models. 3. Intent Counts: All datasets have two intents (primary and non-primary) in one sentence except for Yahoo, 2.6% cases with more than 2 intents so we show all results considering the case of 2 intents in sentence. Our system is also effective for more than two intents by utilizing more pointer network block in the decoder framework, as shown in Appendix A.2. Dataset SNIPS FACEBOOK (English) HWUBANKING CLINC 5-shot 10-shot 5-shot 10-shot 5-shot 10-shot 5-shot 10-shot 5-shot 10-shot Coarse (p, avg) 61.0,49.2 58.1,46.4 61.4,52.1 60.7,47.4 83.5,62.0 58.0,42.8 87.5,67.8 59.5,45.9 57.2,39.3 49.3,34.7 62.2,43.5 58.2,39.2 36.0,28.2 32.5,25.0 46.0,32.9 46.1,31.4 78.4,50.4 69.9,44.0 87.3,65.9 79.3,58.5 Fine (p, avg) 70.9,53.3 67.9,51.7 75.9,63.1 75.1,61.0 76.0,58.3 26.7,20.4 83.5,64.3 34.3,25.2 47.8,29.6 35.5,22.1 62.2,43.3 46.2,31.9 62.3,38.6 56.7,34.4 76.1,52.9 71.2,48.0 76.3,53.4 65.8,44.6 89.6,69.7 79.3,58.5 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 Table 9: Accuracy (A) and F1-Score for coarse and fine intents by RoBERTa(in %) for k-shot, = {5, 10} Experimental Settings: Our experiments are conducted on two Tesla P100 GPUs with 16 GB RAM, 6 Gbps clock cycle, GDDR5 memory and one 80GB A100 GPU, 210MHz clock cycle, 2*960 GB SSD with 5 epochs. We use Adam optimizer with learning rate: 105 with cross-entropy as the loss function, weight decay: 105 and dropout rate of 0.5 is applied on the embeddings to avoid overfitting for all experiments (Details are in Appendix). All methods took less than 120 GPU minutes (except Llama2: 4-5 hrs) for fine tuning and 2 hrs for inference. All the hyperparameters are tuned on the dev set. We have used NLTK, Spacy, Scikit-learn, openai (version=0.28), huggingface_hub, torch and transformers python packages for all experiments and evaluation 8.

6 Loss Function
===============

We calculate loss of different intent classes across all samples for primary, non-primary intents and their respective primary and non primary spans as shown in equation 5, 6 and 7 respectively. For training our model, we minimize the sum of negative log-likelihood loss for classifying the intent and the four pointer locations corresponding to the primary and non primary intent spans as shown in equation 8. Lp = 1 (cid:88) (cid:104) (cid:88) (y1)ijlog(pij) i=1 j=1 1 (cid:88) j=1 log((y1)jn) (cid:105) Lnp = 1 (cid:88) (cid:104) (cid:88) (y2)ijlog(pij) i=1 j=1 (5) 1 (cid:88) j= log((y2)jn) (cid:105) (6) Lspan =

1
N × J
=======

N (cid:88) (cid:88) (cid:104) n=1 j=1 (cid:105) (ep1)jn) + log((stp2)jn (ep2)jn) log((stp1)jn (7) Here, is the number of intent classes and (y1) {inc1, inf 1} and (y2) {inc2, inf 2}. (y1)ij and (y2)ij are the one-hot ground truth labels for sample and class for the primary and non-primary intents respectively, and pij is the predicted probability for sample and class j. represents the nth training instance with being the batch size, represents the jth decoding time step with being the length of the longest target sequence among all instances in the current batch. stp, ep; {p1, p2} respectively represent the softmax scores corresponding to the true start and end positions of the primary and non primary spans. Fig 3 shows the 8All Code / Data details are in https://github.com/ ankan2/multi-intent-pointer-network primary and non-primary intent Labels, when predicted wrongly, are swapped, suggesting that the model is still able to grasp the notion of intent. We shall work on these limitations in future.

Ethical Concerns
================

We use publicly available codes and datasets so there is no ethical concerns.

Acknowledgements
================

The work was supported in part by Prime Minister Research Fellowship (PMRF). (a) Combined loss - Coarse (b) Combined Loss - Fine Figure 3: By RoBERTa based pointer network (PNM) model in MLMCID variation of the overall loss for course and fine intents with respect to the training progress (in terms of epochs) across different datasets. Loss decreases with larger epochs and after 10 epochs the loss decrement is significant to obtain decent outcome. = Lp + Lnp + Lspan (8) Gpt-3.5 turbo documentation.

7 Conclusion
============

Intent detection is crucial in task-oriented conversation systems. Earlier works focus on scenarios with the presence of single intent and do not extract intent spans. This work is one of the first to consider multiple intents in single sentence within conversation system, including primary and non-primary intents. First, we create novel datasets using stateof-the-art datasets with coarse and fine intent labels. Then, we develop Pointer Network-based encoder-decoder framework (MLMCID - multilabel multi-class intent detection) using RoBERTa (for English data) and XLM-R (for non-English data) to jointly extract intent spans from sentences and detect corresponding coarse and fine intents. We show that the MLMCID model even outperforms various LLMs for these specific tasks across different datasets. The approach demonstrates efficacy even in few-shot scenarios. Qualitative analysis shows reasonable grasp of primary and secondary intent concepts. Overall, this highlights the importance of multi-intent modeling for real-world conversational AI, with the datasets and models providing strong foundation for future research.

Limitations and Discussion
==========================

Table 3, 4, 5 shows that even when our model fails to give the correct predictions exactly, it predicts the primary intent correctly most of the time. This is due to the fact we are using the top-2 intents to infer the primary and non-primary intents using the same classifier. Also, in some examples, the Nicholas Asher, Farah Benamara, and Yvette Yannick Mathieu. 2009. Appraisal of opinion expressions in discourse. Lingvisticæ Investigationes, 32(2):279 292. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly arXiv preprint learning to align and translate. arXiv:1409.0473. Xin-Qiang Cai, Peng Zhao, Kai-Ming Ting, Xin Mu, and Yuan Jiang. 2019. Nearest neighbor ensembles: An effective method for difficult problems in streaming classification with emerging new classes. In 2019 IEEE International Conference on Data Mining (ICDM), pages 970975. IEEE. Inigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. 2020. Efficient intent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807. Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Sam Coope, Tyler Farghly, Daniela Gerz, Ivan Vulic, and Matthew Henderson. 2020. Span-convert: Fewshot span extraction for dialog with pretrained arXiv preprint conversational representations. arXiv:2005.08866. Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, et al. 2018. Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. arXiv preprint arXiv:1805.10190. Chen Cui, Wenjie Wang, Xuemeng Song, Minlie Huang, Xin-Shun Xu, and Liqiang Nie. 2019. User attentionguided multimodal dialog systems. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pages 445454. Ali Degirmenci and Omer Karal. 2022. Efficient density and cluster based incremental outlier detection in data streams. Information Sciences, 607:901920. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. Mauajama Firdaus, Asif Ekbal, and Erik Cambria. 2023. Multitask learning for multilingual intent detection Information and slot filling in dialogue systems. Fusion, 91:299315. Rashmi Gangadharaiah. 2019. Joint multiple intent detection and slot labeling for goal-oriented dialog. Souradip Guha, Ankan Mullick, Jatin Agrawal, Swetarekha Ram, Samir Ghui, Seung-Cheol Lee, Satadeep Bhattacharjee, and Pawan Goyal. 2021. Matscie: An automated tool for the generation of databases of methods and parameters used in the computational materials science literature. Computational Materials Science (Comput. Mater. Sci.), 192:110325. Matthew Henderson, Iñigo Casanueva, Nikola Mrkšic, Pei-Hao Su, Tsung-Hsien Wen, and Ivan Vulic. 2019. Convert: Efficient and accurate conversational representations from transformers. arXiv preprint arXiv:1911.03688. Sheng Jiang, Su Zhu, Ruisheng Cao, Qingliang Miao, and Kai Yu. 2023. Spm: split-parsing method for joint multi-intent detection and slot filling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 668675. Ilja Kuzborskij, Francesco Orabona, and Barbara Caputo. 2013. From to n+ 1: Multiclass transfer incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 33583365. Stefan Larson, Anish Mahendran, Joseph Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan Kummerfeld, Kevin Leach, Michael Laurenzano, Lingjia Tang, et al. 2019. An evaluation dataset for intent classification and out-of-scope prediction. arXiv preprint arXiv:1909.02027. Guobo Liao, Peng Zhang, Hongpeng Yin, Xuanhong Deng, Yanxia Li, Han Zhou, and Dandan Zhao. 2023. novel semi-supervised classification approach for evolving data streams. Expert Systems with Applications, 215:119273. Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. 2019a. Benchmarking natural language understanding services for building conversational agents. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Zhiyuan Ma, Jianjun Li, Guohui Li, and Yongjing Cheng. 2022. Unitranser: unified transformer semantic representation framework for multimodal task-oriented dialog system. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 103114. Zhiyuan Ma, Jianjun Li, Zezheng Zhang, Guohui Li, and Yongjing Cheng. 2021. Intention reasoning network for multi-domain end-to-end task-oriented dialogue. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 22732285. Mohammad Masud, Jing Gao, Latifur Khan, Jiawei Han, and Bhavani Thuraisingham. 2010. Classification and novel class detection in concept-drifting data streams under time constraints. IEEE Transactions on Knowledge and Data Engineering, 23(6):859 874. Xin Mu, Kai Ming Ting, and Zhi-Hua Zhou. 2017a. Classification under streaming emerging new classes: IEEE solution using completely-random trees. Transactions on Knowledge and Data Engineering, 29(8):16051618. Xin Mu, Feida Zhu, Juan Du, Ee-Peng Lim, and ZhiHua Zhou. 2017b. Streaming classification with emerging new class by class matrix sketching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31. Ankan Mullick. 2023a. Exploring multilingual intent dynamics and applications. IJCAI Doctoral Consortium. Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159174. Ankan Mullick. 2023b. Novel intent detection and active learning based classification (student abstract). arXiv e-prints, pages arXiv2304. Ankan Mullick, Akash Ghosh, Sai Chaitanya, Samir Ghui, Tapas Nayak, Seung-Cheol Lee, Satadeep Bhattacharjee, and Pawan Goyal. 2024. Matscire: Leveraging pointer networks to automate entity and relation extraction for material science knowledgebase construction. Computational Materials Science, 233:112659. Ankan Mullick, Pawan Goyal, and Niloy Ganguly. 2016. graphical framework to detect and categorize diverse opinions from online news. In Proceedings of the Workshop on Computational Modeling of Peoples Opinions, Personality, and Emotions in Social Media (PEOPLES), pages 4049. Ankan Mullick, Shivam Maheshwari, Pawan Goyal, and Niloy Ganguly. 2017. generic opinion-fact classifier with application in understanding opinionIn Proceedings atedness in various news section. of the 26th International Conference on World Wide Web Companion, pages 827828. Ankan Mullick, Ishani Mondal, Sourjyadip Ray, Raghav, Chaitanya, and Pawan Goyal. 2023. Intent identification and entity extraction for healthcare queries in indic languages. In Findings of the Association for Computational Linguistics: EACL 2023, pages 18251836. Ankan Mullick, Abhilash Nandy, Manav Kapadnis, Sohan Patnaik, Raghav, and Roshni Kar. 2022a. An evaluation framework for legal document summarization. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4747 4753. Ankan Mullick, Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, and Raghav. 2022b. Finegrained intent classification in the legal domain. arXiv preprint arXiv:2205.03509. Ankan Mullick, Shubhraneel Pal, Tapas Nayak, SeungCheol Lee, Satadeep Bhattacharjee, and Pawan Goyal. 2022c. Using sentence-level classification helps entity extraction from material science literature. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4540 4545. Ankan Mullick, Sukannya Purkayastha, Pawan Goyal, and Niloy Ganguly. 2022d. framework to generate high-quality datapoints for multiple novel intent detection. arXiv preprint arXiv:2205.02005. Gyoung Na, Donghyun Kim, and Hwanjo Yu. 2018. Dilof: Effective and memory efficient local outlier detection in data streams. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 19932002. Tapas Nayak and Hwee Tou Ng. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 85288535. OpenAI. 2023. Gpt-4 technical report. Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. arXiv How multilingual is multilingual bert? preprint arXiv:1906.01502. Libo Qin, Fuxuan Wei, Tianbao Xie, Xiao Xu, Wanxiang Che, and Ting Liu. 2021. Gl-gin: Fast and accurate non-autoregressive model for joint multiple intent detection and slot filling. arXiv preprint arXiv:2106.01925. Libo Qin, Xiao Xu, Wanxiang Che, and Ting Liu. 2020. Agif: An adaptive graph-interactive framework for joint multiple intent detection and slot filling. arXiv preprint arXiv:2004.10087. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. Walter Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance Boult. 2012. Toward open set recognition. IEEE transactions on pattern analysis and machine intelligence, 35(7):1757 1772. Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2018. Cross-lingual transfer learning for multilingual task oriented dialog. arXiv preprint arXiv:1810.13327. Mengxiao Song, Bowen Yu, Li Quangang, Wang Yubin, Tingwen Liu, and Hongbo Xu. 2022. Enhancing joint multiple intent detection and slot filling with global intent-slot co-occurrence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 79677977. Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob Eisenstein. 2014. Modeling factuality judgments In Proceedings of the 52nd in social media text. Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 415 420. Yu Sun, Ke Tang, Leandro Minku, Shuo Wang, and Xin Yao. 2016. Online ensemble learning of data streams with gradually evolved classes. IEEE Transactions on Knowledge and Data Engineering, 28(6):15321545. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Gokhan Tur, Dilek Hakkani-Tür, and Larry Heck. 2010. What is left to be understood in atis? In 2010 IEEE Spoken Language Technology Workshop, pages 19 24. IEEE. Min Wang, Ke Fu, Fan Min, and Xiuyi Jia. 2020. Active learning through label error statistical methods. Knowledge-Based Systems, 189:105140. Congying Xia, Wenpeng Yin, Yihao Feng, and Philip Yu. 2021. Incremental few-shot text classification with multi-round new classes: Formulation, dataset and system. arXiv preprint arXiv:2104.11882. Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, Xiao-Ming Wu, and Albert YS Lam. 2020. Unknown intent detection using gaussian mixture model with an application to zero-shot intent classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 10501060. Li-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, XiaoMing Wu, and Albert Lam. 2021. Out-of-scope intent detection with self-supervision and discriminative training. arXiv preprint arXiv:2106.08616.

A Experimental Findings
=======================

A.1 Why encoder decoder model performs well Pointer Network model is state-of-the-art approach which is ideal for extracting multiple spans from sentence using the pointing mechanism to directly select positions in the input sequence, allowing for variable-length outputs and precise boundary identification. Their attention mechanism effectively handles context, enabling accurate span extraction in computationally efficient manner. It is effective also because of - Dynamically predict entity spans within sequence, enhancing adaptability across various NLP tasks Yunhua Zhou, Peiju Liu, and Xipeng Qiu. 2022. Knncontrastive learning for out-of-domain intent classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51295141. capture the interdependence between spans and intents, crucial for tasks where one intents prediction relies on another characteristics within the same context. Reduce the need for manual feature engineering, learning to predict spans directly from input data for more efficient models Finally, enable end-to-end learning by directly predicting entity span positions, facilitating seamless integration with other neural network components. A.2 PNM for more than two intent cases To evaluate the effectiveness of the Pointer Network framework for more than two intents, we experimented with small sample from the MIX_SNiPS, BANKING, and CLINC datasets, incorporating three intents. For instance, the sentence "Will it snow this weekend? Please help me book rental car for Nashville and play that song called Bring the Noise" includes the intents: weather, car_rental, play_music. Table 10 presents the performance of RoBERTa on this annotated sample. The results demonstrate the effectiveness of our system in handling larger number of intents, as reflected by the accuracy (in %). A.3 Scalability We experiment with datasets composed of two intents with the P100 server with 16GB GPU where 6-9 GB GPU VRAM has been utilised. Further we experiment on the dataset with three intents in the same server which use 12-13 GB GPU VRAM so

C Example
=========

Figure 4 shows some examples from MLMCID dataset. Table 12 and 13 shows some examples of fine to coarse label conversion for MLMCID dataset. Table 14 shows some examples of the intent classes predicted with their respective confidence for PNM. Dataset MIX_SNIPS (fine) MIX_SNIPS (coarse) BANKING (fine) BANKING (coarse) CLINC (fine) CLINC (coarse) Intent 1 (%) 81.2 85.4 79.3 83.3 80.7 81.9 Intent 2 (%) 73.8 74.4 60.0 68.9 69.2 71.7 Intent 3 (%) Average (%) 60.3 62.3 56.3 59.6 55.4 58. 71.7 74.0 65.2 70.6 68.4 70.6 Table 10: 3-Intent Detection by Roberta based PNM our approach is scalable and applicable in resource constrained environments. It is also seen that in case of larger numbers of intents with the introduction of additional pointer networks - the system is scalable and does not require large computational costs. So the framework can be useful in real time processing for large scale systems. Though it is also to be noted that most of the datasets are composed with two intents even in the real life sentences. A.4 Single Intent Detection We perform additional experiments on three datasets with various intent sizes - SNIPS (small), BANKING (medium) and CLINC (large) and detect the single-intent text using RoBERTa based pointer network architecture - which is shown in the following table (in %). It shows the effectiveness of our model for coarse (c) and fine (f).

Dataset
SNIPS
BANKING
CLINC
===========================

coarse (%) fine (%) 90.0 83.9 80.0 85.9 81.8 75.3 Table 11: Single Intent Detection

B Experimental Settings
=======================

Our experiments are conducted on two Tesla P100 GPUs with 16 GB RAM, 6 Gbps clock cycle, GDDR5 memory and one 80GB A100 GPU, 210MHz clock cycle, 2*960 GB SSD with 5 epochs. We use Adam optimizer with learning rate: 105 with cross-entropy as the loss function, weight decay: 105 and dropout rate of 0.5 is applied on the embeddings to avoid overfitting for all experiments. All methods took less than 120 GPU minutes (except Llama2: 4-5 hrs) for fine tuning and 2 hrs for inference. All the hyperparameters are tuned on the dev set. We have used NLTK, Spacy, Scikit-learn, openai(version=0.28), huggingface_hub, torch and transformers python packages for all experiments and evaluation. Figure 4: Examples in MLMCID Dataset Dataset Coarse Label Fine Labels Combined Sr. No. 1. SNIPS 2. BANKING Traffic_update App_Service Location_service GetWeather Cancelled_ transfer Card_problem exchange_rate_query General_Enquiry Top_up Account_opening transaction_problem Card_service_enquiry Identity_verification Service_request Malpractice Payment_inconsistency verify_top_up, transfer_fee_charged, lost_or_stolen_phone, fiat_currency_support, top_up_by_cash_or_cheque, cash_withdrawal_charge, card_swallowed, repending_card_payment, lost_or_stolen_card, card_payment_fee_charged, ComparePlaces, GetPlaceDetails, ShareCurrentLocation, SearchPlace, GetDirections RequestRide, BookRestaurant GetTrafficInformation, ShareETA GetWeather cancel_transfer, beneficiary_not_allowed card_linking, card_arrival, activate_my_card, declined_card_payment, verted_card_payment?, card_not_working, pin_blocked, card_payment_not_recognised, card_acceptance exchange_rate, card_payment_wrong_exchange_rate, wrong_exchange_rate_for_cash_withdrawal extra_charge_on_statement, card_delivery_estimate, pending_cash_withdrawal, automatic_top_up, topexchange_via_app, ping_up_by_card, transatm_support, receivfer_timing, exing_money, change_charge, apple_pay_or_google_pay top_up_by_bank_transfer_charge, pending_top_up, top_up_limits, top_up_reverted, top_up_failed age_limit contactless_not_working, transwrong_amount_of_cash_received, fer_not_received_by_recipient, balance_not_updated_after_cheque_or_cash_deposit, pending_transfer, declined_cash_withdrawal, declined_transfer, transaction_charged_twice, failed_transfer visa_or_mastercard, disposable_card_limits, getting_virtual_card, supported_cards_and_currencies, virtual_card_not_working, getting_spare_card, card_about_to_expire, top_up_by_card_charge, country_support unable_to_verify_identity, why_verify_identity, verify_my_identity order_physical_card, get_physical_card, change_pin, verify_source_of_funds, get_disposable_virtual_card compromised_card, cash_withdrawal_not_ recognised direct_debit_payment_not_recognised, Refund_not_showing_up, ance_not_updated_after_bank_transfer edit_personal_details, passcode_forgotten, terminate_account, request_refund, transfer_into_account, balTable 12: Fine to Coarse Labels Conversion Examples for SNIPS and BANKING Dataset Dataset Coarse Label Fine Labels Combined Sr. No. 3. CLINC health_suggestion Restaurant account 4. Facebook Multilingual Dialog Dataset 5. HWU64 communication Reminder banking_enquiry change_alarm_content reminder_service sunset_sunrise get_weather read alarm content alarm audio iot calendar play general datetime takeaway news music weather qa social recommendation cooking email transport lists recredit_limit, transactions, credit_score, restaurestaureport_lost_card, meal_suggestion, reminder_update, nutrition_info, oil_change_how, calories restaurant_reviews, accept_reservations, rant_reservation, rant_suggestion balance, redeem_rewards, rewards_balance, bill_balance, bill_due, spending_history, damaged_card, pin_change, replacement_card_duration, new_card, direct_deposit, credit_limit_change, payday, application_status, pto_request, pto_balance, pto_request_status, pto_used make_call, text remind_update, remind, minder, meeting_schedule account_blocked, freeze_account, interest_rate cancel alarm, modify alarm, set alarm, snooze alarm cancel reminder, set reminder, show reminders weather check sunrise, weather check sunset weather find show alarm, time left on alarm set, remove, query audio_volume_mute, audio_volume_down, audio_volume_other, audio_volume_up iot_hue_lightchange, iot_hue_lightoff, iot_hue_lighton, iot_hue_lightdim, iot_cleaning, iot_hue_lightup, iot_wemo_on, iot_coffee, iot_wemo_off calendar_query, calendar_set, calendar_remove play_music, play_radio, play_podcasts, play_game general_query, general_joke, general_greet, general_negate, general_dontcare, general_repeat, general_affirm, general_confirm, general_explain, general_praise datetime_query, datetime_convert takeaway_query, takeaway_order news_query music_likeness, music_query, music_settings, music_dislikeness weather_query qa_stock, qa_factoid, qa_definition, qa_maths, qa_currency social_post, social_query recommendation_locations, tion_events, recommendation_movies cooking_recipe, cooking_query email_sendemail, email_query, email_querycontact, email_addcontact transport_query, transport_ticket, transport_traffic, transport_taxi lists_query, lists_remove, lists_createoradd general_commandstop, play_audiobook, recommendaTable 13: Fine to Coarse Labels Conversion Examples for Facebook and CLINC Dataset Text Predicted True Label Find store near Sias place where can buy champagne and find me brunch spot in Lower Manhattan (SNIPS) Book cab, is there traffic on the US 50 portion Im going to take to go to my client meeting? (SNIPS) What will the weather be like at my Airbnb this week end? Is there parking at my hotel? (SNIPS) Can you make reservation at lebanese restaurant nearby, for lunch, party of 5? Hows the traffic from here? (SNIPS) set alarm,remind me to pay electric monday (FACEBOOK) is it going to snow in chicago tomorrow, any chance of rain today? (FACEBOOK) how hot will it be, how long will it rain tomorrow (FACEBOOK) what is the average wait for transfers, Im still waiting on my identity verification.(BANKING) My card is due to expire,Why cant get cash out (BANKING) have new email. am in the EU. Can get one of your cards? (BANKING) Can other people top up my account? where did my funds come from? (BANKING) Can you tell me my shopping list items, please? Is tomato on my shopping list? (CLINC) Change the name of your system. Your name from this point forward is george. (CLINC) use my phone and connect please,tell me something thatll make me laugh(CLINC) will there be traffic on the way to walmart,can you help me with rental car(CLINC) Location_Service (Primary), App_Service (Non-Primary) Location_Service, Location_Service App_Service (Primary), Traffic_update (Non-Primary) Traffic_update, App_Service Remarks about prediction Non-Primary Label predicted wrongly Wrong Predictions - swapped groundtruth labels GetWeather (Primary), Location_Service (Non-Primary) GetWeather, tion_Service LocaCorrect Predictions App_Service (Primary), Traffic_update (Non-Primary) App_Service, tion_Service LocaNon-Primary label wrongly predicted set alarm (Primary), set reminder (Non-Primary) set alarm, set reminder Correct Predictions weather find (Primary), set reminder (Non-Primary) weather find, weather find Non-Primary label wrongly predicted weather find (Primary), set reminder (Non-Primary) weather find, weather find Non-Primary label wrongly predicted General_Enquiry Identity_verification Primary) (Primary), (NonGeneral_Enquiry, tity_verification IdenCorrect Predictions card_about_to_expire, declined_cash_withdrawal deCorrect Predictions card_about_to_expire (Primary), clined_cash_withdrawal (Non-Primary) Card_service_enquiry mary), (Non-Primary) (PriGeneral_Enquiry Service_request, Card_service_enquiry verify_source_of_funds (Primary), topping_up_by_card (Non-Primary) shopping_list (Primary), account (Non-Primary) topping_up_by_card, verify_source_of_funds shopping_list, ping_list shopchange_ai_name change_user_name Primary) (Primary), (Nonchange_ai_name, change_ai_name Non-Primary label wrongly predicted sync_device tell_joke (Non-Primary) (Primary), sync_device, tell_joke Correct Predictions traffic (Primary), car_rental (Non-Primary) traffic, car_rental Correct Predictions Incorrect Predictions; Primary Predicted is same as Intent the Non-Primary Ground Truth Label Wrong Predictions - swapped groundtruth labels Non-Primary label wrongly predicted Table 14: Prediction of best-performing models and Respective Confidence

