Title: Cut Your Losses in Large-Vocabulary Language Models

Authors: Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, Philipp Krähenbühl


================================================================================

Abstract
========

As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.

Start
=====

4 2 0 2 3 1 ] . [ 1 9 0 0 9 0 . 1 1 4 2 : r a

Preprint
========

CUT YOUR LOSSES IN LARGE-VOCABULARY LANGUAGE MODELS Erik Wijmans Brody Huval Alexander Hertzberg Vladlen Koltun Philipp Krahenb uhl Apple

ABSTRACT
========

As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence. https://github.com/apple/ml-cross-entropy

INTRODUCTION
============

Progress in large language models (LLMs) has been fueled in part by an increase in parameter count, context length, and vocabulary size (the number of tokens that can be used to represent the input). As LLMs grew, so did the associated infrastructure. Large mini-batch gradient descent (Goyal et al., 2017) combined with data-parallelism (Hillis & Steele, 1986) enabled the harnessing of increasing computational power. ZeRO (Rajbhandari et al., 2020) broke the dependence between the number of GPUs and the memory used for model parameters, gradients, and optimizer state. Activation checkpointing (Chen et al., 2016) reduced the amount of memory used for activations, supporting the development of deeper models. FlashAttention (Dao et al., 2022) reduced the memory used in selfattention from O(N 2) to O(N ), thereby supporting longer context windows. These improvements gradually shifted the memory consumption of LLM training to one single layer the cross-entropy loss, whose memory footprint grows with the product of vocabulary size and number of tokens per batch. The cross-entropy loss is responsible for up to 90% of the memory footprint of modern LLM training (see Fig. 1a). The problem grows only more acute with time, since even the largest contemporary vocabularies (e.g., 256K tokens) may benefit from further expansion (Tao et al., 2024). We propose cross-entropy implementation, Cut Cross-Entropy (CCE), that has negligible memory footprint and scales to arbitrarily large vocabularies. Our key insight is that computation of the loss and its gradient only depends on single log-probability, that of the ground-truth label. With an arithmetic reformulation, we decompose the cross-entropy loss into an index matrix multiplication over single ground-truth label and log-sum-exp operation over all vocabulary entries for each token. Each operation has small and well-defined inputs the network embeddings and classifier Corresponding author: ewijmans@apple.com

Preprint
========

(a) Regular cross-entropy (b) Cut cross-entropy (ours) Figure 1: Memory use and maximum attainable batch size (in millions of tokens) for variety of frontier models on 16-GPU (80 GB each) fully-sharded data-parallel setup (Rajbhandari et al., 2020) with activation checkpointing (Chen et al., 2016) and mixed-precision 16-bit (fp16/bf16) AdamW optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2019). For each model, we break its memory use down into weights and optimizer states, activation checkpoints, and the log-probabilities computed by the cross-entropy loss layer. Our Cut Cross-Entropy (CCE) enables increasing the batch size by 1.5x (Llama 2 13B) to 10x (GPT 2, Gemma 2 2B), with no sacrifice in speed or convergence. Exact values in Table A3. matrix and single scalar output per token. Both operations do, however, rely on large intermediate logit matrix that computes the score for each token and potential vocabulary entry. We show that there is no need to materialize this logit matrix in GPU memory. Instead, we compute logits as needed in SRAM in series of custom CUDA kernels. The result is cross-entropy computation that has negligible memory footprint, with no detrimental effect on latency or convergence. See Fig. 1b for breakdown of memory savings and consequent batch size increases afforded by CCE.

2 RELATED WORK
==============

Attention mechanisms. The effectiveness of transformers (Vaswani et al., 2017) in modeling language has drawn attention to their compute and memory requirements. Multiple works have proposed alternatives to scaled dot-product attention that reduce transformers computation and memory (Kitaev et al., 2020; Wang et al., 2020; Choromanski et al., 2021). Other model classes, such as structured state-space models (Gu et al., 2022; Gu & Dao, 2023), have also shown promising results. We study different part of the model its classifier head that is not considered in these works. Attention implementations. In addition to alternative attention mechanisms, the community has also tackled the daunting memory consumption of LLMs via efficient implementations. Rabe & Staats (2021) developed self-attention implementation that makes use of chunking. Chen et al. (2023) proposed an implementation that broke the operation into two stages, reduction and matrix multiplication. This makes efficient use of GPU memory and registers but requires recomputation in the forward pass. FlashAttention (Dao et al., 2022) uses an online softmax (Milakov & Gimelshein, 2018) and, like CCE, materializes blocks of the 2-sized self-attention matrix in on-chip SRAM rather than slower global DRAM. This is one of the key ideas that CCE builds on to develop memory-efficient cross-entropy formulation. Vocabulary reduction. One way to minimize the amount of memory used by the log-probabilities over the tokens is to reduce the number of active tokens in the vocabulary. Grave et al. (2017) proposed to use vocabulary with hierarchical structure, thereby requiring the log-probabilities for only subset of the vocabulary at any given time. Yu et al. (2023) explore tokenization-free byte-level models that operate on dramatically smaller vocabularies. Efficient cross-entropy implementations. number of recent implementations use chunking to reduce the memory usage of the cross-entropy layer. Yet chunking induces trade-off. Memory

Preprint
========

footprint is minimized when the number of chunks is high, but latency is minimized when the number of chunks is low. CCE utilizes only on-chip SRAM and minimizes both memory footprint and latency. Liger Kernels (Hsu et al., 2024) make efficient use of the GPU via chunking and by computing the loss+gradient simultaneously. The latter requires that any transform applied to the loss (such as masking) is implemented in the kernel itself. CCE has separate forward and backward stages, enabling user-defined transformations on the loss.

3 PRELIMINARIES
===============

Let (x) = (cid:81)N i=1 (xi x1 . . . xi1) be Large Language Model (LLM) over vocabulary . The LLM parameterizes an autoregressive distribution over all possible tokens xi given the preceding 1 tokens. Specifically, this distribution is the combination of backbone network : x1 . . . xi1 RD and linear classifier RDV : (xi x1 . . . xi1) = softmaxxi(Cf (x1 . . . xi1)), softmaxk(v) = exp(vk) exp(vj) (cid:80) . (1) (2) The backbone network (x1, . . . , xi1) RD encodes token sequence in the D-dimensional feature vector. The linear classifier RDV projects the embedding into an output space of the vocabulary . The softmaxk(v) produces the probability over all vocabulary entries from the unnormalized log probabilities (logits) produced by Cf (x1 . . . xi1). 3.1 VOCABULARY LLMs represent their input (and output) as set of tokens in vocabulary . The vocabulary is typically constructed by method such as Byte Pair Encoding (BPE) (Gage, 1994). BPE initializes the vocabulary with all valid byte sequences from standard text encoding, such as utf-8. Then, over large corpus of text, BPE finds the most frequent pair of tokens and creates new token that represents this pair. This continues iteratively until the maximum number of tokens is reached. Large vocabularies enable single token to represent multiple characters. This reduces the length of both input and output sequences, compresses larger and more diverse documents into shorter context windows, thus improving the models comprehension while reducing computational demands. 3.2 INFERENCE AND TRAINING Even with large vocabulary, sampling from an LLM is memory-efficient at inference time. Specifically, the LLM produces one token at time, computing (xix1 . . . xi1) and sampling from this distribution (Kwon et al., 2023). Because the distribution over the vocabulary is only needed for single token at time, the memory footprint is independent of sequence length. At training time, the LLM maximizes the log-likelihood of the next token: ℓ(ˆx) = (cid:88) i=1 log (ˆxiˆx1, . . . , ˆxi1). (3) Due to the structure of most backbones (Vaswani et al., 2017; Gu et al., 2022; Gu & Dao, 2023), (x1), (x1, x2), . . . , (x1, . . . , xN ) is efficiently computed in parallel. However, activations for non-linear layers have to be saved for the backward pass, consuming significant memory. Most LLM training frameworks make use of aggressive activation checkpointing (Chen et al., 2016), sharding (Rajbhandari et al., 2020), and specialized attention implementations (Dao et al., 2022) to keep this memory footprint manageable. With the aforementioned optimizations, the final (cross-entropy loss) layer of the LLM becomes by far the biggest memory hog. For large vocabularies, the final cross-entropy layer accounts for the majority of the models memory footprint at training time (Fig. 1a). For example, the logprobabilities materialized by the cross-entropy layer account for 40% of the memory consumption of Phi 3.5 (Mini) (Abdin et al., 2024) (V = 32,064), 65% of the memory consumption of Llama

Preprint
========

(a) Indexed matmul (b) Linear-log-sum-exp, (forward) forward pass (c) Linear-log-sum-exp, backward pass Figure 2: Access patterns and computation of blockwise (a) indexed matrix multiplication, (b) linear-log-sum-exp forward pass, and (c) linear-log-sum-exp backward pass. See Algorithms 1 to 3 for the corresponding algorithms. 3 (8B) (Dubey et al., 2024) (V = 128,000), and 89% of the memory consumption of Gemma 2 (2B) (Rivi`ere et al., 2024) (V = 256,128). In fact, the log-probabilities of Gemma 2 (2B) for single sequence with length = 80,000 use the entire available memory of an 80 GB H100 GPU. (The sequence length is factor due to the use of teacher forcing for parallelism.) We show that reformulation of the training objective leads to an implementation that has negligible memory consumption above what is required to store the loss and the gradient.

4 CUT CROSS-ENTROPY
===================

Consider the cross-entropy loss ℓi over single prediction of the next token (xix1 . . . xi1): (cid:17) (cid:16) ℓi(x) = log softmaxxi CEi = xi Ei log (cid:88) exp (cid:0)C Ei (cid:1) . Here the first term is vector product over D-dimensional embeddings Ei = (x1 . . . xi1) and classifier C. The second term is log-sum-exp operation and is independent of the next token xi. During training, we optimize all next-token predictions ℓ = [ℓ1 . . . ℓN ] jointly using teacher forcing: (cid:16) (cid:17) CE ℓ = log (cid:88) exp(C E), (4) (cid:16) (cid:17) CE = (cid:2)C x1 E1 . . . xN (cid:3). The first term in Equation (4) is where = [E1 . . . EN ] and combination of an indexing operation and matrix multiplication. It has efficient forward and backward passes, in terms of both compute and memory, as described in Section 4.1. The second term in Equation (4) is joint log-sum-exp and matrix multiplication operation. Section 4.2 describes how to compute the forward pass of this linear-log-sum-exp operation efficiently using joint matrix multiplication and reduction kernel. Section 4.3 describes how to compute its backward pass efficiently by taking advantage of the sparsity of the gradient over large vocabulary. Putting all the pieces together yields memory-efficient low-latency cross-entropy loss. EN 4.1 MEMORY-EFFICIENT INDEXED MATRIX MULTIPLICATION naive computation of indexed matrix multiplication involves either explicit computation of the logits CE with an O(N ) memory cost, or indexing into the classifier Cx = [C x1 . . . xN ] with an O(N D) memory cost. Our implementation fuses the classifier indexing Cx with the consecutive dot product between columns xi and Ei in single CUDA/Triton kernel (Tillet et al., 2019). Our kernel retrieves the value xi, the xi-th column from C, and the i-th column from E, and stores them in on-chip shared memory (SRAM). It then performs dot product between xi and Ei and writes

Preprint
========

the result into global memory. The kernel uses only on-chip SRAM throughout and does not allocate any GPU memory. For efficiency, we perform all operations blockwise to make the best use of GPU cache structure. Algorithm 1 and Fig. 2a summarize the computation and access patterns. Algorithm 1 Memory-efficient indexed matrix multiplication Inputs: Outputs: RDN , RDV , RN . Block sizes NB and DB. = (CE)x RN for blocks En, xn do on = 0NB for blocks En,d do = Cxn,d on += En,d Divide and into blocks of size NB and NB, respectively Zero vector of size NB in on-chip SRAM Divide En into blocks of size DB NB Indexed load into on-chip SRAM Column-wide dot product end for write on end for From on-chip SRAM to main GPU memory It is possible to compute the backward pass using similar kernel. We found it to be easier and more memory-efficient to merge the backward implementation with the backward pass of the linear-logsum-exp operator. The two operations share much of the computation and memory access pattern. 4.2 MEMORY-EFFICIENT LINEAR-LOG-SUM-EXP, FORWARD PASS Implementing serial memory-efficient linear-log-sum-exp is fairly straightforward: use triple for-loop. The innermost loop computes the dot product between and En for the i-th token and the n-th batch element. The middle loop iterates over the vocabulary, updating the log-sum-exp along the way. Finally, the outermost loop iterates over all batch elements. Parallelizing over the outermost loop is trivial and would expose enough work to saturate the CPU due to the number of tokens in training batches (commonly in the thousands). Parallelization that exposes enough work to saturate the GPU is more challenging. Let us first examine how efficient matrix multiplication between the batch of model output embeddings RDN and the classifier RDV is implemented on modern GPUs (Kerr et al., 2017). common method is to first divide the output = CE RV into set of blocks of size MB NB. Independent CUDA blocks retrieve the corresponding parts En of with size NB and blocks Cm of with size MB, and perform the inner product Onm = mEn along the dimension. Due to limited on-chip SRAM, most implementations use for-loop for large values of D. They loop over smaller size DB NB and DB MB blocks and accumulate Onm = (cid:80) mdEnd in SRAM. Each CUDA block then writes Onm back into global memory. This method exposes enough work to the GPU and makes efficient use of SRAM and L2 cache. To produce log-sum-exp(CE), we use the same blocking and parallelization strategy as matrix multiplication. Each block first computes matrix multiplication, then the log-sum-exp along the vocabulary dimension for its block, and finally updates LSE with its result. Note that multiple CUDA blocks are now all writing to the same location of LSE. This includes blocks in the same input range but different vocabulary ranges m. We use spin-lock on an atomic operation in global memory to synchronize the updates by different CUDA blocks as this is simple to implement in our Triton framework and incurs little overhead. Alternative methods, such as an atomic compare-and-swap loop, may perform better when implementing in CUDA directly. Algorithm 2 and Fig. 2b summarize the computation and access patterns. 4.3 MEMORY-EFFICIENT LINEAR-LOG-SUM-EXP, BACKWARD PASS The backward pass needs to efficiently compute two gradient updates: and = λ E = λ exp(CE) (cid:88) log (cid:88) log exp(CE)

Preprint
========

Algorithm 2 Memory-efficient linear-log-sum-exp, forward pass Inputs: RDN and RDV . Block sizes NB, MB, and DB. Outputs: LSE = log (cid:80) exp(C E) RN LSE = vector of size in main GPU memory for all pairs of blocks En, Cm do Divide and into blocks of size NB and MB Zero matrix of size MB NB in on-chip SRAM Divide En and Cm into blocks of DB NB and DB MB Blockwise matrix multiplication Anm += Anm = 0MB NB for blocks En,d, Cm,d do m,d En,d end for LSEnm = log (cid:80) exp(A LSEn = log(exp(LSEn) + exp(LSEnm)) nm) Numerically stable implementation with max Locking thread-safe log-add-exp end for for backpropagated gradient λ = LSE. Formally, the gradient is defined as = (S LSE) and = (S LSE) where = softmax(CE) and refers to the row-by-row elementwise multiplication of the softmax and the gradient LSE: ˆS = LSE. Computationally, the backward pass is double matrix multiplication CE and ˆSC or ˆSE with intermediate matrices and ˆS that do not fit into GPU memory and undergo non-linear operation. We take similar approach to the forward pass, recomputing the matrix CE implicitly in the GPUs shared memory. For the backward pass, we do not need to compute the normalization constant of the softmax, since = softmax(CE) = exp(CE LSE). This allows us to reuse the global synchronization of the forward pass, and compute efficiently in parallel. We implement the second matrix multiplication in the main memory of the GPU, as blockwise implementation would require storing or synchronizing S. Algorithm 3 and Fig. 2c summarize the computation and access patterns. naive implementation of this algorithm requires zero additional memory but is slow due to repeated global memory load and store operations. We use two techniques to improve the memory access pattern: gradient filtering and vocabulary sorting. Gradient filtering. By definition, the softmax sums to one over the vocabulary dimension. If stored in bfloat16 with 7-bit fraction, any value below ε = 212 will likely be ignored due to truncation in the summation or rounding in the normalization.1 This has profound implications for the softmax matrix S: For any column, at most 1 ε = 4096 entries have non-trivial values and contribute to the gradient computation. All other values are either rounded to zero or truncated. In practice, the sparsity of the softmax matrix is much higher: empirically, in frontier models we evaluate, less than 0.02% of elements are non-zero. Furthermore, the sparsity of the softmax matrix grows as vocabulary size increases. In Algorithm 3, we take advantage of this sparsity and skip gradient computation for any block whose corresponding softmax matrix Snm has only negligible elements. We chose the threshold ε = 212 to be the smallest bfloat16 value that is not truncated. In practice, this leads to 3.5x speedup without loss of precision in any gradient computation. See Section 5 for detailed analysis. The efficiency of gradient filtering is directly related to the block-level sparsity of the softmax matrix. We cannot control the overall sparsity pattern without changing the output. However, we can change the order of the vocabulary to create denser local blocks for more common tokens. Vocabulary sorting. Ideally the vocabulary would be ordered such that all tokens with non-trivial gradients would be contiguously located. This reduces the amount of computation wasted by partially populated blocks ideally blocks would either be entirely empty (and thus skipped) or entirely populated. We heuristically group the non-trivial gradients by ordering the tokens by their average logit. Specifically, during the forward pass (described in Section 4.2) we compute the average logit 1The 5 extra bits above the fractional size (7) account for rounding rules, and the consideration that small but not tiny values will likely not get truncated due to the blocking strategies used to compute sum.

Preprint
========

Algorithm 3 Memory-efficient linear-log-sum-exp, backward pass Inputs: RDN , RDV , LSE RN , and LSE RN . Block sizes NB, MB, and DB. Accuracy threshold ε. Outputs: RDN , RDV for all pairs of blocks En, Cm do Divide and into blocks of size NB and MB Zero matrix of size MB NB in on-chip SRAM Divide En and Cm into blocks of DB NB and DB MB Blockwise matrix multiplication Anm = 0MB NB for blocks En,d, Cm,d do m,d En,d Anm += end for Snm = exp(Anm LSEn) if all(Snm < ε) then skip end if for blocks En,d, Cm,d do end for end for Skip computation if below desired numerical precision Compute the softmax C n,d += (Snm LSEn) Cm,d m,d += (Snm LSEn) En,d Divide En and Cm into blocks of DB NB and DB MB Locking thread-safe gradient update Locking thread-safe gradient update per token using an atomic addition. For the backward pass, we divide the vocabulary dimension into blocks with similar average logit instead of arbitrarily. This requires temporary buffer of size O(V ), about 1 MB for the largest vocabularies in contemporary LLMs (Rivi`ere et al., 2024). Putting all the pieces together, we arrive at forward and backward implementations of cross-entropy that have negligible incremental memory footprint without sacrificing speed. Note that in practice, we compute the backward pass of the indexed matrix multiplication in combination with log-sumexp (Algorithm 3). We subtract 1 from the softmax Si,xi for all ground-truth tokens x1 . . . xN .

5 ANALYSIS
==========

5.1 RUNTIME AND MEMORY First we examine the runtime and memory of various implementations of the cross-entropy loss log softmaxxi(CE). We consider batch of 8,192 tokens with vocabulary size of 256,000 and hidden dimension 2,304. This corresponds to Gemma 2 (2B) (Rivi`ere et al., 2024). We use the Alpaca dataset (Taori et al., 2023) for inputs and labels and Gemma 2 (2B) Instruct weights to compute and for C. The analysis is summarized in Table 1. The baseline implements the loss directly in PyTorch (Paszke et al., 2019). This is the default in popular frameworks such as Torch Tune (Torch Tune Team, 2024) and Transformers (Wolf et al., 2019). This method has reasonable throughput but peak memory usage of 28,000 MB of GPU memory to compute the loss+gradient (Table 1 row 5). Due to memory fragmentation, just computing the loss+gradient for the classifier head requires an 80 GB GPU. torch.compile (Ansel et al., 2024) is able to reduce memory usage by 43% and computation time by 33%, demonstrating the effectiveness of kernel fusion (Table 1 row 4 vs. 5). Torch Tune (Torch Tune Team, 2024) includes method to compute the cross-entropy loss that divides the computation into chunks and uses torch.compile to save memory. This reduces memory consumption by 65% vs. Baseline and by 40% vs. torch.compile (to 9,631 MB, see Table 1 row 3 vs. 4 and 5). Liger Kernels (Hsu et al., 2024) provide memory-efficient implementation of the cross-entropy loss that, like Torch Tune, makes uses of chunked computation to reduce peak memory usage. While very effective at reducing the memory footprint, using 95% less memory than Baseline, it has detrimental effect on latency, more than doubling the wall-clock time for the computation (Table 1, row 2 vs. 4). The memory usage of CCE grows with O(N + ), as opposed to O(N ) for Baseline, torch.compile, and Torch Tune, and O(N D) for Liger Kernels. In practice, CCE has negligible memory footprint regardless of vocabulary size or sequence length.

Preprint
========

Method Lower bound Loss Gradient Loss+Gradient Memory Time Memory Time Memory Time 0.004 MB 1,161 MB 1,161 MB 1) CCE (Ours) 2) Liger Kernels (Hsu et al., 2024)2 3) Torch Tune Team (2024) (8 chunks) 4) torch.compile 5) Baseline 1 MB 43 ms 1,163 MB 95 ms 1,164 MB 135 ms 1,474 MB 303 ms 1,474 MB 302 ms 8,000 MB 55 ms 9,631 MB 170 ms 1,630 MB 115 ms 4,000 MB 49 ms 12,000 MB 92 ms 16,000 MB 143 ms 24,000 MB 82 ms 16,000 MB 121 ms 28,000 MB 207 ms 6) CCE (No Vocab Sorting) 7) CCE (No Grad. Filter) 0.09 MB 42 ms 0.09 MB 42 ms 1,162 MB 104 ms 1,162 MB 324 ms 1,162 MB 143 ms 1,162 MB 362 ms Table 1: Peak memory footprint and time to compute the loss, its gradient, and their combination. Note that intermediate buffers can often (but not always) be reused between the loss and gradient computation, resulting in lower peak memory consumption than the sum of the parts. Batch of 8,192 tokens with vocabulary size of 256,000 and hidden dimension 2304. Embedding and classifier matrix taken during Gemma 2 (2B) training on Alpaca. Measured on an A100-SXM4 GPU with 80 GB of RAM, PyTorch 2.4.1, CUDA 12.4, rounded to closest MB. Some numbers are multiples of 1,000 due to dimensions chosen and PyTorchs allocation strategy. Lower bound is the amount of memory required for the output buffer(s), i.e., and C, this is the lower bound for the memory footprint of any method. Compared to the fastest method, torch.compile, CCE computes the loss slightly faster (5%, 4ms, Table 1 row 1 vs. 4). This is because CCE does not write all the logits to global memory. CCE also computes the loss+gradient in slightly faster (6%, 8 ms). This is because CCE is able to make use of the inherit sparsity of the gradient to skip computation. The performance of CCE is enabled by both gradient filtering and vocabulary sorting. Without vocabulary sorting CCE takes 15% (23 ms) longer (Table 1 row 1 vs. 6) and without gradient filtering it is 3.4x (356 ms) longer (row 1 vs. 7). In Appendix A, we demonstrate that CCE (and other methods) can be made up to 3 times faster by removing tokens that are ignored from the loss computation. In Appendix we benchmark with more models. We find that as the ratio of vocabulary size (V ) to hidden size (D) decreases, CCE begins to be overtaken in computation time for the Loss+Gradient, but continues to save substantial amount of memory. 5.2 GRADIENT FILTERING Figure 3: Average probability for the ith most likely token, log-log plot. The probabilities very quickly vanish below numerical precision. Fig. 3 shows the sorted softmax probability of vocabulary entries. Note that the probabilities vanish very quickly and, for the top 105 most likely tokens, there is linear relationship between log rank and log probability. Second, by the 50th most likely token, the probability has fallen bellow our threshold for gradient filtering. This explains why we are able to filter so many values from the gradient computation without affecting the result. At these sparsity levels, most blocks of the softmax matrix are empty. 5.3 TRAINING STABILITY Fig. 4 demonstrates the training stability of CCE. We fine-tune Llama 3 8B Instruct (Dubey et al., 2024), Phi 3.5 Mini Instruct (Abdin et al., 2024), Gemma 2 2B Instruct (Rivi`ere et al., 2024), and 2The gradient and loss are computed simultaneously, not in separate forward/backward passes.

Preprint
========

(a) Gemma 2 2B (b) Phi 3.5 Mini (c) Llama 3 8B (d) Mistral Nemo Figure 4: Training loss curves for four models on the Alpaca dataset (Taori et al., 2023). The loss curves for CCE and torch.compile are nearly indistinguishable, showing that the gradient filtering in CCE does not impair convergence. Results averaged over 5 seeds. Mistral NeMo (Mistral AI Team, 2024) on the Alpaca Dataset (Taori et al., 2023) using CCE and torch.compile as the control. As shown in the figure, CCE and torch.compile have indistinguishable loss curves, demonstrating that the gradient filtering in CCE does not impair convergence.

6 DISCUSSION
============

As vocabulary size has grown in language models, so has the memory footprint of The memory used by this one layer dominates the training-time memory the loss layer. language models. We described CCE, an algorithm to compute footprint of many recent ℓi = log softmaxi(C (x1 . . . xi1)) and its gradient with negligible memory footprint. Beyond the immediate impact on compact large-vocabulary LLMs, as illustrated in Fig. 1, we expect that CCE may prove beneficial for training very large models. Specifically, very large models are trained with techniques such as pipeline parallelism (Huang et al., 2019; Narayanan et al., 2019). Pipeline parallelism works best when all stages are equally balanced in computation load. Achieving this balance is easiest when all blocks in the network have similar memory-to-computation ratios. The classification head is currently an outlier, with disproportionately high memory-tocomputation ratio. CCE may enable better pipeline balancing or reducing the number of stages. We implemented CCE using Triton (Tillet et al., 2019). Triton creates efficient GPU kernels and enables rapid experimentation but has some limitations in control flow. Specifically, the control flow must be specified at the block level and therefore our thread-safe log-add-exp and gradient filtering are constrained to operate at the block level as well. We expect that implementing CCE in CUDA may bring further performance gains because control flow could be performed at finer-grained levels. It could also be interesting to extend CCE to other classification problems where the number of classes is large, such as image classification and contrastive learning.

REFERENCES
==========

Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, et al. Phi-3 technical

Preprint
========

report: highly capable language model locally on your phone, 2024. URL https://arxiv. org/abs/2404.14219. Jason Ansel, Edward Z. Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2024. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016. URL http://arxiv.org/abs/1604.06174. Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann. Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations. In Conference on Computer Vision and Pattern Recognition, Workshops, 2023. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Neural Information Processing Systems, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Philip Gage. new algorithm for data compression. The Users Journal, 12(2):2338, 1994. Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour, 2017. URL http://arxiv.org/abs/1706.02677. Edouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, and Herve Jegou. Efficient softmax approximation for gpus. In International Conference on Machine Learning, 2017. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023. URL https://arxiv.org/abs/2312.00752. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. W. Daniel Hillis and Guy L. Steele. Data parallel algorithms. Commun. ACM, 29(12):11701183, 1986. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, and Siyu Zhu. LigerKernel: Efficient Triton kernels for LLM training, 2024. URL https://github.com/linkedin/ Liger-Kernel. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. In Neural Information Processing Systems, 2019. Andrew Kerr, Duane Merrill, ear algebra in CUDA C++, cutlass-linear-algebra-cuda/. 2017. Julien Demouth, and John Tran. linURL https://developer.nvidia.com/blog/ CUTLASS: Fast Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015.

Preprint
========

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Symposium on Operating Systems Principles, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax, 2018. URL http://arxiv.org/abs/1805.02867. Mistral AI Team. Mistral NeMo, 2024. URL https://mistral.ai/news/mistral-nemo/. Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for DNN training. In ACM Symposium on Operating Systems Principles, 2019. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019. Markus N. Rabe and Charles Staats. Self-attention does not need O(n2) memory, 2021. URL https://arxiv.org/abs/2112.05682. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations In International Conference for High Performance toward training trillion parameter models. Computing, Networking, Storage and Analysis, 2020. Morgane Rivi`ere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, Johan Ferret, et al. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv.org/abs/ 2408.00118. Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies, 2024. URL https://arxiv.org/abs/2407.13623. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023. URL https://github.com/tatsu-lab/stanford alpaca. Philippe Tillet, Hsiang-Tsung Kung, and David D. Cox. Triton: An intermediate language and compiler for tiled neural network computations. In ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, 2019. Torch Tune Team. torchtune, 2024. URL https://github.com/pytorch/torchtune. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020. URL https://arxiv.org/abs/2006.04768. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingfaces transformers: State-of-the-art natural language processing, 2019. Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. MEGABYTE: Predicting million-byte sequences with multiscale transformers. In Neural Information Processing Systems, 2023.

Preprint
========

Method Lower bound Loss Gradient Loss+Gradient Memory Time Memory Time Memory Time 0.004 MB 1,161 MB 1,161 MB 1) CCE (Ours) 2) Liger Kernels (Hsu et al., 2024)3 3) Torch Tune Team (2024) (8 chunks) 2,700 MB 22 ms 2,709 MB 52 ms 4) torch.compile 1,356 MB 18 ms 4,032 MB 32 ms 8,076 MB 28 ms 5,376 MB 41 ms 5) Baseline 1 MB 15 ms 1,163 MB 40 ms 1,164 MB 52 ms 1,228 MB 315 ms 5,410 MB 73 ms 5,388 MB 50 ms 9,420 MB 70 ms 1,230 MB 313 ms 6) CCE (No Vocab Sorting) 7) CCE (No Grad. Filter) 0.05 MB 15 ms 1,162 MB 46 ms 0.05 MB 15 ms 1,162 MB 156 ms 1,162 MB 58 ms 1,162 MB 170 ms Table A1: Table 1 where all methods include filter that removes tokens that are ignored in loss computation. This simple change represents large improvements in practice.

A REMOVING IGNORED TOKENS
=========================

It is common to have tokens that have no loss computation when training LLMs in practice. Examples include padding, the system prompt, user input, etc.. While these tokens must be processed by the backbone to enable efficient batching in the case of padding or to give the model the correct context for its prediction in the case of system prompts and use inputs they do not contribute directly to the loss. In all implementations we are aware of, the logits and loss for these ignored tokens is first computed and then set to zero. We notice that this is unnecessary. These tokens can be removed before logits+loss computation with no change to the loss/gradient and save significant amount of computation. Table A1 shows the performance of all methods in Table 1 with filter that removes ignored tokens before logits+loss computation. This represents significant speed up for all methods but Liger Kernels. Due to heavy chunking in Liger Kernels to save memory, it is bound by kernel launch overhead, not computation. Filtering ignored tokens is also significant memory saving for most all but CCE (because CCE already uses the minimum amount of memory possible).

B ADDITIONAL RESULTS
====================

Table A2 shows additional results for Gemma 2 (9 B), Gemma 2 (27 B), and Llama 3 (Dubey et al., 2024), PHI 3.5 Mini (Abdin et al., 2024), and Mistral NeMo (Mistral AI Team, 2024) in the same setting as Table 1. For each model CCE is able to reduce the total memory consumed by the loss by an order of magnitude from the baseline. For forward (Loss) and backward (Gradient) passes combined, CCE is within 3 MB of the lowest possible memory consumption. Compared to Gemma 2 (2 B) all these models have smaller ratio of the vocabulary size to hidden dimension. This has two impacts. First, the number of tokens that have significant gradient is largely constant (it is dependent on the data type). Therefore proportionally less of the gradient will be filtered out. Second, for all other methods increasing the hidden dimension increase the amount of parallelism that can be achieved. Liger Kernels (Hsu et al., 2024) sets its chunk size based on /D the lower that ratio, the bigger the chunk size. As /D continues to decrease, Liger Kernels is able to make better use of the GPU. All other methods use two matrix multiplications to compute the gradient. The amount of work that can be performed in parallel to compute and is and D, respectively4. The amount of parallel work for CCE is , thus increasing increases the amount of work but not the amount of parallelism. It may be possible leverage ideas from split-k matrix multiplication kernels to expose more parallelism to CCE for large values of D. 3The gradient and loss are computed simultaneously, not in separate forward/backward passes. 4Ignoring split-k matrix multiplication kernels for simplicity.

Preprint
========

Method Gemma 2 (9 B) (Rivi`ere et al., 2024) (V =256,000, D=3,584) Lower bound CCE (Ours) Liger Kernels (Hsu et al., 2024) Torch Tune Team (2024) (8 chunks) torch.compile Baseline Gemma 2 (27 B) (Rivi`ere et al., 2024) (V =256,000, D=4,608) Lower bound CCE (Ours) Liger Kernels (Hsu et al., 2024) Torch Tune Team (2024) (8 chunks) torch.compile Baseline Llama 3 (8 B) (Dubey et al., 2024) (V =128,256, D=4,096) Lower bound CCE (Ours) Liger Kernels (Hsu et al., 2024) Torch Tune Team (2024) (8 chunks) torch.compile Baseline Mistral NeMo (Mistral AI Team, 2024) (V =131,072, D=5,120) Lower bound CCE (Ours) Liger Kernels (Hsu et al., 2024) Torch Tune Team (2024) (8 chunks) torch.compile Baseline Phi 3.5 Mini (Abdin et al., 2024) (V =32,064, D=3,072) Lower bound CCE (Ours) Liger Kernels (Hsu et al., 2024) Torch Tune Team (2024) (8 chunks) torch.compile Baseline Loss Gradient Loss+Gradient Memory Time Memory Time Memory Time 0.004 MB 1,806 MB 1,806 MB 1 MB 65 ms 1,808 MB 140 ms 1,809 MB 202 ms 2,119 MB 420 ms 2,119 MB 419 ms 8,000 MB 75 ms 3,264 MB 168 ms 11,264 MB 243 ms 4,000 MB 70 ms 12,000 MB 133 ms 16,000 MB 205 ms 24,000 MB 102 ms 16,000 MB 164 ms 28,000 MB 270 ms 0.004 MB 2,322 MB 2,322 MB 1 MB 82 ms 2,324 MB 194 ms 2,325 MB 273 ms 2,948 MB 365 ms 2,948 MB 365 ms 8,000 MB 93 ms 4,768 MB 205 ms 12,768 MB 297 ms 4,000 MB 87 ms 12,000 MB 168 ms 16,000 MB 256 ms 24,000 MB 119 ms 16,000 MB 197 ms 28,000 MB 320 ms 0.004 MB 1,066 MB 1,066 MB 0.6 MB 36 ms 1,067 MB 1,317 MB 164 ms 2,004 MB 40 ms 2,004 MB 39 ms 10,020 MB 49 ms 2,521 MB 6,012 MB 8,016 MB 99 ms 1,068 MB 131 ms 1,317 MB 164 ms 4,525 MB 131 ms 91 ms 75 ms 8,016 MB 114 ms 80 ms 12,024 MB 131 ms 0.004 MB 1,360 MB 1,360 MB 0.6 MB 45 ms 1,361 MB 126 ms 1,362 MB 168 ms 1,872 MB 168 ms 5,396 MB 162 ms 3,348 MB 113 ms 94 ms 6,144 MB 8,192 MB 142 ms 99 ms 12,288 MB 160 ms 8,192 MB 1,872 MB 167 ms 2,048 MB 49 ms 2,048 MB 48 ms 10,240 MB 58 ms 0.004 MB 236 MB 0.2 MB 7 ms 488 MB 26 ms 8 ms 502 MB 8 ms 502 MB 2,506 MB 11 ms 236 MB 25 ms 451 MB 1,504 MB 2,004 MB 18 ms 15 ms 16 ms 236 MB 236 MB 487 MB 953 MB 2,006 MB 3,006 MB 31 ms 26 ms 26 ms 23 ms 27 ms Table A2: Memory usage and time of CCE, Liger Kernels, Torch Tune, torch.compile, and Baseline for additional models. Batch of 8,192 tokens. For the smallest /D considered, Phi 3.5 Mini (V =32,064, D=3,072) ours is approximately 35% slower (8 ms) than torch.compile (although it uses substantially less memory). As this ratio grows, the relative performance of CCE increases. RAW NUMBERS FOR FIG. 1 Table A3 contains the raw numbers used to create Fig. 1. The maximum batch size for 16 GPUs was calculated by assuming that the total amount of memory available is 75 16 (i.e., each 80 GB GPU will be fully occupied expect for 5 GB buffer for various libraries), then subtracting the memory used for weights + optimizer + gradients and then diving by the memory used per token.

Preprint
========

Model Logits Activations Weights+Opt+Grad Max Batch Size (Before) Max Batch Size (After) Increase 64,000 MB 1,152 MB 12,564 MB GPT 2 GPT Neo (1.3 B) 12,564 MB 6,144 MB GPT Neo (2.7 B) 12,564 MB 10,240 MB Gemma (2 B) 4,608 MB Gemma 2 (27 B) 64,000 MB 26,496 MB 7,488 MB 64,000 MB Gemma 2 (2 B) 8,000 MB 25,600 MB Llama 2 (13 B) 8,000 MB 16,384 MB Llama 2 (7 B) 32,064 MB 81,920 MB Llama 3 (70 B) 32,064 MB 16,384 MB Llama 3 (8 B) 8,000 MB 16,384 MB Mistral 7 8,000 MB 16,384 MB Mixtral 8x7 12,574 MB 6,144 MB Phi 1.5 8,003 MB 25,600 MB Phi 3 Medium 37,912 MB 16,384 MB Qwen 1.5 (7 B) 1,045 MB 10,421 MB 20,740 MB 19,121 MB 207,727 MB 19,946 MB 99,303 MB 51,410 MB 538,282 MB 61,266 MB 55,250 MB 356,314 MB 10,821 MB 106,508 MB 58,909 MB 5,866,190 4,268,047 3,471,784 1,155,515 739,448 1,108,206 2,203,057 3,164,429 397,019 1,579,333 3,154,108 2,344,949 4,264,482 2,188,824 1,412,087 69,845,595 12,996,042 7,731,585 17,204,330 2,525,554 10,580,057 2,891,512 4,709,560 552,414 4,670,136 4,694,200 3,489,944 12,991,781 2,873,067 4,679,564 11.9 3.0 2.2 14.9 3.4 9.5 1.3 1.5 1.4 3.0 1.5 1.5 3.0 1.3 3.3 Table A3: Raw data for Fig. 1. Memory usage calculated using global batch size of 65,536.

