{
    "paper_title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
    "authors": [
        "Yantao Liu",
        "Zijun Yao",
        "Rui Min",
        "Yixin Cao",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness. To address this, we propose a Pairwise Reward Model (Pairwise RM) combined with a knockout tournament for BoN sampling. Instead of assigning absolute scores, given one math problem, Pairwise RM evaluates two candidate solutions' correctness simultaneously. This approach eliminates the need for arbitrary scoring and enables cross-validation of solutions through parallel comparison. In the knockout tournament, Pairwise RM conducts pairwise comparisons between candidate solutions and eliminates the incorrect ones iteratively. We construct \\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from NumiaMath and annotated using \\texttt{gemini-1.5-flash}, and train the Pairwise RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate significant improvements over traditional discriminative reward models. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\% challenging problems."
        },
        {
            "title": "Start",
            "content": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament Yantao Liu1, Zijun Yao2, Rui Min3, Yixin Cao1, Lei Hou2, Juanzi Li2 1Fudan University, 2Tsinghua University, 3Hong Kong University of Science and Technology ricardoliu@outlook.com, yaozj20@mails.tsinghua.edu.cn Code: https://github.com/THU-KEG/PairwiseRM/"
        },
        {
            "title": "Abstract",
            "content": "Best-of-N (BoN) sampling, common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness. To address this, we propose Pairwise Reward Model (Pairwise RM) combined with knockout tournament for BoN sampling. Instead of assigning absolute scores, given one math problem, Pairwise RM evaluates two candidate solutions correctness simultaneously. This approach eliminates the need for arbitrary scoring and enables crossvalidation of solutions through parallel comparison. In the knockout tournament, Pairwise RM conducts pairwise comparisons between candidate solutions and eliminates the incorrect ones iteratively. We construct PAIRWISE-443K, large-scale dataset of 443K pairwise comparisons derived from NumiaMath and annotated using gemini-1.5-flash, and train the Pairwise RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate significant improvements over traditional discriminative reward models. And 40% to 60% relative improvement is achieved on the top 50% challenging problems."
        },
        {
            "title": "Introduction",
            "content": "Recently, test-time scaling has garnered significant attention from the research community, as it draw blueprint for the next stage of scaling of Large Language Models (LLMs) (Snell et al., 2024; Wu et al., 2024; OpenAI, 2024). One of the most common practice to achieve test-time scaling is to use reward models (RMs) to perform the Best-of-N (BoN) Sampling at test time (Wang et al., 2023; Lightman et al., 2023; Wang et al., 2024b; Zhang et al., 2024b; Yuan et al., 2024b): the LLM generates candidate solutions for given problem, and learned reward model, scoring each candidate solution, selects the best one as the final output. The effectiveness of this strategy hinges on how accurate the score assigned by the reward model is to the candidate solutions. However, assigning accurate and consistent scores is inherently challenging, even for human experts (Jonsson and Svingby, 2007; Abdul Gafoor and Jisha, 2014). An experiment conducted in NeurIPS 2021 shows that for different human experts guided by the same rubric, the scores assigned to the same candidate paper can vary significantly (Beygelzimer et al., 2021). This limitation is particularly pronounced in reward models, which are typically trained to assign relative scores rather than absolute, meaningful scores (Lambert et al., 2024; Liu and Zeng, 2024). As result, the scores assigned by reward models are often arbitrary and inconsistent, hindering the performance of BoN sampling (Liu et al., 2024; Kim et al., 2024). To address this limitation, we propose Pairwise Reward Model (Pairwise RM) combined with knockout tournament for BoN sampling. Instead of assigning absolute scores, Pairwise RMs evaluate two candidate solutions simultaneously, determining which one is better based on predefined criterion. Specifically, inlining with existing work (Lightman et al., 2023; Wang et al., 2023; Snell et al., 2024; Wu et al., 2024), we use math reasoning tasks as the testbed to evaluate the performance of the Pairwise RM. The criterion for the Pairwise RM is to determine which of the two candidate solutions is correct. In this setting, our approach eliminates the need for arbitrary scoring and enables cross-validation of solutions through parallel comparison. To perform BoN sampling, we organize candidate solutions into knockout tournament, where each pairwise comparison acts as match. Rounds of matches are played until only one candidate remains, which is selected as the final output. We construct PAIRWISE-443K, large-scale dataset of 443K pairwise comparisons derived from 5 2 0 J 2 2 ] . [ 1 7 0 0 3 1 . 1 0 5 2 : r Question: If one equilateral triangle in regular hexagon has perimeter of 21 inches, what is the hexagons perimeter? Response : Each side of triangle is 21 3 = 7. The hexagon, made of six such sides, has perimeter of 6 7 = 42 . Response : The triangles perimeter is 21. The hexagon, made of six such triangles, has perimeter of 21 6 = 126 . Ans. 42 Ans. 21 Ans. Response : First calculate the side length of the triangle = 7 (correct). Second, recall the hexagon is made of six such sides = 42 (correct). The final answer is correct. Advanced! Response : First, the triangles perimeter is 21 (correct). Second, the hexagon is made of six such triangles, leading to 21 6 = 126 (incorrect interpretation, mix up the area with perimeter). The final answer is incorrect. Eliminated! Advanced Eliminated N M E R P I M a) Input Prompt and Pairwise Comparison in our Pairwise Reward Model. b) Knockout with Pairwise RM. Figure 1: An example of the knockout tournament with the Pairwise RM. Pairwise RM takes one question and two responses as the input prompt, and outputs the pairwise comparison results to determine the correctness of the responses. The Pairwise RM correctly identifies the first response as correct and the second response as incorrect, leading to the elimination of the second response. Such pairwise comparisons iteratively proceed in the knockout tournament until only one response remains. The final response is selected as the best candidate solution. NumiaMath (LI et al., 2024) and annotated using gemini-1.5-flash. Using this dataset, we train the Pairwise RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate that Pairwise RM significantly outperforms traditional discriminative reward models. On the top 50% most challenging problems in MATH-500, Pairwise RM achieves 40% to 60% relative improvement over the baseline. Furthermore, our method outperforms the recently proposed Critic Model(a.k.a generative reward model (Zhang et al., 2024b)) under the same computational budget. In summary, our contributions are as follows: We propose Pairwise Reward Model (Pairwise RM) combined with knockout tournament for BoN sampling. This approach avoids the limitations of arbitrary scoring in traditional reward models and enables crossvalidation of candidate solutions. We release PAIRWISE-443K, large-scale dataset for training pairwise reward models containing 443K annotated pairwise comparison, along with its construction pipeline. Experiments on MATH-500 and the Olympiad Bench demonstrate significant improvements compared to baselines in BoN sampling. Specifically, on the top 50% most difficult problems, Pairwise RM achieves 40% to 60% relative improvement over baselines."
        },
        {
            "title": "2 Preliminaries",
            "content": "Best-of-N Sampling in Math Reasoning Given math problem and the candidate solutions {y1, y2, . . . , yN } sampled from Large Language Model (LLM), the BoN Sampling aims to select the best candidate solution from the candidate solutions based on an external selection mechanism. Typically, there are two types of reward models (RMs) serving as the external selection mechanism: the Outcome Reward Model and the Process Reward Model. Outcome Reward Model Given math problem and candidate solution y, the Outcome Reward Model assigns numerical score s(y) to the candidate solution y. The Outcome Reward Model selects the candidate solution with the highest score as the final output: = arg max y{y1,y2,...,yN } s(y). (1) The Outcome Reward Model is typically trained on preference dataset D, consisting of pairs (x, yc, yr), where yc is the chosen response and yr is the rejected response. The model is trained to assign higher reward to yc than to yr, optimizing the following objective: = [log σ(Rψ(x, yc) Rψ(x, yr))] (2) is the loss function for preference learning indicating the probability of the chosen response yc being preferred over the rejected response yr. This objective ensures that the reward model learns to identify responses that align better with human preferences. Process Reward Model Given math problem and corresponding candidate solution y, the Process Reward Model first requires to split the candidate solution into sequence of reasoning steps {a1, a2, . . . , aM }. The Process Reward Model assigns numerical score s(ai) to each reasoning step ai. The score of the entire candidate solution is the mean of the scores of all reasoning steps: s(y) ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 s(ai). (3) The Process Reward Model selects the candidate solution with the highest score as the final output with the same mechanism as the Outcome Reward Model in Equation 1. The Process Reward Model is typically trained on dataset with process labels Dproc, where each solution to problem x, the dataset contains series of process labels {l1, l2, . . . , lM }, where li {0, 1} indicates whether the reasoning step ai is correct or incorrect. Then the Process Reward Model is trained to predict the correctness of each reasoning step ai."
        },
        {
            "title": "3 Pairwise Reward Model and Knockout",
            "content": "In this section, we introduce the Pairwise RM and the knockout tournament, which are the core components of our proposed method for performing BoN Sampling at test time."
        },
        {
            "title": "3.1 Pairwise Reward Model",
            "content": "Definition Given math problem and two candidate solutions y1 and y2, the Pairwise RM is designed to simultaneously check the correctness of the two candidate solutions. Specifically, the Pairwise RM is trained to predict the correctness of the two candidate solutions, denoted as c1 and c2, respectively. c1, c2 = PairwiseRM(x, y1, y2), (4) where c1, c2 {0, 1} indicate whether the candidate solutions y1 and y2 are correct or incorrect. Specifically, given math problem and two candidate solutions y1 and y2, the Pairwise RM first generates reasoning text using chain-of-thought (Wei et al., 2022) to verify the correctness of the two candidate solutions. Based on the reasoning text, the Pairwise RM then predicts the correctness of the two candidate solutions by directly generating the correctness labels c1 and c2. The detailed prompt for performing pairwise verification with chain-ofthought is provided in Table 4 in the Appendix."
        },
        {
            "title": "3.2 Knockout Tournament",
            "content": "To perform BoN Sampling with the Pairwise RM, we introduce knockout tournament to select the best candidate solution. Algo 1: Knockout for Best-of-N Sampling Input: Math problem x, candidate solutions = {y1, y2, . . . , yN }, Pairwise reward model PairwiseRM Output: Best candidate solution ybest Step 1: Group candidates into teams Partition into teams, where members of team share the same final answer. Step 2: Initialize the knockout pool Add all candidates to the initial pool P. Step 3: Perform the knockout rounds while > 1 do Pair each candidate yi with an unpaired yj from different team. Remove yi and yj from P. foreach pair (yi, yj) do Compute correctness scores ci, cj using PairwiseRM(x, yi, yj). if ci > cj then yi advances. else if cj > ci then yj advances. else if ci, cj both correct then Randomly select one to advance. else Both incorrect and eliminated. Add advancing candidates back to P. Step 4: Return the best solution Output the last remaining in as ybest. Implementation Inspired by the Generative Reward Model (GenRM) (Zhang et al., 2024b) and LLM-as-a-Judge (Zheng et al., 2023), we implement the Pairwise RM as generative model. Specifically, we first group the candidate solutions into teams, where candidates that share the same answer are placed in the same team. Then, we pair up the candidate solutions from each team to compete with candidate solutions from other teams. In each match, only the candidate solution that receives the correct label from the Pairwise RM advances to the next round. If both candidate solutions receive the correct label, one is randomly selected to advance. This process continues until only one candidate solution remains or early termination occurs when all candidate solutions are from the same team. The detailed procedure of the knockout tournament is shown in Algorithm 1."
        },
        {
            "title": "4 PAIRWISE-443K dataset collection",
            "content": "To train the Pairwise RM, we collect largescale dataset named PAIRWISE-443K, which contains 443M annotated pairwise comparisons derived from NumiaMath (LI et al., 2024) with gemini-1.5-flash. In the following, we describe the detailed procedure of collecting the PAIRWISE-443K dataset."
        },
        {
            "title": "4.1 Dataset Format",
            "content": "Since the Pairwise RM is designed as generative model to judge the correctness of candidate solutions, the training dataset has the same format as the one for Supervised Fine-tuning, consisting of prompt-completion pairs. Specifically, each prompt is constructed by filling the template shown in Table 4 with math problem and two candidate solutions y1 and y2. The completion is chain-of-thought reasoning text that verifies the correctness of the two solutions and provides the correctness labels c1 and c2."
        },
        {
            "title": "4.2 Math Problem Collection",
            "content": "We first collect math problems from the NumiaMath dataset (LI et al., 2024), which contains 860K problems ranging from high school math exercises and international mathematics olympiad competition problems. Because these data are primarily collected from online exam paper PDFs and mathematics discussion forums, we remove low-quality problems with messy formatting, OCR errors, or missing answers. We also remove multiple-choice (MCQ) and True/False questions to avoid random guessing in candidate solutions. Following community conventions, we remove proof problems as well, due to the difficulty of verifying candidate solutions. The detailed filtering criteria are listed in Table 5 of the Appendix."
        },
        {
            "title": "4.3 Candidate Solution Generation",
            "content": "For each math problem x, we generate = 24 candidate solutions {y1, y2, . . . , yk} using LLaMATable 1: Statistics of the datasets before and after filtering. AMC-related datasets shrink significantly because most AMC tasks are multiple-choice. Dataset Original Count Filtered Count AMC/AIME AoPS Forum Chinese K-12 GSM8K Math Olympiads ORCA Math Synthetic AMC Synthetic Math Total 4,070 30,192 276,554 7,342 7,477 150,563 153,314 62,108 167,874 859,494 289 9,017 63,779 6,539 5,988 52,766 149,550 94 136,921 425,943 3.1-8B-instruct. We employ the same four-shot in-context examples for all problems as the prompt. The candidate solutions are decoded with temperature of 1.0 and Top-P value of 0.5 to balance diversity and quality."
        },
        {
            "title": "4.4 Pairwise Verification Annotation",
            "content": "We use gemini-1.5-flash to annotate the Pairwise RM training data on the NumiaMath dataset. To align the generated training data distribution with the solution-comparison distribution in the knockout tournament, we conduct knockout tournament for each math problem and its candidate solutions {y1, y2, . . . , yk} to select the best solution ybest. During the knockout tournament, we record all pairwise comparisons among candidate solutions and retain only those comparisons that correctly judge solution correctness for the Pairwise RM. Specifically, due to cost considerations, we only run the knockout tournament for questions whose candidate solutions are not all correct or all incorrect. As result, we conducted 343K tournaments and recorded 2.2M comparisons. Among these, 1.3M correctly evaluated both candidate solutions and were used as raw training data for the Pairwise RM. Finally, we filtered out samples where the response did not strictly follow the instructions in Table 4, ending up with 443K training samples for the Pairwise RM."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we demonstrate the effectiveness of the Pairwise RM and the knockout tournament in performing BoN Sampling at test time. We first introduce the experimental setup, including the dataset, evaluation metrics, and baselines. Then, we present the experimental results and analysis. Table 2: Different reward models best-of-N sampling performance on MATH-500 and Olympiad Bench with three different LLMs: Llama-3.1-8B-Inst, Qwen-2.5-7B-Inst, and Llama-3.1-70B-Inst. The results are reported in terms of accuracy. The pass@1 accuracy of these three LLMs are 42.0, 73.6, and 59.2 on MATH-500, and 12.3, 35.7, and 25.9 on Olympiad Bench, respectively. @16, @32, and @64 denote the accuracy with Best-of-16, Best-of-32, and Best-of-64 sampling, respectively. The best results are in bold, and the second-best results are underlined. Type Reward Model Llama-3.1-8B-Inst Qwen-2.5-7B-Inst Llama-3.1-70B-Inst @16 @32 @64 @16 @32 @64 @16 @32 @ ORM PRM ORM PRM ArmoRM-Llama3-8B SkyworkRM-Llama3.1-8B EurusRM-7B Math-Shepherd-7B RLHFlow-8B-Mistral-Data RLHFlow-8B-DS-Data RLHFlow-8B-LLaMA-Data Majority Voting Pairwise RM & Knockout ArmoRM-Llama3-8B SkyworkRM-Llama3.1-8B EurusRM-7B Math-Shepherd-7B RLHFlow-8B-Mistral-Data RLHFlow-8B-DS-Data RLHFlow-8B-LLaMA-Data Majority Voting Pairwise RM & Knockout 51.6 51.4 55.2 49.5 51.0 55.2 55.5 57.0 61.0 16.1 19.9 20.4 15.2 16.4 18.5 18. 20.3 22.7 MATH-500 49.2 51.0 53.4 50.1 51.0 57.0 56.8 58. 64.6 49.8 51.0 53.4 49.2 50.2 56.2 56.0 58.8 65.6 77.6 77.6 76. 74.7 75.4 75.8 76.0 77.4 80.2 Olympiad Bench 15.9 20.0 19.6 13.7 14.5 19.6 20. 22.4 24.9 16.7 18.7 20.1 13.1 14.5 19.3 19.7 23.3 25. 39.3 39.9 37.9 34.8 36.1 35.4 35.8 40.0 41.9 77.4 76.4 77.0 75.3 76.2 76.0 76. 77.6 79.8 40.1 40.0 39.4 34.5 35.9 34.8 35.2 40.7 40. 76.4 78.0 77.4 75.9 76.6 76.2 76.5 78.0 80.4 40.4 41.0 39.1 35.1 36.3 34.2 34. 39.9 41.2 64.8 66.4 68.0 63.5 64.0 66.2 66.7 70.2 72. 29.2 29.8 30.1 25.3 26.7 28.9 29.1 35.6 33.9 64.8 66.6 66.6 62.8 63.0 66.4 67. 72.8 75.6 29.8 30.4 30.7 26.0 27.1 29.5 29.4 35.9 36. 65.8 67.4 67.6 63.6 64.8 65.4 66.0 73.6 77.4 30.1 29.8 32.4 24.1 25.2 30.1 30. 36.7 37.8 Avg. 64.2 65.1 66.1 62.7 63.6 66.0 66.3 69. 73.0 28.7 29.4 30.0 24.6 25.9 27.8 28.1 32.8 33."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Dataset We evaluate BoN Sampling on MATH500 (Hendrycks et al., 2021) and Olympiad Bench (He et al., 2024) to coverage from the highschool-level math problems to the olympiad-level math problems. To study the generalizability of our Pairwise RM, we test it with three LLMs that have different capabilities and come from different model families: Llama-3.1-8B-Instruct (AI@Meta, 2024), Llama-3.1-70B-Instruct (AI@Meta, 2024), and Qwen2.5-7B-Instruct (Qwen Team, 2024). Training Details We use Qwen2.5-7B-Instruct as the base model and perform supervised fine-tuning on our PAIRWISE-443K dataset to obtain the Pairwise RM. We set the learning rate to 1 105 with the Adam optimizer and batch size of 128. The model is trained for 8 epochs. Baselines We compare our Pairwise RM with both outcome and process reward model, which is trained to assign score to each candidate solution and then select the candidate solution with the highest score as the final output. For the Outcome Reward Model, we use EurusRM7B (Yuan et al., 2024a), SkyworkRM-Llama3.18B (Liu and Zeng, 2024), and ArmoRM-Llama38B (Gao et al., 2024b) as representatives of stateof-the-art outcome reward models. For the Process Reward Model, we leverage three off-theshelf open-source models: Math-Shepherd (Wang et al., 2023), RLHFlow-8B-Mistral-Data, and RLHFlow-8B-Deepseek-Data (Dong et al., 2024). For fair comparison, we also reimplement the MathShepherd model with MCTS data generated by Llama-3.1-8B-Instruct, denoted as RLHFlow-8BLLaMA-Data. We select the candidate solution with the highest reward-model score as the final output of BoN Sampling. Moreover, we include majority-voting baseline, which selects the candidate solution that receives the most votes from the Figure 2: Comparsion between Process RM, Outcome RM and Pairwise RM on the different difficulty percentile of the MATH-500, generated by (a) LLaMA-3.1-70B-Instr and (b) LLaMA-3.1-8B-Instr respectively. The Process RM and Outcome RM here are EurusRM-7B and RLHFlow-8B-DS-Data, respectively. As shown in the figure, except for the easiest problems, the Pairwise RM consistently outperforms the Process RM and Outcome RM. Specifically, the hardest 50% problems, the Pairwise RM achieves 40% to 60% relative improvement compared to the Process RM and Outcome RM. candidate solutions as the final output. correct answers among the candidate solutions:"
        },
        {
            "title": "5.2 Results",
            "content": "Experimental results are shown in Table 2. Our proposed Pairwise RM consistently outperforms the baseline models, including majority voting, on all datasets and across all generation models. Specifically, the Pairwise RM achieves an average improvement of 6.7% on MATH-500 and 3.9% on Olympiad Bench compared to the strongest baseline model (excluding majority voting). Interestingly, majority voting performs better than the baseline reward model on MATH-500, indicating that the existing reward model may not be robust enough to assign scores to candidate solutions. These results align with the findings in RMBench (Liu et al., 2024; Kim et al., 2024), where the discriminative reward model fails to robustly and stably determine the correctness of candidate solutions."
        },
        {
            "title": "5.3 Difficulty Analysis",
            "content": "To further investigate scenarios in which the Pairwise RM outperforms the baseline reward model, we analyze the performance of the Pairwise RM and the baseline reward model on math problems with different levels of difficulty. We define the difficulty of math problem as the fraction of inDifficulty = #incorrect answers #candidate solutions . (5) Specifically, we calculate this difficulty when the number of candidate solutions is = 64. We then divide the math problems into four percentile groups based on their difficulty level and evaluate the performance of the Pairwise RM and baseline models on each percentile in the MATH-500 dataset. Figure 2 shows the results. Except for the easiest problems, the Pairwise RM consistently outperforms the baseline models across all difficulty levels. On the challenging problems (Difficulty > 0.5), the Pairwise RM achieves relative improvement of 40% to 60% over the baseline models. These findings indicate that the Pairwise RM has strong potential to enhance BoN Sampling on challenging math problems."
        },
        {
            "title": "6 Comparison with Critic Model",
            "content": "Critic Model (Gao et al., 2024a; McAleese et al., 2024), also known as LLM-as-a-Judge (Zheng et al., 2023; Bai et al., 2023), is recent approach that uses one LLM to critique another LLMs response to given prompt. These methods aim to imitate the human judgment process in instructionfollowing settings by evaluating how well response meets the users prompt. Recently, Critic Model has also been applied to the math and code reasoning domains to verify the correctness of candidate solutions and to assign numerical scores. This setting is similar to that of the Pairwise RM because both methods verify whether candidate solutions are correct. The difference is that Pairwise RM verifies two candidate solutions simultaneously, whereas the Critic Modelverifies them one by one. In this section, we investigate the effectiveness of the Pairwise RM and the Critic Modelfor correctness verification and Best-of-N Sampling at test time."
        },
        {
            "title": "6.1 Comparison on Correctness Verification",
            "content": "Here, we compare the performance of the Pairwise RM and the Critic Model on the task of correctness verification. Specifically, given one question and two candidate solutions, the Pairwise RM and the Critic Modelare asked to judge the correctness of these two solutions. For fair comparison, we first train Pairwise RM and Critic Model with the same computational budget and training data. In particular, we use the same questions from the MATH-500 training set (Hendrycks et al., 2021) and the same candidate solutions generated by Llama-3.1-8B-Instruct (AI@Meta, 2024) to build the training data for both Critic Modeland Pairwise RM. Because one training example for the Pairwise RM includes two candidate solutions, the training data for the Critic Modelends up being twice as large. All other training details follow Section 5.1. After training, we evaluate the Pairwise RM and the Critic Modelon correctness verification with the MATH-500 and Olympiad datasets. Specifically, we sample 8,000 candidate solutions from both datasets to form the test set for the Critic Model. To avoid potential bias, these candidate solutions are generated by Qwen-2.5-7B-Instruct (Qwen Team, 2024) using the test split. We then pair each solution with another solution that leads to different answer for the same question, yielding 4,000 pairs of candidate solutions for evaluating the Pairwise RM. The results are shown in Table 3. As shown in Table 3, the Pairwise RM outperforms the Critic Modelon both MATH-500 and Olympiad datasets. This finding suggests that pairwise verification in the Pairwise RM is more effective than singlesolution verification in the Critic Modelwhen judging correctness. Notably, in the more challenging Olympiad dataset, the Pairwise RM achieves Table 3: Comparison of the Pairwise RM and LLM-asa-Judge on the MATH-500 and Olympiad datasets on correctness verification task. Candidates are generated by Qwen-2.5-7B-Instruct. Accuracy is reported."
        },
        {
            "title": "Model",
            "content": "MATH Olympiad Avg. LLM-as-a-Judge Pairwise RM 67.7 70.4 56.9 64.2 62.3 67.3 Figure 3: Comparison of the Critic Model with Majority vote/ Prob Score bewteen Pairwise RM on BoN sampling. The x-axis represents the number of candidates in the BoN sampling, and the y-axis represents the accuracy larger improvement compared to MATH-500, highlighting the potential of the Pairwise RM on difficult math problems."
        },
        {
            "title": "6.2 Comparison on Best-of-N Sampling",
            "content": "As mentioned in Section 6.1, the main difference between the Pairwise RM and the Critic Modellies in the verification process. Pairwise RM verifies the correctness of two candidate solutions simultaneously, whereas the Critic Modelverifies them one by one. This difference makes it difficult to use the Critic Modelfor BoN Sampling at test time. When two candidate solutions with different answers are both verified as correct by the Critic Model, it is unclear which is better. In contrast, the Pairwise RM can directly compare the correctness of two candidate solutions and select the better one. To enable the Critic Modelto perform BoN Sampling at test time, we propose two solutions: Combine with Majority Voting One option is to combine the Critic Modelwith majority voting. Specifically, the Critic Modelverifies the correctness of each candidate solution and removes those marked incorrect. Majority voting is then applied to the remaining candidate solutions to select the final output. Problem: regular hexagon can be divided into six equilateral triangles. If the perimeter of one of the triangles is 21 inches, what is the perimeter, in inches, of the regular hexagon? Solution 1: Since the perimeter of the triangle is 21, each side of the triangle has length 21 made up of six of these triangles, so the perimeter of the hexagon is 6 7 = 42. So the final answer is 42 . 3 = 7. The hexagon is Generative RM correctness score: 0.0006 Solution 2: Since the perimeter of one of the triangles is 21 inches, the side length of the triangle is 21 3 = 7 inches. Since there are six equilateral triangles, the perimeter of the hexagon is six times the side length of one triangle, which is 6 7 = 42 inches. So the final answer is 42 . Pairwise RM correctness check: True Generative RM correctness score: 0.9999 Pairwise RM correctness check: True Figure 4: An comparison example of the generative reward model (Generative RM) and the Pairwise RM (Pairwise RM). Both models are asked to check the correctness of the two correct solution for the math problem. The Generative RM assigns drastically different correctness scores to the two solutions (0.0006 vs. 0.9999), highlighting its inconsistency. In contrast, the Pairwise RM consistently identifies both solutions as correct. Use Probabilistic Score Another approach is to use the probabilistic score assigned by the Critic Modelto each candidate solution. Concretely, the Critic Modelis prompted to generate token correct or incorrect in its reasoning text to indicate the solutions correctness. Zhang et al. (2024b) suggests that the probability of generating the token correct can be used as the score for each candidate solution. The solution with the highest score is then chosen as the final output. To prevent data leakage, we use the MATH500 test split and candidate solutions generated by Qwen-2.5-7B-Instruct (Qwen Team, 2024) to evaluate the Pairwise RM and the Critic Modelon BoN Sampling. For fair comparison, we reuse the Critic Model and Pairwise RM trained in Section 6.1 to perform BoN Sampling. As shown in Figure 3, the Pairwise RM consistently outperforms the Critic Modelon the MATH500 dataset. This result demonstrates that, under the same training budgets, the Pairwise RM is more effective at selecting the best candidate solution than the Critic Model. Another noteworthy observation is that the Critic Model with Probabilistic Score performs worse than the Critic Model with Majority Voting. Upon closer examination, we find that the Critic Model tends to assign highly polarized scores to candidate solutions. As shown in Figure 4, for two similar candidate solutions, the probabilistic scores assigned by the Critic Model differ substantially. This finding suggests that the probabilistic scores assigned by the Critic Model may suffer from the same robustness and stability issues as discriminative reward models (Liu et al., 2024; Kim et al., 2024)."
        },
        {
            "title": "7.1 Test Time Scaling and Best-of-N Sampling",
            "content": "Test time scaling has attracted considerable attention since OpenAI released their first reasoning model, o1 (OpenAI, 2024). The core idea behind test time scaling is to allocate additional computational resources at inference time to improve models performance on complex reasoning tasks, such as math problems (Snell et al., 2024; Wu et al., 2024). Several methods have been proposed to perform test time scaling, including BoN Sampling (Wang et al., 2023; Lightman et al., 2023; Wang et al., 2024b; Zhang et al., 2024b), Monte Carlo Tree Search (Zhang et al., 2024a; Gao et al., 2024b), and the long-chain-of-thought (Min et al., 2024; Qwen Team, Alibaba, 2023). Among these methods, BoN Sampling is the most naive and intuitive approach, where the model generates candidate solutions for given problem, and learned reward model selects the best one as the final output."
        },
        {
            "title": "7.2 Reward Models and Critic Models",
            "content": "Reward models (RMs) in language modeling are typically trained to assign numerical scores to responses generated by LLMs (Lambert et al., 2024; Liu et al., 2024). They are used to guide generation by providing feedback on the quality of the responses during training and inference (Wang et al., 2024a; Lightman et al., 2023; Wang et al., 2024b). Critic Models (Gao et al., 2024a; McAleese et al., 2024; Zheng et al., 2023; Li et al., 2024) is another line of work that provides feedback to LLMs by judging the quality of generated responses. Specifically, in reasoning domain such as math or code, Critic Models are used to verify the correctness of candidate solutions (Gao et al., 2024a; McAleese et al., 2024). In this regard, the Critic Models can be viewed as generative reward models (Zhang et al., 2024b), which provides textual feedback rather than numerical scores. Compared to ours, Pairwise RM verifies the correctness of two candidate solutions simultaneously, while Critic Models verify them one by one."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we propose Pairwise Reward Model (Pairwise RM) combined with knockout tournament for BoN Sampling. Pairwise RM evaluates two candidate solutions simultaneously, eliminating the need for arbitrary scoring and enabling cross-validation of solutions through parallel comparison. In the knockout tournament, Pairwise RM conducts pairwise comparisons between candidate solutions and eliminates the incorrect ones iteratively. We construct large-scale dataset of 443K pairwise comparisons for training the Pairwise RM. Experiments on MathOlympiad demonstrate that the Pairwise RM significantly outperforms baseline reward models. Our work provides new perspective on how to perform BoN Sampling at test time and can potentially be applied to other reasoning tasks beyond math problems. References Abdul Gafoor and Jisha. 2014. study of reliability of marking and absolute grading in secondary schools. Online Submission, 2(2):292298. AI@Meta. 2024. Llama 3 model card. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023. Benchmarking foundation models with language-model-as-an-examiner. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Alina Beygelzimer, Yann Dauphin, Percy Liang, The https: and Jennifer Wortman Vaughan. 2021. neurips 2021 consistency experiment. //blog.neurips.cc/2021/12/08/ the-neurips-2021-consistency-experiment/. Accessed: 2024-12-26. Karel Devriesere, László Csató, and Dries Goossens. 2024. Tournament design: review from an operational research perspective. European Journal of Operational Research, In Press, Corrected Proof. Available online 9 November 2024. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. 2024. RLHF workflow: From reward modeling to online RLHF. Transactions on Machine Learning Research. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, and Baobao Chang. 2024a. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. CoRR, abs/2406.14024. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. 2024b. Interpretable contrastive monte carlo tree search reasoning. Preprint, arXiv:2410.01707. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. NeurIPS. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems. Anders Jonsson and Gunilla Svingby. 2007. The use of scoring rubrics: Reliability, validity and educational consequences. Educational research review, 2(2):130144. Sunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Jungsoo Won, Dongha Lee, and Jinyoung Yeo. 2024. Evaluating robustness of reward models for mathematical reasoning. arXiv preprint arXiv:2410.01729. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward arXiv preprint models for language modeling. arXiv:2403.13787. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. 2024. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv: 2411.16594. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https: //github.com/project-numina/ aimo-progress-prize/blob/main/ report/numina_dataset.pdf). Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets Verify Step by Step. arXiv preprint arXiv:2305.20050. Chris Yuhao Liu and Liang Zeng. 2024. Skywork reward model series. https://huggingface. co/Skywork. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413. OpenAI. 2024. Introducing openai o1 prehttps://openai.com/index/ view. introducing-openai-o1-preview/. Accessed: 2024-09-17. Alibaba Qwen Team. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Qwen Team, Alibaba. 2023. QwQ-32B Prehttps://qwenlm.github.io/zh/ view. blog/qwq-32b-preview/. Accessed: 202412-28. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. 2024a. Secrets of rlhf in large language models part ii: Reward modeling. Preprint, arXiv:2401.06080. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. 2024b. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. 2023. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. 2024a. Advancing llm reasoning generalists with preference trees. Preprint, arXiv:2404.02078. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. 2024b. Free process rewards without process labels. arXiv preprint arXiv:2412.01981. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024a. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024b. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685."
        },
        {
            "title": "Limitation",
            "content": "The main limitation of the proposed method lies on the inference time. To conduct the BoN Sampling with the Pairwise RM, serval rounds of pairwise verification are required to select the best candidate solution. This process is time-consuming and may not be suitable for latency-sensitive applications. However, the proposed method can be potentially accelerated by parallel computing or other optimization techniques to reduce the inference time. For example, the multiple pairwise verification can easily be parallelized to multiple GPUs to speed up the inference process since they are independent of each other. Moreover, with popularization of the inference-time scaling, it is common practice to increase the computational resources to improve the performance of the model in solving complex reasoning tasks like math problems (Snell et al., 2024; Wu et al., 2024)."
        },
        {
            "title": "Ethical Considerations",
            "content": "In this work, all the data and models are acquired from public datasets and pre-trained models, and no human subjects are involved in the experiments. Considering the potential hallucination and bias in the pre-trained models, it is worth nothing that the user should be cautious when applying the proposed method to real-world applications such as use Pairwise RM to check human students homework in the educational system."
        },
        {
            "title": "Application in Reinforcement Learning",
            "content": "In this work, we mainly focus on how to perform the BoN Sampling at test time with the Pairwise RM. This experiment setting follows the existing work (Wang et al., 2023; Lightman et al., 2023; Wang et al., 2024b; Zhang et al., 2024b) and helps us to compare the performance of the Pairwise RM with baseline models and verify the effectiveness of the proposed method. However, the Pairwise RM can also be applied at the Reinforcement Learning (RL) training stage to improve the performance of the model in solving complex reasoning tasks like math problems. To apply in the training stage, the Pairwise RM need to assign numerical score to the candidate solutions just like the discriminative reward model (Lambert et al., 2024; Liu et al., 2024). Such numerical score could acquired by the winning rate of the candidate solutions in the knockout tournament, which can be used as the than only LLama-3.1-8B-Instruct to generate the candidate solutions, and 2) Use more models rather than only gemini-1.5-flash to annotate the training data. This two directions could potentially magnitudes the size of the training data and improve the performance of the Pairwise RM. Long-Cot Base Model: The Pairwise RM is trained with the Qwen-2.5-7B-Instruct. Considering the recent success of the Long-Cot models such as the QwQ-32B (Qwen Team, Alibaba, 2023) in reasoning task, it is worth exploring the application of the Pairwise RM with the Long-Cot models to further improve the performance of the Pairwise RM. reward signal to guide the training of the model. In the future, we plan to explore the application of the Pairwise RM in the RL training stage to improve the performance of the model in solving complex reasoning tasks like math problems."
        },
        {
            "title": "Alternative Tournament Strategies",
            "content": "In this work, we introduce the knockout tournament to select the best candidate solution, where the candidate solutions are viewed as players in the tournament and each pairwise comparison is viewed as match between two players. The main reason for choosing the knockout tournament is that it is one of the most naive tournament design that could select the best candidate under time complexity O(N ), where is the number of candidate solutions. It worth noting that there are tons of alternative tournament strategies that could be used to select the best candidate solution, such as the round-robin tournament, the Swiss-system tournament, and the double-elimination tournament (Devriesere et al., 2024) Such alternative tournament strategies could be potentially used to improve the performance of the Pairwise RM in selecting the best candidate solution, and we plan to explore the application of the alternative tournament strategies in the future work."
        },
        {
            "title": "Potential Improvement",
            "content": "Due to the computational limitation and resource constraints, there are several potential improvements that could be made to further improve the performance of the Pairwise RM. Bigger Model Capacity: Due to the computational limitation, although we presents promising and scalable dataset contruction in Section 4, the Pairwise RM is trained with the Qwen-2.5-7B-Instr, which is relatively small model compared to the state-of-the-art models like the Qwen-2.5-70B-Instr, LLaMA3.1-70B-Instr and QwQ-32B. According to the Chinchilla Law (Hoffmann et al., 2022), under the same training data and training time, model with larger capacity can achieve better performance than model with smaller capacity. More Data Scaling Dimension: The Pairwise RM is trained with the PAIRWISE-443K dataset, which contains 343K training data for the Pairwise RM. Now there are two directions to further improve the performance of the Pairwise RM: 1) use more models rather"
        },
        {
            "title": "Math Problem Filtering Criteria",
            "content": "Table 4: Prompt Template for Pairwise RM, the {question}, {response_a}, and {response_b} are placeholders for the math question, response A, and response B, respectively. Task Objective: Evaluate the correctness of two responses (Response and Response B) to given math question. Perform step-by-step verification of each responses accuracy. After completing the step-by-step checks, provide final correctness judgment for each response. Steps to Follow: 0. Extract Answers from both Responses: - Read and both responses to identify the final answers provided. - If the responses provide different answers, make sure there are is no possible way that both responses can be correct. It must be the case that one response is correct and the other is incorrect or both are incorrect. 1. Step-by-Step Verification of Correctness: - For each response (Response and Response B): Carefully examine each step of the solution provided. Check the following: - Mathematical accuracy: Ensure all calculations, algebraic simplifications, and mathematical operations are correct. - Logical consistency: Verify that each step follows logically from the previous one and that the reasoning is sound. - Completeness: Make sure that all necessary steps are included to fully solve the problem and reach the final answer. While performing this step-by-step evaluation, refer to the Additional Tips section for helpful techniques to validate each responses accuracy. Attention: When checking the correctness of single step, you should never first conclude the correctness of this step (for example, *\"This step is incorrect because...\"* is strictly forbidden). You should neutrally check this step, provide evidence about its correctness, and then finally draw conclusion about the correctness of this step. In other words, you should first employ the techniques in Additional Tips to check the correctness of this step, and then draw conclusion about the correctness of this step. 2. Final Conclusion: - After completing the step-by-step verification for each response, sum up the information you have now, then finally determine whether each responses answer is correct or incorrect. - Provide the final judgment for each response, the output should in-closed with the following tags: - If Response As answer is correct: <resp_a_judge>Correct</resp_a_judge> - If Response As answer is incorrect: <resp_a_judge>Incorrect</resp_a_judge> - If Response Bs answer is correct: <resp_b_judge>Correct</resp_b_judge> - If Response Bs answer is incorrect: <resp_b_judge>Incorrect</resp_b_judge> - Note: The responses and response can be either correct or incorrect, or both correct, or both incorrect. You should provide the final judgment for each response. There is no guarantee that at least one response is correct or incorrect. Additional Tips: - Key Validation Techniques (to apply during Step 1): - Re-derive Key Parts of the Solution: Independently calculate or derive crucial steps of the solution to verify their correctness. - Verify Calculations: Double-check all mathematical operations (e.g., addition, multiplication, division) to confirm accuracy. - Compare Responses: If needed, compare similar steps between Response As and Response Bs answers to identify discrepancies or inconsistencies. - The final output format should be as follows: Final Judgment: Response A: <resp_a_judge>Correct/Incorrect</resp_a_judge> Response B: <resp_b_judge>Correct/Incorrect</resp_b_judge> Question: <question> {question} </question> Response A: <response_a> {response_a} </response_a> Response B: <response_b> {response_b} </response_b> Filter Type Criteria Bad Quality Problems Equations in Ground Truth Multiple Questions Yes/No Questions Text Answers Proof Problems Multiple Choice Questions Problems with messy formatting, OCR errors, or empty ground truth (gt). gt contains = (indicating it might be an equation rather than clear ground true). Problems with patterns indicating multiple sub-questions (MULTI_QUESTION). Solutions with patterns indicating yes/no, true/false (YESNO_QUESTIONS). Ground truth containing patterns indicating textual answers (TEXT_ANSWER). Problems with patterns indicating proof problems (PROVE_PATTERN). Problems with patterns indicating multiple-choice questions (MCQ_OPTIONS). Table 5: Filtering criteria applied to the dataset to remove low-quality, proof-based, or multiple-choice problems."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hong Kong University of Science and Technology",
        "Tsinghua University"
    ]
}