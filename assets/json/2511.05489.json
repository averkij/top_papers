{
    "paper_title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning",
    "authors": [
        "Junwen Pan",
        "Qizhe Zhang",
        "Rui Zhang",
        "Ming Lu",
        "Xin Wan",
        "Yuan Zhang",
        "Chang Liu",
        "Qi She"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 9 8 4 5 0 . 1 1 5 2 : r Preprint. Under review. TIMESEARCH-R: ADAPTIVE TEMPORAL SEARCH FOR LONG-FORM VIDEO UNDERSTANDING VIA SELFVERIFICATION REINFORCEMENT LEARNING Junwen Pan1 Qizhe Zhang1,2 Rui Zhang1 Ming Lu2 Xin Wan1 Yuan Zhang1,2 Chang Liu1 Qi She1 (cid:66) 1 ByteDance {panjunwen,sheqi.roger}@bytedance.com 2 School of Computer Science, Peking University"
        },
        {
            "title": "ABSTRACT",
            "content": "Temporal search aims to identify minimal set of relevant frames from tens of thousands based on given query, serving as foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPOCSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R."
        },
        {
            "title": "INTRODUCTION",
            "content": "Long-form video understanding requires models to navigate through tens of thousands of frames to identify the most relevant information for answering specific questions (Fu et al., 2024; Zhou et al., 2024; Wu et al., 2024). Temporal search lies at the heart of making long-video understanding both accurate and interpretable (Park et al., 2025; Li et al., 2023; Ye et al., 2025). In contrast to the human visual system, which conducts adaptive temporal search (Yarbus, 1967; Hayhoe & Ballard, 2005), current large video-language models (LVLMs) primarily rely on hand-crafted search strategies with static frame sampling (Lin et al., 2023; Bai et al., 2025a; Feng et al., 2025). Humans naturally alternate between broad scanning and targeted inspection, refining their focus iteratively based on intermediate findings (Castelhano & Henderson, 2007; Henderson & Hayes, 2017). In contrast, existing methods are limited to fixed set of frames established before the reasoning process begins. This design presents fundamental contradiction: video reasoning is dynamic process where temporal search interleaves with video reasoning; however, the video frames accessible to the model remain fixed from the outset, ultimately hindering effective reasoning. Equal contribution. Project lead. (cid:66)Corresponding author. 1 Preprint. Under review. Figure 1: (a) Different paradigms of temporal search. Previous works such as VideoAgent (Wang et al., 2024) and T* (Ye et al., 2025) predominantly rely on handcrafted workflows, resulting in suboptimal strategies. Our approach adopts end-to-end reinforcement learning, enabling the model to learn optimal search strategies directly from data. (b) Interleaved text-video thinking process. We reformulate the temporal search task as an interleaved text-video thinking process, where the temporal search is seamlessly interleaved into the reasoning process. Inspired by the gap between human cognition and model reasoning, recent studies have explored interactive video agents that attempt to bridge this divide through multi-turn temporal search, as illustrated in Figure 1 (a). VideoAgent (Wang et al., 2024) first employs large language model (LLM) as the central agent, which iteratively calls tools like vision-language models (VLMs) and CLIP (Radford et al., 2021) for frame captioning and retrieval, and then aggregates information in the textual modality to perform reasoning and predict answers. T* (Ye et al., 2025) extends this paradigm by introducing an object-oriented spatial-temporal search. It first leverages VLM to extract target objects from the question, then employs object detection models (e.g., YOLO (Cheng et al., 2024)) to identify keyframes containing these objects, and finally uses the retrieved frame set to complete the task. Moreover, strategies that introduce tree-structured search to improve efficiency have also been explored (Wang et al., 2025; Li et al., 2025; Pan et al., 2025). However, all of these approaches depend on manually designed workflows, which lead to suboptimal search strategies. This motivates us to explore an end-to-end learning approach that discovers optimal temporal search strategies directly from data. In this work, we reformulate the temporal search task as an interleaved text-video thinking process, and propose TIMESEARCH-R, model that learns to actively search for relevant temporal clips through reinforcement learning (RL). As shown in Figure 1 (b), our model alternates between textual reasoning and temporal exploration, iteratively refining its understanding of the video. We refer to this dynamic process as Thinking with Videosa paradigm where models gradually improves their comprehension by searching for relevant video content conditioned on intermediate reasoning states. This concept extends the recent advances in multimodal reasoning, Thinking with Images (Su et al., 2025; Hu et al., 2024; Zheng et al., 2025), to the long-video domain. Although recent works have successfully applied RL algorithms like Group Relative Policy Optimization (GRPO) (DeepSeek-AI, 2025) to textual (Jin et al., 2025) and spatial search (Zheng et al., 2025), temporal search in videos poses unique challenges. The original GRPO rewards only the final output while ignoring intermediate search decisions, leading to several failure modes illustrated in Figure 2. The first mode, termed insufficient temporal exploration, arises because the final output reward provides no incentive for comprehensive exploration of video frames. LVLMs may arrive at correct answers through partial evidence or language bias without proper visual grounding (Niu et al., 2021), 2 Preprint. Under review. Figure 2: Two failure modes with the original GRPO reward. Left: Insufficient temporal exploration. The model misses critical frames required to correctly answer the question. Right: Inconsistent logical reasoning. The intermediate reasoning process contradicts the final answer. missing critical frames required for reliable understanding. The second mode, termed inconsistent logical reasoning, emerges when models produce plausible thinking processes disconnected from the final answers, phenomenon also observed in text-only reasoning (Lanham et al., 2023). These two failure modes hinder proper temporal search and diminish the benefits of video reasoning. To address these challenges, we propose Completeness Self-Verification (CSV) as supplement to the original GRPO algorithm, providing supervision over the intermediate steps of temporal search. GRPO-CSV tackles insufficient temporal exploration by ensuring the model to acquire sufficient visual evidence through self-verification, and promotes consistency between intermediate reasoning and the final answer by re-answering the question using the searched frames. Besides, we construct high-quality video reasoning dataset to support GRPO-CSV training. Existing datasets contain large number of trivial samples solvable through prue linguistic bias, as well as noisy samples that remain unsolvable even with extensive search, severely hindering progress in long-video reasoning. We implement two-stage data filtering pipeline to curate high-quality samples tailored to the demands of video reasoning, ensuring that the model learns the correct process of temporal search. We evaluate our TimeSearch-R on both temporal search and long-form video understanding tasks, demonstrating its superiority in long video reasoning. On temporal search tasks, TimeSearch-R improving the temporal F1 score on Haystack-LVBench by 5.6% and the accuracy on Haystack-Ego4D by 8.5%, compared to the previous state-of-the-art (SOTA) method. On long-form video understanding tasks, TimeSearch-R establishes new SOTA results with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced reasoning model Video-R1 on LongVideoBench. In summary, our main contributions are three-fold: 1. We propose the TimeSearch-R framework, which reformulates temporal search as interleaved text-video thinking and learns optimal search strategies directly from data. 2. We introduce GRPO-CSV, novel RL algorithm, which ensures sufficient and accurate video exploration by supervising the intermediate steps of temporal search. To support GRPO-CSV training, we also construct high-quality video reasoning dataset via twostage filtering pipeline, enabling the model to learn correct temporal search processes. 3. Extensive experiments demonstrate the superiority of our approach on both temporal search and long-form video understanding. Notably, TimeSearch-R establishes new SOTA on LongVideoBench, outperforming the latest reasoning model Video-R1 by 2.0%."
        },
        {
            "title": "2 METHODS",
            "content": "In this section, we first reformulate the temporal search task as an interleaved text-video thinking process, enabling the model to learn optimal search strategies directly from data. To address the challenges of insufficient temporal exploration and inconsistent logical reasoning, we introduce GRPO-CSV as novel RL algorithm for long videos, which ensures both sufficient and accurate video exploration by supervising intermediate steps of temporal search. Finally, we describe the model training process, including the construction of high-quality long-video reasoning dataset. 3 Preprint. Under review."
        },
        {
            "title": "2.1 TASK FORMULATION",
            "content": "Temporal Search within Thinking Process. To learn optimal search strategies directly from data, we reformulate temporal search as multi-turn thinking process interleaved with video clip retrieval. Given video and corresponding question Q, an initial preview is uniformly sampled from for subsequent reasoning. At each thinking step k, the policy model πθ generates textual reasoning Tk. If Tk contains search instruction, the video environment executes it according to frame timestamps, retrieving clip Vk that is appended to the ongoing chain of thought (CoT) as input for later steps. The interleaved text-video CoT at reasoning step is formalized as: Ck { (T1, V1), (T2, V2), . . . , (Tk, Vk) }. (1) This interaction process repeats until the model emits the final answer or reaches the pre-defined reasoning budget. For further analysis, the entire reasoning chain can be decomposed into two components: temporal search and answer prediction, which can be formulated as: Pθ(A, , Q) = Pθ(C , Q) (cid:125) (cid:123)(cid:122) (cid:124) Temporal Search Pθ(A C, , Q) (cid:125) (cid:123)(cid:122) (cid:124) Answer Prediction (2) . Dynamic Video Frames. During the interleaved thinking process, the model autonomously explores the video by searching for additional clips. At reasoning step k, if the model outputs search instruction, it is also required to specify the temporal boundaries tk to be explored, along with corresponding textual query qk. The video environment then executes frame retrieval function to obtain additional frames Vk = search(V ; tk }. This function serves as an interface to the policy model πθ, employing small VLM (e.g., SigLIP (Zhai et al., 2023)) to calculate the similarity among frames within the specified temporal clip [tk ], as well as the relevance with the textual query qk. The most informative frames are then sampled using determinantal point process (DPP) (Kulesza & Taskar, 2012), which has been widely used for information retrieval (Chen et al., 2018; Celis et al., 2018; Sun et al., 2025). This operation significantly improves the efficiency of temporal search, and more details can be found in Section A. , qk, ) = {f 1 , . . . , and tk , 2 , tk , tk 2.2 GRPO WITH COMPLETENESS SELF-VERIFICATION Evaluating temporal search typically requires frame-level annotations (Ye et al., 2025), which are timeconsuming and labor-intensive. To circumvent this challenge, previous works (Yu et al., 2025; Sun et al., 2025) have treated downstream video understanding task as surrogate metric for assessing the searched frame set. However, these approaches are limited to selecting an optimal subset from predefined pool of candidate frames, lacking interaction with and exploration of the video environment. Inspired by this, we design Completeness Self-Verification (CSV) mechanism for GRPO, which is annotation-free and can be seamlessly integrated into RL training, serving as complementary to the original outcome reward. The overall pipeline of GRPO-CSV is illustrated in Figure 3. GRPO-CSV. We introduce CSV as complement to GRPO with only outcome rewards. During the GRPO rollout phase, the policy model πθ generates text-video interleaved CoT and final answer A. Applying rewards only to the final answer may reduce the effectiveness of intermediate search processes. To address this, we extract the video clips from to form dynamic frame set Vc as the input for the CSV phase. In the CSV rollout phase, the same policy model is required to re-answer the question using only Vc, yielding CSV answer Ac. Critically, the model is prohibited from any further temporal searching and must rely solely on the currently searched frames to answer the question. The CSV answer Ac is expected to remain consistent with the original answer A: Pθ(Ac Vc, Q) Pθ(A C, , Q). (3) Completeness Reward. We design completeness reward for the CSV phase, which is computed using the original answer A, the CSV answer Ac, and the ground-truth answer as follows: Rc = 1[Acc(A, A) > 0.5] Acc(Ac, A). (4) where Acc(A, A) and Acc(Ac, A) denote the correctness scores of the original answer and the CSV answer, respectively, and 1[] is an indicator function activated only when the original answer is correct. This conditional design ensures that the CSV reward is applied only to promising reasoning trajectories, encouraging meaningful temporal search while verifying both the sufficiency of acquired visual evidence and the consistency between the reasoning process and the final answer. 4 Preprint. Under review. Figure 3: Overall pipeline of GRPO-CSV. Building upon the original GRPO, CSV extracts dynamic frame set from the multi-modal CoT and constructs vision-only CoT for re-answering. This design verifies that the searched dynamic frames provide sufficient evidence for correct reasoning, ensuring completeness and consistency without requiring explicit frame-level supervision. the multi-turn reasoning process, validating the structural Format Reward. The format reward enforces adherence to predefined schema throughintegrity of the entire traout jectory rather than individual steps. During reasoning, each step must follow either the <think>...</think><tool_call>...</tool_call> pattern for temporal search or the <think>...</think><answer>...</answer> pattern for the final response. We assign binary score to the full trajectory: 1 if all steps are structurally valid, and 0 otherwise. Accuracy Reward. We evaluate answer accuracy for two task types. For multiple-choice questions, we extract the option letter from the models output and perform an exact match with the ground-truth option. For open-ended questions, we adopt an LLM-as-a-Judge approach (Zheng et al., 2023) to assess the semantic agreement between the models final answer and the reference answer. The scores for both cases are given in binary form, with 1 indicating alignment with the standard answer. Overall Reward. The total reward is the sum of completeness, format, and accuracy components: This composition encourages sufficient temporal exploration (Rc), consistent reasoning structures (Rfmt), and correct final answers (Racc), enhancing the models ability to understand long-form videos. = Rc + Rfmt + Racc. (5) 2.3 MODEL TRAINING Dataset Construction. fundamental challenge in RL for long-video reasoning lies in the fact that large number of samples in the existing datasets can be solved through pure linguistic bias, reducing the reliance on temporal search. Moreover, some noisy samples remain unsolvable even under ideal temporal search, preventing the model from effectively exploring the video. To address these challenges, we implement two-stage data filtering pipeline to construct high-quality dataset tailored to video reasoning. In the first stage, we remove samples that the policy model can solve correctly using only 4 uniformly sampled frames, thereby discouraging reliance on linguistic shortcuts. In the second stage, we further discard samples that remain unsolvable even with multiple temporal searches and numerous video frames, ensuring active video exploration. Additional details of this filtering pipeline are provided in Section B.1. And we enhance dataset diversity through the incorporation of samples sourced from Haystack-Ego4D (Ye et al., 2025), VideoMarathon (Lin et al., 2025), and CinePile (Rawal et al., 2024). detailed analysis of the dataset is presented in Section B.2. 5 Preprint. Under review. Table 1: Temporal search performance. We report temporal similarity, visual similarity, and question-answering (QA) accuracy on Haystack-LVBench, as well as QA accuracy on HaystackEgo4D test-tiny subset. Baseline results are directly cited from Ye et al. (2025). indicates the average number of keyframes determined by the model adaptively. Method Uniform Uniform Uniform Base Model # Frame Temporal Visual QA R F1 F1 LVBench Ego4D Qwen2.5VL-7B GPT-4o GPT-4o Static Frame Sampling 6.3 6.3 24.9 1.4 1.4 1.4 8 8 32 VideoAgent (Wang et al., 2024) Retrieval-based (Ye et al., 2025) T* (Ye et al., 2025) Retrieval-based (Ye et al., 2025) T* (Ye et al., 2025) GPT-4 GPT-4o GPT-4o GPT-4o GPT-4o Adaptive Temporal Search 8.5 6.3 7.1 21.8 28.2 10.1 8 8 32 32 1.2 1.5 1.6 1.3 1.7 2.2 2.2 2.7 2.1 2.3 2.5 2.4 3.1 56.0 56.0 58. 58.8 63.1 58.4 59.9 58.3 72.0 72.0 81.6 73.2 65.5 72.7 80.8 83.2 62.7 62.7 67.3 64.7 64.1 64.3 67.8 67.8 33.7 47.1 50. - - 51.9 - 53.1 32.0 41.5 45.5 - - 45.0 - 46.5 TimeSearch-R (Ours) Qwen2.5VL-7B 8.8 5.4 22.3 8.1 63.2 76.4 69. 52.1 53.5 Text-Video Interleaved Reasoning Model Training. We employ two-stage training scheme for our TimeSearch-R. In the first stage, supervised fine-tuning (SFT) serves as cold start, guiding the model to follow the correct reasoning format and enabling effective policy learning in the subsequent RL stage. SFT training adopts the above dataset construction pipeline, using GPT-4o (OpenAI, 2024) to generate the text-video interleaved reasoning processes and the corresponding final answers. Following practices in the text domain (Jin et al., 2025), we mask the temporal search results during training to force the model to learn meaningful temporal windows and textual queries. The objective in this stage is to minimize the standard cross-entropy loss over reasoning tokens, while excluding masked video tokens from gradient computation. Building on this cold-start, we further conduct RL post-training based on the proposed GRPO-CSV algorithm to stimulate the temporal reasoning capability of the model."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EXPERIMENTAL SETUP Baselines. To comprehensively evaluate the effectiveness of TimeSearch-R, we compare it against three types of baselines: (1) Advanced foundation models with static frame sampling, including both API models (OpenAI, 2024; Team et al., 2024) and open-source models (Bai et al., 2025a). (2) State-of-the-art temporal search agents, such as VideoAgent (Wang et al., 2024), T* (Ye et al., 2025), and VideoTree (Wang et al., 2025). (3) Video reasoning models like Video-R1 (Feng et al., 2025). Datasets. We evaluate TimeSearch-R on two tasks: (1) Temporal search on Haystack-LVBench and Haystack-Ego4D (Ye et al., 2025), where the task is modeled as long video needle-in-a-haystack, measuring temporal and visual similarity as well as QA accuracy. (2) Long-form video understanding on VideoMME (Fu et al., 2024), MLVU (Zhou et al., 2024), and LongVideoBench (Wu et al., 2024). Evaluation Metrics. Besides the original metrics used in the benchmarks, we additionally introduce two metrics to assess the quality of the text-video interleaved thinking process for ablation study. Among them, completeness measures whether the searched frame set is sufficient for the correct answer, while consistency measures the alignment between intermediate reasoning and the final answer. Further details on these two metrics are provided in Section D. Implementation Details. We train TimeSearch-R based on Qwen2.5-VL-7B-Instruct (Bai et al., 2025a). In the RL training, we use the AdamW (Loshchilov & Hutter, 2017) optimizer with learning rate of 1e-6, KL penalty coefficient β = 0.005, and batch size of 4 with 8 rollouts per prompt. We limit each search operation to retrieving at most 8 frames from specified temporal clip, with up to 8 search steps in total. Training is conducted on 32 A100 GPUs. See more details in Section F. Preprint. Under review. Table 2: Video understanding performance. E2E stands for end-to-end optimization. # Frame represents the number of input frames. indicates the keyframes produced by temporal search. Model E2E # Frame VideoMME (w/o sub) MLVU LVB short medium long overall m-avg val Qwen2.5VL-7B (Bai et al., 2025b) GPT-4o (OpenAI, 2024) Gemini-1.5-Pro (Team et al., 2024) VideoAgent (GPT-4) (Wang et al., 2024) VideoTree (GPT-4) (Wang et al., 2025) (GPT-4o) (Ye et al., 2025) Static Frame Sampling 76.3 80.0 81. 768 384 1 fps Adaptive Temporal Search 87 128 32 67.8 69.5 Video-R1-7B (Feng et al., 2025) Video-R1-7B (Feng et al., 2025) Text-only Reasoning 71.1 74.1 32 768 66.0 70.3 74.3 59.9 63.5 59.0 65. Qwen2.5VL-7B + Search TimeSearch-R-7B (Ours) (v.s. Qwen2.5VL-7B) 3.2 MAIN RESULTS Text-Video Interleaved Reasoning 768 768 53.4 76.8 +0. 53.8 67.1 +1.1 54.6 65.3 67.4 49.0 54.2 59.3 49.4 55.6 48.2 56.0 +1.4 65.1 71.9 75. 56.0 64.1 59.9 65.7 51.8 66.6 +1.5 70.2 64.6 56.0 66.7 64.0 61.6 68.4 58.9 71.5 +1.3 56.4 58.1 49.1 60.1 +4.1 Temporal Search. On the temporal search task, TimeSearch-R establishes new state-of-the-art on LV-Haystack, as shown in Table 1. Under budget of 8 keyframes, our method achieves an F1 score of 8.1 in temporal similarity, more than three times the previous best result of 2.5 obtained by T*. In visual similarity, TimeSearch-R reaches an F1 score of 69.2, surpassing the previous SOTA method VideoAgent by 5.5, and even outperforming the retrieval-based method and T* with larger keyframe budgets. For the needle-in-a-haystack QA, our TimeSearch-R consistently outperforms the advanced API model GPT-4o, achieving 52.1% accuracy on Haystack-LVBench and 53.5% on Haystack-Ego4D. These results demonstrate the superiority of end-to-end learned temporal search strategies over handcrafted workflows based on human heuristics. Long-Form Video Understanding. Our TimeSearch-R also achieves strong performance on the long-form video understanding task, which is shown in Table 2. On VideoMME, our method reaches an overall accuracy of 66.6%, surpassing the base model Qwen2.5-VL by 1.5%. As the duration of the video increases, our method can achieve more gains, from 0.5% on short videos to 1.4% on long videos, demonstrating that temporal search becomes more valuable when the video length increases. On MLVU and LongVideoBench, TimeSearch-R achieves 71.5% and 60.1%, improving over the base model by 1.3% and 4.1%, respectively. Compared with video search agents, TimeSearch-R outperforms VideoAgent and T* on VideoMME by 10.6% and 2.5%, highlighting the advantage of end-to-end optimization. Notably, our method consistently surpasses the latest video reasoning model Video-R1 across all benchmarks, validating that text-video interleaved reasoning is more effective than text-only reasoning for long-form video understanding. Moreover, directly applying temporal search to Qwen2.5-VL through CoT prompting without additional training actually degrades performance, underscoring the necessity of RL post-training with the proposed GRPO-CSV. 3.3 ABLATION STUDIES Training Scheme. We explore the impact of different training stages in Table 4a, from zero-shot CoT to SFT and finally RL, yielding two key findings: (1) SFT enables search capability: The model cannot perform the search well only through zero-shot CoT prompts. SFT allows the model to rapidly acquire temporal search skills, dramatically improving temporal F1 from 0.0 to 7.8 and searched frame completeness from 44.2% to 60.5%. (2) RL enhances video reasoning: While RL provides modest improvements to temporal similarity and search completeness, its primary advantage lies in boosting overall understanding performance. The post-training stage improves reasoning consistency by 2.6%, which in turn raises QA accuracy from 59.2% to 66.6%. GRPO-CSV Component. We further conduct an ablation study on the components of GRPO-CSV in Figure 4, and obtain three key findings: (1) GRPO reduces search completeness. Without CSV 7 Preprint. Under review. Method Qwen2.5-VL w/ search SFT Haystack-LVBench VideoMME 0.0 7. 0.0 11.6 F1 0.0 7.8 Comp. Cons. 44.2 60.5 59.4 69.2 Acc. 51.8 59.2 GRPO (Before Collapse) GRPO-CSV w/o Acc. Rwd GRPO-CSV w/ Acc. Rwd 5.22.2 6.11.3 5.42. 18.8+7.2 19.8+8.2 22.3+10.7 7.40.4 8.2+0.4 8.1+0.3 57.23.3 61.2+0.7 60.20.3 69.3+0.1 75.3+6.1 71.8+2.6 65.1+5.9 64.8+5.6 66.6+7.4 (a) Ablation Results (b) Training Dynamics Figure 4: Ablation study of GRPO-CSV. (a) Comparison of different training schemes on temporal search and long-form video understanding. (b) When CSV is removed, training begins to collapse. The model gradually reduces the number of search calls and eventually stops searching altogether. Table 3: Ablation study of data composition. Line 1 shows the accuracy of original Qwen2.5-VL. Ego Exo Filter General Reasoning short medium long overall temporal spatial action object 76.3 74.2 76.4 76. 66.0 62.7 64.7 67.1 54.6 51.3 54.9 56.0 65.1 62.8 65.3 66. 51.4 40.1 54.8 58.8 76.8 67.9 73.2 75.0 56.8 60.0 58.2 62. 59.5 56.2 59.0 61.9 as complement, GRPO drops completeness from 60.5% to 57.2% and temporal F1 from 7.8 to 7.4, demonstrating that outcome-only rewards lead to insufficient temporal exploration. (2) GRPO-CSV improves training stability. As illustrated in Figure4b, removing CSV causes training to collapse around step 300, after which the model ceases to make search calls and completeness drops to zero. (3) GRPO-CSV with accuracy reward achieves the best QA performance. While completeness reward alone achieves the highest completeness and consistency, it slightly reduces QA accuracy by 0.3%. Combining GRPO-CSV with accuracy reward leads to the best overall QA performance. Data Composition. We also analyze the data composition in RL training, as shown in Table 3, revealing the contributions of data filtering and domain diversity. Without data filtering, RL training leads to substantial performance drop compared to the original Qwen2.5-VL. This degradation arises because linguistic biases induce zero advantage in GRPO group computation: when questions can be trivially answered through linguistic shortcuts, all rollouts achieve perfect accuracy and completeness, yielding no learning signal and severely hindering RL efficiency and training stability. After applying data filtering, the model trained solely on egocentric data recovers baseline performance, but the lack of diversity weakens the benefits of RL. By incorporating exocentric data to enhance domain diversity, the model achieves its best general QA accuracy of 66.6%. Notably, although the training data only includes general long-video QA tasks, RL training significantly boosts the models temporal and action reasoning capabilities, improving them by 7.4% and 5.7%, respectively. This remarkable performance demonstrates that TimeSearch-R learns fundamental cognitive patterns through end-toend policy optimization, validating the strong generalization of our proposed GRPO-CSV algorithm. 3.4 CASE STUDIES We analyze the search patterns that emerge during end-to-end RL training, demonstrating how the model executes temporal search within its reasoning process in manner analogous to human cognition. These search patterns exhibit adaptability and flexibility across different task types: Hypothesis-driven search. The model formulates hypotheses based on limited context and executes targeted searches to gather additional video frames as supporting evidence. (Figure 5) Confirmation or elimination. When the initially sampled dynamic frame set provides insufficient support for an answer, the model employs multi-faceted search strategies or elimination methods to collect additional evidence and reduces uncertainties. (Figure 13 and 14) Sequential search. The model performs segment-by-segment analysis to accomplish temporal reasoning tasks that require understanding sequential relationships. (Figure 15) 8 Preprint. Under review. Figure 5: Hypothesis-driven search. Given the context that dogs are lying in row across multiple scenes and remain still, the model hypothesizes that they are waiting to be photographed. It then searches for the person taking photo to gather supporting evidence and provides the final answer."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Temporal Search for Long-Video Understanding. Traditional video understanding methods rely on static frame sampling, such as uniform sampling or heuristic-based strategies (Li et al., 2024; Chen et al., 2024; Bai et al., 2025a), which fail to adapt to varying information density and evolving reasoning contexts. Recent work has explored more sophisticated mechanisms. Similarity-based methods like KeyVideoLLM (Liang et al., 2024) achieve significant compression while maintaining performance , while learning-based approaches such as Frame-Voyager (Yu et al., 2025) rank frame combinations based on prediction losses, emphasizing task-specific selection. Advanced semantic frameworks have emerged to address temporal dependencies. Logic-in-Frames (Guo et al., 2025) defines logical relations including spatial co-occurrence and temporal proximity to guide dynamic frame sampling. T* (Ye et al., 2025) reframes temporal search as spatial search with adaptive zooming mechanisms. Interactive agents like VideoAgent (Wang et al., 2024) and VideoTree (Wang et al., 2025) enable multi-turn temporal exploration through prompt-driven orchestration. However, none of the aforementioned methods adopt end-to-end optimization, resulting in suboptimal search strategies. Reinforcement Learning for Multimodal Reasoning. Recent advances have explored RL to enhance reasoning capabilities in LLMs. GRPO (DeepSeek-AI, 2025) demonstrates that outcomebased rewards can effectively elicit complex reasoning. Search-R1 (Jin et al., 2025) extends this paradigm to text-based search tasks, showing that RL can facilitate adaptive information retrieval. Approaches like MM-Eureka (Meng et al., 2025) and LMM-R1 (Peng et al., 2025) have successfully applied RL to enhance multimodal reasoning, but focus primarily on static image understanding rather than dynamic video interaction. Video-R1 (Feng et al., 2025) applies GRPO to video reasoning but limits the thinking process to prue text without visual interaction, while DeepEyes (Zheng et al., 2025) uses RL for high-resolution image understanding through adaptive cropping operations but focuses on spatial rather than temporal exploration. Despite these advances, applying RL to interactive long video understanding remains largely unexplored and presents unique challenges."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose TimeSearch-R, framework that reformulates temporal search as text-video interleaved thinking to learn optimal search strategies directly from data. To enhance temporal search through RL, we propose CSV as complement to the outcome-only reward of GRPO, addressing the challenges of insufficient temporal exploration and inconsistent logical reasoning. TimeSearch-R achieves strong performance on both temporal search and long-form video understanding tasks, while exhibiting distinct search patterns across different task types. We hope this work contributes meaningful progress toward advancing long video understanding powered by reinforcement learning. 9 Preprint. Under review."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025a. URL https://arxiv.org/abs/2502.13923. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Monica S. Castelhano and John M. Henderson. Initial scene representations facilitate eye movement guidance in visual search. Journal of Experimental Psychology: Human Perception and Performance, 33(4):753763, 2007. doi: 10.1037/0096-1523.33.4.753. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, and Nisheeth Vishnoi. Fair and diverse dpp-based data summarization. In International conference on machine learning, pp. 716725. PMLR, 2018. Laming Chen, Guoxin Zhang, and Eric Zhou. Fast greedy map inference for determinantal point process to improve recommendation diversity. Advances in Neural Information Processing Systems, 31, 2018. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1690116911, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Yifan Guo, Liqiang Zou, Yang Li, Jia Chen, and Joey Tianyi Zhou. Logic-in-frames: Logical dependency modeling for frame selection in video question answering. arXiv preprint arXiv:2501.00212, 2025. Mary M. Hayhoe and Dana H. Ballard. Eye movements in natural behavior. Trends in Cognitive Sciences, 9(4):188194, 2005. doi: 10.1016/j.tics.2005.02.009. John M. Henderson and Taylor R. Hayes. Meaning-based guidance of attention in scenes: Evidence from meaning maps. Journal of Vision, 17(6):23, 2017. doi: 10.1167/17.6.23. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=GNSMl1P5VR. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. 10 Preprint. Under review. Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Found. Trends Mach. Learn., 5(2-3):123286, 2012. doi: 10.1561/2200000044. URL https://doi.org/ 10.1561/2200000044. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning. CoRR, abs/2307.13702, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Chenglin Li, Qianglong Chen, Yin Zhang, et al. Iterative zoom-in: Temporal interval exploration for long video understanding. arXiv preprint arXiv:2507.02946, 2025. Yicong Li, Junbin Xiao, Chun Feng, Xiang Wang, and Tat-Seng Chua. Discovering spatio-temporal rationales for video question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1386913878, 2023. Dengxin Liang, Yixiao Shu, Ruobing Zhang, Songyang Chen, Xuanmo Li, and Minheng Wang. Keyvideollm: Towards large-scale video keyframe selection. arXiv preprint arXiv:2407.03104, 2024. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Hao Chen, Jiebo Luo, Zicheng Liu, and Emad Barsoum. Unleashing hour-scale video training for long video-language understanding. arXiv preprint arXiv:2506.05332, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2503.07365. Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1270012710, 2021. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024. Junwen Pan, Rui Zhang, Xin Wan, Yuan Zhang, Ming Lu, and Qi She. Timesearch: Hierarchical video search with spotlight and reflection for human-like long video understanding. arXiv preprint arXiv:2504.01407, 2025. Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryu, Donghyun Kim, and Michael S. Ryoo. Too many frames, not all useful: Efficient strategies for long-form video qa, 2025. URL https://arxiv.org/abs/2406.09396. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. URL https://arxiv.org/abs/2503. 07536. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 11 Preprint. Under review. Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. CoRR, abs/2405.08813, 2024. Zhaochen Su, Peng Xiang, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, and Yi R. (May) Fung. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. CoRR, abs/2506.23918, 2025. doi: 10.48550/ARXIV.2506.23918. URL https://doi.org/ 10.48550/arXiv.2506.23918. Hui Sun, Shiyin Lu, Huanyu Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Ming Li. Mdp3: training-free approach for list-wise frame selection in video-llms. arXiv preprint arXiv:2501.02885, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Leandro von Werra, Younes Belkada, Lewis Tunstall, and Hugging Face. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Accessed: 2025-09-16. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. ECCV, 2024. Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos, 2025. URL https://arxiv.org/abs/2405.19209. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. Alfred L. Yarbus. Eye Movements and Vision. Plenum Press, New York, 1967. Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, et al. Re-thinking temporal search for long-form video understanding. In CVPR, pp. 85798591, 2025. Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, and Qianru Sun. Frame-voyager: Learning to query frames for video large language models, 2025. URL https://arxiv.org/abs/ 2410.03226. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing \"thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. Preprint. Under review. TIMESEARCH-R: ADAPTIVE TEMPORAL SEARCH FOR LONG-FORM VIDEO UNDERSTANDING VIA SELFVERIFICATION REINFORCEMENT LEARNING"
        },
        {
            "title": "APPENDIX",
            "content": "This appendix provides more details about our methods, dataset, training, more case studies, broader impacts, as well as the LLM usage, organized as follows: Section A: Search Function Section B: Dataset Details Section C: Prompt Design Section D: Evaluation Metrics Section E: Efficiency Analysis Section F: Training Details Section G: More Case Studies Section H: Boarder Impacts Section I: LLM Usage"
        },
        {
            "title": "A SEARCH FUNCTION",
            "content": "A.1 FRAME SELECTION The video search function selects the most informative frames within predicted temporal clips. Specifically, we leverage determinantal point process (DPP) (Kulesza & Taskar, 2012) as the search optimization for its ability to naturally balance query relevance and diversity that penalizes redundancy, which has been widely applied in information retrieval (Celis et al., 2018; Sun et al., 2025). Recall the definition of search in Sec. 2.1, it aims to select optimal frames guided by temporal clip [ts, te] and query from the original video . First, the function first subsamples candidate frames F[ts,te] = {vi}N i=1 within the temporal clip. Subsequently, we obtain visual embedding hi Rd for each candidate frame in F[ts,te], and query embedding Rd for q. Then we define the pairwise cosine similarity for candidate frames as Sij = hj and compute an unnormalized query relevance score for each frame as ri = qhi, which is rescaled to [0, 1] by min-max normalization max rmin r+ϵ , where ϵ is small constant to avoid division by zero. The kernel is constructed ri = by diagonal conditioning with these relevance weights: rimin L = diag(r) diag(r), (6) which is equivalent to Lij = rirj obtained through fast greedy MAP inference (Chen et al., 2018): hj. The optimal subset F[ts,te] with = is then = arg max SF[ts ,te],S=F det(LS). (7) This formulation ensures that selected frames are both diverse and relevant to the query. When available frames are fewer than , the search function degrades to uniform temporal sampling. A.2 FRAME REPRESENTATION The selected clip frames are sparse and non-uniform. To maintain the temporal pace, we attach an explicit absolute timestamp to each frame by inserting short text token with the time in seconds (e.g., 12.3s) immediately before the image. This simple interleaving of timestamp text and the corresponding image maintains absolute temporal grounding when inter-frame intervals vary and 13 Preprint. Under review. Figure 6: Illustration of the proposed two-stage data filtering pipeline. complements the native temporal ids. Explicit absolute timestamp augmented frame representation has also been observed to improve temporal capability in prior work on long-video temporal grounding (Pan et al., 2025). For uniformly sampled preview frames, we employ the native dynamic-FPS and absolute time encoding following Qwen2.5-VL (Bai et al., 2025a), which bind image token sequences to temporal ids aligned with real absolute timestamps."
        },
        {
            "title": "B DATASET DETAILS",
            "content": "B.1 DATASET CONSTRUCTION To ensure high-quality training data, we implement two-stage filtering pipeline as shown in Fig. 6. Stage 1: Visual Dependency Filtering. We uniformly sample 4 frames from each video and feed them along with the question to Qwen2.5-VL for inference. Questions that can be correctly answered with this limited visual information are considered to have low visual dependency and are subsequently filtered out. Only questions requiring richer visual context proceed to the next stage. Stage 2: Search Usefulness Filtering. We increase the frame input to up to 64 frames and employ different LVLMs to perform dynamic temporal search for question-relevant video segments. Specifically, we use GPT-4o to generate SFT (Supervised Fine-Tuning) data and an early version of TimeSearch to obtain RL (Reinforcement Learning) training data. Although this stage produces CoT, only the CoT generated by GPT-4o is used for SFT training, while RL training utilizes only the question-answer pairs. To avoid search format errors, we implement format validation for LVLMs responses, automatically retrying the model until obtaining properly formatted answers. Human Selection for VideoMarathon (Panda-70M). Given that VideoMarathons training set contains automatically generated question-answer pairs with potential unanswerable questions or incorrect ground-truth answers, we conduct manual annotation to ensure data quality. To minimize annotator bias in model answer evaluation, we establish structured annotation protocol. First, annotators assess question reasonableness based on video content, filtering out unanswerable or ambiguous questions. Subsequently, annotators provide manual answers and compare them with synthetic ground-truth labels, removing data samples that are inconsistent with human responses. B.2 DATASET ANALYSIS The dataset exhibits pronounced long-tail distribution in video duration with mean length of 1,659 seconds. Most videos are shorter than 2,000 seconds, while nontrivial tail extends beyond one hour, posing significant challenges for static frame sampling. This distribution motivates adaptive temporal search and multi-turn interaction to progressively retrieve evidence under tight keyframe budgets. 14 Preprint. Under review. Figure 7: Dataset analysis. (1) The training set is mainly composed of long videos. The average length is 1659 seconds, and the maximum length exceeds 10,000 seconds. (2) Egocentric QA pairs come from Haystack-Ego4D, and Exocentric QA data mainly from VideoMarathon and Cinepile, where VideoMarathon employs Panda-70M as the video source. (3) Question types include multiplechoice and open-ended questions. To obtain open-ended QA pairs, we convert some multiple-choice tasks into open-ended questions. We curate data from four major sources to ensure coverage of diverse visual domains and camera styles. As shown in Fig. 7, Ego4D from Haystack-Ego4D (Ye et al., 2025) training set contributes 49.5% of samples, providing egocentric daily activities with frequent viewpoint changes. Panda-70M from VideoMarathon (Lin et al., 2025) accounts for 35.6%, expanding the variety of internet videos with heterogeneous motion patterns and scene dynamics. CinePile (Rawal et al., 2024) provides 9.5% of short videos with narrative structure and rapid scene transitions. The remaining 5.4% are from other sources and serve to reduce distributional bias. Question types are intentionally imbalanced toward open-ended reasoning to better evaluate generative capabilities. Open-ended questions make up 60.3% of the data and emphasize step-by-step analysis, temporal grounding, and explanation quality. Multiple-choice questions comprise 39.7% and offer reliable automatic evaluation signals that complement outcome rewards in RL. This composition yields wide coverage over motion intensity, scene diversity, and narrative structure while maintaining sufficient automatic evaluability. The mixture of long-tail durations and openended questions creates setting where end-to-end RL and adaptive temporal search offer clear benefits over single-shot heuristics. 15 Preprint. Under review."
        },
        {
            "title": "C PROMPT DESIGN",
            "content": "We design prompts to standardize interaction formats, minimize ambiguity, and provide explicit priors for temporal reasoning. Fig. 811 show the templates used during training and evaluation. System Prompt. We follow the tool-use specification of the base Qwen2.5-VL family (Bai et al., 2025a) and adopt its tool_call schema for invoking temporal search. This design ensures deterministic parsing by the environment and stable credit assignment for RL, as illustrated in Fig. 8. System Prompt You are helpful video assistant. # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {\"type\": \"function\", \"function\": {\"name\": \"seek_video_frames\", \"description\": \"Search and select video frames according to textual query and temporal window. Time is in seconds.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"The query is used to describe the object, scene, or event of interest in the video thoroughly and clearly. \"}, \"start_time\": {\"type\": \"number\", \"description\": \"Start time of the segment of interest. \"}, \"end_time\": {\"type\": \"number\", \"description\": \"End time of the segment of interest. \"}, \"num_frames\": {\"type\": \"integer\", \"description\": \"Number of frames to sample (maximum 8). Default is 8.\"}}, \"required\": [\"query\"]}}} </tools> For each function call, return json object with function name and arguments within < tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call> Figure 8: The system prompt with tools. Question Answering Prompt. The QA template enforces thorough reasoning inside <think> before any tool call or final answer. It restricts the output to exactly one of two formats and allows at most eight rounds of <tool_call>. It explicitly provides the line \"The video duration: {duration} seconds.\" to help the model produce absolute timestamps better. See Fig. 9. Question Answering You must ALWAYS conduct thorough reasoning inside <think> and </think> tags BEFORE calling any tool or answering the question. You must invoke tools to explore any video content you are interested in within <tool_call > </tool_call> tags. You are allowed to use <tool_call></tool_call> tags for maximum of 8 rounds. When you have enough information to answer the question, provide your answer within < answer> </answer> tags. Your answer should be supported by evidence from the video. Your output must follow the format: <think>Your reasoning process</think><tool_call> Parameters</tool_call> or <think>Your reasoning process</think><answer>Your answer</answer >Question: {question} The video duration: {duration} seconds. Figure 9: The template for question answering. Clip Frame Sampling and Search Response. After search, the template returns the selected frames and their corresponding timestamps. If the frames are sufficient, the model must place the final answer in <answer>. Otherwise, the template asks the model to call the tool again with different parameters in JSON, thereby encouraging reflection and re-query. See Fig. 10. 16 Preprint. Under review. Temporal Search Response Here are selected frames. They are located at {timestamps}. If the frames provided above are sufficient to answer the users question, please put your final answer within <answer></answer>. Otherwise invoke the tool again with different parameters in JSON format. Figure 10: The response template of the temporal search. Completeness Self-Verification Prompt. The CSV template asks the model to answer as briefly as possible and to say \"I dont know\" when the visual evidence is insufficient. No tools are available in this stage, which prevents new searches and ensures the answer is grounded only on the dynamic frame set gathered earlier. See Fig. 11. Completeness Self-Verification You are helpful assistant. Please answer visual questions as briefly as possible. When you dont have enough visual information, please say dont know. Figure 11: The template for CSV reasoning."
        },
        {
            "title": "D EVALUATION METRICS",
            "content": "Completeness Rate. We measure the proportion of cases where the dynamic visual context alone suffices to produce the correct answer. Concretely, after the multi-turn search, we re-answer the question using only the gathered dynamic frame set and disallow further search, following the CSV procedure in Sec. 2.2 and the prompt illustrated in Fig. 11. The resulting correctness is computed with the same task-specific accuracy used elsewhere, averaged over the whole dataset. Consistency Rate. Consistency evaluates whether the intermediate reasoning coherently supports the final answer under the given question. We prompt LLM model (GPT-4o) with the question, the reasoning text extracted from <think>...</think>, and the final answer from <answer>...</answer>, using the format in Fig. 12 that requires structured output: short analysis in <think> followed by <answer> equal to Yes or No. In implementation, we parse the LLMs output to obtain the binary decision; Yes is counted as 1 and No as 0, and any parsing failure is treated as 0. The Consistency Rate is the dataset average of these binary outcomes. Consistency Score Evaluation <system prompt> You are careful and logical reviewer. Your task is to verify whether the given reasoning process and the final answer are consistent in addressing the given question. Please carefully read the following information: Question: <Question> Reasoning Process: <Reasoning> Final Answer: <Answer> Please follow this format strictly: <think> Your analysis here </think> <answer> Yes/No </answer> Figure 12: The template for calculating consistency. 17 Preprint. Under review."
        },
        {
            "title": "E EFFICIENCY ANALYSIS",
            "content": "Table 4: Efficiency evaluation on Haystack-Ego4D. Baseline results are directly cited from Ye et al. (2025). We report the overall latency of temporal search and answering. Evaluations are conducted on the Haystack-Ego4D using A100 GPUs. Temporal search metrics are reported in Tab. 1. Method Question Grounding Frame Retrieval Latency (sec) VideoAgent Retrieval-based (Detector-based) TimeSearch-R GPT4 LLaVA-OV-7B CLIP-1B YOLO-world-110M YOLO-world-110M SigLIP-400M 34.9 32.2 11. 13.4 TimeSearch-R attains an end-to-end latency of 13.4 seconds on the Haystack-Ego4D test set, yielding 61.6% speed-up over the 34.9-second latency of VideoAgent. Despite operating with the lightweight YOLO-World-110M detector and completing inference in 11.1 seconds, our method maintains comparable runtime while avoiding the complexity of hand-crafted scheduling. As shown in Tab. 1, TimeSearch-R markedly surpasses these baselines in temporal search metrics and QA accuracy, underscoring the effectiveness of reinforcement-driven temporal policies."
        },
        {
            "title": "F TRAINING DETAILS",
            "content": "Table 5: Training hyperparameters of TimeSearch-R. Category Parameter Video Processing Max FPS Max Frames per Video Total Video Tokens Min Tokens per Frame Max Tokens per Frame Interaction Settings Max Search Turns Max Completion Length per Turn GRPO Training Infrastructure Number of Generations KL Penalty Coefficient (β) Scale Rewards Batch Size per GPU Gradient Accumulation Steps DeepSpeed Configuration VLLM Mode Replay Buffer Value 2 768 10,240 12 256 8 256 8 0.005 false 1 2 ZeRO-3 Offload colocate true We summarize the key hyperparameters in Table 5 for reproducibility. Training Configuration. TimeSearch-R employs distributed training setup using PyTorchs native distributed data parallel framework with ZeRO-3 memory optimization through DeepSpeed. The training process leverages gradient accumulation to simulate larger batch sizes while maintaining memory efficiency on GPU clusters. We utilize mixed precision training with bfloat16 to accelerate computation while preserving numerical stability, coupled with Flash Attention 2.0 for efficient attention computation. GRPO Training Setup. The reinforcement learning phase uses Group Relative Policy Optimization with 8 generations per prompt to provide sufficient policy gradient estimates. The KL divergence penalty coefficient β is set to 0.005 to balance between reward optimization and policy regularization. We employ VLLM in colocate mode for efficient inference during rollout generation, enabling faster 18 Preprint. Under review. policy updates. This RL training stage is implemented on top of the TRL library (von Werra et al., 2020), following standard practice for outcome-driven policy optimization in large language models. Video Processing Configuration. The model processes videos with maximum of 768 frames and allocates up to 10,240 tokens for video content representation. Each interaction turn is limited to 8 search operations, with maximum of 8 interaction turns per question to ensure comprehensive temporal exploration while maintaining computational efficiency. Frame tokens are dynamically allocated between 12 and 256 tokens per frame based on content complexity and relevance."
        },
        {
            "title": "G MORE CASE STUDIES",
            "content": "This section provides more case studies of TimeSearch-R, including successful cases and failed cases. Successful Cases. These representative success cases illustrate how TimeSearch-R conducts multiturn exploration to accumulate decisive visual evidence while maintaining alignment between the reasoning trace and the final answer. They encompass confirmation  (Fig. 13)  , elimination  (Fig. 14)  , and sequential exploration patterns  (Fig. 15)  , collectively demonstrating that the policy preserves the high completeness and consistency reported in Sec. D. Failed Cases. Figure 16 illustrates residual failure where the policy halts after reviewing only two of four candidate segments, leading to an incorrect answer. Figure 17 illustrates failure where the model hallucinates information related to riding."
        },
        {
            "title": "H BROADER IMPACTS",
            "content": "TimeSearch-R contributes to several important areas beyond the immediate technical contributions: Advancing Video Interpretability and Explainability. TimeSearch-R introduces interleaved text-video reasoning traces that provide transparent insights into the models decision-making process. The completeness and consistency criteria we propose enable quantitative assessment of long-form video explanations, making temporal search decisions auditable and interpretable. This advancement represents significant step toward more explainable AI systems in the video domain, where understanding the reasoning process is crucial for building trust and ensuring reliability. Transforming Video Reasoning from Static to Dynamic Paradigms. Our approach fundamentally shifts the paradigm from static frame sampling to dynamic, interactive reasoning in video understanding. By operationalizing hypothesis-driven exploration through iterative temporal search, we promote new methodology that emphasizes transparent, stepwise evidence gathering. This contrasts sharply with traditional one-shot inference over fixed visual contexts, encouraging researchers to develop more adaptive and interactive AI systems. The demonstrated effectiveness of our approach may inspire broader adoption of similar interactive paradigms across various multimodal tasks. Exploring Scalable Weakly-Supervised Process Rewards. We introduce outcome-based process supervision that eliminates the need for costly process annotations. Through the integration of weak supervision and reinforcement learning via completeness self-verification, our method successfully aligns intermediate search decisions with correct outcomes. This approach offers scalable solution for training complex interactive systems without requiring fine-grained procedural labels, potentially reducing annotation costs and enabling broader application across diverse domains."
        },
        {
            "title": "I THE USE OF LARGE LANGUAGE MODELS",
            "content": "The authors declare that Large Language Models were used in this paper for polishing the writing. Specifically, the LLM assisted with tasks such as grammar checking, sentence simplification, and improving the overall fluency of the text. It is important to note that the LLM was not used for any literature review or research ideation. All research ideas and experimental analyses presented in the paper were solely conducted by the authors. 19 Preprint. Under review. Figure 13: Search pattern: search confirmation. Figure 14: Search pattern: elimination method. Preprint. Under review. Figure 15: Search pattern: sequential search. 21 Preprint. Under review. Figure 16: Failure case: insufficient search. There were 4 options in total, but only 2 were reviewed before the search was terminated. Figure 17: Failure case: visual hallucination. No information related to riding was found in the search results."
        }
    ],
    "affiliations": [
        "ByteDance",
        "School of Computer Science, Peking University"
    ]
}