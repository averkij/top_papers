{
    "paper_title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining",
    "authors": [
        "Haoyu Dong",
        "Pengkun Zhang",
        "Mingzhe Lu",
        "Yanzhen Shen",
        "Guolin Ke"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows. Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference. Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU."
        },
        {
            "title": "Start",
            "content": "MACHINELEARNINGLM: SCALING MANY-SHOT IN-CONTEXT LEARNING VIA CONTINUED PRETRAINING Haoyu Dong*, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke UCAS, SCUT, Stanford donghaoyu22@mails.ucas.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine-learning (ML) tasksi.e., to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MACHINELEARNINGLM, portable continued-pretraining framework that equips general-purpose LLM with robust many-shot ICL capability while preserving its general knowledge and reasoning for broader chat workflows. Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with randomforest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with token-efficient prompt, enabling 36 more examples per context window and delivering up to 50 amortized throughput via batch inference. Despite modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MACHINELEARNINGLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of 15% on out-of-distribution tabular classification across finance, physics, It exhibits striking many-shot scaling law: biology, and healthcare domains. accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forestlevel accuracy across hundreds of shots. General chat capabilitiesincluding knowledge and reasoningare preserved: it achieves 75.4% on MMLU. Code: https://github.com/HaoAreYuDong/MachineLearningLM Model: https://huggingface.co/MachineLearningLM 5 2 0 2 1 1 ] . [ 3 6 0 8 6 0 . 9 0 5 2 : r Figure 1: MACHINELEARNINGLM on in-context ML tasks: (a) prompt template; (b) 512-shot accuracy across domains vs. Qwen-2.5-7B-Instruct; (c) many-shot scaling (23210 shots) vs. LLMs. At present, MA N ELE I GLM targets tabular ML; broader ML is future work. *Corresponding author."
        },
        {
            "title": "Work in Progress",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "LLMs possess broad world knowledge, general perceptual as well as reasoning abilities, making them promising few-shot learners (Brown et al., 2020). However, they often fail to learn new tasks despite being given many-shot demonstrations on standard ML benchmarks (Agarwal et al., 2024; Gardner et al., 2024), or to fully exploit rich experiences stored in agent memory in interactive, open-ended workflows (Wang et al., 2023; Shinn et al., 2023; Wang et al., 2024). In addition, accuracy gains typically plateau often after just handful of demonstrations and are sensitive to the label biases and choice/order of examples (Chen et al., 2023; Liu et al., 2024a; Zhao et al., 2021; Fei et al., 2023). In practice, LLMs are largely guided by surface-level signals, such as distributional, formatting cues (Min et al., 2022) and nearest-neighbor imitation (Agarwal et al., 2024), and rarely uncover new causal mechanisms or statistical dependencies that are required to yield accurate predictions. Orthogonally, pioneering tabular models (Hollmann et al., 2025; Qu et al., 2025) have demonstrated ML tasks can be solved purely by ICLwithout gradient descent. However, these tabular-specific model architectures cannot leverage the broad prior world knowledge and general-purpose multimodal perception that LLMs acquire during pretraining; consequently, they depend heavily on welldesigned featurization (Shi et al., 2021; Mráz et al., 2025) and abundant labeled data for training. At this intersection, we ask: Can we teach an LLM to do ML in context while preserving general abilities? To this end, we propose pretraining-plus-prompting framework, MACHINELEARNINGLM, that equips LLMs with in-context ML capabilities to fully exploit many-shot in-context examples. MACHINELEARNINGLM performs LoRA-based (Hu et al., 2022) continued pretraining with standard next-token objective on millions of synthetic tabular prediction tasks drawn from SCM-based priors (Peters et al., 2017; Pearl, 2009): task generator samples arbitrarily large numbers of binary/multiclass tasks under SCM-based priors spanning diverse feature types, marginal distributions, and label mechanisms following (Qu et al., 2025). This approach ensures strict non-overlap between our pretraining data and evaluation datasets. After pretraining, the model can directly leverage incontext examples of new task to generate predictions for unseen instanceswithout parameter update. As summarized in Table 1, unlike previous instruction-tuning methods (Wang et al., 2022c; Chung et al., 2022) that rely on limited set of real tasks (typically 103), we train on O(106) synthetic tasks with diverse causal mechanisms and varied shot counts. Our approach also differs from specialized tabular learners as we preserve the versatility of LLMs, which enables them to continue to leverage contextual task descriptions, draw on external knowledge, and interact directly with multimodal, heterogeneous inputs. Through large-scale pretraining, our method is able to equip LLMs with striking many-shot scaling and robust numerical modeling. We hope MACHINELEARNINGLM is promising paradigm to inspire new research that leverages LLMs general perception and reasoning capabilities, and also possibly extending to more modalities beyond text, as detailed in Section 6. Table 1: Comparison across ML paradigms. Method RF, Boosted trees TabPFN, TabICL TabLLM GPT-5, Qwen In-context learning Robust numerical modeling General knowledge priors Native multimodal input MACHINELEARNINGLM Key: Yes, No. Note: Native multimodal primarily means textual + numerical + tabular and can be naturally extended to modalities like images by building on multimodal LLM backbones."
        },
        {
            "title": "Work in Progress",
            "content": "Warm-up training with Random Forest teacher. Directly training on synthetic tasks can lead to model collapse or underperformance, particularly when tasks are too complex to be learn from only few (e.g., 64) examples. This makes strong ML methods no better than random guessing or always predicting the majority class. We stabilize the onset by mimicking random-forest (RF) teacher on each taskfirst matching example predictionsbefore transitioning to self-reliant incontext prediction. This leverages knowledge distillation for improved optimization (Hinton et al., 2015). We choose random forest teacher for its balance of robustness and interpretability. Its decision process can be transparently decomposed into rule paths and feature attributions and directly serialized into interpretable reasoning stepsthen rule chains and faithful local explanations (Deng, 2014; Friedman & Popescu, 2008; Lundberg & Lee, 2017), which naturally align with the chain-of-thought (CoT) reasoning. In this work, we distill the predicted labels only, but future directions could leverage its reasoning steps as rationales for reasoning-augmented training (DeepSeek-AI, 2025) and enhancing the interpretability of model predictions. Token-efficient prompting for in-context ML. LLMs are fundamentally bottlenecked by their context length. Our design enables 3-6 more examples within context window and yields 50 amortization via batch inference. We achieve this with three composable design choices. (i) tabular encoding to organize many-shot examples rather than as scattered and lengthy NL descriptions (Hegselmann et al., 2023; Gardner et al., 2024). Recent works (Dong et al., 2024; Sui et al., 2024) have shown that regular tabular structures can be seamlessly understood by LLMs without requiring special row/column attention (Hollmann et al., 2022). In addition, tabular encoding 1 naturally combines heterogeneous with no mandatory categorization for text. (ii) compact integer-based encoding in which we normalize all numbers for each numerical feature to integers in [0, 999], eliminating the tokenization fragmentation caused by decimal points (.) and leading signs (+/). This not only reduces the token counte.g., cl100k_base treats any integer in [0, 999] as single token, but also avoids common pitfall where LLMs compare decimals as strings (e.g., 1.11 (1.11) vs. 1.9 (1.9)) (Hugging Face, 2024). (iii) sequence-level batch-prediction mechanism that packs dozens of test examples into each sequence and predicts all of them in single forward pass, stabilizing gradients for continued pretraining and amortizing instruction/context overhead (Lin et al., 2024; Cheng et al., 2023). Order-robust, confidence-aware self-consistency. LLM predictions can be sensitive to longcontext position effects (Liu et al., 2024a). At inference, we marginalize over permutations of demonstration and feature order, combining outputs via confidence-weighted self-consistencyi.e., calibrated, probability-weighted vote over samples (Zhao et al., 2021; Wang et al., 2022b). MACHINELEARNINGLM continues pretraining on millions of synthetic tasks with diverse causal mechanisms and varied shot counts; our key experimental highlights are: Many-shot scaling. MACHINELEARNINGLM exhibits striking many-shot scaling across diverse tasks from the TALENT benchmark (Liu et al., 2024b), surpassing both leading open-source (e.g. Qwen-2.5-7B) and closed-source (e.g. GPT-5-mini) LLMs by an average of 15% under high-shot settings. Moreover, it exhibits clear out-of-distribution generalization in context length, from 32k-token pretraining budget to 131k-token inference budget. Competitive many-shot performance vs. state-of-the-art tabular methods. Without any task-specific training, MACHINELEARNINGLM reaches random-forestlevel accuracy across shot counts from 8 to 512 (within 2% relative on average) and clearly surpasses instance-based kNN, demonstrating robust numerical modeling. Heterogeneous (multimodal)-input generalization. By combining LLM versatility with classical MLs robust numerical fitting, MACHINELEARNINGLM natively consumes natural-language (NL) features alongside numerical valueswithout relying on text bucketing or embeddingsand achieves competitive results on the Talent benchmarks. General abilities preserved (chat workflows). MACHINELEARNINGLM retains general knowledge and reasoning capabilities in chat-style workflows; for example, on MMLU it 1It is extensible and can support images, hyperlinks, and embedded objects, as in HTML tables."
        },
        {
            "title": "Work in Progress",
            "content": "attains 73.2% micro accuracy in 0-shot and 75.4% in 50-shot, comparable to strong general-purpose LLMs."
        },
        {
            "title": "2.1 PRETRAINING CORPUS SYNTHESIS",
            "content": "To generate pretraining data, we build task generator following SCM-based priors (Hollmann et al.; Qu et al., 2025) that samples diverse binary/multiclass problems under priors determined by choices of graph structure, mechanisms, feature types, and class formation. SCM graph and mechanisms. We first sample DAG following the structure of fully connected MLP, where each neuron corresponds to variable, then assign structural equations to each node in G: (cid:1) + εv, where fv is drawn from rich pool of functions, following (Qu et al., 2025), including tanh, leakyReLU, ReLU, ReLU6, SELU, SiLU, Softplus, Hardtanh, sign, sine, RBF, exp, gaussian-processsampled random activations, (cid:0)xparents(v) εv i.i.d. noise. xv fv Tree-based SCMs. To inject tree inductive biases beyond neural layers, 30% fvs are replaced by multi-output gradient-boosted regressors (fℓ) fitted on fake targets drawn from Gaussian noise and parent inputs. For each layer: nestimators min(cid:8)4, 1 + Exponential(λ = 0.5)(cid:9), max_depth min(cid:8)4, 2 + Exponential(λ = 0.5)(cid:9), We also fit fℓ : xparents(ℓ) (cid:55) with (0, I), (cid:0)xparents(ℓ) and set xℓ fℓ (cid:1). Feature and label typing. We sample fraction of categorical features, discretize dense columns into bins, and (with some probability) shuffle category IDs to avoid spurious order. Continuous features remain numeric. As for scalar y. We create K-way labels by sampling {2, . . . , 10}, drawing ordered bounds τ1< <τK1 from the empirical distribution of y, and mapping = arg max k{1,...,K} (cid:2)τk1 < τk (cid:3), Followed by random permutation of class IDs to destroy ordinal hints. Class imbalance arises naturally from the sampled bounds. Task sampling policy and corpus scale. We repeatedly instantiate fresh SCMs, yielding corpus of 3 106 distinct SCMs. For each SCM, we propagate noise through the structural equations to generate i.i.d. samples (X, y). For each task, we sample the number of demonstrations (capped at 1,024), fix the number of queries at = 50, and draw the number of features {5, . . . , 50} and classes {2, . . . , 10}. Each task is serialized into prompt following Section 2.4 and comprises the following components: compact tabular many-shot block (features with labels), tabular test block (IDs with features), and an instruction specifying JSON array output (cid:2){\"id\" : , \"label\" : }, . . . (cid:3), as illustrated in Figure 1a. Given token budget of 32k Qwen 2.5-Instruct in our framework, we truncate the number of many-shot samples per task to fit the length limitation for continued pre-training. 2.2 OBJECTIVE OF CONTINUED PRE-TRAINING As illustrated in Figure 1a, we cast each in-context tabular prediction task as conditional sequence modeling problem. For given task, the prompt concatenates: (i) an instruction header describing the task and specifying JSON output format, (ii) many-shot training block with demonstration block = {si}M i=1 of -shot labeled demonstrations (demonstration order immaterial), and"
        },
        {
            "title": "Work in Progress",
            "content": "Figure 2: Heatmap of task density across feature and shot counts. We sample 100k tasks from the synthetic generator and aggregate by feature_num and shot_num bins. Color encodes the sum of num per bin. Training token budget capped at 32k tokens. (iii) query block = {x(i)}N token-efficient prompting in Sec. 2.4. i=1 of unlabeled queries (each with an id). (ii) and (iii) follow the (cid:124)(cid:123)(cid:122)(cid:125) instruction + schema s1 si sM 1 (cid:123)(cid:122) (cid:125) in-context demonstrations (cid:124) x1 xN 1 (cid:123)(cid:122) (cid:125) batched queries (cid:124) The model is required to generate single JSON array. = (cid:2){\"id\" : 0, \"label\" : ˆℓ1}, . . . , {\"id\" : 1, \"label\" : ˆℓN 1}(cid:3). Loss. We introduce no auxiliary heads or bespoke objectives, and use the standard left-to-right loglikelihood language modeling objective over the JSON target for continued pre-training. Formally, let tok() denote the tokenizer and = tok(x), = tok(y) = (y1, . . . , yT ). The training loss for parameters θ is the negative log-likelihood L(θ) = (cid:88) t=1 log pθ(yt x, y<t) , (1) applied to the entire JSON string (brackets, keys, colons, commas, IDs, and label tokens). This encourages (i) learning the mapping from features to labels, (ii) preserving test-row order via aligned \"id\" fields, and (iii) strict adherence to the required JSON format. 2.3 RANDOM-FOREST MIMIC WARM START Directly training on synthetic tasks can cause training collapse as low-signal tasks, limited few-shot examples, severe class imbalance, and randomly sampled label regimes may yield poor local optima, bend the loss curve, and occasionally destabilize training (Ochal et al., 2023). We mitigate this risk by first mimicking Random Forest (RF) teacher on each task, where we match per-example predictions in short warm-upbefore transitioning to standalone in-context prediction. This provides more informative targets and smoother gradients in the early phase, similar to knowledge distillation (Hinton et al., 2015). We choose RF as the teacher because its decision process decomposes transparently into rule paths and feature attributions. These can be serialized as explicit reasoning"
        },
        {
            "title": "Work in Progress",
            "content": "steps (Deng, 2014; Friedman & Popescu, 2008; Lundberg & Lee, 2017), which aligns naturally with the stepwise reasoning patterns large language models can emulate. For each pretraining task, we (i) utilize better-than-random guard at the task level to skip lowsignal or degenerate tasks, and (ii) apply an example-level consensus filter that retains only test examples where the RF prediction matches the ground truth; we filter examples at this stage but do not alter the original train/test labels. After the warm-up ends, we continue to apply (i) but disable (ii), enabling the model to transition from relying on random-forestinduced priors to developing its own in-context ML behavior. This design prunes low-signal tasks before they corrupt optimization and, by supplying high-precision targets, reduces gradient noise, prevents collapse-to-majority under imbalance, and yields smoother warm-up that mimics robust numerical modelingultimately easing the shift from teacher imitation to independent in-context ML. (i) Task-level guard against chance and collapse. To rule out degenerate tasks where the RF performs no better than chance, we compare its accuracy to conservative random baseline (cid:16) (cid:88) p0 = max k= p2 k, max (cid:17) , pk where is the number of classes, and pk denotes the class prior from ytrue. This baseline corresponds to the larger of sampling by prior or always predicting the majority class. Let Ncorrect = #{i : yi = ˆyi} be the number of correct predictions, we then perform one-sided binomial tail test p-value = Pr[X Ncorrect], Bin(N, p0) and require p-value < α. To avoid trivial solutions and collapsed behavior under class imbalance, we enforce the following criteria3each targets distinct failure mode: Chance-corrected agreement: Cohens κ > 0. Imbalance-robust accuracy: Balanced accuracy > 1/K + δbacc. Macro-F1 dominance: 1macro 1(maj) macro + δF1. Non-collapse checks: At least two predicted classes; dominant predicted class fraction τdom. Evaluation setup: = 50 and 2. Tasks failing to meet all criteria are skipped, and only tasks passing all criteria are used in pretraining. (ii) Example-level label-prediction consensus filter (warm-up only). For admitted tasks, we retain in the prompt only those evaluation examples where the RF prediction matches the ground truth (RF = GT), discarding mismatched cases while keeping original labels and splits unchanged. This enforces teacherstudent alignment without relabeling, yielding cleaner supervision in the earliest updates. After the warm-up stage, consensus filtering is discontinued, allowing the model to move beyond imitation and develop standalone in-context policies. After filtering, we reduce the evaluation set size from = 50 to Nconse = 20. For the remaining usable examples, we resample within each label class to restore Nconse = 20, aiming to match the proportions of many-shot demonstrations. This prevents collapse-to-majority under imbalance and avoids sub-optimal supervision. 2.4 TOKEN-EFFICIENT PROMPTING FOR IN-CONTEXT ML The application of many-shot in-context ML is often constrained by the fixed context length and high computational cost of long contexts (Gardner et al., 2024). We alleviate these limitations through three design choices: (i) compact structuring of in-context examples, (ii) compression of numeric tokens, and (iii) sequence-level batching. 2For = 50 the test has relatively low power; we use slightly relaxed α (default 0.2) to avoid discarding borderline-but-promising tasks. 3We use κ > 0.01, δbacc = 0.03, δF1 = 0.00, and τdom = 0.95, where maj denotes the always-majority classifier."
        },
        {
            "title": "Work in Progress",
            "content": "Figure 3: comparison between the NL description style and tabular style of many-shot examples."
        },
        {
            "title": "2.4.1 TABULAR ENCODING",
            "content": "Instead of presenting demonstrations as scattered NL descriptions (Hegselmann et al., 2023; Gardner et al., 2024), we place many-shot examples in concise table-style format. Building on recent findings that normal tabular structures can be seamlessly understood by popular LLMs (Dong et al., 2024; Sui et al., 2024), our approach preserves column/field meaning while dramatically reducing tokens relative to full-sentence templates. The example in 3 contrasts an encoding table expressed as scattered NL descriptions, as in TabLLM (Hegselmann et al., 2023) and TabuLa (Gardner et al., 2024). with our compact tabular encoding, where commas separate features and separates the label (the header is optional). Notably, while feature names and task descriptions may contain contextual semantics that could be readily exploited by LLMs, our approach avoids using such information. This ensures fair comparison with traditional ML methods and avoids data contamination and memorization (Bordt et al., 2024). Moreover, the current tabular encoding jointly represents heterogeneous textual and numeric features, without requiring mandatory categorical bucketing or embedding extraction for text. Toward future extensions of current tabular encoding to multimodal features, HTML tables can be leveraged to support images, hyperlinks, and embedded objects. 2.4.2 COMPACT INTEGRAL-BASED NUMBER ENCODING We normalize all numbers of each numerical feature to non-negative integers in [0, 999] to eliminate the tokenization fragmentation caused by decimal points (.) and leading signs (+/). The resulting values are then represented as plain text. This preserves ordinal structure yet maps each number from sequence of scattered tokens to an integral single token under GPTs cl100k_base. Beyond token savings, integer rendering avoids frequent pitfall where decimals are compared textwise (e.g., 1.11 (1.11) vs. 1.9 (1.9 where 11 is bigger than 9 while 1.11 is smaller than 1.9)) rather than numerically (Spithourakis & Riedel, 2018; Wallace et al., 2019; Golkar et al., 2024; Singh & Strouse, 2024; Hugging Face, 2024). Connection to z-norm. Recent work in tabular modeling, including TabICL (Qu et al., 2025), has adopted z-score normalization (meanvariance standardization) for each feature, which yields realvalued decimals. We also adopt this approach as the initial step before discretizing the normalized values into bounded integers in [0, 999]. Integer mapping. We map each normalized value to an integer [0, 999] via = clip(cid:0) round(120z + 500), 0, 999(cid:1), where round(x) = + 0.5 and clip(x, a, b) = min{max(x, a), b}. Under this mapping, = 0 is mapped to = 500, and = 4.166which covers 99.997% of values under the standard normal distribution maps approximately to = 0 999, with out-ofband values clipped. This transformation yields [0,999]-norm, which preserves numeric order while ensuring that most values are tokenized as single token under cl100k_base. Specifically, GPT"
        },
        {
            "title": "Work in Progress",
            "content": "(cl100k_base) and LLaMA-3 vocabularies merge consecutive digits, commonly treating sequence of three digits as single token. In contrast, Qwen and LLaMA-2 vocabularies tokenize numbers at the level of individual digits, leading to much more fragmented representations. Below illustrates typical fragmentations of popular tokenizers for z-norm decimals and [0-999] non-negative integers. Z-norm (decimal form) GPT (cl100k_base), LLaMA-3: 0.1234 - 0 . 123 4 Qwen, LLaMA-2: 0.1234 - 0 . 1 2 3 4 [0,999]-norm (integer form) GPT (cl100k_base), LLaMA-3: Qwen, LLaMA-2: 486 486 486 4"
        },
        {
            "title": "2.4.3 SEQUENCE-LEVEL BATCH-PREDICTION",
            "content": "As shown in Figure 1, we pack query rows into single sequence and decode all predictions in one forward pass, increasing the effective batch size at the sequence level (Lin et al., 2024; Cheng et al., 2023). Let the sequence consist of shared header (task instruction, schema) and incontext demonstrations S, followed by test queries {x(i)}N i=1 and an output section for predictions {y(i)}N i=1: (cid:124) (cid:123)(cid:122) (cid:125) instruction + schema + shots x(1) x(N ) (cid:125) (cid:123)(cid:122) batched queries (cid:124) = y(1) y(N ) (cid:125) (cid:123)(cid:122) batched predictions (cid:124) . During training, MACHINELEARNINGLM remains purely autoregressive and is optimized with the standard next-token objective (Section 2.2). Increasing accelerates and stabilizes continued pretraining. At inference, the model predicts samples in single pass, amortizing instruction and context overhead (Section 2.4.4). Stability and reliability. big not only consumes the token budget that would otherwise be allocated to demonstrations, but also increases instruction-following errors (e.g., cross-item interference and run-on/non-terminated generations), particularly for smaller open-source models. We therefore set = 50 by default. In addition, we randomize the within-sequence order of {x(i)} to reduce position bias and improve exchangeability of the batched items.4 Importantly, id should be 0-indexed; 1-based indexing often destabilizes instruction following and leads to incorrect IDs. 2.4.4 COMPRESSION AND AMORTIZATION RATIO Token-cost model. We model the amortized token cost per predicted label as follows. Let denote the number of many-shot demonstrations (with labels) and be the number of query examples to be inferred (with IDs). Each row is dominated by its features, so we represent the perrow token cost by R, without distinguishing labels and IDs. The instruction/schema header incurs token cost of H, which is negligible compared to R(M + ) and can be omitted. The resulting amortized token cost per predicted label is therefore = + (M + ) (M + ) . Compression and amortization ratio. The overall compression ratio is defined as the product of three saving factors: (A) tabular structure, (B) number normalization, and (C) batch inference (A) TABULAR STRUCTURE (NLDECIMAL TABULARDECIMAL). We replace scattered NL sentences (e.g., the first feature is 0.1234) with compact, comma-delimited rows, and we apply this format throughout Section 2.4.4. This will substantially reduce the per-row cost R. Counting words, spaces (absorbed into word tokens), numbers, and delimiters, we obtain: 4This simple permutation also helps mitigate long-context position effects observed in batch-prediction."
        },
        {
            "title": "Work in Progress",
            "content": "Model / Tokenizer GPT(cl100k_base), LLaMA-3 Qwen, LLaMA-2 RNL,dec RTab,dec Ratio 2.0 5 1.71 7 10 12 (B) NUMBER NORMALIZATION. Mapping normalized decimals to the integer range [0, 999] preserves the relative rank of decimals while collapsing numbers to single token under cl100k_base (and 23 tokens under Qwens tokenizer). Without delimiters. For example, 0.1234 requires 4 tokens, whereas 234 requires only 1;similarly, 0.1234 requires 5 tokens, compared to 1 for 234. Assuming balanced distribution of positive/negative numbers, the expected token length is (0.5 4 + 0.5 5) = 4.5 before normalization and 1 after. Thus, the expected per-number reduction under cl100k_base is 4.5. In Qwens tokenizer, positives use 6 tokens vs. 3 after, negatives 7 vs. 3 after; the expected length is (0.5 6 + 0.5 7) = 6.5 before and 3 after, giving 6.5/3 2.17. With delimiters. In practice, each number will be followed by delimiter (a single-token comma), which increases numerator and denominator lengths by +1. Hence for cl100k_base, this yields an expected length of 5.5 before normalization versus 2 after, giving 5.5/2 = 2.75. For Qwens tokenizer, the corresponding expectation is 7.5 before versus 4 after, resulting in 7.5/4 1.88. These expected compression ratios propagate to corpus-level savings: Model / Tokenizer w/o delimiters w/ delimiters GPT(cl100k_base), LLaMA-3 Qwen, LLaMA-2 4.5 2.17 2.75 1.88 (C) BATCH INFERENCE. Given C(N ) = R(M +N ) , increasing amortizes the many-shot block. General form: C(1) C(N ) = (M + 1) + (independent of tokenizer since cancels). Derivation for =1024, from =1 to =50: C(N = 1) C(N = 50) = 50 (1024 + 1) (1024 + 50) = 47.7 . Overall compression and amortization. Since Stages (A), (B), and (C) are independent, their effects compound multiplicatively. From compression perspective, (A) structural formatting and (B) number normalization compound multiplicatively to shrink the per-row token cost R, which then will allow the LLM context to accommodate more many-shot demonstrations. Hence Compression (ab): GPT(cl100k_base), LLaMA 3: 2.02.75 5.5 , Qwen, LLaMA 2: 1.711.88 3.2 . Amortization (c, n=50): 47.7 Notably, the above estimate assumes that the prompt header contributes negligibly to the overall computation. Beyond this estimation, we leverage our SCM-based synthetic pretraining corpus to evaluate the amortized token cost per inference sample using both our encoding and TabuLa (Gardner et al., 2024)s. Under this setup, we observe 136 improvement using Qwens tokenizer. 2.5 ORDER-ROBUST, CONFIDENCE-AWARE SELF-CONSISTENCY AT INFERENCE TIME Our approach is similar to self-consistency proposed by (Wang et al., 2022a), but modifies both the source of diversity and the aggregation method. Instead of generating diverse reasoning paths from"
        },
        {
            "title": "Work in Progress",
            "content": "identical prompts via stochastic sampling, we create diversity by shuffling in-context demonstrations or features across prompt variants. We then aggregate the models responses using weighted majority voting (Littlestone & Warmuth, 1994) to select the most consistent prediction. More specifically, given prompt with in-context demonstrations, we generate shuffled variants {P0, P1, . . . , PV 1} and obtain the models next-token probabilities for each variant via parsing responses of LLMs. For each query, we extract the corresponding probability of the next token for each possible label yj {y1, y2, . . . , yk} from the models vocabulary distribution. For prompt variant Pi, let pi(yj) denote the probability assigned by the LLM to the token representing label yj as the next token in the sequence. We compute the aggregate probability for each label by summing across all prompt variants: p(yj) = 1 (cid:88) i=0 pi(yj) (2) The final prediction is then determined by selecting the label with the highest aggregate probability: ˆy = arg maxyj p(yj). We set = 5 in the following experimentsfar fewer than the 32 used by TabICL and TabPFN."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 PRELIMINARY 3.1.1 EVALUATION DATASETS We evaluate on the TALENT benchmark (Ye et al., 2024), which comprises 200 classification tasks, several of which include both tabular and textual features. For the main study, we select 32 datasets via domain clustering and sampling, and add 86 more as an extended set. We exclude one dataset whose combined numerical and textual fields exceeded our context-length budget. All tasks are listed in the Appendix. Each dataset is split 80/20 into train/test, and we report accuracy (ACC). 3.1.2 BASELINES Tree-based learners. We include classical tree ensembles as strong tabular baselines: Random Forest (RF) (Breiman, 2001), LightGBM (Ke et al., 2017), XGBoost (Chen & Guestrin, 2016), and CatBoost (Prokhorenkova et al., 2018). In our main comparisons, we report Random Forest because it is stable and robust from few-shot to many-shot regimes and easy to reproduce at scale. We use the same hyperparameters as the RF teacher in the LLM experiments. For Random Forest, we set n_estimators=30, fix random_state for determinism, and use n_jobs=8 (we already parallelize across processes). Instance-based learner. Recent work by Agarwal et al. (2024) suggests that LLMs can behave similarly to local neural networks when learning linear high-dimensional functions with numerical inputs. Motivated by this observation, we include k-nearest neighbors (kNN) (Cover & Hart, 1967) as simple instance-based baseline. Specifically, we use n_neighbors=8, weights= distance, and the Minkowski distance with p=2. Tabular ICL models. We compare to TabICL (Qu et al., 2025), recent in-context learners for tabular data. We run their public ICL-style inference (no gradient updates), which is faster to evaluate and aligns with our in-context setup. We also compare against TabuLa-8B (Gardner et al., 2024), an 8B LLM fine-tuned specifically for tabular prediction. General-purpose LLMs. For closed-source LLMs, we evaluate GPT-5-mini (latest public snapshot) and the reasoning-oriented o3-mini (OpenAI, 2024; 2025). For open-source LLMs, we use Qwen-2.5-7B-Instruct (Bai et al., 2023) as strong lightweight baseline. MACHINELEARNINGLM is also built on Qwen-2.5-7B-Instruct via continued pretraining, enabling direct apples-to-apples comparison under identical prompting."
        },
        {
            "title": "3.1.3 CONTINUED PRETRAINING SETTING INTRODUCTION",
            "content": "We employ the task generator described in Section 2.1 to construct our synthetic pretraining corpus. Our dataset comprises 3 million synthetic tasks, each containing no more than 1,024 samples with feature dimensions ranging from 5 to 50 and up to 10 class labels. Owing to the 32k-token context limit during training, we truncate the number of in-context examples per task during continued pretraining. We build upon Qwen-7B-Instruct (Bai et al., 2023) as our backbone model, which utilizes rotary positional embeddings. We adopt two-stage continual pretraining approach, implemented using LoRA (Hu et al., 2022) with rank 8 over attn+MLP and the Adam optimizer (Kingma & Ba, 2014), within the LLaMAFactory framework (Zheng et al., 2024). Training was distributed across 5 nodes with 40 A100 GPUs, achieving throughput of 300 tokens per second (100s per iteration, with each iteration processing 30k tokens). We set lr_scheduler_type as cosine to use the cosine learning rate scheduler throughout training. In the first (warm-up) stage, we set the learning rate to 1 105 and trained on approximately 1 million tasks over 100 hours. The second stage resumed from the Stage 1 checkpoint with reduced learning rate of 1 106, continuing on about 2 million tasks for 200 hours. We set the batch size per GPU to 1, and updated model parameters every 8 steps. With 40 GPUs, this corresponds to one parameter update per 320 samples. 3.1.4 TEST TIME SETTING For ICL evaluation, we partition test data into chunks of size , with each chunk serving as test batch. For each test chunk, we randomly sample training examples as in-context demonstrations, and we use different sets of examples for each batch to ensure the results are not biased by particular fixed set. When generating prompt variants as described in Section 2.5, we shuffle only the order of the in-context examples while keeping the test examples and their order fixed. For fair comparison with traditional ML models, we randomly sample training examples for model training and evaluate on the complete test set, as detailed in our public code. Importantly, although the header row in tabular data could be used to incorporate feature names/descriptions that LLMs could easily leverage (Bordt et al., 2024), our method intentionally excludes such information during training and inference. This design choice promotes fair comparison with conventional machine learning approaches and mitigates risks of data leakage and memorization (Bordt et al., 2024), while still delivering competitive performance. Additionally, our current tabular encoding strategy jointly models both textual and numerical features, without relying on categorical bucketing or explicit text embedding extraction. The number of samples for voting is 5 due to computational constraints, while TabICL and TabPFN use 32. 3.2 EXPERIMENT RESULTS"
        },
        {
            "title": "3.2.1 MACHINELEARNINGLM VS. VANILLA LLMS",
            "content": "As Table 2 and Table 3 show, pretraining solely on synthetic tabular tasks yields large, consistent gains over vanilla LLMs. Averaged across tasks, MACHINELEARNINGLM improves absolute accuracy of the backbone Qwen-2.5-7B-Instruct model by 15%  (Table 2)  , with about 50% of tasks seeing 15% improvements across finance, biology, vision, speech, robotics, statistics, and healthcare domains  (Table 3)  ; virtually no task exhibits marked degradation  (Table 3)  . At high shot counts (1281,024), MACHINELEARNINGLM surpasses GPT-5-mini by 12% and o3-mini by 16% on average (cf. Figure 1c and Table 2). More results on extended tasks are presented in Appendix C. 3.2.2 MANY-SHOT SCALING MACHINELEARNINGLM displays striking many-shot scaling law: accuracy increases monotonically as we raise the in-prompt demonstrations from 23 to 210 (within 131k-token inference budget, 4 the 32k-token budget in the pretraining phase), with steady gains across domains (Figure 1c; Table 2). While our model can improve accuracy over 15% from 8 to 512 shots, vanilla LLMs exhibit limited scaling: o3-mini improves by only 1.8% (and even declines from 64 to 512 shots), while GPT-5-mini gains merely 4.7%. This indicates substantially higher sample efficiency of our approach."
        },
        {
            "title": "Work in Progress",
            "content": "Table 2: Test accuracy (%) across number of shots =8512. Notably, as shown in Figure 2, large fraction of training tasks exceed the 32k-token limit when both the number of shots and number of features are big; nevertheless, the model exhibits out-ofdistribution generalization to 131k-token contexts. Abbreviations: Qwen-7B=Qwen-2.5-7BInstruct; Ours=MACHINELEARNINGLM (base: Qwen-2.5-7B-Instruct). Not in-context learning In-context learning Number of shots KNN Random Forest TabICL GPT-5 mini o3-mini Qwen-7B Ours 8 16 32 64 128 256 55.3 60.1 63.4 65.9 68.1 69.5 71.1 59.1 63.5 68.0 71.4 74.3 76.1 77.7 57.4 64.0 68.6 72.6 76.0 79.1 80.9 57.8 60.0 60.2 60.4 61.0 61.7 62.5 57.0 58.6 59.0 59.6 58.4 58.8 58.8 51.8 53.9 55.6 57.2 58.6 59.3 60. 58.4 63.1 66.7 70.0 72.0 74.3 75.3 3.2.3 MACHINELEARNINGLM VS. TABULA-8B Thanks to the token-efficient prompting, MACHINELEARNINGLM supports an order of magnitude more demonstrations per prompt than TabuLa-8B  (Table 3)  , which is typically capped by an 8k context (often 2032 shots). In contrast, we scale stably to 1,024 shots for most tasks while maintaining significant many-shot gains, as shown in Appendix B. In the few-shot regime (e.g., 32-shot), MACHINELEARNINGLM also outperforms TabuLa-8B on averageeven though TabuLa explicitly exploits feature names with particularly large gaps on certain datasets (e.g., 14.5% shortfall for vehicle reported by TabuLa). 3.2.4 COMPETITIVE MANY-SHOT PERFORMANCE VS. STATE-OF-THE-ART TABULAR-SPECIFIC METHODS Without any task-specific training, MACHINELEARNINGLM reaches random-forestlevel accuracy across =8512 shotstypically within 2% relative  (Table 2)  and, even at 512 shots, outperforms random forest on roughly 30% of tasks  (Table 3)  . It also clearly surpasses simple instancebased learners (e.g., kNN) by > 4% relative on average, indicating robust numerical modeling (Table 2; Table 3). 3.2.5 MACHINELEARNINGLM VS. TABULAR ICL MODELS (TABICL / TABPFN) As Table 2 shows, compared to state-of-the-art tabular ICL models like TabICL, MACHINELEARNINGLM can be modestly behind at very high shot countslargely due to LLM compute requirements. Unlike tabular architectures with row/column attention and specialized tokenizer (Hollmann et al.; Qu et al., 2025; Su et al., 2024; Wang et al., 2021), MACHINELEARNINGLM uses general-purpose LLM backbone yet remains order-robust: permuting in-prompt demonstrations leaves performance unchanged on average. Importantly, our method is LLM-compatible by design and thus uniquely positioned to exploit external knowledge, heterogeneous/multimodal inputs, and reasoning-style alignment (e.g., CoT), offering practical path to close the remaining gap while retaining broad capabilities. 3.2.6 MACHINELEARNINGLM IN GENERAL CHAT WORKFLOWS As shown in Table 4, general abilities are well preserved. On MMLU, our model achieves 73.2% (0-shot) and 75.4% (50-shot), comparable to strong general-purpose LLMs. Notably, we observe consistent gains in numeracy-heavy subjects (e.g., high-school statistics/conceptual physics). Although training LLMs on real tables, TabuLa-8B underperforms on MMLU (by 10%), underscoring that our training strategy of reusing the model architecture and tokenizer integrates smoothly with general LLM capabilities and remains compatible with future multimodal/heterogeneous extensions."
        },
        {
            "title": "Work in Progress",
            "content": "Table 3: Per-task test accuracy (%) with {32, 512} shots. Abbreviations: Tabula = TabuLa-8B; Qwen = Qwen-2.5-7B-Instruct; Ours = MACHINELEARNINGLM (base: Qwen-2.5-7B-Instruct). To reduce memorization risks (Bordt et al., 2024), all methods drop feature names/descriptions and encode only values except for TabuLa-8B, because we adopt the papers results on the overlapping datasets, using the setting that keeps the original header names. EL indicates the prompt exceeds TabuLa-8Bs token limit; NA indicates the dataset is not included in TabuLa-8Bs evaluation set. Task # Shots KNN RF TabICL Tabula GPT-5-mini o3-mini Qwen Ours Bank Bank BLE_RSSI_Indoor_Loc. BLE_RSSI_Indoor_Loc. Churn Churn CMC CMC Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_9_5GHz Contaminant_9_5GHz Credit_g Credit_g FICO_HELOC_Cleaned FICO_HELOC_Cleaned FOREX_AUDCHF_Day FOREX_AUDCHF_Day FOREX_AUDJPY_Day FOREX_AUDJPY_Day GAMETES_Heterog. GAMETES_Heterog. HELOC HELOC KC1 KC1 LED24 LED24 LED7 LED7 Maternal_Health_Risk Maternal_Health_Risk PC1 PC1 Phoneme Phoneme Pima_Indians_Diabetes Pima_Indians_Diabetes RingNorm RingNorm RL RL Segment Segment Seismic_Bumps Seismic_Bumps Statlog Statlog Thyroid Thyroid TwoNorm TwoNorm Vehicle Vehicle WallRobot_Navigation WallRobot_Navigation Waveform Waveform Wine Wine Yeast Yeast 84.4 EL NA EL 91.4 EL 35.2 EL NA EL NA EL 70.3 EL NA EL EL EL NA EL NA EL EL EL 82.0 EL NA EL EL EL NA EL 89.8 EL 73.4 EL 70.3 EL NA EL NA EL EL EL NA EL NA EL NA EL NA EL 48.4 EL NA EL NA EL NA EL NA EL 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 32 512 87.3 87.6 33.7 35.1 84.6 86.6 45.1 52.2 69.4 78.8 71.9 79.4 64.0 70.0 59.0 64.3 50.7 44.4 45.5 57.8 47.5 57.8 59.8 65.6 81.8 82.5 30.8 55.8 55.9 68.8 45.8 74.4 92.3 91.9 74.4 83.0 64.9 68.8 53.8 58.6 56.6 60.7 63.6 88.1 93.2 92.8 61.0 67.0 92.6 94.0 94.4 97.0 52.3 64.7 51.4 73.4 74.1 81.6 63.8 65.6 43.1 55.6 87.6 89.0 61.5 69.2 86.5 91.1 45.8 50.8 72.5 85.6 71.0 84.6 69.0 76.0 64.7 71.5 52.3 52.6 46.6 61.3 51.3 59.7 66.0 71.3 79.9 82.7 41.9 69.4 57.7 68.8 52.7 78.3 92.3 93.7 75.3 84.9 74.7 75.3 79.6 93.5 58.5 66.7 68.2 90.9 92.7 92.8 65.0 74.0 92.8 98.0 88.8 95.9 60.0 74.7 74.2 96.7 72.7 82.7 64.6 69.7 41.8 58. 88.3 89.4 61.7 73.5 86.6 92.0 46.1 55.6 73.3 92.1 71.0 89.2 72.0 76.5 68.3 71.3 51.2 71.1 49.0 72.8 46.3 71.3 58.7 72.3 78.4 84.4 46.4 71.7 58.8 70.3 60.1 80.0 90.1 94.1 79.9 84.6 66.9 74.7 84.2 97.0 55.8 68.5 61.0 93.5 93.0 93.0 64.0 72.5 93.3 98.8 95.9 97.6 70.0 84.7 70.1 94.1 76.0 85.1 68.7 75.3 41.8 59.9 13 85.4 87.8 59.4 61.4 82.1 86.1 42.7 41.0 60.8 60.4 55.0 53.1 70.0 71.5 54.6 54.7 48.8 51.2 46.6 54.2 52.2 52.2 49.5 51.4 78.9 81.9 8.3 9.1 55.9 66.6 41.4 73.9 88.7 93.3 71.5 69.6 69.5 62.3 66.2 74.5 53.5 52.7 60.4 68.6 91.3 90.7 60.0 58.0 92.3 92.6 88.2 87.6 47.1 41.2 43.0 48.9 44.2 32.4 61.6 56.2 36.7 42.8 85.3 86.7 61.2 58.4 79.1 83.6 41.4 42.7 53.3 49.4 53.8 48.5 63.5 68.0 57.6 52.6 47.9 49.0 46.3 47.4 48.8 49.1 51.8 51.0 75.8 78.7 14.5 12.3 55.5 57.3 52.7 63.0 88.3 89.2 71.9 64.3 61.0 68.2 75.9 75.3 51.8 54.8 51.5 52.0 86.3 86.5 61.0 63.0 91.2 92.5 96.0 93.1 24.7 27.1 35.8 35.8 40.5 33.0 64.6 56.4 40.4 36.7 87.2 88.1 44.7 52.7 85.4 86.2 37.6 50.2 55.6 62.7 53.5 59.4 58.5 71.0 54.0 52.6 49.9 54.5 45.8 49.3 48.1 49.7 52.1 53.0 81.8 83.4 13.1 11.7 32.8 48.4 45.3 59.6 85.1 93.2 67.0 68.7 65.6 70.8 69.2 69.3 53.9 51.8 32.0 52.2 92.5 93.0 57.5 63.0 92.6 92.6 62.9 74.9 32.9 32.4 42.0 45.2 38.6 43.5 52.0 56.2 33.7 32.3 87.3 88.7 63.7 70.3 86.7 89.5 44.1 50.2 71.7 82.3 73.8 80.4 71.5 71.5 63.1 69.0 51.2 49.6 46.9 62.4 49.1 52.2 62.5 70.8 78.4 83.9 15.3 48.4 58.4 66.9 50.7 78.3 88.7 93.2 73.5 82.0 71.4 75.3 93.2 96.0 57.1 64.1 64.7 87.2 90.3 90.7 63.0 67.0 92.8 95.0 95.0 97.5 62.9 72.9 60.1 83.3 72.5 84.0 64.8 71.0 41.8 59."
        },
        {
            "title": "Work in Progress",
            "content": "Table 4: MMLU results for MACHINELEARNINGLM and baselines. (a) Macro accuracy across k-shot; (b) per-subject accuracies at 50-shot. Number of shots Qwen-2.5-7B-Instruct Qwen-2.5-7B TabuLa-8B MACHINELEARNINGLM 0 10 50 73.5 75.5 75.4 (a) Macro accuracy on the full MMLU benchmark across different k-shot settings at temperature = 0. TabuLa8B (Gardner et al., 2024) uses maximum 8k-token length. 61.6 65.2 N/A 73.2 75.1 75. 73.8 75.9 75.8 (b) Per-subject accuracies at 50-shot. Subject Qwen-2.5-7B-Inst. Qwen-2.5-7B TabuLa-8B MACHINELEARNINGLM high_school_statistics high_school_physics astronomy college_math college_physics conceptual_physics econometrics elementary_math high_school_math 73.6 62.9 86.8 47.0 53.9 74.0 69.3 72.8 55. 69.9 55.6 86.2 54.0 61.8 74.0 64.9 74.1 56.3 59.3 47.7 68.4 32.0 53.9 62.1 49.1 42.9 32.6 74.1 61.6 85.5 49.0 57.8 76.2 65.8 71.4 55.2 All accuracies (%) are reported using 50 shots, except for TabuLa-8B, which suggests maximum sequence length of 8k and therefore uses 20-shot setting. Models are evaluated with temperature = 0.05, votes=3. 3.3 EMPIRICAL STUDY Numeric-dominant tables. On pure numeric tables (e.g., churn, maternal_health_risk, waveform, twonorm, vehicle, yeast, heloc), our model remains competitive, indicating that the same prompt design handles both modalities without architecture changes. Heterogeneous (text+numeric) tables. Our tabular encoding natively mixes free-text/categorical fields with numberswithout mandatory text bucketing or learned text embeddingsyielding such as bank, reliable gains on heterogeneous adult, credit-g, online_shoppers, Bank_Customer_Churn_Dataset, and okcupid_stem, MACHINELEARNINGLM consistently outperforms vanilla LLMs. By contrast, for highly symbolic/abstract textual fields that behave more like token sequences than NL (e.g., DNA base strings in the UCI splice dataset), MACHINELEARNINGLM falls short relative to numerical modeling methods such as Random Forests. On mixed-feature datasets schemas. High-cardinality labels. On tasks with many classes (e.g., kropt, letter (26-way), walking_activity), MACHINELEARNINGLM underperforms strong tabular baselines (RF/TabICL). We attribute this to pretraining that sampled tasks with 10 classes, biasing the decoder toward small label vocabularies; when > 10, labels may become multi-token (e.g., 12). Expanding in synthesis is promising mitigation. Resilience to class imbalance. Across the task suite, MACHINELEARNINGLM remains stable under skewed label distributions: its many-shot accuracy tracks Random-Forestlevel performance while avoiding collapse-to-majority. On notably imbalanced datasetse.g., bank, pc1, and kc1it sustains competitive accuracy. Limits on forecasting-style tasks. On time-series/forecasting tasksparticularly the FOREX variants (FOREX task series)we observe significant declines relative to tabular methods. This gap is expected: our pretraining targets i.i.d. tabular prediction, whereas forecasting requires temporal inductive bias (lagged context, trend/seasonality priors) and higher numeric precision. We view this as an opportunity: future work will incorporate time-aware strategy to better align MACHINELEARNINGLM with forecasting workloads. More limitations are detailed in Section 5."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Many-shot ICL and scaling laws. growing literature observes that simply adding more demonstrations often yields diminishing or even negative returns in LLMse.g., one or few high-quality demonstrations capture most gains (Chen et al., 2023; Zou et al., 2025a; Min et al., 2022), longcontext usage is position-sensitive such as lost in the middle (Liu et al., 2024a; An et al., 2025; Xiong et al., 2024), and many-shot gains can plateau in multimodal VLMs such as Flamingo beyond 32 shots (Alayrac et al., 2022; Tai et al., 2024). Recent efforts improve many-shot ICL via specialized training or mechanisms: DeepMinds study shows task-dependent trends and proposes reinforced/unsupervised ICL variants (Agarwal et al., 2024) to mitigate many-shot scaling; DrICL reweights demonstrations and modifies objectives to mitigate plateauing (Zhang et al., 2025); semisupervised many-shot ICL mitigates many-shot scaling when using self-generated annotations (Gu et al., 2025); multimodal works report mixed but often dataset-specific monotonic gains with longer contexts (Jiang et al., 2024). Moreover, long-context evaluation disentangles retrieval-like tasks from global context understanding, with many models degrading on the latter at 16k tokens (Zou et al., 2025b). TabDPT reports scaling laws with respect to both model size and pretraining-corpus size Ma et al. (2024). Unlike these approaches, MACHINELEARNINGLM induces many-shot ICL through pretraining on millions of synthetic tabular tasks, yielding monotonic example-count scaling while preserving the general reasoning of the base LLM. LM-based ML learner. Compared with TabLLM (Hegselmann et al., 2023)an approach task-specific fine-tuning and hyper-parameter tunthat demands computationally intensive, ingMACHINELEARNINGLM adapts to new tabular tasks purely through ICL at inference time, with no gradient updates. Crucially, it delivers markedly stronger reasoning over large volumes of numerical data, long-standing weakness of TabLLM 5. Most relevant to us, (Gardner et al., 2024) proposes TabuLa-8Bfinetuning Llama 3-8B on web-scale corpus (T4) distilled from TabLib, and introducing row-causal tabular masking (RCTM) scheme that packs samples by table and encourages few-shot behavior (Gardner et al., 2024). Their main evaluations focus on [0, 32] shots. Our synthetic pretraining complements real-world data pretraining and differs in three ways: (i) we continue pretraining general-purpose LLM without architectural modifications without losing the backbones general capabilities, (ii) we pretrain on synthetic, SCM-driven tabular tasks with explicitly controlled diversity, ensuring no dataset leakage from downstream evaluations, and (iii) we target many-shot ICL behavior on tabular tasks (1,024 demonstrations under fixed context budget) using our proposed token-efficient method. In-context tabular ML learners. TabPFN frames tabular classification as ICL with pre-trained transformer hypernetwork and achieves strong few-shot accuracy on small tables (Hollmann et al., 2022). TabICL scales PFN-style ICL to much larger tables via two-stage, column-then-row architecture and synthetic pretraining (Qu et al., 2025). However, these tabular ICL models are trained independently of language modelsincompatible with LM architectures or checkpointsso they lack general text reasoning and open-domain knowledge, which limits application when textual fields carry meaning and constrains performance on multimodal (textnumeric) tasks. In contrast, MACHINELEARNINGLM is built upon pre-trained LLM backbone and therefore combines robust numeric processing with the ability to interpret textual headers, free-text cells, and world knowledge priors, offering unified foundation model for heterogeneous, real-world in-context ML tasks. Tool-using agents for ML vs. in-context learners. complementary line of work evaluates or builds LLM agents that call external ML toolchains (e.g., LightGBM/XGBoost/CatBoost, AutoML) to solve end-to-end ML engineering tasks, as in MLE-Bench (Chan et al., 2024), MLAgentBench (Huang et al., 2023), R&D-Agent (Yang et al., 2025), ML-Master (Anonymous, 2025), and systems targeting Kaggle-style workflows (Anonymous, 2024). However, their performance is bound by the invoked learners and pipelines. In contrast, MACHINELEARNINGLM explores way to internalize the learning procedure: the model performs in-context prediction, requiring neither per-task fine-tuning nor calls to external ML models, and MACHINELEARNINGLM can also be called by the MLE agent as tool. 5https://gael-varoquaux.info/science/carte-toward-table-foundation-models. html"
        },
        {
            "title": "5 LIMITATIONS",
            "content": "While MACHINELEARNINGLM shows strong in-context ML performance, several limitations remain. Task scope. Our pretraining corpus focuses on tabular classification synthesized from SCMs. We do not yet cover regression, ranking, timeseries forecasting, or structured prediction, and we cap the number of classes (K 10). Extending beyond IID rows (e.g., temporal or relational dependencies) is future work. Context length and compute. Continued pretraining used 32k-token context; although inference generalizes beyond this (tested up to 131k tokens), scaling to many thousands of shots like TabPFN/TabICL remains challenging due to compute and memory costs. Numerical encoding trade-offs. The [0, 999] integer mapping preserves order but coarsens magnitudes and can obscure semantically meaningful constants (e.g., age 18). As context windows grow, more expressive number encodings can be explored to retain such cues while remaining token-efficient. Warm-start bias. The RF-mimic warm-up and example-level consensus filtering may bias early training toward tree-like decision boundaries and easy examples. Although we disable consensus after warm-up, some inductive bias may persist; we currently distill labels only (not rationales). How to best leverage external teachers is still an open question. Model scale and adaptation. Results are with 7B backbone and low-rank adaptation (LoRA rank 8). Larger backbones, alternative adapters, or optimizer/regularization choices may further improve many-shot numeracy but were not explored here."
        },
        {
            "title": "6 FUTURE WORK",
            "content": "We hope MACHINELEARNINGLM will ignite new lines of research, as exemplified below. Feel free to contact us for further discussions . Beyond text and numbers: synthetic tasks with multimodal features. Starting from our synthetic tabular tasks, map numerical/categorical features to weakly labeled multimodal surrogatesshort texts, speech snippets, images, geospatial signals, or time seriesusing generative back-ends (LLM/VLM/TTS) conditioned on latent factors to preserve coarse class semantics. This produces controllable, cross-modal pairs aligned with the tabular schema 6 and enables MACHINELEARNINGLM to practice multimodal in-context prediction on heterogeneous signals. Longer contexts via parallelism and system optimizations. Extend context length using tensor/pipeline parallelism and memory-efficient attention/KV caching to support substantially more in-prompt examples. Recent work on cache-augmented architectures demonstrates that principled KV-cache management can significantly improve efficiency and scalability for long-context reasoning (Bhaskar et al., 2025), suggesting promising directions to integrate such mechanisms into MACHINELEARNINGLM. Toward interpretability: narrative distillation and reasoning-augmented learning. Each task in our pretraining corpus is generated from structural causal model (SCM), which we treat as an intrinsic source of explanation. We train the LM to narrate the underlying SCMvariables, relations, and mechanismsas an auxiliary objective aligned with the prediction task, thereby yielding more faithful rationales and stronger reasoning. Moreover, couple feature attribution signals (e.g., SHAP (Lundberg & Lee, 2017) / LIME (Ribeiro et al., 2016)) with MACHINELEARNINGLMs generation to produce faithful instanceand cohort-level narratives, while distilling rules from ensembles (e.g., inTrees (Deng, 2014) / RuleFit (Friedman & Popescu, 2008)) into compact, human-readable reasoning traces. Recent advances such as DeepSeek-R1 (DeepSeek-AI, 2025) demonstrate the promise of incentivizing reasoning capability in LLMs through RL. This integration promises not only accurate predictions but also transparent rationales, enhancing compliance and trust. 6Table formats (e.g., HTML tables) naturally organize images, hyperlinks, and embedded objects."
        },
        {
            "title": "Work in Progress",
            "content": "Uncertainty-aware responses. Recent analyses from OpenAI argue that training/evaluation schemes which penalize uncertainty and implicitly reward guessing are root cause of hallucinations 7. We suspect similar incentive mismatch in our setting. Future work may (i) train on teacher predictive distributionse.g., randomized/ensemble teachers that provide calibrated confidencesinstead of hard labels, optimized with proper scoring rules (NLL/Brier) and selective-prediction losses that allow abstention; (ii) explicitly represent and reward well-calibrated uncertainty in the output schema (e.g., an UNCERTAIN/defer option with riskcoverage evaluation); and (iii) directly optimize task-level metrics (AUROC, RMSE, F1) via reinforcement learning or differentiable surrogates, with robust reward shaping (e.g., clipped/Huberized or quantile-based rewards) to reduce outlier sensitivity and improve end-to-end utility. Combined with Retrieval-Augmented Methods. Retrieval-augmented method has shown that attaching retrieval module to LLMs enables scalable any-shot learning and yields power-law error reductions as data grows (Wen et al., 2025). Integrating such retrieval into MACHINELEARNINGLM can boost many-shot example limits by dynamically injecting the most relevant examples during pretraining and inference. Real-data alignment via continued fine-tuning. Beyond purely synthetic pretraining, we will perform lightweight continued fine-tuning on curated suite of real tabular prediction tasks to better align with practical distributions. Extending MACHINELEARNINGLM to fully leverage Agent Memory. Recent advances leverage memory mechanisms so that agents can recall and reuse successful or similar past experiences to inform their next action (Wang et al., 2023; Lin et al., 2025; Wang et al., 2024). Such experiences may be textual traces, numerical features, or temporal signals. By pretraining the policy model with many-shot in-context experiences, the agent can improve decision-making and robustness in dynamic environments. Integration with MLE agents. Fuse MACHINELEARNINGLM with MLE-agentstyle planners to orchestrate end-to-end MLE workflows. The agent treats MACHINELEARNINGLM both as solver and as an internal tool, enabling flexible interface with heterogeneous real-world MLE tasks."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced MACHINELEARNINGLM, portable continued-pretraining recipe that equips general-purpose LLM with robust in-context tabular prediction without architectural or tokenizer changes. By synthesizing millions of SCM-driven tasks and using Random-Forest warm start, the model acquires strong numerical modeling while preserving general knowledge and reasoning. token-efficient tabular encoding, integer-based number normalization, and sequence-level batchprediction together expand the effective context budget and amortize prompt overhead. Across diverse out-of-distribution classification tasks, MACHINELEARNINGLM remarkably improves over its backbone and approaches random-forestlevel accuracy from 8 to 512 shots; accuracy further increases up to 1,024 shots and is robust to exemplar-order permutations. These results indicate that targeted continued pretraining on synthetic prediction tasks is practical path to scaling many-shot ICL within general-purpose LLMs. Finally, we discuss the current limitations and chart several promising avenues for future work."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "Conceptualization (LLM continued pretraining with SCM-based synthesis, RF-mimic warm-start, token-efficient prompting, and order-robust self-consistency): Haoyu Dong. Methodology: Pretraining recipe and objectives Haoyu Dong, Pengkun Zhang; SCM task synthesis Pengkun Zhang, Haoyu Dong; RF-mimic warm-start (gating and consensus filtering) Haoyu Dong; token-efficient prompting Haoyu Dong; self-consistency Yanzhen Shen, Mingzhe Lu, Haoyu Dong. 7https://openai.com/index/why-language-models-hallucinate/"
        },
        {
            "title": "Work in Progress",
            "content": "Software & Engineering: Pretraining pipeline and training scripts Pengkun Zhang, Haoyu Dong; evaluation harness Yanzhen Shen, Mingzhe Lu, Haoyu Dong; SCM generator and data pipelines Pengkun Zhang, Haoyu Dong; token-efficient prompting utilities Haoyu Dong; code optimization, cross-platform support, and parallelization Mingzhe Lu, Pengkun Zhang. Investigation & Validation: Experiments and statistical aggregation of results Mingzhe Lu, Pengkun Zhang, Haoyu Dong. Writing: Original draft (main text, tables, and figures) Haoyu Dong; review and editing Yanzhen Shen, Pengkun Zhang, Mingzhe Lu, Guolin Ke. Limitations & Future Work: Haoyu Dong, Guolin Ke. ML Expertise: High-level guidance on ML practice, interpretability, and online/continual learning directions Guolin Ke. Moreover, we thank Hui Xue for suggestions on method and evaluation, including adding the MMLU benchmark and future directions that leverage random-forest predictive probabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Rohan Agarwal et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. URL https://arxiv.org/abs/2404.11018. Jean-Baptiste Alayrac et al. Flamingo: visual language model for few-shot learning. NeurIPS, URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf. See supplemental: performance plateaus beyond 32 shots. Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of LLMs fall short? In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id= eoln5WgrPx. ICLR 2025 Poster. Anonymous. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. arXiv preprint arXiv:2411.03562, 2024. URL https://arxiv.org/abs/2411. 03562. Anonymous. Ml-master: Towards ai-for-ai via integration of exploration and reasoning. arXiv preprint arXiv:2506.16499, 2025. URL https://arxiv.org/abs/2506.16499. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, and Danqi Chen. Cache me if you can: How many kvs do you need for effective long-context lms? arXiv preprint arXiv:2506.17121, 2025. Sebastian Bordt, Harsha Nori, Vanessa Cristiny Rodrigues Vasconcelos, Besmira Nushi, and Rich Caruana. Elephants never forget: Memorization and learning of tabular data in large language models. arXiv preprint arXiv:2403.06644, 2024. URL https://arxiv.org/abs/2403. 06644. Leo Breiman. Random forests. Machine Learning, 45(1):532, 2001. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020. URL https://arxiv.org/abs/2005.14165. Jun Shern Chan et al. Mle-bench: Evaluating machine learning agents on real-world competitions. arXiv preprint arXiv:2410.07095, 2024. URL https://arxiv.org/abs/2410.07095."
        },
        {
            "title": "Work in Progress",
            "content": "Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. How many demonstrations do you need for in-context learning? In Findings of EMNLP 2023, pp. 1114911159, 2023. URL https: //aclanthology.org/2023.findings-emnlp.745/. Tianqi Chen and Carlos Guestrin. Xgboost: scalable tree boosting system. In KDD, pp. 785794, 2016. Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis. arXiv preprint arXiv:2301.08721, 2023. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. doi: 10.48550/arXiv.2210.11416. URL https://arxiv.org/abs/2210.11416. Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):2127, 1967. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. Houtao Deng. Interpreting tree ensembles with intrees. arXiv preprint arXiv:1408.5456, 2014. Haoyu Dong, Jianbo Zhao, Yuzhang Tian, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, José Cambronero, Yeye He, Shi Han, et al. Spreadsheetllm: encoding spreadsheets for large language In Proceedings of the 2024 Conference on Empirical Methods in Natural Language models. Processing, pp. 2072820748, 2024. Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. Mitigating label biases for in-context learning. arXiv preprint arXiv:2305.19148, 2023. Jerome H. Friedman and Bogdan E. Popescu. Predictive learning via rule ensembles. The Annals of Applied Statistics, 2(3):916954, 2008. doi: 10.1214/07-AOAS148. Josh Gardner, Juan C. Perdomo, and Ludwig Schmidt. Large scale transfer learning for tabular data via language modeling. arXiv preprint arXiv:2406.12031, 2024. URL https://arxiv.org/ abs/2406.12031. NeurIPS 2024 camera-ready. Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, Bruno RégaldoSaint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, and Shirley Ho. xval: continuous numerical tokenization for scientific language models. arXiv preprint arXiv:2310.02989, 2024. doi: 10.48550/arXiv.2310.02989. URL https://arxiv.org/abs/2310.02989. v2 (Dec 2024). Zhengyao Gu, Henry Peng Zou, Yankai Chen, Aiwei Liu, Weizhi Zhang, and Philip S. Yu. ScalarXiv preprint ing laws for many-shot in-context learning with self-generated annotations. arXiv:2503.03062, 2025. URL https://arxiv.org/abs/2503.03062. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 206 of Proceedings of Machine Learning Research, 2023. URL https://proceedings.mlr.press/ v206/hegselmann23a.html. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. doi: 10.48550/arXiv.1503.02531. URL https://arxiv. org/abs/1503.02531."
        },
        {
            "title": "Work in Progress",
            "content": "Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. Tabpfn: transformer that solves small tabular classification problems in second. In The Eleventh International Conference on Learning Representations. Noah Hollmann, Samuel Müller, and Sebastian Müller. Tabpfn: transformer that solves small tabular classification problems in second. arXiv preprint arXiv:2207.01848, 2022. URL https://arxiv.org/abs/2207.01848. Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with tabular foundation model. Nature, 637(8045):319326, 2025. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302, 2023. URL https: //arxiv.org/abs/2310.03302. Hugging Face. Number tokenization blog, 2024. URL https://huggingface.co/spaces/ huggingface/number-tokenization-blog. Hugging Face Spaces; last updated Nov 29, 2024. Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H. Chen, and Andrew Y. Ng. Many-shot in-context learning in multimodal foundation models. arXiv preprint arXiv:2405.09798, 2024. doi: 10.48550/arXiv.2405.09798. URL https://arxiv.org/ abs/2405.09798. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and TieYan Liu. Lightgbm: highly efficient gradient boosting decision tree. In NeurIPS, 2017. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less. arXiv preprint arXiv:2309.00384, 2024. URL https://arxiv.org/abs/2309. 00384. Published as conference paper at ICLR 2024. Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, et al. Se-agent: Self-evolution trajectory optimization in multi-step reasoning with llm-based agents. arXiv preprint arXiv:2508.02085, 2025. Nick Littlestone and Manfred Warmuth. The weighted majority algorithm. Information and computation, 108(2):212261, 1994. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the ACL, 2024a. URL https://aclanthology.org/2024.tacl-1.9/. Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and Han-Jia Ye. Talent: tabular analytics and learning toolbox. arXiv preprint arXiv:2407.04057, 2024b. Scott Lundberg and Su-In Lee. unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. Junwei Ma, Valentin Thomas, Rasa Hosseinzadeh, Hamidreza Kamkari, Alex Labach, Jesse Cresswell, Keyvan Golestan, Guangwei Yu, Maksims Volkovs, and Anthony Caterini. Tabdpt: Scaling tabular foundation models. arXiv preprint arXiv:2410.18164, 2024. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1104811064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.759. URL https://aclanthology.org/2022.emnlp-main.759/."
        },
        {
            "title": "Work in Progress",
            "content": "Martin Mráz, Breenda Das, Anshul Gupta, Lennart Purucker, and Frank Hutter. Towards benchmarking foundation models for tabular data with text. arXiv preprint arXiv:2507.07829, 2025. Mateusz Ochal, Massimiliano Patacchiola, Jose Vazquez, Amos Storkey, and Sen Wang. Few-shot IEEE Transactions on Artificial Intelligence, 4(5):13481358, learning with class imbalance. 2023. OpenAI. Openai changelog: o3-mini released. https://platform.openai.com/docs/ changelog, 2024. Lists the o3-mini reasoning model. OpenAI. Openai o3 and o4-mini system card. https://openai.com/systems/o3/, 2025. System card covering the o3 family and o4-mini. Judea Pearl. Causality. Cambridge university press, 2009. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT press, 2017. Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: Unbiased boosting with categorical features. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, 2018. Xiaoyu Qu, Yan He, Le Gao, and et al. Tabicl: Scaling in-context learning for tabular data with synthetic pretraining. arXiv preprint arXiv:2501.XXXXX, 2025. URL https://arxiv.org/ abs/2501.XXXXX. Accessed 2025. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 11351144, 2016. Xingjian Shi, Jonas Mueller, Nick Erickson, Mu Li, and Alexander Smola. Benchmarking multimodal automl for tabular data with text fields. arXiv preprint arXiv:2111.02705, 2021. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Aaditya K. Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs. arXiv preprint arXiv:2402.14903, 2024. doi: 10.48550/arXiv.2402.14903. URL https://arxiv.org/abs/2402.14903. Georgios Spithourakis and Sebastian Riedel. Numeracy for language models: Evaluating and improving their ability to predict numbers. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 21042115, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1196. URL https://aclanthology.org/P18-1196/. Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Gang Chen, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, et al. Tablegpt2: large multimodal model with tabular data integration. arXiv preprint arXiv:2411.02059, 2024. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can large language models understand structured table data? benchmark and empirical study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining (WSDM), 2024. doi: 10.1145/3616855.3635752. Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, and Ziwei Liu. Link-context learning for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern URL https://openaccess.thecvf.com/content/ Recognition (CVPR), 2024. CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_ CVPR_2024_paper.pdf."
        },
        {
            "title": "Work in Progress",
            "content": "Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. Do NLP models In Proceedings of the 2019 Conference know numbers? probing numeracy in embeddings. on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 53075315, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1534. URL https://aclanthology.org/D19-1534/. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b. doi: 10.48550/arXiv.2203.11171. URL https://arxiv.org/abs/2203.11171. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, Xudong Shen, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on In Proceedings of the 2022 Conference on Empirical Methods in Natural 1600+ nlp tasks. Language Processing, pp. 50855109, Abu Dhabi, United Arab Emirates, December 2022c. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL https://aclanthology.org/2022.emnlp-main.340/. Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. Tuta: Treebased transformers for generally structured table pre-training. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 17801790, 2021. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024. Jiapeng Wen, Jiashuo Zhao, Jinhao Zhang, Xinyu Tang, Yichong Xu, and Jie Tang. Scalable incontext learning on tabular data via retrieval-augmented large language models. arXiv preprint arXiv:2502.03147, 2025. URL https://arxiv.org/abs/2502.03147. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 46434663, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. naacl-long.260. URL https://aclanthology.org/2024.naacl-long.260/. Xinyu Yang et al. R&d-agent: Automating data-driven ai solution building with dual agents. arXiv preprint arXiv:2505.14738, 2025. URL https://arxiv.org/abs/2505.14738. Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. closer look at deep learning methods on tabular datasets. arXiv preprint arXiv:2407.00956, 2024. Xiaoqing Zhang et al. More is not always better? enhancing many-shot in-context learning with In Proceedings of ACL 2025 (Long Papers), 2025. differentiated and reweighting objectives. URL https://aclanthology.org/2025.acl-long.1475.pdf."
        },
        {
            "title": "Work in Progress",
            "content": "Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pp. 1269712706. PMLR, 2021. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models, 2024. URL https: //arxiv.org/abs/2403.13372. Kaijian Zou, Muhammad Khalifa, and Lu Wang. On many-shot in-context learning for long-context evaluation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2560525639, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1245. URL https://aclanthology.org/2025.acl-long.1245/. Kaijian Zou, Muhammad Khalifa, and Lu Wang. Retrieval or global context understanding? on many-shot in-context learning for long-context evaluation. In Proceedings of ACL 2025 (Long Papers), 2025b. URL https://aclanthology.org/2025.acl-long.1245/."
        },
        {
            "title": "A LIST OF SYMBOLS",
            "content": "Symbol Meaning Range/Default xv fv εv K ˆℓi tok() θ L(θ) z p0 Ncorrect α κ δbacc δF1 τdom pi(yj) p(yj) i=1 DAG of the structural causal model (SCM) Node index in Variable value at node Structural function at node i.i.d. noise term # in-context demonstrations (shots) # query/test rows predicted per batched sequence Feature dimension (columns) # classes Demonstration set {si}M i=1 Query set {x(i)}N Target JSON array of predictions Predicted label for query Tokenizer Target token length Model parameters Training loss (NLL) Token cost of the instruction/schema header Per-row token cost (demo or query) Amortized token cost per predicted label z-score normalized numeric value Integer after [0, 999]-norm mapping Conservative random baseline accuracy # correct predictions by RF per batched sequence One-sided binomial test threshold Cohens κ (chance-corrected agreement) Balanced-accuracy margin Macro-F1 margin Max dominant-class fraction # shuffled prompt variants Next-token prob. for label yj under variant Pi Aggregated prob.: (cid:80)V i=0 pi(yj) 1024 50 5 50 2 10 0.2 > 0.01 0.03 0.00 0.95"
        },
        {
            "title": "Work in Progress",
            "content": "B MANY-SHOT SCALING PER-TASK EVALUATION Table 6: Per-dataset test accuracy from 8 to 1,024 shots on more sampled domain-representative datasets in Talent. Abbreviations: Qwen-2.5-7B = Qwen-2.5-7B-Instruct; Our = MACHINELEARNINGLM (base: Qwen-2.5-7B-Instruct). Dataset Shots KNN RF Qwen-2.5-7B Our bank bank bank bank bank bank bank bank BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization BLE_RSSI_dataset_for_Indoor_localization churn churn churn churn churn churn churn churn cmc cmc cmc cmc cmc cmc cmc cmc Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_10_0GHz Contaminant_9_5GHz Contaminant_9_5GHz Contaminant_9_5GHz Contaminant_9_5GHz Contaminant_9_5GHz Contaminant_9_5GHz Contaminant_9_5GHz Contaminant_9_5GHz credit_g credit_g credit_g credit_g credit_g credit_g credit_g 0.8657 0.8723 0.8725 0.8764 0.8778 0.8757 0.8762 0.8801 0.3385 0.3525 0.3370 0.3360 0.3400 0.3460 0.3515 0.3375 0.8260 0.8530 0.8460 0.8660 0.8650 0.8660 0.8660 0.8680 0.3661 0.4000 0.4508 0.4577 0.4441 0.4848 0.5220 0.4882 0.5479 0.6792 0.6937 0.7084 0.7354 0.7333 0.7875 0.8042 0.5354 0.6542 0.7188 0.7188 0.7354 0.7584 0.7937 0.8375 0.6050 0.6750 0.6400 0.6450 0.6750 0.6700 0.7000 0.8695 0.8759 0.8764 0.8828 0.8847 0.8873 0.8897 0.8947 0.5167 0.5884 0.6154 0.6560 0.6960 0.6865 0.6915 0.6955 0.8460 0.8660 0.8650 0.8710 0.8760 0.8940 0.9110 0.9330 0.3492 0.3492 0.4576 0.4847 0.4881 0.5119 0.5085 0.5017 0.5750 0.6625 0.7250 0.7375 0.8083 0.8292 0.8562 0.8750 0.6313 0.6646 0.7104 0.7500 0.7834 0.8208 0.8458 0.8667 0.6900 0.6850 0.6900 0.6600 0.7400 0.7650 0.7600 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 25 0.8526 0.8666 0.8722 0.8780 0.8798 0.8803 0.8808 0.8795 0.3956 0.4382 0.4467 0.4567 0.4852 0.5153 0.5268 0.5238 0.7970 0.8550 0.8540 0.8630 0.8640 0.8640 0.8620 0.8670 0.3559 0.3492 0.3763 0.4034 0.4237 0.3966 0.4271 0.4508 0.5062 0.5479 0.5563 0.5833 0.5979 0.6146 0.6271 0.5708 0.5042 0.5396 0.5354 0.5750 0.5875 0.6208 0.5938 0.5583 0.6300 0.6650 0.5850 0.6100 0.6150 0.6750 0.7100 0.8704 0.8707 0.8727 0.8771 0.8830 0.8867 0.8870 0.8903 0.5328 0.5889 0.6375 0.6715 0.6950 0.7020 0.7031 0.7126 0.8430 0.8620 0.8670 0.8650 0.8680 0.8870 0.8950 0.8960 0.3356 0.3492 0.4407 0.4780 0.4746 0.4915 0.5017 0.4949 0.5708 0.6833 0.7167 0.7604 0.7688 0.8125 0.8230 0.8230 0.6250 0.6771 0.7375 0.7375 0.7563 0.7875 0.8042 0.8042 0.7050 0.7100 0.7150 0.6850 0.7200 0.7500 0.7150 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset credit_g FICO_HELOC_cleaned FICO_HELOC_cleaned FICO_HELOC_cleaned FICO_HELOC_cleaned FICO_HELOC_cleaned FICO_HELOC_cleaned FICO_HELOC_cleaned FICO_HELOC_cleaned FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audchf_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High FOREX_audjpy_day_High GAMETES_Heterogeneity GAMETES_Heterogeneity GAMETES_Heterogeneity GAMETES_Heterogeneity GAMETES_Heterogeneity GAMETES_Heterogeneity GAMETES_Heterogeneity GAMETES_Heterogeneity heloc heloc heloc heloc heloc heloc heloc heloc kc1 kc1 kc1 kc1 kc1 kc1 kc1 kc1 led24 led24 led24 led24 led24 led24 led24 led24 led7 led7 led7 led7 Table 6 continued from previous page Shots KNN RF Qwen-2.5-7B Our 0.6700 0.5458 0.5620 0.5899 0.6147 0.6258 0.6390 0.6430 0.6512 0.5150 0.5095 0.5068 0.5341 0.4932 0.4768 0.4441 0.4795 0.4986 0.4632 0.4550 0.4796 0.5041 0.5341 0.5777 0.5531 0.5281 0.4875 0.4750 0.4656 0.5125 0.5437 0.5781 0.5906 0.5315 0.5710 0.5975 0.6180 0.6375 0.6455 0.6555 0.6575 0.8033 0.8152 0.8175 0.8176 0.8199 0.8246 0.8247 0.8317 0.1484 0.2094 0.3078 0.3906 0.4703 0.5375 0.5578 0.5922 0.2734 0.4390 0.5594 0.6562 0.7900 0.5757 0.6192 0.6466 0.6795 0.7018 0.7043 0.7149 0.7271 0.5123 0.5150 0.5232 0.5286 0.5259 0.5041 0.5259 0.6240 0.4387 0.4850 0.4659 0.5068 0.5259 0.5395 0.6131 0.6185 0.5156 0.5219 0.5125 0.5250 0.4781 0.5812 0.5969 0.6343 0.5880 0.6190 0.6605 0.6695 0.6870 0.7100 0.7125 0.7075 0.8081 0.8199 0.7986 0.8080 0.8294 0.8223 0.8270 0.8484 0.2203 0.2938 0.4187 0.5625 0.6187 0.6656 0.6938 0.7000 0.3625 0.5141 0.5766 0.6578 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 0.6650 0.5175 0.5094 0.5403 0.5129 0.5215 0.5276 0.5256 0.5362 0.5150 0.5422 0.4986 0.5422 0.5450 0.5613 0.5450 0.5395 0.4496 0.4550 0.4578 0.4796 0.4959 0.4659 0.4932 0.4877 0.4875 0.4625 0.4813 0.4656 0.4906 0.4844 0.4969 0.4938 0.5310 0.5085 0.5210 0.5250 0.5105 0.5105 0.5300 0.5155 0.8104 0.8104 0.8175 0.8270 0.8341 0.8365 0.8341 0.8365 0.0969 0.0969 0.1313 0.1266 0.1172 0.1250 0.1172 0.1375 0.2281 0.2609 0.3281 0.3875 0.7600 0.5544 0.6253 0.6309 0.6785 0.6906 0.7089 0.6901 0.6112 0.5013 0.5014 0.5123 0.4932 0.5232 0.5422 0.4959 0.5150 0.4604 0.4796 0.4687 0.4823 0.5368 0.5340 0.6240 0.5422 0.5281 0.5531 0.4906 0.4687 0.4750 0.5281 0.5093 0.5969 0.5325 0.6060 0.6245 0.6640 0.6855 0.6955 0.7075 0.7115 0.7441 0.8223 0.7844 0.8247 0.8246 0.8341 0.8389 0.8413 0.1125 0.1406 0.1531 0.2469 0.3766 0.4766 0.4844 0.5125 0.3219 0.4828 0.5844 0.6344 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen-2.5-7B Our Table 6 continued from previous page led7 led7 led7 led7 maternal_health_risk maternal_health_risk maternal_health_risk maternal_health_risk maternal_health_risk maternal_health_risk maternal_health_risk maternal_health_risk pc1 pc1 pc1 pc1 pc1 pc1 pc1 pc1 phoneme phoneme phoneme phoneme phoneme phoneme phoneme phoneme Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database Pima_Indians_Diabetes_Database ringnorm ringnorm ringnorm ringnorm ringnorm ringnorm ringnorm ringnorm rl rl rl rl rl rl rl rl segment segment segment segment segment segment segment segment seismic_bumps 0.6875 0.6609 0.6875 0.7156 0.4089 0.4384 0.4582 0.5616 0.6503 0.6896 0.7439 0.8030 0.9280 0.9189 0.9234 0.9234 0.9325 0.9279 0.9189 0.9234 0.6337 0.6965 0.7437 0.7817 0.7909 0.8113 0.8298 0.8612 0.6623 0.6623 0.6493 0.6233 0.6818 0.7143 0.6883 0.7532 0.5284 0.5385 0.5378 0.5426 0.5540 0.5716 0.5858 0.6216 0.5181 0.5413 0.5664 0.5754 0.5946 0.5876 0.6067 0.6097 0.4481 0.5650 0.6364 0.7446 0.8247 0.8442 0.8810 0.8961 0.9226 0.6688 0.6453 0.6875 0.7110 0.4237 0.4877 0.5271 0.5961 0.6995 0.7439 0.7833 0.8276 0.9369 0.9189 0.9234 0.9324 0.9279 0.9459 0.9369 0.9369 0.6734 0.7280 0.7530 0.7835 0.7947 0.8252 0.8492 0.8650 0.6948 0.6818 0.7467 0.7272 0.7857 0.7533 0.7533 0.7468 0.5581 0.6912 0.7959 0.8615 0.8939 0.9169 0.9345 0.9358 0.5393 0.5523 0.5845 0.5905 0.5936 0.6328 0.6670 0.6861 0.4632 0.5390 0.6818 0.8095 0.8745 0.8961 0.9091 0.9156 0.9265 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 0.4437 0.4625 0.4844 0.5250 0.3744 0.4039 0.4532 0.4729 0.5123 0.5468 0.5961 0.5271 0.8964 0.8649 0.8514 0.9234 0.9234 0.9324 0.9324 0.9324 0.6281 0.6549 0.6698 0.6864 0.6864 0.6855 0.6873 0.6883 0.6429 0.6234 0.6558 0.6234 0.6818 0.6883 0.7078 0.7013 0.6034 0.6466 0.6919 0.7108 0.7358 0.7155 0.6932 0.7250 0.5070 0.5181 0.5392 0.5050 0.5060 0.4869 0.5181 0.5121 0.1970 0.2706 0.3203 0.3961 0.4113 0.4654 0.5216 0.4437 0.9110 0.6609 0.6422 0.6688 0.6438 0.4384 0.4975 0.5074 0.6108 0.6700 0.7438 0.7832 0.8424 0.9189 0.8874 0.8874 0.9279 0.9279 0.9369 0.9324 0.9280 0.6540 0.6920 0.7345 0.7484 0.7715 0.8021 0.8196 0.8390 0.6624 0.6883 0.7143 0.7338 0.7208 0.7338 0.7532 0.7402 0.7230 0.8987 0.9324 0.9520 0.9541 0.9628 0.9595 0.9696 0.5201 0.5332 0.5714 0.5804 0.5805 0.5966 0.6408 0.6278 0.4394 0.5779 0.6472 0.7684 0.8160 0.8571 0.8723 0.8853 0.9246 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen-2.5-7B Our Table 6 continued from previous page seismic_bumps seismic_bumps seismic_bumps seismic_bumps seismic_bumps seismic_bumps seismic_bumps statlog statlog statlog statlog statlog statlog statlog statlog thyroid thyroid thyroid thyroid thyroid thyroid thyroid thyroid twonorm twonorm twonorm twonorm twonorm twonorm twonorm twonorm vehicle vehicle vehicle vehicle vehicle vehicle vehicle vehicle wall_robot_navigation wall_robot_navigation wall_robot_navigation wall_robot_navigation wall_robot_navigation wall_robot_navigation wall_robot_navigation wall_robot_navigation waveform_database_generator_version_1 waveform_database_generator_version_1 waveform_database_generator_version_1 waveform_database_generator_version_1 waveform_database_generator_version_1 waveform_database_generator_version_1 waveform_database_generator_version_1 waveform_database_generator_version_1 wine wine wine wine wine wine 0.9226 0.9323 0.9265 0.9303 0.9246 0.9284 0.9265 0.5100 0.5400 0.6100 0.6350 0.6600 0.6850 0.6700 0.6800 0.9250 0.9257 0.9264 0.9278 0.9292 0.9361 0.9403 0.9438 0.5845 0.8979 0.9439 0.9547 0.9595 0.9649 0.9696 0.9723 0.3059 0.3824 0.5235 0.5824 0.5941 0.5706 0.6470 0.6529 0.4332 0.4716 0.5138 0.5604 0.6181 0.6722 0.7344 0.7985 0.4840 0.6330 0.7410 0.7910 0.7940 0.8120 0.8160 0.8220 0.6066 0.6066 0.6379 0.6477 0.6575 0.6947 0.9304 0.9265 0.9323 0.9303 0.9246 0.9284 0.9285 0.6150 0.6450 0.6500 0.6850 0.7050 0.7050 0.7400 0.7400 0.9132 0.9250 0.9277 0.9340 0.9444 0.9632 0.9798 0.9916 0.7155 0.8331 0.8885 0.9297 0.9446 0.9547 0.9588 0.9635 0.3353 0.4353 0.6000 0.6471 0.7059 0.7529 0.7471 0.7529 0.5220 0.5980 0.7417 0.8599 0.9130 0.9368 0.9670 0.9799 0.5540 0.6490 0.7270 0.7690 0.8190 0.7980 0.8270 0.8360 0.6184 0.6360 0.6458 0.6184 0.6791 0.7006 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 512 1,024 8 16 32 64 128 256 0.9284 0.9246 0.9304 0.9304 0.9304 0.9304 0.9304 0.5350 0.6000 0.5750 0.5750 0.6250 0.5900 0.6300 0.6300 0.9125 0.9215 0.9257 0.9243 0.9257 0.9250 0.9257 0.9257 0.4595 0.5230 0.6291 0.6912 0.7095 0.7318 0.7493 0.8000 0.2235 0.2412 0.3294 0.3706 0.4118 0.3941 0.3235 0.3882 0.3727 0.4057 0.4203 0.4258 0.4560 0.4451 0.4524 0.4762 0.3480 0.3890 0.3860 0.4140 0.4160 0.4050 0.4350 0.4210 0.5538 0.5499 0.5205 0.5225 0.5362 0.5714 0.9265 0.9033 0.9168 0.9284 0.9284 0.9303 0.9323 0.5400 0.5950 0.6300 0.6550 0.6700 0.6650 0.6700 0.7050 0.9139 0.9160 0.9277 0.9347 0.9396 0.9458 0.9500 0.9562 0.8858 0.9439 0.9500 0.9568 0.9635 0.9716 0.9750 0.9730 0.3647 0.4294 0.6294 0.6882 0.6588 0.7118 0.7294 0.6883 0.4139 0.4606 0.6007 0.6859 0.7463 0.8077 0.8333 0.8572 0.5480 0.6100 0.7250 0.7800 0.8060 0.8160 0.8400 0.8520 0.6281 0.6086 0.6478 0.6888 0.7006 0.7201 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset wine wine yeast yeast yeast yeast yeast yeast yeast yeast Table 6 continued from previous page Shots KNN RF Qwen-2.5-7B Our 512 1,024 8 16 32 64 128 256 512 1,024 0.6556 0.6810 0.3030 0.3502 0.4310 0.4579 0.5152 0.5522 0.5556 0.5993 0.6967 0.7123 0.3401 0.3603 0.4175 0.4848 0.5185 0.5690 0.5825 0.5993 0.5616 0.5675 0.2256 0.2761 0.3367 0.3232 0.2963 0.3333 0.3232 0. 0.7103 0.7260 0.2997 0.3434 0.4175 0.4949 0.5252 0.5522 0.5926 0.5960 Continued on next page EXTENDED PER-TASK EVALUATION Table 7: Per-dataset test accuracy from 8 to 512 shots on extended datasets in Talent. Evaluation on all 8-1024 shots and full datasets is very computation intensive, and we leave it for the next revision. Abbreviations: Qwen = Qwen-2.5-7B-Instruct; Our = MACHINELEARNINGLM (base: Qwen-2.5-7B-Instruct). Dataset Shots KNN RF Qwen Our Amazon_employee_access Amazon_employee_access Amazon_employee_access Amazon_employee_access Amazon_employee_access Amazon_employee_access Amazon_employee_access BNG_breast_w BNG_breast_w BNG_breast_w BNG_breast_w BNG_breast_w BNG_breast_w BNG_breast_w BNG_cmc BNG_cmc BNG_cmc BNG_cmc BNG_cmc BNG_cmc BNG_cmc BNG_tic_tac_toe BNG_tic_tac_toe BNG_tic_tac_toe BNG_tic_tac_toe BNG_tic_tac_toe BNG_tic_tac_toe BNG_tic_tac_toe Bank_Customer_Churn_Dataset Bank_Customer_Churn_Dataset Bank_Customer_Churn_Dataset Bank_Customer_Churn_Dataset Bank_Customer_Churn_Dataset Bank_Customer_Churn_Dataset Bank_Customer_Churn_Dataset California_Housing_Classification California_Housing_Classification California_Housing_Classification California_Housing_Classification 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 0.9254 0.9221 0.928 0.928 0.9305 0.9276 0.9291 0.7732 0.8894 0.9424 0.9596 0.9666 0.97 0.9724 0.3871 0.4029 0.4189 0.4366 0.4538 0.473 0.4855 0.6153 0.6068 0.6152 0.6429 0.6635 0.6806 0.6971 0.7435 0.721 0.725 0.732 0.7485 0.7435 0.7565 0.5206 0.5378 0.556 0.5816 0.9245 0.9274 0.9311 0.9295 0.9293 0.9313 0.9305 0.8881 0.9294 0.9409 0.9587 0.9672 0.9717 0.9733 0.4004 0.4164 0.4349 0.4586 0.4818 0.4952 0.5085 0.6018 0.6118 0.6391 0.6645 0.6917 0.7127 0.7239 0.776 0.7765 0.79 0.8045 0.816 0.83 0.8405 0.5748 0.6345 0.6977 0.7253 0.8264 0.813 0.8407 0.8895 0.91 0.9215 0.9291 0.7915 0.8027 0.8387 0.8538 0.8405 0.8455 0.849 0.3691 0.3851 0.3884 0.3883 0.4009 0.4062 0.4023 0.529 0.5649 0.5567 0.5763 0.5837 0.6086 0.5982 0.7105 0.7275 0.744 0.757 0.7675 0.776 0.7935 0.5301 0.5434 0.5293 0.5371 0.9227 0.9257 0.9279 0.9266 0.9343 0.9364 0.939 0.921 0.9496 0.9548 0.959 0.9621 0.9511 0.7156 0.3797 0.392 0.4199 0.4415 0.4769 0.4948 0.4442 0.5723 0.6012 0.6171 0.6237 0.6421 0.6545 0.6615 0.731 0.75 0.767 0.787 0.7915 0.806 0.8095 0.6008 0.6432 0.6953 0.734 29 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen Our Table 7 continued from previous page California_Housing_Classification California_Housing_Classification California_Housing_Classification Cardiovascular_Disease_dataset Cardiovascular_Disease_dataset Cardiovascular_Disease_dataset Cardiovascular_Disease_dataset Cardiovascular_Disease_dataset Cardiovascular_Disease_dataset Cardiovascular_Disease_dataset Click_prediction_small Click_prediction_small Click_prediction_small Click_prediction_small Click_prediction_small Click_prediction_small Click_prediction_small Credit_c Credit_c Credit_c Credit_c Credit_c Credit_c Credit_c E_CommereShippingData E_CommereShippingData E_CommereShippingData E_CommereShippingData E_CommereShippingData E_CommereShippingData E_CommereShippingData Employee Employee Employee Employee Employee Employee Employee FOREX_audcad_hour_High FOREX_audcad_hour_High FOREX_audcad_hour_High FOREX_audcad_hour_High FOREX_audcad_hour_High FOREX_audcad_hour_High FOREX_audcad_hour_High FOREX_audjpy_hour_High FOREX_audjpy_hour_High FOREX_audjpy_hour_High FOREX_audjpy_hour_High FOREX_audjpy_hour_High FOREX_audjpy_hour_High FOREX_audjpy_hour_High FOREX_audsgd_hour_High FOREX_audsgd_hour_High FOREX_audsgd_hour_High FOREX_audsgd_hour_High FOREX_audsgd_hour_High FOREX_audsgd_hour_High FOREX_audsgd_hour_High FOREX_audusd_hour_High FOREX_audusd_hour_High 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 0.5919 0.6085 0.6107 0.5215 0.5338 0.5438 0.5458 0.5617 0.5694 0.5978 0.7866 0.7883 0.8095 0.816 0.8151 0.8174 0.8229 0.4335 0.4487 0.4576 0.4726 0.4928 0.5054 0.5218 0.5696 0.5878 0.6041 0.6141 0.6259 0.6391 0.6454 0.5715 0.6294 0.6208 0.6294 0.6702 0.6799 0.7025 0.4965 0.496 0.4971 0.5084 0.5009 0.5077 0.5101 0.5004 0.4985 0.4975 0.5013 0.507 0.5044 0.5154 0.5041 0.5029 0.5123 0.5081 0.505 0.5178 0.5113 0.506 0.502 0.7529 0.7851 0.8071 0.5707 0.6139 0.6445 0.6642 0.6773 0.6888 0.6929 0.7689 0.7677 0.7964 0.8105 0.8172 0.8201 0.8228 0.4757 0.5232 0.5602 0.5895 0.6121 0.6354 0.6514 0.5486 0.5823 0.6055 0.6205 0.6368 0.6532 0.6428 0.5886 0.6348 0.6638 0.7024 0.7605 0.7852 0.8163 0.501 0.5074 0.5067 0.5024 0.5092 0.5183 0.5378 0.5028 0.4945 0.4909 0.4947 0.504 0.5101 0.5179 0.4992 0.5101 0.504 0.5081 0.5024 0.5081 0.5149 0.4967 0.4934 0.5196 0.5339 0.5194 0.5624 0.5636 0.5681 0.5719 0.5705 0.5573 0.5559 0.7016 0.7247 0.7438 0.7768 0.7827 0.7863 0.8054 0.418 0.4502 0.4722 0.5 0.5013 0.5246 0.5288 0.4991 0.4995 0.4932 0.5159 0.5054 0.5127 0.5204 0.6036 0.6079 0.6219 0.6402 0.6487 0.6477 0.6466 0.5013 0.4943 0.5033 0.5075 0.5108 0.5128 0.5082 0.4962 0.5061 0.5006 0.5135 0.4998 0.5083 0.5081 0.4975 0.4995 0.4941 0.5084 0.5112 0.5124 0.5186 0.5059 0. 0.7512 0.7766 0.7822 0.5547 0.5892 0.6179 0.6488 0.6718 0.6878 0.6918 0.7029 0.7068 0.735 0.7803 0.8058 0.8148 0.8239 0.45 0.5124 0.5422 0.5688 0.5957 0.6069 0.6127 0.5454 0.5496 0.5805 0.6132 0.6182 0.6318 0.6304 0.6101 0.6251 0.6391 0.6746 0.7197 0.7486 0.7551 0.5029 0.506 0.4968 0.5171 0.501 0.5218 0.5212 0.5006 0.4995 0.4977 0.4975 0.5108 0.5118 0.515 0.5033 0.5101 0.5056 0.5059 0.4987 0.5168 0.5103 0.5012 0.498 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen Our Table 7 continued from previous page FOREX_audusd_hour_High FOREX_audusd_hour_High FOREX_audusd_hour_High FOREX_audusd_hour_High FOREX_audusd_hour_High FOREX_cadjpy_hour_High FOREX_cadjpy_hour_High FOREX_cadjpy_hour_High FOREX_cadjpy_hour_High FOREX_cadjpy_hour_High FOREX_cadjpy_hour_High FOREX_cadjpy_hour_High Firm_Teacher_Clave_Direction_Classification Firm_Teacher_Clave_Direction_Classification Firm_Teacher_Clave_Direction_Classification Firm_Teacher_Clave_Direction_Classification Firm_Teacher_Clave_Direction_Classification Firm_Teacher_Clave_Direction_Classification Firm_Teacher_Clave_Direction_Classification Gender_Gap_in_Spanish_WP Gender_Gap_in_Spanish_WP Gender_Gap_in_Spanish_WP Gender_Gap_in_Spanish_WP Gender_Gap_in_Spanish_WP Gender_Gap_in_Spanish_WP Gender_Gap_in_Spanish_WP GesturePhaseSegmentationProcessed GesturePhaseSegmentationProcessed GesturePhaseSegmentationProcessed GesturePhaseSegmentationProcessed GesturePhaseSegmentationProcessed GesturePhaseSegmentationProcessed GesturePhaseSegmentationProcessed HR_Analytics_Job_Change_of_Data_Scientists HR_Analytics_Job_Change_of_Data_Scientists HR_Analytics_Job_Change_of_Data_Scientists HR_Analytics_Job_Change_of_Data_Scientists HR_Analytics_Job_Change_of_Data_Scientists HR_Analytics_Job_Change_of_Data_Scientists HR_Analytics_Job_Change_of_Data_Scientists INNHotelsGroup INNHotelsGroup INNHotelsGroup INNHotelsGroup INNHotelsGroup INNHotelsGroup INNHotelsGroup Insurance Insurance Insurance Insurance Insurance Insurance Insurance Intersectional_Bias_Assessment Intersectional_Bias_Assessment Intersectional_Bias_Assessment Intersectional_Bias_Assessment Intersectional_Bias_Assessment Intersectional_Bias_Assessment Intersectional_Bias_Assessment 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 0.5033 0.5073 0.5146 0.5175 0.5209 0.5069 0.5023 0.5021 0.5123 0.5042 0.4973 0.494 0.4102 0.5542 0.644 0.6833 0.7125 0.7301 0.7412 0.4958 0.5095 0.5316 0.5053 0.5358 0.5442 0.5558 0.3104 0.3479 0.4005 0.4299 0.4349 0.439 0.4714 0.6788 0.6733 0.6715 0.6746 0.6855 0.7035 0.7103 0.667 0.7024 0.7159 0.7206 0.7311 0.7432 0.7458 0.6885 0.6794 0.6813 0.6855 0.6875 0.6964 0.7013 0.5559 0.8109 0.8691 0.8941 0.9096 0.9141 0.9209 0.498 0.5091 0.5071 0.5125 0.5212 0.5018 0.5034 0.508 0.5058 0.5018 0.5109 0.5128 0.5111 0.5514 0.6143 0.6639 0.7028 0.7273 0.7556 0.4684 0.5221 0.5389 0.5263 0.5284 0.5495 0.5716 0.3104 0.3514 0.3904 0.438 0.4486 0.4663 0.5073 0.7289 0.7369 0.7416 0.7484 0.7578 0.7724 0.7782 0.6286 0.6713 0.7027 0.7344 0.7591 0.7796 0.7976 0.691 0.6884 0.7097 0.7182 0.7229 0.7244 0.7286 0.6477 0.765 0.8236 0.8787 0.8977 0.9127 0.9264 0.5117 0.5108 0.5232 0.5291 0.5283 0.5078 0.498 0.4989 0.5084 0.5101 0.5099 0.5149 0.3694 0.3833 0.4269 0.4236 0.4366 0.4379 0.4264 0.4368 0.4884 0.4621 0.4548 0.4347 0.4842 0.5221 0.2744 0.2532 0.2927 0.2921 0.3078 0.3235 0.3367 0.6498 0.6485 0.6816 0.6837 0.6801 0.6996 0.7075 0.5247 0.5443 0.5446 0.5474 0.5509 0.5789 0.5904 0.6355 0.6242 0.6463 0.6419 0.63 0.6736 0.6605 0.5091 0.5177 0.5304 0.5359 0.5182 0.519 0. 0.4962 0.5097 0.5112 0.5111 0.5159 0.5066 0.5074 0.5068 0.514 0.5072 0.5163 0.5064 0.3907 0.4829 0.556 0.5986 0.6731 0.681 0.6648 0.52 0.5832 0.5664 0.5884 0.5779 0.5937 0.5895 0.2891 0.3438 0.3489 0.4051 0.4253 0.3899 0.4385 0.6738 0.6843 0.7098 0.7252 0.7333 0.75 0.7388 0.5745 0.6168 0.6471 0.6906 0.7134 0.7294 0.7127 0.6609 0.6654 0.7061 0.7234 0.7346 0.7437 0.748 0.5077 0.5541 0.6695 0.7418 0.8145 0.8314 0.8341 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset JapaneseVowels JapaneseVowels JapaneseVowels JapaneseVowels JapaneseVowels JapaneseVowels JapaneseVowels Long Long Long Long Long Long Long MagicTelescope MagicTelescope MagicTelescope MagicTelescope MagicTelescope MagicTelescope MagicTelescope PhishingWebsites PhishingWebsites PhishingWebsites PhishingWebsites PhishingWebsites PhishingWebsites PhishingWebsites Rain_in_Australia Rain_in_Australia Rain_in_Australia Rain_in_Australia Rain_in_Australia Rain_in_Australia Rain_in_Australia SDSS17 SDSS17 SDSS17 SDSS17 SDSS17 SDSS17 SDSS17 Shipping Shipping Shipping Shipping Shipping Shipping Shipping Telecom_Churn_Dataset Telecom_Churn_Dataset Telecom_Churn_Dataset Telecom_Churn_Dataset Telecom_Churn_Dataset Telecom_Churn_Dataset Telecom_Churn_Dataset VulNoneVul VulNoneVul VulNoneVul VulNoneVul VulNoneVul Table 7 continued from previous page Shots KNN RF Qwen Our 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 0.1445 0.1751 0.1872 0.2027 0.2173 0.2474 0.274 0.6474 0.6417 0.6306 0.6294 0.6462 0.6529 0.6607 0.6078 0.6454 0.6687 0.6961 0.719 0.7403 0.75 0.5577 0.7277 0.7834 0.8268 0.8471 0.8748 0.8869 0.7417 0.7583 0.767 0.7732 0.775 0.7799 0.7828 0.4949 0.5023 0.5132 0.5235 0.5299 0.5431 0.5544 0.5437 0.5546 0.5763 0.5827 0.6146 0.625 0.6255 0.8531 0.8515 0.8575 0.8651 0.8635 0.8755 0.8816 0.9886 0.9886 0.9886 0.9886 0.9886 0.2629 0.3658 0.4947 0.6342 0.7486 0.8334 0.8786 0.7478 0.8359 0.8627 0.8862 0.9085 0.9308 0.9352 0.6104 0.6488 0.7119 0.7594 0.7968 0.8089 0.8207 0.6929 0.7973 0.858 0.9 0.9068 0.9145 0.9222 0.7263 0.7519 0.7668 0.782 0.79 0.803 0.8087 0.6039 0.7126 0.8072 0.883 0.9278 0.9473 0.9556 0.5423 0.5582 0.5818 0.6036 0.6218 0.6327 0.6241 0.859 0.8576 0.8621 0.8636 0.8726 0.8861 0.907 0.9868 0.9868 0.9886 0.9886 0.9886 0.1239 0.1244 0.133 0.131 0.1485 0.128 0.137 0.6496 0.6674 0.6551 0.6763 0.673 0.6774 0.6897 0.5949 0.5886 0.6199 0.6383 0.6364 0.6441 0.658 0.5658 0.5803 0.5889 0.6192 0.6133 0.6432 0.6105 0.6996 0.7135 0.7209 0.7178 0.717 0.7219 0.7058 0.468 0.5077 0.5296 0.5457 0.5591 0.5653 0.548 0.4928 0.5045 0.4977 0.4996 0.4845 0.5077 0.4946 0.8006 0.7781 0.7886 0.8051 0.8081 0.8156 0.8426 0.9631 0.9859 0.9789 0.9868 0. 0.2223 0.3236 0.4646 0.6106 0.7501 0.7963 0.842 0.6551 0.7121 0.7857 0.808 0.8806 0.8303 0.865 0.6078 0.6861 0.7235 0.7603 0.7713 0.79 0.6643 0.6377 0.711 0.7838 0.8223 0.8621 0.8707 0.891 0.6702 0.703 0.7242 0.7486 0.7748 0.786 0.7926 0.533 0.6122 0.6808 0.7328 0.7722 0.7976 0.8195 0.5564 0.5514 0.5827 0.5986 0.6041 0.6036 0.6268 0.862 0.8591 0.8606 0.8591 0.8636 0.8681 0.8696 0.9763 0.9798 0.9859 0.9886 0.9886 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen Our Table 7 continued from previous page VulNoneVul VulNoneVul Water_Quality_and_Potability Water_Quality_and_Potability Water_Quality_and_Potability Water_Quality_and_Potability Water_Quality_and_Potability Water_Quality_and_Potability Water_Quality_and_Potability Wilt Wilt Wilt Wilt Wilt Wilt Wilt abalone abalone abalone abalone abalone abalone abalone ada_prior ada_prior ada_prior ada_prior ada_prior ada_prior ada_prior adult adult adult adult adult adult adult airlines_seed_0_nrows_2000_nclasses_10_ncols_100 airlines_seed_0_nrows_2000_nclasses_10_ncols_100 airlines_seed_0_nrows_2000_nclasses_10_ncols_100 airlines_seed_0_nrows_2000_nclasses_10_ncols_100 airlines_seed_0_nrows_2000_nclasses_10_ncols_100 airlines_seed_0_nrows_2000_nclasses_10_ncols_100 airlines_seed_0_nrows_2000_nclasses_10_ncols_100 allbp allbp allbp allbp allbp allbp allbp artificial_characters artificial_characters artificial_characters artificial_characters artificial_characters artificial_characters artificial_characters compass compass compass 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 0.9886 0.9886 0.5411 0.5259 0.5442 0.5503 0.5442 0.5366 0.5122 0.9523 0.9472 0.9513 0.9544 0.9523 0.9544 0.9596 0.4294 0.4785 0.5167 0.5455 0.5634 0.5933 0.6088 0.6528 0.6594 0.6408 0.6835 0.6879 0.7031 0.6999 0.6705 0.6785 0.6908 0.6946 0.708 0.7095 0.7162 0.51 0.5075 0.4725 0.5 0.535 0.535 0.5175 0.9431 0.9536 0.9536 0.9536 0.9536 0.9576 0.9603 0.1595 0.184 0.227 0.2994 0.3591 0.4193 0.4594 0.532 0.5614 0.5816 0.9886 0.9886 0.5335 0.5732 0.5701 0.5747 0.593 0.5762 0.6067 0.9482 0.9347 0.944 0.9544 0.9565 0.9627 0.97 0.4809 0.5095 0.5239 0.5383 0.555 0.5694 0.5993 0.7043 0.7327 0.7491 0.7765 0.7962 0.793 0.8247 0.7319 0.7531 0.7734 0.7959 0.8098 0.8197 0.8337 0.54 0.52 0.5125 0.5075 0.56 0.59 0.5575 0.943 0.9523 0.955 0.9563 0.9576 0.9563 0.9669 0.16 0.1854 0.2446 0.3068 0.4095 0.4731 0.5259 0.5554 0.5678 0.6176 0.9886 0.9868 0.5671 0.5519 0.5549 0.5701 0.5869 0.5686 0.5945 0.9016 0.8829 0.912 0.9254 0.9295 0.941 0.9482 0.3947 0.3971 0.3708 0.3768 0.4174 0.3888 0.4653 0.7262 0.7426 0.7591 0.7524 0.7601 0.7568 0.7371 0.7313 0.7485 0.7737 0.7674 0.7736 0.7713 0.7651 0.9285 0.9325 0.9471 0.9364 0.9471 0.9483 0.57 0.9484 0.9351 0.8954 0.943 0.9563 0.955 0.951 0.1135 0.1154 0.1389 0.1453 0.1771 0.205 0.1937 0.5173 0.5296 0. 0.9886 0.9886 0.4969 0.5686 0.5533 0.5351 0.5869 0.59 0.5915 0.9451 0.9461 0.9462 0.9388 0.9617 0.9689 0.9741 0.4629 0.5096 0.5371 0.5299 0.5706 0.5718 0.6017 0.7306 0.7415 0.7426 0.7656 0.7821 0.7908 0.805 0.7344 0.7579 0.766 0.7761 0.791 0.7985 0.8174 0.5375 0.515 0.51 0.495 0.5275 0.54 0.5475 0.9496 0.951 0.9523 0.9536 0.9563 0.9563 0.9523 0.1477 0.1786 0.2148 0.2794 0.3576 0.4095 0.4618 0.5389 0.5551 0.5918 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Table 7 continued from previous page Dataset compass compass compass compass connect_4 connect_4 connect_4 connect_4 connect_4 connect_4 connect_4 credit credit credit credit credit credit credit customer_satisfaction_in_airline customer_satisfaction_in_airline customer_satisfaction_in_airline customer_satisfaction_in_airline customer_satisfaction_in_airline customer_satisfaction_in_airline customer_satisfaction_in_airline dabetes_130_us_hospitals dabetes_130_us_hospitals dabetes_130_us_hospitals dabetes_130_us_hospitals dabetes_130_us_hospitals dabetes_130_us_hospitals dabetes_130_us_hospitals default_of_credit_card_clients default_of_credit_card_clients default_of_credit_card_clients default_of_credit_card_clients default_of_credit_card_clients default_of_credit_card_clients default_of_credit_card_clients delta_ailerons delta_ailerons delta_ailerons delta_ailerons delta_ailerons delta_ailerons delta_ailerons dry_bean_dataset dry_bean_dataset dry_bean_dataset dry_bean_dataset dry_bean_dataset dry_bean_dataset dry_bean_dataset eeg_eye_state eeg_eye_state eeg_eye_state eeg_eye_state eeg_eye_state eeg_eye_state eeg_eye_state electricity Shots KNN RF Qwen Our 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 0.5843 0.5788 0.6056 0.6098 0.6032 0.6122 0.6147 0.6139 0.6181 0.621 0.6282 0.5073 0.5121 0.5214 0.5139 0.5166 0.5178 0.5059 0.5158 0.5242 0.5368 0.5451 0.5536 0.559 0.565 0.4986 0.5083 0.5099 0.5139 0.5166 0.5187 0.5221 0.7355 0.7432 0.7354 0.7437 0.7414 0.7463 0.7486 0.6851 0.744 0.8233 0.8843 0.9096 0.9187 0.9242 0.3992 0.4741 0.5207 0.5476 0.5762 0.5832 0.6049 0.511 0.5487 0.5721 0.6015 0.6752 0.7447 0.8021 0.5282 0.6254 0.6416 0.6575 0.6606 0.5793 0.6011 0.6152 0.6334 0.6429 0.6622 0.6762 0.5375 0.5558 0.5881 0.6174 0.6357 0.6428 0.6533 0.6521 0.7118 0.7682 0.8106 0.8412 0.8646 0.883 0.5214 0.5215 0.5397 0.5449 0.557 0.5742 0.5868 0.7335 0.7535 0.7582 0.7756 0.7919 0.798 0.802 0.7686 0.866 0.9088 0.9166 0.9243 0.9257 0.9327 0.5237 0.6739 0.765 0.8281 0.8652 0.8818 0.8931 0.5391 0.5481 0.5718 0.6202 0.6562 0.6852 0.747 0.6019 0.5458 0.541 0.5455 0.5293 0.5593 0.6038 0.6171 0.6246 0.64 0.6483 0.653 0.4986 0.5178 0.5186 0.5238 0.5283 0.5328 0.5151 0.5669 0.5964 0.6067 0.5961 0.6107 0.6193 0.5726 0.5231 0.5298 0.5323 0.5353 0.5283 0.5346 0.5288 0.6715 0.6756 0.7177 0.7456 0.7478 0.7588 0.764 0.5301 0.5533 0.5406 0.5617 0.5652 0.6052 0.5743 0.227 0.267 0.3092 0.3606 0.426 0.4815 0.5094 0.525 0.528 0.5254 0.5451 0.5384 0.5464 0.548 0. 0.5978 0.6326 0.6435 0.6419 0.6055 0.618 0.6315 0.6417 0.6322 0.643 0.65 0.571 0.6156 0.6539 0.6826 0.7152 0.7197 0.7248 0.5982 0.6621 0.74 0.7777 0.7994 0.8113 0.8294 0.497 0.5032 0.5119 0.5246 0.5419 0.5474 0.5518 0.6913 0.7298 0.7244 0.7723 0.7849 0.7995 0.7819 0.758 0.7763 0.8654 0.8885 0.9005 0.9004 0.92 0.455 0.6287 0.7664 0.8061 0.8358 0.8656 0.8329 0.5157 0.5261 0.5277 0.5801 0.6155 0.6812 0.716 0.5696 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen Our Table 7 continued from previous page electricity electricity electricity electricity electricity electricity eye_movements eye_movements eye_movements eye_movements eye_movements eye_movements eye_movements eye_movements_bin eye_movements_bin eye_movements_bin eye_movements_bin eye_movements_bin eye_movements_bin eye_movements_bin house_16H house_16H house_16H house_16H house_16H house_16H house_16H in_vehicle_coupon_recommendation in_vehicle_coupon_recommendation in_vehicle_coupon_recommendation in_vehicle_coupon_recommendation in_vehicle_coupon_recommendation in_vehicle_coupon_recommendation in_vehicle_coupon_recommendation internet_firewall internet_firewall internet_firewall internet_firewall internet_firewall internet_firewall internet_firewall jm1 jm1 jm1 jm1 jm1 jm1 jm1 jungle_chess_2pcs_raw_endgame_complete jungle_chess_2pcs_raw_endgame_complete jungle_chess_2pcs_raw_endgame_complete jungle_chess_2pcs_raw_endgame_complete jungle_chess_2pcs_raw_endgame_complete jungle_chess_2pcs_raw_endgame_complete jungle_chess_2pcs_raw_endgame_complete kdd_ipums_la_97_small kdd_ipums_la_97_small kdd_ipums_la_97_small kdd_ipums_la_97_small kdd_ipums_la_97_small kdd_ipums_la_97_small 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 0.53 0.5396 0.5553 0.5795 0.6172 0.6376 0.3548 0.3634 0.3653 0.3693 0.3817 0.4061 0.3956 0.4973 0.4869 0.5046 0.5125 0.5453 0.5303 0.5309 0.6064 0.6486 0.6646 0.6567 0.6708 0.6686 0.676 0.5207 0.5353 0.5199 0.5412 0.5408 0.5613 0.5712 0.7062 0.8121 0.8669 0.8908 0.9099 0.921 0.9245 0.7781 0.7804 0.7754 0.7731 0.779 0.7851 0.7998 0.5064 0.5851 0.6188 0.6523 0.672 0.6961 0.7317 0.5963 0.6715 0.7042 0.7746 0.8025 0.8064 0.6489 0.6945 0.7169 0.7411 0.7622 0.7728 0.3633 0.3771 0.3817 0.4021 0.431 0.4573 0.4718 0.4993 0.4921 0.5 0.5197 0.5243 0.523 0.546 0.6334 0.7187 0.7517 0.7865 0.8065 0.8184 0.8262 0.5205 0.5345 0.5412 0.5664 0.5605 0.6042 0.6437 0.7704 0.8463 0.8898 0.9105 0.9242 0.9323 0.9327 0.7607 0.7772 0.7827 0.7915 0.7979 0.7869 0.8011 0.5282 0.5868 0.629 0.6713 0.7033 0.7392 0.7615 0.6686 0.7755 0.8199 0.8275 0.8343 0.8565 0.5952 0.603 0.6158 0.6225 0.6142 0.6152 0.3436 0.3535 0.3568 0.3594 0.3436 0.3555 0.343 0.4816 0.5072 0.4796 0.477 0.4882 0.4764 0.4915 0.4922 0.5015 0.4967 0.513 0.5048 0.5204 0.5152 0.4927 0.4888 0.5002 0.4982 0.4841 0.5211 0.5124 0.5475 0.6043 0.6278 0.6365 0.6446 0.6133 0.5876 0.7423 0.7658 0.7818 0.7869 0.7997 0.813 0.8089 0.4732 0.5069 0.5116 0.5227 0.5422 0.5462 0.5543 0.5617 0.5915 0.605 0.6435 0.5896 0. 0.6126 0.6654 0.6925 0.7056 0.7304 0.7458 0.337 0.3522 0.366 0.4034 0.4192 0.41 0.4409 0.4974 0.4796 0.4757 0.4869 0.5328 0.5138 0.5152 0.5771 0.6442 0.6839 0.7135 0.7231 0.7717 0.7413 0.5089 0.5277 0.5483 0.5309 0.5306 0.5577 0.5692 0.7163 0.7863 0.7993 0.8101 0.8031 0.7941 0.7995 0.7217 0.7478 0.7606 0.7754 0.8002 0.8062 0.8209 0.4986 0.5723 0.6074 0.6405 0.6785 0.7001 0.6779 0.63 0.7052 0.763 0.7987 0.7688 0.7871 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen Our Table 7 continued from previous page kdd_ipums_la_97_small kr_vs_kp kr_vs_kp kr_vs_kp kr_vs_kp kr_vs_kp kr_vs_kp kr_vs_kp kropt kropt kropt kropt kropt kropt kropt law_school_admission_bianry law_school_admission_bianry law_school_admission_bianry law_school_admission_bianry law_school_admission_bianry law_school_admission_bianry law_school_admission_bianry letter letter letter letter letter letter letter mammography mammography mammography mammography mammography mammography mammography microaggregation2 microaggregation2 microaggregation2 microaggregation2 microaggregation2 microaggregation2 microaggregation2 mobile_c36_oversampling mobile_c36_oversampling mobile_c36_oversampling mobile_c36_oversampling mobile_c36_oversampling mobile_c36_oversampling mobile_c36_oversampling mozilla4 mozilla4 mozilla4 mozilla4 mozilla4 mozilla4 mozilla4 national_longitudinal_survey_binary national_longitudinal_survey_binary national_longitudinal_survey_binary national_longitudinal_survey_binary 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 0.8218 0.5 0.5703 0.7219 0.7281 0.7797 0.8359 0.8562 0.1354 0.1454 0.1725 0.2015 0.2363 0.2753 0.3175 0.6377 0.6538 0.6447 0.6483 0.6714 0.6822 0.693 0.0825 0.132 0.184 0.2665 0.3688 0.488 0.605 0.9772 0.9772 0.9772 0.9767 0.9781 0.9803 0.9821 0.4627 0.4627 0.4895 0.5033 0.519 0.518 0.5218 0.6599 0.7128 0.7902 0.8269 0.8485 0.8669 0.8854 0.6822 0.7507 0.7932 0.8205 0.8546 0.8749 0.8852 0.5855 0.5896 0.6222 0.6273 0.8468 0.5531 0.6141 0.7422 0.8422 0.925 0.9578 0.9656 0.1431 0.1623 0.1855 0.2201 0.2651 0.3282 0.379 0.7245 0.8238 0.9279 0.982 0.9971 0.9998 1 0.109 0.156 0.228 0.3377 0.4742 0.616 0.718 0.9772 0.9763 0.9759 0.9763 0.9808 0.9826 0.9848 0.436 0.468 0.4748 0.5102 0.5188 0.535 0.5465 0.6687 0.7639 0.8164 0.8513 0.8735 0.8899 0.9011 0.7533 0.8636 0.8945 0.9051 0.9218 0.9267 0.9306 0.7271 0.8584 0.9399 0.9827 0.5829 0.4734 0.4672 0.5109 0.4875 0.5234 0.5203 0.5359 0.0882 0.087 0.0775 0.077 0.0823 0.083 0.0752 0.6048 0.6365 0.6565 0.7058 0.717 0.7414 0.7685 0.0398 0.0387 0.0375 0.0425 0.044 0.042 0.039 0.9472 0.9625 0.9566 0.9687 0.9705 0.9678 0.9723 0.417 0.4475 0.4733 0.491 0.503 0.523 0.5393 0.5412 0.5549 0.5759 0.6104 0.6309 0.6525 0.6751 0.6057 0.6224 0.6494 0.6825 0.6687 0.6822 0.6536 0.6059 0.6029 0.6283 0. 0.7033 0.5219 0.5344 0.6109 0.6547 0.725 0.75 0.7641 0.0699 0.0661 0.0665 0.0624 0.0764 0.1005 0.1162 0.6738 0.7413 0.7996 0.8642 0.9029 0.925 0.9437 0.0383 0.0375 0.0462 0.068 0.127 0.1777 0.215 0.9567 0.9597 0.9665 0.9687 0.9759 0.9812 0.983 0.3815 0.436 0.4945 0.5238 0.5375 0.5385 0.551 0.6654 0.7429 0.7937 0.825 0.8474 0.8667 0.8838 0.7089 0.7668 0.7951 0.834 0.8523 0.8575 0.6208 0.5377 0.5968 0.6446 0.6843 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset Shots KNN RF Qwen Our Table 7 continued from previous page national_longitudinal_survey_binary national_longitudinal_survey_binary national_longitudinal_survey_binary okcupid_stem okcupid_stem okcupid_stem okcupid_stem okcupid_stem okcupid_stem okcupid_stem online_shoppers online_shoppers online_shoppers online_shoppers online_shoppers online_shoppers online_shoppers page_blocks page_blocks page_blocks page_blocks page_blocks page_blocks page_blocks pendigits pendigits pendigits pendigits pendigits pendigits pendigits pol pol pol pol pol pol pol rice_cammeo_and_osmancik rice_cammeo_and_osmancik rice_cammeo_and_osmancik rice_cammeo_and_osmancik rice_cammeo_and_osmancik rice_cammeo_and_osmancik rice_cammeo_and_osmancik shill_bidding shill_bidding shill_bidding shill_bidding shill_bidding shill_bidding shill_bidding shrutime shrutime shrutime shrutime shrutime shrutime shrutime shuttle shuttle 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 0.6283 0.6436 0.6426 0.6824 0.6887 0.6884 0.6902 0.6964 0.6955 0.688 0.854 0.8573 0.8686 0.8763 0.8824 0.8873 0.8917 0.8822 0.8868 0.8868 0.8922 0.8959 0.9087 0.9251 0.2865 0.4279 0.5925 0.7408 0.8436 0.8963 0.9359 0.5548 0.6113 0.6743 0.7506 0.7928 0.8414 0.8686 0.769 0.8491 0.8583 0.8662 0.8635 0.8675 0.8806 0.8822 0.8759 0.8704 0.8885 0.8798 0.8814 0.8861 0.7465 0.75 0.7445 0.741 0.7365 0.743 0.753 0.7998 0.8209 0.9939 0.998 0.998 0.6793 0.6769 0.6807 0.696 0.6996 0.706 0.7135 0.8447 0.852 0.858 0.8718 0.8791 0.884 0.8917 0.8904 0.8959 0.8996 0.9178 0.9297 0.9407 0.9535 0.326 0.4529 0.6362 0.7776 0.8467 0.9086 0.9418 0.5627 0.6411 0.7888 0.8533 0.9063 0.9281 0.9395 0.8491 0.8858 0.891 0.9134 0.916 0.9081 0.9081 0.8656 0.8814 0.8799 0.8775 0.879 0.8735 0.8751 0.747 0.775 0.7935 0.803 0.8155 0.825 0.84 0.8302 0.8661 0.6456 0.6467 0.6558 0.6374 0.6419 0.6488 0.6477 0.6778 0.6726 0.6747 0.7137 0.6833 0.8086 0.7977 0.8025 0.8285 0.8321 0.821 0.8156 0.8648 0.8639 0.8803 0.8777 0.8822 0.1073 0.1323 0.1514 0.171 0.1792 0.2278 0.2178 0.5052 0.5101 0.5251 0.5196 0.5448 0.5543 0.5538 0.5407 0.6457 0.6706 0.7612 0.7703 0.7743 0.811 0.845 0.8466 0.8348 0.8553 0.8727 0.8672 0.8783 0.6965 0.7425 0.756 0.7695 0.774 0.7875 0.792 0.6834 0. 0.7465 0.7597 0.8065 0.6079 0.657 0.6752 0.6968 0.711 0.7155 0.7135 0.7891 0.8269 0.8362 0.854 0.8654 0.8763 0.8844 0.8356 0.8521 0.8886 0.9114 0.9133 0.9306 0.9398 0.2628 0.4443 0.6367 0.7781 0.8276 0.8872 0.8608 0.5305 0.5791 0.7025 0.7764 0.8547 0.8904 0.8394 0.7939 0.8504 0.8478 0.8937 0.8976 0.9029 0.8058 0.8205 0.838 0.8553 0.8775 0.8838 0.8877 0.8973 0.732 0.7665 0.7835 0.789 0.7965 0.8125 0.824 0.7713 0.8181 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset shuttle shuttle shuttle shuttle shuttle splice splice splice splice splice splice splice sylvine sylvine sylvine sylvine sylvine sylvine sylvine telco_customer_churn telco_customer_churn telco_customer_churn telco_customer_churn telco_customer_churn telco_customer_churn telco_customer_churn texture texture texture texture texture texture texture thyroid_ann thyroid_ann thyroid_ann thyroid_ann thyroid_ann thyroid_ann thyroid_ann thyroid_dis thyroid_dis thyroid_dis thyroid_dis thyroid_dis thyroid_dis thyroid_dis turiye_student_evaluation turiye_student_evaluation turiye_student_evaluation turiye_student_evaluation turiye_student_evaluation turiye_student_evaluation turiye_student_evaluation walking_activity walking_activity walking_activity walking_activity walking_activity walking_activity walking_activity Table 7 continued from previous page Shots KNN RF Qwen Our 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 8 16 32 64 128 256 512 0.8523 0.8956 0.9443 0.9706 0.9818 0.4624 0.4624 0.5094 0.5344 0.5643 0.6191 0.6724 0.5073 0.5444 0.5697 0.5707 0.6107 0.6683 0.7073 0.7161 0.7218 0.7303 0.7608 0.7616 0.7573 0.7665 0.0375 0.0625 0.0844 0.1 0.1906 0.3781 0.4844 0.9033 0.9205 0.9178 0.9179 0.9179 0.9206 0.9232 0.5464 0.55 0.5322 0.5268 0.5464 0.5947 0.5768 0.2646 0.2655 0.3127 0.3497 0.3823 0.4004 0.4098 0.1241 0.14 0.1502 0.166 0.1848 0.2169 0.2577 0.9063 0.958 0.9873 0.9941 0.9952 0.4608 0.5204 0.6269 0.7053 0.79 0.8511 0.906 0.599 0.7434 0.8263 0.882 0.8858 0.9063 0.9073 0.6834 0.7303 0.7303 0.7814 0.7814 0.7722 0.7843 0.0375 0.05 0.0625 0.0969 0.2437 0.3563 0.5312 0.9298 0.9245 0.9391 0.947 0.9537 0.9589 0.9762 0.5607 0.5893 0.5947 0.6464 0.6661 0.6964 0.7036 0.2466 0.2835 0.3359 0.3858 0.4304 0.4399 0.4587 0.1355 0.1702 0.2148 0.2593 0.3094 0.3573 0.4078 0.7641 0.7905 0.8018 0.7899 0.8112 0.4107 0.3887 0.4828 0.4294 0.4749 0.4546 0.4874 0.6068 0.6215 0.601 0.6098 0.6644 0.6547 0.5991 0.6452 0.6899 0.6707 0.7069 0.6991 0.7005 0.6934 0.1273 0.1709 0.1964 0.21 0.2427 0.2436 0.2365 0.8781 0.9179 0.9192 0.9086 0.9179 0.9166 0.9231 0.4268 0.4179 0.5018 0.4947 0.4304 0.4929 0.3929 0.2294 0.2388 0.2543 0.2809 0.2843 0.3016 0.3033 0.0192 0.0175 0.0164 0.0154 0.0173 0.0167 0. 0.876 0.9319 0.9541 0.9626 0.8215 0.4671 0.4467 0.5 0.5047 0.5203 0.5345 0.5298 0.5571 0.6693 0.7395 0.8078 0.8283 0.8556 0.84 0.6387 0.6742 0.7189 0.7587 0.7637 0.7714 0.7842 0.2118 0.2664 0.3927 0.53 0.6464 0.6455 0.6506 0.9272 0.9245 0.9324 0.9245 0.9325 0.9073 0.9232 0.4553 0.5625 0.5875 0.65 0.6411 0.6768 0.6572 0.238 0.25 0.2809 0.3127 0.3634 0.3832 0.4175 0.0138 0.0092 0.0089 0.0135 0.0267 0.0508 0.0629 Continued on next page"
        },
        {
            "title": "Work in Progress",
            "content": "Dataset water_quality water_quality water_quality water_quality water_quality water_quality water_quality wine_quality_white wine_quality_white wine_quality_white wine_quality_white wine_quality_white wine_quality_white wine_quality_white Table 7 continued from previous page Shots KNN RF Qwen Our 8 16 32 64 128 256 512 8 16 32 64 128 256 512 0.8856 0.8857 0.8819 0.8825 0.8819 0.8813 0.8819 0.3694 0.3908 0.4031 0.4194 0.4072 0.4439 0.4541 0.8763 0.8694 0.8781 0.8782 0.8775 0.8769 0.8825 0.4051 0.4163 0.4561 0.4663 0.4888 0.4867 0.5224 0.8275 0.8294 0.8656 0.8469 0.8531 0.8737 0.8737 0.3429 0.3888 0.3939 0.4306 0.4245 0.4234 0. 0.8619 0.8644 0.8688 0.8807 0.885 0.8893 0.8907 0.3296 0.3929 0.4459 0.4337 0.4439 0.4806 0."
        }
    ],
    "affiliations": [
        "SCUT",
        "Stanford",
        "UCAS"
    ]
}