{
    "paper_title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models",
    "authors": [
        "Gonçalo Faria",
        "Noah A. Smith"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 9 7 3 0 . 4 0 5 2 : r Preprint. Under review. Sample, Dont Search: Rethinking Test-Time Alignment for Language Models Gonçalo Faria1, Noah A. Smith1,2 1University of Washington, 2Allen Institute for AI gfaria@cs.washington.edu"
        },
        {
            "title": "Abstract",
            "content": "Increasing test-time computation has emerged as promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QALIGN, new test-time alignment approach. As we scale test-time compute, QALIGN converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QALIGN on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the TÜLU 3 preference dataset, QALIGN outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training."
        },
        {
            "title": "Introduction",
            "content": "Language models (LMs) have demonstrated remarkable capabilities through learning from human preferences (Stiennon et al., 2022; Fernandes et al., 2023; Kaufmann et al., 2023; Peters & Schaal, 2007; Peng et al., 2019; Korbak et al., 2022b;a; Go et al., 2023). However, there are number of problems with deploying single aligned model: alignment approaches typically average multiple human preferences, constructing monolithic preference model and target policy. Furthermore, approaches to adapting these models through finetuning have become increasingly impractical, requiring enormous computational resources. They are entirely impossible when model weights are private, as with many state-of-the-art models (OpenAI, 2024; Anthropic, 2024; Gemini, 2024). While conventional alignment methods optimize single model for aggregate performance across distribution of prompts and then produce random generation during inference, researchers have found great improvement from scaling the amount of compute expended at test time for each prompt (Brown et al., 2024; Snell et al., 2024). Here, multiple outputs are generated and then used to produce final answer either via reward maximization (known as best-of-n, or BoN; Gao et al., 2022; Stiennon et al., 2020), majority voting (MV; Wang et al., 2023b) or weighted majority voting (WMV; Li et al., 2023). While promising, existing search-based methods face fundamental limitations (Liu et al., 2024; Wu et al., 2024; Zhang et al., 2023; Xie et al., 2023): as inference compute scales, these methods over-optimize learned reward models (RMs), which are inherently imperfect proxies (Gao et al., 2022). In this paper, we introduce QALIGN, test-time alignment method that converges to sampling from the optimal aligned distribution (defined in 2 as the true target for existing RLHF methods) for 1 Preprint. Under review. Figure 1: Average error rate across multiple evaluation datasets (GSM8K, MATH500, MMLU-Redux, TruthfulQA, and IFEval) as function of inference-time floating point operations (FLOPS) in log scale. We compare QALIGN with TÜLU3-8B-SFT against four baselines: majority vote (MV) TÜLU3-8B-DPO, and applied to TÜLU3-8B-SFT the methods best-of-n (BoN), MV, and weighted MV (WMV) . All experiments use temperature 1.0 with reasoning included in model outputs. The TÜLU3-8B-DPO model results from preference finetuning TÜLU3-8B-SFT (approximately 1.75 1019 FLOPs). The costs of this process are not accounted for in this plot. each prompt, individually, as the test-time compute budget is increased. This enables more aligned responses without requiring access to the logits or training any underlying LM (only sampling from it). Specifically, we adapt QUEST (Faria et al., 2024). QUEST was designed to repurpose language model, creating Markov chain converging toward (Gibbs) distribution defined by by machine translation quality estimation metrics. In this work, we show that we can use QUEST to align LMs at test time according to an RM learned from preference data (in place of the quality estimation metric) and to provide single good final prediction. In our experiments (4), we compare QALIGN with existing test-time compute methods on mathematical reasoning benchmarks (GSM8K, Cobbe et al., 2021, and GSM-Symbolic, Mirzadeh et al., 2024), by training task-specific RM (4.1), and using it to increase the capability of LLAMA3.18B-INSTRUCT (Llama, 2024). We obtain consistent reduction in error as we scale inference time computation across scales, as shown in Figure 2. Unlike BoN, QALIGNs performance does not degrade as we increase the compute per question. Furthermore, when applied with more realistic RMs ( 4.2), trained on the TÜLU3 (Lambert et al., 2024) preference dataset, where over-optimization starts to appear at lower inference budget, and applied to TÜLU3-8B-SFT, our experiments show that QALIGN can consistently outperform direct preference optimization (DPO; Rafailov et al., 2024), which updates the model weights, as well as BoN, MV, and WMV on diverse range of datasets in mathematical reasoning (GSM8K and MATH500;Hendrycks et al., 2021; Lightman et al., 2023), knowledge recall (MMLU-Redux; Gema et al., 2025), TruthfulQA (Lin et al., 2022) and instruction following (IFEval; Zhou et al., 2023b), as shown in Figure 1.1 Our contributions are: We propose QALIGN, method for making local approximations of the optimal aligned distribution at test time. We show 35% increase in accuracy on GSM8K and 98% on GSM-Symbolic relative to single generation, using task-specific RM, beating BoN, MV, and WMV. We show that QALIGN outperforms DPO when applied to TÜLU3 family of models, in general RMs trained on the TÜLU3 preference dataset, even when compared with the same compute budget at test-time via MV, achieving 57% increase in average accuracy relative to single generation across suite of benchmarks. 1www.questdecoding.com/alignment 2 Preprint. Under review."
        },
        {
            "title": "2 Background: Language Model Alignment",
            "content": "Generating text from autoregressive LMs involves conditioning on prompt encoding the particular question we want to solve. Let denote the set of possible LM responses (outputs). The distribution over responses Y, given the prompt x, can be factorized as the product of conditional probabilities over individual tokens y1, y2, . . . , yN, where each yi V, fixed vocabulary. The probability of particular response for given input can be written as pLM(y x) = i=1 pLM(yi y<i, x). (1) Here, y<i = y1, y2, . . . , yi1. Finetuning pretrained LM pLM(y x) to align with preferences encoded by reward function r(y, x) can be cast as Bayesian inference (Korbak et al., 2022a). We seek π(y x), which is initially equal to prior pLM(y x), and updated to conform to evidence provided by the human preferences model, where p(γ = 1 y, x) exp(r(y, x)/β) is the likelihood that the prompt-response pair (x, y) is human-preferred. Using Bayes rule, the posterior takes the following form: π(y x) p(y γ = 1, x) = Zβ(x) pLM(y x) exp (cid:17) (cid:16) r(y,x) β . (2) It is both intractable to compute the partition function Zβ(x) = yY pLM(y x) exp(r(y, x)/β), and to sample from the distribution in Eq. 2 exactly. To approximate π(y x), many methods have been proposed (Stiennon et al., 2022; Fernandes et al., 2023; Kaufmann et al., 2023; Peters & Schaal, 2007; Peng et al., 2019; Korbak et al., 2022b;a; Go et al., 2023). Given dataset of prompts D, one common approach is to estimate parameterized variational approximation qθ(y x): (cid:2)rϕ(y, x)/β(cid:3) DKL (qθ(y x) pLM(y x)) (cid:3). (cid:2)E xD (3) yqθ (yx) max θ Methods like PPO (Schulman et al., 2017) and others (Shao et al., 2024; Rafailov et al., 2024; Ahmadian et al., 2024) optimize this objective with low-variance gradient estimates, resulting in stable optimization. Importantly, qθ(y x) is newly trained LM initialized with pLM(y x). The large-scale efforts to align language models generally alternate between learning qθ(y x) and r(y, x), with human preferences on samples generated from qθ(y x) (Llama, 2024; 2023; Gemini, 2024). This avoids allowing the approximate posterior to exploit regions where the reward is out of distribution and also to refine the resolution of the reward towards promising regions."
        },
        {
            "title": "3 Test-Time Alignment via MCMC",
            "content": "Casting language model alignment as posterior inference decouples our target goal from the procedure used to achieve it. While current approaches rely on variational inference to learn single model qθ(y x) (Eq. 3), this strategy faces four limitations. First, it requires expensive model finetuning, which is costly with large models. Second, many models weights are not openly shared, including those of state-of-the-art models like GPT-4 (OpenAI, 2024) and Gemini (Gemini, 2024). Third, amortizing the approximation across all prompts necessarily sacrifices the quality of approximation for any individual prompt to achieve good average performance. Fourth, the approach assumes monolithic notion of human preferences encoded in r(y, x), offering no flexibility to adapt to varying user preferences or contexts at inference time. These limitations motivate shift toward local posterior approximations, where we achieve better approximation as we increase the compute budget at test time on single prompt x. As we will describe in (3.1), our key insight is that rather than estimating single parametric approximation, we can use recent advancements in Markov chain Monte Carlo (MCMC) sampling with LMs (Faria et al., 2024) and obtain sequence of samples = y0, y1, . . . , yT from π(y x) and use them for optimal decision-making. principled way to select final response from the generated candidate set from π(y x) is by selecting the most common element (mode) i.e., via majority voting (MV; Wang et al., 2023b). However, this only works for tasks with well-defined answers, such as mathematical reasoning or multiple-choice questions. For open-ended generation tasks, we apply generalization of this 3 Preprint. Under review. approach through the minimum Bayes risk (MBR) principle (Kumar & Byrne, 2002; Eikema & Aziz, 2020; Farinhas et al., 2023; Bertsch et al., 2023). The MBR framework selects the output ˆy that maximizes the expected task-specific utility u(y, y): ˆy = arg max yS yπ(yx) (cid:2)u(y, y)(cid:3) arg max yS 1 + 1 t=0 u(y, yt). (4) When the utility metric u(y, y) is defined as an exact match over the final answer, MBR amounts to MV strategy over S. We employ ROUGE (Lin, 2004) as our utility metric for open-ended generation tasks, following Bertsch et al. (2023). While this results in O(T2) similarity computations, ROUGE is computationally lightweight, and efficient low-rank approximations (Trabelsi et al., 2024) can be used when dealing with heavier neural-based metrics. In the following sections, we describe two practical approaches to obtain final answer based on MBR to samples from π(y x). First, QALIGN (3.1), our novel MCMC-based approach to sample from the aligned distribution π(y x) at test time, allowing us to apply MBR directly to these samples. Second, in 3.2, we present importance sampling, classical alternative that circumvents direct sampling from the aligned distribution by reweighting samples from the base model pLM(y x) to approximate MBR as if they were drawn from π(y x). Finally, in 3.3, we describe best-of-n sampling (BoN), simple and widely used baseline approach for test-time alignment that does not explicitly optimize an MBR objective but instead selects single high-reward sample from the base model. 3.1 MCMC for Text Generation With the goal of generating sequence of samples from πβ(y x), we will construct Markov chain (y0, y1, . . . , yT) that has πβ(y x) as its equilibrium distribution. The chain starts from hypothesis y0 pLM(y x). On the tth iteration, it draws new hypothesis from proposal distribution q(y yt, x) , and this hypothesis is accepted with an acceptance probability αβ(y, yt) [0, 1]. The proposal distribution q(y yt, x) we use is the one proposed by QUEST (Faria et al., 2024). It is based on LM that samples suffixes pLM(yi:N y<i, x) starting at uniformly sampled index i. This proposal can be written as q(y yt, x, i) = pLM(yi:N yt <i, x) 1{y1:j = yt 1:j}, (5) Following the Metropolis-Hastings algorithm (MH; Hastings, 1970), the acceptance probability is αβ(y, yt) = min (cid:8)1, πβ (y x) (cid:0)yt y, x(cid:1) (cid:14) πβ (cid:0)yt x(cid:1) (cid:0)y yt, x(cid:1)(cid:9) . (6) If the candidate is accepted, the next state in the chain becomes yt+1 = y; if rejected, the chain stays at yt+1 = yt. The process repeats for some number of steps T. In the end, it returns the set of accepted samples. Note that, while computing the likelihood πβ(y x) of particular hypothesis under the target distribution is intractable (due to the partition function Zβ(x)), evaluating the acceptance criterion αβ(y, yt) is easy, because it depends only on the likelihood ratio, in which the normalization constants cancel out: πβ(y x) πβ(yt x) (cid:19) pLM(y x) pLM(yt x) (cid:18) r(y,x)r(yt,x) β = exp (7) . MH converges to the unique stationary distribution πβ(y x), regardless of the initial distribution, because the transition distribution of the Markov chain, p(yt yt1, x) which results from generating candidate from q(yt yt1, x) followed by an accept/reject step, satisfies the Markov chain ergodic theorem (Neal, 2011). Note that, under samples from the proposal from Eq. 5, the likelihood ratio from Eq. 7 is proportional to the inverse of the probability of returning: q(yt y, x, i) q(y yt, x, i) = pLM(yt i:N yt pLM(yi:N yt <i, x) <i, x) , (8) 4 Preprint. Under review. this allows simplifying the criterion as: αβ(y, yt) = min (cid:26) 1, exp (cid:18) r(x,y)r(x,yt) β (cid:19) yt/y (cid:27) . (9) The length ratio yt/y comes from the ratio between the uniform index distributions. Note that this means that we can sample from the aligned distribution in Eq. 2 without any access to logits or parameter weights. In summary, we propose simple and effective procedure for sampling from π(y x). We characterize the QALIGN sampling process as repeating the following for steps: 1. Given an instance yt with length yt, sample an index uniformly. 2. Generate completion yi:N from pLM(yi:N yt 3. Compute the probability of acceptance on the reward difference in Eq. 9. Sample random <i, x). boolean based on this probability. If we reject, yt+1 = yt; if we accept, yt+1 = y. The QUEST proposal is simple and relies solely on the models ability to generate text left-to-right, with the consequence that successive samples are only conditionally dependent up to the index i, which limits the speed at which we can explore (i.e., the effective sample size). While more complex proposals could be built based on LMs prompted to self-refine (Zhou et al., 2023a; Madaan et al., 2023; Yao et al., 2023), recent work demonstrates that even strong LMs often fail to improve reasoning due to models inability to gauge the correctness of their outputs and localize errors (Huang et al., 2024). Because of this, growing body of research tries to teach LMs how to self-correct by creating data-augmentation recipes (Welleck et al., 2022; Havrilla et al., 2024) or new datasets (Wang et al., 2023a; Chen et al., 2024; Lee et al., 2024; Saunders et al., 2022; Schick et al., 2022). However, when the proposal is different from the base model, we lose the simplicity of the acceptance criterion in Eq. 9 and need to keep track of two sets of logits to calculate the acceptance criterion in Eq. 6. As our approach only requires access to the ability to generate continuations given prefix (not access to base model parameters or logits), it can also be used with closed LMs through an API. However, we limit our experiments to open-weight models. We provide in Appendix an analysis of the computational cost associated with running QALIGN. 3.2 Importance Sampling natural competing approach to QALIGN for approximating the intractable expectation in Eq. 4 is to use importance sampling (IS; Kahn, 1950). Rather than attempting to directly sample from the target distribution π(y x), IS generates samples from the base LM pLM(y x) and reweights them to match the target distribution π(y x): yπ(yx)[u(y, y)] = ypLM(yx) (cid:20) π(y x) pLM(y x) u(y, y) (cid:21) = ypLM(yx) (cid:34) exp( 1 β r(y, x)) Zβ(x) (cid:35) u(y, y) Note that the partition function Zβ(x) can be written as an expectation over samples from the base model. So in the end, making this approximation boils down to the following steps: 1. Sample y(0), . . . , y(T) pLM(y x). 2. Evaluate the generations with the reward model r(y, x) resulting in r(0), . . . , r(T). 3. Obtain the importance weights w(i) = exp(r(i)/β) (cid:46) 4. For each hypothesis y, compute yπ(yx)[u(y, y)] j=0 exp(r(j)/β) . i=0 w(i)u(y, y(i)). In the LM literature, this procedure, for the specific case of tasks with well-defined final answers (i.e., the utility metric u(y, y) is defined as an exact match), the MBR output is the same as weighted 5 Preprint. Under review. majority voting (WMV; Li et al., 2023). Similar to QALIGN, WMV is guaranteed to converge to the optimal decision rule as computational resources increase. However, because it relies solely on independent samples drawn from the base LM, the generation process cannot be directly steered toward more promising regions of the output space. This limitation becomes particularly problematic when the base LM infrequently produces high-reward responses. In such cases, the approximation of the target expectation, and corresponding final decision, will be compromised. 3.3 Best-of-n Sampling Best-of-n (BoN) sampling is simple approach for aligning language model predictions at test time. BoN has emerged as strong baseline in the literature, requiring no additional training, while still achieving compelling performance. Unlike QALIGN and IS, which explicitly optimize for MBR objective, BoN uses heuristic selection process. Given prompt x, we generate candidate responses y1, y2, . . . , yn independently from the base LM pLM(y x). Each response is evaluated using the RM r(x, y), and the candidate with the highest reward is chosen as the final output: y(n) = arg maxyi r(x, yi). This procedure implicitly biases the output distribution toward higher-reward responses. It is easy to see that in the limit of n, we are sampling from the maximum reward distribution; however, BoN is remarkably performant even when is small (Gao et al., 2022; Llama, 2024). (10) With some strong assumptions, we can show that the probability density of the BoN distribution can be approximated by the following form: p(y(n) x) exp (cid:18) r(x, y(n)) β(n) (cid:19) pLM(y(n) x), where β takes the form β(n) = (cid:112) σd(x) 2 log nd (log log nd+log(4π)) 2 2 log nd . (11) (12) where σd(x) is the standard deviation of the dominant component of the distribution of reward, under the assumption of Gaussian mixture, which we empirically observe (see Appendix C), when evaluated on samples from our base LM conditioned on x, and nd = wd(x)n, the dominant components sample size, where wd(x) is the weight of the dominant component. full derivation of this approximation is provided in Appendix A. Note that this density equals our target distribution π(y x) from Eq. 2 if we set β = β(n). This rough equivalence, not noted in past literature to our knowledge, provides new insight into key distinction between BoN and the other methods we have discussed, including QALIGN. In QALIGN, we explicitly define target distribution π(y x), and increasing the test-time budget refines our approximation of this fixed objective. In contrast, BoN does not converge to predefined distribution; instead, increasing progressively shifts the selection process toward responses with higher rewards and deviating further from pLM(y x). Insofar as the reward model is well-aligned with human preferences, this should make BoN highly effective. However, in realistic settings where the reward is imperfect, increasing can lead to over-optimization, i.e., selecting responses that score well under the reward model but degrade in actual quality. As result, BoN can suffer from diminishing returns or even performance degradation as test-time compute increases, which we observe in our experiments (4.14.2) and previous work (Gao et al., 2022)."
        },
        {
            "title": "4 Experiments",
            "content": "Our experiments are centered around two main questions. First, task-specific tuning: given powerful task-specific RM trained on preference data derived from ground-truth evaluations of base model outputs, can QALIGN outperform BoN and WMV (4.1)? Second, general alignment: when applied as an alternative to DPO, where the preference dataset encompasses multiple tasks aimed at improving the instruction-following capabilities of chat models, can QALIGN outperform state-of-the-art alignment methods (4.2)? 6 Preprint. Under review. 4.1 Task-Specific Tuning We evaluate QALIGN for task-specific tuning using LLAMA-3.1-8B-INSTRUCT as the base model and custom Bradley & Terry (1952) RM trained on our preference dataset for mathematical reasoning. This RM was initialized with LLAMA-3.1-8B-INSTRUCT and trained on 64 modelgenerated response pairs per prompt. We selected pairs based on ground truth answers.2 Baselines We compare against BoN, and WMV sampling applied to LLAMA-3.1-8B-INSTRUCT using our trained RM and MV from samples from LLAMA-3.1-8B-INSTRUCT. For each of the methods we generate 4096 solutions per problem. Datasets We evaluate on GSM8K (Cobbe et al., 2021), dataset of grade school math problems, and GSM-Symbolic (Mirzadeh et al., 2024), new benchmark designed to assess out-of-distribution generalization in mathematical reasoning. As demonstrated by Mirzadeh et al. (2024), GSM-Symbolic exposes substantial performance degradation across state-of-the-art LLMs exhibiting drops of up to 65% compared to their GSM8K scores when faced with simple variations like changed numerical values and extra redundant clauses. Sampling configurations We sampled both datasets with temperature 1.0 and β = 1. Since the acceptance rate is function of σr(x)/β (where σr(x) is the distribution of rewards under the base model), we selected this value to achieve approximately 50% acceptance rate based on tuning with 128 samples from the GSM8K training data. This relatively high acceptance rate ensures the chain mixes well and avoids getting stuck in single mode. Results The results of the task-specific tuning experiments are summarized in Figure 2. QALIGN shows progressively lower error rates across both problems as we spend more computational resources. In contrast, on GSM8K, MV shows initial improvement but quickly saturates, while BoN displays improvement with additional compute, temporarily outperforming QALIGN, but eventually reaches an inflection point, after which error rates begin to increase. This behavior aligns with the observations from Gao et al. (2022) and theoretical results from 3.3. WMV performs well on GSM8K but becomes relatively worse after budget of approximately 28 1012.56 FLOPs. While all methods show some performance drop on GSM-Symbolic compared to GSM8K, the magnitude varies significantly. BoN and WMV, which rely on the RM, struggle more with the distribution shift, likely because the RM is fit to particular GSM8K-specific patterns. Relative to these two, MV shows greater robustness to these changes. This is primarily because under the distribution shift the RM becomes less reliable, and since MV does not use it, MVs performance degrades less. However, even under these circumstances, QALIGN is able to leverage the RM and match MV on this dataset with enough compute. This suggests that QALIGN is robust to imperfections in the RM and able to extract useful signal even when the RMs reliability is compromised. 4.2 General Alignment We evaluate QALIGN for more general alignment using TÜLU3-8B-SFT as the base model and TÜLU3-8B-RM as the RM. The TÜLU3 model family (Lambert et al., 2024) was selected for being fully open source, providing instruction-tuned, and aligned versions, along with their corresponding training code and datasets. This enables direct comparison of QALIGN on top of the instruction-tuned model to the aligned model, sharing the initial starting point. Additionally, they provide an RM trained on the data used to train the DPO model, which we use in our experiments. Baselines We compare against (1) MV applied to TÜLU3-8B-DPO, (2) BoN sampling applied to TÜLU3-8B-SFT using TÜLU3-8B-RM, (3) WMV applied to TÜLU3-8B-SFT using TÜLU3-8BRM, and (4) MV applied to TÜLU3-8B-SFT. Note that TÜLU3-8B-DPO model is the result of doing preference finetuning on the TÜLU3-8B-SFT with 271k preference pairs (approximately 1.75 1019 FLOPs). Comparing against DPO with MV additionally allows for an inference FLOPs-adjusted comparison between model explicitly optimized for alignment and our QALIGN approach. This 2Data and RM will be provided upon publication. 7 Preprint. Under review. Figure 2: Average accuracy vs. floating point operations (FLOPS) in log scale. We compare QALIGN with LLAMA-3.1-8B-INSTRUCT against three baselines also applied to LLAMA-3.1-8BINSTRUCT: best-of-n (BoN), majority vote (MV), and weighted MV (WMV). Left: Error rate (lower is better) on GSM8K test dataset. Right: Error rate on GSM-Symbolic test dataset. All experiments use temperature 1.0 with reasoning included in model outputs. allows us to test whether QALIGN is not only better test-time alignment approach, but better overall alignment approach when significant compute is available at test-time. Datasets We evaluate again on GSM8K; we add MATH500, verified subset of the MATH test dataset that contains advanced mathematical problems across algebra, geometry, and calculus (Hendrycks et al., 2021), MMLU-Redux (Gema et al., 2025), refined subset of commonsense and academic knowledge questions, TruthfulQA (Lin et al., 2022), which contains misleading questions designed to elicit truthful responses, and IFEval (Zhou et al., 2023b), which measures adherence to complex multi-step instructions. Among these datasets, IFEval is an open-ended generation and, therefore, requires the use of MBR with ROUGE-1 instead of MV and WMV. For simplicity, we still refer to the method as MV and WMV to denote MBR from base model samples and the weighted version with importance sampling, respectively, in our comparisons. Sampling Configurations For all datasets, we sampled the model with temperature of 1.0 and prompted it to provide reasoning. Complete prompts are available in Appendix D. As with the mathematical reasoning experiments, we tuned the β parameter (in this case, β = 0.5), for TÜLU38B-RM, using 128 samples from the GSM8K training data to achieve an average acceptance rate of 50%, and using the same β for all datasets. Results The results of the general alignment experiments are summarized in Table 1. Figure 1 plots the average error rate across all of the datasets as function of the floating point operations (FLOPS), and Appendix contains all of the error plots for each individual problem. Similar to our task-specific findings, QALIGN, MV, and WMV consistently reduce error rates across all five general alignment datasets as computation increases, with MV exhibiting early saturation. However, similar to the results from GSM-Symbolic, MV outperforms both WMV and BoN. MV applied to the DPO model begins with lower error rates but saturates earlier, ultimately reaching error rates that are close to but still worse than QALIGNs final results. Furthermore, we observe that BoNs inflection point occurs at lower computational budget than observed in task-specific experiments. This suggests that for real-world alignment problems, BoN is not an effective test-time approach, while QALIGN maintains its improvement trajectory even with general RMs."
        },
        {
            "title": "5 Related Work",
            "content": "MCMC for Text Generation. Our work builds on QUEST (Faria et al., 2024), which uses MCMC to sample diverse translations. While QUEST successfully generates multiple translations, its effectiveness as selection mechanism for single high-quality response remained unexplored until now. Earlier works have applied MCMC approaches to both autoregressive and masked language models (MLMs). For instance, Miao et al. (2018) and Zhang et al. (2020) use MH with proposal distribution that makes token-level modifications for constrained generation tasks. 8 Preprint. Under review. Method MATH500 GSM8K (0 shot, CoT) (0 shot, CoT) TQA (MC2, 0 shot, CoT) MMLU-Redux (0 shot, CoT) IFEval (prompt loose) Avg TÜLU3-8B-SFT BON MV WMV QALIGN TÜLU3-8B-DPO MV 31.6 49.4 53.0 58. 55.8 74.3 86.3 85.2 87.8 87.9 45.3 43.7 46.1 51.5 45.4 57.1 59.4 62.2 62. 62.7 59.3 72.6 49.5 75.4 53.5 62.3 59.2 67.1 76.2 65.6 Table 1: Overview of the results on general alignment. Results show accuracy percentages with fixed 1024 sampled solutions across different methods and models. Our proposed QALIGN outperforms other approaches on most benchmarks, achieving the highest scores on 5 out of 5 datasets when applied to TÜLU3-8B-SFT, and attaining an average performance better than TÜLU3-8BDPO. The highlighted cells indicate best result per benchmark. In the case of MLMs, previous works have explored various forms of Gibbs sampling (Berglund et al., 2015; Su et al., 2018; Wang & Cho, 2019; Yamakoshi et al., 2022). However, as Goyal et al. (2022) show, the conditional distributions from MLMs result in invalid Gibbs samplers. In response, they propose an MH correction on the masked conditionals, resulting in higher quality generations. Building on this, Mireshghallah et al. (2022) and Forristal et al. (2023) apply MLMs for controlled text generation. Furthermore, several works have adapted Hamiltonian MCMC algorithms originally designed for high-dimensional continuous distributions (Duane et al., 1987; Neal, 2011) to discrete text generation (Kumar et al., 2022; Qin et al., 2022; Amini et al., 2023; Du et al., 2023). Test-Time Scaling. Test-time scaling methods have emerged as an important approach for improving LM performance without additional training. While our work focuses on local approximations to the optimal aligned distribution from Eq. 2, parallel line of research explores heuristic search strategies. Best-of-n (BoN; Gao et al., 2022; Stiennon et al., 2020), majority voting (MV; Wang et al., 2023b), weighted majority voting (WMV; Li et al., 2023) are examples of such approaches. As we outline in 3.23.3, BoN and WMV can be interpreted as doing test-time alignment. Furthermore, the RMs we consider in this work are outcome-based RMs, they only work on full generations. Several recent methods use process-based RMs (PRMs) that evaluate partial generations (Lightman et al., 2023; Wang et al., 2024; Uesato et al., 2022). Many techniques have been developed to take advantage of PRMs through guided beam search (Xie et al., 2023), Monte-Carlo tree search (MCTS; Liu et al., 2024; Zhang et al., 2023), and REBASE (Wu et al., 2024). However, as outlined in DeepSeek-AI (2025), PRMs face significant practical limitations."
        },
        {
            "title": "6 Conclusion and Future work",
            "content": "We introduced QALIGN, test-time alignment method that can sample from the optimal aligned distribution without any model retraining (requiring only reward model). Our empirical results consistently demonstrate that QALIGN outperforms both search-based approaches that attempt to maximize imperfect reward models (like BoN) and principled alternatives that rely on independent samples from the base LM (such as WMV and MV). Additionally, QALIGN outperforms DPO-tuned models even when they are allowed to match QALIGNs compute budget at test-time. By enabling strong alignment results without model retraining, QALIGN opens new possibilities for deploying and improving language models in resource-constrained environments and enabling the use of private RMs with closed-source LMs. Looking ahead, we believe this work sets the stage for further exploration of test-time alignment methods. While we have demonstrated success across several benchmarks, several important directions remain unexplored. These include the development of more sophisticated proposal distributions and using MCMC as tool to understand the meta-generation strategies that emerge in RL-trained reasoning models. 9 Preprint. Under review."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Alisa Liu and Assaf Harel for their helpful and constructive feedback on the initial versions of the paper. This work was supported in part by NSF grant 211353 and NVIDIA resources provided through the National AI Research Resource Pilot (NAIRR)."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. URL https://arxiv.org/abs/2402.14740. Afra Amini, Li Du, and Ryan Cotterell. Structured voronoi sampling, 2023. Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. URL https://api. semanticscholar.org/CorpusID:268232499. Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo Kärkkäinen, Akos Vetek, and Juha Karhunen. Bidirectional recurrent neural networks as generative models - reconstructing gaps in time series, 2015. Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew R. Gormley. Its mbr all the way down: Modern generation techniques through the lens of minimum bayes risk, 2023. URL https://arxiv.org/abs/2310.01387. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. ISSN 00063444, 14643510. URL http://www.jstor.org/stable/2334029. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural language feedback, 2024. URL https://arxiv.org/abs/2303.16749. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168. F. N. David and Emil Julius Gumbel. Statistics of extremes. Biometrika, 47:209, 1960. URL https://api.semanticscholar.org/CorpusID:123509308. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Li Du, Afra Amini, Lucas Torroba Hennigen, Xinyan Velocity Yu, Jason Eisner, Holden Lee, and Ryan Cotterell. Principled gradient-based markov chain monte carlo for text generation, 2023. Simon Duane, A.D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics Letters B, 195(2):216222, 1987. ISSN 0370-2693. doi: https://doi.org/ 10.1016/0370-2693(87)91197-X. URL https://www.sciencedirect.com/science/ article/pii/037026938791197X. Bryan Eikema and Wilker Aziz. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 45064520, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020. coling-main.398. URL https://aclanthology.org/2020.coling-main.398/. 10 Preprint. Under review. Gonçalo R. A. Faria, Sweta Agrawal, António Farinhas, Ricardo Rei, José G. C. de Souza, and André F. T. Martins. Quest: Quality-aware metropolis-hastings sampling for machine translation, 2024. URL https://arxiv.org/abs/2406.00049. António Farinhas, José G. C. de Souza, and André F. T. Martins. An empirical study of translation hypothesis ensembling with large language models, 2023. URL https://arxiv.org/abs/ 2310.11430. Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and André F. T. Martins. Bridging the Gap: Survey on Integrating (Human) Feedback for Natural Language Generation. Transactions of the Association for Computational Linguistics, 11:16431668, 12 2023. ISSN 2307-387X. doi: 10.1162/tacl_a_00626. URL https://doi.org/10.1162/ tacl_a_00626. R. A. Fisher and L. H. C. Tippett. Limiting forms of the frequency distribution of the largest or smallest member of sample. Mathematical Proceedings of the Cambridge Philosophical Society, 24(2):180190, 1928. doi: 10.1017/S0305004100015681. Jarad Forristal, Niloofar Mireshghallah, Greg Durrett, and Taylor Berg-Kirkpatrick. block metropolis-hastings sampler for controllable energy-based text generation, 2023. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. Are we done with mmlu?, 2025. URL https://arxiv.org/abs/2406.04127. Team Gemini. Gemini: family of highly capable multimodal models, 2024. URL https: //arxiv.org/abs/2312.11805. Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through f-divergence minimization, 2023. Kartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick. Exposing the implicit energy networks behind masked language models via metropolishastings, 2022. Peter Hall. On the rate of convergence of normal extremes. Journal of Applied Probability, 16(2): 433439, 1979. ISSN 00219002. URL http://www.jstor.org/stable/3212912. W. K. Hastings. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57:97109, 1970. URL https://api.semanticscholar.org/CorpusID: 21204149. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve llm reasoning via global and local refinements, 2024. URL https://arxiv.org/abs/2402.10963. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet, 2024. URL https: //arxiv.org/abs/2310.01798. Kahn. Random sampling (monte carlo) techniques in neutron attenuation problems. i. Nucleonics (U.S.) Ceased publication, Vol: 6, No. 5, 05 1950. URL https://www.osti.gov/biblio/ 4441935. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925, 2023. 11 Preprint. Under review. Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc Dymetman. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting, 2022a. Tomasz Korbak, Ethan Perez, and Christopher Buckley. Rl with kl penalties is better viewed as bayesian inference, 2022b. Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained sampling from language models, 2022. Shankar Kumar and William Byrne. Minimum bayes-risk word alignments of bilingual texts. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP 02, pp. 140147, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1118693.1118712. URL https://doi.org/10.3115/1118693.1118712. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2024. URL https://arxiv.org/ abs/2411.15124. Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms, 2024. URL https://arxiv.org/abs/2308.07317. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, Toronto, Canada, jul 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.291. URL https://aclanthology.org/2023.acl-long.291/. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022. URL https://arxiv.org/abs/2109.07958. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with valueguided monte-carlo tree search decoding, 2024. URL https://arxiv.org/abs/2309. 15028. Team Llama. Llama 2: Open foundation and fine-tuned chat models, 2023. Team Llama. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407. 21783. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. URL https://arxiv.org/abs/2303.17651. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. CGMH: constrained sentence generation by metropolis-hastings sampling. CoRR, abs/1811.10996, 2018. URL http://arxiv.org/ abs/1811.10996. Preprint. Under review. Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor Berg-Kirkpatrick. Mix and match: Learningfree controllable text generationusing energy language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 401415, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.31. URL https://aclanthology.org/2022.acl-long.31. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models, 2024. URL https://arxiv.org/abs/2410.05229. Radford M. Neal. Probabilistic inference using markov chain monte carlo methods. 2011. URL https://api.semanticscholar.org/CorpusID:9690330. OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019. Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th International Conference on Machine Learning, ICML 07, pp. 745750, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595937933. doi: 10.1145/1273496.1273590. URL https://doi.org/10.1145/ 1273496.1273590. Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based constrained text generation with langevin dynamics, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv. org/abs/2206.05802. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: collaborative language model, 2022. URL https://arxiv.org/abs/2208.11663. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/ 2408.03314. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In NeurIPS, 2020. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022. Jinyue Su, Jiacheng Xu, Xipeng Qiu, and Xuanjing Huang. Incorporating discriminator in sentence generation: gibbs sampling method, 2018. Preprint. Under review. Firas Trabelsi, David Vilar, Mara Finkelstein, and Markus Freitag. Efficient minimum bayes risk decoding using low-rank matrix completion algorithms, 2024. URL https://arxiv.org/ abs/2406.02832. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback, 2022. URL https://arxiv.org/abs/2211.14275. Alex Wang and Kyunghyun Cho. BERT has mouth, and it must speak: BERT as markov random field language model. CoRR, abs/1902.04094, 2019. URL http://arxiv.org/abs/1902. 04094. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.08935. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: critic for language model generation, 2023a. URL https://arxiv.org/abs/2308. 04592. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023b. URL https://arxiv.org/abs/2203.11171. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct, 2022. URL https://arxiv.org/ abs/2211.00053. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Self-evaluation guided beam search for reasoning, 2023. URL https://arxiv.org/abs/ 2305.00633. Takateru Yamakoshi, Thomas Griffiths, and Robert Hawkins. Probing BERTs priors with serial reproduction chains. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 39773992, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.314. URL https://aclanthology.org/2022.findings-acl.314/. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv. org/abs/2210.03629. Maosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue. Language generation via combinatorial constraint satisfaction: tree search enhanced Monte-Carlo approach. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 12861298, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.115. URL https://aclanthology.org/2020. findings-emnlp.115. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation, 2023. URL https://arxiv.org/ abs/2303.05510. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification, 2023a. URL https://arxiv.org/ abs/2308.07921. 14 Preprint. Under review. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023b. URL https://arxiv.org/abs/2311.07911. 15 Preprint. Under review. Best-of-n Sampling as Test-Time Alignment Best-of-n (BoN) sampling is widely used technique for improving model performance by selecting the highest scoring sample among candidates. In this section, we show that BoN sampling can be interpreted as an implicit form of test-time alignment that approximates the mode of the target distribution π(y x) in Eq. 2. In A.1, we derive the probability density function (pdf) for BoN sampling. Next, in A.2, we use extreme value theory to obtain tractable approximation of rewards of this distribution, and in A.3 derive an expression for the distribution of rewards of the target density π(y x). Finally, in A.4, employing Laplace approximation of the reward distribution, we establish an explicit relationship between and the effective parameter β of the aligned distribution π(y x). A.1 Distribution of Maximum Reward In BoN sampling, given prompt x, we generate candidate responses y1, y2, . . . , yn independently from the base LM pLM(y x). Each response is evaluated using the RM r(y, x), and the candidate with the highest reward is chosen as the final output: y(n) = arg max yi r(x, yi). (13) Let us assume that the reward function r(x, y) is (locally) injective, that is, each maps to unique reward value. Then, if we denote by p(r x) the probability density (pdf) of reward values for particular under the base LM pLM(y x) change of variables yields: (cid:12) (cid:12) (cid:12), = r1(r, x) (cid:17)(cid:12) (cid:12) (cid:12)Jr1 (r) p(r x) = pLM (14) (cid:16) where r1 is the inverse mapping and Jr1 (r) is its Jacobian determinant. Given independent samples, the cumulative distribution function (cdf) FR(rx) we can write the cdf for the maximum reward rmax as and the respective pdf as Fbon(r (n) maxx) = FR(r (n) max x)n, pbon(r (n) maxx) = FR(r (n) max x)n1 p(r = (n) max x). (15) (16) Since each candidate corresponds to reward r(x, y), the probability density over the best-of-n response can be written as: p(y(n)x) = FR (cid:0)r(x, y(n)) x(cid:1)n pLM(y(n)x). (17) Eq. 17 shows how the BoN procedure emphasizes the upper tail of the reward distribution. The term nFR(r x)n1 becomes increasingly significant as grows, effectively pushing the probability mass towards reward values for which FR(r x) 1. A.2 Extreme Value Theory Recall that our goal was to establish an explicit relationship between the number of samples we maximize over and the parameter β of the aligned distribution in Eq. 2. One way to establish this relationship is through the reward distribution. However, even assuming p(r x) is normal, the mode of the distribution in Eq. 16 lacks closed-form expression. Following classic results from extreme value theory (Fisher & Tippett, 1928; David & Gumbel, 1960), we know that the distribution of the maximum (appropriately rescaled) converges to Gumbel distribution. For the specific case where we assume that for given prompt the reward function values are distributed as mixture of two Normal distributions with means µ1(x) and µ2(x), variances σ2 1 (x) and σ2 2 (x), and mixture weights w1(x) and w2(x) (where w1(x) + w2(x) = 1). Appendix provides empirical evidence that this assumption is reasonable. The tail behavior is dominated by the 16 Preprint. Under review. Gaussian component with the heavier tail, typically the one with larger variance (or larger mean if variances are equal). 1 (n) max (r) wd(x) (cid:18) 1 Φ (cid:18) µd(x) σd(x) (cid:19)(cid:19) , (18) where Φ is the standard normal CDF, the index of the component with the largest variance, i.e., = arg maxi σ2 (x), or if variances are equal, = arg maxi µi(x) is the index of the component with the largest mean. The work of Hall (1979) provides asymptotic expressions for location and scale parameters. In particular, one finds that the maximum reward is approximately: (n) max an µd(x) + σd(x) (cid:32) (cid:112) 2 log nd (log log nd + log(4π)) (cid:112) 2 2 log nd (cid:33) , (19) and fluctuations about this location are on the order of bn σd(x) , and where nd = nwd(x) 2 log nd denotes the effective sample count from the dominant normal component. So in the limit of the distribution of maximum reward can be expressed as: Figure 3 compares this Gumbel approximation to the empirical distribution of maximum rewards, showing close fit for 32. p(r (n) maxx) Gumbel(an, bn). (20) (n) Figure 3: Distribution of the normalized maximum reward (r max an)/bn for varying n, overlaid with the standard Gumbel distribution. The empirical distribution is estimated using 10,000 trials, each consisting of random samples drawn from Normal distribution. The fit between the empirical distribution and the Normal distribution improves as increases, showing good agreement for 32. A.3 Distribution of Rewards of π(y x) Given the aligned distribution π(y x) from Eq. 2, we want an expression for the distribution of rewards under samples from π. Following the empirical observations from Appendix C, let us assume that for given prompt x, the reward values under the base language model pLM(y x) follow mixture of two Normal distributions with means µ1(x) and µ2(x), variances σ2 2 (x), and mixture weights w1(x) and w2(x) (where w1(x) + w2(x) = 1), i.e.: 1 (x) and σ2 p(r x) = w1(x) (cid:113) 1 2πσ2 1 (x) (cid:32) exp (cid:33) (r µ1(x))2 1 (x) 2σ2 + w2(x) (cid:113) 1 2πσ2 2 (x) (cid:32) exp (r µ2(x))2 2 (x) 2σ2 (cid:33) . Under π(y x), the distribution of rewards can be derived through change of variables. The probability density of rewards under the policy, denoted as π(r x), is proportional to: π(r x) exp (cid:19) (cid:18) β p(r x). 17 (21) Preprint. Under review. Substituting the mixture form of p(r x) and combining terms in the exponent and through completion of the square in the exponents for each component, we can show that this is equivalent to: π(r x) w1(x)C1 (cid:113) 1 2πσ2 1 (x) + w2(x)C2 (cid:113) 1 2πσ2 2 (x) exp exp 1 2σ2 1 (x) 1 2σ2 2 (x) µ1(x) (cid:32) (cid:32) µ2(x) (cid:33)2 σ2 1 (x) β (cid:33)2 , σ2 2 (x) β (22) (23) where C1 = exp (cid:18) (cid:19) β + σ2 µ1(x) 1 (x) 2β2 and C2 = exp (cid:18) β + σ2 µ2(x) 2 (x) 2β (cid:19) . Therefore, the aligned distribution of rewards remains mixture of Normals with adjusted mixture weights and means, and preserved variances: π(r x) = wπ,1N (r; µπ,1(x, β), σ2 1 (x)) + wπ,2N (r; µπ,2(x, β), σ2 2 (x)), (24) where µπ,i(x, β) = µi(x) + σ2 (x)/β for {1, 2}, and the adjusted mixture weights are given by: wπ,1 = w1(x)C1 w1(x)C1 + w2(x)C2 , wπ,2 = w2(x)C2 w1(x)C1 + w2(x)C2 . (25) This result shows that β shifts the mean of each component by factor proportional to that components variance and inversely proportional to the parameter β, while preserving the variances of the original reward distributions. Additionally, as β approaches zero, the mixture weight of the component with the larger variance (or larger mean if variances are equal) approaches 1, causing the aligned distribution to collapse to single Gaussian with an increasingly high mean. For this reason, we write the reward target density expression as approximately only the dominant Gaussian: π(r x) (r; µπ,d(x, β), σ (x)), (26) where = arg maxi σ2 are equal, = arg maxi µi(x) is the index of the component with the largest mean. (x) is the index of the component with the largest variance, or if variances A.4 Relating and β via Mode Matching The Gumbel approximation has mode that can be explicitly expressed as its location parameter an, and the reward distribution of the aligned distribution in Eq. 26 as µπ,d(x, β). Following the results from Appendix A.3, the distribution of rewards under the target density π(rx) is also normal, but with mean µπ,d(x, β) = µd(x) + σd(x)2/β. By matching this mode with the location parameter an of the Gumbel distribution for BoN sampling i.e.: µd(x) + σd(x)2 β = µd(x) + σd(x) we obtain: (cid:32) (cid:112) 2 log nd (log log nd + log(4π)) (cid:112) 2 2 log nd (cid:33) , (27) β = (cid:112) σd(x) 2 log nd (log log nd+log(4π)) 2 2 log nd . (28) Figure 4 compares the empirical reward distribution to the target distribution π(rx) when β = β. The approximation closely matches the mode of the empirical distribution of maximum rewards, though it struggles to capture the variance accurately. The variance of the BoN reward distribution decreases as function n, while the one from π(y x) stays constant. Preprint. Under review. (n) max µπ,d(x, β))/σd(x) for varying Figure 4: Distribution of the normalized maximum reward (r n, overlaid with the standard Normal distribution. The empirical distribution is estimated using 10,000 trials, each consisting of random samples drawn from Normal distribution. While the mode of our approximation matches, the approximation does not capture the variance of the empirical distribution."
        },
        {
            "title": "B Computational Cost of QALIGN",
            "content": "As outlined in Faria et al. (2024), when using the suffix proposal distribution from QUEST, each step samples, on average, an index at the midpoint of the sentence from the uniform distribution. Assuming fixed sentence length for simplicity, this requires generating only 2 new tokens on average per step. With typical transformer-based models, this allows us to reuse the majority of the key-value cache from previous iterations in both the base model and reward model. The reuse extends even beyond half of the computation due to the static prompt. However, unlike when generating independent samples, the computation for each prompt is sequential. We need to first generate the sentence yt before generating the sentence yt+1. This means that regardless of the compute capability, compared with sampling independent samples, there is always an inherent latency overhead. In summary, for chain of steps, QALIGN is expected to generate (T+1)N tokens in total: 2 tokens for the initial hypothesis, plus an average of 2 tokens for each of the remaining 1 steps. In comparison, generating independent samples requires decoding tokens. Therefore, for an equal number of samples, QUEST requires approximately half as many tokens from generation alone than sampling independently, translating to roughly half the FLOPs."
        },
        {
            "title": "C Empirical Observations on Distribution of Rewards",
            "content": "When analyzing reward model predictions across independently generated responses from the base model for individual prompts, we consistently observed bimodal distribution forming twocomponent Normal mixture with distinct means and variances (Figure 5). This pattern appeared across all three reward models used. We suspect this bimodal structure directly relates to the Bradley-Terry training objective, which naturally tries to separate the responses into two clusters (preferred vs. non-preferred). The clusters appear to have normal distribution most likely because of the central limit theorem, i.e., the predictions result from neural net that is the sum of billions of random variables."
        },
        {
            "title": "D Prompts Used for Evaluation",
            "content": "This appendix documents the prompt templates used across different datasets in our experiments. The prompt for GSM8K is only used in the general alignment experiments. The placeholders (text within <{...}>) are dynamically replaced with specific content from each dataset during the experiments. 19 Preprint. Under review. Figure 5: Histogram of rewards assigned by TÜLU3-8B-RM to 1, 024 responses generated by TÜLU3-8B-SFT for 9 randomly sampled prompts from GSM8K. For each prompt, we fit twocomponent Gaussian mixture model to characterize the reward distribution. D.1 GSM8K Dataset Solve the following grade school math problem step-by-step: <{question}> D.2 MATH-500 Dataset Solve the following math problem step-by-step: Present the answer in LaTex format: boxed{Your answer} <{question}> D.3 Multiple Choice Datasets (TQA, MMLU) Choose the correct answer to the following multiple-choice question about <{subject}>. Question: <{question}> A). <{choice_A}> B). <{choice_B}> C). <{choice_C}> D). <{choice_D}> Provide your reasoning about the answer and finish your answer with the letter corresponding to the correct option (e.g., A, B, C, or D). 20 Preprint. Under review."
        },
        {
            "title": "E Full General Alignment Plots",
            "content": "Figure 6 plots the average error rate across all of the datasets as function of the floating point operations (FLOPS). Figure 6: Error rate across multiple evaluation datasets (GSM8K, MATH500, MMLU-Redux, TruthfulQA, and IFEval) as function of the floating point operations (FLOPS) in log scale. We compare QALIGN with TÜLU3-8B-SFT against four baselines: majority vote (MV) TÜLU38B-DPO, and applied to TÜLU3-8B-SFT the methods best-of-n (BoN), MV, and weighted MV (WMV) . All experiments use temperature 1.0 with reasoning included in model outputs. Note that TÜLU3-8B-DPO model is the result of doing preference finetuning on the TÜLU3-8B-SFT with 271k preference pairs. The costs associated with this process are not accounted for in this plot."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "University of Washington"
    ]
}