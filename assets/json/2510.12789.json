{
    "paper_title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
    "authors": [
        "Kevin Li",
        "Manuel Brack",
        "Sudeep Katakol",
        "Hareesh Ravi",
        "Ajinkya Kale"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 9 8 7 2 1 . 0 1 5 2 : r UniFusion: Vision-Language Model as Unified Encoder in Image Generation Kevin (Yu-Teng) Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale Adobe Applied Research Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility. To maximize the benefits of the joint multimodal reasoning and representation capacity of VLMs, we present UniFusion, diffusion-based generative model conditioned on frozen large vision-language model (VLM) that serves as unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of frozen VLM to condition diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (Verifi), which conditions diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. Verifi combines the alignment of the conditioning distribution with the VLMs reasoning capabilities for increased capabilities and flexibility at inference. With an 8B VLM and an 8B DiT, UniFusion surpasses Flux.1 [dev] and BAGEL on DPG-Bench with smaller training set (<1 billion samples), while comparing favorably against Flux.1 Kontext [dev] and Qwen-Image-Edit in editing tasks without any post-training. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion. Date: October 15, 2025 Correspondence: <yutengl, mbrack>@adobe.com Project Page: https://thekevinli.github.io/unifusion/"
        },
        {
            "title": "1 Introduction",
            "content": "The rapid advancement of generative image models has had profound impact on creative tasks. However, recent creative workflows demand models that go beyond text-to-image generation to support editing, referencebased composition, and iterative instruction-following. While natively multimodal systems have emerged to handle such tasks [6, 10, 37, 40, 43, 46], they require joint training over both text and image modalities. This setting significantly increases computational and data requirements, potentially leading to adverse effects on image fidelity. In this work, we focus on developing an image-generation model that achieves these capabilities without the complexity of joint multimodal training. Authors contributed equally. Kevin led the model design, training, and ablation experiments. Manuel led the evaluation, presentation, and writing of the paper. 1 Vision-Language Model as Unified Encoder in Image Generation Figure 1 Diverse text-to-image generation with UniFusion. (Zoom in for more details) 2 Vision-Language Model as Unified Encoder in Image Generation Figure 2 Diverse textual image editing and image reference workflows with UniFusion. All images encoded by VLM features only, no VAE tokens involved. (Zoom in for more details) 3 Vision-Language Model as Unified Encoder in Image Generation (a) Zero-shot reasoning. Our Verifi paradigm allows UniFusion to leverage the world knowledge and reasoning of the VLM encoder. (b) Zero-shot multi-reference generations for UniFusion model only trained on single-reference samples. Figure 3 Zero-shot capabilities by UniFusion, which was not explicitly trained for. Our unified encoder setup enables the transfer of many capabilities from the VLM encoder to generative image applications. 4 Vision-Language Model as Unified Encoder in Image Generation Figure 4 UniFusion architecture and inference paradigm. We extract multimodal representations from multiple layers of frozen LLM and aggregate with learnable layerwise attention pooling (LAP) module. subsequent refiner counteracts the VLMs position bias due to causal attention. VLM-Enabled Rewriting Injection with Flexible Inference (Verifi) rewrites the original input in-context. The rewritten tokens used for DiT conditioning leverage the VLMs reasoning capabilities to contextualize the target scene into unified representation. Current image-generation models typically condition on separate representation spaces for text and image inputs. Most commonly, T5 embeddings [9] for text and variational auto-encoder (VAE) latents for images [21, 34]. However, these encoders operate at fundamentally different levels of abstraction: T5 captures highlevel semantic meaning presented via text prompts while VAEs preserve low-level pixel-level detail from images. This mismatch is evident concretely in editing tasks, where models struggle to balance content preservation with instruction adherence, often producing either unnatural copy-paste artifacts or excessive modifications [38]. We argue that separate encoding spaces force the DiT to expend capacity aligning heterogeneous features rather than synthesizing images, and that unified semantic space can alleviate this burden. VLMs naturally offer such shared representation for both text and images, but prior works conditioning on VLM features report failure to preserve the fine-grained visual details required for high-fidelity editing [1, 36]. We propose UniFusion  (Fig. 4)  , framework for building image-generation models with unified text and image encoding. frozen VLM serves as unified encoder for both modalities, eliminating the need for separate conditioning spaces. The framework comprises two key components: (1) Layerwise Attention Pooling (LAP), which aggregates information across multiple VLM layers to capture both fine-grained visual details and high-level semantic abstractions, and (2) VLM-Enabled Rewriting Injection with Flexible Inference (Verifi), which only exposes the DiT rewritten target prompt based on the original user input. Verifi reduces distribution shift between different input prompt formats and incorporates VLMs reasoning capabilities and world knowledge into the representations. We demonstrate the effectiveness of the UniFusion framework by training single model that achieves competitive performance on both text-to-image generation and editing compared to strong contemporaries, without requiring any supervised fine-tuning or reinforcement learning. Notably, the resulting model exhibits remarkable zero-shot generalization capabilities: it handles multi-reference image inputs despite being trained only on single-reference editing data, and can perform image-to-image variations when exclusively trained on text-conditional generation. We further observe cross-task positive transfer, where training on editing tasks improves the models text-to-image prompt adherence and aesthetic quality. In Fig. 1, 2, and 3a, we showcase exemplary use cases of UniFusion and the benefits of tight conditioning on unified encoder. One single model enables 1) high-fidelity text-image-generation with strong prompt following for complex instructions, 2) reliably usage of reference images for content and style, 3) text-driven image editing, 4) strong (visual) reasoning for complicated tasks, 5) usage of multiple image inputs and references for complex use cases, 6) generalization to unseen tasks, such as multi-reference images and cross-aspect ratio 5 Vision-Language Model as Unified Encoder in Image Generation object consistency. Our contributions can be summarized as follows: We propose UniFusion, framework for image generation with unified text and image encoding via frozen VLM, comprising two key components: Layerwise Attention Pooling (LAP) for multi-layer feature aggregation and VLM-Enabled Rewriting Injection with Flexible Inference (Verifi) for distribution alignment. We conduct extensive ablations across prominent conditioning strategies, demonstrating that LAP outperforms conventional last-layer extraction and alternative fusion schemes while maintaining architectural flexibility. We train and validate single model that achieves competitive performance on both text-to-image generation and editing tasks using only VLM input features, eliminating the need for separate VAE-based image reference conditioning. We demonstrate zero-shot generalization capabilities, including multi-reference composition despite training only on single-reference data, and image-to-image variation despite training exclusively on text-conditional generation. The rest of this work is structured as follows. In Sec. 2, we explore potential architectures for VLM conditioned image generation. We consider existing methods and propose novel approaches for extracting representations from multiple VLM layers. In direct comparison of these approaches, LAP outperforms all other methods. However, switching to unified VLM encoder requires additional considerations beyond current conditioning paradigms. Consequently, Sec. 3 goes into more detail on further design choices. We apply all of our insights to final UniFusion model which we introduce and evaluate in Sec. 4. Given the strong zero-shot capabilities we observed for UniFusion, we dedicate Sec. 5 to investigating these in detail before concluding."
        },
        {
            "title": "2 Architecture Selection",
            "content": "In this section, we first formally introduce potential paradigms for VLM-conditioned image generation. We perform direct comparisons in which LAP outperforms all other methods. Then, we evaluate the preservation of fine-grained image details through LAP and compare two different paradigms for feature injection."
        },
        {
            "title": "2.1 VLM conditioning candidates",
            "content": "We consider four different architectural paradigms for VLM-conditioned unified encoder as depicted in Fig. 5. For all approaches, we extract features from frozen VLM that are used to condition generative DiT. Notation. We use the following notations to describe different methods. Consider the frozen VLM and trainable DiT with NE and ND layers, respectively. Here, = 0 corresponds to the input layer and = to the last hidden state of the respective transformer. At any encoder layer lE , we consider hidden states xn in the shape of (bs, sl, hE), denoting batchsize, sequence length, and the VLMs hidden dimension, respectively. Note that the token sequence will consist of system and user prompts and contains multimodal tokens for text and images. For simplicity, we abstract any transformer block to operation Attn = softmax(QK )V , since details on scaling, multiple heads, and normalization are not affected by the considered methods. Last-Layer Hidden State Encoding. An intuitive approach is to extract representations from the last hidden layer of the VLM as drop-in replacement for text conditioning in existing architectures (Fig. 5a). Bellagente et al. [1] proposed an early application of this method. More recently, multiple papers have similarly used the last hidden layer of strong auto-regressive model [5, 36, 39]. The most important design choice in this setup is the post-processing or pooling of the extracted representation. For example, Bellagente et al. [1] reported that they needed additional fine-tuning of the VLM to produce useful embeddings, while Xie et al. [39] only added an RMSNorm layer [44]. Other variants of this approach have been proposed that use the penultimate layer instead of the last one [32]. 6 Vision-Language Model as Unified Encoder in Image Generation . An optional More formally, we extract conditioning as = xN as the hidden state of the encoder layer lE adapter A(c) = might project hD into the DiTs target dimension or implement additional normalization. is then concatenated with the noised VAE tokens svae, svae. Subject to further embedding, this concatenated sequence is the input to the first DiT layer lD . Consequently, the implementation of the DiT 0 layers lD remains unaffected. Layerwise Key-Value Fusion. One of the first proposed methods utilizing information from multiple layers is layer-wise Key-Value Fusion (Fig. 5b). Liu et al. [26] proposed to match the number of layers and hidden dimension of the image generator to the encoder model. In each attention layer, we then concatenate the Keys and Values of the DiT with the respective Keys and Values of the encoder. Key value fusion requires that NE = ND and hE = hD we extract KD and VD from the Attn operation in the VLM. The Attn operation in the respective DiT layers is adjusted such that Attn = softmax(QD(K on the sequence dimension. Consequently, we still concatenate the encoder sequence xn with the sequence of noisy VAE tokens svae, but do so on every and on the Attention Keys and Values instead of the residual stream between transformer blocks. layer VE) with concatenation 1. For each layer ln, ))(VD T Hidden State Injection (HSI). We also consider an improvement over the previous approach that eliminates the need for Key-Value matching. Instead, we inject the representation from corresponding layers directly in the DiT through numerical addition of the residual stream after each block (Fig. 5c). Again, we require that NE = ND and hdE = hdD which we add to the corresponding hidden state of the DiT xE 2. For each layer ln, , such that xE , we extract the hidden state xE . = xE + xD , Layerwise Attention Pooling (LAP). We propose to aggregate information from intermediate layers using learnable pooling module (Fig. 5d). LAP consists of 2 self-attention blocks that attend to the same token across layers, followed by fully connected (FC) layer pooling the representations into one feature (See App. Fig. 19). This LAP setup can be flexibly integrated into the DiT architecture in various ways (Sec. 2.2). , NE we extract the hidden state xE For each layer lE . We then stack this tensor of shape (bs, sl, n, hE) as (bs*sl, n, hE). The LAP module consists of two standard transformer blocks with full self-attention on layers n: = Attn(Attn(X E). We then unstack into its original shape and input it to FC layer, such that = C(X E) of shape bs, sl, hE. We can inject into the DiT as described for Last-Layer Injection. We also consider learning dedicated LAP for each DiT layer lD with each being injected via Hidden State Injection. , Benefits and Shortcomings of each Approach. The main limitation of Last-Layer Hidden State encodings is the restriction to representations from only one layer. Multiple prior works have established that transformer layers at different depths encode varying levels of information [11, 12, 17, 25]. Crucially, we argue that intermediate layers also carry different levels of semantic abstraction and fine-grained details that are necessary for unified encoder. Prior works have reported that the last-layer hidden state is insufficient in capturing detailed image contents and only provides semantic abstraction [1, 36]. While Layerwise Key-Value Fusion and Hidden State Injection extract features from multiple layers, they force tight coupling between the encoder VLM and the generative model, losing flexibility in the architectural design of the DiT. Since the number of layers and hidden dimensions is tied to the VLM, scaling the parameters of the generative model becomes challenging. Conversely, LAP offers great flexibility in DiT architecture design while aggregating representations of different semantic granularity from the VLM. 1Additionally, the number of number of attention heads, and attention heads dimensions need to match as well. 2However, the number of number of attention heads, and attention heads dimensions between Encoder and Decoder can differ. 7 Vision-Language Model as Unified Encoder in Image Generation (a) Last-Layer Hidden State (b) Layerwise Key-Value Fusion (c) Hidden State Injection Layerwise Attention Pooling (d) (Ours) Figure 5 Overview of considered architectures for unified VLM conditioning. Blue blocks within the VLM and DiT module denote selected layers, red denotes trainable modules, and gray denotes frozen modules. Figure 6 Comparison of different unified encoder candidates after 200k training steps on text-to-image performance (measured by VQA Score [24] on GenAI-Bench [23]). LAP stands fusion strategy, out as whereas Key-Value Fusion consistently exhibits the lowest performance. Neither naive drop-in replacement of Llama-3.1 surpasses the T5 baseline. the best"
        },
        {
            "title": "2.2 Experimental Evaluation",
            "content": "Experimental Setup All direct comparisons in this section are conducted with the following setup to ensure fair comparisons. We utilize standard latent DiT architecture with full self-attention and 2x2 patchification. The DiT has 32 layers with 32 attention heads and hidden dimension of 4096, resulting in 5 Billion parameter model. In line with previous work [26, 30], we use frozen Llama3.1-8B [16] for text-to-image tasks. Subsequently, we apply our findings to multimodal tasks using InternVL2.5-8B [7]. We also train model conditioned on Flan T5 XXL [9] to serve as baseline for text-to-image generation. For all experiments, we use the InstaFlow training objective [27] and AdamW optimizer [29]. We train on global batch size of 1024 for 200k steps, unless stated otherwise. While initially considered longer-running ablations, we observed that performance gaps at 200k steps serve as reliable indicator of final model performance after extended training. We train the Llama and T5 checkpoints on text only, whereas the InternVL version sees mix of 85%/5%/10% text/image/text-image batches, respectively. For these ablations, we use paired image-caption data as multimodal training samples. In all our settings, the respective encoding model remains frozen. For text-to-image generation, we track the VQA score on GenAI bench [23]. Through careful human comparison of pairwise VQA scores, we established the following evaluation setting as statistically meaningful. We observed VQA scores to correlate well with preferences in under-trained regimes below 80%. In these settings, gain of over 1 percentage points equates to noticeable performance improvement. The image-to-image tasks mainly serve as proxy for image encoding abilities; thus, we consider standard image difference metrics such as LPIPS [45], along with the more recent DreamSim metric [14]. Text-to-Image Prompt following In Fig. 6, we provide GenAI Bench performance of all architecture candidates. Overall, LAP stands out as the best option in terms of prompt adherence. The Llama 3.1 LAP version outperforms Last Hidden layer and HSI approaches. In comparison, Key-Value fusion performs significantly worse than all other methods. We argue that the prior success of this architecture [26] can be mainly attributed to high-quality training captions, rather than the conditioning methodology. We hypothesize two reasons leading to the shortcomings of Key-Value Fusion. First, naive Key-Value concatenation without 8 Vision-Language Model as Unified Encoder in Image Generation (a) Qualitative Examples (b) Quantitative Reconstruction Quality Figure 7 Image reconstruction quality when the input image is patchified into 1, 5, and 10 tiles before being fed to the VLM. LAP extracted VLM features are capable of preserving input image details without additional feature injection. Small, fine-grained images require more input image tiles for the VLM to be captured accurately. dedicated projections for each target layer is likely to lead to feature misalignment between the encoder and DiT. Second, the approach shares similarities with text-latent cross-attention in U-Nets, which we also found to be suboptimal compared to full self-attention text-conditioning. While LAP emerged as the best candidate, no Llama-3 conditioned checkpoint reaches the performance of the T5 baseline. These results align with independent observations [30] and can be attributed to multiple factors. For one, we observed T5 conditioned models to converge faster than Llama ones. After 400k training steps the gap in VQA score between the T5 and Llama-3 LAP model closes to 0.007 percentage points (from 0.041 points at 200k). Despite its prominent usage in prior works [26, 30], we also found Llama to be suboptimal encoder candidate. When using InternVL in the same setting, for example (Sec. 3.2, we observed significantly smaller gap to the T5 baseline. Additionally, further manual inspection reveals that while the Llama conditioned model does better on prompt understanding in many examples, it fails to produce key subject in the prompt for some samples. T5, in contrast, performs much more consistently. The positional bias introduced by the causal attention masking of auto-regressive models is major factor in this consistency gap [30]. Leveraging strong auto-regressive, decoder-only (multimodal) LLMs for conditioning in generative image tasks is thus not strictly plug-and-play but requires some additional adjustments over current paradigms (see Sec. 3.2). Based on our Llama-3.1 ablations, we decided to move forward with LAP as the architecture for UniFusion. It outperforms HSI and last hidden layer approaches, and provides higher flexibility than HSI with no inherent requirements on layer count or hidden dimension. We explore how to best utilize our LAP setting in Sec. 3, which ends up clearly outperforming T5 baselines on text-to-image generation and simultaneously supports further use cases. UniFusion Feature Extraction We extract features from multiple layers of the encoder model We aggregate these activations using Layerwise Attention Pooling (LAP) module consisting of two transformer blocks and fully connected layer Image Information Preservation With the benefits of LAP over other architectures established on textto-image tasks, we shift our focus to image inputs. unified encoder should be able to preserve fine-grained visual details to obtain precise edits, but previous work reported that VLM-based features specifically fall short of that hurdle [1, 36]. In addition to the importance of utilizing features over multiple layers of the VLM, the representation capacity of the extracted features also plays an important role, The number of image tokens at given hidden dimension 9 Vision-Language Model as Unified Encoder in Image Generation is often significantly lower than that of comparable VAEs, for example. Naturally, in such setting, adding VAE-encoded image input tokens improves the preservation of fine-grained details. We compared different numbers of image tiles used in the image encoding of the VLM. As shown in Fig. 7, the preservation of small features does indeed scale with the number of VLM tiles. At 10 tiles, any reconstruction errors become largely imperceptible. Even fine-grained structures, such as hairs or complex patterns, are preserved well. Thus, we conclude that VLM features are sufficient for image encoding. However, we need to accommodate high number of tiles or image tokens and utilize features from earlier layers. UniFusion Image Input Encoding UniFusion only uses extracted VLM features to encode input and reference images Increasing the number of image tiles in VLM image input encoding is crucial for preservation of fine-grained image details Thus, UniFusion eliminates the need to add VAE encoded image tokens to the DiT input Representation Injection When aggregating features with LAP, we are presented with different options on how and where to inject representations into the DiT. The two main options are: 1) to learn dedicated LAP module for each DiT layer lD for which we aim to inject features, and 2) Only extract single pooled representation c, which we concatenate with noised VAE tokens svae as input to the DiT We evaluate this design choice by comparing two models using LLama-3.1-8B [16] as an encoder. Here, the first model learns dedicated LAP for each target injection layer of DiT, whereas the second uses single representation injected in the DiTs input sequence. To control for total capacity across LAP modules, we scale the single LAP in the second setting to have similar parameter count as all LAP modules in the first setting. svae, similar to current conditioning approaches (See Sec. 2). , In this direct comparison, the single, pooled LAP representation setting strongly outperforms its counterpart where LAP features are injected into DiT layers (App. A.2). These results suggest that injecting conditioning into later layers of the DiT may be counterproductive, as we show in Sec. 3.1. UniFusion Feature Injection We use single LAP module, converting all layer activations into one feature These tokens are input as standard conditioning by pre-pending to the noisy VAE tokens in the DiT input sequence"
        },
        {
            "title": "3 UniFusion Design",
            "content": "We have established Layerwise Attention Pooling (LAP) as the most promising conditioning strategy for unified encoder in Sec. 2. In this section, we go into more detailed design choices of UniFusion and LAP."
        },
        {
            "title": "3.1 Layer Selection",
            "content": "While the previous results have shown clear benefits of aggregating representations from multiple layers, not all layers will be equally relevant, and information captured across different layers may be redundant. Thus, utilizing all layers may cause high memory overhead and potentially incentivize DiT to overfit on small subset of layers instead of the full capacity of VLM. We begin our analysis by visualizing the weights of the learnable pooling layer within LAP modules as shown in Fig. 8. We see that not all VLM layers contribute equally to the final representation. The model shows clear tendency to allocate higher weights to early-to-middle VLM layers when given the freedom to do so. This observation aligns with the intuition that semantic information useful for downstream finetuning lives in the earlier part of VLM. 10 Vision-Language Model as Unified Encoder in Image Generation We further discovered that LAP often pools the information from contiguous set of layers to form the final representation for single token. When plotting the Query-Key norms for individual tokens  (Fig. 9)  , we found highly clustered activation patterns. For example, the word \"drinking\" shows clusters for the 1st-4th or 21st-24th layer. Activations of adjacent layers in the transformer are highly similar, as shown in Fig. 10 and in other works [19, 22, 28]. We argue that while the image generator still benefits from extracted representations from all depths of VLM encoder, considering every layer adds unnecessary redundancy and suboptimal parameter utilization. Based on these insights, our final LAP architecture takes in every third layer of the input encoder. This setting balances the capture of relevant information against computational overhead and eliminates local clusters. model trained with this revised configuration now exhibits more uniform weight allocation as seen in Fig. 11. Notably, the penultimate VLM layer contributes the least to the pooled representation, despite its prominent use in current methods. In the final UniFusion architecture, we perform VLM layer dropout experiment as demonstrated in Fig. 12. We observe that image generation does not strongly rely on the first and last layers. When zeroing out the respective weights during pooling, the overall image composition remains unchanged. In contrast, dropping information from the middle layers results in significant deviations in the output. UniFusion LAP Layer Selection We extract features from every third layer across the depth of the VLM as the input for LAP This setup reduces overhead while maximizing information extraction from the VLM encoder"
        },
        {
            "title": "3.2 Position Bias",
            "content": "In our initial analysis in Sec. 2.2, we identified cases where the model fails to accurately capture key subject from the text prompt. This issue can be attributed to bias introduced by causal attention masking in the encoder transformer [30]. Since given token will only be attended to by the ones following it, information about subject mentioned late in the prompt will be insufficiently represented. We combat this bias by adding simple refiner to the representation adapter. Similar to Ma et al. [30], this module consists of two standard transformers with full self-attention over the sequence length sl. We found that this small bi-directional refiner significantly boosts performance over crude hidden-state extraction (See Ma et al. [30] for detailed ablations). Consequently, our final UniFusion adapter combines Layerwise Figure 8 Weight visualization of LAP modules pooling layers in Representation Injection setup (Sec 2.2). Each value denotes the magnitude of weights assigned to each VLM layer at given LAP modules pooling layer (smaller y-coordinate denotes layers closer to DiT input). On average, early VLM layers contribute more than later ones, while layer injection at later DiT blocks has lower weights. 11 Vision-Language Model as Unified Encoder in Image Generation Figure 9 Qualitative example of local clusters in LAP Key-Value activations norm. On many tokens, the model utilizes implicit clusters of adjacent layers. Values are averaged over tokens if word has more than 1 token. Attention Pooling with two-block refiner, operating on the pooled representation. We find that both components are crucial in achieving optimal performance. We provide more details in App. A.3. These results further support our findings from Sec . 2.2 that LAP remains the superior extraction approach. UniFusion Refiner The final UniFusion adaptor combines Layerwise Attention Pooling with bi-directional refiner This refiner adds two transformer blocks of full self-attention on the aggregated sequence to mitigate position biases"
        },
        {
            "title": "3.3 VLM-Enabled Rewriting Injection with Flexible Inference (Verifi)\nNext, we consider in more detail how to best leverage the VLM’s inherent capabilities for conditioning the\nDiT. We propose VLM-Enabled Rewriting Injection with Flexible Inference (Verifi), which folds the VLM’s\nworld knowledge and reasoning into our unified representation space.",
            "content": "Given user input consisting of text and images prompt, local image patches, image thumbnail, we use dedicated system prompt to instruct the VLM to generate target prompt as detailed description of the intended image. Prompt rewriting has become common practice for providing dense, detail-rich instructions to generative image models [2, 3, 13, 26]. However, rewriting prompts in our setting is fundamentally different from these approaches and offers some additional benefits. Firstly, Verifi does not require standalone rewriter with subsequent input encoding of the adjusted prompt. Instead, we perform single forward pass without re-encoding. Thus, the target tokens will still attend Figure 10 Adjacent layers in the VLM produce highly similar representations. Consequently, extracting features across each layer in the residual stream gives redundant information. Figure 11 An LAP using only every 3rd layer of the encoder VLM learns more uniform layer weights. 12 Vision-Language Model as Unified Encoder in Image Generation Figure 12 Qualitative analysis of different layer impact on final image. We drop crossed-out layers in LAP aggregation. Middle layers are crucial to capture the overall scene composition. In contrast first and last layers only capture rudimentary aspects of the scene. to the original prompt and retain that context in the final representations. For multi-modal inputs, the context provided through attention produces aligned features between modalities. Secondly, repetition of important information from the original prompt can further mitigate position attention biases (Sec. 3.2). For text-to-image generation, Verifi only uses rewritten tokens for DiT conditioning. In multimodal settings, we also inject all image tokens (patch and thumbnail) to ensure preservation of fine-grained details. We depict qualitative results and benchmark evaluations for self-rewriting in Fig. 13. In general, we found that Verifi significantly improves prompt following performance. In some cases, the differences between images are small. But crucially, it reliably mitigates catastrophic failure cases that miss important aspects of the prompt entirely. In the examples in Fig. 13, Verifi correctly places the parrot on the buildings, adds the mouse to the generated image, and depicts ancient buildings. Further, we observed that Verifi also improves performance on already long, detailed prompts. Consequently, the benefits extend beyond embellishing details and also contribute to mitigating position bias. While we use the same system prompt during training, we can meaningfully influence the models behavior by changing the system prompt at inference. Since we keep the VLM frozen and use its standard chat template during rewriting, all original capabilities of the model remain intact. We found that the usage of the chat template, an instruction-tuned model that was optimized for, is crucial in extracting meaningful features. Since Verifi imitates turn in regular chat interaction, we remain in-distribution of the VLM. Verifi also enables zero-shot reasoning over complex inputs, which we explore in more detail in Sec. 5. UniFusion Image Input Encoding The VLM in UniFusion uses Verifi to generate the final text prompt We use all image tokens for DiT conditioning, but only rewritten text"
        },
        {
            "title": "3.4 Finetuning vs. Training from Scratch",
            "content": "By now, we have established clear theoretical and practical benefits of unified encoder. However, training such model from scratch comes with significant computational requirements. Consequently, many prior works have sought out more efficient approaches to add new input modalities and capabilities to existing models [1, 31, 42]. We conduct controlled experiment comparing training model with VLM encoder from scratch against adopting pre-existing T5 checkpoint. To that end, we took model trained on T5 for 100k steps and then switched to multimodal conditioning using InternVL-2.5-8B LAP. We observed that roughly 10k steps are 13 Vision-Language Model as Unified Encoder in Image Generation (a) Qualitative Examples (b) GenAI Bench Evaluation Figure 13 Verifi improves prompt following. Comparison of InternVL-2.5-8B conditioned DiT with and without Verifi. Especially, complex prompts involving multiple subjects are generated more accurately. sufficient for the new conditioning setup to generate coherent images. When controlling for the total number of training samples, we find little difference in performance between models trained from scratch and switching from T5 halfway through. Both the benchmark performance and qualitative capabilities, including self-rewrite, multimodal, and zero-shot editing (see Sec. 5), are preserved in the continued model (details in App. A.4). These results allow us to conclude that continual pre-training with unified encoders from pre-existing model is as valid as training from scratch. Unless there are additional changes to the training setup, adopting an existing checkpoint will save compute resources with no obvious drawbacks. Conversely, there is also no benefit in using T5 for early training steps, as any model trained with UniFusion approach will quickly converge to better text-to-image performance while enabling additional use cases. UniFusion Training Regime For better compute utilization, we train the final UniFusion model by adapting an early T5-conditioned checkpoint. This yields no performance difference from training from scratch, further enabling UniFusion to be used on any pretrained T5-conditioned models."
        },
        {
            "title": "4 Final UniFusion Model",
            "content": "Finally, we integrate all the learnings from previous sections into scaled-up model. We increase the DiT parameters to 8 billion and the total number of training samples to approximately 830 million. Instead of InternVL-2.5, we use InternVL3-8B [47]."
        },
        {
            "title": "4.1 UniFusion Design & Training\nWe design our final UniFusion model to extract features from every third layer of the VLM and aggregate\nthem into a single representation via our LAP module. The LAP contains two transformer blocks aggregating\nthe representation of any token across layers. This sequence is then pooled into one dense representation\nwith a simple fully connected layer. The LAP is followed by a refiner of two bidirectional transformer blocks,\nmitigating position bias across the input sequence. We inject the extracted representation only in the DiT’s\ninput sequence, which operates on a VAE latent space with a compression factor of 16.\nWe only encode input images through the VLM and do not concatenate any additional VAE tokens to the\nDiT input. UniFusion leverages self-rewrite of user inputs, with only the image and rewritten tokens being\nused in DiT conditioning. For image inputs, we train the model on up to 10 tiles. Given our insights from\nSec. 3.4, we optimize compute usage by doing an early checkpoint handoff from a pre-existing T5 checkpoint.\nAs described in Sec. 2, we train the base model on a mixture of text-to-image, image reconstruction, and\njoined text-and-image samples. Subsequently, we continue training with instruction data for image editing",
            "content": "14 Vision-Language Model as Unified Encoder in Image Generation Figure 14 Qualitative comparison on long form text-to-image generation prompts comparing UniFusion to Bagel [10], Flux.1 [dev] [20] and Qwen-Image [36]. 15 Vision-Language Model as Unified Encoder in Image Generation Category Macro Avg Micro Avg entity - whole entity - part entity - state attribute - color attribute - size attribute - shape attribute - texture attribute - other relation - spatial relation - non-spatial global - other - count other - text Model Size Bagel [10] Avg. Top-4 0.715 0.786 0.904 0.814 0.667 0.823 0.740 0.702 0.703 0.652 0.706 0.579 0.639 0.769 0.600 0.901 0.873 0.988 0.929 0.925 0.962 0.904 0.873 0.922 0.891 0.942 0.807 0.864 0.933 0.771 14B MoT Flux.1 [dev] [20] Qwen-Image [36] UniFusion (Ours) Avg. Top-4 Avg. Top-4 Avg. Top-4 0.693 0.753 0.880 0.785 0.617 0.779 0.730 0.662 0.647 0.625 0.677 0.549 0.641 0.765 0.655 0.899 0.851 0.984 0.924 0.890 0.958 0.882 0.873 0.900 0.894 0.946 0.761 0.897 0.944 0.833 0.802 0.841 0.942 0.869 0.733 0.866 0.795 0.777 0.779 0.720 0.778 0.701 0.714 0.850 0.900 20B 0.943 0.914 0.995 0.969 0.940 0.984 0.914 0.919 0.945 0.928 0.967 0.890 0.888 0.961 0.958 0.731 0.787 0.894 0.805 0.673 0.803 0.771 0.682 0.729 0.698 0.712 0.643 0.688 0.791 0.615 8B 0.915 0.880 0.989 0.938 0.943 0.963 0.930 0.879 0.926 0.931 0.947 0.826 0.898 0.955 0.771 12B Table 1 UniFusion achieves competitive performance against much larger models trained on more data. Scores on modified DPG-Bench. We report average and best generation across four seeds at 1024px resolution. Macro Average is taken as the mean over scores per category, whereas Micro averages scores across all prompts. Results are scored by Gemma-3-27B with extensive CoT to reduce hallucinations in scoring. and denote best and second-best score, respectively. and reference workflows. We found roughly 10k steps of instruction training to be sufficient to support this task. For all stages of training, we use no web-scraped data and only rely on images with permissive licenses for generative image training. For this study, we do not perform any further post-training, which we leave for future work."
        },
        {
            "title": "4.2 Evaluation",
            "content": "Qualitative Examples. We showcase text-to-image outputs of the UniFusion model in Figs. 1 and 3a. Additionally, Figs. 2 and 3b depict image editing and reference examples. We use the same model for both image editing and text-to-image workflows. Interestingly, we observed that continued training on image editing and image reference tasks also improved the models text-to-image capabilities (see Sec. 5). In general, we find UniFusion to show strong performance for its size, efficient training regime, and without any supervised finetuning or reinforcement learning. UniFusion is capable of accurately generating images from long, complex prompts and excels in aesthetically pleasing, photorealistic generations, especially. In direct comparison with larger models, UniFusion remains highly competitive and especially benefits from improved visual understanding in image reference tasks. Importantly, for the direct text-to-image comparisons in Fig. 14 and image editing in Fig. 15, Bagel and UniFusion are the only models using the same checkpoint for both tasks. In contrast, Flux and Qwen-Image rely on dedicated versions for each task. Quantitative Evaluation. Naturally, we ran several standardized benchmarks to judge the performance of the final model. Contrary, however, to under-trained ablations like those we conducted in Sec. 3, the usefulness of these benchmarks and metrics diminishes significantly when approaching their saturation. For example, consider GenEval [15] and DPG-Bench [18], two popular benchmarks for evaluating promptfollowing capabilities. For both, we found the originally proposed evaluation settings to have immensely high error rates in accurately judging generated images. The noise of the benchmark itself far exceeded performance differences between models of few percentage points. We also observed other crucial issues, 16 Vision-Language Model as Unified Encoder in Image Generation Figure 15 Qualitative comparison on image reference and editing tasks comparing UniFusion to Bagel [10], Flux.1 Kontext [21] and Qwen-Image [36]. (Zoom in for details) such as unintentional penalization of photorealistic outputs, questions that cannot be answered objectively, or inaccurate score aggregation in DPG-Bench3. We provide more details in App. B.1. In Tab. 1, we report UniFusions performance in comparison to other models on revised version of DPG-Bench (See App. B.2). Since most generative image applications provide users with up to four outputs, we report both the best-out-of-4 score and the average. Despite no post-training and limited training 3Surprisingly, this implementation error appears to have gone largely unnoticed, despite being documented as an issue in the official GitHub repository. This bug has led to multiple papers reporting mathematically impossible results. For example, in the Qwen-Image paper, all category scores are reported to be higher than the overall average (Tab. 3, Page 21 [36]). 17 Vision-Language Model as Unified Encoder in Image Generation Figure 16 The unified VLM encoder enables advanced visual reasoning for textual image editing. (Examples on early checkpoint and not indicative of final model quality) sample, UniFusion remains competitive with significantly larger, heavily post-trained models. The qualitative comparison in Fig. 14 also highlights the competitive prompt-following capabilities of UniFusion. Further we found, UniFusion less likely to generate characteristic AI artifacts like over-saturated colors and smoothed textures. When directly comparing the scores of Qwen-Image and UniFusion, we see larger gap between the Avg. and best-out-4 scores for UniFusion. The difference in Macro Avg. scores is 0.142 and 0.184 for Qwen-Image and UniFusion, respectively. We believe this to be direct result of the post-training for Qwen-Image."
        },
        {
            "title": "5 Emergent Abilities",
            "content": "During our experiments, we observe UniFusion to exhibit many valuable zero-shot capabilities without explicitly being trained for them. This behavior is direct benefit of unified VLM encoder architecture. Any of the capabilities learned from the VLMs extensive training regime are retained and transferred to image generation tasks. Additionally, the unified space of contextualized text and image eliminates large distribution shifts between tasks."
        },
        {
            "title": "5.1 Reasoning & Complex Prompts",
            "content": "Verifi allows the models to explicitly leverage the world knowledge and reasoning capabilities of the encoder VLM. In Fig. 3a, we show text-to-image examples using highly abstract text inputs. The model is capable of decomposing these instructions without any external sources. (a) Zero-shot image-to-image generation. Examples generated by model only trained on text-to-image generation. When presented with image features, the model captures overall scene composition and high level of detail. (b) Zero-shot image editing. Examples generated by model never trained on image editing. Figure 17 Conditioning image generation on LAP extracted VLM features enables zero-shot generalisation to unseen tasks and modalities. 18 Vision-Language Model as Unified Encoder in Image Generation Figure 18 UniFusion-Edit leads UniFusion-Base by significant margin in text-to-image A/B test with 180 annotators, 616 prompts across diverse concepts, with 2 seeds each (3 votes per image pair). For example, given the prompt: \"The animal that represents the zodiac sign between Aries and Gemini\", InternVL rewrites \"A majestic bull stands in field of golden wheat, its horns curved in fierce display of strength and virility\". The VLM correctly references the zodiac sign Taurus, which is represented by bull, and decodes the user input into new prompt, allowing the DiT to successfully generate the animal. We observed similar capabilities for visual reasoning examples. For example, we can decompose hypothetical scenarios to perform image editing requiring multi-hop reasoning and world knowledge. We showcase some examples in Fig. 16. The VLM correctly reasons about hypothetical effects of the mass of different animals or impacts of temperature changes over time and decomposes those into an edit instruction. The DiT is then able to perform an edit satisfying the original user intent."
        },
        {
            "title": "5.2 Generalization to unseen modalities",
            "content": "Throughout all experiments, we observed models to generalize well to inputs that were never observed during DiT training. Instead, these capabilities are key benefit of unified input space. For example, model solely trained on text-to-image generation can still capture the semantics of image inputs as seen in Fig. 17a. While the reconstruction is not pixel-perfect, the generated image still accurately captures the important aspects of the input image. This behavior can be attributed to the fact that the VLM yields decently aligned representation spaces for text and image, enabling zero-shot transfer to new modalities. Similarly, we find that models only trained on text or Image sequences, but not multimodal ones, can still be used for image editing. We show examples in Fig 17b where we successfully manipulate the content of an image by changing the respective textual scene description. Importantly, if model is trained equally on text and image sequences, image tokens always take precedence at inference. This observation aligns with the findings of Bellagente et al. [1]. While they manipulated attention values directly to counteract this imbalance, we found that adjusting the training data composition works equally well. Specifically, when we condition the DiT with image tokens for the first 10-20% of steps, and text tokens for the remaining 80-90%, the output image preserves most of the input images content and semantics, even though the model was never trained with any textual image editing data."
        },
        {
            "title": "5.3 Cross-Task Improvements",
            "content": "In Section 4, we observed that continued training on image editing and image reference tasks also improved the models text-to-image quality. In Tab. 2, we see significant improvement on DPG-Bench of over 2 percentage points in Micro Avg. We further conducted human user study comparing checkpoints before and after training on editing data, as shown in Fig. 18. Annotators strongly prefer the images generated by the UniFusion-Edit checkpoint across all aspects of text-to-image generation. We hypothesize that this 19 Vision-Language Model as Unified Encoder in Image Generation Category UniFusion-Base UniFusion-Edit Avg. Top-4 Avg. TopMacro Avg Micro Avg entity - whole entity - part entity - state attribute - other attribute - color attribute - size attribute - shape attribute - texture relation - spatial relation - non-spatial global - other - count other - text 0.699 0.760 0.876 0.772 0.627 0.661 0.785 0.744 0.660 0.704 0.681 0.591 0.670 0.752 0.560 0.906 0.863 0.993 0.938 0.919 0.912 0.970 0.925 0.890 0.920 0.943 0.826 0.893 0.961 0.688 0.731 0.787 0.894 0.805 0.673 0.698 0.803 0.771 0.682 0.729 0.712 0.643 0.688 0.791 0.615 0.915 0.880 0.989 0.938 0.943 0.931 0.963 0.930 0.879 0.926 0.947 0.826 0.898 0.955 0.771 Table 2 Image Editing and Image Reference Training significantly improves UniFusion capabilities in text-to-image generation. Scores on modified DPG-Bench. We report average and best generation across four seeds at 1024px resolution. Macro Average is taken as the mean over scores per category, whereas Micro averages scores across all prompts. Results are scored by Gemma-3-27B with extensive CoT to reduce hallucinations in scoring. behavior is direct benefit of unified encoder architecture. Since the representation space always supported multimodal inputs, the transition from text-to-image towards editing is not significant shift. Instead, this stage increases concept coverage and refines the models representations. Since UniFusion eliminates the need to introduce VAE-encoded image reference inputs, the DiT does not need to adjust its embedding behavior. Consequently, we are now able to reap the benefits of further task coverage without the adverse effect of new input structure."
        },
        {
            "title": "5.4 Zero-shot multi-reference capabilities",
            "content": "Lastly, we also observed strong zero-shot abilities for image reference tasks. The editing data in Sec. 4, contained only examples with single reference image. Additionally, all training samples fix the input and output images to the same aspect ratio. Nonetheless, the examples in Fig. 3b demonstrate that UniFusion is capable of accurately composing scenes from multiple reference images. In these use cases UniFusion also seamlessly handles input and output images of different aspect ratios and resolutions and applies unprompted shifts in perspective when needed. For example, the scene reference on top of the pyramids is given in different aspect ratio than the output image. UniFusion expands the scene and slightly shifts the perspective to account for that change while preserving fine-grained image details."
        },
        {
            "title": "6 Conclusion",
            "content": "Limitations & Discussion. While our UniFusion approach provides significant benefits over other conditioning methods, there are some limitations worth discussing. Naturally, auto-regressive self-rewriting of all input prompts with an 8B transformer comes with an increase in compute and runtime during encoding. However, given the prominence of prompt rewriting in general and other approaches using VLM conditioning, this limitation is not unique to UniFusion. Furthermore, we identified some issues related to rendering text in scenes, which also impact the respective scores in Tab. 1. In general, the model is capable of generating and editing typography, as shown in Figs. 1 and 2. However, we found InternVL to be particularly bad at spelling. Consequently, the model often misspells 20 Vision-Language Model as Unified Encoder in Image Generation text in the rewritten prompt, leading to the generation of incorrect or illegible text. We can further pinpoint this issue to InternVL specifically by having an external model perform the rewriting. In this scenario, even when re-encoding the text through InternVl, UniFusion reliably generates text in images. So far, our experiments have focused on InternVL as candidate encoder. To ensure that UniFusion generalises beyond one VLM family, we trained an additional model based on Gemma. Overall, we found Gemma-based models to work similarly well and conclude that UniFusion is not limited to any specific VLM. We share more details on the Gemma experiments in App. A.5. In conclusion, this work introduced UniFusion, framework that uses single Vision-Language Model (VLM) as unified encoder for generative image models. We proposed novel Layerwise Attention Pooling (LAP) module, which aggregates features from multiple layers of frozen VLM. Through structured experiments, we demonstrated that LAP outperforms other architectures, such as last-layer encoding and key-value fusion, in both prompt adherence and the preservation of fine-grained image details. Additionally, we provide strong evidence for critical design choices in best leveraging VLMs for generative image tasks. We derive practical suggestions on layer selection, bi-directional refiners, and the benefits of Verifi. The UniFusion approach successfully eliminates the need for multiple image encoders. By leveraging the powerful reasoning and world knowledge of the VLM, UniFusion gains significant zero-shot capabilities and generalises well to unseen use cases. The model can interpret complex, abstract prompts and perform visual reasoning and image reference tasks without explicit training. Furthermore, this framework allows for efficient adaptation of existing models, making it computationally viable approach. Overall, our findings establish that the UniFusion approach is robust and flexible strategy to use VLMs as unified encoders. This research paves the way for developing more capable and intuitive image generation systems."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Alexandru Costin and Jingwan Lu for their support. We thank Sai Bi for continued discussions, especially on the Llama series of ablations. Thank you to Mingze Xu, Felix Friedrich, Melissa Hall, and Rena Ju for their feedback on the initial draft of the paper."
        },
        {
            "title": "References",
            "content": "[1] Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, Andrés Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, and Samuel Weinbach. 2023. Multifusion: Fusing pre-trained models for multi-lingual, multimodal image generation. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, and OpenAI. 2023. Improving image generation with better captions. [3] Manuel Brack, Sudeep Katakol, Felix Friedrich, Patrick Schramowski, Hareesh Ravi, Kristian Kersting, and Ajinkya Kale. 2025. How to train your text-to-image model: Evaluating design choices for synthetic training captions. [4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. 2019. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155. [5] Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, and Baobao Chang. 2025. Multimodal representation alignment for image generation: Text-image interleaved control is easier than you think. arXiv preprint arXiv:2502.20172. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. 2025. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811. 21 Vision-Language Model as Unified Encoder in Image Generation [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2025. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. [8] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. 2022. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). [9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research (JMLR), 25. [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. 2025. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683. Introduces the unified multimodal model BAGEL; v3 (2025-07-27). [11] Nadir Durrani, Fahim Dalvi, and Hassan Sajjad. 2023. Discovering salient neurons in deep NLP models. Journal of Machine Learning Research (JMLR), 24. [12] Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and Yonatan Belinkov. 2020. Analyzing individual neurons in In Proceedings of the Conference on Empirical Methods in Natural Language pre-trained language models. Processing (EMNLP). [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML). [14] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. 2023. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [15] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. 2023. Geneval: An object-focused framework for evaluating text-to-image alignment. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu 22 Vision-Language Model as Unified Encoder in Image Generation Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, 23 Vision-Language Model as Unified Encoder in Image Generation Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. [17] Lucas Torroba Hennigen, Adina Williams, and Ryan Cotterell. 2020. Intrinsic probing through dimension selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). [18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. 2024. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135. [19] Felix Krause, Timy Phan, Vincent Tao Hu, and Björn Ommer. 2025. TREAD: token routing for efficient architecture-agnostic diffusion training. CoRR, abs/2501.04765. [20] Black Forest Labs. 2024. Flux. https://github.com/black-forest-labs/flux. [21] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. 2025. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742. [22] Tim Lawson, Lucy Farnik, Conor Houghton, and Laurence Aitchison. 2025. Residual stream analysis with multi-layer saes. In Proceedings of the International Conference on Learning Representations (ICLR). [23] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, and Deva Ramanan. 2024. Genai-bench: Evaluating and improving compositional text-to-visual generation. Preprint, arXiv:2406.13743. [24] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. 2024. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291. [25] Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. 2025. On the biology of large language model. Transformer Circuits Thread. [26] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. 2024. Playground v3: Improving text-to-image alignment with deep-fusion large language models. Preprint, arXiv:2409.10695. [27] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. 2024. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In Proceedings of the International Conference on Learning Representations (ICLR). [28] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and Beidi Chen. 2023. Deja vu: Contextual sparsity for efficient llms at inference time. In Proceedings of the International Conference on Machine Learning (ICML). [29] Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations (ICLR). [30] Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. 2024. Exploring the role of large language models in prompt encoding for diffusion models. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [31] Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, and Ran Xu. 2023. Gluegen: Plug and play multi-modal encoders for x-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). [32] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, and Peng Gao. 2025. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758. 24 Vision-Language Model as Unified Encoder in Image Generation [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML). [34] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, Xiaowen Jian, Huafeng Kuang, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yanzuo Lu, Zhengxiong Luo, Tongtong Ou, Guang Shi, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Wenxu Wu, Yonghui Wu, Xin Xia, Xuefeng Xiao, Shuang Xu, Xin Yan, Ceyuan Yang, Jianchao Yang, Zhonghua Zhai, Chenlin Zhang, Heng Zhang, Qi Zhang, Xinyu Zhang, Yuwei Zhang, Shijia Zhao, Wenliang Zhao, and Wenjia Zhu. 2025. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427. Technical report; v2 (2025-09-28). [35] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. [36] Qwen Team. 2025. Qwen-image technical report. arXiv preprint. [37] Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, and Afshin Dehghan. 2025. Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [38] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. 2025. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083. [39] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. 2025. SANA: efficient high-resolution text-to-image synthesis with linear diffusion transformers. In Proceedings of the International Conference on Learning Representations (ICLR). 25 Vision-Language Model as Unified Encoder in Image Generation [40] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. 2025. Show-o: One single transformer to unify multimodal understanding and generation. In Proceedings of the International Conference on Learning Representations (ICLR). [41] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [42] Fulong Ye, Guang Liu, Xinya Wu, and Ledell Wu. 2024. Altdiffusion: multilingual text-to-image diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). [43] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. 2023. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591. Introduces CM3leon (Chameleon). [44] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. In Proceedings of the Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS). [45] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). [46] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039. [47] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479. 26 Vision-Language Model as Unified Encoder in Image Generation Figure 19 UniFusion adapter for layerwise representation aggregation. Representations from multiple VLM layers are aggregated using Layerwise Attention Pooling (LAP). The aggregated representations are subsequently passed through Refiner to mitigate position bias."
        },
        {
            "title": "Appendix",
            "content": "A Additional Results & Experimental Details In this section, we provide additional experimental details and results. A.1 UniFusion layerwise representation aggregation We provide visual aid for UniFusions representation aggregation adapter in Fig. 19. As mentioned in Sec. 4, UniFusion extracts features from every third layer of the VLM and aggregates them into single representation via our LAP module. The LAP contains two transformer blocks aggregating the representation of any token across layers. This sequence is then pooled into one dense representation with simple fully connected layer. The pooled LAP representation is followed by refiner of two bidirectional transformer blocks, mitigating position bias across the input sequence. In this context, our transformer blocks use multi-head attention with 32 attention heads. We apply RMS normalization before and after self-attention. These operations are followed by feed-forward block using Silu as the activation function. We expand and contract the hidden dimension by factor of 1.3 for the non-linear activation. A.2 Representation Injection In Fig. 20 we compare different injection paradigms for LAP. In the first setting, we train dedicated LAP for each DiT layer and inject the respective representation through hidden state injection. In the second setting, we only extract one LAP representation and input it to the DiT without injections in later layers. In this direct comparison, the latter setting strongly outperforms the former. These results suggest that injecting conditioning into later layers of the DiT may be counterproductive. A.3 Bidirectional Refiner In Fig. 21, we measure the benefit of bi-directional refiner. We compare T5 baseline against two InternVL2.5 8B models. The first uses bi-directional refiner on penultimate layer features, and the second combines 27 Vision-Language Model as Unified Encoder in Image Generation (a) Qualitative Examples (b) GenAI Bench Evaluation Figure 20 Injecting aggregated representations at different DiT depth does not improve performance. Comparison of InternVL-2.5-8B with LAP feature extraction. First version injects dedicated representations per DiT layer (with HSI), second version pools one representation for DiT conditioning (without HSI). Comparison at 200k training steps an LAP with bi-directional refiner. Comparing these results to Fig. 6, we observe that the combination of InternVL instead of Llama and the addition of bi-directional refiner now closes the gap to the T5 baseline on text-to-image capabilities. Nonetheless, layer-wise attention pooling still outperforms representation extraction from last layers. Consequently, both multi-layer feature extraction and bi-directional refinement are crucial when using decoder-only auto-regressive models for input encoding. A.4 Continued Training vs Finetuning In order to assess whether UniFusion encoding requires training from scratch or could benefit from continued training from T5 model, we make direct comparison. With total compute budget of 250k steps, we train two different models. One that was trained for 100k steps using T5 and changes to InternVL-2.5-8B for the remaining 150k steps. The second one is trained using InternVL-2.5-8B from scratch. As shown in Fig. 22, both models converge to the exact same performance and substantially outperform the T5 baseline. Based on these results, we can draw two conclusions. First, given an existing T5-conditioned model, we can save compute by continuing late (a) Qualitative Examples (b) GenAI Bench Evaluation Figure 21 Evaluation of bi-directional refiner impact. InternVL2.5-8B model with refiner closes the performance gap to the T5 baseline (cf. Fig. 6. Nonetheless, layer-wise attention pooling still outperforms representation extraction from the last layers. Comparison at 250k training steps. 28 Vision-Language Model as Unified Encoder in Image Generation (a) Qualitative Examples (b) GenAI Bench Evaluation Figure 22 Evaluation of training UniFusion conditioning from scratch vs. continuing from T5. Both approaches produce models with the same capabilities. Comparison at 250k training steps. Continued checkpoint switches from T5 to InternVL2.5-8B at 100k steps. A.5 Gemma-Based UniFusion In addition to the InternVL-based models in the main body, we also trained UniFusion version based on Gemma-3-12B-it [35]. With Verifi the model achieves strong VQA score of 84.4% on GenaiBench [23]. We provide qualitative examples in Fig. 23 for text-to-image generation and image reconstruction. For image reconstruction using one tile (i.e., thumbnail) as input to the VLM, we observe slight variations. Based on our insights in Sec 2.2, we expect these artifacts can be resolved by increasing the tiling in the VLM inputs. Additionally, Gemma has higher compression ratio for InternVL when using the same number of (a) Text-to-image examples generated with UniFusion-Gemma using self-rewrite. (b) Image reconstruction with UniFusion-Gemma at 1 input tile. Similar to the experiments in Sec. 2.2, we observe slight variations when using only one tile. We expect these artifacts to resolve themselves at increased input resolution. Figure 23 Text-to-image and image reconstruction examples of the UniFusion-Gemma model. 29 Vision-Language Model as Unified Encoder in Image Generation Prompt-ID Question diffusiondb3 diffusiondb3 partiprompts162 midjourney21 partiprompts122 partiprompts159 partiprompts83 partiprompts126 71 COCOval2014000000513096 posescript2 countbench16 countbench17 Is there an Indian woman? Is there Chinese man? Is the car tableau dreamlike? Is the sculpture majestic? Does the scene feel expansive? Are the hues uplifting? Is the cup lovestruck? Does the squirrel have rebellious punk rock vibe? Are the printers humming with activity? Is the man in the suit explaining the significance of the exhibit? Does the individual exhibit bodily awareness? Are the plates likely originating from London in the year 1752? Do the photographs have historical significance? Table 3 Examples from questions in DPG-Bench that are hard to assess objectively from generated images. tiles. These results provide evidence that our UniFusion approach works reliably across different models and architectures."
        },
        {
            "title": "B On the reliability of Image Generation Benchmarks",
            "content": "In this Section, we discuss common issues we observed in prevalent generative image benchmarks. While we focus this analysis on text-to-image generation, we have observed the same issues on benchmarks in other tasks. Subsequently, we discuss our revised version of DPG-Bench that resolves some of these issues. In general, we still advocate for identifying more reliable metrics that robustly work for strong models. B.1 Evaluating popular benchmarks In general, the limited reliability of these benchmarks and respective metrics can be broken down into three categories. Automated evaluation error. The majority of benchmarks rely on separate models to evaluate generated images. We observed the error rate of these models to far exceed reasonable metrics. For example, GenEval [15] relies on pre-trained object detection model [4, 8] and CLIP [33] for attribute matching. For benchmark to remain useful, everybody should follow pre-determined setting, making scores comparable. Unfortunately, these models become outdated quickly and have high failure rates for the designated tasks. We show examples of incorrect GenEval assessments in Fig. 24a where either the initial object detection, object count, or attribute binding fails. For some model evaluations, we found incorrectly flagged generation fails of this setup to exceed 70%. Given that current models tend to achieve good performance on these benchmarks, the error of the metric itself tends to exceed the difference between the compared models. Thus, discerning any perceived improvements from measurement noise becomes impossible. We found question-answering-based settings like the one proposed by DPG-Bench [18] to suffer from similar issues. We depict some examples in Fig. 24b. Specifically, the VLM proposed by DPG-Bench hallucinates incorrect answers at an alarming rate. As shown, these failures even occur for well-composed images, with no major artifacts and the subject in question clearly visible in the image. While some of these problems can be attenuated by using more capable models and more comprehensive evaluation setting (See App. B.2), the underlying problems remain. Ill-formulated tasks. Since comprehensive benchmarks are time-consuming to build, they often rely on LLMs to construct instructions or evaluation targets. However, this has led to an increasing number of evaluation objects that are impossible to evaluate objectively. 30 Vision-Language Model as Unified Encoder in Image Generation (a) Examples of incorrect object, attribute, and count assessments in GenEval. (b) Examples of Question-Answering failures in DPG-Bench assessments. Figure 24 Current generative image benchmarks incorrectly score simple examples, including the presence of clear subjects in the image. For example, DPG-Bench contains multitude of questions that are subjective to some extent, cannot be grounded in single image, or are otherwise questionable. We provide some examples in Tab. 3. In general, there is large number of questions attempting to ascertain the nationality of people, which is impossible to assess without context. Further, since lot of the underlying text prompts are heavily embellished with subjective adjectives. Given the collection methodology of DPG-Bench, this likely stems from synthetically written prompts. Crucially, the GPT-written questions often pick up on these adjectives. However, assessing if painting does radiate or if squirrel is rebellious is highly subjective and should not be central to an objective benchmark. Lastly, some questions like the historical significance of photograph are next to impossible to assess from an image alone, without providing further context. Questionable capability prioritization. Naturally, generative image tasks have to satisfy multipleoften orthogonalconstraints. However, we found that current benchmarks and metrics tend to heavily prioritize very literal prompt adherence. Take, for example, the image of the toilet and mouse in Fig. 24a. One could argue that this scene composition satisfies some aesthetic aspects by placing the toilet only partially visible in the background. In general, we found all evaluation settings to judge incomplete objects or out-of-focus backgrounds as violating prompt adherence. However, both might be intended behavior for aesthetic quality and composition, as well as accurate depth of field for photographic image styles. Even human-preference metrics like ImageReward [41], tend to prioritize very literal prompt following over other aspects. However, from in-house user studies, we found that this implicit waiting for strict prompt adherence over other quality aspects does not necessarily correlate with actual human preference. B.2 Refined DPG benchmark For our analysis in Sec. 4.2, we made the following adjustments to DPG-Bench. Upgrade Question-Answering Model. We changed the VLM used for question answering to Gemma3-27b-it [35]. We specifically chose the strongest model from the Gemma family, since we were evaluating models conditioned on InternVL and QwenVL models. Consequently, to remove unintended evaluation bias, 31 Vision-Language Model as Unified Encoder in Image Generation we opted for the strongest open-weight VLM outside of these model families. Instead of prompting the model to directly generate yes/no answer, we extended the inference time to compute for each question. To that end, we tasked the model to perform extensive chain-of-thought generation for all image aspects relevant to the question, before generating yes/no answer. We provide the system prompt for this model in Tab. 4. Fix score aggregation. The official DPG evaluation script provided in the authors GitHub does not aggregate scores correctly. While the overall score is calculated across all images per prompt, the subcategories only use the score of the last image. This implementation bug, has also been pointed out by other users 4 but remains unfixed at the time of writing. Consequently, we re-implemented score aggregation to ensure correct results. Improved presentation. Since subcategories in DPG-Bench are heavily skewed towards entities, we not only report the overall mean (Micro Avg), but also the mean of category-wise performance (Macro Avg). In line with classic Computer Vision literature, we also report the best-out-of-n performance in addition to the mean over multiple seeds. This score more accurately reflects the experience of most users, since many image generation platforms and local setups will provide multiple seeds to pick from. 4https://github.com/TencentQQGYLab/ELLA/issues/60 Vision-Language Model as Unified Encoder in Image Generation Youre specialized visual assistant for Visual Question Answering (VQA) task. Your main job is to answer users question about an image with simple **yes** or **no**. Your analysis **must** be based **only** on what you can clearly see in the image. Before giving your final answer, you **must** explain your reasoning using the **Chain of Thought** method inside of <think></think> tags. ## Core Directive: Clear Interpretation Your analysis needs to be **strict**, **literal**, and based purely on visual evidence. You should still allow for small level of artistic interpretation and account for objects being out of focus, in the background, or partially obscured. Your goal is to answer based only on whats unambiguously visible. * **Base on Visual Evidence:** Your answer **must** come directly from whats visible in the image. Dont guess whats outside the frame or what an object might imply. If you cant see it, it doesnt count. * **Literal Meaning Only:** Take the question and the image at face value. Dont look for symbolic, artistic, or metaphorical meanings. * **Object Clarity is Required:** Only identify objects you can see with **reasonable confidence**. An objects main features have to be visible, even if theyre bit blurry or seen from weird angle. Dont identify things based on vague shapes. * **No Assumptions on Quantity:** If the question asks about number of items, you **must** see that exact number. Dont assume some are hidden. number **larger** than the requested quantity is also acceptable. ## Your Thought Process (Chain of Thought) Follow these four steps inside your <think> block for every question: 1. **Analyze the Image:** Briefly describe the images key parts and how clear they are. 2. **Deconstruct the Question:** Pinpoint the specific object, feature, or number the question is asking about. 3. **Synthesize and Reason with Strictness:** Connect the question to the visual evidence. Apply the rules to verify if the item is clearly visible. 4. **Formulate Conclusion:** State your final decision based on the visual evidence youve found (or havent found). ## Examples in Action ### Example 1: Object Identification * **Image:** photo of person running on city street. Far in the background, slightly blurry, octagonal red shape is visible on pole. * **Question:** Is there stop sign in the image? **Your Response:** <think> 1. **Analyze the Image:** The image shows jogger on city street. The background is bit out of focus, but theres red, octagonal shape on pole. 2. **Deconstruct the Question:** The question asks if theres \"stop sign.\" stop signs key features are its red color and octagonal shape. 3. **Synthesize and Reason with Strictness:** The object in the background clearly shows the two main features of stop sign: its red and its an octagon. Even though its too blurry to read any text, that combination is enough to identify it with reasonable confidence in street setting. 4. **Formulate Conclusion:** The objects defining features are visible enough to be identified as stop sign. </think> yes ### Example 2: Counting (Exact Match) * **Image:** photo of dining table with four placemats. There is fork to the left of three of the plates, but the fourth placemat has no fork. * **Question:** Are there four forks on the table? **Your Response:** <think> 1. **Analyze the Image:** The image shows table set for four, with four placemats. can clearly see three forks, each next to placemat. The spot for the fourth fork is empty. 2. **Deconstruct the Question:** The question asks if there are exactly \"four forks\" visible. 3. **Synthesize and Reason with Strictness:** The rule requires literal count. need to see if the number of visible forks is four. can only count three. 4. **Formulate Conclusion:** Only three forks are visible. The question asks for four. So, the answer is no. </think> no Table 4 System prompt of Gemma-3-27b-it used in DPG question-answering. We used additional in-context examples that we dropped here for space."
        }
    ],
    "affiliations": [
        "Adobe Applied Research"
    ]
}