{
    "paper_title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics",
    "authors": [
        "Lee Chae-Yeon",
        "Oh Hyun-Bin",
        "Han EunGi",
        "Kim Sung-Bin",
        "Suekyeong Nam",
        "Tae-Hyun Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 8 0 3 0 2 . 3 0 5 2 : r Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics Lee Chae-Yeon1 Oh Hyun-Bin2 Han EunGi1 Kim Sung-Bin2 Suekyeong Nam3 Tae-Hyun Oh1,2,4 1Grad. School of AI, POSTECH 2Dept. of Electrical Engineering, POSTECH 3KRAFTON 4School of Computing, KAIST Figure 1. What defines perceptually accurate lip movement for speech signal? In this work, we define three criteria to assess perceptual alignment between speech and lip movements of 3D talking heads: Temporal Synchronization, Lip Readability, and Expressiveness (a). The motivational hypothesis is the existence of desirable representation space that models and complies well with the three criteria between diverse speech characteristics and 3D facial movements, as illustrated in (b); where representations with the same phonemes are clustered, are sensitive to temporal synchronization, and follow certain pattern as the speech intensity increases. Consequently, we build rich speech-mesh synchronized representation space that exhibits the desirable properties."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteriaTemporal Synchronization, Lip Readability, and Expressivenessare crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that desirable representation space exists to meet these three criteria, we introduce speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss *These authors contributed equally. significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/. 1. Introduction Speech-driven 3D talking head generation focuses on generating 3D facial movements synchronized with input speech signals. This plays key role in enhancing communication within multimedia applications, such as virtual reality, entertainment, and education [39]. To provide users with more realistic and immersive experience, it is crucial that the facial and lip movements of 3D avatars are synchronized with the various aspects of speech. This synchronization should be perceptually accurate from human perspective, ensuring that the avatars expressions are both natural and convincing. While recent works in learning-based 3D talking head generation [8, 9, 21, 27, 33, 46] aim to enhance the lip synchronization capabilities, they commonly rely on minimizing the Mean Squared Error (MSE) loss between generated 3D facial motion and ground truth motion as learning objective. This approach is practical, as it directly contributes to minimizing the Lip Vertex Error (LVE), commonly used metric that measures the MSE between the lip vertices of generated 3D facial motions and the ground truth. Despite the improvements in the LVE metric, existing models still struggle to correlate lip movements with some speech characteristics, such as wider mouth openings as speech volume increases. These characteristics are not adequately captured by existing datasets [8, 12], which have limited ranges of facial motion patterns due to the small dataset scale and restricted intensity range. Moreover, relying on MSE and LVE is insufficient for learning or assessing perceptually plausible lip motions [21, 37], as it focuses solely on vertex-wise geometric differences and overlooks the true correspondence between the speech signals and lip movements. lower MSE and LVE do not necessarily correspond to more perceptually accurate lip movement. These observations raise critical questions: What defines perceptually accurate lip movement in response to speech signal, and how can we enhance this accuracy? We draw inspiration from findings on human audio-visual perception: 1) Humans are sensitive to temporal asynchrony between speech and lip movements; slight discrepancies can disrupt the perception of natural synchronization [43]. 2) Humans rely on accurate viseme-phoneme correspondence when assessing lip-sync accuracy, expecting visual lip movements to match the spoken phonemes [3]. 3) There is proportional increase in jaw and lip movements as speech intensity increases, contributing to the expressiveness perceived in natural speech [18, 28, 34, 40]. Through our human study, we reveal an intriguing finding: participants favor lip movements with intensity that corresponds to speecheven if they exceed the established maximum acceptable asynchrony [43] by twiceover those that are perfectly synchronized but lack expressive alignment (Table 1-[Right]). This reveals that humans are more sensitive to expressiveness than temporal synchronization when perceiving, highlighting the importance of expressiveness. Building upon these insights, we define three criteria that significantly impact the perceptual lip synchronization of 3D talking heads: Temporal Synchronization, Lip Readability, and Expressiveness (Fig. 1 (a)). Motivated by our hypothesis that desirable representation space exists to meet these three criteria, we propose speech-mesh synchronized and rich representation that captures the intricate correspondence between speech and 3D face mesh. We design transformer-based architecture that maps time-sequenced speech and mesh inputs to shared representation space. To effectively train this system, we employ two-stage training method: we first develop robust audio-visual speech representation using large-scale 2D video dataset [1], which then serves as an anchor for learning speech-mesh representation. We found that the first step is important and leads to emergent desirable properties (illustrated in Fig. 1 (b)) in the final representation. This sequential approach ensures that the model first establishes space that captures wide range of speech characteristics, and then extends to explore the relationships between speech and 3D face mesh across diverse speech intensities and facial movements. Adopting this representation, we introduce plug-and-play perceptual loss adaptable to any existing 3D talking head generation models [11, 21, 46], enhancing the perceptual quality of 3D talking heads. Furthermore, to assess the three criteria, we introduce three metrics for each aspect. We leverage our learned representation as perceptual metric, Perceptual Lip Readability Score (PLRS), to evaluate the perceptual lip readability of lip movements. Also, we propose two physically grounded lip synchronization metrics: Mean Temporal Misalignment (MTM) for temporal synchronization and Speech-Lip Intensity Correlation Coefficient (SLCC) for expressiveness. Extensive experiments demonstrate that our perceptual loss significantly enhances all three aspects: Temporal Synchronization, Lip Readability, and Expressiveness, which are demonstrated across various metrics: existing metric, our newly proposed metrics, and human evaluations. We also find that incorporating an additional pseudo-dataset [45], which captures diverse ranges of speech and lip movement intensities, can further improve expressiveness. Our main contributions are summarized as follows: Defining three aspectsTemporal Synchronization, Lip Readability, and Expressivenessthat affect the perceptual quality of 3D talking heads and proposing three evaluation metrics for these aspects. Constructing speech-mesh representation space that captures rich and diverse correspondences between speech and lip movements. Proposing plug-in perceptual loss using the constructed speech-mesh representation and demonstrating improvements on existing metric, our newly proposed metrics, and human evaluations. 2. Related Work Speech-driven 3D talking head generation. Speech-driven 3D talking head generation aims to generate realistic 3D facial movements aligned with given speech. Among recent data-driven methods [8, 9, 11, 21, 22, 33, 46, 48], FaceFormer [11] introduces transformer-based autoregressive model and leverages pre-trained speech model to capture long-term audio context and past facial movements. CodeTalker [46] employs VQ-VAE to construct discrete facial motion space, addressing the over-smoothing problem. Diffusion models [17, 31] have also been demonstrated to be effective for 3D talking head generation [33, 36]. In addition to synthesizing neutral facial motions, several works extend the 3D talking head to express specific aspects, such as emotional expressions [9, 22], multilingual capabilities [37], or laughter [38]. Despite these advances, existing methods rely on minimizing MSE loss without clear definition of perceptually accurate lip movement, overlooking the multifaceted nature of lip synchronization. To address this, we define three critical aspects of lip synchronization and propose rich speech-mesh representation, along with its application as perceptual loss in plug-and-play manner, enhancing all three aspects of lip synchronization quality in existing 3D talking head generation models [11, 21, 46]. Speech-face representation learning. Well-aligned representation spaces learned from large-scale datasets such as CLIP [24] and ImageBind [14] are valued for their scalability and versatility. These spaces enable wide range of applications, including auxiliary loss [42], intermediate representation [41], and evaluation metrics [16]. With this context, audio-visual representation spaces trained specifically on speech and 2D face videos have been proposed [6, 15, 29, 35]. For instance, SyncNet [6], CNNbased model learns to detect audio-visual temporal synchronization and has been applied to several tasks, such as active speaker detection [1, 7] and 2D talking head generation [23, 44, 49]. Similarly, transformer-based AVHuBERT [29] has demonstrated remarkable effectiveness in various tasks, including lip reading [30], audio-visual translation [5], and 2D talking head generation [44]. While there has been significant progress in the 2D domain, advancements in the 3D domain remain under-explored. Yang et al. [48] extends the SyncNet architecture to accommodate speech and 3D face meshes; however, its application is limited to evaluating 3D talking heads. In this work, we demonstrate the versatility of our representation space as plug-in module to improve the perceptual accuracy of the existing speech-driven 3D talking head generation models [11, 21, 46], as well as to assess their performance. Evaluation metrics for speech-driven 3D talking head. The prevalent evaluation metric, Lip Vertex Error (LVE) [27], measures the L2 distance between predicted lip vertices and ground truth. Additional metrics, such as Upper Face Dynamics Deviation (FDD) [46] and Lip Readability Percentage (LRP) [21], consider different regions of facial motion. These metrics, however, focus on vertex-wise geometric differences between the ground-truth 3D facial motions and neglect speech-related information. To incorporate both speech and 3D face mesh data for evaluation, MultiTalk [37] introduces an Audio-Visual Lip Readability (AVLR) metric, which assesses perceptual accuracy of lip readability using pre-trained Audio-Visual Speech Recognition model [2]. Yet, AVLR relies on speech and 2D face video rendered from the 3D face mesh, which may not align with the 3D talking head domain. To address these limitations, we introduce novel and comprehensive evaluation metrics that focus on diverse aspects of lip synchronization: Mean Temporal Misalignment (MTM), Perceptual Lip Readability Score (PLRS), and Speech-Lip Intensity Correlation Coefficient (SLCC). 3. Essential Criteria for Perceptually Accurate 3D Talking Head Generating 3D talking head with lip movements that are perceptually accurate to human observers requires clear understanding of the components that influence perceptual quality. Although existing works have focused on improving partial aspects of these talking head generation models, comprehensive exploration for establishing perceptually accurate 3D talking head model has barely been undertaken. Drawing inspirations from extensive research in existing works [10, 18, 21, 28, 34, 40, 43, 44] and our studies, we identify three fundamental criteria essential for achieving perceptually accurate lip movements in 3D talking heads. Temporal Synchronization. This alignment is particularly crucial in media involving human speech, where viewers expect lip movements to precisely match the corresponding speech in time. Temporal misalignment between speech and lip movements indeed distracts viewers, reduces user experiences, and negatively impacts audience perception [25]. Vatakis et al. [43] find that viewers are particularly sensitive to speech-lip asynchrony compared to other audio-visual asynchrony, such as music. They note that mismatches become noticeable when speech precedes lip movements by more than 50 ms or follows them by more than 220 ms. These findings may hold the same in 3D talking head generation, where any misalignment between speech and lip movements can break the illusion of realism, leading to diminished user experience and reducing the perceived authenticity of the virtual character. Thus, we define the temporal synchronization of the talking head as an important aspect of lip synchronization quality. Lip Readability. Visemes (or lip movements) must correspond accurately to the speech phonemes to ensure the spoken words are visually intelligible [3]. This aspect is widely acknowledged as important in existing literature in both 2D [44] and 3D [10, 21] speech-driven talking head models, which leverage lip reading experts as an auxiliary guidance to improve the visual intelligibility of the spoken word. However, the mapping between speech and lip movements is not one-to-one, making lip readability challenging to define. For example, the size and shape of the opening mouth and the dynamic movement patterns of the lips in response to specific utterance differ at every moment [48] or per individual [32]. To capture this complexity, we define the lip readability within speech-mesh synchronized representation space learned from large-scale dataset, capturing the comprehensive and nuanced correspondence between speech and lip movements. Expressiveness. Speech conveys not only linguistic content but also varies in intensity. For instance, speaker may express the same text softly or loudly, with jaw and lip movements proportionally increasing as speech intensity rises, 3 Samples Temp. Exp. Prefer (%) 17.4 82.6 Table 1. Human studies on alignment criteria. [Left] Preference scores (1-3) for 3D talking heads with varying lip movement intensities paired with different speech intensities. [Right] Human preference between (A) samples with precise timing but low expressiveness, and (B) samples with high expressiveness but 100ms asynchronytwice the commonly accepted 50ms threshold [43]. which contributes to perceived expressiveness in real-world face recognition [18, 28, 40]. To demonstrate that the positive correlation of human preference between the intensity of speech and lip movements also exists in 3D talking head field, we conduct human study by presenting 3D talking heads with varied lip movement intensities paired with different speech intensities in Table 1-[Left]. Participants prefer the lip movements with the intensity that match the intensity of speech. Despite this distinct human preference for synchronization between the intensity of corresponding speech and lip movements, this aspect has barely been explored in the talking head generation field. We thus define the expressiveness in 3D talking head as the correlation between speech and lip movement intensity, which is crucial for establishing genuine talking heads and is expected to guide future research aimed at improving perceptual lip synchronization. Focus of this work. Among the three criteria for perceptually accurate 3D talking head, we find that recent 3D talking head generation models achieve reasonable temporal alignment between speech and lip movements (see supplementary for visualization of temporal synchronization). Furthermore, we design an vs. test, prompting participants to choose between two samples: (A) temporally synchronized one while lacking expressive synchronization, and (B) the other with expressive synchronization but with 100ms asynchrony. Table 1-[Right] shows that users prefer sample B, suggesting that humans may prioritize expressive synchronization over strict temporal alignment. This insight directs our focus toward enhancing all three aspects to better capture perceptual realism in 3D talking head. Details of the human study are in the supplementary material. Before introducing metrics to assess the criteria, we present our synchronized representation for designing our perceptual loss. 4. Speech-Mesh Synchronized Representation We hypothesize that desirable representation space exists to meet the three criteria defined in Sec. 3. Motivated by this hypothesis, we develop rich speech-mesh synchronized representation which captures the intricate correspondence between speech and the 3D face mesh. We found that our learned representation exhibits desirable properties, and we adopt it as perceptual loss to improve the perceptual accuracy of existing models with respect to these three criteria. Overview. Directly learning synchronized speech-mesh representation presents challenges due to the scarcity of speech-3D face mesh datasets. One potential solution is to construct pseudo-GT 3D face meshes using reconstruction models [13], although relying solely on pseudo-GT may not suffice for building robust representation. To overcome this, we leverage the extensive knowledge from the speech-2D lip video representations and adapt this to accommodate 3D face meshes. Specifically, we propose two-stage training process: in stage 1, we learn an audio-visual speech representation that accurately reflects lip movements from the unlabeled in-the-wild 2D synchronized talking face video dataset, LRS3 [1]. Subsequently, we leverage the pre-trained speech representation from stage 1, using it as the anchor space to learn synchronized speech-mesh representation. Stage 1. Learning audio-visual speech representation. This stage aims to learn rich speech-2D lip video representation that effectively captures the correlation between varying speech characteristics and lip dynamics. Motivated by prior works [15, 35], we extend the integration of masked autoencoder (MAE) and cross-modal contrastive learning to learn synchronized speech-2D lip video representation using 2D videos. The architecture for stage 1 consists of two modality-specific encoders, cross-modal fusion encoder, and two modality-specific decoders. (see Fig. 2-[Stage 1]). Given speech and 2D lip video pair, (Xs, Xv) D, we begin by patchifying and tokenizing speech spectrograms and video frames into speech and video tokens as = (s1, . . . , sN ) and = (v1, . . . , vM ), where si, vj RH . We then randomly mask out % portion of tokens. The remaining unmasked speech tokens Sunmask and video tokens Vunmask are respectively fed into the speech encoder Es and video encoder Ev, each consisting of Ne transformer layers. Each encoder extracts uni-modal embeddings, Zl v(Vunmask), where (1, . . . , Ne) denote the layer indices. Also, the mean pooled speech and video embeddings, cl s) and cl = MeanPool(Zl v), are derived from the corresponding uni-modal embeddings. Following the uni-modal encoders, we introduce multi-modal fusion encoder Esv to exploit complementary information from each extracted uni-modal embedding. The speech and video embeddings are jointly processed through this encoder, resulting in multi-modal fusion embeddings Fs and Fv, i.e., [Fs, Fv] = Esv([Zs, Zv]), where [, ] denotes concatenation. With the fusion embeddings Fs and Fv, we utilize each modalitys decoder to reconstruct the original signals. Specifically, we employ s(Sunmask) and Zl = MeanPool(Zl = El = El Figure 2. Pipeline of speech-mesh synchronized representation learning. We train our speech-mesh representation space in two-stage manner. In the first stage, we learn rich audio-visual representation in 2D domain to capture the synchronization between lip movement and speech. In the second stage, we train the 3D mesh encoder to align the 3D mesh space with the frozen speech space. As an application of our speech-mesh representation space, we propose plug-in perceptual loss to 3D talking head models to enhance the quality of lip movements. speech decoder Ds and video decoder Dv, each consisting of Nd transformer layers. We pad each multi-modal fusion embedding with trainable masked tokens at their original positions, resulting in v. Each decoder then reconstructs the respective signals; the reconstructed speech spectrogram and video tokens as ˆSmask=Ds(F s) and ˆVmask=Dv(F and v). Training the model involves two objectives: learning the cross-modal alignment between the speech-2D lip video signals, while reconstructing their original signals. Given speech-2D lip video token pairs in batch, {Si, Vi}B i=1, we treat the true speech-2D lip video pair as positive samples, while the others in the batch are considered negative samples. To encourage alignment between temporally synchronized speech-2D lip video instances, we utilize cross-modal contrastive learning strategy, InfoNCE [20]. We maximize the cosine similarity between positive sample of the mean pooled speech embedding cl )) and the corresponding synchronized mean pooled video embedding cl )). We first define the speech-centric loss as: v,i = MeanPool(El s,i = MeanPool(El v(Vunmask s(Sunmask LSV = 1 B (cid:88) i=1 log exp(cl s,icl j=1 exp(cl v,i/τ ) s,icl v,j /τ ) (cid:80)B , (1) where τ is temperature hyperparameter. Also, we make the objective symmetric by defining video-centric loss as LVS. We sum LSV and LVS across selected encoder layers to obtain our final contrastive learning objective: (cid:88) LInfoNCE = LSV + LVS. (2) lL For reconstruction, the model is trained in self-supervised manner by minimizing the reconstruction loss LMAE as: LMAE = 1 (cid:80)B i= (cid:104)(cid:80) ˆSmask Smask Smask 2 + (cid:80) ˆVmask Vmask Vmask 2 (cid:105) , (3) and Vmask where Smask denote the number of masked speech and video tokens, respectively. Overall, our objective function is defined as: = LMAE + λLInfoNCE, (4) where λ is the weight factor for cross-modal contrastive loss. We observe that the representation space trained with the transformer architecture and rich and large-scale 2D face video dataset in this way already possesses the desired properties we pursue, illustrated in Fig. 1 (see supplementary for visualizations of pre-trained speech representation). Motivated by this, we transfer these emergent properties to the speech-mesh representation space as follows. Stage 2. Learning speech-mesh representation. In this stage, we design 3D mesh encoder that maps 3D face mesh to the speech representation pre-trained in stage 1. This pretrained speech representation, derived from rich speech-2D face video data, serves as robust anchor space for learning the correlation between diverse speech characteristics and 3D facial motions. We use contrastive learning loss to align 3D facial motion embeddings with the anchored speech representations, as shown in Fig. 2-[Stage 2]. = MeanPool(El Given speech and 3D face mesh pair (Xs, Xm) D, we patchify and tokenize speech spectrograms into speech tokens S, and map these into pre-trained uni-modal mean pooled speech embeddings cl s(S)). Similarly, we patchify 3D face mesh into mesh tokens M, and feed them into the 3D mesh encoder Em consisting of Ne transformer layers to extract mesh embeddings = El Zl m(M) and the corresponding mean-pooled mesh embedding cl m. For the learning objective, given speech3D face mesh token pairs in batch, {Si, Mi}B i=1, we first define speech-anchored loss as: LSM = 1 (cid:88) i=1 log s,icl exp(cl j=1 exp(cl m,i/τ ) s,icl m,j /τ ) (cid:80)B , (5) as well as LMS which is the mesh-anchored loss. Similar to Eq. (2), we sum LSM and LMS across selected encoder layers to train the 3D mesh encoder Em while the speech encoder fixed. As result, this two-stage training approach yields robust speech-mesh representation, which significantly enhances the alignment between the speech and 3D face mesh modalities. Adopting it as perceptual loss. key application of the speech-mesh representation learned through the two-stage training process is its use as perceptual loss to enhance the perceptual accuracy of the 3D talking head model (see Fig. 2). Leveraging this representation as perceptual loss ensures that the generated lip movements are perceptually accurate and aligned well with the speech. Given speech and generated 3D face mesh pairs in i=1, we define our perceptual loss with the batch, {Si, ˆMi}B symmetric InfoNCE loss (Eq. (2)) as Lpercp = (cid:88) lL LSM + LMS. (6) Our perceptual loss encourages synchronized speech and mesh embeddings to pull closer together, while unsynchronized ones to push apart. The effectiveness and analysis of our perceptual loss will be discussed in later sections. 5. Evaluation Metrics In this section, we describe our proposed evaluation metrics that assess each criterion impacting the quality of 3D lip accuracy, as discussed in Sec. 3. Here, we outline the highlevel concepts of our proposed metrics. Additional details and experiments are provided in the supplementary material. Mean Temporal Misalignment (MTM). To measure the temporal discrepancy between speech and corresponding lip movements, temporal correspondence annotations, such as onset times for each modality, would typically be required. As proxy, we determine temporal correspondence and measure temporal discrepancies between the ground truth and predicted lip vertex displacement sequences by using Derivative Dynamic Time Warping (DDTW) [19], which robustly identifies local structural similarities compared to standard Dynamic Time Warping (DTW) [4]. For simplicity, we focus on the central vertices of the upper and lower lips when extracting displacement sequences, and use local extrema in the DDTW process to measure temporal misalignment by pinpointing precise time steps for mouth opening and closing events. Consequently, Mean Temporal Misalignment (MTM) is defined as = 1 k=1 tk, where is the total number of video clips and tk is the averaged temporal misalignment of the k-th video clip. Perceptual Lip Readability Score (PLRS). While Lip Vertex Error (LVE) measures the accuracy of generated lip articulations against the ground truth, it does not fully assess (cid:80)K whether lip movements are perceptually aligned with the given speech. To address this, we leverage our speech-mesh representation in Sec. 4 as perceptual lip readability evaluator. We compute the perceptual alignment using the cosine similarity of the mean pooled speech and mesh embeddings. Since this representation has learned rich distribution of speech correspondences across various facial movements, our metric correlates highly with human perception, measuring perceptual lip movement alignment more accurately than LVE (see supplementary for the human study on metrics). Speech and Lip Intensity Correlation Coefficient (SLCC). As shown in Table 1-[Left], humans prefer aligned intensity between speech and lip movement. Thus, the intensity of generated lip movements should positively correlate with the corresponding input speech. To quantify this, we define the Speech and Lip Correlation Coefficient rSL as: rSL = (cid:80)K k=1(SIk SI)(LIk LI) (cid:80)K k=1(SIk SI)2 (cid:80)K k=1(LIk LI)2 , (7) (cid:80)K k=1 SIk and LI= 1 where SIk and LIk denote Speech (SI) and Lip Intensity (LI), respectively, SI= 1 k=1 LIk. We define SI using speech loudness, specifically the znormalized Root Mean Square (RMS) value, which is widely accepted measure of speech intensity in signal processing. To define LI, we measure the averaged lip displacement value of video clip, followed by the z-normalization. (cid:80)K 6. Experiments We first outline the evaluation setup, and then present thorough analyses of the experimental results. Due to the space limitation, we present more implementation details and additional experiments in the supplementary material. 6.1. Experimental Settings Datasets. Most of the existing speech-driven 3D talking head generation methods rely on VOCASET [8] and BIWI [12] to train and test the models. However, these datasets have limited ranges of facial motion patterns due to the small dataset scale and restricted intensity range, which restricts their ability to fully capture the intricate relationship between speech and 3D face mesh. To address the lack of training dataset, we construct two large-scale speech-3d face mesh benchmark datasets, LRS3-3D and MEAD-3D, by processing LRS3 and MEAD videos using two monocular face reconstruction methods: SPECTRE [13] for LRS3 [1], which ensures accurate lip movements, and SMIRK [26] for MEAD [45], which captures diverse speech and lip movement intensities. We use LRS3 in the first stage and LRS33D in the second stage to train our speech-mesh synchronized representation. Then, for base model training and evaluations, we adopt two configurations: (1) training and testing with VOCASET, in line with existing work, and (2) 6 Method Temporal Synchronization Lip Readability Expressiveness Method Perceptual Loss MTM () LVE () PLRS () SLCC / () VOCASET FaceFormer [11] + Ours rep. CodeTalker [46] + Ours rep. SelfTalk [21] + Ours rep. - 53.6 52.2 61.8 60.9 50.1 49.2 - 3.357 3.091 3.700 3. 2.971 2.924 0.409 0.368 0.463 0.381 0.388 0.414 0.418 0.34 / - 0.26 / 0.08 0.37 / 0.03 0.38 / 0.04 0.35 / 0.01 0.41 / 0.07 0.35 / 0.01 Table 2. Quantitative results of lip synchronization on VOCASET [8] test set. We evaluate the base models on our proposed lip synchronization metrics. We denote as the difference in SLCC between the model and those measured on the data distribution. lower indicates the model more closely represents the intensity correlation of the dataset. We demonstrate the effectiveness of our representation in consistently enhancing all three aspects of lip synchronization. Figure 3. Qualitative results of the effectiveness of our perceptual loss for lip readability. Our perceptual loss guides baselines [11, 21, 46] to generate perceptually accurate lip movements. combining MEAD-3D with VOCASET during training to endow expressiveness and testing on MEAD-3D. Base methods. We use three state-of-the-art 3D talking head generation models [11, 21, 46] to evaluate the effectiveness of our perceptual loss. Metrics. To comprehensively evaluate the three aspects of lip synchronization, we assess MTM, PLRS, and SLCC, corresponding to temporal synchronization, lip readability, and expressiveness, respectively. Additionally, we compute the level-wise SLCC for the MEAD-3D test set, which includes three distinct emotional intensity levels, to evaluate the expressive capability of 3D talking head generation models. We also measure LVE as part of the lip readability evaluation. 6.2. Experimental Results and Analysis We conduct evaluations to assess the effectiveness of our speech-mesh synchronized representation and the incorporation of an expressive speech-3D face mesh paired dataset (i.e., MEAD-3D) in enhancing the three criteria. How well do existing 3D talking head models achieve lip synchronization in all three aspects? The results are summarized in Table 2. For temporal synchronization, most base models achieve MTM values between 50 and 60ms, indicating performance close to the acceptable asynchrony Temporal Synchronization Lip Readability Expressiveness MTM () LVE () PLRS () SLCC / () - 53.6 55.6 55.3 52.2 61.8 59.6 55.9 60. - 3.357 3.316 3.278 3.091 3.700 4.319 3.579 3.579 0.409 0.368 0.435 0.400 0.463 0.381 0.379 0.374 0. 0.34 / - 0.26 / 0.08 0.38 / 0.04 0.42 / 0.08 0.37 / 0.03 0.38 / 0.04 0.14 / 0.20 0.23 / 0.11 0.35 / 0.01 VOCASET FaceFormer [11] CodeTalker [46] - 3D SyncNet Ours w/o 2D prior Ours w/ 2D prior 3D SyncNet Ours w/o 2D prior Ours w/ 2D prior 3D SyncNet Ours w/o 2D prior Ours w/ 2D prior SelfTalk [21] 50.1 49.5 54.4 49.2 Table 3. Ablation study on architectural choice and 2D prior knowledge. We validate the effectiveness of the transformer-based architecture and curriculum learning with pre-trained 2D speech representation by ablating them from our proposed representation. 0.41 / 0.07 0.35 / 0.01 0.39 / 0.05 0.35 / 0.01 0.414 0.405 0.417 0.418 2.971 2.941 3.149 2. Figure 4. t-SNE plot of ablation study. We plot the t-SNE graph for each perceptual critic model. We represent the features with same phoneme as same color. Squared and circled points denote mesh and speech features from each representation, respectively. Figure 5. Behaviors of our representation in temporal and expressiveness sensitivity. We demonstrate the effectiveness of our representation in temporal synchronization and expressiveness using cosine similarity graph and speech feature plots, respectively. We color the point as low, medium, and high intensity. threshold. Regarding lip readability, SelfTalk [21] achieves the best performance, while FaceFormer [11] has the lowest PLRS score. Furthermore, CodeTalker [46] demonstrates the closest SLCC values to the ground truth VOCASET mesh, while FaceFormer exhibits the highest SLCC discrepancy. Does our speech-mesh representation improve lip synchronization? Yes. Table 2 and Fig. 3 show consistent improvements in the three aspects with our perceptual loss. What makes our speech-mesh representation have lip synchronization ability? We hypothesize that the transformerbased architecture and curriculum learning with pre-trained 7 Figure 6. Qualitative results for the expressiveness. Given high and low intensity levels of speech, models trained on both MEAD-3D and VOCASET show more expressive lip movements compared to those trained on VOCASET alone, and even better with our perceptual loss. audio-visual speech representation contribute significantly to improved lip synchronization. To validate this, we conduct ablation studies, summarized in Table 3, examining the roles of pre-trained speech representation and architectural design. Without the pre-trained speech representation, base models show lower performance across metrics, and CNN-based architectures, 3D SyncNet, do not clearly show effectiveness, while ours with 2D prior consistently show improvement. The t-SNE plots of perceptual critic models in Fig. 4 show that, in (a) 3D SyncNet, speech and mesh features corresponding to the same phoneme group are separated. In (b) ours without 2D priors, speech and mesh features lack separation and appear scattered without clustering. Our representation (c), notably, tends to form more distinct clusters according to phonemes, with vowels and consonants grouped closely, potentially contributing to enhancing lip readability. We also observe directional progression in the feature space, shifting from phonemes with mouth opening (e.g., /aj/) to those with mouth closing (e.g., /f/). We also examine our representations performance in other aspects, temporal synchronization and expressiveness. Figure 5-(a) demonstrates temporal sensitivity, as cosine similarity drops when temporal misalignment is introduced between input speech and 3D face mesh. In Fig. 5-(b), we plot speech features at varying speech intensities, showing directional trend as intensity increases from lowest to highest. Figures 4 and 5 imply that our representation holds favorable properties discussed in Fig. 1 for the three criteria. Can we unlock the expressive power of 3D talking heads? Likely. Since VOCASET [8] lacks the range of diverse speech and lip movement intensities, evaluating expressive power requires testing on dataset with broader range of intensities. To study this, we examine the expressiveness of the base models on the MEAD-3D dataset, which includes three intensity levels. We assess SLCC at each intensity level to evaluate expressiveness and identify any expressiveness bounds as the level of intensity increases. We first test the base models trained on VOCASET [8] against the MEAD-3D test set. Table 4 shows these models demonstrate limited expressiveness, with SLCC values showing minimal increase across intensity levels. We hypothesize that Method Temporal Synchronization Lip Readability MTM () LVE () PLRS () MEAD-3D FaceFormer [11] + MEAD-3D + Ours rep. CodeTalker [46] + MEAD-3D + Ours rep. - 59.6 59.5 55. 60.7 60.9 58.6 - 3.207 1.139 1.114 3.236 2.954 2.705 0.230 0.299 0.176 0. 0.294 0.154 0.221 Expressiveness SLCC / () Lv1 Lv2 Lv Avg 0.24 / - 0.30 / - 0.39 / - 0.42 / - 0.08 / 0.16 0.07 / 0.23 0.07 / 0.32 0.06 / 0.36 0.26 / 0.02 0.30 / 0.00 0.34 / 0.05 0.35 / 0.07 0.27 / 0.03 0.27 / 0.03 0.32 / 0.07 0.33 / 0. 0.02 / 0.22 0.03 / 0.27 0.03 / 0.36 0.02 / 0.40 0.09 / 0.15 0.12 / 0.18 0.06 / 0.33 0.11 / 0.31 0.18 / 0.06 0.29 / 0.01 0.31 / 0.08 0.31 / 0.11 SelfTalk [21] 53.4 54.2 52.7 3.396 1.238 1.192 + MEAD-3D + Ours rep. 0.14 / 0.10 0.14 / 0.16 0.17 / 0.22 0.15 / 0.27 0.16 / 0.08 0.28 / 0.02 0.32 / 0.07 0.31 / 0.11 0.17 / 0.07 0.29 / 0.01 0.34 / 0.05 0.33 / 0. 0.294 0.216 0.230 Table 4. Quantitative results of lip synchronization on MEAD3D test set. We evaluate the base models on the MEAD-3D test set. We also compute the level-wise SLCC to evaluate the expressive capability of the models. this limitation arises, because VOCASETs smaller scale and intensity range restrict the models ability to learn relationships between speech and lip intensity. To address this, we integrate MEAD-3D with VOCASET for training, aiming to boost expressiveness in the base models. This approach consistently improves SLCC across all intensity levels. However, simply adding MEAD-3D degrades lip synchronization metrics, except for expressiveness, i.e., nontrivial. To counterbalance this, we leverage our perceptual loss, which effectively mitigates the degradation introduced by MEAD-3D while improving expressiveness (see Fig. 6). 7. Conclusion This paper addresses challenges in existing 3D talking head generation models, which often overlook the true correspondence between speech and lip movements, making it difficult to accurately link lip movements with varying speech characteristics. To overcome this issue, we identify three essential aspectsTemporal Synchronization, Lip Readability, and Expressivenessthat influence the perceptual quality of lip movements, and develop specific metrics for each aspect. We introduce speech-mesh synchronized representation that exhibits these emergent properties and adopt it as perceptual loss. Our extensive analyses demonstrate that our perceptual loss consistently enhances models across three aspects. We believe that our defined aspects will guide future research in generating more realistic 3D talking heads, and our representation will serve as key component. 8 Acknowledgments. This research was supported by grant from KRAFTON AI, and was also partially supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2021-II212068, Artificial Intelligence Innovation Hub; No.RS-2023-00225630, Development of Artificial Intelligence for Text-based 3D Movie Generation; No. RS-2024-00457882, National AI Research Lab Project) and Culture, Sports and Tourism R&D Program through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sports and Tourism in 2024 (Project Name: Development of barrier-free experiential XR contents technology to improve accessibility to online activities for the physically disabled, Project Number: RS-2024-00396700, Contribution Rate: 25%)."
        },
        {
            "title": "References",
            "content": "[1] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496, 2018. 2, 3, 4, 6, 11, 13 [2] Mohamed Anwar, Bowen Shi, Vedanuj Goswami, Wei-Ning Hsu, Juan Pino, and Changhan Wang. Muavic: multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation. In INTERSPEECH 2023, 2023. 3 [3] Helen Bear and Richard Harvey. Phoneme-to-viseme mappings: the good, the bad, and the ugly. Speech Communication, 2017. 2, 3 [4] Donald Berndt and James Clifford. Using dynamic time warping to find patterns in time series. In Proceedings of the 3rd international conference on knowledge discovery and data mining, 1994. 6 [5] Jeongsoo Choi, Se Jin Park, Minsu Kim, and Yong Man Ro. Av2av: Direct audio-visual speech to audio-visual speech translation with unified audio-visual speech representation. In CVPR, 2024. 3 [6] J. S. Chung and A. Zisserman. Out of time: automated lip sync in the wild. In Workshop on Multi-view Lip-reading, ACCV, 2016. 3, 15, 17 [7] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. arXiv preprint Voxceleb2: Deep speaker recognition. arXiv:1806.05622, 2018. 3 [8] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael Black. Capture, learning, and synthesis of 3d speaking styles. In CVPR, 2019. 1, 2, 6, 7, 8, 12, 13, 14, 15 [9] Radek Danˇeˇcek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael Black, and Timo Bolkart. Emotional speechdriven animation with content-emotion disentanglement. In SIGGRAPH Asia 2023 Conference Papers, pages 113, 2023. 1, 2 [10] Han EunGi, Oh Hyun-Bin, Kim Sung-Bin, Corentin Nivelet Etcheberry, Suekyeong Nam, Janghoon Ju, and Tae-Hyun Oh. Enhancing speech-driven 3d facial animation with audiovisual guidance from lip reading expert. In Interspeech 2024, pages 29402944, 2024. 3 [11] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. Faceformer: Speech-driven 3d facial animation with transformers. In CVPR, 2022. 2, 3, 7, 8, 13, 15, 16 [12] Gabriele Fanelli, Juergen Gall, Harald Romsdorfer, Thibaut Weise, and Luc Van Gool. 3-d audio-visual corpus of affective communication. IEEE TMM, 2010. 2, 6, 13 [13] Panagiotis P. Filntisis, George Retsinas, Foivos ParaperasPapantoniou, Athanasios Katsamanis, Anastasios Roussos, and Petros Maragos. Spectre: Visual speech-informed perceptual 3d facial expression reconstruction from videos. In CVPRW, pages 57455755, 2023. 4, 6, 13, 18 [14] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. [15] Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James R. Glass. Contrastive audio-visual masked autoencoder. In The Eleventh International Conference on Learning Representations, 2023. 3, 4 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. 3 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 2 [18] Jessica Huber and Bharath Chandrasekaran. Effects of increasing sound pressure level on lip and jaw movement parameters and consistency in young adults. 2006. 2, 3, 4 [19] Eamonn J. Keogh and M. Pazzani. Derivative dynamic time warping. In In First SIAM International Conference on Data Mining, 2001. 6, 13 [20] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [21] Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, and Zhaoxin Fan. Selftalk: selfsupervised commutative training diagram to comprehend 3d talking faces. In ACM MM, 2023. 1, 2, 3, 7, 8, 15, 16 [22] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Emotalk: Speech-driven emotional disentanglement for 3d face animation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2 [23] Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. lip sync expert is all you need for speech In Proceedings of the 28th to lip generation in the wild. ACM International Conference on Multimedia, page 484492. Association for Computing Machinery, 2020. 3 [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. on Mach. Learn., pages 87488763. PMLR, 2021. 3 [25] Byron Reeves and David Voelker. ffects of audio-video asynchrony on viewers memory, evaluation of content and detec9 [40] Stephen Tasko and Michael McClean. Variations in articulatory movement with changes in speech task. 2004. 2, 3, [41] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR, 2023. 3 [42] Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zerocap: Zero-shot image-to-text generation for visual-semantic In Proceedings of the IEEE/CVF Conference arithmetic. on Computer Vision and Pattern Recognition, pages 17918 17928, 2022. 3 [43] Argiro Vatakis and Charles Spence. Audiovisual synchrony perception for speech and music assessed using temporal order judgment task. Neuroscience letters, 393(1):4044, 2006. 2, 3, 4, 13 [44] Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby Tan, and Haizhou Li. Seeing what you said: Talking face generation guided by lip reading expert. In CVPR, pages 1465314662, 2023. 3 [45] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: large-scale audio-visual dataset for emotional talking-face generation. In ECCV, 2020. 2, 6, 13 [46] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven In CVPR, 3d facial animation with discrete motion prior. 2023. 1, 2, 3, 7, 8, 15, [47] Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Bärmann, Seymanur Akti, Hazım Kemal Ekenel, and Alexander Waibel. Audio-visual speech representation expert for enhanced talking face video generation and evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60036013, 2024. 17 [48] Karren Yang, Anurag Ranjan, Rick Chang, Raviteja Vemulapalli, and Oncel Tuzel. Probabilistic speech-driven 3d facial motion synthesis: New benchmarks, methods, and applications. In CVPR, 2024. 2, 3 [49] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In CVPR, pages 86528661, 2023. 3 tion ability. In Research Report, Standford University, 1993. 3 [26] George Retsinas, Panagiotis Filntisis, Radek Danecek, Victoria Abrevaya, Anastasios Roussos, Timo Bolkart, and Petros Maragos. 3d facial expressions through analysis-byneural-synthesis. In CVPR, 2024. 6, 13, 18 [27] Alexander Richard, Michael Zollhöfer, Yandong Wen, Fernando De la Torre, and Yaser Sheikh. Meshtalk: 3d face animation from speech using cross-modality disentanglement. In ICCV, 2021. 1, [28] Richard Schulman. Articulatory dynamics of loud and normal speech. The Journal of the Acoustical Society of America, 85 (1):295312, 1989. 2, 3, 4 [29] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed. Learning audio-visual speech representation by masked multimodal cluster prediction. In ICLR, 2022. 3, 17 [30] Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed. Robust self-supervised audio-visual speech recognition. arXiv preprint arXiv:2201.01763, 2022. 3 [31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2 [32] Wenfeng Song, Xuan Wang, Shi Zheng, Shuai Li, Aimin Hao, and Xia Hou. Talkingstyle: Personalized speech-driven 3d facial animation with style preservation. IEEE Transactions on Visualization and Computer Graphics, 2024. [33] Stefan Stan, Kazi Injamamul Haque, and Zerrin Yumak. Facediffuser: Speech-driven 3d facial animation synthesis using diffusion. In Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games, pages 111, 2023. 1, 2 [34] Trinity Suma, Birate Sonia, Kwame Agyemang Baffour, and Oyewole Oyekoya. The effects of avatar voice and facial expression intensity on emotional recognition and user perception. In SIGGRAPH Asia 2023 Technical Communications, pages 14. 2023. 2, 3 [35] Licai Sun, Zheng Lian, Bin Liu, and Jianhua Tao. Hicmae: Hierarchical contrastive masked autoencoder for self-supervised audio-visual emotion recognition. Information Fusion, 108: 102382, 2024. 3, 4 [36] Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, and Yong-Jin Liu. Diffposetalk: Speech-driven stylistic 3d facial animation and head pose generation via diffusion models. ACM TOG, 43(4), 2024. 2 [37] Kim Sung-Bin, Lee Chae-Yeon, Gihun Son, Oh Hyun-Bin, Janghoon Ju, Suekyeong Nam, and Tae-Hyun Oh. Multitalk: Enhancing 3d talking head generation across languages with multilingual video dataset. In Interspeech 2024, pages 1380 1384, 2024. 2, 3 [38] Kim Sung-Bin, Lee Hyun, Da Hye Hong, Suekyeong Nam, Janghoon Ju, and Tae-Hyun Oh. Laughtalk: Expressive 3d talking head generation with laughter. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 64046413, 2024. 2 [39] Hiroki Tanaka, Satoshi Nakamura, et al. The acceptability of virtual characters as social skills trainers: usability study. JMIR human factors, 9(1):e35358, 2022. 10 Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics"
        },
        {
            "title": "Contents",
            "content": "A Supplementary Video Emergent Properties of 2D Speech Representation Speech-Mesh Synchronized Representation C.1 Network architecture C.2 Training pipeline C.3 Dataset statistics Details of Human Study on Lip Synchronization Criteria Evaluation Metrics E.1 Definition and implementation details E.2 Human study on perceptual metric Implementation Details of Ablation Study Additional Results G.1 Human study on applying perceptual loss G.2 FDD evaluation on applying perceptual loss G.3 Qualitative result of temporal synchronization G.4 Stability comparison on loss and cosine similarity Discussion A. Supplementary Video This work focuses on 3D facial motions, which are best viewed in video format. Please refer to the attached supplementary video. The video contains qualitative results of lip synchronization on the VOCASET and MEAD-3D test sets, demonstrating the effectiveness of our method in enhancing lip synchronization in aspects of lip readability and expressiveness. B. Emergent Properties of 2D Speech Representation In this section, we conduct further analyses of 2D speech representation (i.e., 2D prior knowledge), which motivate the transfer of the emergent properties of 2D speech representation to the speech-mesh representation space using curriculum learning approach. We observe that the 2D audio-visual speech representation, trained with transformer architecture and an extensive video dataset [1], inherently exhibits the desirable properties for lip synchronization that we aim to achieve. We visualize cosine similarity versus temporal offset graph and t-SNE visualization of the 2D audio-visual speech representation in Fig. S1. The speech representation exhibits the properties regarding the critical aspects of lip synchronization: 11 Figure S1. Emergent properties of 2D speech representation. We visualize cosine similarity versus temporal offset graph and t-SNE visualization of the 2D audio-visual speech representation. The 2D speech representation already possesses desired properties we pursue, which motivates us to transfer the emergent properties to the speech-mesh representation space. (1) Temporal sensitivity in Fig. S1-(a), (2) clear separation and clustering of speech features corresponding to the same phoneme group in Fig. S1-(b), and (3) directional progression of speech features as intensity increases from the lowest to the highest levels in Fig. 5-(b) of the main paper *. This motivates us to transfer these emergent properties to the 3D speech-mesh representation through the curriculum learning approach, as mentioned in Sec. 4 of the main paper. Furthermore, as shown in Figs. 4 and 5 of the main paper, we demonstrate that these properties are successfully transferred to the speech-mesh representation. C. Speech-Mesh Synchronized Representation We provide more details on the network architecture of audiovisual speech representation and speech-mesh representation (Sec. C.1). In addition, we provide the training details of the two-stage training process (Sec. C.2) and dataset statistics of speech-mesh benchmark datasets (Sec. C.3). C.1. Network architecture To improve the reproducibility of our speech-mesh representation, we further illustrate the detailed network architectures for the audio-visual speech representation and the speechmesh representation, which are shown in Table S1. *We freeze the pre-trained speech encoder from stage 1 and utilize it as the speech encoder in stage 2, which ensures that the speech representation in both stages shares the same favorable property of expressiveness. Stage Module Input Output Layer Operation Speech Tokenizer Xs(Cs, Hs, Ws) S(N, H) Conv2D((1, 16), (1, 16), H) Speech Encoder Sunmask(N unmask, H) Zs(N unmask, H) [MHSA(H, 8) FFN(H)] 10 LN Speech Decoder ˆS(N mask, Cs Hs Ws) Concat(Linear(H, 384)+ PE(N unmask), PE(N mask)) MHSA(384, 8) FFN(384 4) [MHSA(384, 8) MHCA(Zs, 384, 6) FFN(384 4)] 3 LN Linear(Cs Hs Ws) Slice[N unmask :] Video Tokenizer Xv(Cv, T, Hv, Wv) V(M, H) Conv3D(cid:0)(1, 16, 16), (1, 16, 16), H(cid:1) Video Encoder Vunmask(M unmask, H) Zv(M unmask, H) [MHSA(H, 8) FFN(H)] 10 LN Video Decoder ˆV(M mask, Cv Hv Wv) Fusion Encoder Zs, Zv Fs(N unmask, H) Zs, Zv Fv(M unmask, H) Concat(Linear(H, 384)+ PE(M unmask), PE(M mask)) MHSA(H, 8) FFN(H) [MHSA(H, 8) MHCA(Zv, H, 6) FFN(H)] 3 LN Linear(Cv Hv Wv) Slice[M unmask :] [MHSA(H, 8) MHCA(Zv, H, 8) FFN(H 4)] 2 [MHSA(H, 8) MHCA(Zs, H, 8) FFN(H 4)] 2 Mesh Tokenizer Xm(T, 3) M(T, H) Linear(H) Mesh Encoder Zm(T, H) [MHSA(H, 8) FFN(H)] 10 LN 2 Table S1. Architecture details. The parameters of network architectures. Conv2D(k, s, n) denotes 2D Convolutional layer with kernel size k, stride size s, and output channel of n. MHSA(d, nhead) denotes multi-head self-attention layer with the input channels and the number of heads in multi-head attention nhead. MHCA(ca, d, nhead) denotes multi-head cross-attention layer with additional cross-attention input ca. PE(a) is position embedding layer where denotes the length of the position vector. FFN(d) is feed-forward layer. Linear(n) denotes linear layer with output channels of n. LN denotes layer normalization and Slice[s :] denotes slice operation. C.2. Training pipeline Two-stage training process. In our experiment, we set = 5, = 512, and = 30. For training the audio-visual speech representation, we use Cs = 1, Hs = 64, Ws = 128, = 512 for speech modality and Cs = 3, Hv = 160, Wv = 160, = 500 for video modality. We train the audio-visual speech representation using LRS on two NVIDIA A6000 for 100 epochs with the AdamW optimizer (β1 = 0.9, β2 = 0.95 and ϵ = 1e-8), where the learning rate is initialized as 3e-4, and the mini-batch size is set as 40. For training the speech-mesh representation, we use the number of vertices = 5023. We train the speech-mesh representation using LRS-3D with the mini-batch size of 80, and other hyper-parameters remain unchanged as Stage 1. Perceptual loss. We employ our speech-mesh representation as perceptual loss to enhance the perceptual accuracy of the 3D talking head model. We finetune our speech-mesh representation using the VOCASET [8] train split on an NVIDIA A6000 for 5 epochs with the initial learning rate 1e4 and other hyper-parameters remain unchanged as Stage 2. To train the 3D talking head models with our perceptual loss, we split the generated mesh from the model into 5 frames using sliding window size of 1. We make batch of size 80 and get uni-modal embeddings from our representation. We Dataset # Vertex clips # Speaker IDs Total hours FPS VOCASET BIWI LRS3-3D MEAD-3D 475 1109 17752 8765 12 14 788 15 0.5 1.4 61.1 10. 30 25 25 30 Table S2. Statistics of speech-mesh paired benchmark. We use VOCASET, LRS3-3D and MEAD-3D speech-mesh paired datasets in our experiments. We construct two large-scale speech-mesh benchmark datasets, LRS3-3D and MEAD-3D, using monocular face reconstruction methods. Figure S2. Speech and lip intensity distributions across datasets. We present speech and lip intensity distributions and corresponding standard deviation values across datasets. additionally apply the InfoNCE loss with weight of 1e-7 to the original training loss of the model. 12 C.3. Dataset statistics We construct LRS3-3D and MEAD-3D by processing LRS3 [1] and MEAD [45] videos using two monocular face reconstruction methods, respectively: SPECTRE [13] for LRS3, which ensures accurate lip movements, and SMIRK [26] for MEAD, which captures diverse speech and lip movement intensities. We construct test split for LRS3D, involving 934 clips. We split MEAD-3D to construct test split, which includes 3470 clips. Table S2 and Fig. S2 show the statistics of the existing (VOCASET [8], BIWI [12]) and the newly proposed large-scale speech-mesh benchmark datasets (LRS3-3D and MEAD-3D). As shown in Table S2, LRS3-3D and MEAD3D have notably larger data sizes than VOCASET and BIWI. Fig. S2 presents the broader speech and lip intensity* distributions of LRS3-3D and MEAD-3D with higher standard deviations (σ), indicating greater variability in facial motions. In contrast, VOCASET and BIWI show limitations in both scale and diversity. D. Details for Human Study on Lip Synchronization Criteria Human preference between the speech and lip intensities. We conduct preliminary experiment to demonstrate the positive correlation of human preference between the intensity of speech and lip movements in the 3D talking face field. Using the intensity annotations from the MEAD dataset [45], we first split the MEAD-3D dataset into three categories: Level 1, Level 2, and Level 3, representing different intensity levels. Then, we train 3D talking face model [11] using VOCASET [8] (to ensure the quality of generation) and each intensity split separately. This results in three distinct models, each of which tends to generate lip movements biased toward the intensity level present in its training data, regardless of the speech intensity provided as input. We input three speeches with intensity levels ranging from Level 1 to Level 3 into each of the three biased models, producing nine intensity configurations in the generated mesh sequences as shown in Tab.1-[Left] of the main paper. We then asked 17 participants, balanced group of males and females from non-expert background in the field, to rank their preferences in three videos, assigning score from 1 (least preferred) to 3 (most preferred). Each video has the same speech (identical in utterance and intensity) but differs in the intensity of the lip movements. Human preference on Temporal sync. vs. Expressiveness. We design simple A/B test to investigate an interesting aspect of human perception for lip synchronization. We use the two biased models from the previous human study: one *Lip intensity was normalized by eye distance to account for differences between FLAME and BIWI topologies. 13 trained to generate Level 1 lip movements and the other trained to generate Level 3 lip movements, regardless of the speech intensity. For each model, we create two types of samples. Sample is temporally synchronized but lacks expressive synchronization (e.g., speech of Level 3 intensity and lip movements of Level 1 intensity). In contrast, sample has expressive synchronization (e.g., speech of Level 3 intensity and lip movements of Level 3 intensity) but is temporally misaligned. To introduce the temporal mismatch in Sample B, we make the speech lead the lip movements by 100ms, which exceeds twice the established maximum acceptable synchrony [43]. We then asked 28 participants, comprising balanced group of males and females from non-expert background in the field, to choose which sample they prefer based on how well the lip movements correspond to the speech in sample vs. B. E. Evaluation Metrics We present the comprehensive definitions of the evaluation metrics and their implementation details (Sec. E.1). In addition, we provide the human study on the perceptual metric (Sec. E.2), which demonstrates the correlation between our perceptual metric and human preference. E.1. Definition and implementation details Mean Temporal Misalignment (MTM). Let V(t) represent the ground truth vertex sequences, where each frame consists of vertex positions vt RN 3, with being the number of vertices. Similarly, ˆV(t) represents the predicted vertex sequences, with predicted vertex positions ˆvt RN 3. For each sample k, we select two specific vertices that correspond to the center of the upper and lower lips, extracting the upper-lip vertex sequence Vu(t) RT 3 and the lower-lip vertex sequence Vl(t) RT 3 (refer to Fig. S3). We then calculate the Euclidean distance between the upper and lower lip vertices over time to derive the ground truth lip distance sequence dv(t) = Vu(t) Vl(t). The same process is applied to obtain the predicted lip distance sequence ˆdv(t). To reduce noise, we apply Gaussian filter to both lip distance sequences. Next, we compute the first-order derivatives of the smoothed lip distance sequences to capture the dynamic changes in lip movement. We then use Derivative Dynamic Time Warping (DDTW) [19] to determine the optimal alignment path = {(i, j)} between the derivative sequences ˆdv(t). We identify local extrema (peaks and valδ dv(t) and δ leys) in each derivative sequence and match only extrema of the same type (i.e., both maxima or both minima) to compute the absolute time difference δtn = j (refer to Fig. S4). For each sample k, the sample mean temporal misalignment tk is computed as tk = m=1 δtn, where is the number of matched extrema pairs in the sample. 1 (cid:80)M Figure S3. Central vertices of the lower and upper lips. We select two specific vertices that correspond to the center of the upper and lower lips to extract the lip vertex displacement sequences. Figure S4. An example of DDTW matching results between ground truth and predicted lip distance sequences. We present an example of the DDTW local extrema correspondences of the ground truth and predicted lip vertex displacement sequences. We represent matched local extrema using green lines. (cid:80)K 1 by = Finally, the overall mean temporal misalignment is given k=1 tk, where is the total number of samples. smaller indicates better temporal alignment of the predicted sequences with the ground truth lip movements. To express the Mean Temporal Misalignment (MTM) in milliseconds, we multiply by the frame duration for the given dataset. For instance, for dataset with 25 FPS, the MTM is obtained by multiplying by 40ms. Refer to Algorithm 1 for more details on the MTM calculation. Furthermore, to validate the physical accuracy of our proposed temporal synchronization metric, we present graph showing the relationship between the temporal offset and the corresponding MTM values. Specifically, we introduce temporal mismatch to the ground truth mesh sequences of VOCASET [8] by making the speech leading the mesh sequences by 0 to 10 frames (i.e., 0 to 333ms for VOCASET). Figure S5 shows that MTM accurately captures the degree of temporal mismatch across the samples, demonstrating the effectiveness and physical accuracy of our proposed temporal synchronization metric. Perceptual Lip Readability Score (PLRS). We train speech-mesh representation using our proposed two-stage training process with different datasets, initializations, and batch sizes. For both Stage 1 and Stage 2, we use batch size Figure S5. Physical accuracy of Mean Temporal Misalignment. We introduce temporal mismatch to the ground truth mesh sequences of VOCASET [8] by shifting the speech to lead the mesh sequences by 0 to 10 frames (where 0 represents no mismatch). For each temporal offset, we calculate the average MTM and plot graph showing the relationship between the temporal offset and the corresponding MTM values. of 256. Given speech and generated mesh pair (Xs, ˆXm), we split the generated mesh into 5 frames with sliding window size of 5 to make mesh tokens { ˆMi}G i=1 , and the speech is also converted into corresponding speech tokens {Si}G i=1. We then compute the average cosine similarity between mean pooled speech embeddings {cs,i}G i=1 and mesh embeddings {cm,i}G i=1: LRS(S, ˆM) = 1 (cid:88) i=1 cs,i cm,i cs,i cm,i . (a) Speech-Lip Intensity Correlation Coefficient (SLCC). First, we define speech intensity using speech loudness, specifically the Root Mean Square (RMS) value, which is widely accepted measure of speech intensity in signal processing. RMS loudness effectively captures the energy of the speech signal and provides an accurate representation of perceived speech intensity. However, since RMS values can vary based on recording conditions (e.g., microphone gain and distance from the microphone), we perform identity-wise z-normalization on the RMS values to standardize them, assuming that clips belonging to the same identity are recorded under similar conditions. The Speech Intensity (SI) is thus defined as: SIk = RMSk µs,i σs,i , (b) where RMSk is the averaged RMS value of k-th video clip and µs,i and σs,i are the mean and standard deviation of the 14 speech RMS values for the clips with identity I. To define Lip Intensity (LI), we first measure the averaged lip displacement value of k-th video clip Distk. as:"
        },
        {
            "title": "Metric\nLVE\nPLRS",
            "content": "Spearmans ρ 0.166 0.437 Distk = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nTk − 1",
            "content": "(cid:32) Tk1 (cid:88) t="
        },
        {
            "title": "1\nVl",
            "content": "Vl(cid:88) v=1 (cid:33)2 lt+1,v lt,v , (c) Table S3. Human study on perceptual metric. We conduct human study to validate our proposed perceptual metric, PLRS. We compute the Spearmans correlation coefficient ρ to compare the PLRS rankings with the human preference rankings. where Tk is the number of frames in clip k, Vl is the number of vertices in the lip region, and lt,v R3 represents vertex position in the lip region at time t. Similar to Speech Intensity, we perform identity-wise z-normalization to the lip displacement values to mitigate individual bias in lip movement as: LIk = Distk µl,i σl,i , (d) where µl,i and σl,i are the mean and standard deviation of the lip displacement values for the clips belonging to identity I. Finally, we can obtain the Speech and Lip Correlation Coefficient as: rSL = (cid:113)(cid:80)K (cid:80)K k=1(SIk SI)(LIk LI) k=1(SIk SI)2 k=1(LIk LI)2 (cid:113)(cid:80)K , (e) where SI = 1 (cid:80)K k=1 SIk and LI = 1 (cid:80)K k=1 LIk. E.2. Human study on perceptual metric To validate that our proposed perceptual metric, Perceptual Lip Readability Score (PLRS), effectively evaluates perceptual alignment, we conduct human study that assesses the correlation between the metric scores and human preferences. We collect meshes from the ground-truth VOCASET [8] dataset and those generated by FaceFormer [11], CodeTalker [46] and SelfTalk [21]. We measure the PLRS and the existing evaluation metric Lip Vertex Error (LVE) for the generated meshes of each model, and subsequently rank the models by their PLRSs and LVEs. We ask 16 participants, evenly balanced in gender and from non-expert backgrounds, to rank the models based on their preferences. We then compute the Spearmans correlation coefficient ρ to compare the PLRS rankings and the LVE rankings with the human preference rankings. As shown in Table S3, PLRS exhibits far more positive correlation with human preferences compared to the LVE. This highlights the efficacy of our proposed metric in evaluating perceptual lip readability from human perspective. F. Implementation Details of Ablation Study In this section, we provide implementation details of model variants ablated from our speech-mesh representation: the 3D SyncNet and the representation w/o 2D prior. 15 3D SyncNet. Inspired by Chung et al. [6], we train 3D SyncNet to evaluate the performance of our transformerbased model compared to CNN-based model. 3D SyncNet is trained using InfoNCE loss with batch size of 80. The architecture of 3D SyncNet consists of the mesh encoder comprising three dilated convolutional layers and the speech encoder with six convolution layers followed by two linear layers. The mesh and speech features are extracted from each encoder, respectively. We train 3D SycnNet on an NVIDIA RTX 3090 GPU for 20 epochs using LRS3-3D. Also, for imposing the perceptual loss to 3D talking head models with 3D SyncNet, we finetune the model with VOCASET [8] train split for 5 epochs, as our speech-mesh representation model does. Ours w/o 2D prior. We train speech-mesh representation without Stage 1 training to evaluate the effectiveness of our learned 2D prior. We train the speech encoder and mesh encoder, both with the same architecture as Stage 2, and the other hyperparameters are the same as in Stage 2. G. Additional Results In this section, we present quantitative results on human studies (refer to Sec. G.1) and Upper Face Dynamics Deviation (FDD) evaluation (refer to Sec. G.2), comparing samples generated by the base models [11, 21, 46] with and without perceptual loss to demonstrate the effectiveness of our speech-mesh representation. Additionally, we provide the qualitative result of temporal synchronization for the base models [11, 46] (refer to Sec. G.3). We also provide comparisons on the stability of perceptual loss and cosine similarity for ablated model variants (refer to Sec. G.4). G.1. Human study on applying perceptual loss We conduct human study to evaluate the perceptual preference for our method with two configurations: (1) training and testing on VOCASET, and (2) training on the combined MEAD-3D and VOCASET and testing on MEAD-3D, as mentioned in Sec. 6.1 of the main paper. In the first configuration, we ask participants, evenly balanced group of males and females with non-expert backgrounds, to compare two videos: one generated by the base model [11, 21, 46] without our perceptual loss and the other with it. To assess the quality of generated meshes, we design two separate descriptionsone focusing on lip synchronization and the other on overall quality. For lip synchronization, participants are provided with the following description: Please evaluate the lip synchronization between the speech and the lip movements in videos and B, and choose the one that is more realistic and preferred. total of 18 participants take part in this evaluation. Table S4 shows that the participants significantly favor the models incorporating our perceptual loss with an overall preference rate of 72.9%. For overall quality, the description is as follows: Please evaluate the overall quality of facial movements in videos and B, and choose the one that is more realistic and preferred. This evaluation involves 15 participants. As shown in Table S5, the participants show strong preference for the model incorporating perceptual loss, with an overall preference rate of 73.3%, indicating that the perceptual loss not only improves lip synchronization but also enhances the overall quality of facial movements. In the second configuration, we ask 14 participants, also an evenly balanced group of males and females with nonexpert backgrounds, to compare three videos: one generated by the base model [11, 21, 46] trained on VOCASET, another generated by the base model trained on both MEAD-3D and VOCASET without our perceptual loss, and the other generated by the base model trained on both MEAD-3D and VOCASET with our perceptual loss. The description is as follows: Please rate the lip synchronization between the speech and the lip movements in videos through C, with 3 being the most realistic and preferred, and 1 being the least. As indicated in Table S6-(a) and (b), the participants significantly prefer the models incorporating MEAD-3D and our perceptual loss each by in 76.9% and 67.9% overall. Notably, incorporating both MEAD-3D dataset and the perceptual loss results in 84.6% of participants favoring the model, as shown in Table S6-(c), compared to the original models. This preference on the two configurations highlights the effectiveness of our speech-mesh representation as plug-in module in enhancing lip synchronization from the perspective of human perception. G.2. FDD evaluation on applying perceptual loss In Table S7, we measure Upper Face Dynamics Deviation (FDD) [46], widely used metric for the upper face evaluation, to assess the effectiveness of our perceptual loss. The models applying our perceptual loss achieve similar or improved FDD scores. It is expected because FDD is not the main focus of our work due to no direct relationship with the quality of lip movements. G.3. Qualitative result of temporal synchronization We present the qualitative result of temporal synchronization using existing base models [11, 21, 46] (See Fig. S8). Given"
        },
        {
            "title": "Model",
            "content": "w/o Our rep. w/ Our rep."
        },
        {
            "title": "Overall",
            "content": "13.7% 32.4% 35.3% 27.1% 86.3% 67.6% 64.7% 72.9% Table S4. Human study results on lip synchronization in configuration 1. We adopt A/B test and report the percentage (%) of preferences for (Ours) over B, assessing the generated meshes on lip sync. Participants significantly favor the models incorporating our perceptual loss by in overall 72.9%."
        },
        {
            "title": "Model",
            "content": "w/o Our rep. w/ Our rep."
        },
        {
            "title": "FaceFormer\nCodeTalker\nSelfTalk",
            "content": "Overall 14.4% 27.8% 37.8% 26.7% 85.6% 72.2% 62.2% 73.3% Table S5. Human study results on overall quality in configuration 1. We adopt A/B test and report the percentage (%) of preferences for (Ours) over B, assessing the generated meshes on overall quality. Participants show strong preference for the models applying our perceptual loss, with an overall preference rate of 73.3%. Figure S6. Perceptual loss stability. We visualize the perceptual loss between GT speech-mesh pairs on VOCASET samples. Our representation demonstrates strong generalization capability and provides stable training signal compared to 3D SyncNet and our representation without 2D prior. rendered 3D face mesh sequences, we place vertical line with two pixel points near the lip region and extract the y-t slices of the mesh sequences to visualize the timing of lip closure and opening. Next, we align the y-t slices with their corresponding speech waveforms and mel-spectrograms along the time axis. We observe that these models already have reasonable temporal synchronization capability. Specifically, the timing of lip closure (e.g., for the /p/ sound) in the y-t slices aligns with minimal amplitude in both the speech waveforms and mel-spectrogram, while the timing of lip 16 Model (a) (b) (c) Original Original + MEAD-3D Original + MEAD-3D Original + MEAD-3D + Our rep. Original Original + MEAD-3D + Our rep. FaceFormer CodeTalker SelfTalk Overall 33.3% 17.9% 17.9% 23.1% 66.7% 82.1% 82.1% 76.9% 32.1% 34.6% 29.5% 32.1% 67.9% 65.4% 70.5% 67.9% 19.2% 19.0% 17.9% 15.4% 80.8% 91.0% 82.1% 84.6% Table S6. Human study results on lip synchronization in configuration 2. We report the percentage (%) of preferences for over B, assessing the generated meshes on lip sync. Overall 84.6% of participants prefer the model with MEAD-3D and our perceptual loss. Figure S7. Cosine similarity stability. We visualize the cosine similarity between GT speech-mesh pairs on VOCASET samples. Our representation demonstrates strong generalization capability compared to 3D SyncNet and our representation without 2D prior. FDD (107mm) FaceFormer + Ours rep. CodeTalker + Ours rep. SelfTalk + Ours rep. 3.789 3.325 3.414 3.259 3.319 3.424 Table S7. FDD evaluation. We report Upper Face Dynamics Deviation (FDD) scores to evaluate the variation in upper facial dynamics, which is not the main focus of our work. As expected, the models trained with our perceptual loss show similar or improved FDD scores. opening (e.g., for the /r/ sound) in the y-t slices coincides with large amplitude in both speech representations. G.4. Stability comparison on loss and cosine similarity To utilize our speech-mesh synchronized representation as perceptual loss, it is essential to provide stable training signal to the 3D talking head model. In the domain of 2D audio-visual speech representation, Yaman et al. [47] reveal that the transformer-based architecture [29] learns more robust representation and provides more stable guidance to talking head models compared to CNN-based approach [6]. 17 Figure S8. Qualitative results of temporal synchronization on existing models. We plot y-t slices of rendered 3D face mesh sequences on the lip region with corresponding speech waveforms and mel-spectrogram. We also indicate the time steps of lip closure and opening with vertical lines. This implies that existing models already exhibit reasonable temporal sync. capability. To explore whether these observations hold for 3D speechmesh representations, we evaluate both the lip-sync loss and cosine similarity across 3D SyncNet, our representation without 2D prior and our final representation. This analysis aims to validate the effectiveness of the transformer-based architecture and curriculum learning with pre-trained 2D speech representation. Specifically, we measure the perceptual loss and cosine similarity, computing the mean and standard deviation for both the train and test samples. Figures S6 and S7 show the comparisons of perceptual loss and cosine similarity comparison across the three representation variants. We denote the train samples as green box plots and test samples as orange box plots, respectively. Our speech-mesh representation (Figs. S6-(c) and S7- (c)) demonstrates the highest stability, exhibiting the lowest standard deviations (the height of the box plots) on test set in both lip-sync loss and cosine similarity. In contrast, the representation without 2D prior (Figs. S6-(b) and S7- (b)) reveals significant discrepancies between the train and test samples on both the lip-sync loss and cosine similarity, indicating poor generalization capability. Additionally, it shows the highest standard deviations, which potentially cause unstable training. Meanwhile, 3D SyncNet (Figs. S6- (a) and S7-(a)) displays the worst mean values of perceptual Algorithm 1 Mean Temporal Misalignment Calculation Require: GT vertex sequence (t), Predicted vertex sequence ˆV (t) Ensure: Overall mean temporal misalignment Initialize time differences list: {δtn} Extract lip vertices: 1: Initialize list of sample mean misalignments: {tk} 2: for each sample do 3: 4: 5: 6: 7: 8: 9: 10: Upper lip vertex Vu(t) R3 from (t) Lower lip vertex Vl(t) R3 from (t) Predicted upper lip vertex ˆVu(t) R3 from ˆV (t) Predicted lower lip vertex ˆVl(t) R3 from ˆV (t) Compute lip distance sequences: dv(t) = Vu(t) Vl(t) (cid:13) (cid:13) ˆdv(t) = ˆVu(t) ˆVl(t) (cid:13) (cid:13) (cid:13) (cid:13) Smooth sequences using Gaussian filter: dv(t) = Gauss(dv(t)) ˆdv(t) = Gauss( ˆdv(t)) Compute derivatives: δ dv(t) = dv(t) dv(t 1) ˆdv(t 1) ˆdv(t) ˆdv(t) = δ Perform DDTW to find alignment path = {(i, j)} Identify local extrema in dv(t) and for each aligned pair (i, j) do ˆdv(t) if and are matching extrema of same type then if is within neighboring extrema range of in dv(t) then Compute time difference: δtn j Append δtn to {δtn} end if end if end for if {δtn} = then Compute mean delta time for clip k: tk = 1 Append tk to {tk} (cid:80)N n=1 δtn 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end if 31: 32: 33: end for 34: 35: 36: = if {tk} = then 37: else 38: 39: end if is undefined 1 (cid:80)K k=1 tk Compute overall mean temporal misalignment: Time Mem. (MB) (sec.) FaceFormer + Ours rep. CodeTalker + Ours rep. SelfTalk + Ours rep. 0.447 0.537 0.138 0.289 0.175 0.320 1461 1738 3393 3675 8204 Table S8. Training efficiency. We compared the memory consumption and single-iteration speed during training with and without the perceptual loss. loss and cosine similarity among the three. H. Discussion Limitations. While our perceptual loss is applied only during training, which ensures that the resource requirements at inference remain unchanged, it requires additional computational resources during training. In Table S8, we compare memory consumption and single-iteration speed during training, measured on single A6000 GPU. Also, to capture the intricate correspondence between speech and 3D face mesh, we construct large-scale speech-mesh paired datasets, LRS33D and MEAD-3D. To this end, we utilize state-of-the-art monocular face reconstruction methods [13, 26], which may impose limitations on the quality of the 3D mesh in the reconstructed datasets. Ethical considerations. Our method can generate realistic 3D talking faces from arbitrary audio signals, relying on both the 3D scan data collected from actors and the reconstructed data from 2D talking videos. Thus, while this technology has powerful applications, it also poses risks of misuse, such as creating harmful or embarrassing content. To mitigate these risks, we emphasize raising public awareness and promoting ethical and responsible use through continued research."
        }
    ],
    "affiliations": [
        "Dept. of Electrical Engineering, POSTECH",
        "Grad. School of AI, POSTECH",
        "KRAFTON",
        "School of Computing, KAIST"
    ]
}