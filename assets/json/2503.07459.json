{
    "paper_title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning",
    "authors": [
        "Xiangru Tang",
        "Daniel Shao",
        "Jiwoong Sohn",
        "Jiapeng Chen",
        "Jiayi Zhang",
        "Jinyu Xiang",
        "Fang Wu",
        "Yilun Zhao",
        "Chenglin Wu",
        "Wenqi Shi",
        "Arman Cohan",
        "Mark Gerstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark."
        },
        {
            "title": "Start",
            "content": "MEDAGENTSBENCH: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning Xiangru Tang Fang Wu , Yilun Zhao Yale University , Daniel Shao , Jiwoong Sohn , Chenglin Wu , Jiapeng Chen , Jiayi Zhang , Jinyu Xiang , , Wenqi Shi , Arman Cohan , Mark Gerstein"
        },
        {
            "title": "UT Southwestern Medical Center",
            "content": "xiangru.tang@yale.edu 5 2 0 2 0 1 ] . [ 1 9 5 4 7 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MEDAGENTSBENCH, benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planningscenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DEEPSEEK R1 and OPENAI O3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/ medagents-benchmark."
        },
        {
            "title": "Introduction",
            "content": "LLMs have demonstrated remarkable capabilities in medical natural language processing tasks, from answering clinical questions to assisting in diagnostic processes (Singhal et al., 2025; Jin et al., 2022; Chen et al., 2023a,b; Zhou et al., 2023; Gao Equally contribution. Figure 1: Performance analysis of large language models on medical tasks. Overall Pass@1 accuracy comparison across models in zero-shot setting. The score is an average of seven test sets results (MedQA, PubMedQA, MedMCQA, MedBullets, MMLU, MMLU-Pro, MedExQA, and MedXpertQA). et al., 2024). However, as shown in Figure 1, even OPENAI O3, GPT-4O and CLAUDE 3.5 SONNET struggle with complex medical scenarios that require deep domain expertise and multi-step reasoning (Xu et al., 2024; Fan et al., 2025; Shi et al., 2024). To enhance LLMs medical reasoning capabilities, researchers have proposed various approaches. As summarized in Table 1, these methods range from general-purpose techniques like CHAIN-OFTHOUGHT (COT) and SELF-CONSISTENCY (SC) (Wei et al., 2022; Wang et al., 2022) to domainspecific frameworks such as MEDPROMPT (Chen et al., 2024b). While these traditional approaches provide modest improvements, recent evidence suggests that agent-based methods, or agent workflows, demonstrate superior performance. Methods like MEDAGENTS (Tang et al., 2023) and MDAGENTS (Kim et al., 2024a) leverage multiagent collaboration frameworks to achieve more robust medical reasoning. However, with the adMethod Chain-of-Thought (Wei et al., 2022) Self-Consistency (Wang et al., 2022) MedPrompt (Chen et al., 2024b) Multi-Persona (Wang et al., 2023) Self-Refine (Madaan et al., 2024) MedAgents (Tang et al., 2023) MDAgents (Kim et al., 2024a) AFlow (Zhang et al., 2024) SPO (Xiang et al., 2025) Description Elicits reasoning in large language models Improves chain of thought reasoning through sampling diverse reasoning paths Multi-round prompting with ensemble voting for medical question answering Task-solving agent through multi-persona self-collaboration Iterative refinement with self-feedback Collaborative multi-agent framework for zero-shot medical decision making Dynamic multi-agent collaboration framework for medical reasoning Automating agentic workflow generation Self-supervised prompt optimization Table 1: Methods Overview. The reasoning approaches spanning four categories: baseline prompting methods , advanced prompting techniques , agent-based frameworks , and search-based agent methods . vent of advanced thinking models like OPENAI O3-MINI and DEEPSEEK R1, as well as the development of search-based agent frameworks, it remains an open question how these models perform in medical reasoning tasks. Several critical challenges in the evaluation of medical reasoning capabilities create significant gap in our ability to assess advanced LLMs and agent frameworks. (a) As shown in Table 2, existing medical reasoning datasets, while extensive, contain substantial proportion of straightforward questions derived from standardized examinations. On these simpler questions, even base LLMs achieve high performance (see Table 3, FULL columns), obscuring meaningful evaluation of advances in reasoning methods. (b) The inconsistent sampling practices across different studieswhere researchers arbitrarily extract approximately 300 questions from datasets containing thousands of examples (Tang et al., 2023; Kim et al., 2024a)further inhibit reliable comparisons between approaches. (c) Moreover, current benchmarks fail to systematically capture the critical interplay between performance, computational costs, and inference timefactors that significantly impact real-world deployment decisions. This landscape motivates our work on MEDAGENTSBENCH, benchmark specifically designed to evaluate complex medical reasoning capabilities where standard benchmarks fall short. Unlike existing benchmarks that either focus on general medical knowledge or suffer from ceiling effects, MEDAGENTSBENCH employs rigorous pipeline that: (1) draws from seven diverse established medical datasets (MedQA, PubMedQA, MedMCQA, MedBullets, MMLU, MMLU-Pro, MedExQA, and MedXpertQA); (2) applies adversarial filtering to identify truly challenging questions where models currently struggle; (3) conducts thorough contamination analysis to ensure validity; and (4) incorporates human annotations from medical professionals to verify reasoning depth requirements. Our comprehensive experiments yield several key insights: (a) thinking models like DEEPSEEK R1 and OPENAI O3 substantially outperform traditional approaches, achieving 15-25% higher accuracy on complex medical reasoning tasks; (b) among traditional and agent-based approaches, advanced search-based agent methods like AFLOW offer the best performance-to-cost ratio, achieving results that approach thinking models while requiring fewer computational resources; and (c) opensource models can achieve competitive results at significantly lower operational costs."
        },
        {
            "title": "2 Related Work",
            "content": "The evolution of medical reasoning has progressed with numerous specialized datasets. Popular benchmarks like MedQA (Jin et al., 2021), PubMedQA (Jin et al., 2019), and MedMCQA (Pal et al., 2022) established the foundation with standardized multiple-choice questions from medical licensing exams and PubMed abstracts. The field then expanded to domain-specific resources like clinical notes (Pampari et al., 2018) and question summarization (Abacha et al., 2021). This evolution continued with integrating visual elements through datasets like pathology images (He et al., 2020), radiology questions (Soni et al., 2022; Bae et al., 2023), and dental care (Zeng et al., 2025), broadening the scope of medical reasoning beyond text-only questions. To address linguistic and regional diversity, multilingual datasets emerged (Vilares and Gómez-Rodríguez, 2019; Hertzberg and Lokrantz, 2024; Olatunji et al., 2024). Additionally, event-driven resources like COVID-QA (Möller et al., 2020) were developed to address pandemicspecific information needs. The ecosystem further matured with comprehensive collections such as MultiMedQA (Singhal et al., 2023), which combines seven distinct medical datasets. Research in applying LLMs to medical tasks has progressed through several distinct phases. Initial efforts focused on evaluating the capabilBenchmark MedQA (Jin et al., 2021) PubMedQA (Jin et al., 2019) MedMCQA (Pal et al., 2022) MedBullets (Chen et al., 2024a) MedExQA (Kim et al., 2024b) MedXpertQA (Zuo et al., 2025) MMLU (Hendrycks et al., 2020) MMLU-Pro (Wang et al., 2024) MEDAGENTSBENCH Size Avg Lens Options 4 167.1 1273 3 316.1 500 4 18.7 2816 5 213.1 308 19.1 935 4 257.4 2450 55.9 1089 57.4 818 147.4 862 Description Multiple choice questions from medical licensing exams Questions based on PubMed abstracts Questions from AIIMS & NEET PG entrance exams Questions from Medbullets online medical study platform Multiple-choice questions across additional five medical specialties 10 Advanced medical questions with understanding (U) and reasoning (R) Multitask questions covering medical and other academic domains Multitask questions covering medical and other academic domains HARD subset across all datasets 4 3-10 3Table 2: Medical Question-Answering Datasets. Knowledge-based QA datasets are curated from medical literature, professional journals, and educational resources. Traditional benchmarks , recently emerging benchmarks , and general purpose benchmarks are shown with corresponding colors. Figure 2: Performance analysis of agents and models on MEDAGENTSBENCH. Cost-performance trade-off analysis showing Pass@1 accuracy versus cost per sample (in log scale), with marker sizes indicating inference time. Different markers represent various prompting methods , while colors distinguish different models. The Pareto frontier (red dashed line) indicates optimal cost-performance trade-offs. ities of general-purpose LLMs in medical contexts, with numerous surveys and benchmarks (Thirunavukarasu et al., 2023; Liévin et al., 2024; Gilson et al., 2023) establishing baseline performance metrics. Foundation models (Singhal et al., 2023; Achiam et al., 2023) have achieved physician-level performance through careful prompt engineering. This was followed by wave of fine-tuned or pretrained open-source models specifically for the medical domain (Han et al., 2023; Wu et al., 2024; Chen et al., 2023c). The field then advanced toward specialized reasoning methods (Liu et al., 2024; Shi et al., 2024) and retrieval-augmented generation (Jin et al., 2023; Jeong et al., 2024; Xiong et al., 2024). More recently, agent-based frameworks have shown particular promise, with MedAgents (Tang et al., 2023) and MDAgents (Kim et al., 2024a) leveraging collaborative multi-agent systems for complex medical decision-making. These developments collectively represent shift toward more sophisticated reasoning capabilities in medical AI, with thinking models demonstrating exceptional performance on complex medical reasoning challenges. 3 MEDAGENTSBENCH MEDAGENTSBENCH is carefully curated benchmark designed to evaluate complex medical reasoning tasks. Drawing from eight established medical datasets (MedQA (Jin et al., 2021), PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022), MedBullets (Chen et al., 2024a), MMLU (Hendrycks et al., 2020), MMLUPro (Wang et al., 2024), MedExQA (Kim et al., 2024b), and MedXpertQA (Zuo et al., 2025)), we systematically construct challenging subset that focuses on more complex reasoning scenarios. detailed description can be found in Appendix A. As shown in Table 3, these source datasets vary Figure 3: Distribution of model performance across eight medical datasets (MedQA, MedMCQA, PubMedQA, MedBullets, MMLU-Pro, MMLU, MedExQA, and MedXpertQA. Each subplot shows the number of questions answered correctly by different proportions of models (x-axis: k/N, where is the number of correct models and is the total number of models). Questions are categorized as either hard (left of the dashed line, < 50% of models correct) or easy (right of the dashed line, 50% of models correct), with selected questions highlighted in darker shades. The total question count for each dataset is indicated in the subplot titles. significantly in size (from 174 to 2,816 questions), average token length (18.7 to 316.1), and number of options (3 to 10), providing diverse evaluation contexts. Our hard-set selection process is based on three key criteria: 1. Model Performance Distribution As visualized in Figure 3, we analyze the proportion of models that correctly answer each question (k/N ratio). Questions, where less than 50% of models provide correct answers (left of the dashed line in Figure 3), are categorized as hard candidates. This ensures our benchmark focuses on truly challenging questions that current models struggle with. 2. Question Diversity We carefully balanced our dataset to ensure comprehensive coverage across various medical knowledge domains. Our HARD subset includes precisely 100 questions each from MedQA, PubMedQA, MedMCQA, MMLU-Pro, and MedExQA. From MedXpertQA, we incorporated both its Reasoning (MedXpertQA-R) and Understanding (MedXpertQA-U) subsets (100 questions each), which were annotated in the original paper to distinguish between questions requiring complex clinical reasoning versus those primarily testing medical knowledge. Additionally, we included 89 questions from MedBullets and 73 questions from MMLU. This distribution maintains proportional representation while ensuring sufficient sampling depth to evaluate model performance across diverse medical subdomains and reasoning requirements. 3. Reasoning Depth We prioritize questions that require multi-step reasoning, as evidenced by the performance gap between base models and agentbased approaches. As shown in Table 3, while models achieve high accuracy on the FULL set (e.g., GPT-4o: 87.8% on MedQA), their performance drops significantly on our HARD subset (e.g., GPT4o: 32.0% on MedQA-HARD), confirming the increased difficulty. In summary, we first evaluate each candidate question across multiple model architectures (as shown in Table 3) to ensure the architectureindependent difficulty. Secondly, we conduct data contamination analysis using MELD (Memorization Affects Levenshtein Detector) (Nori et al., 2023). This analysis involves providing models with the first half of each question in our test set and measuring the similarity between their generated continuations and the original second half. Table 3: Performance heatmap by base models and datasets. For each task, accuracy values are in percentages, with separate columns for FULL and HARD. The best values and the second-best values are highlighted. Model GPT-4O-MINI GPT-4O DEEPSEEK-V O1-MINI O3-MINI QWQ-32B DEEPSEEK-R1 LLAMA-3.3-70B CLAUDE-3.5-S CLAUDE-3.5-H MedQA FULL HARD PubMedQA FULL HARD MedMCQA FULL HARD MedBullets FULL HARD MMLU FULL HARD MMLU-Pro FULL HARD MedExQA FULL HARD MedXpert-R MedXpert-U FULL HARD FULL HARD 73.4 87.8 79. 89.9 92.7 78.6 92.0 76.8 77. 63.4 22.0 32.0 16.0 49.0 53. 29.0 47.0 14.0 18.0 13.0 76. 79.2 73.6 77.4 79.6 77.8 76. 77.8 76.4 73.8 10.0 9.0 12. 11.0 16.0 16.0 13.0 13.0 10. 12.0 66.0 76.6 74.3 73.2 77. 69.7 81.9 71.4 68.8 62.9 17. 25.0 19.0 21.0 24.0 24.0 31. 20.0 10.0 23.0 53.6 70.5 61. 73.1 82.1 54.2 79.2 61.7 56. 49.4 10.1 19.1 13.5 38.2 50. 12.4 43.8 16.9 9.0 10.1 84. 91.3 89.7 90.7 93.4 87.0 95. 85.2 86.9 79.7 12.3 24.7 15. 31.5 35.6 19.2 43.8 12.3 16. 11.0 57.5 69.1 64.7 67.8 70. 65.2 79.6 61.7 64.2 57.5 11. 21.0 12.0 19.0 15.0 28.0 37. 10.0 14.0 12.0 78.4 84.7 83. 82.5 85.2 81.5 86.6 79.3 81. 77.3 4.0 18.0 7.0 15.0 18. 10.0 26.0 7.0 13.0 13.0 13. 22.5 18.7 29.0 33.9 17.7 37. 20.2 20.4 14.6 6.0 7.0 6. 29.0 25.0 9.0 25.0 9.0 9. 8.0 16.6 23.3 23.3 27.0 31. 17.3 37.5 22.4 24.1 16.5 5. 6.0 9.0 14.0 15.0 6.0 26. 9.0 11.0 6.0 Table 4: Performance heatmap by methods and datasets. All tasks are evaluated on the HARD set with accuracy in % using two base models: GPT-4O-MINI and GPT-4O. The best values and the second-best values are highlighted. Method MedQA PubMedQA MedMCQA MedBullets MMLU MMLU-Pro MedExQA MedXpert-R MedXpert-U Average ZERO-SHOT FEW-SHOT COT COT-SC"
        },
        {
            "title": "MULTIPERSONA",
            "content": "SELF-REFINE MEDPROMPT MEDAGENTS MDAGENTS SPO AFLOW 4O-M 4O 4O-M 22.0 30.0 21. 20.0 29.0 32.0 29.0 24.0 22. 19.0 30.0 32.0 28.0 39.0 37. 45.0 41.0 34.0 43.0 36.0 31. 48.0 10.0 22.0 13.0 11.0 13. 12.0 14.0 12.0 23.0 25.0 15. 4O 9.0 20.0 10.0 6.0 15. 13.0 11.0 15.0 11.0 31.0 18. 4O-M 4O 4O-M 4O 4O-M 4O 4O-M 4O 4O-M 4O 4O-M 17. 31.0 26.0 20.0 21.0 24.0 30. 22.0 16.0 20.0 25.0 25.0 29. 30.0 35.0 25.0 34.0 26.0 30. 22.0 30.0 31.0 10.1 23.6 18. 16.9 15.7 15.7 13.5 15.7 14. 22.5 15.7 19.1 23.6 28.1 30. 29.2 28.1 22.5 27.0 21.3 29. 34.8 12.3 28.8 28.8 28.8 26. 27.4 20.5 24.7 17.8 19.2 24. 24.7 27.4 26.0 30.1 37.0 34. 26.0 28.8 24.7 32.9 38.4 11. 10.0 35.0 34.0 36.0 31.0 34. 3.0 9.0 32.0 29.0 21.0 4. 9.0 35.0 43.0 42.0 34.0 22. 8.0 8.0 36.0 37.0 25.0 14. 19.0 17.0 16.0 18.0 12.0 10. 14.0 7.0 18.0 24.0 24.0 22. 21.0 22.0 16.0 19.0 13.0 19. 22.0 6.0 16.0 6.0 10.0 7. 7.0 6.0 4.0 8.0 11.0 7. 4O 7.0 14.0 12.0 10.0 10. 17.0 14.0 3.0 4.0 15.0 13. 4O-M 5.0 8.0 10.0 13.0 12. 12.0 13.0 5.0 9.0 11.0 7. 4O 6.0 11.0 15.0 14.0 16. 19.0 9.0 6.0 5.0 16.0 18. 4O-M 4O 10.8 21.6 19.1 19. 19.6 19.7 19.8 13.6 14.4 19. 17.8 18.0 20.7 24.3 25.3 26. 26.9 20.1 20.0 16.1 26.7 28. As shown in Figure 4, the HARD subset selected for our MEDAGENTSBENCH benchmark consistently demonstrates lower similarity scores (2040%) compared to the original datasets, suggesting our filtering process effectively selects questions that test genuine reasoning rather than memorization. Complete details of our MELD methodology and its limitations are discussed in Section 5.1. Finally, four medical professionals (M.D. students) review the final question set to verify clinical relevance and reasoning complexity. The resulting MEDAGENTSBENCH benchmark contains 862 questions with an average token length of 147.4."
        },
        {
            "title": "4 Experiments",
            "content": "We conduct comprehensive evaluation of both base models and agentic reasoning methods across our MEDAGENTSBENCH benchmark. Our experiments follow standardized protocol to ensure fair comparison, with consistent prompting strategies and evaluation metrics across all models and methods."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "For base model comparison, we evaluate both closed-source models (GPT-4O, GPT-4O-MINI, CLAUDE-3.5-SONNET, CLAUDE-3.5-HAIKU, O1MINI, and O3-MINI) and open-source alternatives (DEEPSEEK-V3, DEEPSEEK-R1, LLAMA-3.370B, and QWQ-32B). Additionally, we evaluate 11 distinct agentic reasoning approaches spanning three categories. The first category includes baseline prompting methods such as ZERO-SHOT, FEW-SHOT, CHAINOF-THOUGHT, and SELF-CONSISTENCY. The second encompasses advanced prompting techniques: MULTI-PERSONA, SELF-REFINE, and MEDPROMPT. The third category covers agentbased frameworks, including medical-specific collaboration frameworks (MEDAGENTS and MDAGENTS, which we reimplemented to ensure accurate parsing of multiple-choice responses) and search-based agent methods (SPO and AFLOW) with search strategies consistent with the original setting.. Our data contamination analysis (Figure 4) revealed that OpenAI models demonstrate consistently lower memorization metrics across all datasets compared to other model families. This finding guided our decision to use GPT-4O and GPT-4O-MINI as primary models for agentic reasoning to minimize performance advantages stemFigure 4: Data contamination analysis across medical question-answering datasets using MELD. The boxplots display similarity percentages between model-generated text and original question text, with higher values potentially indicating memorization of training data. Lower similarity scores suggest minimal data contamination, while higher values may indicate potential contamination in model training data. ming from potential training data contamination. Standardized evaluation All experiments utilize identical prompt templates and evaluation protocols to ensure fair comparison. We implement standardized two-round inference protocol per query for agent-based methods requiring multiple inference rounds (e.g., MEDAGENTS). Multi-agent approaches (e.g., MULTI-PERSONA) consistently employ three distinct agent roles for all evaluations. This standardization mitigates confounding factors that might result from implementation variations, allowing us to more accurately attribute performance differences to the methods themselves rather than differences in their implementations."
        },
        {
            "title": "4.2 Cost Estimation",
            "content": "To analyze cost-performance trade-offs, we followed standardized evaluation protocol. Understanding these trade-offs is particularly important given the significant computational resources required by advanced thinking models, where their enhanced reasoning capabilities often come with substantially higher inference costs and longer processing times than traditional models. Similarly, multi-agent frameworks necessitate multiple rounds of API calls for agent interactions, further increasing both computational costs and inference times. For API-based commercial models (OPENAI and CLAUDE), we calculated costs using their published pricing rates based on total token usage (input + output). Based on their platform rates, we estimated costs for open-source models run on Together AI . The total cost of experimentation was $226.17. We measured inference time as wall-clock time per sample, including prompt construction and model inference, with agent-based methods including their complete interaction cycles."
        },
        {
            "title": "4.3 Main Results",
            "content": "Most models demonstrate significant difficulty with our challenging benchmark, with even powerful systems like GPT-4O achieving only 32.0% accuracy on MedQA and 18.0% on MedExQA in our HARD subset. This substantial performance drop confirms that our selection criteria effectively identifies questions requiring advanced reasoning capabilities. Amid these challenges, DEEPSEEK-R1 demonstrates remarkable performance, achieving the highest scores on five datasets Figure 5: Cost-performance analysis across seven medical datasets, comparing open and closed-source language models. Each subplot shows Pass@1 accuracy (%) versus cost per sample (USD, log scale). Marker shapes distinguish thinking models from non-thinking models, while colors indicate open-source (blue) versus closed-source (red) models. Marker sizes represent inference time, and the red dashed line shows the Pareto frontier of optimal cost-performance trade-offs. (MedMCQA: 31.0%, MMLU: 43.8%, MMLUPro: 37.0%, MedExQA: 26.0%, and MedXpertQAU: 26.0%). Similarly, O3-MINI excels on three datasets (MedQA: 53.0% and MedBullets: 50.6%, with tie on PubMedQA at 16.0%). When examining reasoning methods in Table 4, we find that advanced approaches generally outperform baseline methods. Specifically, AFLOW demonstrates superior performance, achieving the highest scores on four datasets with GPT-4O (MedQA: 48.0%, MedBullets: 34.8%, MMLU: 38.4%, and MedXpertQA-U: 18.0%), which indicates the effectiveness of automated agentic workflow generation for medical reasoning. Additionally, MULTIPERSONA performs exceptionally well on MedQA (45.0% with GPT-4O) and contributes to the second-best results on several other datasets, highlighting the benefits of multi-persona selfcollaboration. Traditional methods like CHAIN-OFTHOUGHT with SELF-CONSISTENCY (COT-SC) show consistent improvements over basic COT, with average gains of 2-3% across datasets, and excel particularly on MMLU-Pro (42.0% with GPT4O). However, domain-specific methods like MEDPROMPT show mixed resultsperforming well on specific datasets but lacking consistency across different medical tasks. Despite their specialized design for medical scenarios, agent-based methods like MEDAGENTS (best: 43.0% on MedQA with GPT-4O) and MDAGENTS dont consistently outperform the latest thinking models across all datasets, and they incur significantly heavier computational overhead. This suggests that while agent frameworks provide benefits for specific tasks, the inherent reasoning capabilities of advanced base models may be more important for complex medical reasoning than the framework itself. Overall, our results demonstrate that thinking models, particularly DEEPSEEK-R1 and O3-MINI, consistently excel in complex medical reasoning tasks, while search-based agent methods like AFLOW also show promising performance in handling intricate medical queries."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we comprehensively analyze our experimental results, focusing on two key aspects that impact the performance and reliability of LLMs in medical reasoning tasks."
        },
        {
            "title": "5.1 Data Contamination",
            "content": "To ensure the reliability of our benchmark evaluation, we employ memorization effects Levenshtein Detector (MELD), initially introduced by Nori et al. (2023) to analyze potential data contamination across various datasets and language models. MELD quantifies memorization by splitting each question into two halves: the first half serves as context for the model, which must then generate the second half. It calculates the proportion of matching characters by measuring the Levenshtein distance ratio between the generated output and the original unseen portion. Higher similarity scores suggest memorization rather than genuine reasoning. Since our benchmarks are multiple-choice questions with short answers represented as index letters (e.g., A, B, C, D), including the answer index in MELD has minimal impact on the similarity score. This is because the primary determinant of memorization is whether the question itself has been seen. While seeing question does not necessarily mean the model has seen its exact answer, it significantly increases the risk of contamination compared to model that has not encountered the question at all. The ability to reproduce question text verbatim indicates potential exposure during training, which could give certain models an unfair advantage regardless of whether theyve memorized the specific answer. The analysis spans GPT-3.5/4, CLAUDE-3.5, and various open-source LLMs, providing comprehensive coverage of different model architectures and training approaches. MELD exhibits high precision but unknown recall, meaning that while detected match strongly indicates memorization, the absence of match does not guarantee that the data was not seen during training. For instance, Nori et al. (2023) report that GPT-4 reproduces SQuAD 2.0 questions with 99% character overlap in 17% of cases, highlighting significant memorization. Figure 4 reveals distinct memorization patterns between commercial and open-source models. OpenAI models demonstrated consistently lower similarity scores (median 20-25%) across medical datasets, suggesting minimal verbatim memorization. In contrast, several open-source models (DEEPSEEK-R1, QWQ-32B, and LLAMA-3.370B) exhibited substantially higher scores, particularly on MedExQA (median 25-30%, with outliers exceeding 80% similarity). Concerningly, some open-source models reproduced question texts with over 95% character-level accuracy on certain benchmarks, with MMLU and MMLU-Pro showing vulnerability in the 60-90% similarity range. There were clear instances of training data contamination, challenging the validity of fair comparison. HARD set selected from MEDAGENTSBENCH demonstrated significantly lower MELD scores across all models, confirming its utility for more reliable performance assessment. Meanwhile, manual review of high-similarity cases in MedMCQA and MedExQA revealed that while some shared text consisted of standard medical terminology, this likely contributed to the high similarity in these specialist QA datasets rather than direct data contamination."
        },
        {
            "title": "5.2 Cost-Performance Trade-off",
            "content": "Our evaluation examines both model architecture efficiency and reasoning method effectiveness through cost-performance lens. We analyze these dimensions separately using two complementary visualizations."
        },
        {
            "title": "5.2.1 Base Model Cost-Efficiency",
            "content": "As shown in Figure 5, we compare ten base models across the performance-cost spectrum, categorizing them as open-source (blue) or closed-source (red), and as thinking models (circles) or non-thinking models (squares). This analysis reveals several important patterns: The Pareto frontier (red dashed line) identifies models that deliver optimal performance for their costany model below this line represents suboptimal investment, while models along the line represent the most efficient options available. DEEPSEEK-R1 and O3-MINI consistently appear as Pareto-optimal solutions across multiple datasets, indicating their exceptional efficiency in medical reasoning tasks. Domain-specific patterns emerge across different medical benchmarks. For example, MedQA demonstrates steep performance improvements with increased computational investment (jumping from 20% to over 50% accuracy with higher-cost models), while PubMedQA shows more modest gains, suggesting diminishing returns from larger models in certain domains. Thinking models (circles) consistently outperform non-thinking counterparts (squares) at comparable cost points, with performance differentials of 5-10% in complex tasks like MedBullets. This suggests that structured reasoning capabilities justify their computational overhead, particularly for challenging diagnostic scenarios. Open-source models demonstrate surprisingly competitive performance despite their lower costs. DEEPSEEK-R1 achieves comparable or superior performance to many closed-source alternatives while requiring 10x more computational costs, most notably in MedMCQA (81.9% accuracy) and MMLU-Pro (79.6% accuracy)."
        },
        {
            "title": "5.2.2 Reasoning Method Efficiency",
            "content": "Figure 3 extends our analysis to compare various reasoning methods applied to base models, revealing additional insights about cost-effective approaches to medical reasoning: Search-based agent methods like AFLOW achieve exceptional efficiency on the Pareto frontier, delivering performance comparable to much more expensive base models. This suggests that architectural improvements in reasoning strategies can offset raw model size and parameter count. Traditional reasoning methods like COT-SC demonstrate consistent improvements over basic COT, particularly on datasets like MMLU-Pro, where they achieve up to 43.0% accuracy with GPT-4O. Advanced prompting techniques occupy different positions relative to the Pareto frontier, with MULTIPERSONA showing exceptional efficiency for MedQA (45.0% with GPT-4O) but less consistent performance across other datasets. Specialized medical frameworks like MEDAGENTS show mixed efficiency profileswhile they excel at specific tasks (43.0% on MedQA with GPT-4O), they dont consistently outperform simpler approaches across all datasets, suggesting that general reasoning capabilities may sometimes be more important than domain-specific frameworks. The overall Pareto frontier reveals hierarchy of efficiency, where certain combinations of models and methods (e.g., DEEPSEEK-R1 with basic prompting or O3-MINI with minimal augmentation) deliver optimal performance per dollar spent, making them particularly valuable for resourceconstrained deployment scenarios. These complementary analyses demonstrate that both model architecture and reasoning method significantly impact the cost-efficiency of medical AI systems. When selecting approaches for medical reasoning tasks, practitioners should consider both the base models capabilities and the reasoning framework applied, evaluating them within the context of specific medical domains and available computational resources."
        },
        {
            "title": "6 Conclusion",
            "content": "Through MEDAGENTSBENCH, we provide several important contributions to medical AI evaluation. First, our comprehensive experiments demonstrate that thinking models, particularly DEEPSEEK-R1 and OPENAI O3-MINI, consistently excel in complex medical reasoning tasks, outperforming traditional approaches across diverse domains. Second, advanced search-based agent methods like AFLOW show promising performance-to-cost ratios, especially on diagnostic reasoning tasks requiring multistep inference. Our cost-performance analysis reveals that open-source models can achieve competitive results at significantly lower operational costs, with DEEPSEEK-R1 emerging as particularly effective option for resource-constrained environments. Beyond raw performance metrics, our findings suggest important directions for future research. We observe that agent frameworks provide substantial benefits for specific medical tasks but may not consistently outperform base thinking models across all scenarios. This indicates the need for more task-specific adaptation of reasoning frameworks. Additionally, our contamination analysis highlights the importance of robust evaluation protocols in medical AI, where data contamination can significantly confound performance assessments. Future work should explore hybrid approaches combining the inherent reasoning strengths of thinking models with specialized medical knowledge frameworks, and develop more sophisticated verification mechanisms for ensuring clinical accuracy and safety."
        },
        {
            "title": "Limitations",
            "content": "While MEDAGENTSBENCH provides rigorous benchmark for evaluating medical reasoning capabilities, several important limitations remain: First, our benchmark primarily focuses on medical question-answering tasks based on educational resources, which may not fully reflect the complexity and nuance of real-world clinical scenarios. more comprehensive evaluation would require incorporating real-world clinical cases, physicianpatient dialogues, and diagnostic decision-making processes. Second, we lack systematic verification of model outputs by practicing clinicians. This raises concerns about the reliability and alignment of modelgenerated reasoning paths with established medical knowledge. Future work should establish more rigorous verification framework involving domain experts to assess answer correctness, the validity of reasoning steps, and potential hallucinations. Finally, while our work demonstrates the effectiveness of multi-agent and ensemble approaches in medical reasoning, we have only scratched the surface of potential ensemble strategies. Sophisticated ensemble methods like step-wise verification, task-wise verification, and dynamic agent collaboration could yield even better performance. For instance, verifying intermediate reasoning steps through model consensus, utilizing heterogeneous model combinations, or implementing adaptive voting strategies based on agent expertise remain unexplored. Future research could investigate: (1) More sophisticated voting and aggregation strategies beyond simple majority voting. (2) Adaptive ensemble methods that dynamically adjust agent weights based on task characteristics. (3) Hierarchical ensemble approaches that combine both step-wise and task-wise verification. (4) Methods for increasing response diversity through systematic prompt variation and temperature tuning. (5) Integration of expert knowledge to guide ensemble selection and verification. While our current approach shows promising results, we lack thorough theoretical understanding of why specific ensemble configurations outperform others in medical reasoning tasks. more systematic study of ensemble properties - such as diversity, correlation, and calibration - could guide the development of more effective medical reasoning systems."
        },
        {
            "title": "References",
            "content": "Asma Ben Abacha, Yassine Mrabet, Yuhao Zhang, Chaitanya Shivade, Curtis Langlotz, and Dina Demner-Fushman. 2021. Overview of the mediqa 2021 shared task on summarization in the medical domain. In Proceedings of the 20th Workshop on Biomedical Language Processing, pages 7485. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo Oh, Lei Ji, Eric Chang, Tackeun Kim, et al. 2023. Ehrxqa: multi-modal question answering dataset for electronic health records with chest x-ray images. Advances in Neural Information Processing Systems, 36:38673880. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2024a. Benchmarking large language models on answering and explaining challenging medical questions. arXiv preprint arXiv:2402.18060. Qingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, and Hua Xu. 2023a. Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. arXiv e-prints, pages arXiv2305. Qingyu Chen, Yan Hu, Xueqing Peng, Qianqian Xie, Qiao Jin, Aidan Gilson, Maxwell Singer, Xuguang Ai, Po-Ting Lai, Zhizheng Wang, et al. 2023b. systematic evaluation of large language models for biomedical natural language processing: benchmarks, baselines, and recommendations. arXiv preprint arXiv:2305.16326. Xuhang Chen, Shenghong Luo, Chi-Man Pun, and Shuqiang Wang. 2024b. MedPrompt: Cross-modal prompting for multi-task medical image translation. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pages 6175. Springer. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023c. Meditron70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079. Zhihao Fan, Lai Wei, Jialong Tang, Wei Chen, Wang Siyuan, Zhongyu Wei, and Fei Huang. 2025. Ai hospital: Benchmarking large language models in multi-agent medical interaction simulator. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1018310213. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. 2024. Empowering biomedical discovery with ai agents. Cell, 187(22):61256151. Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash, et al. 2023. How does chatgpt perform on the united states medical licensing examination (usmle)? the implications of large language models for medical education and knowledge assessment. JMIR medical education, 9(1):e45312. Tianyu Han, Lisa Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno Bressem. 2023. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020. PathVQA: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286. Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, and Zuozhu Liu. 2024. Medcot: Medical chain of thought via hierarchical expert. arXiv preprint arXiv:2412.13736. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Niclas Hertzberg and Anna Lokrantz. 2024. MedQASWE: clinical question & answer dataset for In Proceedings of the 2024 Joint Inswedish. ternational Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1117811186. Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. 2024. Improving medical reasoning through retrieval and self-reflection with retrievalaugmented large language models. Bioinformatics, 40(Supplement_1):i119i129. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146. Qiao Jin, Won Kim, Qingyu Chen, Donald Comeau, Lana Yeganova, John Wilbur, and Zhiyong Lu. 2023. MedCPT: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Bioinformatics, 39(11):btad651. Qiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu, Huaiyuan Ying, Chuanqi Tan, Mosha Chen, Songfang Huang, Xiaozhong Liu, and Sheng Yu. 2022. Biomedical question answering: survey of approaches and challenges. ACM Computing Surveys (CSUR), 55(2):136. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, and Hae Won Park. 2024a. MDAgents: An adaptive collaboration of llms in medical decision making. arXiv preprint arXiv:2404.15155. Yunsoo Kim, Jinge Wu, Yusuf Abdulle, and Honghan Wu. 2024b. MedExQA: Medical question answering benchmark with multiple explanations. arXiv preprint arXiv:2406.06331. Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. 2024. Can large language models reason about medical questions? Patterns, 5(3). Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-Refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36. Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. COVID-QA: question answering dataset for COVID-19. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375. Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, et al. 2024. AfriMed-QA: pan-african, multi-specialty, medical question-answering benchmark dataset. arXiv preprint arXiv:2411.15640. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248260. PMLR. Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrQA: large corpus for question answering on electronic medical records. arXiv preprint arXiv:1809.00732. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Haotian Sun, Hang Wu, Carl Yang, and May Wang. 2024. Medadapter: Efficient test-time adaptation of large language models towards medical reasoning. arXiv preprint arXiv:2405.03000. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172180. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. 2025. Toward expert-level medical question answering with large language models. Nature Medicine, pages 18. Sarvesh Soni, Meghana Gudala, Atieh Pajouhi, and Kirk Roberts. 2022. RadQA: question answering dataset to improve comprehension of radiology reports. In Proceedings of the thirteenth language resources and evaluation conference, pages 6250 6259. Ziyun Zeng, Ashwin Ramesh, Jinglong Ruan, Peirong Hao, Nisreen Al Jallad, Hoonji Jang, Oriana LyMapes, Kevin Fiscella, Jin Xiao, and Jiebo Luo. 2025. Use of artificial intelligence to detect dental caries on intraoral photos. Quintessence international (Berlin, Germany: 1985), page 0. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. 2024. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762. Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam Chen, Peilin Zhou, Junling Liu, et al. 2023. survey of large language models in medicine: Progress, application, and challenge. arXiv preprint arXiv:2311.05112. Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. 2025. MedXpertQA: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2023. MedAgents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537. Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models in medicine. Nature medicine, 29(8):1930 1940. David Vilares and Carlos Gómez-Rodríguez. 2019. HEAD-QA: healthcare dataset for complex reasoning. arXiv preprint arXiv:1906.04701. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023. Unleashing the emergent cognitive synergy in large language models: task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. 2024. PMC-LLaMA: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843. Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu, and Yuyu Luo. 2025. Self-supervised prompt optimization. arXiv preprint arXiv:2502.06855. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, pages 62336251. Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei Li, Hanqi Jiang, Yi Pan, Junhao Chen, et al. 2024. Towards next-generation medical agent: How o1 is reshaping decision-making in medical scenarios. arXiv preprint arXiv:2411.14461."
        },
        {
            "title": "A Detailed Description of Datasets",
            "content": "This appendix provides detailed descriptions of the seven established medical datasets used in the construction of MEDAGENTSBENCH. MedQA MedQA is large-scale open-domain question answering dataset collected from profesIntroduced by Jin sional medical board exams. et al., it covers three languages: English, simplified Chinese, and traditional Chinese. The English portion contains 1,273 questions, with an average token length of 167.1 tokens per question. Each question is accompanied by 4 multiple-choice options. Questions are sourced from the United States Medical Licensing Examination (USMLE) and similar professional medical board exams. The questions test wide range of medical knowledge, including diagnosis, treatment, and medical concepts. MedQA is notable for its multilingual coverage and focus on professional-level medical knowledge assessment. The questions require both factual medical knowledge and clinical reasoning abilities. PubMedQA PubMedQA (Jin et al., 2019) is biomedical question answering dataset collected from PubMed abstracts, designed to test reasoning over biomedical research texts. The dataset contains 500 expert-annotated QA instances used in our benchmark, with an average token length of 316.1 tokens per question. Each question has 3 possible answers (yes/no/maybe). Each PubMedQA instance consists of research question derived from an article title, context from the corresponding abstract (excluding its conclusion), and an answer that summarizes whether the research supports yes, no, or maybe conclusion. PubMedQA is unique in requiring reasoning over biomedical research texts, including understanding of quantitative research findings and statistical evidence. It tests the ability to synthesize scientific information rather than recall medical facts. MedMCQA MedMCQA (Pal et al., 2022) is multiple-choice question answering dataset designed to address real-world medical entrance exam questions. Our benchmark includes 2,816 questions from this dataset, with an average token length of 18.7 tokens. Each question has 4 possible answers. Questions are collected from AIIMS and NEET PG entrance exams, covering 2,400+ healthcare topics and 21 medical subjects. The questions test various reasoning abilities across wide range of medical domains. MedMCQA stands out for its topical diversity and focus on entrance exam questions that test not just knowledge but application of medical concepts in practical scenarios. MedBullets MedBullets (Chen et al., 2024a) comprises USMLE Step 2/3 style clinical questions collected from the Medbullets online medical study platform. The dataset contains 308 questions with an average token length of 213.1 tokens per question. Each question is accompanied by 5 multiple-choice options. Questions are designed to simulate clinical scenarios similar to those encountered in medical licensing exams. Each question is paired with case description, answer choices, and explanations of correct and incorrect answers. MedBullets questions are specifically chosen to be challenging, focusing on realistic clinical scenarios that require integration of medical knowledge with clinical reasoning. The inclusion of expert explanations makes this dataset particularly valuable for evaluating reasoning paths. MedExQA MedExQA (Kim et al., 2024b) is benchmark designed to evaluate LLMs understanding of medical knowledge through explanations across multiple specialties. The dataset contains 935 questions with an average token length of 19.1 tokens per question. Each question has 4 multiplechoice options. MedExQA spans five distinct medical specialties that are underrepresented in current datasets: Audiology, Nutrition, Occupational Therapy, Physical Therapy, and Speech-Language Pathology. Each question-answer pair is accompanied by multiple explanations. MedExQA uniquely focuses on the ability of models to generate nuanced medical explanations, moving beyond classification accuracy to assess deeper understanding. It specifically addresses specialties where current LLMs demonstrate limited knowledge. MedXpertQA MedXpertQA (Zuo et al., 2025) is challenging benchmark designed to evaluate expert-level medical knowledge and advanced reasoning capabilities. The dataset includes 2,450 questions with an average token length of 257.4 tokens. Questions have up to 10 possible answers. MedXpertQA spans 17 medical specialties and 11 body systems, incorporating specialty board questions to improve clinical relevance. The dataset is divided into two subsets: MedXpertQAUnderstanding (U) for testing basic medical knowledge comprehension and MedXpertQA-Reasoning (R) for evaluating complex clinical reasoning. MedXpertQA is specifically designed to challenge advanced models with expert-level questions. Its reasoning-oriented subset is particularly valuable for assessing the capabilities of thinking models on complex medical decision-making tasks. MMLU (Medical Subset) The Massive Multitask Language Understanding benchmark (Hendrycks et al., 2020) includes several subsets focused on medical knowledge, which They extract for our benchmark. They include 1,089 medical-related questions from MMLU, with an average token length of 55.9 tokens per question. Each question has 4 multiple-choice options. The medical portions of MMLU include subjects such as clinical knowledge, anatomy, college medicine, medical genetics, professional medicine, and more. The benchmark covers both basic and advanced medical concepts. MMLU tests knowledge across many difficulty levels, from elementary to advanced professional. Its standardized format makes it useful for comparing medical reasoning to other domains of knowledge. MMLU-Pro MMLU-Pro (Wang et al., 2024) is an enhanced version of MMLU designed to be more challenging and reasoning-focused, with an expanded choice set. We include 818 medicalrelated questions from MMLU-Pro with an average token length of 57.4 tokens. Questions have between 3-10 multiple-choice options. MMLU-Pro eliminates trivial questions from MMLU and integrates more challenging, reasoning-focused questions that require deeper understanding of concepts. MMLU-Pro is specifically designed to address the ceiling effects observed in MMLU as models improved. Its expanded choice set and focus on reasoning rather than simple knowledge recall make it more discriminative for advanced models."
        }
    ],
    "affiliations": [
        "Yale University"
    ]
}