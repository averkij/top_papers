{
    "paper_title": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment",
    "authors": [
        "Ran Tian",
        "Yilin Wu",
        "Chenfeng Xu",
        "Masayoshi Tomizuka",
        "Jitendra Malik",
        "Andrea Bajcsy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment."
        },
        {
            "title": "Start",
            "content": "Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment . Ran Tian1, Yilin Wu2, Chenfeng Xu1, Masayoshi Tomizuka1, Jitendra Malik1, Andrea Bajcsy2 Abstract Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-users visual representation and then constructs dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment. More details (e.g., videos) are at the project website. Keywords Reinforcement learning from human feedback, visuomotor policy learning, representation learning, alignment"
        },
        {
            "title": "1 Introduction",
            "content": "Visuomotor robot policieswhich predict actions directly image observationsare being from high-dimensional revolutionized by pre-training on large-scale datasets. For example, robots like manipulators (Brohan et al. 2023; Chi et al. 2024), humanoids (Radosavovic et al. 2023), and autonomous cars (Hu et al. 2023; Tian et al. 2024b) rely on pre-trained vision encoders (Deng et al. 2009) for representing RGB images and for learning multimodal behavior policies from increasingly large observation-action teleoperation datasets (Padalkar et al. 2023). Despite this remarkable progress, these visuomotor policies do not always act in accordance with human enduser preferences. For instance, consider the scenario on the left of Figure 1 where robot manipulator is trained to imitate diverse teleoperators picking up bag of chips. At deployment, the end-user prefers the chips to remain intact. However, the manipulator frequently grasps the bag by squeezing the middlerisking damage to the chipsinstead of holding the packaging by its edges like the user prefers. foundational approach to tackle this misalignment between pre-trained policy and an end-users hard-tospecify preferences is reinforcement learning from human feedback (RLHF) (Christiano et al. 2017). By presenting the end-user with outputs generated by the pre-trained model and collecting their preference rankings, RLHF trains reward model that is then used to fine-tune the base policy, enabling it to produce outputs that better align with the end-users preferences. While RLHF has emerged as the predominant alignment mechanism in non-embodied domains such as large language models (LLMs) (Ouyang et al. 2022) and text-to-image generation models (Lee et al. 2023), it has not shown the same impact for aligning visuomotor robot policies. Fundamentally, this challenge arises because learning high-quality visual reward function requires an impractically large amount of human preference in our hardware experiments, collecting 200 feedback: preference rankings for single task takes approximately 1 day. If we hope to align generalist, pre-trained visuomotor policies, it is essential to adapt the RLHF paradigm to operate with significantly less human feedback. that Our approach is motivated by seminal work in inverse reinforcement learning (Abbeel and Ng 2004; Ziebart the desired reward et al. 2008), which states function we seek to learn via RLHF makes the robots behavior indistinguishable from the humans ideal behavior. Mathematically, the divergence between the feature distribution of the robots policy and that of the end-users optimal demonstrations (Pomerleau 1988; Dadashi et al. 2021; Sun et al. 2019; Swamy et al. 2021). However, when constructing visual reward can be modeled as this 1UC Berkeley 2Carnegie Mellon University This work has been supported in part by the Google Research Scholar Award. 4 2 0 2 ] . [ 1 5 3 8 4 0 . 2 1 4 2 : r 2 Figure 1. Representation-Aligned Preference-based Learning (RAPL), is an observation-only method for learning visual robot rewards from significantly less human preference feedback. (center) Unlike traditional reinforcement learning from human feedback, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-users visual representation. The aligned representation is used to construct an optimal transport-based visual reward for aligning the robots visuomotor policy. (left) Before alignment, the robot frequently picks up bag of chips by squeezing the middle, risking damage to the contents. (right) After alignment with our RAPL reward, the robot adheres to the end-users preference and picks up the bag by its edges. rewards, this feature matching has to occur within the endusers visual representation of the world, which is unknown to the robot priori. This implies that critical aspect of reward learning is identifying which visual features are important to the end-user. Instead of jointly learning visual features and divergence measure through human feedback (Christiano et al. 2017), our key idea is to allocate the limited human preference budget exclusively to fine-tuning pretrained vision encoders, aligning their visual representations with those of the end-user. Once the visual representation is fine-tuned, the reward function can be directly instantiated as dense feature matching using techniques such as optimal transport (Kantorovich and Rubinshtein 1958) within this aligned visual representation space (center, Figure 1). In Section 4, we formalize the visual representation alignment problem for robotics as metric learning problem in the humans representation space. We then propose Representation-Aligned Preference-based Learning (RAPL), tractable observation-only method for learning visual robot rewards for aligning visuomotor robot policies. In Section 5, we perform simulation experiments in the X-Magical benchmark and in manipulation with the Franka Panda robot, ablating the way we learn visual representations and how we design reward predictors. We find that RAPL learns visual rewards that closely align with ground truth rewards, enables more human dataefficient RLHF paradigm for aligning robot visuomotor policies, and shows strong zero-shot generalization when the visual representation is learned on different embodiment from the robots. Finally, in Section 6, we instantiate RAPL in hardware experiments for efficiently aligning Diffusion Policies (Chi et al. 2024) deployed in three real world object manipulation tasks: chip bag grasping, cup picking, and fork pick-and-place into bowl. We instantiate RAPL in the context of direct preference optimization (Rafailov et al. 2024), variant of RLHF that aligns generative models directly using preference rankings without learning. RAPLs visual reward requiring reinforcement reduces reliance on real human preference rankings by 5x, generating high-quality synthetic preference rankings at scale and aligning the robots visuomotor policy with the humans preferences (right, Figure 1). This work is an extended version of the conference paper Tian et al. (2024a). We expand the content of this paper via: New ablations and baselines in simulation. In Section 5.2.2, we expanded our evaluation by including additional visual reward baselines, providing visualizations of representations, and evaluating RAPL on cluttered robot manipulation task with visual distractors. These further validate RAPLs ability to disentangle the visual features that influence end-user preferences. the visual Aligning generative robot policies in the real world. In Section 6, we extended our approach from simulation to hardware. We use RAPL as synthetic preference generator built from only 20 real human preference rankings and use the rankings to align diffusion-based visuomotor policy in three realworld manipulation tasks. These results highlight how RAPL can alleviate human feedback burden while still ensuring high-quality policy alignment."
        },
        {
            "title": "2 Related Work",
            "content": "Preference alignment of generative models. Generative models, such as large language models (LLMs), text-toimage generation models, and behavior cloning models, are predominantly trained using an imitative objective. While this approach simplifies training with internet-scale data, this objective serves only as proxy for the true training goal: the human internal reward function. As result, these generative models may be misaligned with end-user preferences or may even lead to safety-critical scenarios (e.g., generating unsafe texts (Mu et al.), images (Lee et al. 2023), and robot motions (Lu et al. 2023)). Preference alignment, particularly through reinforcement learning from human feedback (RLHF) (Christiano et al. 2017), has emerged as key strategy for aligning generative models with human preferences, especially in non-embodied domains. RLHF involves three key stages: feedback elicitation (e.g., presenting users with two or more model generations and gathering preference rankings), reward modeling (e.g., training reward model to replicate the rankings), and policy optimization via reinforcement learning (RL) (Schulman et al. 2017). Traditional RL the model methods require simulator to gather the learning agents experiences under the current reward model. To address this dependency, direct preference optimization (DPO) (Rafailov et al. 2024) has been proposed as variant of RLHF that directly updates the generative model using preference rankings through contrastive learning. Despite the success of RLHF in non-embodied domains, and regardless of its algorithmic instantiation, its application in embodied settings remains prohibitively expensive due to the challenges of collecting extensive human feedback on robot rollouts. Our work seeks to adapt the RLHF paradigm for embodied domains, enabling efficient visuomotor robot policy alignment. Scaling preference alignment with synthetic feedback. Recent works in the LLM domain have explored leveraging AI feedback to automatically generate preference rankings, thereby facilitating the reward learning. These approaches typically rely on either single teacher model (Bai et al. 2022; Lee et al.; Mu et al.), an ensemble of teacher models (Tunstall et al. 2023), or targeted context prompting techniques, which prompt to generate both positive and negative outputs for constructing comparisonbased preference rankings (Yang et al. 2023; Liu et al. 2024). However, these methods are not directly applicable to embodied contexts, such as robotic manipulation, due to the lack of high-quality, open-source, input-unified foundation models. Input-unified foundation models are particularly challenging in robotics because different embodied models often rely on incompatible input modalities or features, making it difficult to transfer feedback effectively across models. Unlike previous works that focus on scaling human feedback, our approach aims to effectively learn end-user rewards from limited amount of human feedback. Visual reward learning in robotics. Visual reward models aim to capture task preferences directly from image leverage task observations. Self-supervised approaches progress inherent in video demonstrations to learn how far the robot is from completing the task (Zakka et al. 2022; Kumar et al. 2023; Ma et al. 2023) while other approaches identify task segments and measure distance to these subgoals (Sermanet et al. 2016; Tanwani et al. 2020; Shao et al. 2020; Chen et al. 2021). However, these approaches fail to model preferences during task execution that go beyond progress (e.g., spatial regions to avoid during movement). Fundamental work in IRL uses feature matching between the expert and the learner in terms of the expected state visitation distribution to infer rewards (Abbeel and Ng 2004; Ziebart et al. 2008), and recent work in optimal transport (OT) has shown how to scale this matching to high dimensional state spaces (Xiao et al. 2019; Dadashi et al. 2021; Papagiannis and Li 2022; Luo et al. 2023). However, the key to making this matching work from highdimensional visual input spaces is good visual embedding. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycleconsistency (Dadashi et al. 2021). However, all these proxy tasks bypass the humans input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In 3 contrast to prior works that rely on using only self-supervised signals, we propose an OT-based visual reward that is trained purely on videos (no action labels needed) that are ranked by the end-users preferences. Representation alignment in robot learning. Representation alignment studies the agreement between the representations of two learning agents. As robots will ultimately operate in service of people, representation alignment is becoming increasingly important for robots to interpret the world in the same way as we do. Previous work has leveraged user feedback, such as human-driven feature selection (Bullard et al. 2018; Luu-Duc and Miura 2019), interactive feature construction (Bobu et al. 2021; Katz et al. 2021), or similarity-implicit representation learning (Bobu et al. 2023), to learn aligned representations for robot behavior learning. But they either operate on manually defined feature set or learning features in low-dimensional state space settings (e.g., positions). In the visual domain, (Zhang et al. 2020) uses per-image reward signal to align the image representation with the preferences encoded in the reward signal; however, when the main objective is learning the humans reward then assuming priori access to such reward signal is not feasible. Instead, our work utilizes human preference feedback to align the robots visual representations with the end user."
        },
        {
            "title": "3 Problem Setup",
            "content": "Human policy. We consider scenarios where the robot wants to learn how to perform task for human H. The human knows the desired reward which encodes their preferences for the task. The human acts via an : nAH optimized approximately optimal policy π under their underlying reward function. Here, denotes the probability simplex over the humans nAH -dimensional Instead of directly consuming the raw action space. perceptual input, research suggests that humans naturally build visual representations of the world (Bonnen et al. 2021) that focus on task-relevant attributes (Callaway et al. 2021). We model the humans representation model as ϕH : ZH, mapping from the perceptual input to the humans latent space ZH which captures their task and preferencerelevant features. Visumotor policy alignment. We denote the robots visumotor policy as πR : nAR where denotes the probability simplex over the robots nAR -dimensional action space. In the RLHF paradigm, we seek to fine-tune πR to maximize reward function: π = arg max πR Eop(oπR) (cid:104) (cid:88) t=0 (cid:105) γt r(ϕR(ot)) , (1) where γ [0, 1) is the discount factor and = {o0, o1, . . .} is the image observation trajectory induced by the robots policy. The robots reward also relies on visual representation, ϕR : ZR, which maps from the image observation to lower-dimensional latent space, ZR. In general, this could be hand-crafted, such as distances to objects from the agents end-effector (Ziebart et al. 2008; Levine et al. 2011; Finn et al. 2016), or the output of an encoder pre-trained on large-scale datasets 4 (Chen et al. 2021; Ma et al. 2023). The optimization problem in Equation 1 can be approximately solved through reinforcement learning algorithm such as PPO (Schulman et al. 2017), where the robot repeatedly interacts with simulated environment and receives reward feedback to improve its policy. When simulated environment is not accessible or real-world interaction is prohibitively expensive, direct preference optimization (DPO) (Rafailov et al. 2024) has been proposed as variant of RLHF that directly updates the generative model through contrastive learning using preference rankings drawn from reward (Liu et al. 2024): max πR (a+ ,a ,o)Dpref P[a+ Ro], P[a+ Ro] = log (cid:1) exp (cid:0)α log πR(a+ πref(a exp (cid:0)α log πR(aR,o) (cid:80) πref(aR,o) aR{a+ ,o) ,o) ,a } (2) (cid:1) , and where πref is the robots initial reference visumotor policy, Dpref is preference dataset. Each training example consists of (a+ R, R, o): an initial observation and pair of action sequences, a+ R, sampled from the reference policy given this initial observation. The plus and minus denote preferred and unpreferred behaviors under the humans Ro = (cid:80) ) > internal (cid:80) r(o , denotes the observations when rolling out a+ respectively. Before the robot can optimize for πR using either approach, it is faced with two questions: what visual representation ϕR should it use to encode the environment, and which reward should it optimize to align its behavior with π ), where o+ and reward such that a+ r(o+ H?"
        },
        {
            "title": "Problem for Robotics",
            "content": "We follow the formulation in Sucholutsky and Griffiths learning domain. (2023) and bring this to the robot Intuitively, visual representation alignment is defined as the degree to which the output of the robots encoder, ϕR, matches the humans internal representation, ϕH, for the same image observation, O, during task execution. We utilize triplet-based definition of representation alignment as in (Jamieson and Nowak 2011) and (Sucholutsky and Griffiths 2023). Let Definition 1. Triplet-based Representation Space. = {ot}T t=0 be sequence of image observations over timesteps, ϕ : be given representation model, and ϕ(o) := {ϕ(o0), . . . , ϕ(oT )} be the corresponding embedding trajectory. For some distance metric d(, ) and two observation trajectories oi and oj, let d(cid:0)ϕ(oi), ϕ(oj)(cid:1) be the distance between their embedding trajectories. The triplet-based representation space of ϕ is: (cid:110) Sϕ = (oi, oj, ok) : oji oki, oi,j,k Ξ (3) (cid:111) , Intuitively, this states that the visual representation ϕ helps the agent determine how similar two videos are in lowerdimensional space. For all possible triplets of videos that the agent could see, it can determine which videos are more similar and which videos are less similar using its embedding space. The set Sϕ contains all such similarity triplets. Definition 2. Visual Representation Alignment Problem. Recall that ϕH and ϕR are the human and robots visual representations respectively. The representation alignment problem is defined as learning ϕR which minimizes the difference between the two agents representation spaces, as measured by function ℓ which penalizes divergence between the two representation spaces: min ϕR ℓ(SϕR , SϕH ). (4)"
        },
        {
            "title": "4.2 Representation Alignment via",
            "content": "Preference-based Learning Although this formulation sheds light on the underlying problem, solving Equation 4 exactly is impossible since the functional form of the humans representation ϕH is unavailable and the set SϕH is infinite. Thus, we approximate the problem by constructing subset SϕH SϕH of triplet queries. Since we seek representation that is relevant to the humans preferences, we ask the human to rank these triplets based on their preference-based similarity (e.g., r(ϕH(oi)) > r(ϕH(oj)) > notion of r(ϕH(ok)) = oji oki). With these rankings, we implicitly learn ϕH via neural network trained on these triplets. We interpret humans preference over the triplet (oi, oj, ok) SϕH via the Bradley-Terry model (Bradley and Terry 1952), where oi is treated as an anchor and oj, ok are compared to the anchor in terms of preference similarity as in Equation 3: P(oji oki ϕH) ed(ϕH(oi), ϕH(oj )) ed(ϕH(oi), ϕH(oj )) + ed(ϕH(oi), ϕH(ok)) . (5) One remaining question is: what distance measure should we use to quantify the difference between two embedding trajectories? In this work, we use optimal transport as principled way to measure the feature matching between any two videos. For any video and for given representation ϕ, the induced empirical embedding distribution be ρ = 1 t=0 δϕ(ot), where δϕ(ot) is Dirac distribution centered on ϕ(ot). Optimal transport finds the optimal transport plan µ RT that transports one embedding distribution, ρi, to another video embedding distribution, ρj, with minimal cost. This comes down to an optimization problem that minimizes the Wasserstein distance between the two distributions: let (cid:80)T µ = arg min µM(ρi,ρj ) (cid:88) (cid:88) t=1 t=1 c(cid:0)ϕ(ot i), ϕ(ot )(cid:1)µt,t. (6) where Ξ is the set of all possible image trajectories for the task of interest, and oji oki denotes d(cid:0)ϕ(oi), ϕ(oj)(cid:1) < d(cid:0)ϕ(oi), ϕ(ok)(cid:1). where M(ρi, ρj) = {µ RT : µ1 = ρi, µT 1 = ρj} is the set of transport plan matrices, : Rne Rne is cost function defined in the embedding space (e.g., cosine distance), and ne is the dimension of the embedding space. Solving the above optimization in Equation 6 exactly is generally intractable for high dimensional distributions. In practice, we solve an entropy-regularized version of the problem following the Sinkhorn algorithm (Peyre et al. 2019) which is amenable to fast optimization: µ = arg min µM(ρi,ρj ) (cid:88) (cid:88) t=1 t=1 c(cid:0)ϕ(ot i), ϕ(ot j)(cid:1)µt,t ϵH(µ), (7) where denotes the entropy term that regularizes the optimization and ϵ is the associated weight. The optimal transport plan gives rise to the following distance that measures the feature matching between any two videos and is used in Equation 5: d(ϕ(oi), ϕ(oj)) = (cid:88) (cid:88) t=1 t=1 c(cid:0)ϕ(ot i), ϕ(ot )(cid:1)µ t,t. (8)"
        },
        {
            "title": "Our final optimization is a maximum likelihood estimation",
            "content": "problem: ϕH := max ϕH (cid:88) P(oji oki ϕH). (9) (oi,oj ,ok) SϕH Since the robot seeks visual representation that is aligned with the humans, we set: ϕR := ϕH. (10)"
        },
        {
            "title": "4.3 Preference-aligned Visual Reward Model",
            "content": "Given our aligned visual representation, we seek robot visual reward function that approximates the end-users reward function for aligning the robots visuomotor policy with human preference. Traditional IRL methods (Abbeel and Ng 2004; Ziebart et al. 2008) are built upon matching the feature distribution of the robots policy with that of the end-users demonstration. Specifically, we seek to match the observation distribution induced by the robots policy πR, and the observation distribution of humans preferred video demonstration, o+ in the aligbned representation space. The optimal transport plan between the two distributions precisely defines reward function that measures this matching (Kantorovich and Rubinshtein 1958) and yields the reward which is optimized in Equation 1: r(ot R; ϕR, o+) = (cid:88) t= c(cid:0)ϕR(ot R), ϕR(ot +)(cid:1)µ t,t. (11) This reward has been successful in prior vision-based robot learning (Haldar et al. 2023b,a; Guzey et al. 2023) with the key difference in our setting being that we use RAPLs aligned visual representation ϕR for feature matching."
        },
        {
            "title": "5.1 Experimental Design\nPreference dataset: ˜SϕH. While the ultimate test is learning\nfrom real end-user feedback, in this section, we first use a\nsimulated human model, which allows us to easily ablate\nthe size of the preference dataset, and gives us privileged\naccess to r∗ for direct comparison. In all environments, the\nsimulated human constructs the preference dataset ˜SϕH by\nsampling triplets of videos uniformly at random from the set*\nof video observations ˜Ξ ⊂ Ξ, and then ranking them with\ntheir reward r∗ as in Equation 5.\nIndependent & dependent measures. Throughout our\nexperiments, we vary the visual reward signal used for robot\npolicy optimization and the preference dataset size used for\nrepresentation learning. We measure robot task success as a\nbinary indicator of if the robot completed the task with high\nreward r∗.\nControlling for confounds. Our ultimate goal is to have\na visual robot policy, πR, that takes as input observations\nand outputs actions. However,\nto rigorously compare\npolicies obtained from different visual rewards, we need\nto disentangle the effect of the reward signal from any\nother policy design choices, such as the input encoders\nand architecture. To have a fair comparison, we follow\nthe approach from (Zakka et al. 2022; Kumar et al. 2023)\nand input the privileged ground-truth state into all policy\nnetworks, but vary the visual reward signal used during\npolicy optimization. Across all methods, we use an identical\nreinforcement learning setup and Soft-Actor Critic (SAC) for\ntraining (Haarnoja et al. 2018) with code base from (Zakka\net al. 2022). When running SAC, the reward (Equation 11)\nto an expert demonstration\nrequires matching the robot\nvideo. For all policy learning experiments, we use 10 expert\ndemonstrations as the demonstration set D+ for generating\nthe reward. To choose this expert observation, we follow\nthe approach from (Haldar et al. 2023a). During policy\noptimization, given a robot’s trajectory’s observation oR\ninduced by the robot policy πR, we select the the “closest”\nexpert demonstration o∗\n+ ∈ D+ to match the robot behavior\nwith. This demonstration selection happens via:",
            "content": "o +=arg min o+D+ min µM(ρR,ρ+) (cid:88) (cid:88) c(cid:0)ϕ(ot R), ϕ(ot +)(cid:1)µt,t. (12) t=1 t=1 In Section 5.2, we first control the agents embodiment to be consistent between both representation learning and robot optimization (e.g., assume that the robot shows video triplets of itself to the human and the human ranks them). In Section 5.3, we relax this assumption and consider the more realistic cross-embodiment scenario where the representation learning is performed on videos of different embodiment than the robots."
        },
        {
            "title": "5 Reinforcement Learning in Simulation",
            "content": "with RAPL We first experiment in the X-Magical environment (Zakka et al. 2022), and then in the realistic IsaacGym simulator. In this section, we design series of experiments to evaluate RAPLs effectiveness in learning visual rewards and aligning the robot visuomoto policy in simulations through reinforcement learning. To minimize the bias of this set, we construct Ξ such that the reward distribution of this set under is approximately uniform. Future work should investigate study this set design further, e.g., (Sadigh et al. 2017). 6 Figure 2. X-Magical & IsaacGym tasks. Top row are high-reward behaviors and bottom row are low-reward behaviors according to the humans preferences. 5.2.1 X-Magical Experiments Tasks. We design two tasks inspired by kitchen countertop cleaning. The robot always has to push objects to goal region (e.g., trash can), shown in pink at the top of the scene in Figure 2. In the avoiding task, the end-user prefers that the robot and objects never enter an off-limits zone during pushing (blue box in left Figure 2). In the grouping task, the end-user prefers that objects are pushed efficiently together (instead of one-at-a-time) to the goal (center, Figure 2). is: The reward Privileged state & reward. For avoiding, the true state is 7D: planar robot position (pR R2) and orientation (θR), planar position of the object (pobj R2), distance between goal region and object, (dobj2goal), and distance between the off-limits zone and the object (dobs2obj). The humans reward avoid(s) = dgoal2obj 2 I(dobs2obj < dsafety), where is: dsafety is safety distance and is an indicator function giving 1 when the condition is true. For grouping, the state is 9D: := (pR, θR, pobj1, pobj2, dgoal2obj1 , group(s) = dgoal2obj2). humans max(dgoal2obj1, dgoal2obj2 ) pobj1 pobj22. Baselines. We compare our visual reward, RAPL, against (1) GT, an oracle policy obtained under r, (2) RLHF, which is vanilla preference-based reward learning (Christiano et al. 2017; Brown et al. 2019) that directly maps an image observation to scalar reward, and (3) TCC (Zakka et al. 2022; Kumar et al. 2023) which finetunes pre-trained encoder via temporal cycle consistency constraints using 500 task demonstrations and then uses L2 distance between the current image embedding and the goal image embedding as reward. We use the same preference dataset with 150 triplets for training RLHF and RAPL. Visual model backbone. We use the same setup as in (Zakka et al. 2022) with the ResNet-18 visual backbone (He et al. 2016) pre-trained on ImageNet. The original classification head is replaced with linear layer that outputs 32-dimensional vector as our embedding space, ZR := R32. The TCC representation model is trained with 500 demonstrations using the code from (Zakka et al. 2022). Both RAPL and RLHF only fine-tune the last linear layer. All representation models are frozen during policy learning. Hypothesis. RAPL is better at capturing preferences beyond task progress compared to direct reward prediction RLHF or TCC visual reward, yielding higher success rate. Results. Figure 3 shows the rewards over time for three example video observations in the avoid (left) and group task (right). Each video is marked as preferred by the endusers ground-truth reward or disliked. Across all examples, RAPLs rewards are highly correlated with the GT rewards: when the behavior in the video is disliked, then reward is low; when the behavior is preferred, then the reward is increasing. TCCs reward is correlated with the robot (E) and (F) where making spatial progress (i.e., plot observations get closer to looking like the goal image), but it incorrectly predicts high reward when the robot makes spatial progress but violates the humans preference ((C) and (D) in Figure 3). RLHF performs comparably to RAPL, with slight suboptimality in scenarios (C) and (D). Figure 4 shows the policy evaluation success rate during RL training with each reward function (solid line is the mean, shaded area is the standard deviation, over 5 trials with different random seeds.). Across all environments, RAPL performs comparably to GT (avoid success: 80%, group success: 60%) and significantly outperforms all baselines with better sample efficiency, RAPL takes 10 epochs to reach 70% success rate in the avoid task (GT requires 100) and takes 100 epochs to reach 40% success rate in the avoid task (GT requires 150), supporting our hypothesis. 5.2.2 Robot Manipulation Experiments In the X-Magical toy environment, RAPL outperformed progress-based visual rewards, but direct preference-based reward prediction was competitive baseline. Moving to the more realistic robot manipulation environment, we want to 1) disentangle the benefit of our fine-tuned representation from the optimal transport reward structure, and 2) understand if our method still outperforms direct reward prediction in more complex environment? Task. We design robot manipulation task in the IsaacGym physics simulator (Makoviychuk et al. 2021). We replicate the tabletop grouping scenario, where Franka robot arm needs to learn that the end-user prefers objects be pushed efficiently together (instead of one-at-a-time) to the goal region (light blue region in the middle of Figure 2). In addition, we consider more complex robot manipulation tasknamed clutter to further validate RAPLs ability to disentangle visual features that underlie an end-users preferences. We increase the difficulty of the the grouping environment by adding visual distractors that are irrelevant to the humans preferences (left figure in Figure 2). The environment has multiple objects on the table of various colorsred, green, and goal-region-blueand some of the objects are cubes while others are rectangular prisms. The Franka robot arm needs to learn that the end-user prefers to push the rectangular objects (instead of the cubes) efficiently together (instead of one-at-a-time) to the goal region. Privileged state & reward. For grouping, the state is 18D: robot proprioception (θjoints R10), 3D object positions (pobj1,2 ), and object distances to goal (dgoal2obj1,2 ). The grouping reward is identical as in Section 5.2.1. For clutter, the state is 34D: robot proprioception (θjoints R10), 3D object positions (pobj1,2 , pobj1,...,4 ), and object distances to goal (dgoal2obj1,2 ). The simulated humans , dgoal2obj1,..., cube rect rect cube 7 Figure 3. X-Magical. (left & right) examples of preferred and disliked videos for each task. (center) reward associated with each video under each method. RAPLs predicted reward follows the GT pattern: low reward when the behavior are disliked and high reward when the behavior are preferred. RLHF and TCC assign high reward to disliked behavior (e.g., (D)). (6): ImageNet-OT, which is ResNet-18 encoder pretrained on ImageNet; (7) TCC-OT (Dadashi et al. 2021) which embeds images via the TCC representation trained with 500 task demonstrations. We use the same preference dataset with 150 triplets for training RLHF and RAPL. Visual model backbone. All methods except MVP-OT and Fine-Tuned-MVP-OT share the same ResNet-18 visual backbone and have the same training setting as the one in the X-Magical experiment. MVP-OT and Fine-Tuned-MVPOT use an off-the-shelf visual transformer (Xiao et al. 2022) pre-trained on the Ego4D data set (Grauman et al. 2022). All representation models are frozen during policy learning. Hypotheses. H1: RAPLs higher policy success rate are driven by its aligned visual representation. H2: RAPL outperforms RLHF with less human preference data. Results: Reward Prediction. In the center of Figure 5 we show three video demos in the grouping task: an expert video demonstration, preferred video, and disliked video. On the right of Figure 5, we visualize the optimal transport plan comparing the expert video to the disliked and preferred videos under three represenative visual representations, ϕRAPL, ϕTCC-OT, ϕMVP-OT. Intuitively, peaks exactly along the diagonal indicate that the frames of the two videos are aligned in the latent space; uniform values in the matrix indicate that the two videos cannot be aligned (i.e., all frames are equally similar to the next). RAPLs representation induces precisely this structure: diagonal peaks when comparing two preferred videos and uniform when comparing preferred and disliked video. Interestingly, we see diffused peak regions in all transport plans under both TCC-OT and MVP-OT representations, indicating their representations struggle to align preferred behaviors and disentangle disliked behaviors. This is substantiated by the left of Figure 5, which shows the learned reward over time of preferred video and disliked video. Across all examples, RAPL rewards are highly correlated to GT rewards while baselines struggle to disambiguate. We further conducted quantitative analysis to investigate the relationship between the learned visual reward and the end-users ground-truth reward. For the robot manipulation Figure 4. X-Magical. Policy evaluation success rate during policy learning. Colored lines are the mean and variance of the evaluation success rate. RAPL can match GT in the avoiding task and outperforms baseline visual rewards in grouping task. reward is: group(s) = max(dgoal2objrect,1, dgoal2objrect,2) pobj1 rect pobj2 rect 2 0.1 4 (cid:88) i=1 pobji cube pobji,init cube 2. Baselines. In addition to comparing RAPL against (1) GT and (2) RLHF, we ablate the representation model but control the visual reward structure. We consider five additional baselines that all use optimal transport-based reward but operate on different representations: (3) MVPOT which learns image representation via masked visual pretraining; (4) Fine-Tuned-MVP-OT, which fine-tunes MVP representation using images from the task environment; (5) R3M-OT, which is an off-the-shelf ResNet-18 encoder (Nair et al. 2022) pre-trained on the Ego4D data set (Grauman et al. 2022) via learning objective that combines time contrastive learning, video-language alignment, and sparsity penalty; 8 Figure 5. Manipulation: Reward Prediction. (center) Expert, preferred, and disliked video demo. (left) Reward of each video under each method. RAPLs predicted reward follows the GT pattern. RLHF assigns high reward to disliked behavior. (right) OT coupling matrix for each visual representation. Columns are embedded frames of expert demo. Rows of top matrices are embedded frames of preferred demo; rows of bottom matrices are embedded frames of disliked demo. Peaks exactly along the diagonal indicate that the frames of the two videos are aligned in the latent space; uniform values in the matrix indicate that the two videos cannot be aligned (i.e., all frames are equally similar to the next). RAPL exhibits the diagonal peaks for expert-and-preferred and uniform for expert-and-disliked, while baselines show diffused values no matter the videos being compared. Figure 6. (Left) Manipulation: Qualitative RLHF Comparison. We visualize the attention map for RLHF-150 demos, RLHF-300 demos, and RAPL with 150 demos for both Franka and Kuka (cross-embodiment). Each entry shows two observations from respective demonstration set with the attention map overlaid. Bright yellow areas indicate image patches that contribute most to the final embedding; darker purple patches indicate less contribution. ϕRLHF 150 is biased towards paying attention to irrelevant areas that can induce spurious correlations; in contrast RAPL learns to focus on the task-relevant objects and the goal region. ϕRLHF 300s attention is slightly shifted to objects but still pays high attention to the robot embodiment. (Right) Manipulation: Quantitative RLHF Comparison. RAPL outperforms RLHF by 75% with 50% less human preference data. tasks Franka Group and Franka Clutter, we computed the average Spearmans correlation coefficient between the ground-truth reward trajectory and any other approachs reward trajectory across 100 video trajectories (showed in the left two columns of Table 1). We found that RAPLs learned visual reward shows the strongest correlation to the GT reward compared to baselines. Results: Policy Learning. Figure 7 shows the policy evaluation history during RL training with each reward function. Across all the manipulation environments, we see RAPL performs comparably to GT (succ. rate: 70%) while all baselines struggle to achieve success rate of more than 10% with the same number of epochs, supporting H1. Results: Sample Complexity of RAPL vs. RLHF. Its surprising that RLHF fails in more realistic environment since its objective is similar to ours, but without explicitly considering representation alignment. To further investigate Spearmans Correlation Franka Group Franka Clutter Kuka Group RAPL RLHF MVP-OT FT-MVP-OT ImNet-OT R3M-OT 0. 0.38 -0.1 0.19 -0.09 0.03 0. 0.26 0.08 0.11 -0.02 -0.17 0. 0.31 0.02 0.02 0.12 -0.14 Table 1. Spearmans rank correlation coefficient between the GT reward and each learned visual reward. this, we apply linear probe on the final embedding and visualize the image heatmap of what RAPLs (our representation model trained with 150 training samples), RLHF-150s (RLHF trained with 150 samples), and RLHF300s (RLHF trained with 300 samples samples) final 9 with the GT rewards even when deployed on crossembodiment robot. In the right-most column of Table 1, we show the average Spearmans correlation coefficient between the ground-truth reward trajectory and any other approachs reward trajectory across 100 videos in the crossembodiment manipulation environment (grouping task). We found that RAPLs learned visual reward shows the strongest correlation to the GT reward compared to baselines. In Figure 10, we show the policy evaluation histories during RL training with each reward function in the crossembodiment X-Magical environment and the manipulation environment. We see that in all cross-embodiment scenarios, RAPL achieves comparable success rate compared to GT and significantly outperforms baselines which struggle to achieve more than zero success rate, supporting our hypothesis. In the bottom row of Figure 6, we visualize the attention maps of RAPL and RLHF. We see that ϕRAP learns to focus on the objects, the contact region, and the goal region while paying less attention to the robot arm compared to RLHF even when the encoder is learned from Franka robot and deployed on Kuka robot. Furthermore, we note an interesting finding in the XMagical grouping task when the representation is trained on videos of the short stick agent, but the learning agent is the medium stick agent (Figure 9). Because the short stick agent is so small, it has harder time keeping the objects grouped together; in-domain results from Section 5.2 show success rate of 60% (see Figure 4). In theory, with well-specified reward, the task success rate should increase when the medium stick agent does the task, since it is better suited to push objects together. Interestingly, when the short stick visual representation is transferred zero-shot to the medium stick, we see precisely this: RAPLs task success rate improves by 20% under cross-embodiment transfer (succ. rate 80% as shown in the middle plot of (Figure 10). This indicates that RAPL can learn task-relevant features that can guide correct task execution even on new embodiment."
        },
        {
            "title": "World with RAPL",
            "content": "tasks In the previous section, we demonstrated how RAPLs reward can align robot policies via reinforcement learning in simulation. However, high-fidelity simulators are often impractical (e.g., for many real-world robotics deformable objects) and reinforcement learning in the real world is still an open research problem. Motivated by this, we turn to an algorithmic variant of RLHF, Direct Preference Optimization (DPO) described in Section 3, for aligning visuomotor policies without simulator. DPO updates the policy via contrastive learning using preference rankings on the behavior generations; however, to reliably update the policy it requires significant amount of preference labels. In this section, we demonstrate how our RAPL reward enables the scalable generation of synthetic preference rankings, significantly minimizing the number of real human labels while still achieving high policy alignment."
        },
        {
            "title": "6.1 Experiment Design\nTasks. We consider three real-world robot manipulation\ntasks. In the picking up cup task, the end-user prefers",
            "content": "Figure 7. Manipulation: Policy Learning. Success rate during robot policy learning under each visual reward. embedding pays attention to in the top row of Figure 6 (left). We see that ϕRAP learns to focus on the objects, the contact region, and the goal region while paying less attention to the robot arm; ϕRLHF 150 is biased towards paying attention to irrelevant areas that can induce spurious correlations (such as the robot arm and background area); ϕRLHF 300s attention is slightly shifted to objects while still pays high attention to the robot embodiment. When deploying ϕRLHF 300 in Franka manipulation policy learning (Figure 6 (right)), we observe that policy performance is slightly improved, indicating that with more feedback data, preferencebased reward prediction could yield an aligned policy. Nevertheless, RAPL outperforms RLHF by 75% with 50% less training data, supporting H2. While all the RAPL results above used 150 preference queries to train the representation, we also train visual representation with 100, 50, and 25 preference queries. We measure the success rate of the robot manipulation policy trained for each ablation of RAPL. We find that RAPL-150 achieves 70% success rate, RAPL-100 achieves 65%, RAPL-50 achieves 58%, and RAPL-25 achieves 45% policy success rate despite using only 17% of the original preference dataset."
        },
        {
            "title": "Robot Embodiments",
            "content": "So far, the preference feedback used to align the visual representation was given on videos Ξ generated on the same embodiment as that of the robot. However, in reality, the human could give preference feedback on videos of different embodiment than the specific robots. We investigate if our approach can generalize to changes in the embodiment between the preference dataset SH and the robot policy optimization. Tasks & baselines. We use the same setup for each environment as in Section 5.2. Cross-domain agents. In X-Magical, reward functions are always trained on the short stick agent, but the learning agent is gripper in avoid and medium stick agent in grouping task (illustrated in Figure 9). In robot manipulation we train RAPL and RLHF on videos of the Franka robot, but deploy the rewards on the Kuka robot (illustrated in Figure 8). Hypothesis. RAPL enables zero-shot cross-embodiment transfer of the visual reward compared to other baselines. Results. Figure 9 and Figure 8 show the learned rewards over time for the three cross-embodiment video observations (marked as preferred by the end-users ground-truth reward or disliked) in the cross-embodiment X-Magical environment and the manipulation environment. Across all examples, RAPL rewards are highly correlated 10 Figure 8. Manipulation: Cross-Embodiment Reward Transfer. (center) Expert video of Franka robot, preferred video of Kuka, and disliked Kuka video. (left) Predicted reward under each method trained only on Franka video preferences. RAPLs reward generalizes to the Kuka robot and follows the GT pattern. (right) OT plan for each visual representation shown in the same style as in Figure 5. RAPLs representation shows diagonal OT plan for expert-and-preferred demos vs. uniform for expert-and-disliked, while baselines show inconsistent plan patterns. Figure 9. X-Magical: Cross-Embodiment Reward Transfer. RAPL discriminates preferred and disliked videos of novel robots. Figure 10. Cross-Embodiment Policy Learning. Policy evaluation success rate during policy learning. Colored lines are the mean and variance of the evaluation success rate. RAPL achieves comparable success rate compared to GT with high learning efficiency, and outperforms all baselines. that the robot gripper utilizes the cups handle to pick it up, avoiding contact with the interior of the cup to prevent contamination of the water inside. In the picking up chips task, the end-user prefers the robot gripper to hold the packaging by its edges rather than squeezing the middle, which may crush the chips. In the placing fork task, the enduser prefers the robot gripper to pick the fork by the handle and gently place the fork into the bowl rather than picking by the tines or dropping it from an inappropriate height, which would cause the sanitation issue or forceful fall into the bowl. Robot visuomotor policy. We use Diffusion Policy (Chi et al. 2024) as our robot visuomotor policy. The model takes the current and previous image observations from the wrist and third-person cameras as inputs to predict the distribution of Franka robots future motions. The image 11 Figure 11. Diffusion Policy Alignment Results. (Top) The pre-trained visuomotor policy exhibits undesired behaviors: grasping the interior of the cup (left), crushing the chips (middle), and making contact with the tines of the fork and dropping it out of the bowl (right). (Bottom) After alignment using RAPL rewards, the robots behaviors are aligned with the end-users preferences. observations are encoded using ResNet-19 visual encoder trained together with the policy network. Initial policy training & preference dataset construction. For each task, we collect 100 demonstrations covering multiple behavior modes to train the initial reference robot policy, which we denote as πref. After training, we rollout πref under different initializations of the task by randomizing the task configurations (e.g., object states, robot states, etc.) to collect deployment video demonstrations. We query the enduser for 20 preference rankings ( SϕH = 20) among the deployment video demonstrations to train the RAPL reward. We use our visual reward to automatically construct an order of magnitude more synthetic preference rankings, 200, for aligning the reference policy following Equation 2. Independent & dependent measures. Throughout our experiments, we vary the visual reward signal used to construct preference rankings for policy alignment and the size of the preference dataset for representation learning. To evaluate the robots preference alignment in task, we assess its behavior across 15 random task configurations. For each configuration, the robot samples 100 motion plans from its diffusion policy, aggregates them into 4 modes using the non-maximum suppression (NMS) scheme from Seff et al. (2023), and executes the most likely mode. The end-user then grades the robots behavior as good or bad, and we measure the frequency of which the executed mode aligns with the end-users preference. Hypothesis. RAPL generates accurate synthetic preference rankings from few real human preferences and enables better policy alignment compared to baselines. Baselines. Similar to the evaluations in Section 5.2, we consider the following visual reward baselines for constructing the preference rankings: (1) GT (we manually label 100 preference rankings); (2) RLHF (trained on the same 20 preference rankings that RAPL is trained on); (3) RLHF-L (RLHF but with 3x larger preference dataset of 60 rankings); (4) TCC-OT; (5) R3M-OT; and (6) MVP-OT."
        },
        {
            "title": "6.2 Results\nResults: Qualitative. Figure 11 visualizes robot behaviors\nunder both the initial visuomotor policy and the policy\naligned using synthetic preference rankings constructed by\nRAPL across three manipulation tasks. In the top row, we\nobserve that the reference policy, while completing the tasks,",
            "content": "exhibits undesired behaviors: contacting the interior of the cup (left), crushing the chips (middle), and making contact with the tines of the fork, resulting in the fork being dropped out of the bowl (right). After aligning the policy with the RAPL reward, the robot exhibits behaviors that align with the end-users preferences: it picks up the cup by the handle, grasps the chips by the corner of the bag, and holds the handle of the fork when placing it in the bowl. Results: Quantitative. We present the alignment scores of the reference and each fine-tuned policy in Table 2. The results show that robot policies aligned using preference rankings constructed by RAPL achieve significantly higher alignment scores compared to all baselines across the three tasks and demonstrate performance comparable to GT but with 5x less human annotations. Notably, preference rankings labeled by the vanilla RLHF show minimal improvement and, in some cases, degrade the alignment score of the initial reference policy. This suggests that RLHF fails to learn accurate rewards capable of distinguishing between preferred and non-preferred behaviors. While RLHF-L does improve the reference policys alignment, it uses 3x more preference rankings than RAPL to learn the visual reward, supporting our hypothesis that RAPL minimizes human feedback while maximizing alignment. Behavior Alignment Score () Ref. GT RAPL RLHF RLHF-L TCC-OT R3M-OT MVP-OT Cup Fork Bag 0.2 0.2 0.4 0.7 0. 0.6 0.8 0.5 0.7 0.3 0. 0.4 0.7 0.4 0.4 0.4 0. 0.6 0.4 0.0 0.1 0.5 0. 0.4 Table 2. Hardware Experiments: Behavior Alignment Score. Robot visuomotor policies aligned using RAPL outperform all baselines and demonstrate performance comparable to GT but with 5x less human annotations."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we presented Representation-Aligned observationPreference-based Learning only, human data-efficient method for aligning visuomotor policies. Unlike traditional RLHF, RAPL focuses human preference feedback on fine-tuning pre-trained vision encoders to align with the end-users visual representation (RAPL), an 12 and then constructs dense visual reward via feature matching in this aligned representation space. We validated the effectiveness of RAPL through extensive experiments in both simulated and real-world settings. In simulation, we showed that RAPL can learn high-quality rewards with half the amount of human data traditional RLHF approaches use, and the learned rewards can generalize to new robot embodiments. In hardware experiments, we demonstrated that RAPL can successfully align pre-trained Diffusion Policies for three object manipulation tasks while needing 5x less human preference data than prior methods. We hope that our first steps with RAPL will bolster more research on how to align next-generation visuomotor policies with end-user needs while reducing human labeling burden. References Abbeel and Ng AY (2004) Apprenticeship learning via inverse In: Proceedings of the twenty-first reinforcement learning. international conference on Machine learning. p. 1. Bai Y, Kadavath S, Kundu S, Askell A, Kernion J, Jones A, Chen A, Goldie A, Mirhoseini A, McKinnon et al. (2022) arXiv Constitutional ai: Harmlessness from ai feedback. preprint arXiv:2212.08073 . Bobu A, Liu Y, Shah R, Brown DS and Dragan AD (2023) Sirl: Similarity-based implicit representation learning. International Conference on Human Robot Interaction . Bobu A, Wiggert M, Tomlin and Dragan AD (2021) Feature expansive reward learning: Rethinking human input. In: Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction. pp. 216224. Bonnen T, Yamins DL and Wagner AD (2021) When the ventral visual stream is not enough: deep learning account of medial temporal lobe involvement in perception. Neuron 109(17): 27552766. Bradley RA and Terry ME (1952) Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika 39(3/4): 324345. Brohan A, Brown N, Carbajal J, Chebotar Y, Chen X, Choromanski K, Ding T, Driess D, Dubey A, Finn et al. (2023) Rt2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818 . Brown D, Goo W, Nagarajan and Niekum (2019) Extrapolating beyond suboptimal demonstrations via inverse reinforcement In: International conference on learning from observations. machine learning. PMLR, pp. 783792. Bullard K, Chernova and Thomaz AL (2018) Human-driven feature selection for robotic agent learning classification tasks from demonstration. In: 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, pp. 69236930. Callaway F, Rangel and Griffiths TL (2021) Fixation patterns in simple choice reflect optimal information sampling. PLoS computational biology 17(3): e1008863. Chen AS, Nair and Finn (2021) Learning generalizable robotic reward functions from in-the-wild human videos. Robotics: Science and Systems . Chi C, Xu Z, Feng S, Cousineau E, Du Y, Burchfiel B, Tedrake and Song (2024) Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research . Christiano PF, Leike J, Brown T, Martic M, Legg and Amodei (2017) Deep reinforcement learning from human preferences. Advances in neural information processing systems 30. Dadashi R, Hussenot L, Geist and Pietquin (2021) Primal International Conference on wasserstein imitation learning. Robot Learning . Deng J, Dong W, Socher R, Li LJ, Li and Fei-Fei (2009) Imagenet: large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee, pp. 248255. Finn C, Levine and Abbeel (2016) Guided cost learning: Deep inverse optimal control via policy optimization. In: International conference on machine learning. PMLR, pp. 49 58. Grauman K, Westbury A, Byrne E, Chavis Z, Furnari A, Girdhar R, Hamburger J, Jiang H, Liu M, Liu et al. (2022) Ego4d: Around the world in 3,000 hours of egocentric video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1899519012. Guzey I, Dai Y, Evans B, Chintala and Pinto (2023) See to touch: Learning tactile dexterity through visual incentives. arXiv preprint arXiv:2309.12300 . Haarnoja T, Zhou A, Abbeel and Levine (2018) Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with In: International conference on machine stochastic actor. learning. PMLR, pp. 18611870. Haldar S, Mathur V, Yarats and Pinto (2023a) Watch and match: Supercharging imitation with regularized optimal transport. In: Conference on Robot Learning. PMLR, pp. 3243. Haldar S, Pari J, Rai and Pinto (2023b) Teach robot to fish: Versatile imitation from one minute of demonstrations. Robotics: Science and Systems . He K, Zhang X, Ren and Sun (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770778. Hu Y, Yang J, Chen L, Li K, Sima C, Zhu X, Chai S, Du S, Lin T, Wang et al. (2023) Planning-oriented autonomous driving. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1785317862. Jamieson KG and Nowak RD (2011) Low-dimensional embedding In: 2011 49th using adaptively selected ordinal data. Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, pp. 10771084. Kantorovich LV and Rubinshtein (1958) On space of totally additive functions. Vestnik of the St. Petersburg University: Mathematics 13(7): 5259. Katz SM, Maleki A, Bıyık and Kochenderfer MJ (2021) Preference-based learning of reward function features. arXiv preprint arXiv:2103.02727 . Kumar S, Zamora J, Hansen N, Jangir and Wang (2023) Graph inverse reinforcement learning from diverse videos. In: Conference on Robot Learning. PMLR, pp. 5566. Lee H, Phatale S, Mansoor H, Mesnard T, Ferret J, Lu KR, Bishop C, Hall E, Carbune V, Rastogi et al. (????) Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In: Forty-first International Conference on Machine Learning. Lee K, Liu H, Ryu M, Watkins O, Du Y, Boutilier C, Abbeel P, Ghavamzadeh and Gu SS (2023) Aligning text-toarXiv preprint image models using human feedback. arXiv:2302.12192 . Levine S, Popovic and Koltun (2011) Nonlinear learning with gaussian processes. J, Zemel R, Bartlett P, Pereira in Neural and Weinberger (eds.) Advances 24. Curran volume URL https://proceedings. inverse reinforcement In: Shawe-Taylor Information Processing Associates, neurips.cc/paper_files/paper/2011/file/ c51ce410c124a10e0db5e4b97fc2af39-Paper. pdf. Systems, Inc. Liu A, Bai H, Lu Z, Kong X, Wang S, Shan J, Cao and Wen (2024) Direct large language model alignment through self-rewarding contrastive prompt distillation. arXiv preprint arXiv:2402.11907 . Lu Y, Fu J, Tucker G, Pan X, Bronstein E, Roelofs R, Sapp B, White B, Faust A, Whiteson et al. (2023) Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios. In: 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, pp. 75537560. Luo Y, zhengyao jiang, Cohen S, Grefenstette and Deisenroth MP (2023) Optimal transport for offline imitation learning. In: The Eleventh International Conference on Learning URL https://openreview.net/ Representations. forum?id=MhuFzFsrfvH. Luu-Duc and Miura (2019) An incremental feature set in programming by demonstration scenario. refinement In: 2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM). IEEE, pp. 372377. Ma YJ, Sodhani S, Jayaraman D, Bastani O, Kumar and Zhang (2023) Vip: Towards universal visual reward and representation International Conference on via value-implicit pre-training. Learning Representations . Makoviychuk V, Wawrzyniak L, Guo Y, Lu M, Storey K, Macklin M, Hoeller D, Rudin N, Allshire A, Handa et al. (2021) Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470 . Mu T, Helyar A, Heidecke J, Achiam J, Vallone A, Kivlichan I, Lin M, Beutel A, Schulman and Weng (????) Rule based rewards for language model safety . Nair S, Rajeswaran A, Kumar V, Finn and Gupta (2022) R3m: universal visual representation for robot manipulation. Conference on Robot Learning . Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, Zhang C, Agarwal S, Slama K, Ray et al. (2022) Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35: 2773027744. Padalkar A, Pooley A, Jain A, Bewley A, Herzog A, Irpan A, Khazatsky A, Rai A, Singh A, Brohan et al. (2023) Open xembodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864 . Papagiannis and Li (2022) Imitation learning with sinkhorn distances. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pp. 116 131. Peyre G, Cuturi et al. (2019) Computational optimal transport: With applications to data science. Foundations and Trends in Machine Learning 11(5-6): 355607. 13 Pomerleau DA (1988) Alvinn: An autonomous land vehicle in neural network. Advances in neural information processing systems 1. Radosavovic I, Xiao T, James S, Abbeel P, Malik and Darrell (2023) Real-world robot learning with masked visual pretraining. In: Conference on Robot Learning. PMLR, pp. 416 426. Rafailov R, Sharma A, Mitchell E, Manning CD, Ermon and Finn (2024) Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems 36. Sadigh D, Dragan AD, Sastry and Seshia SA (2017) Active Robotics: preference-based learning of reward functions. Science and Systems . Schulman J, Wolski F, Dhariwal P, Radford and Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 . Seff A, Cera B, Chen D, Ng M, Zhou A, Nayakanti N, Refaat KS, Al-Rfou and Sapp (2023) Motionlm: Multi-agent motion forecasting as language modeling. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 85798590. Sermanet P, Xu and Levine (2016) Unsupervised perceptual rewards for imitation learning. Robotics: Science and Systems . Shao L, Migimatsu T, Zhang Q, Yang and Bohg (2020) Concept2Robot: Learning manipulation concepts from In: Proceedings of instructions and human demonstrations. Robotics: Science and Systems (RSS). Sucholutsky and Griffiths TL (2023) Alignment with human arXiv representations supports robust few-shot preprint arXiv:2301.11990 . learning. Sun W, Vemula A, Boots and Bagnell (2019) Provably efficient In: International imitation learning from observation alone. conference on machine learning. PMLR, pp. 60366045. Swamy G, Choudhury S, Bagnell JA and Wu (2021) Of moments and matching: game-theoretic framework for closing the In: International Conference on Machine imitation gap. Learning. PMLR, pp. 1002210032. Tanwani AK, Sermanet P, Yan A, Anand R, Phielipp and Goldberg (2020) Motion2vec: Semi-supervised representation In: 2020 IEEE International learning from surgical videos. Conference on Robotics and Automation (ICRA). IEEE, pp. 21742181. Tian R, Xu C, Tomizuka M, Malik and Bajcsy (2024a) What matters to you? towards visual representation alignment International Conference on Learning for robot learning. Representations . Tian T, Li B, Weng X, Chen Y, Schmerling E, Wang Y, Ivanovic and Pavone (2024b) Tokenize the world into object-level knowledge to address long-tail events in autonomous driving. In: 8th Annual Conference on Robot Learning. Tunstall L, Beeching E, Lambert N, Rajani N, Rasul K, Belkada Y, Huang S, von Werra L, Fourrier C, Habib et al. (2023) Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944 . Xiao H, Herman M, Wagner J, Ziesche S, Etesami and Linh TH (2019) Wasserstein adversarial imitation learning. arXiv preprint arXiv:1906.08113 . 14 Xiao T, Radosavovic I, Darrell and Malik (2022) Masked arXiv preprint visual pre-training for motor control. arXiv:2203.06173 . Yang K, Klein D, Celikyilmaz A, Peng and Tian (2023) Rlcd: Reinforcement learning from contrast distillation for language model alignment. arXiv preprint arXiv:2307.12950 . Zakka K, Zeng A, Florence P, Tompson J, Bohg and Dwibedi (2022) Xirl: Cross-embodiment inverse reinforcement learning. In: Conference on Robot Learning. PMLR, pp. 537 546. Zhang A, McAllister R, Calandra R, Gal and Levine (2020) Learning invariant representations for reinforcement learning without reconstruction. International Conference on Learning Representations . Ziebart BD, Maas AL, Bagnell JA, Dey AK et al. (2008) Maximum In: Aaai, volume 8. entropy inverse reinforcement learning. Chicago, IL, USA, pp. 14331438."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "UC Berkeley"
    ]
}