{
    "paper_title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model",
    "authors": [
        "Zhaochong An",
        "Guolei Sun",
        "Yun Liu",
        "Runjia Li",
        "Junlin Han",
        "Ender Konukoglu",
        "Serge Belongie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at https://github.com/ZhaochongAn/GFS-VL"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 2 8 2 6 1 . 3 0 5 2 : r Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model Zhaochong An1,2, Guolei Sun2, Yun Liu3*, Runjia Li4, Junlin Han4, Ender Konukoglu2, Serge Belongie1 1 Department of Computer Science, University of Copenhagen 2 Computer Vision Laboratory, ETH Zurich 3 College of Computer Science, Nankai University 4 Department of Engineering Science, University of Oxford"
        },
        {
            "title": "Abstract",
            "content": "Generalized few-shot 3D point cloud segmentation (GFSPCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D visionlanguage models (3D VLMs), generalizing across openworld novel classes, contain rich but noisy novel class knowledge. In this work, we introduce GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide solid foundation for advancing GFS-PCS in the real world. The code is at here. 1. Introduction Understanding dense 3D semantics is essential for many vision applications [20, 35, 42, 52, 53, 61, 73], and few-shot point cloud semantic segmentation (FS-PCS) has emerged *Corresponding authors: Guolei Sun and Yun Liu Figure 1. Comparison of our framework with previous work. Top: Prior work [56, 66] primarily enhances prototypes through interaction modules that integrate support/query features, making predictions based on refined prototypes. However, they are limited by the sparse knowledge from few-shot samples. Bottom: Our framework addresses this limitation by leveraging the extensive open-world knowledge from 3D VLMs through pseudo-labels. We mitigate the noise inherent in 3D VLMs by calibrating their raw pseudo-labels with precise few-shot samples, thereby effectively expanding novel class knowledge while ensuring reliability. as valuable task [2, 77], enabling models to extend from base to novel classes with minimal annotations for novel classes. However, typical few-shot models require additional support samples for each novel class at inference and only predict novel classes, ignoring base classes. To address this, generalized few-shot point cloud semantic segmentation (GFS-PCS) [66] was introduced, allowing models to directly segment both base and novel classes after few-shot adaptation, making it more practical for real-world use. 1 Current GFS-PCS models [17, 33, 34, 66] primarily utilize prototype learning [51], representing each class as prototype and predicting based on the relationship between query points and these prototypes. As shown in Fig. 1, these methods mainly focus on refining prototypes through interactions with support/query samples for enhanced segmentation. For instance, CAPL [56] adapts base prototypes to novel classes using co-occurrence knowledge from support samples and contextual information from queries. GW [66] encodes shared geometric structures into geometry prototypes to enhance semantic prototypes. However, these approaches remain limited in novel class generalization due to the sparse knowledge available from few-shot samples. In parallel, 3D vision-language models (3D VLMs) have been developed to enable open-vocabulary recognition by aligning 3D and language features. Leveraging language models trained on vast open-text data, 3D VLMs exhibit strong generalization abilities, allowing recognition of open-set classes in 3D. Since paired 3D-text data is scarce, some approaches [5, 18, 44, 54, 75] distill 2D features from 2D VLMs [12, 27] into their 3D encoders, while others [9, 10, 21, 67] use captioning models [47, 60] to generate sceneor region-level descriptions, enabling point-language alignment for direct 3D learning. Recognizing the openworld potential of 3D VLMs, we propose utilizing their rich knowledge to enhance GFS-PCS. straightforward method to integrate 3D VLMs [44, 67] is to generate dense pseudolabels of novel classes as additional supervision. However, predictions from 3D VLMs are often noisy, compounding errors in GFS-PCS models. Meanwhile, sparse support samples offer accurate annotations for novel classes. Therefore, given the dense but noisy pseudo-labels from 3D VLMs and the accurate yet limited support samples, we propose new GFS-PCS framework, named GFS-VL, to combine the strengths of both, as in Fig. 1. Specifically, GFS-VL incorporates three novel techniques. First, we introduce pseudo-label selection technique that uses accurate few-shot data to filter pseudolabels, retaining only high-quality regions while excluding noisy predictions. Second, as filtered wrong predictions will leave some regions unlabeled, potentially corresponding to novel objects, we present an adaptive infilling approach to enrich these regions. It combines knowledge from pseudo-label contexts and few-shot samples to construct an adaptive prototype set to label unlabeled regions, effectively considering both the completion of incomplete masks and the discovery of missing classes. Third, to further utilize few-shot samples, we propose novel-base mix strategy, embedding support samples into training scenes. Unlike traditional 3D data augmentation [39, 49, 64, 65, 71, 82], which mainly aims at fully-supervised segmentation and mixes object contexts, our approach emphasizes preserving contextual cues, which is crucial for novel class learning [56] by helping identify challenging novel classes. Furthermore, we identify limitations in current evaluation benchmarks. Existing benchmarks based on ScanNet [7] and S3DIS [3] datasets include only six novel classes, limiting diversity and failing to represent the complexity of real-world scenarios where novel classes are constantly varying. To address this, we introduce two challenging benchmarks: one with 40 novel classes from ScanNet200 [48] and another with 18 novel classes from ScanNet++ [70]. As detailed in Sec. 3.2, these benchmarks provide broader and more representative coverage of novel classes, enabling more comprehensive evaluation of models generalization capabilities. By fully integrating the benefits of 3D VLMs and fewshot data, our approach achieves state-of-the-art GFS-PCS performance on both existing and newly established benchmarks. Experiments demonstrate the effectiveness and generalizability of our framework across various models and datasets. Additionally, previous baselines exhibit limited performance when evaluated on our new benchmarks, underscoring the necessity of our benchmarks for assessing real-world generalization. Together, our methods and benchmarks would offer critical insights and tools to advance future research on GFS-PCS. 2. Related Work 2.1. Few-shot 3D Point Cloud Segmentation 3D point cloud segmentation is fundamental in understanding scene semantics, with many fully-supervised methods advancing this field [8, 15, 23, 24, 28, 40, 43, 58, 63, 76, 78]. However, these methods need expensive large-scale pointlevel annotations and have fixed output spaces. To address these limitations, FS-PCS was introduced in attMPTI [77], aiming to generalize to novel classes with limited support samples. FS-PCS research can be divided into two categories based on how relationships between query points and support classes are modeled: i) Feature optimization [16, 31, 37, 41, 59, 62, 72, 80, 81] These methods refine support prototypes or query features to enhance class separation. Final predictions are made using non-parametric, distance-based metrics, which implicitly model supportii) Correlation optimization [1, 2] query relationships. These approaches directly optimize correlations between support and query samples, explicitly modeling their relationships. COSeg [2] pioneered this approach recently and corrected two issues of foreground leakage and sparse point distribution in the previous FS-PCS setting. 2.2. Generalized Few-shot 3D Point Cloud Segmentation While standard few-shot models adapt effectively to novel classes with limited data, they are constrained to predict 2 only novel classes and require support samples to specify target classes during inference. more practical task, generalized few-shot segmentation, occurs to require predicting both base and novel classes at inference without support samples, as first introduced in 2D segmentation [4, 14, 17, 25, 33, 34, 36, 38, 56]. The pioneer work PIFS [4], using the prototype learning paradigm [51], fine-tunes base and novel prototypes with distillation loss, while CAPL [56] refines base prototypes using co-occurrence priors from support samples and dynamic contextual information from queries. For 3D, GW [66] introduces this setting to point cloud segmentation. GW models shared local geometric structures across base and novel classes as geometry words and then builds geometric prototypes to enhance the semantic prototypes, which are learned similarly to CAPL [56] by leveraging contextual information. To mine background semantics, Tsai et al. [57] clustered background points to generate pseudo-class prototypes distinct from base classes, leveraging multiple 2D views and 2D foundation models to link these points with class prompts. 2.3. 3D Vision-Language Models 3D VLMs align 3D point cloud features with language features, enabling open-world 3D understanding. However, developing these models poses unique challenges compared to 2D, mainly due to the scarcity of paired 3D-text data. To address this, recent work leverages multi-view 2D images, commonly associated with 3D point clouds, as intermediaries [30]. Some methods [5, 18, 19, 44, 54, 60, 75, 79] distill 2D features from 2D VLMs [12, 27, 46, 68] into their 3D encoders. For instance, OpenScene [44] aligns 3D and text representations by optimizing 3D-2D alignment using 2D VLMs [12, 27]. However, extracting these 2D features is computationally expensive, and the learned 3D features may inherit 2D prediction errors. Other approaches [9, 10, 21] generate point-language paired data by using captioning models [47, 60] to produce text descriptions of images. While effective, these captions are often at the scene level, limiting their ability to capture fine-grained 3D features. RegionPLC [67] recently introduced highquality region-level 3D-language associations, supporting robust 3D learning by dense regional language supervision. 2.4. Point Cloud Data Augmentation To address data limitations in the 3D domain, numerous methods have been developed to expand point cloud distributions. One category augments individual point clouds by altering geometric properties [22, 29, 32, 45, 50], using techniques such as shape transformations and patch shuffling. Another approach uses mixing techniques for 3D objects [6, 26, 74] and 3D scenes [11, 49, 64, 65, 71, 82]. For instance, Mix3D [39] mixes points from two scenes as an out-of-context augmentation for semantic segmentaDataset Base Novel Max (F) Min (F) Max (P) Min (P) S3DIS ScanNet ScanNet200 ScanNet++ 7 13 12 6 6 45 18 185 411 733 143 29 133 102 82 59,929 4,479 12,641 84,375 30,013 1,148 279 604 Table 1. Statistics for GFS-PCS benchmarks across four datasets. Base/Novel indicates the number of base and novel classes. Max/Min (F) is the maximum and minimum occurrences of each novel class across the entire dataset, while Max/Min (P) is the maximum and minimum average number of points per novel class. tion, while methods like [49] select domain-specific points with semantic information to mix across domains. Other methods, such as [64], relocate objects to less frequent locations to vary spatial distributions. Notably, most augmentation methods in semantic segmentation [39, 64] and detection [82] modify the original context of target classes, pushing models to learn object patterns independently of surroundings. However, we argue that preserving contextual dependencies is crucial for GFS-PCS, where novel classes often involve challenging, hard-to-detect objects. Isolated object patterns alone are insufficient for effective novel class generalization [56]. In contrast, our proposed novelbase mix augmentation retains key contextual information when incorporating support samples into training scenes, enhancing models ability to recognize novel classes. 3. GFS-PCS Overview 3.1. Problem Definition GFS-PCS requires the model to identify both base and novel classes present in test scenes. Let Cb denote the set of base class names with size Nb and Cn the set of novel class names with size Nn. The total number of classes is Nc = Nb + Nn. The base and novel class sets are mutually exclusive, i.e., Cb Cn = . Evaluation is conducted on the test dataset Dtest to assess segmentation across all classes Cb Cn. For training, the model is first trained on base classes and then registered with novel classes. The base dataset Dbase is defined as Dbase = {Xi b}i, where Xi represents the i-th point cloud and Yi contains the corresponding base class labels. For the novel class dataset, we define Dnovel as the collection of K-shot support samples: (cid:8){Xc k}K k, Yc . Each novel class has supk=1 port samples {Xc k=1 with exclusive labels {Yc k}K k=1. The base classes occupy class indices in the range [0, Nb), while novel classes are indexed in the range [Nb, Nc), with background regions labeled as 1. For simplicity, we denote data sample from Dbase as Xb and Yb, while Xc and Yc refer to support sample of class in Dnovel. b, Yi k}K (cid:9)Nc c=Nb 3.2. New Evaluation Benchmarks Current evaluation benchmarks for GFS-PCS utilize the ScanNet [7] and S3DIS [3] datasets. However, we iden3 Figure 2. Overview of the proposed GFS-VL. (a), (b) Given an input point cloud Xb, we apply novel-base mix to embed support samples into the training scene while preserving essential context. The scene is then processed by 3D VLM, using all class names as prompts to generate raw predictions ˆY. Leveraging support prototypes {pc}, the raw predictions undergo pseudo-label selection to filter out noisy regions, followed by adaptive infilling to label the filtered, unlabeled areas, yielding refined supervision for training the 3D segmentor. (c), (d) illustrate the details of the pseudo-label selection and adaptive infilling processes. tify limitations in the number and diversity of novel classes in these benchmarks. As shown in Tab. 1, while ScanNet includes 13 base classes and S3DIS has 7, both datasets include only 6 novel classes. Our analysis of the frequency and the average number of points per novel class further highlights the lack of diversity among these limited novel classes in these two benchmarks. This indicates that current evaluations are not sufficiently representative of the complexity of real-world novel categories, failing to robustly assess models generalization abilities. To address this, we propose two new and more challenging evaluation benchmarks based on ScanNet200 [48] and ScanNet++ [70] datasets. These new benchmarks, as detailed in Tab. 1, feature larger number and greater diversity of novel classes, providing more realistic and comprehensive testbed for evaluating model generalization to novel categories. 4. Method 4.1. Overview In our proposed framework GFS-VL, we adopt canonical segmentor architecture consisting of backbone and linear classification head. This minimalistic structure retains flexibility and simplicity, facilitating reproducibility. Following the standard GFS-PCS procedure [66], the segmentor is initially trained on base classes. The backbone, denoted as Φ, and the linear classifier for base classes, Hb, are employed to make base class predictions Pb: = Φ(Xb) RNpD, Pb = Hb(F) RNpNb , (1) where Np is the number of points in the point cloud input Xb, and represents backbone features with channel dimension of D. Then, when registering novel classes, new linear classifier Hn is introduced to handle the novel classes. The concatenated predictions from both classifiers form the complete output for all base and novel classes: = [Hb(F), Hn(F)] RNpNc , (2) where [, ] denotes concatenation. Subsequently, the model is fine-tuned on the base and few-shot samples, enabling it to simultaneously segment both base and novel classes. To fully exploit the rich knowledge from 3D VLMs while minimizing potential noise interference, we propose GFSVL, as shown in Fig. 2, to maximize the utility of limited but accurate novel samples to guide the learning process. We detail each designed module in the following sections, presented under the 1-shot setting for clarity. 4.2. Pseudo-label Selection direct method for utilizing 3D VLMs in GFS-PCS is to use their predictions as pseudo-labels for novel classes. However, these raw predictions are noisy, hindering fewshot models from learning good novel class representations. Moreover, such noisy pseudo-labels could introduce error accumulation from the 3D VLMs into the few-shot models. Therefore, we introduce an effective pseudo-label selection method by utilizing the valuable few-shot support samples to guide the selection of reliable novel class predictions. Specifically, for each novel class, we first com4 pute support prototypes using the few-shot samples. This is achieved by applying the vision encoder Θv of the 3D VLM Θ to compute masked average features for each novel class: (cid:80)Np Fc i=1 Fc (cid:80)Np = Θv(Xc n,iYc n,i , i=1 Yc n,i pc = n) RNpDv , = Nb, . . . , Nc 1, (3) where Dv is the feature dimension of the 3D VLM, and pc RDv represents the support prototype for novel class c. For clarity, we define this prototype extraction process as Fpool. Thus, Eq. (3) becomes: pc = Fpool(Xc n, Yc n). Next, given the current base training input Xb with base labels Yb, we prompt the 3D VLM Θ using all base and novel class names to obtain predictions ˆY RNp . Let ˆCn be the novel class indices existing in ˆY. We then compute the predicted prototype uc for each novel class in ˆCn: uc = Fpool(Xb, 1[ ˆY=c]), ˆCn. (4) Here, 1[ ˆY=c] is binary mask, set to 1 where ˆY equals the class index and to 0 otherwise. Then, we can filter the raw predictions to select high-quality novel class pseudo-labels: ˆYi = 1, if ˆYi [0, Nb) or (cid:0) ˆYi [Nb, Nc) and sim(u ˆYi , ˆYi ) < τ (cid:1), ˆYi, otherwise. (5) Here, if the predicted label ˆYi for the i-th point is base class, or novel class with cosine similarity below threshold τ between the predicted class prototype ˆYi and the support prototype ˆYi , we filter this pseudo-label by setting it to 1. Otherwise, we retain the original pseudo-label. Note this filtering process can be efficiently implemented using mask-based indexing without iterating each point. Now the updated ˆY contains only reliable pseudo-labels for novel classes. Given the original base class labels Yb, its background region (labeled as 1) serves as potential area for novel classes. Therefore, we merge the updated ˆY into the background region in Yb to generate augmented labels with additional reliable novel class pseudo-labels: b = Yb, = 1] = ˆY[Y b[Y = 1]. (6) 4.3. Adaptive Infilling After selection, includes reliable supervision for novel classes, while some regions remain unlabeled due to filtered low-quality predictions. These filtered predictions from the 3D VLM usually assign wrong labels, either by entirely missing true novel areas or partially mislabeling them [55]. 5 Consequently, tially correspond to missing or incomplete novel labels. contains unlabeled regions that potenTo address these gaps, we propose an adaptive infilling approach that utilizes both the few-shot samples and the current labels to build an adaptive prototype set for novel classes. This set allows us to assign novel labels adaptively to unlabeled regions, ensuring more comprehensive coverage of novel classes. We begin by extracting novel class prototypes from b: vc = Fpool(Xb, 1[Y b=c]), Cy n, (7) where Cy represents the novel class indices present in b. Using both these extracted prototypes and pre-computed support prototypes from the few-shot samples, we construct an adaptive prototype set, defined as {mc}, where: (cid:40) mc = if Cy vc, n, pc, otherwise, for = Nb, . . . , Nc 1. (8) This set {mc} incorporates novel class prototypes vc from if they exist; otherwise, it defaults to the few-shot support prototypes pc. By adapting to the current pseudolabels, this set facilitates the completion of incomplete novel class pseudo-labels while allowing for the discovery of missed novel classes based on support prototypes. = Next, we initialize b. For each unlabeled point b,i with feature Fb,i from Θv, we calculate its cosine similarity with each prototype mc as Sc b,i = sim(Fb,i, mc) and assign the corresponding novel class label if the maximum similarity exceeds threshold δ: b,i = arg max 1, Sc b,i, Sc b,i δ, if max otherwise. (9) This adaptive infilling mechanism effectively integrates knowledge from few-shot support samples with the current pseudo-label context, creating adaptive prototypes that help discover missed novel objects and complete partial pseudolabels, thereby enhancing the quality of novel region labels. 4.4. Novel-Base Mix To more sufficiently utilize support samples, we introduce novel-base mix approach that effectively integrates these valuable samples with the training data. Specifically, we start by randomly sampling novel sample Xc from Dnovel. To enhance the models focus on novel class, we crop the local bounding region based on the novel class mask Yc n: Xlocal , Ylocal = Fcrop(Xc n, Yc n), (10) where Fcrop represents the local cropping operation. We then construct new training input by mixing the with the current base input cropped novel sample Xlocal Xb. Unlike previous mixup methods [39, 64], which discard scene context, we argue that context information is essential for models to better recognize challenging novel objects and propose to preserve it. To achieve this, we extract the four corners in the XY plane for both Xlocal and Xb, and then select pair of opposite corners between them: Lb, Llocal = Fpair(Fcorner(Xb), Fcorner(Xlocal )), (11) where Fcorner extracts the four corner points in the XY projections, and Fpair randomly selects an opposing corner pair. Possible pairs include, for example, the leftmost corner of Xb with the rightmost corner of Xlocal , or the uppermost corner of Xb with the lowermost corner of Xlocal . Here, Lb and Llocal are the coordinates of the selected corn ners. To align Xb with Xlocal at the chosen corners, we compute translation vector = Lb Llocal and apply it to translate Xlocal , yielding the final mixed result. This ensures close connection between the samples without losing context, which is crucial for effective novel class learning. Visualizations of the output can be found in Sec. 5.3, with further details in the supplementary material. n 5. Experiments 5.1. Experimental Setup Datasets. Our new evaluation benchmark builds on two datasets: 1) ScanNet200 [48] This dataset extends the labeling scope of ScanNet [7] from 20 to 200 classes, adding finer-grained subclasses of existing categories and numerous new classes. 2) ScanNet++ [70] Comprising 460 scenes with annotations for over 1000 unique classes, ScanNet++ is designed to capture wide range of object types. To establish comprehensive GFS-PCS benchmark, we selected the most frequent classes from each dataset, ensuring adequate representation across scenes. Our final benchmark includes 57 classes for ScanNet200 (with 40 novel classes) and 30 classes for ScanNet++ (with 18 novel classes). Full class lists are in the supplementary material. We follow the standard training/testing splits for each dataset, adhering to the preprocessing and augmentation settings from [63], where raw input points are voxelized at 0.02m grid size. Notably, unlike prior GFS-PCS evaluations [66, 77], which test models on small blocks, we test on whole scenes to better simulate real-world scenarios. Implementation Details. Our framework uses straightforward segmentor with backbone and linear classification head, optimized for efficiency and simplicity. By default, we use Point Transformer V3 (PTv3) [63] as the backbone and 3D VLM RegionPLC [67]. The segmentor is first pre-trained on base classes of each dataset, after which we add separate linear classification head for novel classes, enabling lightweight and efficient adaptation to novel classes in 20 fine-tuning epochs. The pre-training 6 Figure 3. Qualitative comparison between GW [66] and our GFS-VL on ScanNet200. Class colors are shown at the top. setting follows [63] with 800 epochs, and we use Adam optimizer for fine-tuning with learning rates: 0.001 for ScanNet200 and ScanNet, and 0.007 for ScanNet++. More details are in the supplementary material. For evaluation, we adopt metrics outlined in [66]: mean Intersection-overUnion (mIoU) for base classes (mIoU-B), novel classes (mIoU-N), all classes (mIoU-A), and the harmonic mean of mIoU-B and mIoU-N (HM) which captures overall performance while mitigating bias towards base classes [69]. 5.2. Experimental Results In our evaluation, we benchmark our method against baseline models attMPTI [77] and PIFS [4] following [66], along with the state-of-the-art GFS-PCS model GW [66] and FS-PCS model COSeg [2]. For fairness, all baseline models are retrained using the same backbone as our model. We also include Fully Supervised model as an upper bound, obtained by fine-tuning an identical model to ours on ground-truth labels for both base and novel classes. Following [56, 66], we evaluate two support scenarios, 5-shot and 1-shot, averaging performance across five randomly-seeded support set versions. As shown in Tab. 2, on ScanNet200, our model achieves substantial improvements across all metrics and support scenarios, including 28.57% increase in HM and 23.37% boost in mIoU-N over the closest baseline, GW, in the 5-shot setting. Qualitative comparisons in Fig. 3 further illustrate the superior segmentation accuracy of our model compared to GW. Similar trends are seen on ScanNet++ (see Tab. 3), where our model improves HM by 17.88% and mIoU-N by 12.79% compared to GW (1-shot). For the traditional ScanNet benchmark with only six novel classes, we report results in Tab. 4 using baseline performance from [66], where our model significantly surpasses all baselines with an impressive 34.94% gain in mIoU-N and 39.33% in HM for the 1-shot task. The substantial and consistent gains across diverse datasets and metrics highlight our methods strong adaptability to diverse and complex novel classes by effectively integrating valuable few-shot samples with the semantic insights of 3D VLMs [9, 44]. In contrast, baseline models rely Method Fully Supervised PIFS [4] attMPTI [77] COSeg [2] GW [66] GFS-VL (ours) 5-shot 1-shot mIoU-B mIoU-N mIoU-A 68.70 28.78 37.13 57.67 59.28 67.57 39.32 3.82 4.99 5.21 8.30 31.67 45.51 9.07 11.76 16.25 19.03 39. HM 50.02 6.71 8.79 9.54 14.55 43.12 mIoU-B mIoU-N mIoU-A 68.70 17.84 54.84 47.03 55.23 68.48 39.32 2.87 3.28 4.03 6.47 29.18 45.51 6.02 14.14 13.09 16.74 37. HM 50.02 4.88 6.17 7.42 11.56 40.92 Table 2. Comparisons of our method with baselines on the new ScanNet200 benchmark. The best results are highlighted in bold. Method Fully Supervised PIFS [4] attMPTI [77] COSeg [2] GW [66] GFS-VL (ours) 5-shot 1-shot mIoU-B mIoU-N mIoU-A 65.45 39.98 55.89 59.34 51.35 60.05 37.24 5.74 4.19 6.96 11.03 21.66 48.53 19.44 24.87 27.91 27.16 37. HM 47.47 10.03 7.78 12.45 18.15 31.82 mIoU-B mIoU-N mIoU-A 65.45 36.66 53.16 58.49 46.71 61.39 37.24 4.95 3.55 6.24 6.63 19.42 48.53 17.63 23.40 27.14 22.66 36. HM 47.47 8.71 6.66 11.26 11.59 29.47 Table 3. Comparisons of our method with baselines on the new ScanNet++ benchmark. The best results are highlighted in bold. Method Fully Supervised attMPTI [77] PIFS [4] CAPL [56] GW [66] GFS-VL (ours) 5-shot 1-shot mIoU-B mIoU-N mIoU-A 78.71 16.31 35.14 38.22 40.18 78.30 60.37 3.12 3.21 14.39 18.58 51.22 72.91 12.35 25.56 31.07 33.70 69. HM 68.33 5.21 5.88 20.88 25.39 61.91 mIoU-B mIoU-N mIoU-A 78.71 12.97 35.80 38.70 40.06 78.56 60.37 1.62 2.54 10.59 14.78 49.72 72.91 9.57 25.82 30.27 32.47 69. HM 68.33 2.88 4.75 16.53 21.55 60.88 Table 4. Comparisons of our method with baselines on the old ScanNet benchmark. The best results are highlighted in bold. solely on limited few-shot samples, constraining their capacity for novel class generalization and resulting in lower scores on the benchmarks. Moreover, the reduced performance on our new benchmarks emphasizes their value as rigorous and comprehensive evaluation setting. These benchmarks challenge models to demonstrate robust generalization across diverse classes, fostering deeper understanding of GFS-PCS for real-world applications. 5.3. Ablation Studies We conducted ablation studies on the ScanNet200 [48] dataset using single set of 5-shot support samples. Effect of Design Components. We assessed the effectiveness of each module in GFS-VL in Tab. 5a. The baseline (1st row) shows performance using raw pseudo-labels directly from the 3D VLM. Adding Pseudo-label Selection (PS) in the 2nd row significantly improves pseudo-label quality, resulting in clear performance boost. Introducing Adaptive Infilling (AI) further enhances results by effectively assigning labels to unlabeled regions of novel classes (3th row). Visualizations in Fig. 4 illustrate the quality improvements achieved with PS and AI, affirming their effectiveness. Lastly, Novel-Base Mix (NB-Mix), whether used with PS (4rd row) or combined with PS and AI (5th row), improves generalization by effectively integrating novel knowledge into the training samples. Results with Different 3D VLMs. We tested GFS-VLwith two prominent 3D VLMs, RegionPLC [67] and Openscene [44], as shown in the 2nd and 4th rows of Tab. 5b. Compared to their zero-shot results (1st and 3rd rows), our approach achieves substantial gains by effectively leveraging few-shot samples to refine and enhance the noisy knowledge in each 3D VLM. This demonstrates our frameworks flexibility and effectiveness across diverse 3D VLMs. Impact of the Threshold in AI. Adaptive Infilling (AI) assigns novel class labels to unlabeled points based on similarity to adaptive prototypes, controlled by threshold δ. In Tab. 5c, we explore different δ values and observe that δ = 0.9 achieves the highest performance, which best balances enriching novel classes and maintaining label quality. Effect of the Threshold in PS. Pseudo-label Selection (PS) refines pseudo-labels by retaining only the most reliable regions predicted by the 3D VLM, based on threshold τ . In Tab. 5d, we evaluate the effect of varying τ . The method performs robustly across different τ values, with the optimal PS AI NB-Mix mIoU-B mIoU-N mIoU-A HM 3D VLM mIoU-B mIoU-N mIoU-A HM δ mIoU-B mIoU-N mIoU-A HM 65.50 69.26 66.25 66.94 67.42 22.30 26.51 28.03 28.21 31.81 31.40 35.51 36.07 36.36 39.30 33.28 38.35 39.39 39.69 43.22 RegionPLC [67] Ours (RegionPLC) Openscene [44] Ours (Openscene) 46.97 67.42 53.07 68.56 23.77 31.81 15.16 20.09 28.65 39.30 23.14 30.29 31.56 43.22 23.58 31.07 0.80 0.85 0.90 0.95 66.67 66.29 67.42 66. (a) (b) 38.04 38.43 39.30 38.17 41.77 42.24 43.22 41.94 30.41 31.00 31.81 30.68 (c) τ mIoU-B mIoU-N mIoU-A HM mIoU-B mIoU-N mIoU-A HM 0.5 0.6 0.7 0.8 67.19 67.42 66.69 68.06 31.33 31.81 30.56 30.67 (d) 38.88 39.30 38.17 38.54 42.73 43.22 41.92 42.28 1 2 3 4 67.40 67.95 66.94 67.84 35.61 36.18 36.36 36.23 38.68 39.36 39.69 39. 27.13 27.71 28.21 27.80 (e) Backbone mIoU-B mIoU-N mIoU-A HM Mix mIoU-B mIoU-N mIoU-A HM PTv3 [63] 67.42 31.81 39.30 43.22 SCN [13] 61. 31.94 38.24 42.13 Instance Mix Mix3D [39] NB-Mix 68.29 68.50 67.95 23.93 24.80 27. 33.27 34.00 36.18 35.44 36.42 39.36 (f) (g) Table 5. Ablation study. (a) Effect of design components. (b) Results with different 3D VLMs. (c) Impact of the threshold in AI. (d) Effect of the threshold in PS. (e) Impact of different mix blocks. (f) Results with different backbones. (g) Comparison of mix strategies. Figure 4. Visualization of the improvements in pseudo-label quality after applying Pseudo-label Selection (PS) and Adaptive Infilling (AI). Note that AI effectively discovers missed novel classes in the red circles and completes partial pseudo-labels in the green circles. tive mix strategies for integrating novel support samples into training data in Tab. 5g. Specifically, we compared our NB-Mix with Instance Mix, which randomly inserts novel class objects from foreground masks into scenes, and Mix3D [39], which overlays two scenes for out-of-context augmentation. Fig. 5 shows the visual examples of these strategies. Our method outperforms these alternatives, highlighting the importance of preserving local context to effectively learn diverse and challenging novel classes. 6. Conclusion This work introduces GFS-PCS framework GFS-VL that synergizes dense but noisy pseudo-labels from 3D VLMs with accurate yet sparse few-shot samples, overcoming current GFS-PCS limitations in novel knowledge learning. GFS-VL utilizes prototype-guided pseudo-label selection to target high-quality regions and adaptive infilling to enrich pseudo-labels. Besides, the novel-base mix embeds few-shot samples into training scenes, preserving essential context for improved novel class learning. Identifying the limited diversity in current GFS-PCS evaluations, we introduce two benchmarks with broader, more diverse novel classes for more comprehensive generalization evaluation. GFS-VL achieves leading results and generalizes effectively across models and datasets, showing the potential of 3D VLMs in advancing GFS-PCS. We hope our method and benchmarks serve as foundation for future research. Figure 5. Visual illustration of mixing strategies. The red and green boxes represent the two novel samples mixed into the scene. performance achieved at τ = 0.6. This robustness indicates that PS effectively selects relevant reliable regions without being overly sensitive to the threshold choice. Impact of Different Mix Blocks. In NB-Mix, we can adjust the number of novel blocks, denoted as n, used to augment each training sample. Tab. 5e shows stable performance (with the AI module disabled) across different block counts, suggesting that NB-Mix effectively integrates novel class information. By default, we use three blocks. Results with Different Backbones. Besides varying 3D VLMs, we examined the effect of different backbones. We evaluated two backbones, PTv3 [63] and SparseConvNet (SCN) [13] in Tab. 5f. Our approach consistently performs well across both networks, confirming that GFS-VL is generalizable and not dependent on specific backbone. Comparison of Mix Strategies. We investigated alterna-"
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by the Pioneer Centre for AI, DNRF grant number P1."
        },
        {
            "title": "References",
            "content": "[1] Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, and Serge Belongie. Multimodality helps few-shot 3D point cloud semantic segmentation. In ICLR, 2024. 2 [2] Zhaochong An, Guolei Sun, Yun Liu, Fayao Liu, Zongwei Wu, Dan Wang, Luc Van Gool, and Serge Belongie. Rethinking few-shot 3D point cloud semantic segmentation. In CVPR, pages 39964006, 2024. 1, 2, 6, 7 [3] Iro Armeni, Ozan Sener, Amir Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In CVPR, pages 1534 1543, 2016. 2, 3 [4] Fabio Cermelli, Massimiliano Mancini, Yongqin Xian, Zeynep Akata, and Barbara Caputo. Prototype-based incremental few-shot semantic segmentation. In BMVC, page 155, 2021. 3, 6, 7 [5] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. CLIP2Scene: Towards label-efficient 3D scene understanding by CLIP. In CVPR, pages 70207030, 2023. 2, 3 [6] Yunlu Chen, Vincent Tao Hu, Efstratios Gavves, Thomas Mensink, Pascal Mettes, Pengwan Yang, and Cees GM Snoek. PointMixup: Augmentation for point clouds. In ECCV, pages 330345. Springer, 2020. [7] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, pages 58285839, 2017. 2, 3, 6 [8] Hao Deng, Kunlei Jing, Shengmei Chen, Cheng Liu, Jiawei Ru, Bo Jiang, and Lin Wang. Linnet: Linear network for efficient point cloud representation learning. In NeurIPS, 2024. 2 [9] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. PLA: Language-driven openvocabulary 3D scene understanding. In CVPR, pages 7010 7019, 2023. 2, 3, 6 [10] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Lowis3D: Language-driven IEEE open-world instance-level 3D scene understanding. TPAMI, 46(12):85178533, 2024. 2, 3 [11] Jin Fang, Xinxin Zuo, Dingfu Zhou, Shengze Jin, Sen Wang, and Liangjun Zhang. LiDAR-Aug: general renderingbased augmentation framework for 3D object detection. In CVPR, pages 47104720, 2021. 3 [12] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In ECCV, pages 540557, 2022. 2, [13] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3D semantic segmentation with submanifold sparse convolutional networks. In CVPR, pages 92249232, 2018. 8, 3 [14] Sina Hajimiri, Malik Boudiaf, Ismail Ben Ayed, and Jose Dolz. strong baseline for generalized few-shot semantic segmentation. In CVPR, pages 1126911278, 2023. 3 [15] Jiawei Han, Kaiqi Liu, Wei Li, and Guangzhi Chen. Subspace prototype guidance for mitigating class imbalance in In ECCV, pages 255 point cloud semantic segmentation. 272, 2024. 2 [16] Shuting He, Xudong Jiang, Wei Jiang, and Henghui Ding. Prototype adaption and projection for few-and zero-shot 3D IEEE TIP, 32:3199 point cloud semantic segmentation. 3211, 2023. 2 [17] Mir Rayat Imtiaz Hossain, Mennatullah Siam, Leonid Sigal, and James Little. Visual prompting for generalized fewshot segmentation: multi-scale approach. In CVPR, pages 2347023480, 2024. 2, 3 [18] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang, and Wangmeng Zuo. CLIP2Point: Transfer CLIP to point cloud classification with image-depth pre-training. In ICCV, pages 22157 22167, 2023. 2, [19] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, et al. ConceptFusion: Open-set multimodal 3D mapping. In ICRA2023 Workshop on Pretraining for Robotics (PT4R), 2023. 3 [20] Jincen Jiang, Qianyu Zhou, Yuhang Li, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang, and Xuequan Lu. Pcotta: Continual test-time adaptation for multitask point cloud understanding. In NeurIPS, 2024. 1 [21] Li Jiang, Shaoshuai Shi, and Bernt Schiele. Open-vocabulary 3D semantic segmentation with foundation models. In CVPR, pages 2128421294, 2024. 2, 3 [22] Sihyeon Kim, Sanghyeok Lee, Dasol Hwang, Jaewon Lee, Seong Jae Hwang, and Hyunwoo Kim. Point cloud augIn ICCV, mentation with weighted local transformations. pages 548557, 2021. 3 [23] Maxim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, and Danila Rukhovich. OneFormer3D: One transformer for unified point cloud segmentation. In CVPR, pages 20943 20953, 2024. 2 [24] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transIn CVPR, pages former for 3D point cloud segmentation. 85008509, 2022. 2 [25] Chunbo Lang, Gong Cheng, Binfei Tu, and Junwei Han. Learning what not to segment: new perspective on fewshot segmentation. In CVPR, pages 80578067, 2022. 3 [26] Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee, Minhyeok Lee, Sungmin Woo, and Sangyoun Lee. Regularization strategy for point cloud via rigidly mixed sample. In CVPR, pages 1590015909, 2021. [27] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In ICLR, 2022. 2, 3 9 [28] Lei Li. Hierarchical edge aware learning for 3D point cloud. In Computer Graphics International Conference, pages 81 92. Springer, 2023. 2 [29] Ruihui Li, Xianzhi Li, Pheng-Ann Heng, and Chi-Wing Fu. Pointaugment: an auto-augmentation framework for point cloud classification. In CVPR, pages 63786387, 2020. 3 [30] Ruihuang Li, Zhengqiang Zhang, Chenhang He, Zhiyuan Ma, Vishal Patel, and Lei Zhang. Dense multimodal alignment for open-vocabulary 3D scene understanding. In ECCV, 2024. 3 [31] Zhaoyang Li, Yuan Wang, Wangkai Li, Rui Sun, and Tianzhu Zhang. Localization and expansion: decoupled framework for point cloud few-shot semantic segmentation. In ECCV, 2024. [32] Chuandong Liu, Chenqiang Gao, Fangcen Liu, Pengcheng Li, Deyu Meng, and Xinbo Gao. Hierarchical supervision and shuffle data augmentation for 3D semi-supervised object detection. In CVPR, pages 2381923828, 2023. 3 [33] Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Learning orthogonal prototypes for generalized few-shot semantic segmentation. In CVPR, pages 1131911328, 2023. 2, 3 [34] Weide Liu, Zhonghua Wu, Yang Zhao, Yuming Fang, Chuan-Sheng Foo, Jun Cheng, and Guosheng Lin. Harmonizing base and novel classes: class-contrastive approach for generalized few-shot segmentation. IJCV, 132(4):1277 1291, 2024. 2, 3 [35] Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, and Xiang Bai. Lion: Linear group rnn for 3D object detection in point clouds. In NeurIPS, 2024. 1 [36] Zhihe Lu, Sen He, Da Li, Yi-Zhe Song, and Tao Xiang. Prediction calibration for generalized few-shot semantic segmentation. IEEE TIP, 32:33113323, 2023. 3 [37] Yongqiang Mao, Zonghao Guo, LU Xiaonan, Zhiqiang Yuan, and Haowen Guo. Bidirectional feature globalization for few-shot semantic segmentation of 3D point cloud scenes. In 3DV, pages 505514, 2022. [38] Josh Myers-Dean, Yinan Zhao, Brian Price, Scott Cohen, and Danna Gurari. Generalized few-shot semantic segmentation: All you need is fine-tuning. arXiv preprint, 2021. 3 [39] Alexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3D: Out-of-context data augIn 3DV, pages 116125. IEEE, mentation for 3D scenes. 2021. 2, 3, 6, 8 [40] Dong Nie, Rui Lan, Ling Wang, and Xiaofeng Ren. Pyramid architecture for multi-scale processing in point cloud segmentation. In CVPR, pages 1728417294, 2022. 2 [41] Zhenhua Ning, Zhuotao Tian, Guangming Lu, and Wenjie Pei. Boosting few-shot 3D point cloud segmentation via query-guided enhancement. In ACM MM, pages 18951904, 2023. 2 [42] Zhiyi Pan, Wei Gao, Shan Liu, and Ge Li. Distribution guidance network for weakly supervised point cloud semantic segmentation. In NeurIPS, 2024. 1 [43] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast point transformer. In CVPR, pages 1694916958, 2022. 2 [44] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. OpenScene: 3D scene understanding with open vocabularies. In CVPR, pages 815824, 2023. 2, 3, 6, 7, [45] Shoumeng Qiu, Jie Chen, Chenghang Lai, Hong Lu, Xiangyang Xue, and Jian Pu. Leveraging smooth deformation augmentation for LiDAR point cloud semantic segmentation. IEEE Transactions on Intelligent Vehicles, 9(2):33163329, 2024. 3 [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 3 [47] Steven Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, pages 70087024, 2017. 2, 3 [48] David Rozenberszki, Or Litany, and Angela Dai. Languagegrounded indoor 3D semantic segmentation in the wild. In ECCV, pages 125141, 2022. 2, 4, 6, 7, 3 [49] Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Fabio Poiesi, and Elisa Ricci. Compositional semantic mix for domain adaptation in point cloud segmentation. IEEE TPAMI, 45(12):1423414247, 2023. 2, 3 [50] Shivanand Venkanna Sheshappanavar, Vinit Veerendraveer Singh, and Chandra Kambhamettu. Patchaugment: Local neighborhood augmentation in point cloud classification. In ICCV, pages 21182127, 2021. [51] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical In NeurIPS, pages 4077 networks for few-shot learning. 4087, 2017. 2, 3 [52] Hongyu Sun, Qiuhong Ke, Yongcai Wang, Wang Chen, Kang Yang, Deying Li, and Jianfei Cai. Point-prc: prompt learning based regulation framework for generalizable point cloud analysis. In NeurIPS, 2024. 1 [53] Yuliang Sun, Xudong Zhang, and Yongwei Miao. review of point cloud segmentation for understanding 3D indoor scenes. Visual Intelligence, 2(1):14, 2024. 1 [54] Ayca Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-vocabulary 3D instance segmentation. In NeurIPS, pages 6836768390, 2023. 2, 3 [55] Anirud Thyagharajan, Benjamin Ummenhofer, Prashant Laddha, Om Ji Omer, and Sreenivas Subramoney. SegmentFusion: Hierarchical context fusion for robust 3D semantic segmentation. In CVPR, pages 12361245, 2022. [56] Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalized few-shot semantic segmentation. In CVPR, pages 1156311572, 2022. 1, 2, 3, 6, 7 [57] Chih-Jung Tsai, Hwann-Tzong Chen, and Tyng-Luh Liu. Pseudo-embedding for generalized few-shot 3D segmentation. In ECCV, pages 383400, 2024. 3 [58] Changshuo Wang, Meiqing Wu, Siew-Kei Lam, Xin Ning, Shangshu Yu, Ruiping Wang, Weijun Li, and Thambipillai Srikanthan. GPSFormer: global perception and local structure fitting-based transformer for point cloud understanding. In ECCV, pages 7592, 2024. 2 10 [73] Guowen Zhang, Lue Fan, Chenhang He, Zhen Lei, Zhaoxiang Zhang, and Lei Zhang. Voxel mamba: Group-free state space models for point cloud based 3D object detection. In NeurIPS, 2024. 1 [74] Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujin Chen, Yanmei Meng, and Danfeng Wu. PointCutMix: Regularization strategy for point cloud classification. Neurocomputing, 505:5867, 2022. [75] Junbo Zhang, Runpei Dong, and Kaisheng Ma. CLIP-FO3D: Learning free open-world 3D scene representations from 2D dense CLIP. In ICCV, pages 20482059, 2023. 2, 3 [76] Nan Zhang, Zhiyi Pan, Thomas Li, Wei Gao, and Ge Li. Improving graph representation for point cloud segmentation via attentive filtering. In CVPR, pages 12441254, 2023. 2 [77] Na Zhao, Tat-Seng Chua, and Gim Hee Lee. Few-shot 3D point cloud semantic segmentation. In CVPR, pages 8873 8882, 2021. 1, 2, 6, 7, 3 [78] Yu Zheng, Guangming Wang, Jiuming Liu, Marc Pollefeys, and Hesheng Wang. Spherical frustum sparse convolution network for lidar point cloud semantic segmentation. In NeurIPS, 2024. 2 [79] Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, and Hao Su. Point-sam: Promptable 3D segmentation model for point clouds. In ICLR, 2025. 3 [80] Guanyu Zhu, Yong Zhou, Rui Yao, and Hancheng Zhu. Cross-class bias rectification for point cloud few-shot segmentation. IEEE TMM, 25:91759188, 2023. 2 [81] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, and Peng Gao. No time to train: Empowering non-parametric networks for few-shot 3D scene segmentation. In CVPR, 2024. [82] Ziyue Zhu, Qiang Meng, Xiao Wang, Ke Wang, Liujiang Yan, and Jian Yang. Curricular object manipulation in LiDAR-based object detection. In CVPR, pages 11251135, 2023. 2, 3 [59] Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, and Tong Heng Lee. Few-shot point cloud semantic segmentation via contrastive self-supervision and multi-resolution attention. In ICRA, pages 28112817, 2023. 2 [60] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In ICML, pages 2331823340, 2022. 2, 3 [61] Yizhou Wang, Longguang Wang, Qingyong Hu, Yan Liu, Ye Zhang, and Yulan Guo. Panoptic segmentation of 3D point clouds with gaussian mixture model in outdoor scenes. Visual Intelligence, 2(1):10, 2024. 1 [62] Lili Wei, Congyan Lang, Ziyi Chen, Tao Wang, Yidong Li, and Jun Liu. Generated and pseudo content guided prototype refinement for few-shot point cloud segmentation. In NeurIPS, pages 3110331123, 2024. 2 [63] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer V3: Simpler faster stronger. In CVPR, pages 48404851, 2024. 2, 6, 8, 3 [64] Yanhao Wu, Tong Zhang, Wei Ke, Congpei Qiu, Sabine Susstrunk, and Mathieu Salzmann. Mitigating object dependencies: Improving point cloud self-supervised learning In CVPR, pages 2305223061, through object exchange. 2024. 2, 3, [65] Aoran Xiao, Jiaxing Huang, Dayan Guan, Kaiwen Cui, Shijian Lu, and Ling Shao. PolarMix: general data augmentation technique for LiDAR point clouds. In NeurIPS, pages 1103511048, 2022. 2, 3 [66] Yating Xu, Conghui Hu, Na Zhao, and Gim Hee Lee. Generalized few-shot point cloud segmentation via geometric words. In ICCV, pages 2150621515, 2023. 1, 2, 3, 4, 6, 7 [67] Jihan Yang, Runyu Ding, Weipeng Deng, Zhe Wang, and Xiaojuan Qi. RegionPLC: Regional point-language contrastive learning for open-world 3D scene understanding. In CVPR, pages 1982319832, 2024. 2, 3, 6, 7, 8 [68] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. DetCLIP: Dictionary-enriched visual-concept paralleled pretraining for open-world detection. In NeurIPS, pages 9125 9138, 2022. 3 [69] Han-Jia Ye, Hexiang Hu, and De-Chuan Zhan. Learning adaptive classifiers synthesis for generalized few-shot learning. IJCV, 129:19301953, 2021. 6 [70] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. ScanNet++: high-fidelity dataset of 3D indoor scenes. In ICCV, pages 1222, 2023. 2, 4, [71] Jinglin Zhan, Tiejun Liu, Rengang Li, Jingwei Zhang, Zhaoxiang Zhang, and Yuntao Chen. Real-Aug: Realistic scene synthesis for LiDAR augmentation in 3D object detection. arXiv preprint, 2023. 2, 3 [72] Canyu Zhang, Zhenyao Wu, Xinyi Wu, Ziyu Zhao, and Song Wang. Few-shot 3D point cloud semantic segmentation via stratified class-specific attention based transformer network. In AAAI, pages 34103417, 2023. 2 11 Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model"
        },
        {
            "title": "Supplementary Material",
            "content": "Algorithm 1: Pseudo-code of Novel-Base Mix in PyTorch style. # Input: # novel_cloud: point cloud of the novel class, shape ( N, 3) 3) # novel_mask: binary mask for points belonging to the novel class, shape (N,) # base_cloud: point cloud of base classes, shape (M, # random_corner: function to randomly select corner (bottom, top, left, right) # crop_fn: function to crop novel point clouds based # corner_fn: function to compute corner points in the # returning dictionary with keys: [top, bottom, on the mask XY plane, left, right] # Step 1: Crop the novel point cloud based on the mask novel_local, novel_local_mask = crop_fn(novel_cloud, novel_mask) # Step 2: Compute corner points for both point clouds base_corners = corner_fn(base_cloud) novel_corners = corner_fn(novel_local) # Step 3: Randomly select corner for alignment selected_corner = random_corner([bottom, top, left, right]) # Step 4: Calculate the translation vector based on selected corners if selected_corner == \"bottom\": base_point = base_corners[bottom] # Lowest point of base cloud in novel_point = novel_corners[top] # Highest point of novel cloud in elif selected_corner == \"top\": base_point = base_corners[top] # Highest point of base cloud in novel_point = novel_corners[bottom] # Lowest point of novel cloud in elif selected_corner == \"left\": base_point = base_corners[left] # Leftmost point of base cloud in novel_point = novel_corners[right] # Rightmost point of novel cloud in else: # \"right\" base_point = base_corners[right] # Rightmost point of base cloud in novel_point = novel_corners[left] # Leftmost point of novel cloud in translation_vector = [base_point[0] - novel_point[0], base_point[1] - novel_point[1], 0] # No z-translation yet # Step 5: Translate the novel cloud in the XY plane novel_local_translated = novel_local + translation_vector # Step 6: Align the z-coordinates z_adjustment = min(base_cloud[:, 2]) - min( novel_local_translated[:, 2]) novel_local_translated[:, 2] += z_adjustment # Step 7: Combine base cloud and translated novel cloud mixed_cloud = torch.cat([base_cloud, novel_local_translated], dim=0) 7. Additional Details on Novel-Base Mix To effectively utilize support samples, the Novel-Base Mix approach is designed to integrate them into the base training inputs while preserving essential scene context. This ensures effective learning of challenging novel classes. We provide the pseudo-code for Novel-Base Mix in Algorithm 1. Below, we present step-by-step explanation of the process. Step 1. The process begins with cropping the region of novel objects from the novel point cloud. Given the randomly sampled novel point cloud and its corresponding binary mask, cropping operation is applied to extract the relevant local region. This ensures that only the novel object region is considered for mixing, while extraneous unnecessary points are excluded. Step 2. Next, to align the cropped novel sample with the base point cloud, we identify key spatial anchors in the XY plane. These anchors correspond to the top, bottom, left, and rightmost corner points of both the base point cloud and the cropped novel point cloud. These anchors serve as reference points for spatial alignment in subsequent steps. random corner from {top, bottom, Step 34. left, right} is selected for alignment. For instance: If the bottom corner is chosen, the lowest corner of the base point cloud is aligned with the highest corner of the novel point cloud. Conversely, if the left corner is selected, the leftmost corner of the base point cloud is aligned with the rightmost corner of the novel point cloud. This pairing strategy introduces diversity in the placement of novel objects while ensuring the preservation of contextual integrity. Based on the selected corner pair, translation vector is computed to spatially align the novel sample with the base point cloud. Step 56. The computed translation vector is applied to the cropped novel point cloud, ensuring that it is positioned next to the base point cloud in the XY plane. Additionally, Z-axis alignment is performed by adjusting the Z-coordinates of the translated novel point cloud. This step ensures that the novel sample is grounded at the same level as the base point cloud, preventing it from floating above or sinking below the base scene. Step 7. Finally, the aligned novel sample is merged with the base point cloud to form the mixed training input. This effectively integrates the novel sample into the training 1 Figure 6. Visualization of the outputs from the proposed Novel-Base Mix. The red and green boxes represent the two novel samples mixed into the scene. The novel class colors are shown at the top. scene while retaining its original context, which helps the model recognize complex and challenging novel classes. We provide additional visualizations of the outputs from our Novel-Base Mix in Fig. 6. 8. Additional Details on the new Benchmarks As discussed in Sec.5.1, we leverage two recent datasets, ScanNet200 [48] and ScanNet++ [70], to construct comprehensive evaluation benchmarks for GFS-PCS. ScanNet200 [48] extends the labeling space of ScanNet [7] from 20 to 200 categories, introducing finer-grained subclasses of existing categories and numerous novel object types. These expansions enhance the datasets granularity and diversity, making it valuable resource for evaluating GFS-PCS methods. Meanwhile, ScanNet++ [70] offers annotations for 460 scenes encompassing over 1,000 unique object classes. This dataset captures broad range of object categories, reflecting the complexity and variability of real-world environments. Together, these datasets form rich and diverse foundation for constructing robust GFSPCS evaluation benchmarks. Benchmark Design. To create meaningful and representative GFS-PCS benchmarks, we carefully selected classes based on their occurrence counts in the respective datasets, ensuring sufficient representation across scenes. The selection process involved computing the occurrence count of each class across the dataset, ranking the classes by their occurrence counts, and assigning them to base and novel sets as follows: ScanNet200: Classes with occurrence counts exceeding 100 were retained, yielding 57 classes in total. The 12 most frequently occurring classes were designated as base classes, while the remaining 45 were assigned as novel 2 classes. ScanNet++: Classes with occurrence counts exceeding 80 were retained, resulting in 30 classes in total. The top 12 most frequent classes formed the base class set, while the remaining 18 were assigned as novel classes. These frequency thresholds were carefully chosen to strike balance between class diversity and adequate representation, ensuring that both base and novel classes are well-suited for evaluating GFS-PCS performance. Benchmark Classes. lists for the two benchmarks: ScanNet200: The following are the specific class Base Classes: [refrigerator, desk, curtain, bookshelf, bed, table, window, cabinet, door, chair, floor, wall] Novel Classes: ceiling, [trash can, doorframe, object, shelf, sink, picture, backpack, couch, box, pillow, radiator, mirror, whiteboard, lamp, toilet, book, monitor, towel, tv, clothes, coffee table, office chair, nightstand, bag, dresser, toilet paper, recycling bin, kitchen cabinet, bathtub, telephone, plant, stool, keyboard, shoe, jacket, shower curtain, armchair, microwave, computer tower, bathroom vanity, kitchen counter, shower wall, paper towel dispenser, file cabinet] ScanNet++: Base Classes: [wall, floor, door, ceiling, table, window, box, ceiling lamp, light switch, cabinet, chair, heater] Novel Classes: of- [monitor, fice chair, bottle, doorframe, keyboard, window frame, mouse, paper, blinds, trash can, telephone, book, shelf, sink, windowsill, bag, whiteboard, smoke detector] 10. Additional Visualizations Overall, our benchmarks provide more robust and comprehensive testbed for evaluating GFS-PCS methods. By better reflecting real-world challenges, our benchmarks enable researchers to rigorously assess models performance and generalization to novel categories under realistic scenarios. 9. Additional Implementation Details Our framework employs straightforward segmentor consisting of backbone and linear classification head, designed for both efficiency and simplicity to facilitate reproducibility. The training process consists of two stages: pretraining on the base classes of each dataset, followed by fine-tuning with adding separate linear classification head for novel classes. For prompting the 3D VLMs, we adopt the default prompt used in RegionPLC [67] and OpenScene [44]: CLASS NAME in scene. We evaluate two widely used backbones in our experiments: Point Transformer V3 (PTv3) [63] and SparseConvNet (SCN) [13]. All experiments were conducted using 4 NVIDIA RTX 4090 GPUs. For pretraining, we adhere to the default configurations provided in [63]. When using PTv3 as the backbone, the model is trained for 800 epochs with the AdamW optimizer. The learning rate is set to 0.006, with reduced learning rate of 0.0006 for the backbone blocks, and weight decay of 0.05. The OneCycleLR scheduler is employed to adjust the learning rate during training. When using SCN as the backbone, the model is trained for 600 epochs on ScanNet200 and 800 epochs on ScanNet++ and ScanNet. The SGD optimizer is employed, with learning rate of 0.05 and weight decay of 0.0001. Similar to PTv3, the learning rate is scheduled using the OneCycleLR strategy. For fine-tuning, the network is trained for 20 epochs endto-end with the Adam optimizer. constant learning rate is used, with value of 0.001 for ScanNet200 and ScanNet, and 0.007 for ScanNet++. The backbone learning rate is reduced by factor of 0.1 to stabilize training. During training, the preprocessing follows the steps outlined in [63]. The raw input points are voxelized using grid size of 0.02m, and random cropping operation is applied to ensure that the number of points in each training input remains within maximum limit, such as 102,400 points. In the evaluation phase, the input point clouds only undergo voxelization without any further cropping or sampling operations. This enables testing on full scenes, as opposed to small blocks used in previous GFS-PCS evaluations [66, 77]. By evaluating on entire scenes, it better simulates real-world scenarios and provides more realistic and comprehensive assessment of models performance. In this section, we present additional qualitative results to further illustrate the efficacy of our approach in addressing GFS-PCS tasks. These results highlight the superiority of our model in novel class generalization and segmentation quality, providing deeper insights into the design and impact of our proposed modules. Comparison with State-of-the-Art Methods. Figure 7 showcases additional segmentation results comparing our proposed framework, GFS-VL, against the previously established state-of-the-art method, GW [66], on the ScanNet200 [48] benchmark. For clarity, class colors used in the visualizations are displayed on the right side of the figure and are restricted to those present in the ground truth. These visualizations clearly demonstrate the superior performance of GFS-VL, which effectively integrates dense semantic knowledge embedded in 3D VLMs with precise guidance from few-shot samples. This synergy enables GFS-VL to achieve robust novel class generalization in the challenging benchmarks. The qualitative results highlight improved boundary delineation, more accurate segmentation of novel objects, and better overall alignment with ground truth. Despite achieving better performance, Figure 7 also exposes some limitations. Specifically, our model exhibits suboptimal performance on small objects (e.g., Trash Can in the third row), thin objects (e.g., Curtain in the third row), and objects within complex backgrounds (e.g., Bathroom Vanity in the first row). Addressing these challenges presents promising directions for future work. Improvements in Pseudo-label Quality. In Figure 8, we provide additional visualizations of the refinement process for raw pseudo-labels, illustrating the role of our Pseudolabel Selection (PS) and Adaptive Infilling (AI) modules. PS filters noisy predictions from the 3D VLM by anchoring them to the accurate few-shot samples, ensuring high reliability. AI discovers novel objects that were initially missed in the raw pseudo-labels, as indicated in the red circles in Figure 8, and completes partially segmented regions, as shown in the green circles. By integrating few-shot support samples with the current pseudo-label context, the AI module creates adaptive prototypes that facilitate both the discovery of missed novel objects and the completion of partial pseudo-labels, thereby enhancing the quality of novel region labels. Together, PS and AI play distinct yet complementary roles in pseudolabel refinement. By combining dense knowledge from 3D VLMs with the precision of few-shot samples, these modules significantly improve pseudo-label quality, achieving better alignment with the full-class ground truth. For visualization, the class colors in Figure 8 are dis3 Figure 7. Qualitative comparison between GW [66] and our GFS-VL on ScanNet200. The visualizations demonstrate the superior segmentation performance and novel class generalization capabilities of GFS-VL. For clarity, class colors are displayed on the right and are restricted to those present in the ground truth annotations. played at the top and correspond to labels present in the full-class annotations. 4 Figure 8. Visualization of pseudo-label refinement using Pseudo-label Selection (PS) and Adaptive Infilling (AI). Red circles indicate novel objects discovered by AI that were missed in the raw pseudo-labels, while green circles indicate regions where AI completes previously partially segmented areas. For clarity, class colors are displayed at the top and correspond to labels present in the full class annotations."
        }
    ],
    "affiliations": [
        "College of Computer Science, Nankai University",
        "Computer Vision Laboratory, ETH Zurich",
        "Department of Computer Science, University of Copenhagen",
        "Department of Engineering Science, University of Oxford"
    ]
}