{
    "paper_title": "When Do Transformers Learn Heuristics for Graph Connectivity?",
    "authors": [
        "Qilin Ye",
        "Deqing Fu",
        "Robin Jia",
        "Vatsal Sharan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as a testbed, we explain this phenomenon both theoretically and empirically. We consider a simplified Transformer architecture, the disentangled Transformer, and prove that an $L$-layer model has capacity to solve for graphs with diameters up to exactly $3^L$, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the training-dynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter $\\leq 3^L$) drive the learning of a correct algorithmic solution while beyond-capacity graphs drive the learning of a simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within a model's capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic."
        },
        {
            "title": "Start",
            "content": "WHEN DO TRANSFORMERS LEARN HEURISTICS FOR GRAPH CONNECTIVITY? Qilin Ye University of Southern California qilin.ye@duke.edu, {deqingfu,robinjia,vsharan}@usc.edu Duke University Vatsal Sharan Deqing Fu Robin Jia 5 2 0 2 2 2 ] . [ 1 3 5 7 9 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as testbed, we explain this phenomenon both theoretically and empirically. We consider simplified Transformer architecture, the disentangled Transformer, and prove that an L-layer model has capacity to solve for graphs with diameters up to exactly 3L, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the trainingdynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter 3L) drive the learning of correct algorithmic solution while beyond-capacity graphs drive the learning of simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within models capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) based on the Transformer architecture have demonstrated remarkable capabilities, yet their success is often shadowed by failures on tasks that demand robust, algorithmic reasoning. growing body of evidence shows that, instead of learning generalizable algorithms, these models frequently rely on brittle shortcuts and spurious correlations that exploit statistical cues in the training data (Niven & Kao, 2019; Geirhos et al., 2020; Tang et al., 2023; Yuan et al., 2024; Zhou et al., 2024b; Ye et al., 2024). This shortcut reliance contributes to poor out-of-distribution (OOD) generalization, vulnerability to adversarial prompts, and unreliability on multi-step reasoning tasks (Zou et al., 2023; Deng et al., 2024; Li et al., 2024). Evidence spans domains: in natural language inference, models pick up lexical-overlap heuristics rather than syntactic reasoning (McCoy et al., 2019; Cosma et al., 2024); and in mathematical problem solving, strong in-distribution scores often fail to transfer as problem structure or size shifts (Saxton et al., 2019; Kao et al., 2024; Zhou et al., 2025). This motivates foundational question: When and why do Transformers learn heuristics over verifiably correct algorithms, even when the task admits an algorithmic solution? To study when Transformers learn algorithms rather than shortcuts, we adopt graph connectivity as controlled testbed. Connectivity offers unique ground-truth solution: given an adjacency matrix with self-loops, reachability equals the transitive closure and is computable by classical dynamic programming (Warshall, 1962; Floyd, 1962), so the target is unambiguous. Connectivity is fundamental algorithmic problem and is one of the most well-studied problems in terms of its complexity (Wigderson, 1992). It is natural starting point since it sits low in the complexity hierarchy, undirected st connectivity is known to lie in deterministic logspace (Reingold, 2008). Recent theory further shows that Transformers with depth growing logarithmically in input length can implement nontrivial parallel algorithmsincluding connectivityvia repeated squaring-style constructions (Merrill & Sabharwal, 2025). At the same time, connectivity admits simple heuristics based on global statistics such as node degrees or local density, which can be predictive on many graphs but are misleading on others. By combining an unambiguous algorithmic target, complexity Equal Contribution 1 landscape tied to Transformer depth, and natural shortcut baselines, connectivity is principled stress test of whether training yields multi-step algorithmic composition or superficial cues. Our contributions. Despite theoretical expressivity guarantees, whether gradient descent can help In this work, our preliminary exTransformers find the algorithmic solution remains unknown. periments reveal clear algorithm-heuristic tension on graph connectivity (see 3.3 and Figure 1): Transformer models achieve perfect in-distribution accuracy but fail to generalize. We consider simplified Transformer architecture, the disentangled Transformer (Friedman et al., 2023; Nichani et al., 2024) to analyze when training recovers an algorithmic solution rather than shortcut. Finally we empirically show that the same pattern transfers to standard Transformer models. We summarize our contributions below: 1. Non-asymptotic capacity tied to diameter. Prior work establishes log-depth expressivity for connectivity: Transformer models require depth of Θ(log(n)) in the number of nodes (Merrill & Sabharwal, 2025). In Theorem 4.4, we prove non-asymptotic capacity theorem that depends on instance difficulty, characterized by the graph diameter rather than only on the number of nodes n. Let diam(G) denote the maximum shortest-path distance between any two connected nodes. We show that an L-layer model solves connectivity on all graphs with diam(G) 3L. We refer to 3L as the models capacity. We complement this by proving matching capacity bound of 3L, and we empirically validate the diameter-depth scaling by training both disentangled and standard Transformer models. 2. An algorithm-heuristic decomposition. We prove in Theorem 4.6 and 4.3 that if the model has certain symmetries such as being invariant to relabellings of the vertices of the graph, the learned weights for disentangled Transformer are superposition of an algorithmic and heuristic channel. We empirically validate that trained models have this invariance property. The algorithmic channel is responsible for multi-hop composition or the computation of matrix powers of the adjacency matrix (via repeated squaring). The heuristic channel determines if two nodes are connected based on the degrees of the two nodes, and similar higher-order generalizations of the degree based on the local neighborhood of the two nodes. 3. Training dynamics. Our analysis of the training dynamics reveals sharp dichotomy driven by the data distribution. For graphs within the models capacity (diameter 3L), population gradients suppress the heuristic channel and favor the algorithmic channel that implements matrix powering (Theorem C.5). Conversely, when the distribution contains significant share of beyond-capacity graphs (diameter > 3L) the gradients instead strengthen the heuristic channel, promoting the simple degree-counting shortcut (Theorem C.9). This precise characterization hinges on our exact 3L capacity bound; an asymptotic one, such as the O(exp(L)) result from Merrill & Sabharwal (2025), would not yield such clear predictive implications. 4. The Data Lever. These theoretical insights point to direct mitigation strategy we call the data lever: restricting the training data exclusively to within-capacity graphs. Our experiments in 5 confirm the effectiveness of this approach, showing that it boosts the algorithmic component and improves out-of-distribution robustness (Figure 4), and that these benefits transfer successfully to standard Transformer models (Figure 7). With graph connectivity as testbed, our results together pinpoint precise breaking points of Transformers and how the training data influences the learning of generalizable algorithmic components versus brittle heuristics. Our analysis yields strategy to reduce dependence on heuristics, via the data lever, demonstrating that the theory also has some prescriptive power."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Computational Complexity and Expressivity of Transformers.Theoretical analyses aim to define what Transformers can and cannot compute. Although Transformers are universal approximators for continuous sequence-to-sequence functions (Yun et al., 2020), they also face sharp complexitytheoretic limits. Fixed-depth attention struggles with periodic or hierarchical patterns (Hahn, 2020), and standard Transformers are restricted to the complexity class TC0 (Merrill & Sabharwal, 2023), with hard-attention variants also confined to low-level circuit classes (Hao et al., 2022; Barcelo et al., 2024). This computational power can be expanded. Allowing model depth to scale logarithmically with input length enables recognition of regular languages and solves graph connectivity (Merrill & Sabharwal, 2025), while chain-of-thought generation also strictly increases expressivity (Merrill & Sabharwal, 2024). Programmatic abstractions like RASP offer another lens, identifying which 2 algorithms can be implemented in length-generalizing way (Weiss et al., 2021; Zhou et al., 2023). Empirically for the graph connectivity problem, Fu et al. (2024b) shows frontier LLMs can reach almost perfect performance on small graphs and Saparov et al. (2025) shows transformer has greater difficulty in learning the task when graph size increases. Additionally, Sanford et al. (2024) proves that logarithmic depth is both necessary and sufficient for parallelizable graph tasks, with supporting GraphQA evidence. Mechanistic Interpretability of Transformers. growing body of work reverse-engineers the algorithmic circuits that Transformers learn for tasks like copying, induction, and reasoning (Elhage et al., 2021; Olsson et al., 2022; Wang et al., 2022; Brinkmann et al., 2024). These can range from Fourier-style circuits for modular addition (Nanda et al., 2023; Zhou et al., 2024a) to Newton-like updates for in-context linear regression (Fu et al., 2024a). Researchers validate hypotheses by compiling programs into model weights (Lindner et al., 2023), decompiling models into code (Friedman et al., 2023), and using causal interventions to localize function (Chan et al., 2022; Meng et al., 2022; Yao et al., 2024; Chang et al., 2024). On the other hand, Wen et al. (2023) show that head-byhead mechanistic explanations can mislead: even on simple parentheses task, transformers spread stack-like computation across many parts of the network. Finally, theoretical work on inductive biases, like preference for low-sensitivity functions, helps explain why models often favor robust heuristics over exact algorithms (Vasudeva et al., 2025)."
        },
        {
            "title": "3 PROBLEM SETUP AND PRELIMINARY STUDY",
            "content": "3.1 GRAPH CONNECTIVITY TASK Definition 3.1 (Self-loop-augmented adjacency matrix). Let = (V, E) be finite simple graph with vertices. We define the self-loop-augmented adjacency matrix {0, 1}nn as: Ai,j = 1 if {vi, vj} or = j, and 0 otherwise. This definition is equivalent to taking the standard adjacency matrix and adding the identity matrix (AG + In). key consequence is that the (i, j)-th entry of the matrix power Ak counts the number of walks of length from vi to vj. With self-loops, these walks may stay at the same vertex from one step to the next. Henceforth, adjacency matrix will refer to this self-loop-augmented version. Definition 3.2 (Connectivity). For any graph = (V, E) with nodes, we define the connectivity matrix {0, 1}nn as follows: Ri,j = 1 if there is path between vi and vj and 0 otherwise. In particular, Ri,j = 1 if and only if [An]i,j > 0. Our learning objective is to learn models : {0, 1}nn Rnn. For graph with adjacency matrix and connectivity matrix R, if satisfies [M(A)]i,j > 0 Ri,j = 1, then we say is perfect on graph G. We train on Erdos-Renyi ER(n, p) graphs: on vertices, each edge is present independently with probability p. 3.2 TRANSFORMER ARCHITECTURES We first introduce our setups on standard transformer models without causal attention masking. Definition 3.3 (Transformers for Graph Connectivity). Let be the self-loop-augmented adjacency matrix of G. Fix depth and hidden width > n. Define the linear read-in and read-out maps ReadIn(X) := XWin, Win Rnd, ReadOut(H) := HW out, Wout Rnd. An L-layer single-head transformer model for graph connectivity acts as TransformersL(cid:0)ReadIn( A)(cid:1)(cid:17) Θ(A) := ReadOut TFL (cid:16) Rnn where TransformersL is standard pre-norm Transformer with self-attention and with no causal attention masks. There is no additional positional encoding since In is already added to the input as the absolute positional encoding. full definition can be found in Definition A.1. 3.3 PRELIMINARY STUDY We train 2-layer Transformer models on ER(n = 20, = 0.08) graphs and test them on two out-of distribution datasets: (1) 2Chain(n = 20, = 10) graphs with nodes consisting two isolated chains each with nodes, and (2) 2Clique(n = 20, = 10) graphs with nodes consisting two Figure 1: We train 2-layer Transformers models on Erdos-Renyi graphs. (Left) Visualization of input, target and model prediction of sample graph. (Right) Although trained models are able to predict perfectly on every edge within distribution, they failed to generalize to out-of-distribution graphs such as graphs with two isolated chains or cliques. isolated k-Cliques. We measure the performance of model via exact match accuracy on our graph distribution G, i.e., the fraction of graphs on which is perfect. Formally, its defined as ExactMatchAcc(M, G) = EG=(V,E)G (cid:105) vi,vj 1 {[M(AG)]i,j = [RG]i,j} (cid:104)(cid:81) . Transformers Fail to Generalize. As shown in Figure 1, the 2-layer Transformer model is able to achieve almost perfect exact match accuracy on the held-out set of the training distribution. However, it fails to learn an algorithmic solution which would transfer to other distributions. When the model is tested on the 2Chain and 2Clique distributions, its exact match accuracy falls to nearly zero, indicating over-fitted heuristics have dominated the model prediction. We repeat the experiments via extensive hyperparameter search and scaling up the number of layers, but all models fail to generalize. This motivates us to further investigate this generalization failure on why transformers prefer learning heuristics, when this happens during the training dynamics, and if models can be mitigated to learn actual algorithmic solutions instead of overfitting heuristics."
        },
        {
            "title": "4 THEORY",
            "content": "4.1 DISENTANGLED TRANSFORMER To understand the generalization failure in 3.3 theoretically, we pivot to simplified disentangled Transformer which helps us with not only expressivity/capacity analysis in 4.2 but also with training dynamics analysis in 4.3. In the disentangled Transformer, each attention block appends its output as new coordinate slice of the residual stream rather than summing, so the representation dimension grows with depth and the read/write pathways become traceable (Friedman et al., 2023; Nichani et al., 2024). This model is more than theoretical convenience; it serves as valid proxy for its standard counterpart. Nichani et al. (2024) show that any standard attention-only transformer can be re-expressed as disentangled model by specializing attention to implement feature concatenation, and Chen et al. (2024) adopt this architecture precisely because it preserves the computations of interest while being markedly more amenable to theoretical analysis. We now formalize the model. Definition 4.1 (Disentangled Transformer for Graphs). Let be the number of nodes for any graph with adjacency matrix Rnn. Let be the depth of the disentangled transformer, and {d0, d1, , dL} be the set of dimensions of its hidden states with dℓ = 2ℓ+1n. Let {Wℓ}L ℓ=1 be the attention matrices with Wℓ Rdℓ1dℓ1 . Let WO RndL = [In, , In] be the output matrix. Let Θ = {Wℓ}L Θ acts on any graphs self-loop ℓ=1. An L-layer disentangled transformer TFL 4 augmented adjacency matrix by Input hidden state Hidden states at layer ℓ h0 := [In, A] Rnd0 hℓ := [hℓ1, Attn (hℓ1; Wℓ)] Rndℓ Output layer"
        },
        {
            "title": "TFL",
            "content": "Θ(A) := hLW where Attn (hℓ1; Wℓ) := 1 ReLU (cid:0)hℓ1Wℓh ℓ (cid:1) hℓ1 (1) (2) We remark that hℓ Rndℓ where dℓ = 2ℓ+1n grows exponentially with respect to ℓ."
        },
        {
            "title": "4.2 EXPRESSIVITY AND CAPACITY",
            "content": "If 2-layer transformer fails to generalize in 3.3, should we attribute this to the architectures expressivity? We argue not. Theorem 4.3 shows that an L-layer disentangled transformer can implement the matrix powering algorithm and is perfect on graphs of diameter at most 3L. Moreover, Theorem 4.4 shows this 3L threshold is tight and exact. To make this precise, we first formalize graph distance and diameter in Definition 4.2. Definition 4.2 (Graph distances and diameter). Let = (V, E) be finite, simple, undirected graph. Following standard definitions, for u, , we let dG(u, v) be the shortest-path distance between u, v, which is finite if they are connected and infinite otherwise. For connected component, we define its diameter to be the longest path length within the component. Throughout, we define the diameter of graph, denoted diam(G), to be the maximum diameter among its connected components. Note this differs from the common convention on disconnected graphs, where the latter sets maxu,v dG(u, v) = . Ours is always finite. We begin by establishing the expressive power of the disentangled transformer, showing that with sufficient depth, it can implement the correct matrix powering algorithm to solve connectivity. Theorem 4.3 (Expressivity). There exists an L-layer disentangled transformer that makes perfect predictions for every graph satisfying diam(G) 3L. Sketch of proof. For all ℓ, setting Wℓ = Idℓ1 suffices. These choices of weights implements the matrix powering algorithm (cid:80)diam(G) αjAj with nonnegative coefficients αj > 0. j= The expressivity result shows what is possible, we next show capacity bound which reveals the models inherent limitations. We now prove tight, non-asymptotic upper bound on the graph diameter an L-layer model can handle, linking model depth directly to instance difficulty. Theorem 4.4 (Capacity). Fix 1 and let TFL Θ be an L-layer disentangled transformer on = Ω(3L) nodes. Further assume that the weights Wℓ 0 for each ℓ. Then there exists graph with diameter > 3L on which TFL Θ(A) is not perfect. In other words, diameter 3L upper bounds the capacity of any L-layer disentangled transformer. In particular, taking (7/3) 3L + 2 suffices. Sketch of proof. We split by whether false positive across different connected components occurs at some intermediate layer; the full proof can be found in Section B.2. Case 1 (False positive occurs Lemma B.1). Suppose for some graph and layer ℓ, positive score appears at (u, v) in different components. Take ℓ minimal (the earliest layer across all graphs where false positive emerges). We will create new graph that (i) preserves this false positive on (u, v) and (ii) contains path of length > 3L. To do so, we backtrack the computation DAG, tracing the sources that contribute to the false positivity of (u, v). This gives us two subgraphs (one for u, one for v) with roots at layer ℓ 1 that we call certificates. Because ℓ is minimal, every previous layer is free of false positives; hence, the two certificates induce two disjoint sets of graph nodes. Finally, create new graph which embeds these two certificates and arranges all other nodes into long chain. Then has the properties we seek. Case 2 (No false positives Lemma B.2). Suppose now that no intermediate layer has false positives. We show that information spreads no faster than 3L so that it never predicts Yes on node pairs with distance beyond 3L. We first apply the no-false-positives assumption on the empty (self-loopsonly) graph. Inductively, each column of each hidden states is supported on exactly one row, which 5 Figure 2: Capacity of Disentangled Transformers. We train 2-layer (left) and 3-layer (right) disentangled transformers on ER(n = 24) and ER(n = 64) graphs respectively. When evaluated on hold-out sets, both models can only make reliable predictions ( 99% accuracy) on node pairs u, if and only if dG(u, v) 3L. It resonates with our theoretical observations in Theorem 4.4. ranges from 1 to n. This naturally gives label for each column in each hidden states. The crux of the proof is to inductively show that at layer ℓ, two columns can share information, thereby creating positive score on (u, v), only if their labels, interpreted as graph nodes, are within distance 3ℓ. Consequently, no-false-positive model cannot recognize connected pair with distance > 3L. In both cases one can construct graphs with diameters > 3L on which TFL Θ is not perfect. Given the tight 3L capacity bounds for transformers, it is natural and crucial to introduce dichotomy around the 3L capacity. For any connected node pair (u, v), they are said to be within capacity if dG(u, v) 3L and beyond capacity otherwise. Formally, we define the dichotomy as follow: Definition 4.5 (Within-capacity and beyond-capacity pairs at depth L). Fix graph and depth L. We say pair of nodes (i, j) is within capacity if [A3L ]i,j > 0 and beyond capacity otherwise. In other words, pair (i, j) is within capacity iff their shortest-path distance is 3L. 4.3 TRAINING DYNAMICS If capable 2-layer Transformer is able to perfectly predict connectivity up to path length 32 = 9, and the 2Chain(n = 20, = 10) dataset does not contain longer paths, why didnt the Transformer model in 3.3 learn the algorithm? In this section, we show that this is because the training distribution contains too many graphs beyond the 3L capacity, and those samples reward global shortcut over algorithm. Equipped with Theorem 4.6, we can analyze the gradient dynamics in the twochannel parameterization (a superposition of heuristic and algorithmic channels). Theorems C.5 and C.9 give simple criterion made possible by the exact 3L characterization: if within-capacity pairs dominate, the algorithmic channel wins; if beyond-capacity pairs prevail, the shortcut wins. Parameterizing Model Weights. We first need to formalize the criteria that define good TFL Θ, especially if ER(n, p) exceeds the models capacity described in Theorem 4.4. The first notion is equivariance. Our data and targets are symmetric under node relabeling, in the sense that the ground truth mapping that maps adjacency matrices to connectivity matrices also maps AP (cid:55) RP for any permutation matrix . It is therefore natural to demand that good models output transform in the same way, i.e., TFL Θ(P AP ). We use generalized notion described in Appendix C.1. In addition, models attaining the tight capacity bound do so by learning the powering algorithm, so reasonable second constraint is to enforce this: we require that TFL ). In Theorem C.2 we pin down the exact characterization of Wℓ under these two conditions: the (2ℓn) (2ℓn) matrix Wℓ + ℓ must equal the Kronecker product of some (2ℓ 2ℓ) matrix Λℓ with In. be identically supported, i.e., supp(TFL Θ(A)) = supp(A3L Θ(A)P = TFL Θ(A) and A3L For our analyses, however, we use stronger assumption of layerwise attention equivariance, defined and characterized as follows: Theorem 4.6 (Layerwise Permutation-Equivariant Parameterization). Suppose an L-layer Disentangled Transformer TFL Θ is layer-wise permutation equivariant, i.e., for each ℓ and any hidden states Rndℓ1 , Θ has non-negative weights, i.e., Wℓ 0 for all ℓ. Then TFL Attn(P h(IKℓ1 ); Wℓ) = Attn(h; Wℓ) (IKℓ1 ), 6 if and only if Wℓ = Aℓ In + Bℓ Jn for some Aℓ, Bℓ R2ℓ2ℓ Kronecker product and Jn = 11T the all-ones (n n) matrix. for all ℓ, where denotes the Sketch of proof. Sufficiency is immediate: If Wℓ admits this form, then conjugating by any node permutation leaves both factors invariant, as InP = In and JnP = Jn. For necessity, the key observation is that with ReLU inactive due to non-negativity assumption, the layer map becomes bilinear in h. With algebra, the equivariance assumption can be shown to imply conjugation-alike identity on weights: Writing σ(P ) = IKℓ1 and = σ(P )Wℓσ(P ) Wℓ, the following must hold: hhhσ(P ) = 0 for all 0. We then argue that this forces = 0, i.e., Wℓ is conjugation-invariant, by testing the above equation with special matrices . Next, we inspect small blocks Wℓ[u, v] of size nn in Wℓ R(2ℓn)(2ℓn), and argue that Wℓ[u, v] must commute with all permutations . This forces each Wℓ[u, v] to lie in span(In, Jn). Aggregating all subblocks, Wℓ can therefore be decomposed as AℓIn+BℓJn. This form is closed under gradients (Theorem C.4). As result, the training dynamics yield clean characterization via two channels: the algorithmic, powering mechanism via Aℓ, and the degreecounting heuristic via Bℓ, detailed immediately below. Importantly, we validate the layerwise attention equivariance assumption by showing models trained on these graphs rapidly display (approximate) layerwise equivariance in their hidden states (Figure 8). In addition, we show applicability of our training dynamics characterization on models not forced to be equivariant in 5. The Two-Channel Regime. Under the conditions of Theorem 4.6, each layer weight splits as Wℓ = Aℓ In + Bℓ Jn. This yields two functionally distinct channels (cf. equation 11): The algorithmic In-channel (Aℓ In). It implements matrix powering algorithm: across layers one realizes combinations of Ak (k-hop features), and the final readout layer combines them to compute (cid:80)diam(G) αjAj in Theorem 4.3 and achieves the capacity bound in Theorem 4.4. j=1 The heuristic Jn-channel (Bℓ Jn). The factor Jn = 11 is rank-1 and satisfies, for vector x, Jnx = (1x) 1, AJn = (A1)1 = 1, JnA = 1 (1A) = 1 d, and more generally AkJn = (Ak1)1. Thus any use of the Jn-channel reduces to functions of := Ak1, the k-walk degree vectors (with d(1) = the usual degree vector with di = d(k) (cid:80)n j=1 Ai,j the degree of node i.). Consequently, Bℓ Jn contributes degree-counting heuristics and their k-walk generalizations, shared across nodes via broadcast. Equipped with this algorithm-heuristic dual-channel view, we can analyze the training dynamics by tracking the two parameters Aℓ and Bℓ. Before that, we make certain mild assumptions on training data distribution, model parameterization and loss objectives. Assumption 4.7. When analyzing training dynamics of the disentangled transformers, we make the following assumptions 1. Data Distribution. Let ER(n, p) be the Erdos-Renyi distribution with edge-probability (0, 1). Assume PGER(n,p){G is disconnected} is bounded away from 0. 2. Nonnegativity & Equivariant Parameterization. For each layer ℓ, we assume Wℓ 0 and analyze the permutation-equivariant family span{In, Jn} on the node side, i.e., Wℓ = Aℓ In + Bℓ Jn, Aℓ, Bℓ R2ℓ2ℓ . (3) 3. Surrogate Loss. Given scores := TFL Θ() Rnn 0 , define the link ϕ(z) := 1 eαz with α > 0;the entrywise Bernoulli cross-entropy with respect to the connectivity matrix is L(Z; R) := (cid:88) i,j (cid:0)Ri,j log ϕ(Zi,j) + (1 Ri,j) log(1 ϕ(Zi,j))(cid:1). (4) Its gradient with respect to is Z = α (1 R/ϕ(Z)) Rnn. 7 Figure 3: Training Dynamics of Disentangled Transformers. We train an 1-layer disentangled transformer on graphs from ER(n = 8, = 0.2) distribution. Weight will approximately approach to In + Jn form. (Left) There are two major phases during training, where during Phase 1, model focuses on learning the equivariant parameterizations so both and channels share of energy in increases, and during Phase 2, the algorithmic I-channel is promoted and the heuristic J-channel is suppressed. (Right) Visualization of the learned weights during training and its projection to the closest ˆW = ˆA In + ˆB Jn form. Temporal Evolution. Roughly speaking, training consists of two phases. Phase 1: Both channels pick up easy examples. In early updates, both channels quickly ramp up mass because there are plenty of within-component, within-capacity pairs. Concretely, the local I-channel composes neighborhood information, while the global J-channel can also boost underpredicted positives without facing much penalty (Remark C.7). Phase 1 ends once those easy connected pairs are mostly saturated, at which point false positives on cross-component terms become decisive, and the data-driven dichotomy of Phase 2 takes over. Overall, Phase 1 is transient: in Figure 3 (left), it only occupies around 2 102 steps out of 104 total. Phase 2: Data determines which channel wins. Once in this regime, the growth of Bℓ is determined by the population-level balance (see Theorem C.5 for full details and notations): informally, (cid:34) Derivative of Bℓ = α (cid:88) Di,j Ri,j =0 (cid:124) (cid:123)(cid:122) (cid:125) penalty from cross component (cid:35) . (5) Di,j (cid:88) 1 ϕ(Zi,j) ϕ(Zi,j) (cid:123)(cid:122) weighted reward on under-predicted positives Ri,j =1 (cid:124) (cid:125) There are two outcomes, depending on the sign of equation 5. If batches carry significant mass of disconnected, within-capacity graphs, cross-component penalty dominates and suppresses the Jchannel. Under these conditions, the only KKT stationary point in the B-coordinates is then Bℓ = 0 (Theorem C.5(ii)), effectively only implementing the powering algorithm, making the model largely algorithmic. For instance, on Figure 4 (right, restricted), this shows up over the middle-to-late training window as the A-share climbs toward 1 while the B-share decays to 0. Conversely, if the batches are rich in connected, beyond-capacity graphs, training instead promotes the J-channel, activating the dense degree-style, making the model largely heuristic. helpful way to interpret Phase 2 is via mixture of graph types. The per-sample results (Theorem C.9) say: within-capacity graphs reward the local I-channel and, when disconnected, penalize any (falsely) activated J-channel; large-diameter connected graphs do the opposite and push J-channel up. Aggregating over the data distribution leads to the conclusion that the outcome of Phase 2 depends on the fraction of beyond-capacity connected graphs (Remark C.13). Figure 6 visualizes the effects of various mixtures. It is possible that Ri,j = 1 while Zi,j = 0, resulting in undefined gradient L/Z. To circumvent this, we approximate via ϕϵ = 1 (1 ϵ)eαz. All subsequent analyses hold verbatim by replacing ϕ with ϕϵ. 8 Figure 4: Following insights from Theorems C.5 and C.9, we repeat the same experiment setup as in Figure 3 but only training on within-capacity graphs (see Definition 4.5 and the left figure). As shown in the solid lines in the right figure, restricting training samples by capacity pushes the energy share of the algorithmic mechanism (the In channel) further to nearly 100% in the weight . It simultaneously prevents the growth of the heuristic portion (the Jn channel)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We test the theory in two parts. First, in 5.1 we verify the 3L capacity threshold (Theorems 4.3 and 4.4) by measuring the maximum reliable path length one model can handle perfectly. Then, we trace training dynamics by projecting learned weights into the algorithmic channel and the heuristic channel (Theorem 4.6). Next, in 5.2 we introduce simple data lever that upweights within-capacity graphs, and shows this simple method suppresses the heuristic and promotes the algorithmic channel, as predicted by Theorems C.5 and C.9. Finally, we show this data lever prescribed by our theoretical analysis on disentangled transformers can transfer back to standard Transformers, and boost their generalization capabilities. 5.1 CAPACITY AND TRAINING DYNAMICS L-layer Transformers Hit Their Capacity at Exactly 3L. We train disentangled transformers with 2 layers or 3 layers on Erdos-Renyi graphs with 24 or 64 nodes respectively. As shown in Figure 2, neither of the two models could make reliable predictions on node pairs (u, v) with dG(u, v) > 3L but their predictions on node pair with dG(u, v) 3L are almost perfect with an > 99% accuracy. As shown in Figure 10, 2-layer standard Transformer model also have the same capacity. It resonates with our exact capacity bound of disentangled transformers in Theorem 4.4 and justifies our dichotomy in Definition 4.5. Overall, for any graph = (V, E), the decisive factor for transformer model depth is not simply the asymptotic Θ(log ) relation to the number of nodes = but more importantly the non-asymptotically exact relation to log3 diam(G). It allows us to better understand the training dynamics in 4.3 with dichotomy, something impossible without an exact relation. We note that our experiments did not enforce non-negativity of model weights and it demonstrates the predictive power of our theoretical analysis. Transformers Learn an Algorithm-Heuristic Mixture. In understanding the training dynamics, we first train 1-layer disentangled transformer model on ER(n = 8, = 0.2) graphs. We did not enforce any parameterization assumptions here. As show in Figure 3, randomly initialized can converge to approximately In + Jn decomposition for some matrices A, R22 and Jn = 11 Rnn. Such parameterization also applies to deeper models as shown in Figure 9. They show the applicability of our decomposition Theorem 4.6. Then, we project the final weight onto this algebra as ˆW = ˆA In + ˆB Jn by minimizing ˆW and observe that the increases as training progresses but the share of ˆB Jn energy share (see D) of ˆA In in 2 first increases and then decreases, showing interesting training dynamics to be studied. 5.2 ENCOURAGING TRANSFORMERS TO LEARN ALGORITHMS OVER HEURISTICS Now that we understand the cause of why Transformers and disentangled transformer models learn heuristics that hurt their algorithmic computations (as shown in Figures 1 and 3), natural question is whether we can mitigate it and encourage the models to up-weight the algorithm channel. 9 Figure 5: With 1-layer disentangled transformers with capacity Cap = 3 following Theorem 4.4, we vary the such that we restrict our training graphs to have diam(G) d. We also vary the edge probability of our training distribution ER(n = 8, = ) for generality. We test on 2Chain(n = 8, = ) graphs with = 2 or 3 and show the exact match accuracy on configurations where the accuracy is non-zero for readability. We find if the training Cap, models still learns the algorithmic solution up to problem size (see = 2, = 2 case on the left in orange) but fails to length generalize (see = 2, = 3 in orange on the right). On the other hand, if the training > Cap, model struggles to learn the algorithmic solution (see = 4 cases in red on both = 2 or 3). The best case overall is when setting = Cap, i.e., preventing the model from seeing beyondcapacity samples but still preserving at-capacity samples for better generalization. As shown in the green lines, with = 3, model achieves balanced testing accuracy on both = 2 and 3. Figure 6: We vary the proportion of beyond-capacity graphs, and train the same disentangled transformer on stratified ER distribution and test on the same OOD 2Chain distribution. We find that Transformers are robust towards small amount of noises (beyond-capacity graphs). Although the is not exactly in the In form, the model still perform perfectly when the energy share of I-channel dominates (beyond roughly 90%). As shown on the right, there is linear correspondence between model accuracy (averaged over individual node pairs) and algorithmic channel share. Mitigation via the Data Lever. We propose data-centric method: instead of training on all graphs from the ER distribution, we up-weight graphs whose diam(G) is within capacity following the dichotomy in Definition 4.5. We dissect = G> into two sub-distributions where = {G : diam(G) 3L} only includes graphs containing no beyond capacity node pairs and G> includes the rest. In Figure 4, we only train the 1-layer disentangled transformer on ER, and then find the algorithmic ˆA In channel is significantly promoted so that the learned weight only contains the algorithm channel. Furthermore, we find at-capacity graphs are crucial. In the case of Figures 5 and 11, where no graphs are to have diam(G) > 2, model also fails to learn generalizable solutions due to transformers poor length generalization abilities. It implies simply scaling up the model depth wont equip it algorithmic capabilities naturally. Robustness towards Noise. To test the predictiveness of our theory, we evaluate if one beyondcapacity node pair is enough to encourage the model learning heuristic-dominated method. We define ρ(G) = EGG{(u, v) V, dG(u, v) > 3L}/n2 be the fraction of beyond-capacity node pairs in graph distribution G. In practice, ρ can be controlled via stratified sampling from the mixture distribution Gq = qG + (1 q)G>. In Figure 6, we did stratified sampling between ER and ER> and find that with small ρ(G), the model is still able to maintain high energy in the algorithm channel, and make perfect predictions on out-of-distribution 2Chain graphs. It suggests that there exists small ρ > 0 such that the model can still rely on the algorithm-channel to make predictions if the training distribution satisfies ρ(G) ρ. Transferability to Standard Transformers models. Our theory from 4.3 makes prescriptive suggestion to remove beyond capacity graphs to reduce dependence on heuristics, and Figure 4 demonstrated the effectiveness of this approach on disentangled transformers. We now evaluate this on standard Transformers. We train the same 2-layer Transformers model as in our preliminary study in 3.3 but this time, we train on the restricted distribution ER instead where all graphs ER has diam(G) 32 = 9. As shown in Figure 7, when tested on the OOD 2Chain dataset with maximum chain length 10, the one trained on ER can successfully generalize but the one trained on unconstrained distribution ER cannot."
        },
        {
            "title": "6 DISCUSSION AND CONCLUSION",
            "content": "Figure 7: Standard transformer models learn generalizable solutions from within capacity data. In this paper, we separate expressivity from capacity and training dynamics for Transformers on graph connectivity. We prove that an L-layer model can implement matrix powering and is perfect on graphs with diam(G) 3L, and we show this 3L threshold is tight. The failures in 3.3 are explained by capacity mismatch: training mass beyond 3L steers learning toward global shortcut rather than the intended multi-hop algorithmic computation. Our two-channel view makes this explicit and turns generalization into property of the data distribution: when within-capacity pairs dominate, the algorithmic channel is selected. Experiments confirm the threshold and show that simple capacity-aware data lever that up-weights within-capacity graphs suppresses the shortcut, promotes out-of-distribution generalization, and transfers to standard Transformers. By pinpointing when model reaches for shortcut and showing how simple data choices can steer it towards the true algorithmic solution, we outline path to systematically control training data and model capacity to enable Transformers to learn solutions that generalize better."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "The authors acknowledge the Center for Advanced Research Computing (CARC) at the University of Southern California for providing computing resources that have contributed to the research results reported within this publication. We also acknowledge the use of the USC NLP cluster provided by USC NLP Group. This work used the Delta system at the National Center for Supercomputing Applications through allocation CIS250737 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. DF and RJ were also supported by gifts from the USC-Capital One Center for Responsible AI and Decision Making in Finance (CREDIF) and the USC-Amazon Center on Secure and Trusted Machine Learning. RJ was also supported by the National Science Foundation under Grant No. IIS-2403436. VS was supported by an NSF CAREER Award CCF-2239265, an Amazon Research Award, Google Research Scholar Award and an Okawa Foundation Research Grant. The work was done in part while DF and VS were visiting the Simons Institute for the Theory of Computing. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the funding agencies."
        },
        {
            "title": "REFERENCES",
            "content": "Pablo Barcelo, Alexander Kozachinskiy, Anthony Widjaja Lin,"
        },
        {
            "title": "Logical",
            "content": "languages skii. (ICLR), 2024. In International Conference on Learning Representations https://proceedings.iclr.cc/paper_files/paper/2024/file/ 5f0fdc1acd47431f7f3bb8ee85598cef-Paper-Conference.pdf. 2 accepted by transformer and Vladimir Podolencoders with hard attention. URL Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt. mechanistic analysis of transformer trained on symbolic multi-step reasoning task. In Findings of the Association for Computational Linguistics: ACL 2024, 2024. 3 Lawrence Chan, Adria Garriga-Alonso, Nix Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: method for rigorously testing interpretability hypotheses. Alignment Forum, 2022. Ting-Yun Chang, Jesse Thomason, and Robin Jia. Do localization methods actually localize memIn NAACL-HLT, pp. 31903211, 2024. URL orized data in llms? tale of two benchmarks. https://doi.org/10.18653/v1/2024.naacl-long.176. 3 Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction heads: Provable training dynamics and feature learning in transformers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=4fN2REs0Ma. 4 Andrei Cosma, Viorel Andrei, Dan Istrate, and Roxana Istrate. How hard is this test set? nli characterization by dataset-driven heuristics. In EMNLP, 2024. URL https://aclanthology. org/2024.emnlp-main.175.pdf. 1 Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. In NAACL, 2024. URL https: //aclanthology.org/2024.naacl-long.482.pdf. 1 Nelson Elhage, Neel Nanda, Catherine Olsson, and et al. mathematical framework for transformer circuits. https://transformer-circuits.pub/2021/framework/index.html, 2021. Robert W. Floyd. Algorithm 97: Shortest path. Communications of the ACM, 5(6):345, 1962. doi: 10.1145/367766.368168. URL https://dl.acm.org/doi/10.1145/367766. 368168. 1 Dan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 3, 4 Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn to achieve second-order convergence rates for in-context linear regression. In Advances in Neural Information Processing Systems (NeurIPS), 2024a. 3 Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. IsoBench: Benchmarking multimodal foundation models on isomorphic representations. In First Conference on Language Modeling (COLM), 2024b. 3 Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. doi: 10.1038/s42256-020-00257-z. URL https://doi.org/10.1038/s42256-020-00257-z. Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171, 2020. doi: 10.1162/tacl 00306. 2 Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Transactions of the Association for Computational Linguistics, 10:800810, 2022. doi: 10.1162/tacl 00490. URL https://aclanthology. org/2022.tacl-1.46/. 2 12 Kuan-Chieh Kao, Shumin Deng, Fan Yang, Zheng Li, and et al. Can large language models solve In Findings of EMNLP, 2024. URL complex math problems? comprehensive assessment. https://aclanthology.org/2024.findings-emnlp.980.pdf. 1 Yao Li, Francois Guerin, and Chenghua Lin. Latesteval: Addressing data contamination in language model evaluation. Proceedings of AAAI, 2024. URL https://ojs.aaai.org/index. php/AAAI/article/view/29822/31427. David Lindner, Janos Kramar, Sebastian Farquhar, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as laboratory for interpretability, 2023. URL https: //arxiv.org/abs/2301.05062. 3 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach, 2019. URL https://arxiv.org/abs/1907.11692. 29 R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic In ACL, 2019. URL https://aclanthology. heuristics in natural language inference. org/P19-1334/. 1 Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing facIn Advances in Neural Information Processing Systems (NeurIPS), URL https://proceedings.neurips.cc/paper_files/paper/2022/ tual associations in gpt. 2022. file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531545, 2023. doi: 10.1162/tacl 00562. URL https://aclanthology.org/2023.tacl-1.31/. 2 William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In International Conference on Learning Representations (ICLR), 2024. URL https: //openreview.net/forum?id=CDmerQ37Zs. 2 William Merrill and Ashish Sabharwal. little depth goes long way: The expressive power of log-depth transformers. arXiv preprint arXiv:2503.03961, 2025. URL https://arxiv.org/ abs/2503.03961. 1, 2 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=9XFSbDPmdW. 3 Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with gradient descent, 2024. URL https://arxiv.org/abs/2402.14735. 2, Timothy Niven and Hung Yu Kao. Probing neural network comprehension of natural language arguments. In ACL, 2019. URL https://aclanthology.org/P19-1459/. 1 Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, In-context learning and induction Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. heads, 2022. URL https://arxiv.org/abs/2209.11895. 3 Omer Reingold. Undirected connectivity in log-space. Journal of the ACM, 55(4):17:117:24, 2008. doi: 10.1145/1391289.1391291. Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph algorithms, 2024. URL https://arxiv.org/abs/2405.18512. 3 Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Seyed Mehran Kazemi, Najoung Kim, and He He. Transformers strugIn International Conference on Learning Representations (ICLR), gle to learn to search. 2025. doi: 10.48550/arXiv.2412.04703. URL https://openreview.net/forum?id= 9cQB1Hwrtw. ICLR 2025. 3 13 David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. 2019. URL https://arxiv.org/abs/1904.01557. 1 Ruixiang Tang, Weiguang Shi, Mengnan Wang, Yong Chen, and Xia Hu. Large language models can be lazy learners: Analyze shortcuts in in-context learning. In Findings of ACL, 2023. URL https://aclanthology.org/2023.findings-acl.284/. 1 Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, and Vatsal Sharan. Transformers learn low sensitivity functions: Investigations and implications. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=4ikjWBs3tE. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. 3 Stephen Warshall. theorem on boolean matrices. Journal of the ACM, 9(1):1112, 1962. doi: 10. 1145/321105.321107. URL https://dl.acm.org/doi/10.1145/321105.321107. 1 Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In Proceedings of the 38th International Conference on Machine Learning (ICML), volume 139 of PMLR, pp. 1108011090, 2021. URL https://proceedings.mlr.press/v139/weiss21a/weiss21a.pdf. 3 Kaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski. Transformers are uninterpretable with myopic methods: case study with bounded dyck grammars. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=OitmaxSAUu. 3 Avi Wigderson. The complexity of graph connectivity. In International Symposium on Mathematical Foundations of Computer Science, pp. 112132. Springer, 1992. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Knowledge circuits in pretrained transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 Weili Ye, Li Shen, Peng Yang, Xiao Li, Zhiqiang Xu, and Zhi-Qin John Xu. Spurious correlations in machine learning: survey. arXiv:2402.12715, 2024. URL https://arxiv.org/abs/ 2402.12715. 1 Yifei Yuan, Shiqi Cheng, Yutai Hou, Xu Sun, and Lei Li. Do llms overcome shortcut learning? an evaluation of shortcut robustness in large language models. In EMNLP, 2024. URL https: //aclanthology.org/2024.emnlp-main.679.pdf. 1 Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations (ICLR), 2020. URL https://openreview.net/ forum?id=ByxRM0Ntvr. 2 Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? study in length generalization, 2023. URL https://arxiv.org/abs/2310.16028. 3 Rui Zhou, Hangyu Tong, Kaifu Zhang, and Xiang Li. Math for ai: On the generalization of learning mathematical problem solving. In ICLR, 2025. URL https://openreview.net/forum? id=th63j8qHa6. Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use fourier features to compute addition. In Advances in Neural Information Processing Systems (NeurIPS), 2024a. 3 14 Yuchen Zhou, Pai-Heng Gao, Xingyu Liu, Bang An, and Furong Wang. Explore spurious In ACL, 2024b. URL https:// correlations at the concept level in language models. aclanthology.org/2024.acl-long.28.pdf. 1 Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv:2307.15043, 2023. URL https://arxiv.org/abs/2307.15043."
        },
        {
            "title": "STUDIES",
            "content": "Definition A.1 (Transformer for Graph Connectivity: full specification). Input and output. Given simple graph on nodes with adjacency matrix {0, 1}nn, let = + In be its self-loop augmented adjacency matrix. We treat as input embedding: row is the token for node i; column indexes feature tied to node j. The model outputs an score matrix TFL Θ(A), the predicted connectivity matrix. Dimensions and parameters. Fix depth L, hidden dimension > n, number of heads with = Hdh, and feed-forward width dff . The parameters we need include: Win, Wout Rnd,"
        },
        {
            "title": "W Q",
            "content": "ℓ,h, ℓ,h, ℓ,h Rddh,"
        },
        {
            "title": "W O",
            "content": "ℓ RHdhd (1) ℓ Rddff , b(1) ℓ Rdff , (2) ℓ Rdff d, b(2) ℓ Rd for ℓ = 1, . . . , and heads = 1, . . . , H. We use pre-norm residual blocks with LayerNorm (LN) and GeLU activations. We do not use attention mask or any extra positional encoding; the identity in already pins each token to node. The forward map. The read-in is linear: h(0) = AWin Rnd. From there, for each ℓ = 1, . . . , L, let = LN(h(ℓ1)) as we use pre-norm. Within each block: Multi-head self-attention Attention scores Qℓ,h = hW αℓ,h = 1 ReLU(1/ ℓ,h, Kℓ,h = hW ℓ,h, dh Qℓ,hK ℓ,h), (cid:112) Vℓ,h = hW ℓ,h zℓ,h = αℓ,hVℓ,h Concatenation & residual zℓ = [zℓ,1 . . . zℓ,H ]W ℓ Rnd, Feed-forward ˆuℓ = LN(uℓ), FFNℓ(ˆuℓ) = GeLU(ˆuℓW (1) ℓ + b(1) uℓ = h(ℓ1) + zℓ ℓ + b(2) ℓ )W (2) ℓ and finally h(ℓ) = uℓ + FFNℓ(ˆuℓ) Rnd. The read-out is linear: TFL Θ(A) = h(L)W out Rnn. Metrics for Permutation Equivariance. Let Sn be the corresponding permutation matrix for any σ Sn. For given graph adjacancy matrix A, we compute the models prediction in respect to Pσ as M(PσAP σ ). Now we define equivariance consistency metric, Equivariance Consistency via Frobenius Cosine Similarity: ConsFrob(M) = EσSn,AG (cid:20) M(PσAP M(PσAP σ ), PσM(A)P σ σ )F PσM(A)P σ (cid:21) (6) When measuring intermediate model computations, this metric is modified depending on the model type. For standard transformer models, Mℓ computes the final Readout to the hidden states at layer ℓ. For Disentangled transformers we are computing PσMℓ(A)(Pσ In)."
        },
        {
            "title": "B DETAILS FOR CAPACITY",
            "content": "B.1 EXPRESSIVITY Theorem 4.3. There exists an L-layer disentangled transformer that makes perfect predictions for every graph satisfying diam(G) 3L. Proof. Set Wℓ = Idℓ1 for all layers and note that all matrices are entrywise nonnegative, so ReLU and the factor 1/n never changes supports. With h0 = [I A] = [A0 A1] and update hℓ = [hℓ1 (hℓ1h ℓ1)hℓ1/n], we can show by induction that every block of hℓ lies in span{A0, . . . , A3ℓ }, and that some block contains A3ℓ with positive coefficient. Indeed, the base case holds trivially; for the inductive step, if block within hℓ1 contains Am, then ℓ1)hℓ1 contains A2mAm = A3m. Finally, the readout simply sums over all these blocks, (hℓ1h so supp(TFL Θ(A)) = supp(A3L ). Finally, because has self-loops, supports are monotone in power and stabilizes at diam(G). Thus, if diam(G) 3L we get supp(TFL Θ(A)) = supp(Adiam(G)). B.2 CAPACITY Theorem 4.4. Fix 1 and let TFL Θ be an L-layer disentangled transformer on = Ω(3L) nodes. Further assume that the weights Wℓ 0 for each ℓ. Then there exists graph with diameter > 3L on which TFL Θ(A) is not perfect. In other words, diameter 3L upper bounds the capacity of any L-layer disentangled transformer. In particular, taking (7/3) 3L + 2 suffices. For each layer ℓ we define the post-ReLU score Rℓ = ReLU(hℓ1Wℓh ℓ1). The proof of the theorem will be partitioned into two branches: whether some intermediate Rℓ gives false positive on some graph, or all Rℓs are free of false positives on all graphs. We say pair of nodes (u, v) from is witness to false positives if they belong to different connected components while Rℓ(G)u,v > 0. Throughout this section, we set (7/3) 3L + 2. Lemma B.1. Assume the setup in Theorem 4.4. Suppose there exist some n-node graph, layer index ℓ {1, . . . , L}, and vertices u, belonging to different connected components of the graph such that (Rℓ )u,v > 0. Further assume that ℓ is globally minimal, in the sense that for all n-node graphs and all ℓ < ℓ, the corresponding Rℓ has no false positive entries across components. Then there exists graph such that diam(G) > 3L, TFL Θ(A(G))u,v > 0, where u, lie in different connected components of G. Proof. The proof roughly partitions into two parts. In the first half, we backtrack the computation DAG, tracing the sources that contribute to the false positiveness of (u, v). This gives us subgraphs which we call certificates that, if kept untouched, suffice to guarantee false positiveness of (u, v). In the second half, we construct graph that preserves these certificates while also containing path of length > 3L disjoint from both certificates, and we show that TFL Θ preserves false positiveness of (u, v) on G, thereby proving the claim. STEP 1. CONSTRUCTING THE CERTIFICATES. Intuitively, since Rℓ (H)u,v > 0, there exist column indices p, with (Wℓ )p,q > 0, hℓ1(H)u,p > 0, and hℓ1(H)v,q > 0. We will backtrack the entries that contribute to the positiveness of the hidden states entries hℓ1(H)u,p and hℓ1(H)v,q > 0 in the computation DAG, iteratively visiting previous layers. Formally, we define certificate for an entry ht(H)i,c > 0 to be small tree whose nodes are triples [of form (layer, row, column)] recording earlier entries that must be positive to guarantee that the current one is positive. The root is (t, i, c) and we build it top-down by repeating one of the two rules until we hit the first layer, which we know looks like [In A]. We now describe how to backtrack. Since ht = [ht1 Attn(ht1; Wt)], we split the recursion on layer into two cases: whether the entry lies in the first half (ht1) or the second half (Attn(ht1; Wt)). (First half) If column is in the inherited block of ht, add single child (t 1, i, c) to (t, i, c), as the value is simply copied from the previous layer ht1. 17 (Second half) If column is in the newly appended block, then by definition ht(H)i,c = 1 (cid:88) Rt(H)i,kht1(H)k,c for some c, and since this is sum of nonnegative terms, there exists at least one with Rt(H)i,k > 0 and ht1(H)k,c > 0. In turn, Rt(H)i,k = (cid:88) r,s ht1(H)i,r(Wt)r,sht1(H)k,s > 0, which implies that there exist indices r, with (Wt)r,s > 0, ht1(H)i,r > 0, and ht1(H)k,s > 0. Thus, for such (t, i, c), we create three children: (t 1, k, c), (t 1, i, r), (t 1, k, s). Now let s(t) denote the maximal number of vertices needed to realize single certificate for some entry ht() > 0 by the recursive procedure above. At = 0 we may assume s(0) 2. The recursion gives s(t) 3s(t 1), so s(t) 2 3t. Since u, lie in different connected components of H, and ℓ is minimal, every index selected by certificate at any layer ℓ 1 stays within the same component as its i, so the two certificates induce trees Tu, Tv that occupy disjoint vertex sets Su, Sv, with Su Sv 4 3ℓ1 4 3L1 vertices. STEP 2. BUILDING NEW GRAPH. Initialize to the edgeless graph, keeping node isolated. We then embed Tu, Tv onto by adding edges according to the trees. Finally, we connect every vertex outside Su Sv arbitrarily into long chain. We claim is the graph we seek. On one hand, every sum used by the certificates is sum of nonnegative terms, and we have preserved strictly positive summand at each step that appears in the tree. Hence Rℓ (G)u,v > 0 with u, also disconnected in G. On the other hand, under the choice of specified by Theorem 4.4, there exist at least 3L + 2 vertices outside Su Sv, so connecting them into long path guarantees diam(G) > 3L. The claim then follows. Lemma B.2. Assume the setup in Theorem 4.4. Further assume that for every n-node graph and every layer ℓ {1, . . . , L}, the post-ReLU scores Rℓ(G) has no positive entry between distinct connected components of G. Then, for every graph and every u, (G), if TFL Θ(A(G))u,v > 0, we must have distG(u, v) 3L. Consequently, if contains connected component of diameter > 3L then TFL Θ is not perfect on G. Proof. Under the no-false-positives assumption, the idea is to show that information spreads no faster than power base 3 so TFL Θ never predicts Yes on node pairs with distance beyond 3L. Concretely, columns exchange information as attention scores are calculated. We first define the distances between columns by giving each column label {1, . . . , n}, and then show that by layer ℓ, two columns can share information if and only if their labels, interpreted as graph nodes, are within distance 3ℓ. STEP 1. GIVING EACH COLUMN LABEL. We first consider trivial graph G0 with isolated nodes: immediately h0(G0) = [In In] and, by hypothesis, every Rℓ(G0) have no off-diagonal positives. Inductively this shows that every column of hℓ(G0) has support in exactly one row. We define the label of this column to be the row index {1, . . . , n} where the unique support is. With labels defined, the remaining proof is based on establishing the following locality claim. CLAIM. Fix graph G, layer ℓ, and i, {1, . . . , n}. If column of hℓ(G) has label and if hℓ(G)i,c > 0, then distG(i, j) 3ℓ. In other words, every column spreads at most 3ℓ hops away from its label by depth ℓ. STEP 2. ESTABLISHING THE CLAIM. We prove this claim via induction. The base case ℓ = 0 directly follows from the fact that h0(G) = [In A(G)]. For the inductive step, we assume that the claim holds at depth ℓ 1 with radius 3ℓ1. As in Lemma B.1, there are two column types in hℓ: inherited or newly appended columns. The former case is easy; if is inherited from hℓ1, then 18 hℓ(G)i,c = hℓ1(G)i,c, so the bound follows from the inductive hypothesis. We now assume is newly appended. Suppose (Rℓ(G)hℓ1(G))i,c > 0 for column with label j. Then there exists row with Rℓ(G)i,k > 0 and hℓ1(G)k,c > 0. By the IH, distG(k, j) 3ℓ1. Then we expand Rℓ(G)i,k > 0 to obtain column witnesses p, q, with hℓ1(G)i,p > 0, hℓ1(G)k,q > 0, and (Wℓ)p,q > 0, as in Lemma B.1. Let a, be the labels of p, q, respectively. By IH again, distG(i, a) 3ℓ1 and distG(k, b) 3ℓ1. We now split the analysis into two cases. If = b, we derive contradiction to the no-false-positives assumption by reusing the certificate procedure from Lemma B.1. Because Wℓ 0 entrywise, every positive entry in ht() admits certificate supported on at most s(t) 2 3t vertices. In particular, there exist certificates witnessing hℓ1(G)i,p > 0 (labeled a) and hℓ1(G)k,q > 0 (labeled b). Let Sa, Sb be the corresponding certificate vertex sets. Form new graph on the same vertices whose connected components are two disjoint induced copies of the subgraphs on Sa, Sb (leaving all other vertices outside Sa Sb isolated). Note this is feasible because Sa + Sb 4 3L1 assumed by Theorem 4.4. By construction, there exist S with hℓ1(G)i,p > 0 and hℓ1(G)k,q > 0. Thus, and S a, (hℓ1(G)Wℓhℓ1(G))i,k hℓ1(G)i,p(Wℓ)p,qhℓ1(G)k,q > 0, meaning Rℓ(G)i,k > 0. But i, belong to different connected components in G, contradiction! Therefore, = b. Triangle inequality gives distG(i, j) distG(i, a)+distG(a, k)+distG(k, j) 33ℓ1 = END PROOF OF CLAIM / STEP 2. 3ℓ, completing the induction. Θ(A(G)) = hL(G)W The models output TFL is an entrywise nonnegative sum over the blocks of hL(G). Since each block respects the 3L locality bound, we have TFL Θ(A(G))u,v = 0 whenever distG(u, v) > 3L. Hence, on any graph whose largest component has diameter > 3L, the model will inevitably miss pair (u, v) of nodes realizing this diameter. Proof of Theorem 4.4. Combine Lemmas B.1 and B.2."
        },
        {
            "title": "C DETAILS FOR TRAINING DYNAMICS",
            "content": "C.1 CHARACTERIZING BLOCK WEIGHTS Wℓ As discussed in Section 4.3, due to the symmetric nature of the graph connectivity problem, it is natural to demand that good model should map not only adjacency matrices to connectivity matrices R, but also AP to RP for any permutation . We further generalize equivariance. Observe that given permutation matrix and any hidden states Rn(kn) consisting of consecutive n, the mapping (cid:55) h(IK ) relabels both rows and columns within each block in way that is consistent with the effects of . Hence, the notion of equivariance can be generalized to any (nonnegative) hidden states, beyond just the ones induced by adjacency matrices. Similarly, we are now also able to define an L-layer disentangled transformer on arbitrary inputs of appropriate dimensions. For any nonnegative initial state h0 Rn2n, recursively define hℓ = [hℓ1 Attn(hℓ1; Wℓ)] for ℓ = 1, . . . , L. Let Sum(h) denote the sum of the consecutive leftaligned blocks of h. Then the generalized output is TFL Θ(h0) = Sum(hL). We define two equivariance-related conditions. The first one is direct generalization of TFL Θ(A)P = TFL Θ(P AP ); the second one, as discussed in Section 4.3, makes theoretical analysis significantly more tractable while also being supported by empirical evidence. Definition C.1 (Output Equivariance and Layerwise Attention Equivariance). Let TFL layer disentangled transformer with nonnegative weights. Let Kℓ = 2ℓ+1. (i) For h0 Rn2n Θ is output-level value Θ be an Land for any , define hP Θ(h0)P = TFL Θ(hP 0 = h0(IK0 ). We say TFL 0 ) holds for all and all h0 Rn2n equivariant iff TFL 0 . 0 (ii) We say TFL Θ is layer-wise attention equivariant iff for each ℓ and any hidden states Rndℓ1 (i.e., any hidden states of dimension feasible for layer ℓ), Attn(P h(IKℓ1 ); Wℓ) = Attn(h; Wℓ) (IKℓ1 ), Theorem C.2 (Parameterization of Good Models). Let = Ω(3L) as in Theorem 4.4. Fix an L-layer Disentangled Transformer TFL Θ with nonnegative weights. Suppose that (i) TFL (ii) TFL Θ is output-level value-equivariant, and Θ reaches its capacity bound of 3L, supp(A3L ). i.e., for every graph, we have supp(TFL Θ(A)) = Then, either TFL ℓ, there exists nonnegative matrix Λℓ RKℓ1Kℓ1 such that Wℓ + words, Wℓ can be decomposed into this form up to an antisymmetric part. Θ or functionally equivalent version of it satisfies the following: for each layer ℓ = Λℓ In. In other (Note that the theorem is direct generalization of equivariance under all graph permutations; replacing h0 by [In A] gives the desired result for fixed graph with adjacency matrix A.) Proof. To prove the claim, it suffices to show that if we partition Wℓ into Kℓ1 Kℓ1 contiguous sub-blocks of size n, then each block must be diagonal, with symmetry conditions meeting Wℓ + ℓ = Λℓ In. To do so, the proof is split into two parts: we prove that each block must be diagonal using (ii) and Lemma B.2, and that the diagonal entries must realize the said forms by examining the forward maps under curated, parameterized class of initial hidden states. STEP 1: EACH BLOCK MUST BE DIAGONAL. In this step, we argue that if block admits positive off-diagonal entry, then the certificate trick from Lemma B.1 will create false positive entry on some output, contradicting (ii). Formally, let Rℓ = ReLU(hℓ1Wℓh ℓ1). If for some graph and some ℓ, there exists false positive entry (Rℓ)i,k > 0 for some i, across different connected components, then the false positiveness would persist to the output, contradicting (ii). Hence TFL Θ must have no false positives. 20 Consider feeding the graph G0 of isolated vertices into TFL Θ, so that h0(G0) = [In In]. The premises of Lemma B.2 hold, so every column of every hℓ(G0) is supported in exactly one row, which we called its label in {1, . . . , n}. Hence, if we write hℓ(G0) = [X (ℓ) ] of 1 contiguous blocks, then each such block (ℓ) must be nonnegative and diagonal. Now expand . . . (ℓ) Kℓ Rℓ = ReLU(hℓ1Wℓh ℓ1) = hℓ1Wℓh ℓ1 = (cid:88) r,s (ℓ1) Wℓ[r, s](X (ℓ1) ). We first claim that every sub-block Wℓ[r, s] is diagonal. Suppose not, that there exist indices r, and distinct nodes = such that (Wℓ[r, s])i,k > 0. For node and block r, we say (i, r) is activatable at depth ℓ 1 if there exists some graph such that (ℓ1) (G)[i, i] = hℓ1[i, (r 1)n + i] > 0. Two cases: If at least one of (i, r) or (k, s) is not activatable, then for every graph G, at least one factor (ℓ1) (G)[k, k] is zero, and thus (Wℓ[r, s])i,k is functionally inert and never contributes to any Rℓ entry. Hence we may simply set it to 0 without altering the models output on any graph. (G)[i, i] or (ℓ1) If both (i, r) and (k, s) are activatable, take graphs Gi, Gk that make (ℓ1) (Gi)[i, i] > 0 and (ℓ1) (Gk)[k, k] > 0. Using the certificate mechanism in Lemma B.1, each positiveness admits finite certificate subgraph with at most 2 3ℓ1 vertices. We then create new graph and disjointly embed both certificates into it, leaving all other vertex isolated. The two labels i, k, viewed as nodes, now lie in different components. But then the product Xr(G)[i, i] (Wℓ[r, s])i,k Xk(G)[k, k] > 0, making (Rℓ(G))i,k > 0, contradiction. Therefore Wℓ[r, s] is diagonal for all block indices (r, s). This concludes STEP 1. STEP 2. Wℓ IS NODE-SYMMETRIC. Given triplet (ℓ, r, s), we can now write Wℓ[r, s] as diag(wℓ,r,s(1), . . . , wℓ,r,s(n)). Our goal is to show that for each (ℓ, r, s), wℓ,r,s(j) + wℓ,s,r(j) = wℓ,r,s(k) + wℓ,s,r(k) for all j, [n]. We formalize this in matrix form: For each node [n] and each layer ℓ, let Λ(i) ℓ = [wℓ,r,s(i)]r,s RKℓ1Kℓ1 and define the symmetric part Sym(Λ(i) ℓ + Λ(i)T are the same, so that Sym(Wℓ) = Λℓ In or equivalently, Wℓ + )/2; the goal is to show that given ℓ, all Λ(i) ℓ ℓ ) = (Λ(i) ℓ = Λℓ In, as claimed. ℓ Throughout out this step, we will use family of special hidden states parameterized by scalar λ > 0 and vector = (u1, u2) R2 0. Fix distinct nodes = k. For λ, u, define the initial state h0(λ, u) Rn2n by setting exactly four entries nonzero: (cid:26)h0(λ, u)[j, j] = λu1 h0(λ, u)[k, k] = λu1 h0(λ, u)[j, + j] = λu2 h0(λ, u)[k, + k] = λu2. Note that h0(λ, u) is invariant under the transposition = (j, k), i.e., h0(λ, u)(IK0 ) = h0(λ, u). Therefore, by assumption (i), we must have TFL Θ(h0)k,k. Let hℓ(λ, u) be the network state at depth ℓ. Because of STEP 1, there is no cross-row interaction for this input at any depth. Writing the row-i vector as v(i) ℓ (λ, u) RKℓ, recursion gives, for {j, k}, Θ(h0)j,j = TFL v(i) 0 (λ, u) = λu, ℓ (λ, u) = [v(i) v(i) ℓ1(λ, u) q(i) ℓ (λ, u)v(i) ℓ1(λ, u)] where q(i) ℓ (λ, u) = 1 Taking ℓ1-norms gives v(i) ℓ1(λ, u) Sym(Λ(i) ℓ ) v(i) ℓ1(λ, u). v(i) ℓ (λ, u) = (1+q(i) ℓ (λ, u))v(i) ℓ1(λ, u) v(i) (λ, u) = v(i) 0 (λ, u) (cid:89) (1+q(i) ℓ (λ, u)). ℓ=1 (7) and 21 Because the readout weight WO is concatenation of Ins, and under our specific input h0(λ, u), every nonzero row lies in columns with indices modulo n, the (i, i) output numerically equals v(i) (λ, u). Hence, assumption (i) requires v(j) (λ, u) = v(k) (λ, u). ℓ ) = Sym(Λ(k) ℓ ). If no such ℓ exists for all = k, ℓ are the same given any fixed ℓ, and STEP 2 holds. Otherwise, for every ℓ < ℓ, the ℓ1(λ, u) and q(j) (λ, u) for all λ, u. We Let ℓ be the minimal layer such that Sym(Λ(j) then all Λ(i) symmetric parts coincide, and v(j) may use vℓ1(λ, u) to denote both v(j) ℓ (λ, u) = q(k) ℓ1(λ, u) for they are now equal. ℓ1(λ, u) and v(k) ℓ1(λ, u) = v(k) ℓ Because of the structure of h0(λ, u), by induction, the row vectors of each hidden state admits an odd power expansion v(i) ℓ1(λ, u) = λu + λ3ξ1,ℓ1(u) + λ5ξ2,ℓ1(u) + . . . from which we conclude q(i) q(j) ℓ (λ, u)q(k) ℓ (λ, u) = ℓ (λ, u) = O(λ2) for every ℓ. In particular, at ℓ = ℓ, 1 vℓ1(λ, u)(Sym(Λ(j) ℓ )Sym(Λ(k) ℓ ))vℓ1(λ, u) = λ2mc(u)+o(λ2m) for some 1 and some nondegenerate polynomial c(u) as λ 0. In particular, ℓ (λ, u) q(k) q(j) ℓ (λ, u) = Θ(λ2m). We now put this back into the comparison between the outputs (j, j) and (k, k) entry. Recall that v(j) 0 (λ, u) = v(k) 0 (λ, u) = λu. Further, since vℓ1 = λu + O(λ3), we know qℓ(λ, u) = O(λ2) for every ℓ and every i. We drop λ, for notational simplicity. It follows from equation 7 that (cid:34) (cid:104) (cid:34) (cid:35) (cid:104) (cid:105) 1 + q(j) ℓ (cid:105) (cid:89) (cid:104) 1 + q(j) ℓ 1 + q(k) ℓ (cid:105) (cid:89) (cid:104) 1 + q(k) ℓ v(j) v(k) = λu (1 + qℓ) (cid:89) (cid:35) (cid:105) = λu (cid:34) l<ℓ (cid:89) ℓ<ℓ (1 + O(λ2)) ℓ>ℓ (cid:35) (cid:104) ℓ q(k) q(j) ℓ (cid:34) (cid:105) (cid:89) ℓ>ℓ ℓ>ℓ (cid:35) (1 + O(λ2)) = λuΘ(λ2m)(1 + o(1)) = Θ(λ2m+1) which is nonzero for small λ. Hence the (j, j) and (k, k) entries can be made different, contradicting assumption (i), and the proof is complete! Theorem 4.6. Suppose an L-layer Disentangled Transformer TFL Suppose TFL Rndℓ1 , Θ has nonnegative parameters. Θ is layerwise permutation equivariant, i.e., for each ℓ and any hidden states Attn(P h(IKℓ1 ); Wℓ) = Attn(h; Wℓ) (IKℓ1 ), then each block Wℓ = Aℓ In + Bℓ Jn for some Aℓ, Bℓ RKℓ1,Kℓ1 . In other words, each block-aligned submatrix of Wℓ necessarily lies in span{In, Jn}. Remark C.3. The equivariance condition presented in the theorem is strictly harder than what we need for graph-level, layerwise equivariance: Attn(hℓ1(P AP ); Wℓ) = Attn(hℓ1(A); Wℓ) (IKℓ1 ). For graphs, it suffices to assume that the hidden states are induced by some n-node graph. Proof. STEP 1. RELATING TO WEIGHT CONJUGATION. Fix layer ℓ. Write = Kℓ1, = hℓ, = Wℓ, and let σ(P ) = IK . The first step is to relate the conjugation of hidden states, TP (h) : (cid:55) h(IK ), to conjugation of layer weights, Wℓ (cid:55) σ(P )Wℓσ(P ). Concretely, since Wℓ 0, ReLU. Hence Attn(TP (h); ) = = 1 1 ReLU[(P hσ(P )) (σ(P )hP )](P hσ(P )) [hσ(P ) (σ(P )h)](hσ(P )) 22 and 1 Layer-wise attention equivariance requires the two quantities above to equal for all h, and left multiplication by 1 gives (hW h)hσ(P ). TP (Attn(h; )) = hhh σ(P ) = 0 for all 0 where := σ(P )W σ(P ) W. (*) STEP 2. PROVING = 0. To do so, we consider special hidden states, with only two nonzero entries hi,p = 1 and hj,q = t. Equivalently, pick columns = and rows/nodes = and set hi, = , and = 0 everywhere else, where ep is standard basis vector pivoted at p. , hj, = te Because only uses columns and q, the matrix hh can be embedded on rows/columns {i, j} with values (cid:18) p,p tq,p tp,q t2q,q hh = (cid:19) . Recall σ(P ) is permutation on columns; let π be the permutation induced by it. Since hσ(P ) has the same two nonzero rows with (hσ(P ))i, = π(q), we get that (hh)(hσ(P )) only has rows and potentially nonzero: π(p) and (hσ(P ))j, = te (cid:40)row : p,pe row : tq,pe π(p) + t2p,qe π(q) π(p) + t2q,qe π(q). But recall (*): (hh)(hσ(P )) = 0 for all > 0. The two standard basis vectors eπ(p), eπ(q) are linearly independent, so the coefficients must be uniformly zero! Hence p,p = p,q = q,p = q,q = 0. Finally, because = were arbitrary, this forces = 0 entrywise. and that σ(P )Wℓσ(P ) = Wℓ for this . And because is arbitrary, we conclude that σ(P )W σ(P ) = for every permutation . STEP 3. RELATING TO BLOCKS. Consider any block [u, v] of where 1 u, Kℓ. Using σ(P ) = IKℓ and taking the (u, v) block on both sides, (σ(P )W σ(P ))[u, v] = (cid:88) (IKℓ)u,aP [a, b]P (IKℓ)b,v = [u, v]P. The LHS equals [u, v], so we conclude that a,b [u, v]P = [u, v] for all Sn. In other words, layerwise equivariance implies each block must be invariant under ()P . Taking any transposition forces all diagonal entries of block to equal, while for any = j, = ℓ, any arbitrary permutation mapping π(i) = k, π(j) = ℓ forces entries (i, j) and (k, ℓ) to be equal. This implies that each block lies in span{In, Jn} as claimed. C.2 POPULATION GRADIENT LIVES IN THE EQUIVARIANT ALGEBRA Theorem C.4 (Population gradient lives in the equivariant algebra). Under Assumption 4.7, in particular using layerwise parameterization Wℓ = Aℓ In + Bℓ Jn, fix layer ℓ and let = Kℓ1. Then the population gradient with respect to Wℓ lies in MK(R)span{In, Jn}: there exist matrices G(I) ℓ ℓ RKK such that , G(J) (cid:105) (cid:104) Wℓ = G(I) ℓ In + G(J) ℓ Jn. (8) Proof. We let Sn act on node indices. Since Wℓ can be parametrized as Wℓ = Aℓ In + Bℓ Jn, the attention map is equivariant under left-right action: Attn(P h(IK ); Wℓ) = Attn(h; Wℓ)(IK ), 23 and so is the full map (cid:55) Z. For any fixed permutation , the data ER(n, p) is permutationinvariant, i.e., and AP are identically distributed. Because the model map and the loss are equivariant under (cid:55) AP with (cid:55) RP , the sample gradient covaries as WℓL(P AP ) = (Ik )WℓL(A)(IK )."
        },
        {
            "title": "Taking expectation over A gives",
            "content": "EA[WℓL(P AP )] = (Ik )EA[WℓL(A)](IK ) for every . Hence the population gradient lies in the commutant of {IK : Sn}. It remains to identify this commutant. View Gℓ as block matrix with sub-blocks. The relation (IK )Gℓ(IK ) = Gℓ says each block satisfies BP for all permutations , so the block must have one value on the diagonal and one on the off-diagonals. It is well known that the fixed-point algebra of conjugation on matrices is span(In, Jn). Hence every block lies in this span, i.e., Gℓ MK(R) span{In, Jn}. C.3 WHICH CONDITIONS ENCOURAGE Wℓ Aℓ In? To facilitate the following analyses, it will be beneficial to first (re)introduce some notations. Throughout the analysis of training dynamics, we inherit the notations used in Assumption 4.7: we use to denote the model output, the reachability matrix, the adjacency matrix, = L(Z; R) the loss, and R(Θ) the population risk R(Θ) := EGER(n,p)[L(TFL Θ(AG); RG)]. Fix layer ℓ and nonnegative direction 0 in the J-channel. Write = [] (more details Bℓ in Theorem C.5). We say node pair (i, j) is active for if Di,j > 0. In particular, we say is active on cross-component pairs if Di,j > 0 for some (i, j) belonging to different connected components (note could also be active on within-component pairs). Because we constrain Wℓ 0, under the parameterization Wℓ = Aℓ In + Bℓ Jn, we must also have Bℓ 0. Then, the appropriate notion of stationarity is KKT: in our setting, this reduces to Bℓ R(Θ) 0, Bℓ 0, and Bℓ R(Θ) Bℓ = 0 which we use in the Theorem below. Theorem C.5 (Population Training Conditionally Suppresses the J-Channel). Assume Assumption 4.7. Fix any layer ℓ and decompose Wℓ = Aℓ In + Bℓ Jn. Let be the output, the reachability matrix (ground truth), = L(Z; R) the loss, and R(Θ) the population risk. 1. (Directional derivative on nonnegative J-channel directions.) Let RKℓ1Kℓ1 be en- [] Rnn. Then trywise nonnegative and define the one-sided Frechet derivative := Bℓ 0 entrywise, and the population directional derivative satisfies DBℓR(Θ)[] = (cid:28)(cid:104) Z (cid:105)(cid:29) , (cid:34) = α (cid:88) Di,j Ri,j =0 (cid:124) (cid:123)(cid:122) (cid:125) penalty from cross component (cid:35) . (9) Di,j (cid:88) 1 ϕϵ(Zi,j) ϕϵ(Zi,j) (cid:123)(cid:122) weighted reward on under-predicted positives Ri,j =1 (cid:124) (cid:125) In particular, DBℓR(Θ)[] 0 iff cross-component penalty within-component reward. Throughout this section, we will be using these names to denote the two competing sums whenever an expression like equation 9 appears. 2. (Consequences for KKT stationary points.) Assume Θ is KKT-stationary for Bℓ 0: BℓR(Θ) 0, Bℓ 0, and BℓR(Θ) Bℓ = (10) Let = Bℓ (entrywise absolute value) and let = [Bℓ]. If, with positive probability unBℓ der ER(n, p), activates at least one cross-component pair, and if the cross component penalty term strictly dominates the within-component reward, then Bℓ = 0. Equivalently, under activation at = Bℓ and strict dominance by cross-component penalty, the only KKT stationary point in the Jn-channel is Bℓ = 0. 24 Lemma C.6 (Monotonicity in the J-channel). Fix ℓ and hold all parameters except Bℓ. Write hℓ1 = [X1 . . . XKℓ1 ] and up = Xp1 Rn 0. Then hℓ1Wℓh ℓ1 = (Aℓ)p,qXpX + (cid:88) p,q (Bℓ)p,qupu . (cid:88) p,q (11) Consequently, for every nonnegative direction 0 in the J-channel, the one-sided Frechet derivative at 0+ exists and is entrywise nonnegative. Hence, along the ray {Bℓ + δ δ 0}, the output is entrywise nondecreasing: Bℓ [] Rnn 0 , Z(Bℓ + δ) Z(Bℓ) 0 for all δ 0. Moreover, if is disconnected, and either (i) p,p > 0 for block such that up has support in at least two components, or (ii) there exist blocks p, with p,q > 0 and up, uq supported in different components, then there exist cross component pairs (i, j) with ( Bℓ [])i,j > 0. Proof. Since Jnx = (1x)1 for Rn, we have XpJnX the displayed decomposition. For Bℓ (cid:55) Bℓ + δ with 0, the layer scores = (Xp1)(Xq1) = upu , yielding Rℓ(Bℓ + δ) Rℓ(Bℓ) = δ (cid:88) p,q p,qupu 0, so the one-sided derivative exists and is entrywise nonnegative. Because all subsequent maps are entrywise monotone, this implies Z(Bℓ + δ) Z(Bℓ) 0 as stated. For the moreover part, in casse (i), upu ponentns where up > 0, and in case (ii), upu components supporting up and uq. Monotonicity propagates these positives to = Bℓ places positive mass on index pairs spnaning the comq (or its transpose) places positive mass across two []. Proof of Theorem C.5. For the population risk R(Θ) = E[L(Z; R)], applying definitions gives the directional derivative along gives DBℓ R(Θ)[] = (cid:28) (cid:105) (cid:104) Z , (cid:29) (cid:104) (cid:88) = α (cid:18) 1 i,j Ri,j ϕϵ(Zi,j) (cid:19) (cid:105) . Di,j Separating indices by Ri,j {0, 1} proves equation 9. For the second claim, evaluate equation 9 at = Bℓ. Under the activation premise (Lemma C.6) and strict dominance by cross-component penalty, we obtain DBℓR(Θ)[Bℓ] = BℓR(Θ), > 0. Since BℓR(Θ) 0 and Bℓ 0, strictly positive inner product violates the KKT complementary condition BℓR(Θ) Bℓ = 0 unless Bℓ = 0. Remark C.7. While Theorem C.5 mostly discusses the suppression of Bℓ, its (i) in fact reveals quite interesting, opposite phenomenon: early training promotes Bℓ. Before the model learns to pick up easy connected pairs, the corresponding values ϕϵ(Zi,j) 1. Consequently, the fractions (1 ϕϵ(Zi,j))/ϕϵ(Zi,j) are large, making equation 9 negative. Gradient descent then pushes Bℓ up without feeling pressure. As training proceeds, these easy connected pairs saturate (ϕϵ(Zi,j) 1), while simultaneously begins to active cross pairs (the moreover part of Lemma C.6), increasing the = 0 term in equation 9 and potentially flipping the sign. This is when the J-channel starts to incur penalty. This explains the transient Phase 1 in 4.3. Remark C.8. The Bℓ Jn-channel injects rank-one dense terms upu into the attention core. On disconnected graphs, these terms produce cross-component positives, which the reachability target labels as negatives. Because disconnected graphs appear with positive probability in the data, the population gradient penalizes every nonnegative direction in the J-channel active on crosscomponent pairs whenever the cross-component penalty dominates within-component reward. Under the same activation and cross-component penalty dominance assumptions, any KKT stationary point must have Bℓ = 0. In short: under these conditions, population drives the node-side factor towards locality, i.e., Wℓ Aℓ In. C.4 WHICH SAMPLES PUSH WHICH CHANNEL? (LOCAL In VS. GLOBAL Jn) Recall Wℓ = Aℓ In + Bℓ Jn and Lemma C.6. The I-channel controls local propagation within components; the J-channel couples to the global / mean direction and injects dense rank-one terms. In this section, we first shift to micro-level perspective, focusing on the effects of individual samples (graphs), and then draw connection to how the training distribution determines the models eventual behavior (algorithmic vs. heuristic, 4.3). We decompose the single-sample loss LG(Θ) := L(TFL Θ(A); R) and examine directional derivatives at fixed Θ, with the link gradient L/Z = α(1 R/ϕϵ(Z)). Throughout, we say pair (i, j) is saturated if its per-pair loss gradient vanishes; for within-component pairs (Ri,j = 1) this is equivalent to ϕϵ(Zi,j) = Ri,j. We say direction is active over (i, j) if the correpsonding channel directive Di,j > 0, where denotes Aℓ [] as appropriate. [] or Bℓ Our first main result is the following Theorem, which intuitively claims two things: (Within capacity) Small-diameter graphs reward the local I-channel and, if disconnected, penalizes the global J-channel if activated. (Beyond capacity) Large-diameter connected graphs demand global shortcut: the J-channel is promoted, while the I-channel remains confined to short-range corrections. Theorem C.9 (Per-sample pushes by diameter). Fix layer ℓ and nonnegative directions A, 0 for Aℓ, Bℓ, respectively. Assume B1 = . . . = BL = 0. (i) (Within capacity) If diam(G) 3L, then DAℓLG(Θ)[A] 0, with strict < 0 whenever is active on at least one unsaturated within-component pair. If, in addition, is disconnected, then DBℓLG(Θ)[B] > 0 if both of the following hold: is active at at least one cross-component pair, and cross component penalty dominates within-component reward (cf. equation 9). (ii) (Beyond capacity) If diam(G) > 3L and is connected, then we have DAℓLG(Θ)[A] 0 where only within-capacity pairs can contribute, and DBℓLG(Θ)[B] < 0 for that is active on at least one unsaturated pair. To prove this Theorem, we split the argument into the following four lemmas, each isolating one ingredient of the dynamics. Firstly, Lemma C.10 shows that the local I-channel is monotone: any nonnegative Aℓ cannot increase the loss and is strictly helpful on unsaturated within-component pairs. This lets us treat local corrections as harmless, while Lemma C.11 analyze the sign of the global J-channel (connected vs. disconnected), and Lemma C.12 determines which pairs are ever affected when = 0. Together, they yield the two cases in Theorem C.9. Lemma C.10 (Local channel always helps). Assume B1 = . . . = BL = 0. For any graph G, any layer ℓ, and any direction 0 in the I-channel, (cid:28) Z with strict inequality whenever there exists within-component, unsaturated pair, on which is active. DAℓLG(Θ)[] = Aℓ 0, (12) [] (cid:29) , Proof. From the block decomposition from equation 11, (cid:80) contributes p,q p,qXpX ., which is block-diagonal with respect to the component partition. Hence [] has support only on pairs (i, j) in the same component. On those pairs, Ri,j = 1, and thus (cid:19) I-channel Aℓ the (cid:19) (cid:18) (cid:18) Z = α 1 i,j 1 ϕϵ(Zi,j) = α 1 ϕϵ(Zi,j) ϕϵ(Zi,j) 0, with strict negativity whenever ϕϵ(Zi,j) < 1. Entrywise, nonnegativity of the forward map (Lemma C.6) gives [] 0. Therefore the Frobenius inner product 0, and < 0 under Aℓ the stated conditions. We now switch from the local I-channel to the global J-channel and will use that the forward sensitivity in the J-channel is entrywise nonnegative, so the sign of the directional derivative is controlled entirely by the per-pair loss gradient. 26 Lemma C.11 (Global channel helps connected graphs and conditionally hurts disconnected graphs). Fix layer ℓ and nonnegative direction 0 in the J-channel. (i) If is connected, then DBℓ LG(Θ) 0, with strict < 0 whenever there exists an unsaturated pair (i, j) (i.e., ϕϵ(Zi,j) < 1) on which is active (Di,j > 0). (ii) If is disconnected, then DBℓLG(Θ)[] = α (cid:34) (cid:88) (cid:88) Di,j Ri,j =0 Ri,j =1 1 ϕϵ(Zi,j) ϕϵ(Zi,j) (cid:35) Di,j , (13) hence DBℓLG(Θ)[] 0 whenever the cross component mass penalty dominates the ratioweighted within-component reward. Strict > 0 holds if the inequality is strict, and is active on at least one cross pair. Proof. By the chain rule, DBℓLG(Θ)[] = (cid:28) Z , Bℓ (cid:29) (cid:28) (cid:18) [] = α 1 (cid:19) (cid:29) , ϕϵ(Z) . (14) By Lemma C.6 , 0 entrywise; moreover, Di,j > 0 exactly on pairs where is active. (i) If is connected, then the reachability matrix is all-ones. Hence Z = α(1 ϕϵ(Z))/ϕϵ(Z) 0 entrywise, with strict negativity whenever ϕϵ(Zi,j) < 1. Pairing with 0 and Di,j > 0 on active pairs gives DBℓLG(Θ)[] 0 and strict < 0 under the stated saturation / activation conditions. (ii) If is disconnected, split equation 14 over Ri,j = 0 and Ri,j = 1 to obtain the displayed identity. Since 0, the stated dominance condition yields 0. Strictness requires cross pair with Di,j > 0, holds exactly when is active on at least one cross-component pair. We now show that when the global J-channel is disabled, the model can only light up withincapacity pairs. Note this is somewhat converse to Theorem C.2, where good model that only lights up within-capacity pairs necessarily have each Wℓ[r, s] diagonal. The following Lemma isolates the role of the J-channel as the only nontrivial shortcut. Recall from Definition 4.5: for depth and graph with adjacency matrix A, we call pair (i, j) within capacity if [A3L ]i,j > 0 and beyond capacity otherwise. Lemma C.12 (I-channel reaches within-capacity pairs; J-channel is the only dense shortcut). At any Θ with B1 = . . . = BL = 0, the output satisfies Zi,j > 0 = [A3L ]i,j > 0. Equivalently, beyond-capacity pairs receive no positive mass from the I-channel alone. In contrast, for any ℓ and any 0 in the J-channel, [] 0 and is strictly positive on active pairs by Bℓ definition. Proof. Since Bℓ = 0 implies hℓ1Wℓh from Lemma C.6, it is easy to see that they are block-diagonal w.r.t. connected components. Hence Lemma B.2 applies and support expands by at most factor of 3 per layer, and only within-capacity pairs receive mass. The density statement follows from Lemma C.6: For any 0 in the J-channel, we have = Bℓ (cid:80) 0. The strict positiveness characterization follows directly from Lemma C.6. p,q(Aℓ)p,qXpX p,q p,qupu ℓ1 = (cid:80) With the previous lemmas established, we can now assemble the per-sample sign rules. Intuitively, the I-channel makes only local corrections, never hurting the loss and only touching within-capacity pairs when = 0, while the J-channel is the sole dense shortcut, helpful on connected graphs but penalized by cross-component pairs when the graph is disconnected. 27 Proof of Theorem C.9. Let ℓ, A, be given as described. Set DA = Aℓ Bℓ [B]. Recall from chain rule [A] and DB = D()LG(Θ)[] = (cid:28) Z , () (cid:29) [] (cid:28) = α 1 ϕϵ(Z) , () (cid:29) [] . (i) (Within capacity) By Lemma C.10, for any 0 the I-channel directional derivative is 0, with strict inequality under the stated conditions. The result on disconnected graphs follows from Lemma C.11. (ii) By Lemma C.12, with = 0, only within-capacity pairs can be affected by the I-channel, so Lemma C.10 gives DAℓ LG(Θ)[A] 0. Since is connected and diam(G) > 3L, there will be unsaturated pairs; then Lemma C.11(i) yields DBℓLG(Θ)[B] < 0, as claimed. Remark C.13 (Population-level consequence under ER(n, p)). Fix layer ℓ and nonnegative directions A, 0. Partition the graphs into G0 = {G : diam(G) 3L} and G1 = {G : diam(G) > 3L}. Writing the population directional derivatives as mixtures, DBℓR(Θ)[B] = P(G0)E[DBℓLG(Θ)[B] G0] + P(G1)E[DBℓLG(Θ)[B] G1]. (15) We claim the following on the population gradient. (i) (Local) From Lemma C.10, once the global J-channel has been suppressed, the local I-channel is consistently promoted until saturation. (ii) (Global) The population gradient along the global J-channel is an explicit mixture of two regimes: large, connected graphs beyond capacity that promote the J-channel, and small, disconnected graphs within capacity that suppress it whenever cross-component errors persist. Formally: (ii.a) If is connected and diam(G) > 3L, then by Lemma C.12, every beyond-capacity pair has Zij = 0 while Rij = 1. For those pairs, we haveL/Z = α(1 ϕϵ(Z))/ϕϵ(Z) < 0. By Lemma C.6, the inner product L/Z, Z/Bℓ[B]F < 0 too. Integrating over all beyond-capacity, connected graphs yields E[DBℓLG(Θ)[B] G1 & connected] < 0. (16) (ii.b) If is disconnected and diam(G) 3L, then by Lemma C.11, DBℓ LG(Θ)[B] 0 with strict > 0 if cross-component errors persist (the (cid:80) R=1(1 ϕϵ(Z))/ϕϵ(Z) term), and if is active on cross pairs (i.e. D(B) ij > 0 for some Rij = 0). The latter holds by Lemma C.6 if is active on at least one cross pair. Integrating thus yields R=0 term strictly dominates the (cid:80) E(cid:2)DBℓ LG(Θ)[B] (cid:12) and strictly > 0 provided the two additional assumptions above. (cid:12) G0 & disconnected(cid:3) 0, (17)"
        },
        {
            "title": "D ADDITIONAL EXPERIMENT DETAILS AND RESULTS",
            "content": "D.1 EXPERIMENT DETAILS Standard Transformers. When training 2-layer standard Transformers, we adopt the implementation from RoBERTa (Liu et al., 2019) with single-head per-layer and using normalized ReLU activation function as defined in Definition A.1. We use hidden dimension of = 512 to make sure the hidden size is not the blocker for expressivity. We trained on 1 Billion ER graphs with batch size of 1000 and 106 steps. Each graph is only seen by the model once to resembling the training regime of modern LLMs. We note that although 1 billion graphs sounds lot but with = 20 nodes, this is far from enumerating all possible graphs: there can be 2(n 2) graphs if we dont consider graph isomorphism. When = 20, this is about more than 1057 graphs in total, and 1 billion (109) is only very small number of training instances. We train with AdamW optimizer with learning rate of 1e-4 and weight decay of 1e-4 and cosine learning rate decay. Disentangle Transformers. For 1-layer disentangle transformers in Section 5, we train on fixed set 4096 i.i.d. samples of ER(n = 8) graphs and running standard Gradient Descent without any mini-batching. In this case, we have learning rate of 0.1 with cosine learning rate decay. For 2layer disentangled Transformers, we train on the same set of 1 billion number of ER(n = 20) graphs as with standard Transformers. For 3-layer models, we train on 1 billion number of ER(n = 64) graphs. Both 2and 3-layer models are trained with AdamW with learning rate of 1e-3. We would like to note that the hidden dimensions dℓ of disentangle Transformers are fixed to be dℓ = 2ℓn rather than hyper-paramter (see Definition 4.1). Computing Energy Share of In/Jn Channels. In the experiments on 1-Layer disentangled Transformers, we compute energy shares of the In and Jn within 2 . Here is the formalized versions. We consider the noisy decomposition = ˆA In + ˆB Jn + Wϵ, where Wϵ is the projection error term. We define Frobenius-norm energy share on the In channel as EnegeryShare( ˆA In, ) = W, ˆA In 2 and by symmetry, the Jn-channel share is = EnergyShare( ˆB Jn, ) = W, ˆB Jn 2 = ˆA In2 + ˆA In, ˆB Jn + ˆA In, Wϵ 2 , ˆB Jn2 + ˆB Jn, ˆA In + ˆB Jn, Wϵ 2 . you obtain W, ˆAIn+ ˆBJn+Wϵ, This is well-designed quantity because if you expand 2 and the I/J-channels energy shares will sum to one when the projection error Wϵ converges to zero. D.2 TRAINING DYNAMICS OF DISENTANGLED AND STANDARD TRANSFORMERS In Figure 8, we show the training dynamics of 3-Layer disentangled Transformer. In Figure 9, we show the learned weights by disentangled transformers. Figure 8: We plot the model behavior of 3-Layer Disentangled Transformer model trained on ER(n = 64) graphs. They also quickly pickup almost layer-wise equivariant properties (measured by Eqn. 6). All layers show very small projection error onto the In + Jn decomposition, resonating our theoretical claims in Theorem 4.6. In Figure 9, we show that the trained 2-layer and 3-layer converge to weight spaces Wℓ = Aℓ In + Bℓ Jn in the particular form echoing Theorem 4.6. Figure 9: Here we visualize the weights Wℓ learned by 2-Layer and 3-Layer disentangled Transformer respectively. All models are randomly initialized without any restriction on parameterization. Resonating Theorem 4.6, they all converge to form of Wℓ = Aℓ In + Bℓ Jn. In Figure 10, we show that the capacity theorems (Theorem 4.4) also transfer to standard 2-layer Transformer models. Figure 10: (left) Standard Transformers models studied in 3.3 also hit its capacity wall at 3L, showing that our theoretical results transfer beyond the theoretical simplification of disentangled transformers. (right) Standard Transformer models also learn an almost layer-wise equivariant solution measured by Eqn. 6. 30 D.3 SCALING EFFECTS OF DIAMETER AND CAPACITY (a) When training 1-layer disentangled Transformers, instead of restricting training graphs to have diameter at most 3, we restrict diam(G) 2 and varying the edge probability in ER(n = 8, = p) training distribution. When measured by exact match accuracy, restricting diam(G) 2 make the models unable to generalize as well, indicating the importance of at-capacity graphs (diam(G) = 3) (b) When restricting diam(G) 3, with reasonable [0.1, 0.32], 1layer disentangled transformer can learn the algorithmic channel. (c) When restricting diam(G) 4, allowing some beyond-capacity graphs, 1-layer disentangled transformer struggle to learn the algorithmic channel, and starts to rely on the heuristic Jn-channel to make predictions on beyond-capacity graphs (red lines). Figure 11: Effects of at-capacity graphs (diam(G) = 3L) for = 1. Without at-capacity graphs, models struggle to learn the algorithmic solution. With beyond-capacity graphs, models weight too much on heuristics. In short, models not only need most graphs within capacity and but also require at-capacity graphs to learn algorithms over heuristics."
        }
    ],
    "affiliations": [
        "Duke University",
        "University of Southern California"
    ]
}