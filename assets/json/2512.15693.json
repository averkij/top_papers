{
    "paper_title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "authors": [
        "Yifei Li",
        "Wenzhao Zheng",
        "Yanran Zhang",
        "Runze Sun",
        "Yu Zheng",
        "Lei Chen",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection."
        },
        {
            "title": "Start",
            "content": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning Yifei Li, Wenzhao Zhen, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu Department of Automation, Tsinghua University Project Page: https://joeleelyf.github.io/Skyra 5 2 0 2 7 1 ] . [ 1 3 9 6 5 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with finegrained human annotations. We then develop two-stage training strategy that systematically enhances our models spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, benchmark comprising 3K high-quality samples generated by over ten state-ofthe-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection. Our code, models, and datasets are publicly available at https://github.com/JoeLeelyf/Skyra. 1. Introduction With the rapid evolution of diffusion-based [3, 31, 49, 65, 79] and multimodal generative models [12, 30, 46], synthetic videos now achieve unprecedented levels of authenticity, enabling anyone to produce photorealistic content from simple text prompts or reference images. While this progress reshapes entertainment, communication, and design, its misuse poses growing threats to social safety [56, 75]. Driven by this urgent need, the community has developed several detection models [1, 5, 25, 48, 83, 88], Project leader. * Corresponding author. Figure 1. Performance on ViF-Bench. Our method outperforms both binary and existing MLLM-based detectors. datasets [5, 7], and benchmarks [5, 7, 43] to detect AIgenerated videos. The rise of multimodal large language models (MLLMs) [2, 24, 33, 37, 52] has attracted the attention of AI-generated content detection researchers due to their capacity for interpretable reasoning [8, 19, 36, 62, 86]. Still, we empirically find that even state-of-the-art (SoTA) general MLLM [2, 11, 47] achieves near-random performance in identifying AI-generated videos, and fails to capture human-perceivable artifacts, even with carefully designed chain-of-thought (CoT) [70] prompts. While recent works such as BusterX++ [71, 72] attempt to adapt pretrained MLLMs for AI-generated video detection, the resulting model acts more as general content descriptors and overemphasizes superficial cues (e.g., visual quality, lighting) while neglecting the intrinsic, physics-violating artifacts that humans rely on to identify AI-generated videos (Figure 2). DAVID-XR1 [15] advancing the field by introducing human annotations of AI-generated video artifacts. However, the classification taxonomy of their annotations is vague, with the number of valid samples being limited, and the resulting models performance far from satisfactory. To overcome these limitations, we introduce Skyra, specialized multimodal large language model AI-generated video detection via grounded artifact reasoning, which 1 Figure 2. Skyra leverages human-perceivable artifacts in AI-generated videos as grounded evidence for detection and explanation. Compared to off-the-shelf MLLMs and previous MLLM-based detectors, Skyra demonstrates superior artifact perception and detection capabilities. identifies artifacts and leverages them as spatio-temporally grounded evidence. As shown in Figures 1 and 2, Skyra achieves substantially higher detection accuracy while providing fine-grained, human-interpretable artifact localization, consistently outperforming both binary classifiers and prior MLLM-based approaches. Recognizing that off-the-shelf MLLMs lack sensitivity to subtle generative artifacts, we construct the first large-scale human-annotated AI-generated video artifacts dataset, ViF-CoT-4K, which enables supervised fine-tuning and yields Skyra-SFT. We further propose second-stage reinforcement learning procedure that pushes forward the models ability to mine discriminative artifacts, producing additional gains in both detection and explanation quality, resulting in our final model Skyra-RL. To comprehensively evaluate the ability of existing methods, we release ViF-Bench, which includes high-quality samples generated by over ten latest models, with real and fake samples aligned in both semantics and formats, mitigating shortcut signals, and providing fair testbed of artifact-based detection. 2. Related Work AI-Generated Video Detection. AI-generated video detection has largely focused on binary classification [1, 5, 7, 25, 39], relying on visual artifact detection in synthetic content. Early methods like AIGVDet [1] and DeCoF [39] exploit spatio-temporal features and frame consistency to detect discrepancies, achieving strong performance on curated datasets such as GVD [1] and GVF [39]. Recent works, including DeMamba [5], D3 [88], ReStraV [25], and NSGVD [83], explore more discriminative and robust feature spaces, demonstrating success on updated benchmarks like GenVideo [5] and GenVidBench [43]. However, these approaches lack interpretability, leaving the detection process opaque and limiting their applicability in scenarios that require manual verification. The emergence of MLLMs [2, 10, 24, 37, 64, 67, 81, 82] has enabled more explainable detection [19, 48, 61, 76, 86], providing both predictions and reasoning processes. X2DFD [8] and VERITAS [62] demonstrate MLLM effectiveness in deepfake face detection through pattern-aware reasoning and feature enhancement. MLLMs have also succeeded in AIGC image detection, with frameworks like FakeVLM [73] and LEGION [27] enabling fine-grained artifact identification. For video content, MLLM-based methods remain nascent. IVY-Fake [84], DAVID-XR1 [15], and BusterX++ [71, 72] pioneer explainable video detection, providing interpretable reasoning on motion, texture, and temporal artifacts. Despite progress, current methods struggle with complex temporal dynamics and fine-grained reasoning, often relying on MLLM-generated annotations [64, 84] or basic fine-tuning [15, 84]. In contrast, our approach leverages high-quality human annotations and precise spatio-temporal supervision to enhance artifact perception and reasoning. Multimodal Large Language Models for Video. Recent advances in video MLLMs have yielded specialized architectures for video processing and reasoning. VideoChatGPT [40] integrates video-adapted visual encoder with an LLM for detailed understanding through conversation. 2 Figure 3. Overview of the ViF-CoT-4K dataset. (a) The hierarchical taxonomy of AI-generated video artifacts. (b) Visual examples of artifacts under our taxonomy. (c) Construction pipeline of ViF-CoT-4K dataset, including authentic data collection and AI-generated video collection, manual annotation, and the step-by-step chain-of-thought explanation data construction process. Video-LLaMA [81] employs multimodal encoders for spatiotemporal reasoning, integrating audio and video for enhanced comprehension. Meanwhile, general vision-language models have also demonstrated strong video capabilities. Qwen2-VL [67] introduces dynamic-resolution tokenization and unified image-video encoding. Recent models like Seed1.5VL [16], InternVL3.5 [68], and Qwen3VL [52] further advance visual-temporal feature integration, achieving strong performance on diverse benchmarks [34, 44, 69, 74]. Despite these advances, base MLLMs exhibit limited reasoning without task-specific fine-tuning. Post-training strategies, particularly reinforcement learning (RL) [60, 63], have emerged to address this gap. In the image domain, OpenThinkIMG [59] and DeepEyes [89] incorporate RL to enhance visual reasoning through structured, multi-stage processes. For video understanding, Video-R1 [13] introduces T-GRPO for temporally consistent reasoning, while LongVILA-R1 [9] integrates RL with large-scale reasoning tasks to support longer inputs. Recent methods [14, 21, 78] enhance reasoning by incorporating tool use into trajectories, achieving state-of-the-art performance. These RLbased strategies are essential for enhancing MLLMs perception [32, 38, 85] and reasoning [18, 23, 87], particularly for challenging video understanding tasks [63]. 3. ViF Dataset With rapid advances in AI video generation technologies [12, 46, 65, 79], numerous datasets [5, 7, 71] and benchmarks [5, 7, 43] have emerged for detection research. However, existing datasets face three key limitations: (1) Significant Real-Fake Discrepancy: In datasets like [5, 7, 43], real videos exhibit 2-3 higher duration and FPS than fake counterparts. Moreover, Domain and style distributions also differ substantially [7], enabling models to exploit spurious correlations through shortcut learning [48]. (2) Limited Diversity and Authenticity: Most datasets include few [48] or outdated generative models [5]. For example, VidGuard-R1 [48] relies solely on HunyuanVideo [31] and Figure 4. Statistics of the ViF-CoT-4K and ViF-Bench. (a) Distribution of samples generated by different generators in ViF-CoT-4K (train) and ViF-Benchmark (test) set. (b) Distribution of artifacts types in ViF-CoT-4K. Detailed proportion is provided in the Appendix. (c) Word cloud of the CoT annotations in ViF-CoT-4K. CogVideoX [79], while GenVidBench [43] mainly includes models released over two years ago [3, 22, 29, 66]. This homogeneity limits utility for real-world applications, where models such as Sora-2 [46], Wan2.2 [65], and Kling [30] now generate highly realistic visual content. (3) Lack of Detailed Artifact Annotations: Mainstream detection methods and datasets[5, 43, 83] focus solely on binary classification without detection rationale. While IVY-Fake [84] and VidGuard-R1 [48] attempt explanations by prompting general MLLMs [11], they lack systematic artifact taxonomy and grounded localization. To address these limitations, we propose dataset and benchmark construction pipeline with fine-grained manual annotations, as illustrated in Figure 3. This section presents the statistical analysis (3.1), our artifact taxonomy (3.2), and dataset construction process(3.3). 3.1. Dataset Statistics We provide statistical report of our constructed dataset and benchmark in Figure 4. Distribution of video generators and numbers of different annotated artifact types, and word cloud of the CoT explanation are exhibited. Additional details, including video statistics, annotation guidelines, and complete CoT prompts, are provided in the Appendix. 3.2. Artifact Taxonomy comprehensive and unambiguous artifact taxonomy is essential for high-quality manual annotation and model reasoning. Prior MLLM-based detection methods either lack [71, 72] or provide vague, coarse-grained taxonomies [15, 84]. Categories such as Space Anomaly and Spatial Relationships [15, 84] lack granularity and cause ambiguity. To address these limitations, we propose hierarchical taxonomy for fine-grained classification of humanperceivable artifacts. Our taxonomy comprises three layers. Layer 1 (L1) defines two high-level categories: Low-level forgery (perceptual quality artifacts) and Violation of Laws (physical and logical inconsistencies). Layer 2 (L2) refines these into eight categories: Low-level Forgery includes Color/Light Anomaly, Texture Anomaly, and Motion Forgery; Violation of Laws includes Object Inconsistency, Interaction Inconsistency, Violation of Causality, Violation of Commonsense, and Unnatural Movement. Layer 3 (L3) provides the most fine-grained, observable artifacts. For instance, the Object Inconsistency in L2 divides into Abnormal Object Disappearance, Abnormal Object Appearance, and Person Identity Inconsistency. This hierarchical structure progresses from abstract concepts to specific observable patterns. The complete taxonomy with examples is shown in Figure 3. 3.3. Dataset and Benchmark Construction Real and AI-Generated Videos Collection. We sample around 3.5K real videos from Panda-70M [6] and 1.5K from Kinetics-400 [28], covering diverse content types including real-life recordings, TV shows, and human actions. We also include high-resolution videos from HD-VILA-100M [77] in our benchmark to test the generalization ability. We then utilize variety of MLLMs [2, 10, 11, 24, 47] to generate detailed video descriptions, which are transformed into prompts for generation models after manual quality inspection. These prompts drive text-to-video (T2V) models [4, 20, 26, 31, 42, 50, 51, 53, 65, 79]. For image-to-video (I2V) generation [4, 20, 31], we extract the first frame from real videos as conditions. An automatic filtering pipeline using GPT-4o-mini [45] ensures semantic consistency between AI-generated and real videos, addressing Limitation (1). To address Limitation (2), we incorporate diverse state-ofthe-art generators spanning open-source and commercial domains. For training, we use Wan2.2-TI2V-5B [65], Wan2.1T/I2V-1.3B [65], CogVideoX-1.5-5B [79], and HunyuanVideo [31]. For evaluation, we include recent models like Wan2.2-T/I2V-A14B [65], LTX-Video-13B [20], MiniMaxHailuo [42], and Sora-2 [46]. See Table 1 for details. Fine-grained Manual Annotation. We collaborate with 4 Figure 5. Overview of Skyra. We leverage two-stage training pipeline to improve Skyras artifacts perception and detection capabilities: (a) cold-start initialization with balanced fake and real explanation response templates to endow the base model with basic AI-generated artifacts perception capability. (b) reinforcement learning with adapted rewards to encourage the models self-driven visual probe process. domain experts to develop detailed annotation guidelines and build an easy-to-use annotation platform. Professional annotators familiar with video generation models identify all visible artifacts (fake evidence) in AI-generated videos, annotating: (1) artifact Type from our taxonomy, (2) Textual Explanation, (3) temporal-spatial localization via Timestamps and Bounding Boxes. Notably, we display AI-generated videos alongside real counterparts, prompting annotators to identify corresponding real evidence in real videos for each fake evidence. This helps to validate that artifacts are truly generation-induced rather than compression-related degradation. Multiple review cycles ensure annotation quality and inter-annotator consistency. Chain-of-Thought (CoT) Annotation. Chain-of-thought reasoning improves MLLM performance on complex visual tasks [70]. While precise, our manual labels lack stepby-step reasoning valuable for model training. For each AI-generated video, we feed artifact Type, Textual Explanation, Timestamps, and Bounding Boxes aligned with sampled frames to Gemini-2.5-Pro [11]. For real videos, we substitute fake evidence with real evidence as model input. To improve the quality of annotation generated by Gemini2.5-Pro, we employ two prompt-engineering strategies. (1) Self-Curation: we instruct the model to follow an observeunderstand-draft-review-conclude process, grounding CoT in visual details; (2) In-Context Learning: we provide detailed definitions and carefully crafted CoT examples for each artifact type, filtering mismatched annotations. 4. Skyra In this section, we analyze the characteristics of the AIgenerated video detection and explanation, examine the challenges of applying off-the-shelf MLLMs to this task, and present our design motivation (Section 4.1). We then introduce our two-stage training strategies as depicted in Figure 5, i.e., supervised fine-tuning to endow the model with basic detection and explanation capabilities (Section 4.2), and reinforcement learning to enhance its ability to perceive and reason about AI-generated artifacts (Section 4.3). 4.1. Analysis of the AI-Generated Video Detection and Explanation Task How Humans Identify AI-Generated Videos. Conventional AIGC detection approaches typically extract handcrafted [25, 83, 88] or learned features [1, 5, 7] from generative samples and perform binary classification in this feature space. However, this paradigm often devolves into continuous adversarial cycle between detectors and generators: as new generative models emerge, previous discriminative features may lose effectiveness. This results in detectors that lack generalizability and remain fragile when encountering unseen samples [90]. To move beyond this limitation, we examine how humans identify AI-generated videos. Humans first perceive the overall semantic and temporal context, then actively search for spatio-temporal inconsistencies, such as abrupt object disappearance, unnatural motions, or implausible scene transitions that reveal synthetic content. Through continuous interaction with the real world, humans develop grounded understanding of physical and temporal coherence [55, 57, 58], enabling intuitive recognition of violations. We refer to such cues as intrinsic evidences, as they are universal, model-agnostic indicators of deviation from real-world dynamics. Challenges of Adapting MLLMs for AI-Generated Video Detection. Pre-trained on large-scale vision-language datasets, MLLMs have acquired foundational understanding of the real world to some extent. Inspired by prior work [86], we explored directly prompting off-the-shelf MLLMs for AIGC video detection. However, both direct question-answer prompting and carefully designed chain-ofthought (CoT) strategies yield limited accuracy, often below 60% on our benchmarks (Table. 1). Our experimental analysis reveals two key issues: (1) most existing MLLMs [2, 47, 68, 81], even with explicit step-by-step CoT guidance, struggle to uncover subtle spatiotemporal forgery cues; and (2) some models [11] misinterpret natural video degradations (e.g., compression artifacts, motion blur) as forgery signs, leading to false positives on real videos. Detailed examples are provided in the Appendix. These observations motivate our approach to emulate human reasoning, i.e., enhancing the model ability to discover essential forgery cues while incorporating self-verification to re-examine suspected regions in real videos, improving both precision and confidence. 4.2. Cold-start Initialization Response Template Design. We require the model to ground its judgment on careful video inspection and explicitly expose its reasoning process. This chain-of-thought (CoT) supervision is essential for improving accuracy and credibility (see Ablation-II in Section 5.3). The model follows the format Fouter: <thinking>[thinking process]</thinking> <answer>[Fake / Real]</answer> For fake videos, we guide attention to forged regions using Ff ake: <type>[Forgery Type]</type> in <t>[t_start, t_end]</t> at <bbox>[x_min, y_min, x_max, y_max]</bbox>. For real videos, the model instead inspects suspected tags Freal: regions with the same temporalspatial <t>[t_start, t_end]</t> at <bbox>[x_min, y_min, x_max, y_max]</bbox> Training Process. We fine-tune Qwen2.5-VL-7B on our ViF-CoT-4K dataset. Text and video are encoded by pretrained textual and visual encoders, fused, and fed into the decoder for autoregressive generation. Given the groundtruth response sequence = (y ), the model is trained with standard cross-entropy loss: 1, . . . , LSFT = (cid:88) t=1 log pθ (y <t, t, v) , (1) where θ denotes model parameters. We show in ablation experiments (Ablation-II (Section 5.3)) that this cold-start stage is crucial for endowing the model with essential detection and explanation abilities. Without this initialization, the base models forgery recognition capacity remains weak, leading to sparse rewards in the subsequent RL stage and preventing effective learning of meaningful forensic cues. 4.3. RL Training We observe that during the data annotation process, human annotators struggled to identify precise artifact cues in certain high-quality generated samples. Conversely, for lowquality samples, human-provided labels often contain significant noise. Therefore, in this stage, we employ reinforcement learning (RL) to elicit the models inherent capability for self-coherent forgery cue discovery. This approach also aims to continuously improve adaptability to new domains, mitigating the need for iterative manual annotation. We adopt Group Relative Policy Optimization (GRPO) [17, 54] as our 6 RL algorithm, where we re-design the reward score to adapt it to our task. For each query-completion pair (x, y), the total reward R(x, y) is defined as: R(x, y) = wacc racc(x, y) + wchk rchk(x, y) (2) where wacc = 0.8 and wchk = 0.2 in our experiments. The rewards racc(x, y) and rchk(x, y) are defined as follows: Accuracy Reward racc(x, y). We apply an asymmetric reward structure with more severe penalties for false positives: if ypred = ygt if ygt = Fake ypred = Real if ygt = Real ypred = Fake 1.0 0.0 0.2 racc(x, y) = (3) Check Reward rchk(x, y). This reward is activated only when the models response adheres to the prescribed format Fouter. We extract the number of valid check blocks Ncheck from the model output using regular expressions. The matching pattern follows Ffake when the prediction is Fake, and Freal when the prediction is Real: rchk(x, y) = min(ln(1 + Ncheck), ln(1 + 3)) (4) Our reward function encourages active cue exploration while strictly supervising the classification. We observed that symmetric penalties for both error types (false positives and false negatives) caused the model to overfit and develop strong bias towards predicting Fake. This stems from the inherent asymmetry of the task, i.e., identifying Fake requires finding just one artifact, while confirming Real requires comprehensively ruling out all inconsistencies. We validate our asymmetric design in Ablation-III (Section 5.3). 5. Experiments 5.1. Experimental Setup Implementation Details. We build upon Qwen2.5-VL7B-Instruct popular in video-related tasks [13, 35, 41, 72], trained on 8 NVIDIA H200 GPUs. During training, we uniformly sample 16 frames from each video and resize them to 256p. In the SFT stage, we perform full-parameter finetuning with batch size 1 per device for 5 epochs at learning rate 1e-5. In the RL stage, we set the actor learning rate to 5e-7 and the KL coefficient to 0.02. For binary detectors, we use identical video pairs with fake/real labels and follow their original training protocols. Evaluation Protocols. For binary detectors, use weights trained on our dataset and follow their preprocessing scripts. For off-the-shelf MLLMs (like GPT-4.1-mini [47]) and ours, we apply chain-of-thought prompts (Figure 5) that guide stepby-step inspection for AI-generated artifacts. For BusterX++, we follow its original prompt [72] to align with its evaluation setups. We report accuracy, recall, and F1-score on both our benchmark and GenVideo [5]. Table 1. Detection performances on ViF-Bench. Method Metric Wan2.1 -1.3B CogV -X1.5 Wan2.2 -5B Hunyuan Video VACE -1.3B Wan2.2 -14B Skyreels -V LTX-Video -13B Gen4 -Turbo Hailuo-02 Pika -V2 Pixverse -V4-5 Kling -V Sora-2 Mean T2V T2V I2V T2V I2V T2V T2V I2V T2V I2V T2V I2V T2V Binary Detectors AIGVDet DeMamba NSG-VD VideoLLaMa-3(7B) Qwen2.5-VL(3B) Qwen2.5-VL(7B) Qwen2.5-VL(72B) InternVL-3(8B) GPT-4.1-mini Gemini -2.5-flash BusterX++(7B) Ours-SFT(7B) Ours-RL(7B) Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 82.12 70.91 79.86 65.45 99.39 74.21 50.00 99.39 66. 50.92 2.45 4.76 52.12 5.45 10.23 51.21 4.85 9.04 52.42 9.09 16.04 48.16 13.50 20.66 55.83 18.40 29.41 57.67 72.39 63.10 54.85 10.30 18.58 97.58 99.39 97.62 96.97 100.00 97.06 5.2. Main Results 67.58 62.73 77.44 90.00 58.48 71.52 72.12 75.15 66.46 65.31 67.48 56.25 81.21 41.82 32.12 61.59 96.92 23.64 49.70 50.91 56.97 39.63 37.50 41.72 17.86 69.09 56.33 46.29 73.19 90.65 36.28 63.57 64.62 69.63 54.17 51.95 56.20 28.99 78.62 65.76 65.76 60.00 65.85 60.30 61.82 65.76 61.82 65.76 64.02 64.06 64.11 62.50 100.00 100.00 88.48 100.00 89.09 92.12 100.00 92.12 100.00 96.34 96.88 96.32 91.96 75.12 74.49 68.87 74.55 69.18 70.70 74.49 70.70 74.49 72.81 72.94 72.85 71.03 74.49 49.39 48.34 50.30 49.70 50.30 49.70 50.00 50.00 49.39 49.70 49.09 49.06 50.00 50.45 98.18 100.00 98.79 100.00 98.79 99.39 99.39 98.18 98.79 97.56 97.50 99.39 100.00 100.00 96.03 65.02 65.99 60.67 69.08 70.07 26.67 44.88 43.80 40.40 56.76 59.41 64.23 63.67 64.29 97.81 100.00 100.00 100.00 96.00 96.66 72.54 73.00 73.22 49.67 49.65 50.36 98.67 98.66 66.22 66. 66.80 66.26 66.80 66.26 66.53 66.53 65.99 66.26 65.71 65.68 66.53 66.87 75.06 48.68 96.71 65.33 75.20 49.29 97.87 65.87 73.68 53.95 67.21 66.78 58.87 21.99 34.83 67.02 55.30 15.89 26.23 66. 66.83 Open-source Multimodal Large Language Models 53.68 7.98 14.69 54.85 10.91 19.46 50.3 3.03 5.75 51.82 7.88 14.05 57.98 33.13 44.08 59.82 26.38 39.63 52.76 62.58 56.98 59.39 19.39 32.32 97.58 99.39 97.62 96.36 98.79 96.45 3.68 7. 3.07 5.92 1.85 3.61 1.54 2.99 1.22 2.38 0.61 1.21 1.23 2. 5.52 10.40 51.84 51.23 50.62 51.53 52.45 50.00 50.31 50.31 50.00 50.31 52.45 50.90 2.70 1.23 4.29 1.25 5.52 0.62 5.22 2.41 8.19 2.45 10.40 1.22 52.5 49.69 50.00 50.61 50.00 50.00 50.00 50.91 50.61 50.91 50.00 49.7 1.79 0.61 1.21 2.42 0.61 6.25 3.03 5.81 4.68 3.45 1.20 11.63 1.20 2.37 50.61 49.39 50.00 50.62 51.53 50.00 49.7 2.68 3.64 1.82 4.88 9.09 3.49 5.08 6.86 50.61 51.52 54.55 51.21 49.39 50.00 50.30 53.12 52.45 50.00 50.91 4.88 10.62 9.20 5.49 6.06 6.25 6.67 10.99 8.94 18.48 16.22 11.11 10.00 13.04 22.68 12.02 46.01 48.77 50.00 50.61 55.83 45.71 46.01 45.40 46.60 50.62 49.69 45.95 9.20 10.49 18.12 16.56 11.71 14.56 22.33 25.69 27.15 39.50 13.66 14.56 12.75 16.43 26.85 24.77 17.81 3.03 1.21 2.37 5.81 51.21 51.22 51.21 50.91 4.24 4.85 9.04 7.95 50.3 4.85 8.89 14.72 17.28 18.40 28.83 3.75 5.52 7.06 10.23 2.42 4.68 50.3 3.03 5. 2.44 4.65 1.21 2.34 4.85 9.04 4.24 7.82 3.03 5.65 13. 7.27 7.98 8.59 9.20 Proprietary Multimodal Large Language Models 53.37 57.06 55.25 54.60 54.60 53.68 53.68 51.23 54.94 54.69 55.83 53.60 13.50 20.86 17.28 15.95 15.95 14.11 14.11 16.67 16.25 18.40 12.61 22.45 32.69 27.86 26.00 26.00 23.35 23.35 15.87 27.00 26.40 29.41 21.37 57.67 55.21 55.56 50.92 58.90 49.08 49.39 53.07 46.30 57.19 59.20 53.60 72.39 67.48 67.90 58.90 74.85 55.21 55.83 63.19 49.38 70.62 75.46 65.77 63.10 60.11 60.44 54.55 64.55 52.02 52.45 57.38 47.90 62.26 64.91 58. 9.20 MLLM-based Detectors 2.42 4.71 25.45 40.38 18.90 31.63 93.9 52.42 50.30 59.15 49.70 50.91 62.42 49.70 65.76 50.00 56.25 50.00 50.89 2.68 1.21 5.45 32.12 0.00 0.00 10.29 5.17 2.38 48.40 92.12 79.09 96.67 83.03 95.15 76.83 96.25 80.67 74.55 90.30 84.24 70.3 84.85 72.73 92.07 88.48 62.42 97.58 94.55 57.93 96.88 65.64 54.46 80.56 95.12 71.43 96.27 77.26 68.16 96.7 89.74 82.19 93.79 91.82 74.91 92.12 87.58 94.82 93.64 79.09 96.36 84.55 95.76 78.96 95.94 83.74 79.46 90.30 81.21 95.73 93.33 64.24 98.79 75.15 97.58 64.02 98.12 73.62 66.07 91.98 86.73 94.86 93.62 75.44 96.45 82.94 95.83 75.27 96.02 81.91 76. 0.61 13.12 0.61 1.20 23.08 1.21 0.00 0.00 50.37 0.74 1.46 50.36 1.46 2.86 50.00 2.19 4.20 51.82 8.76 15.38 47.06 10.29 16.28 50.00 8.09 13.92 50.74 58.09 54.11 61.68 24.09 38.60 97.08 98.54 97.12 95.99 98.54 96.09 49.67 0.00 0.00 50.99 3.31 6.33 50.99 3.97 7.50 53.31 10.60 18.50 50.00 17.33 25. 54.33 16.00 25.95 60.33 78.67 66.48 76.82 54.30 70.09 97.35 99.34 97.4 96.36 99.34 96.46 49.67 0.00 0.00 50.33 1.97 3.82 50.00 2.63 5.00 51.97 8.55 15.12 48.00 11.33 17.89 55.33 16.67 27.17 57.33 70.67 62.35 75.66 51.97 68.10 97.37 99.34 97.42 96.05 98.68 96.15 50.00 0.71 1.41 49.65 0.00 0.00 49.65 1.42 2.74 48.58 2.13 3.97 44.29 5.71 9. 50.71 8.57 14.81 45.36 47.14 46.32 52.84 5.67 10.74 95.04 95.04 95.04 94.68 96.45 94.77 51.01 50.91 2.39 2.03 4.57 3.97 50.7 50.00 2.62 1.33 2.60 4.94 52.00 50.57 3.53 6.00 11.11 6.63 49.67 51.26 6.99 4.00 7.36 12.44 46.28 48.58 9.46 14.31 14.97 21.31 48.99 54.08 14.90 4.05 7.36 24.21 43.58 53.36 45.27 63.78 44.52 57.48 52.33 56.90 5.33 14.40 10.06 21.94 87.33 90.11 79.33 84.65 86.23 88.76 91.00 91.02 88.67 88.35 90.78 90.27 Results on Our Benchmark. We compare with three baseline groups: (1) Binary detectors including AIGVDet [1], DeMamba [5], and the recent NSG-VD [83]. (2) Off-theshelf MLLMs including VideoLLaMA-3 [81], Qwen2.5-VL series [2], InternVL-3 [68], GPT-4.1-mini [47], and Gemini2.5-flash [11]. (3) MLM-based detector, i.e., BusterX++ [72] which is the only open-sourced implementation available. Table 1 shows our model consistently outperforms all baselines, achieving +26.73% absolute accuracy and +17.27% F1 over the second-best DeMamba. Compared to MLLM baselines, we notably achieve +34.12% accuracy, +24.57% recall, and +32% F1. Our RL training further improves over SFT, especially on hard I2V samples, with +3.74% on recall. Results on GenVideo Benchmark. GenVideo serves as our out-of-domain test, containing low-quality samples from outdated generators with near-static content [7]. Our model achieves +11.07% accuracy over the best binary detector and 7 +7.8% accuracy, +16.9% recall over Skyra-SFT, as shown in Table 2. To demonstrate the ability to quickly adapt to OOD scenarios, we initialize our framework with Skyra-SFT and perform RL training on only 2.2K data randomly selected from the GenVideo-100K training set, following the manyto-many settings in GenVideo [5]. The RL training process, without any additional human-annotation, and is trained for only 1 epoch, quickly adapting Shyra to such new domain, with the resulting model Skyra-RL-GenVideo achieving +19.22% accuracy, +42.06% recall, and +31% F1-score gain than Skyra-RL. Robustness Study. Real-world videos are usually represented in degraded formats, causing potential perturbations on the detecting performance. We evaluate robustness under five degradation types: Compression (JPEG), Transformation (Zoom), Gaussian Noise, Light Transform (/+), and color-transform (/+). Table 3 shows our model maintains state-of-the-art performance under all degradations. Table 2. Detection performances on GenVideo."
        },
        {
            "title": "DeMamba",
            "content": "NSG-VD Ours-SFT(7B) Ours-RL(7B) Ours-RLGenVideo(7B) ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC 50.36 0.71 1.42 62.00 96.43 71.32 49.79 99.14 66.38 52.71 6.14 11.50 57.21 16.71 28.09 79.93 66.29 76.76 50.21 0.43 0.28 61.36 98.43 71.81 50.14 99.29 66.57 62.07 24.43 39.18 69.14 40.29 56.63 94.43 95.86 94.51 50.00 0.00 0.00 62.06 99.84 72.46 49.60 98.88 66.24 69.01 38.98 55.17 81.71 65.34 78.13 96.09 97.92 96."
        },
        {
            "title": "Hot\nShot",
            "content": "50.07 0.14 0.29 56.36 99.84 66.22 50.29 99.14 66.60 53.21 7.00 13.01 57.50 17.57 29.25 88.50 82.71 87.79 Show1 Gen"
        },
        {
            "title": "Sora",
            "content": "50.00 0.00 0.00 61.50 85.57 71.61 50.07 99.43 66.57 51.64 4.86 9.13 56.71 16.57 27.68 83.50 72.00 81.36 50.00 0.00 0.00 62.75 97.14 72.65 50.22 99.28 66.60 66.99 34.64 51.21 75.94 54.49 69.37 95.25 96.23 95.30 50.18 0.36 0.71 61.80 98.91 72.13 49.86 99.00 66.38 77.04 54.72 70.44 86.16 73.75 84.20 95.92 98.07 96. 50.04 0.07 0.14 61.29 98.86 71.67 50.29 99.21 66.62 67.86 36.29 53.03 78.50 59.14 73.34 94.32 94.29 94.32 50.00 0.00 0.00 62.50 97.93 72.37 50.00 100.00 66.67 77.86 55.36 71.26 86.61 73.21 84.54 95.54 95.64 95."
        },
        {
            "title": "Wild\nScrape",
            "content": "50.00 0.00 0.00 55.51 88.11 66.45 40.94 99.36 57.86 61.62 24.56 39.02 68.34 38.88 55.11 86.56 78.63 85.41 Avg. 50.09 0.17 0.34 60.71 95.94 70.91 49.12 99.27 65.65 63.98 28.70 41.00 71.78 45.60 59.00 91.00 87.66 90.00 Table 3. Robustness evaluation of different detectors on ViF-Bench."
        },
        {
            "title": "Origin",
            "content": "Compression Transformation"
        },
        {
            "title": "Gaussian\nNoise",
            "content": "Light-transform (+) () Color-transform (+) ()"
        },
        {
            "title": "DeMamba",
            "content": "NSG-VD Qwen2.5-VL(7B) BusterX++(7B) Ours-SFT(7B) Ours-RL(7B) ACC F1 ACC F1 ACC ACC F1 ACC F1 ACC F1 ACC F1 69.08 44.88 56.76 64.29 96.66 73.00 49.65 98.66 66.21 51.26 6.99 12.44 56.90 14.40 21.94 90.11 84.65 88.76 91.02 88.35 90.27 70.33 50.42 58.43 63.94 96.68 72.28 48.71 99.20 65.34 49.95 4.16 7.60 55.02 10.64 17.43 80.52 85.54 81.06 80.80 88.64 81.93 54.91 0.57 1.03 64.62 96.79 72.69 48.60 98.97 65. 48.93 5.12 9.04 59.10 23.04 33.09 86.21 91.31 86.64 83.26 96.37 85.17 56.30 5.57 8.57 63.18 96.41 71.80 48.97 99.72 65.58 50.68 5.15 9.41 59.54 23.27 33.04 83.70 94.77 85.28 83.48 96.34 85.33 62.33 20.84 30.21 64.06 96.92 72.40 48.71 99.19 65.34 48.93 5.13 9.04 59.12 23.04 33.09 88.67 89.93 88.51 83.26 96.37 85.17 64.01 22.87 33.13 62.81 96.90 71.71 49.14 98.80 65. 54.90 10.41 17.20 54.90 10.41 17.20 88.12 80.68 86.18 90.66 85.78 89.62 72.93 54.83 62.18 63.25 96.55 71.88 48.73 99.22 65.35 48.93 5.13 9.04 59.12 23.04 33.09 88.50 85.21 87.43 83.26 96.37 85.17 77.52 64.88 69.25 63.69 96.81 72.18 49.21 98.94 65.50 54.94 10.48 17.31 54.94 10.48 17.31 88.51 80.85 86.58 90.67 85.81 89.64 5.3. Ablation Study We conduct ablation studies to validate the effectiveness of our design, as shown in Table. 4 Effects of Training Strategies: Both the CoT reasoning process and the RL boost models detection performance. In this part, we comprehensively evaluate the effectiveness of our method design by conducting the following ablation studies: (1) Without CoT in answer: we trained our model to answer with Real/Fake when inquired Is the video real or fake video?. This naive implementation yields nearrandom performance. Without careful reasoning, the model cannot learn detection signals. (2) Without cold-start: we train the base model to perform AIGC video detection and explanation by CoT prompting and GRPO-based reinforcement learning using the design in Section. 4.3. While similar 8 No."
        },
        {
            "title": "Type",
            "content": "Ablation-I wo CoT w/o Cold-Start w/o RL Table 4. Results of ablation studies. Acc 90.11 Ours-SFT 84.65 F1 88. Acc 91.02 Ours-RL 88.35 F1 90.27 54.04 (-36.07) 9.36 (-75.29) 16.72 (-72.04) 50.09 (-40.93) 90.11 (-0.91) 0.18 (-88.17) 84.65 (-3.70) 0.37 (-89.90) 88.76 (-1.51) Ablation-II wo Asymmetric Reward wo Inspection Reward 76.24 (-14.78) 99.07 (+10.72) 80.65 (-9.62) 89.30 (-0.97) 87.55 (-0.80) 90.05 (-0.97) (a) Through our cold-start initialization stage with our highquality human-annotated dataset, and the RL stage training, which further enhances our models artifacts perception capability, we are delighted to find that Skyra can perceive tiny-grained AIGC evidence that are even hard for humans to identify. (b) For real videos, Skyra follows descriptioninspection-review-conclusion process, which guides the models attention to those areas that are likely to contain artifact evidence, avoiding missing any possible forgery evidence, balancing the training process gap between real and AIGC videos, and also making the models explanation more persuasive. We provide more cases in the Appendix. 6. Conclusion and Discussions In this paper, we introduce Skyra, specialized multimodal large language model designed for interpretable, artifactcentric AI-generated video detection. Built upon the finegrained, human-annotated ViF-CoT-4K dataset and twostage training pipeline that integrates supervised initialization with reinforcement learning, Skyra exhibits strong spatio-temporal artifact perception and produces coherent, grounded explanations. Extensive experiments on ViFBench and GenVideo demonstrate substantial improvements over existing binary and MLLM-based detectors, while also uncovering systematic patterns in generative artifacts and model reasoning behavior. We hope that Skyra, together with our dataset and benchmark, can support future research toward more transparent, robust, and trustworthy AIGC video detection systems, contributing to the broader effort of mitigating societal risks associated with synthetic media. Limitations. Our training data are still bound by the specific generators and collection pipeline used in ViF-CoT-4K and ViF-Bench. Although we cover diverse set of recent text-tovideo and image-to-video models, the benchmark does not yet encompass all emerging media distributions (e.g., ultralong videos or non-photorealistic, stylized content). Also, Skyra does not assess the intent, context, or potential societal harm of video. Its natural-language rationales are designed to be persuasive and human-readable, but they may still be overconfident or partially hallucinated. This highlights the importance of calibrated uncertainty estimation, human-inthe-loop use, and complementary safeguards when deploying such models in safety-critical scenarios. Figure 6. Case Study. More examples are provided in the appendix. approaches, as introduced by DeepSeek-R1-Zero [17], have proven to be effective in various visual tasks [60, 63], duplicating its success to our tasks is not naive. The resulting model achieves even worse performance than (1). Considering the base models inability in AI-generated video detection tasks, purely RL can harly equip the model with sufficient artifacts identifying capability without our coldstart initialization process [80]. (3) Without the RL stage: our reinforcement training stage further boosts the detection performance of the supervised finetuned model. Effect of Reward Design: Direct real-fake binary reward yields suboptimal performance. In our reward score, we introduce two special designs: asymmetric accuracy reward and inspection reward. When setting the accuracy reward to 0.0 for both false positive and negative, the model quickly overfitted to fake, with sharp decline in both accuracy (-14.78%) and F1-score (-9.62%). We also observe performance drop when we replace the inspection reward with the normal format reward that only inspects whether the models response follows Fouter. 5.4. Case Study of the Explanation Ability We provide two response cases of Skyra to demonstrate the explanation ability in Figure 6. Our analysis is as follows:"
        },
        {
            "title": "References",
            "content": "[1] Jianfa Bai, Man Lin, Gang Cao, and Zijie Lou. Ai-generated video detection via spatial-temporal anomaly learning. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pages 460470. Springer, 2024. 1, 2, 5, 7 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 4, 5, 7 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 4 [4] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. 4 [5] Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, et al. Demamba: Ai-generated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024. 1, 2, 3, 4, 5, 6, 7, 16 [6] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple crossmodality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 4, 19 [7] Weiliang Chen, Wenzhao Zheng, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu, and Yueqi Duan. Genworld: Towards detecting ai-generated real-world simulation videos. arXiv preprint arXiv:2506.10975, 2025. 1, 2, 3, 5, 7 [8] Yize Chen, Zhiyuan Yan, Guangliang Cheng, Kangran Zhao, Siwei Lyu, and Baoyuan Wu. X2-dfd: framework for explainable and extendable deepfake detection. arXiv preprint arXiv:2410.06126, 2024. 1, 2 [9] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025. [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 2, 4 [11] Google DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/ google - deepmind / gemini - model - thinking - updates-march-2025/, 2025. Accessed: 2025-11-14. 1, 4, 5, 7, 17 [12] Google DeepMind. Veo 3: Advanced generative video model. https://aistudio.google.com/models/veo3, 2025. Accessed: 2025-11-14. 1, 3 [13] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 3, 6 [14] Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xiaohua Xie, and Wei-Shi Zheng. Love-r1: Advancing long video understanding with an adaptive zoom-in mechanism via multistep reasoning. arXiv preprint arXiv:2509.24786, 2025. 3 [15] Yifeng Gao, Yifan Ding, Hongyu Su, Juncheng Li, Yunhan Zhao, Lin Luo, Zixing Chen, Li Wang, Xin Wang, Yixu Wang, et al. David-xr1: Detecting ai-generated videos with explainable reasoning. arXiv preprint arXiv:2506.14827, 2025. 1, 2, [16] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 3 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 6, 9 [18] Shasha Guo, Liang Pang, Xi Wang, Yanling Wang, Huawei Shen, and Jing Zhang. Geovlmath: Enhancing geometry reasoning in vision-language models via cross-modal reward for auxiliary line creation. arXiv preprint arXiv:2510.11020, 2025. 3 [19] Xiao Guo, Xiufeng Song, Yue Zhang, Xiaohong Liu, and Xiaoming Liu. Rethinking vision-language model in face forensics: Multi-modal interpretable forged face detector. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 105116, 2025. 1, 2 [20] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 4 [21] Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, and Yu Cheng. Framethinker: Learning to think with long videos via multi-turn frame spotlighting. arXiv preprint arXiv:2509.24304, 2025. [22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 4 [23] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 3 [24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 2, 4 [25] Christian Internò, Robert Geirhos, Markus Olhofer, Sunny Liu, Barbara Hammer, and David Klindt. Ai-generated 10 video detection via perceptual straightening. arXiv preprint arXiv:2507.00583, 2025. 1, 2, [26] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 4 [27] Hengrui Kang, Siwei Wen, Zichen Wen, Junyan Ye, Weijia Li, Peilin Feng, Baichuan Zhou, Bin Wang, Dahua Lin, Linfeng Zhang, et al. Legion: Learning to ground and explain for synthetic image detection. arXiv preprint arXiv:2503.15264, 2025. 2 [28] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 4, 19 [29] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 4 [30] KlingAI. Klingai: Creative video generation platform. https://klingai.com/, 2025. Accessed: 2025-1114. 1, 4 [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 3, [32] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns arXiv preprint and interaction turns for visual search. arXiv:2509.07969, 2025. 3 [33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 1 [34] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 3 [35] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 6 [36] Yixuan Li, Xuelin Liu, Xiaoyang Wang, Bu Sung Lee, Shiqi Wang, Anderson Rocha, and Weisi Lin. Fakebench: Probing explainable fake image detection via large multimodal models. IEEE Transactions on Information Forensics and Security, 2025. 1 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, [38] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 3 [39] Long Ma, Jiajia Zhang, Hongping Deng, Ningyu Zhang, Qinglang Guo, Haiyang Yu, Yong Liao, and Pengyuan Zhou. Decof: Generated video detection via frame consistency: The first benchmark dataset. arXiv e-prints, pages arXiv2402, 2024. 2 [40] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 2 [41] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, et al. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. 6 [42] MiniMax. Hailuo 02: Global ai video generation model by minimax. https://hailuo-02.com/, 2025. Accessed: 2025-11-14. 4 [43] Zhenliang Ni, Qiangyu Yan, Mouxiao Huang, Tianning Yuan, Yehui Tang, Hailin Hu, Xinghao Chen, and Yunhe Wang. Genvidbench: challenging benchmark for detecting aigenerated video. arXiv preprint arXiv:2501.11340, 2025. 1, 2, 3, [44] Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1890218913, 2025. 3 [45] OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence. https : / / openai . com / index / gpt - 4o - mini - advancing - cost - efficient - intelligence/, 2024. Accessed: 2025-11-14. 4 [46] OpenAI. Sora 2 is here: Next-generation video-and-audio generation model. https://openai.com/index/sora2/, 2025. Accessed: 2025-11-14. 1, 3, 4 [47] OpenAI. Introducing gpt-4.1 in the api. https://openai. com/index/gpt-4-1/, 2025. Accessed: 2025-11-14. 1, 4, 5, 6, 7 [48] Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, and Lili Qiu. Vidguard-r1: Ai-generated video detection and explanation via reasoning mllms and rl. arXiv preprint arXiv:2510.02282, 2025. 1, 2, 3, 4 [49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [50] Pika Art. Pika.art. https://pika.art/, 2025. Accessed: 2025-11-14. 4 [51] PixVerse AI. Pixverse ai video generator from text & photos. https://app.pixverse.ai/, 2025. Accessed: 202511-14. 4 [52] Qwen Team. Qwen3-vl: Sharper vision, deeper thought, https : / / qwen . ai / blog ? id = broader action. 99f0335c4ad9ff6153e517418d48535ab6d8afef& from = research . latest - advancements - list, 2025. Accessed: 2025-10-23. 1, 3 [53] Runway AI, Inc. Introducing runway gen-4. https: / / runwayml . com / research / introducing - runway-gen-4, 2025. Accessed: 2025-11-14. 4 [54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 6 [55] Roger Shepard. Perceptual-cognitive universals as reflections of the world. Psychonomic Bulletin & Review, 1(1): 228, 1994. 5 [56] Mohamed Shoaib, Zefan Wang, Milad Taleby Ahvanooey, and Jun Zhao. Deepfakes, misinformation, and disinformation in the era of frontier ai, generative ai, and large ai models. In 2023 international conference on computer and applications (ICCA), pages 17. IEEE, 2023. [57] Elizabeth Spelke. Initial knowledge: Six suggestions. Cognition, 50(1-3):431445, 1994. 5 [58] Elizabeth Spelke and Katherine Kinzler. Core knowledge. Developmental science, 10(1):8996, 2007. 5 [59] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 3 [60] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 3, [61] Zhihao Sun, Haoran Jiang, Haoran Chen, Yixin Cao, Xipeng Qiu, Zuxuan Wu, and Yu-Gang Jiang. Forgerysleuth: Empowering multimodal large language models for image manipulation detection. arXiv preprint arXiv:2411.19466, 2024. 2 [62] Hao Tan, Jun Lan, Zichang Tan, Ajian Liu, Chuanbiao Song, Senyuan Shi, Huijia Zhu, Weiqiang Wang, Jun Wan, and Zhen Lei. Veritas: Generalizable deepfake detection via patternaware reasoning. arXiv preprint arXiv:2508.21048, 2025. 1, 2 [63] Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, et al. Video-lmm post-training: deep dive into video reasoning with large multimodal models. arXiv preprint arXiv:2510.05034, 2025. 3, 9 [64] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [65] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 3, 4 [66] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [67] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 3 [68] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 3, 5, 7 [69] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Ming Ding, Xiaotao Gu, Shiyu Huang, Bin Xu, et al. Lvbench: An extreme long video understanding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2295822967, 2025. 3 [70] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. 1, 5 [71] Haiquan Wen, Yiwei He, Zhenglin Huang, Tianxiao Li, Zihan Yu, Xingru Huang, Lu Qi, Baoyuan Wu, Xiangtai Li, and Guangliang Cheng. Busterx: Mllm-powered ai-generated video forgery detection and explanation. arXiv preprint arXiv:2505.12620, 2025. 1, 2, 3, 4, 17 [72] Haiquan Wen, Tianxiao Li, Zhenglin Huang, Yiwei He, and Guangliang Cheng. Busterx++: Towards unified cross-modal ai-generated content detection and explanation with mllm. arXiv preprint arXiv:2507.14632, 2025. 1, 2, 4, 6, 7, 17, 18 [73] Siwei Wen, Junyan Ye, Peilin Feng, Hengrui Kang, Zichen Wen, Yize Chen, Jiang Wu, Wenjun Wu, Conghui He, and Weijia Li. Spot the fake: Large multimodal model-based synthetic image detection with artifact explanation. arXiv preprint arXiv:2503.14905, 2025. [74] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 3 [75] Danni Xu, Shaojing Fan, and Mohan Kankanhalli. Combating misinformation in the era of generative ai models. In Proceedings of the 31st ACM International Conference on Multimedia, pages 92919298, 2023. 1 [76] Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, and Jian Zhang. Fakeshield: Explainable image forgery detection and localization via multi-modal large language models. arXiv preprint arXiv:2410.02761, 2024. 2 [77] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with largescale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50365045, 2022. 4, 19 [78] Ziang Yan, Xinhao Li, Yinan He, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. 12 Videochat-r1. 5: Visual test-time scaling to reinforce multimodal reasoning by iterative perception. arXiv preprint arXiv:2509.21100, 2025. [79] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 3, 4 [80] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 9 [81] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2, 3, 5, 7 [82] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024. 2 [83] Shuhai Zhang, ZiHao Lian, Jiahao Yang, Daiyuan Li, Guoxuan Pang, Feng Liu, Bo Han, Shutao Li, and Mingkui Tan. Physics-driven spatiotemporal modeling for ai-generated video detection. arXiv preprint arXiv:2510.08073, 2025. 1, 2, 4, 5, 7, 16 [84] Wayne Zhang, Changjiang Jiang, Zhonghao Zhang, Chenyang Si, Fengchang Yu, and Wei Peng. Ivy-fake: unified explainable framework and benchmark for image and video aigc detection. arXiv preprint arXiv:2506.00979, 2025. 2, 4 [85] Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025. [86] Yue Zhang, Ben Colman, Xiao Guo, Ali Shahriyari, and Gaurav Bharaj. Common sense reasoning for deepfake detection. In European conference on computer vision, pages 399415. Springer, 2024. 1, 2, 5 [87] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 3 [88] Chende Zheng, Ruiqi Suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, and Chao Shen. D3: Training-free ai-generated video detection using secondorder features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1285212862, 2025. 1, 2, 5 [89] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 3 [90] Yueying Zou, Peipei Li, Zekun Li, Huaibo Huang, Xing Cui, Xuannan Liu, Chenghanyu Zhang, and Ran He. Survey on ai-generated media detection: From non-mllm to mllm. arXiv preprint arXiv:2502.05240, 2025."
        },
        {
            "title": "Appendix",
            "content": "Content of Appendices Section A. The ViF Dataset. A.1. Definition of Each Artifact Category. A.2. Annotation Platform. A.3. Chain-of-Though Annotation Prompt Design. A.4. Detailed Statistics of ViF-CoT-4K. A.5. Generated Video Examples. Section B. Analysis of Baselines Detection Capabilities. B.1. Binary Detectors. B.2. Off-the-Shelf MLLMs. B.3. Existing MLLM-based Detectors. Section C. Additional Examples. C.1. Design of Prompts. C.2. Examples of Skyras Responses. Section ??. Limitations and Broader Impacts. ??. Limitations. D. Broader Impacts. Section E. License. A. The ViF Dataset A.1. Definition of Each Artifact Category We provide detailed definitions of each category in our artifact taxonomy (Section 3.2) as follows. Low-Level Forgery. This group summarizes characteristic visual cues that frequently make current AI-generated videos appear unnatural. These cues typically do not explicitly violate physical laws, but reflect systematic limitations of mainstream video generation models. Texture Anomaly. This category focuses on abnormal patterns in local textures. Structure Anomaly. Regions with rich structures (e.g., fences, grids, lattices) exhibit unnatural distortion, twisting, or interlacing, leading to inconsistent or implausible geometric patterns. Texture Jittering. Surface textures show high-frequency flickering or drifting over time, manifesting as crawling patterns, grid-like noise, or temporally unstable blur, instead of stable, physically plausible textures. Unnatural Blur. Blur and degradation patterns differ from typical natural degradations, such as Gaussian blur or compression artifacts. The blur may be spatially inconsistent, texture-selective, or temporally unstable in way rarely observed in real videos. Lighting Inconsistency. Global or local illumination changes abruptly or violently over time, or shows strong intensity fluctuations that cannot be explained by realistic changes of light sources, exposure, or scene configuration. Motion Forgery. This category describes unnatural camera-related motion patterns. Camera Motion Inconsistency. The apparent camera motion is abnormal, such as erratic zooming in/out, unnatural high-frequency panning, or irregular shaking. These artifacts are often accompanied by inconsistent changes in object positions, scales, or spatial relations that do not match physically plausible camera trajectory. Violation of Laws. This group contains artifacts that clearly violate real-world constraints, including object permanence, physical laws, causality, and basic common sense. Detecting these cues generally requires spatio-temporal reasoning and background knowledge about how objects and scenes behave in reality. Object Inconsistency. This category focuses on violations of object permanence and identity over time. Abnormal Object Disappearance. An object disappears suddenly during its motion without any plausible interaction or occlusion. For example, runner on track vanishes abruptly while continuing to move forward. Abnormal Object Appearance. An object suddenly appears and starts to move without any reasonable cause or prior indication. For example, new runner appears out of nowhere on the track in the middle of the video. Person Identity Inconsistency. The identity of person changes over time, especially in facial features or other stable identity cues. For example, persons face disappears and reappears with clearly different facial characteristics, leading to mismatch in perceived identity. General Object Identity Inconsistency. The identity of generic object changes significantly over time without any obvious external cause. For example, chair being rotated by person ends up with drastically different color or structure compared to its initial state. Shape Distortion. Rigid objects exhibit non-rigid deformations during motion. For instance, human body suddenly scales up and down or undergoes frequent surface distortions and twisting that are incompatible with rigid-body motion. Color and Lighting Anomaly. This category captures implausible color or illumination patterns that deviate from natural imaging conditions. Color Over-saturation. Certain regions exhibit excessively saturated or overly vivid colors (often in blue, red, or green), with insufficient tonal variation or shading, making the area visually stand out unnaturally. Interaction Inconsistency. This category captures physically implausible interactions between multiple objects. Abnormal Rigid-Body Crossing. Rigid objects that should remain non-interpenetrating instead of intersecting or passing through each other. For example, barbell that should move in front of persons body passes unrealistically through the persons head. Abnormal Multi-Object Merging. Two or more distinct objects gradually or abruptly merge into single object during motion, without any plausible explanation (e.g., three people in motion merge into two). Abnormal Object Splitting. single object splits into multiple distinct objects during motion, again without any reasonable cause (e.g., one person splits into two separate people). General Interaction Anomaly. Other abnormal or implausible phenomena occurring during interactions between two or more objects, such as missing collisions, inconsistent contact, or contradictory occlusion relations. Unnatural Movement. This category denotes motion patterns that contradict the typical kinematics of humans, animals, or objects. Unnatural Human Movement. Human body motion deviates from normal biomechanics or everyday experience. For example, person walks without leg crossing, exhibiting pure lateral sliding of the legs instead of realistic gait cycles. Unnatural Animal Movement. Animal motion is incompatible with known locomotion patterns. For example, running horse moves its hind legs in parallel translation without proper alternating strides. Unnatural General Object Movement. Objects other than humans and animals follow trajectories or undergo transformations that are inconsistent with real-world dynamics, such as erratic acceleration, unnatural smoothness, or implausible temporal discontinuities. Violation of Causality Law. This category collects artifacts that violate physical laws or general causal relationships. Violation of Physical Laws. The motion of objects contradicts basic physical principles, such as forceacceleration relationships or conservation laws. For example, ball moves or changes velocity in the absence of any visible force, or instantaneously teleports at unrealistic speeds. Violation of General Causality Violation. Events occur without observable causes, or actions fail to produce their expected effects. For example, boy spills milk onto table, but no milk traces appear on the table surface. Violation of Common Sense. This category covers structural or semantic inconsistencies that conflict with basic commonsense knowledge. Abnormal Human Body Structure. The generated human body deviates from normal anatomical structure. Examples include extra or missing body parts (e.g., two heads, three or six fingers), or impossible body bending that is incompatible with human physiology. Abnormal General Object Structure. Non-human objects exhibit structures that are inconsistent with their typical shapes or assembly, such as missing essential components or impossible connections. Text Distortion. Text appearing in the scene is severely distorted, malformed, or rendered as illegible gibberish without coherent semantic content, beyond mild degradation commonly observed in real footage. A.2. Annotation Platform Our annotation platform presents each AI-generated video alongside its corresponding real counterpart in synchronized comparison view (Figure 7). This side-by-side layout allows annotators to directly contrast suspicious regions in the fake video with how the same scene should plausibly appear in real footage, making it easier to distinguish genuine physical phenomena from artifacts that only occur in AIGC videos. For every identified clue, annotators are required to select fine-grained artifact category, provide detailed textual explanations for both the fake and real videos, and supply precise spatio-temporal annotations by marking time spans and bounding boxes in both streams. By enforcing mirrored annotations on fakereal pairs, the platform encourages annotators to explicitly encode both what is wrong in the generated video and what is normal in the real video, guiding the model toward learning an unbiased perceptual representation that treats real and synthetic content in symmetric manner. Figure 7. Annotation platform UI. A.3. Chain-of-Thought Annotation Prompt Design To transform concise human annotations into trainingready step-by-step supervision, we design structured prompt for Gemini-2.5-Pro that operates on each fakereal video pair. For every annotated instance, the model receives sampled frames from the fake and real videos together with the curated artifact Type, Textual Explanation, Timestamps, and Bounding Boxes, and is instructed 15 Figure 9. Visualization of Class Activation Maps (CAMs) produced by DeMamba on real video samples. T-SNE embedding space, fake and real samples are highly overlapping and difficult to separate. CAM heatmaps further reveal that, for real-labeled videos, the model consistently focuses on similar spatial locations across different samples, particularly in the third and fourth frames of the sequences. This suggests that the model may be overly sensitive to fixed visual patterns or preferred spatial locations in the scene, rather than learning generalized content-based cues such as human motion or manipulation traces. Overall, the model does not attend to regions that are discriminative for authenticity, but rather to textures outside the main content of the frame, indicating limited generalization. NSG-VD: This classifier leverages reference dataset, providing NSG feature baselines from real videos during inference. Specifically, the normalized spatiotemporal gradient (NSG) features of the reference data are used to model the distribution of real video dynamics, and the maximum mean discrepancy (MMD) between test videos and this reference distribution is computed. test video is classified as AI-generated if its MMD exceeds predefined threshold. In our reproduction, the model achieved near-perfect AUROC on the validation set but behaved poorly on the test set, indicating strong tendency to overfit. This overfitting may stem from the intrinsic sensitivity of NSG-based methods to subtle distributional shifts: the model struggles to generalize when the spatiotemporal dynamics of real videos deviate even slightly from those observed during training. When evaluating NSG-VD on the OOD GenVideo Benchmark, we consider it inappropriate to use the real samples within the GenVideo Benchmark as the reference dataset. Doing so would contradict the purpose of forgery detection and could introduce data leakage. Therefore, we retain the same reference dataset used during training. Similarly, in our robustness study, we do not use real samples under various degraded formats as reference data, because assuming access to degradation-specific real videos for each testing condition is impractical in real-world applications. This decision to reuse the training-time reference dataset may partly explain the suboptimal performance of NSG-VD in both experiments. Overall, while the reference-dataset mechanism in NSGFigure 8. The T-SNE result of Demamba. to produce two independent CoT strings: one that carefully discovers all artifacts in the fake video and one that systematically clears the corresponding regions in the real video. The prompt enforces standardized JSON output format (with separate fake_cot_annotation and real_cot_annotation fields), requires extagging of temporal spans and spatial regions, plicit through an oband guides serveunderstanddraftreviewconclude workflow with incontext examples. This design allows us to automatically expand precise but terse human labels into rich, consistent CoT supervision suitable for SFT. The complete prompt is provided in Figure 13 the reasoning process A.4. Detailed Statistics of ViF-CoT-4K We further reveal the statistic details of ViF-CoT-4K and ViF-Bench, including detailed report of the proportion of different types of artifacts annotated in ViF-CoT-4K (Table. 6), and technical details of the video generation model in ViF-CoT-4K and ViF-Bench (Table. 7). A.5. Generated Video Examples We demonstrate the quality of our dataset and benchmark by showing several examples randomly selected from the ViFBench (Figures 14& 15). As shown in the image, fake samples in our dataset are generated by latest video generation models, and are closely aligned with their real counterparts to mitigate their gap in semantics and format. B. Analysis of Detection Capabilities B.1. Binary Detectors We take Demamba [5] and NSG-VD [83] as examples of underperforming classifiers that exhibit strong tendency to label samples as fake. Demamba: Through T-SNE visualization (Figure 8) and CAM heatmap analysis (Figure 9), we observe that in the Figure 10. Response examples of off-the-shelf MLLMs. VD provides strong detection capability when the distribution of the target data is known or partially accessible, its reliance on such reference information limits its effectiveness in OOD or noisy scenarios, where the real samples to be evaluated are either unavailable or should not be incorporated into the reference set in the first place. B.2. Off-the-Shelf MLLMs Figure 10 provides concrete examples of the failure modes discussed in the main paper when directly prompting off-theshelf MLLMs for AI-generated video detection. In panel (a), we show false negatives on fake videos. Even with explicit chain-of-thought instructions, most models focus on highlevel semantics and overall visual appeal (e.g., the scene looks natural or the movements are smooth) while overlooking intrinsic forgery cues such as inconsistent geometry or physics-violating motion. As result, they confidently classify clearly synthetic videos as real and provide rationales that largely describe the content instead of analyzing subtle spatiotemporal artifacts. Panel (b) shows the opposite pattern. Models such as Gemini-2.5-flash [11] tend to over-interpret natural video degradations, including compression artifacts, motion blur, and low-light noise, as evidence of forgery. In these cases, the model produces detailed yet incorrect explanations that attribute the degradations to AI generation rather than common acquisition or post-processing effects. This confirms our quantitative findings that off-the-shelf MLLMs tend to conflate quality with authenticity: they are sensitive to superficial visual cues but struggle to distinguish genuine forgery artifacts from benign imperfections in real-world videos. B.3. Existing MLLM-based Detectors We further analyze BusterX++ [71, 72], recent MLLMbased detector that adapts pretrained models for AIGC video detection. Figure 11 (a) shows success case where BusterX++ correctly identifies an AI-generated video. In such scenarios, the synthetic content exhibits obvious stylistic or aesthetic discrepancies from typical real videos (e.g., overly smooth textures or globally inconsistent lighting), which align well with the models training biases and allow it to reach the correct decision. However, panels (b) highlight the limitations of relying primarily on global scene appearance. Here, the AIgenerated clip contains subtle but critical physics-violating artifacts, which humans readily notice. BusterX++, however, focuses on the overall coherence and visual quality of the scene and fails to attend to these localized spatiotemporal inconsistencies, leading to an incorrect real prediction. Together, these examples corroborate our main observation that current MLLM-based detectors behave more like general content describers: they emphasize superficial, distributionlevel cues and natural degradations, but are not yet equipped to systematically discover and reason about intrinsic forgery artifacts that are crucial for reliable AI-generated video detection. 17 Figure 11. Response examples of existing MLLM-based detector, BusterX++ [72]. C. Additional Examples C.1. Design of Prompts We specify the system and user prompt that Skyra uses in Figure 12. The system prompt specifies the models role as an AI video analyst, clearly defines the output format (a <think> reasoning block followed by one-word <answer> verdict), and constrains the reasoning to our artifact taxonomy, requiring that all findings be tagged with explicit categories, time spans, and bounding boxes. In contrast, the user prompt focuses on supplying multimodal evidence: we interleave sampled frames with their timestamps (e.g., [T=0.00s] <image> . . . [T=5.00s] <image>), so that the model can reason over the evolution of the scene, align artifacts with precise temporal positions, and improve its ability to detect subtle, time-dependent inconsistencies. C.2. Examples of the Responses of Skyra We provide the inference examples of Skyra on more samples in ViF-Bench. Figures 16 and 15 demonstrate its responses when encountering real videos. Figure 1825 exhibit different types of evidence that Skyra uses when determining that video is AI-generated. Figure 12. System prompt and user prompt design. D. Broader Impacts Our work is motivated by the growing societal risks posed by AI-generated videos, including large-scale misinformation, impersonation, and erosion of trust in authentic media. By focusing on interpretable, artifact-centric detection, Skyra aims to provide not only predictions but also grounded visual evidence that can assist journalists, fact-checkers, regulators, 18 Table 5. License of source datasets in ViF-CoT-4K and ViF-Bench."
        },
        {
            "title": "License",
            "content": "Kinetics-400 [28] Panda-70M [6] HD-VILA-100M [77] AGPL-3.0 CC BY 4.0 Snap Inc. Non-Commercial Research and platform moderators in assessing the authenticity of suspicious content. The ViF-CoT-4K dataset and ViF-Bench further offer standardized testbed for evaluating new detectors on diverse, up-to-date generators, which may contribute to more reliable and transparent AIGC safety tools. At the same time, releasing detailed artifact taxonomy, benchmark, and detector introduces dual-use concerns. In principle, insights into the failure modes of current detectors could inform future attempts to design more robust and evasive generative models. We believe that, on balance, the benefits of enabling the research community, civil society, and industry to build stronger and more interpretable defenses outweigh these risks. To mitigate potential misuse, our datasets contain only curated, non-sensitive content, and we emphasize that Skyra is intended to support human-inthe-loop verification rather than fully automated decision making or mass surveillance. We encourage downstream users to deploy our models and data in accordance with relevant regulations, to combine them with complementary safeguards such as provenance and watermarking, and to continuously stress-test detectors as the landscape of generative video models evolves. E. License ViF-CoT-4K and ViF-Bench are provided to the community under CC BY 4.0 license. By downloading our dataset from our website or other sources, the user agrees to adhere to the terms of CC BY 4.0 and the licenses of the source datasets. Licenses of the source datasets are listed in the Table 5. 19 Figure 13. Chain-of-Thought Annotation Prompt. 20 Table 6. Hierarchical distribution of artifact categories (L1L2L3) in ViF-CoT-4K. L1 Category Ratio L2 Category Ratio L3 Category Low-Level Forgery 17.2%"
        },
        {
            "title": "Texture Anomaly",
            "content": "Color & Lighting Anomaly"
        },
        {
            "title": "Move Forgery",
            "content": "11.2% Structure Anomaly Texture Jittering Unnatural Blur Color Over-Saturation Lighting Inconsistency 1.6% Camera Motion Inconsistency 5.4%"
        },
        {
            "title": "Object Inconsistency",
            "content": "28.1%"
        },
        {
            "title": "Interaction Inconsistency",
            "content": "10.0%"
        },
        {
            "title": "Unnatural Movement",
            "content": "10.0% Violation of Causality Law 6.90%"
        },
        {
            "title": "Violation of Commonsense",
            "content": "27.8% Abnormal Object Disappearance Abnormal Object Appearance Person Identity Inconsistency General Object Identity Inconsistency Shape Distortion Abnormal Rigid-Body Crossing Abnormal Multi-Object Merging Abnormal Object Splitting General Interaction Anomaly Unnatural Human Movement Unnatural Animal Movement Unnatural General Object Movement Violation of Physical Law Violation of General Causality Law Abnormal Human Body Structure Abnormal General Object Structure Text Distortion"
        },
        {
            "title": "Violation of Laws",
            "content": "82.8%"
        },
        {
            "title": "Ratio",
            "content": "3.5% 3.1% 3.6% 2.8% 2.5% 1.6% 3.6% 5.8% 1.1% 2.4% 15.2% 2.7% 2.7% 1.4% 3.2% 6.6% 0.5% 2.9% 4.1% 2.8% 10.5% 3.2% 14.1% Table 7. Overview of video generation models used to synthesize forged samples in our dataset. Ref. Cond. denotes the typical conditioning modes (T2V: text-to-video, I2V: image-to-video, TI2V: text+image-to-video). For commercial closed-source systems, parameter sizes are not publicly disclosed and thus marked as N/A (closed)."
        },
        {
            "title": "Release Date",
            "content": "Parameter Ref. Cond."
        },
        {
            "title": "Inference Sample Number HyperLink",
            "content": "2025-02-25 Wan2.1-1.3B-T 2024-08 CogVideoX1.5-T 2024-08 CogVideoX1.5-I 2025-08-28 Wan2.2-TI2V-5B(T2V) 2025-08-28 Wan2.2-TI2V-5B(I2V) 2024-12-03 HunyuanVideo HunyuanVideo-I2V 2025-05-06 VACE-1.3B-T (Wan2.1-VACE-1.3B) 2025-05-14 2025-08-28 Wan2.2-T2V-A14B 2025-08-28 Wan2.2-I2V-A14B 2025-04-21 SkyReels-V2-T2V 2025-04-21 SkyReels-V2-I2V 2025-05-06 LTX-Video(T2V) 2025-05-06 LTX-Video(I2V) Gen4-Turbo 2025-04 Hailuo-02 Pika-V2 Pixverse-V4-5 Kling-V1 Sora-2 1.3B 5B 5B 5B (MoE) 5B (MoE) 13B 13B 1.3B 14B (MoE) 14B (MoE) 14B 14B 13B 13B N/A (closed) 2025-06-18 N/A (closed) 2025-08-15 N/A (closed) N/A (closed) N/A (closed) 2024-02-15 N/A (closed) 2025-05 2024-06 T2V T2V I2V T2V I2V T2V I2V T2V T2V I2V T2V I2V T2V I2V I2V T2V T2V T2V T2V T2V"
        },
        {
            "title": "Local\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nLocal\nAPI\nAPI\nAPI\nAPI\nAPI\nAPI",
            "content": "750 744 760 747 748 750 968 165 165 165 165 164 165 165 121 137 151 152"
        },
        {
            "title": "Link\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink\nLink",
            "content": "Note. Dates are approximate and refer to the first public announcement or open release of the corresponding model family. For commercial systems with undisclosed architecture/size, Parameter is marked as N/A (closed). 21 Figure 14. ViF-Bench Video Sample Examples-I 22 Figure 15. ViF-Bench Video Sample Examples-II Figure 16. Skyras Response Example on Real Videos, Figure 17. Skyras Response Example on Real Videos, II 24 Figure 18. Skyras Response Example on Fake Videos, Texture Anomaly-Structure Anomaly Figure 19. Skyras Response Example on Fake Videos, Color & Lighting Anomaly-Color Over-Saturation Figure 20. Skyras Response Example on Fake Videos, Move Forgery-Camera Motion Inconsistency Figure 21. Skyras Response Example on Fake Videos, Object Inconsistency-Shape Distortion 26 Figure 22. Skyras Response Example on Fake Videos, Interaction Inconsistency-Abnormal Rigid-Body Crossing Figure 23. Skyras Response Example on Fake Videos, Unnatural Movement-Unnatural Human Movement Figure 24. Skyras Response Example on Fake Videos, Violation of Causality Law-Violation of Physical Law Figure 25. Skyras Response Example on Fake Videos, Violation of Commonsense-Text Distortion"
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University"
    ]
}