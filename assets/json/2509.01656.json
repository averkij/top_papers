{
    "paper_title": "Reinforced Visual Perception with Tools",
    "authors": [
        "Zetong Zhou",
        "Dongping Chen",
        "Zixian Ma",
        "Zhihan Hu",
        "Mingyang Fu",
        "Sinan Wang",
        "Yao Wan",
        "Zhou Zhao",
        "Ranjay Krishna"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT."
        },
        {
            "title": "Start",
            "content": "Zetong Zhou1, Dongping Chen1,3, Zixian Ma2, Zhihan Hu1, Mingyang Fu1, Sinan Wang1, Yao Wan1, Zhou Zhao4, and Ranjay Krishna2 1ONE Lab, HUST, 2University of Washington, 3University of Maryland, 4Zhejiang University Visual reasoning, cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReV PT to enhance multi-modal LLMs abilities to reason about and use visual tools through reinforcement learning. We introduce novel RL algorithm based on GRPO, designed to train models to reason with suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReV PT-3B and ReV PT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT. 1. Introduction Visual reasoning is core component of human intelligence. It enables us to solve wide range of problems, from daily activities such as navigation to challenging geometry problems. Unlike verbal reasoning, visual reasoning is more complex multifaceted process that requires not just straightforward logical reasoning but also sound visual perception, which further relies on atomic abilities such as object recognition, edge 5 2 0 S 1 ] . [ 1 6 5 6 1 0 . 9 0 5 2 : r Figure 1: Our Re T-3B and 7B models outperform their instruct and text-only GRPO counterparts on perceptioncentric tasks while maintaining strong general capabilities across multiple benchmarks. Dongping Chen is the project leader. Corresponding author(s): Yao Wan: wanyao@hust.edu.cn, Ranjay Krishna: ranjay@cs.washington.edu, Dongping Chen: dongping@umd.edu. Technical Report: Reinforced Visual Perception with Tools Figure 2: Re T-3B and 7B are able to solve problems through thinking and selectively employing visual tools to reach strong visual perception. We illustrate some step-by-step visual reasoning breakdowns for challenging examples that model leveraging visual tools to solve complex perception tasks and outperform commercial models GPT-4.1. detection, depth estimation, etc. Due to the complexity of visual perception, the computer vision community has developed specialized models for different perceptual tasks, such as RecognizeAnything for object recognition, DepthAnything for depth estimation, and SegmentAnything for segmentation (Yang et al., 2024, Liu et al., 2023d, Zhang et al., 2023). To leverage these advances in perception for enhanced visual reasoning and problem-solving, prior work attempts to augment (multimodal) language models with vision specialists. For example, VisProg first showcases that augmenting large language models (LLMs) with various vision models boosts models performance on diverse visual tasks (Gupta and Kembhavi, 2022). Similarly, VisualSketchPad finds that prompting GPT-4o to use sketching tools and depth models improves its performance on challenging perception and visual math benchmarks by large margins (Hu et al., 2024a). More recently, few works demonstrate that open-source multimodal language models also benefit from using visual tools and reasoning about their outputs after supervised finetuning on tool-use data (Qi et al., 2024, Liu et al., 2023c). Nonetheless, there are few limitations to learning to reason with visual tools with supervised finetuning (SFT): first, it relies on expensive data curation. Prior work heavily relies on expensive commercial models like GPT-4 to generate high-quality tool-use and reasoning traces (Qi et al., 2024, Ma et al., 2024, Shao et al., 2024a). Second, it often requires careful data filtering. Previous efforts reveal that aggressive filtering based on data format, answer correctness and other heuristics is crucial to performance gains (Hu et al., 2024b, Ma et al., 2024). Most importantly, supervised finetuning results in limited generalization: it relies on offline trajectories that pre-define particular tools to use; there is no incentive for the model to explore alternative tools or adapt to new environment outputs, thus limiting models generalization to unseen tools or problems (Guo et al., 2025, Chen et al., 2025a). To mitigate these limitations, we borrow inspiration from recent work on enhancing LLM reasoning and generalization with reinforcement learning (RL) (Zhang et al., 2025a, Guo et al., 2025, Huang et al., 2025, 2 Technical Report: Reinforced Visual Perception with Tools Figure 3: An overall pipeline of our ReV PT. (Top): Model-generated tool requests are managed by local environmentbased Tool Controller, which independently deploys visual tool services (e.g., Depth, Object Detection). These tools outputs are then fed back to the LVLM for iterative reasoning. (Bottom): When processing visual reasoning problem, Re employs K-turn rollouts where the model interacts with the tool environment to learn an adaptive policy, culminating in the final model. Zhan et al., 2025, Shen et al., 2025) and propose Reinforced Visual Perception with Tools (ReV PT). ReV PT enhances multimodal language models visual perception and general question-answering by training them to reason with visual tools via RL. ReV PT consists of cold-start and GRPO stages to enable effective RL on multimodal language models with 4 visual tools. Unlike SFT, where all the tool-use reasoning traces need to be generated and filtered in advance, we only need to select the appropriate visual questions to perform RL on. Moreover, instead of supervising the model with one correct tool-use trajectory for each question, RL allows the model to explore and learn from different possible solutions and incentivizes the model to output the best one. Our experiments with models of different sizes3B and 7Bdemonstrate that Re achieves state-ofthe-art performance (Figure 1), consistently outperforming SFT baselines across five perception-intensive benchmarks, including MMVP, MMStar, CV-Bench, BLINK, and BLINK-Hard. Notably, our approach surpasses the original instruct-tuned models by significant margins, achieving over 5% on average across all benchmarks, with significant 9% improvement on CV-Bench. Furthermore, our models outperform commercial models GPT-4.1 and Gemini-2.0-Flash on the challenging BLINK-Depth and Relation subsets. Lastly, we contribute valuable resource to the community by fully open-sourcing ReV PT along with its code and datasets. Technical Report: Reinforced Visual Perception with Tools 2. ReVPT: Thinking with Images and Tools We introduce Re T, novel two-stage framework designed to train multi-modal language models for better visual perception. It is built upon the GRPO algorithm (Shao et al., 2024b), aiming to enhance the models ability to tackle complex visual problems by integrating visual processing tools as reasoning steps. Given multi-modal query, ReV PT agent first generates reasoning process about analyzing the query and deciding whether to call tools or answer directly. Then, by iteratively analyzing the results from the tools, the model generates final response to the query. The overall architecture are illustrated in Figure 3. 2.1. Preliminary: GRPO Group Relative Policy Optimization (GRPO) is reinforcement learning algorithm that evaluates policy performance by directly comparing group of candidate responses. samples set of responses, {o1, o2, . . . , oN}. The process begins with prompt q, for which the policy œÄŒ∏ Each response is evaluated by reward function, yielding rewards ri = R(q, oi). GRPO then calculates group-relative advantage Ai for each response by normalizing its reward against the statistics of the entire group: Ai = ri mean{r1, r2, . . . , rN} std{r1, r2, . . . , rN} . (1) The policy is then updated to responses with higher relative advantages by maximizing clipped surrogate objective function. This objective is designed to ensure stable training by preventing excessively large policy updates. The full objective is: JGRPO(Œ∏) = qP(Q),{oi}G i=1 œÄŒ∏old (Oq)[ 1 i=1 min(si Ai, clip(si, 1 Œµ, 1 + Œµ)Ai) Œ≤ KL(œÄŒ∏ œÄref)] . (2) is the importance sampling ratio that measures the change between the new policy Here, si = œÄŒ∏(oiq) œÄŒ∏old (oiq) and the old policy œÄŒ∏old œÄŒ∏ KL(œÄŒ∏œÄref) regularizes the policy update by penalizing large deviations from reference policy œÄref. used to generate the samples. The Kullback-Leibler (KL) divergence penalty, 2.2. Visual Tools The efficacy of leveraging visual tools for visual reasoning has been well-established. We choose to focus on four specific visual tools as they are the most suitable for perception-oriented tasks. While we experiment with larger suite of tools, we find that the model is reluctant to use them. Our framework integrates these four high-performing visual tools, enabling their execution and subsequent result incorporation during the MLLM inference process to construct comprehensive rollout. While Table 1 offers detailed summary of each tools parameters and specifications, their core functionalities and representative use cases are elaborated upon below. Object Detection (Fu et al., 2025) is capable of open-vocabulary detection. It takes an input image for to locate instances of described objects, submitting their boundaries ‚Ñ¨out and textual query qtext Iin objects localization. oom facilitates focused analysis by taking an input image ‚Ñêin and specified region of interest ‚Ñõ 4 Technical Report: Reinforced Visual Perception with Tools Tool Description Arguments Tool Output Object Detection Open-vocabulary object detection Zoom In Edge Detection Depth Estimat ion Return depth map in colorful style Crop region and augment it Edge detection using scharr algorithm Image Image Image + Text Prompt Image + Coordinates + Factor Cropped Image Annotated Image + Boxes Edge Map Depth Map Table 1: Visual tools integrated within Re T. We illustrate their arguments, result formats, and core funtions description. to output magnified view ‚Ñê ocus mitigating the influence of irrelevant information and amplifying core features. of that region. It helps model to concentrate on pertinent areas, thereby ge De ec on is designed to identify and delineate the perimeters of objects and significant textural variations, this tool transforms an input image ‚Ñêin that emphasizes these structural edges. This grants models sharper perception of object silhouettes and surface patterns. into feature map ‚Ñ∞out Depth Estimation (Yang et al., 2024) computes spatial depth from monocular visual input. Given an image ‚Ñêin which encodes the estimated distance of scene elements from the observer. We use depth_anything_v2 to serve as the tool, equipping models with more nuanced understanding of three-dimensional arrangements and the relative positioning of objects. , it generates corresponding depth map ùíüout Figure 4: Reinforced visual tool-usage training requires high-quality and verified data. We transform two datasets into multiple-choice question, then filter out too easy or hard questions for Qwen2.5-VL-7B by multiple inference. 2.3. Cold Start Initially, our objective is to train multimodal agent using the R1-Zero method. However, during the training process, we observe progressive decline in the agents propensity to utilize tools. This phenomenon likely stemmed from the fact that solving visual tasks did not inherently require tool usage, and reasoning based on processed images represented distribution shift from the models initial training data. Consequently, we opt to incorporate cold-start data during the initial training phases as shown in Figure 4. This strategy aims to facilitate the models initial acquisition of tool utilization skills for solving visual problems. Currently, we employ GPT-4.1 as the agent to synthesize high-quality data for tool-augmented reasoning in Technical Report: Reinforced Visual Perception with Tools visual problem solving. We require the GPT model to follow predefined format when answering questions. Specifically, it should first engage in reasoning process, considering the potential utility of external tools, and then employ multi-turn generation strategy to address the problem. We explicitly encourage the model in our prompt to generate more tool-assisted rollouts. Subsequently, we filter out rollout trajectories generated by the GPT model that contain errors. After synthesizing tool-integrated reasoning data, we perform supervised fine-tuning on it. Denote the query as Qi, the rollout trajectory œÑi as sequence of actions ai . We optimize the cross-entropy loss: and observations oi ‚ÑíSFT(Œ∏) = 1 = 1 N i=1 i=1 log PŒ∏(œÑ Qi ) ni t=1 log PŒ∏(ai t, Qi , ai <t, oi <t) . (6) (7) By minimizing ‚ÑíSFT providing solid foundation for the subsequent reinforcement learning phase. , the model acquires robust Cold-Start policy for sequential vision-tool invocation, 2.4. Reward Design Vision tasks often possess ground truth data, facilitating the use of verified rewards to evaluate models responses. This approach circumvents the introduction of neural network-based reward models, thereby preventing reward hacking. Correctness. We restructure the dataset into verifiable format. The model is required to put the final direct answer or option in the box, enabling reliable rule-based verification of correctness. If the answer match the ground truth, it gets through checking. Format. In each turn, the response should enclose its thinking process in <think></think> and enclose If the response its tool call in <tool_call></tool_call> or answer in <answer></answer>. matches the pattern, it gets through format checking. Given the query and the rollout o, the reward is defined as: Reward(q, o) = 1, FormatCorrect(o) AnswerCorrect(o) 1, otherwise We adopt this naive binary reward design because previous research has demonstrated the effectiveness of using simple, rule-based reinforcement learning to produce strong reasoning behaviors (Zhou et al., 2025). 3. Experiment 3.1. Experiment Setups Models. We conduct most of our experiments on Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct (Wang et al., 2024a) for their strong visual perception capability and how different perception baseline contribute to final capability. 6 Technical Report: Reinforced Visual Perception with Tools Model Avg. Count Relation Depth Distance Avg. CV-Bench Gemini-2.0-Flash GPT-4.1 TACO (Qwen2-VL-7B) 76.09 71.95 76.06 67.77 58.55 63.32 86.92 92.00 81.08 87.50 94.50 59.83 82.17 89.50 57. 81.50 84.76 66.00 72.55 Qwen2.5-VL-3B-Instruct 58.98 68.65 79.42 Qwen2.5-VL-3B-SAT-SFT 60.99 60.66 72.90 60.60 69.16 Qwen2.5-VL-3B-GRPO Re VP T-3B (Ours) 65.67 70.43 81.20 +6.69 +1.78 +17.70 +11.50 +6.66 +8.65 71.67 84.50 61.50 78.33 76.00 87.33 80.17 87.50 74.92 90.15 81.23 92.62 74.41 Qwen2.5-VL-7B-Instruct 68.23 64.97 77.03 Qwen2.5-VL-7B-SAT-SFT 62.70 60.28 83.32 69.38 72.59 Qwen2.5-VL-7B-GRPO 84.23 69.42 74.11 Re VP T-7B (Ours) +5.73 +9.14 +3.85 +16.67 +8.67 +9.82 88.46 84.46 95.08 92.31 72.00 85.33 83.17 88.67 74.00 82.67 84.83 82.67 BLINK(sub) MMVP MMSTAR BLINK-HARD MMMU MMBench MathVista 76.37 68.80 65.50 64.34 63.79 60.66 72.35 +8. 80.41 67.91 75.87 73.64 -6.77 79.34 88.00 67.00 62.67 58.33 69.00 68.70 +6.03 74.00 62.67 75.33 72.00 -2.00 69.40 69.80 49.53 53.40 50.84 46.60 53.87 -0. 61.80 49.80 62.27 61.07 -0.73 68.28 66.13 33.13 41.67 56.71 51.61 60.48 +18.81 50.00 57.00 61.00 62.37 +12.37 71.7 74.00 44.0 49.00 42.11 45.44 49.00 0. 52.00 45.22 51.88 50.89 -1.11 89.05 86.60 81.80 80.97 80.09 83.69 83.21 +2.24 85.08 82.49 86.83 87.20 +2.12 73.1 70.40 41.9 61.20 56.66 54.95 57.90 -4. 68.10 59.50 63.20 66.00 -2.10 Table 2: Comparison between different models and method on vision-centric benchmarks. Our Re outperform direct SFT and text-space GRPO in perception-specific tasks and preserve general capability. Dataset Construction. high quality cold-start dataset and difficult dataset for RL training is very important for model to learn test-time scaling itself (Yu et al., 2025). Therefore, we filter the SAT dataset (Ray et al., 2024) and Taco (Ma et al., 2024) training set with Qwen2.5-VL-7B-Instruct and retain the samples it answers incorrectly. SAT is synthetic VQA dataset designed to improve models visual perception, which has been shown to be effective in prior work (Zhou et al., 2025). TACO dataset was also utilized in previous research (Ma et al., 2024) for multi-modal chain-of-thought SFT. We selected it based on the assessment that its VQA content is better suited for tool-assisted responses. Then, we split it into cold-start data synthetic and training data. To construct high-quality cold-start dataset, we leverage GPT-4.1 to synthesize tool trajectory and retain the subset that it answers correctly. Finally, we obtain 1.5k cold-start dataset with well-curated reasoning chain and tool trajectory and 20k question for RL process. Baselines. We compare Re against the following models and approaches: (1) Commercial Models: We select GPT-4.1 (OpenAI, 2025a) and Gemini-2.0-Flash (Google, 2024) as strong multimodal baselines. Both are evaluated in zero-shot setting without external tools as strong generalist benchmarks. (2) Taco: Learns to invoke 15 external tools (e.g., OCR, depth, etc.) by generating Chain-of-Thought-and-Action (CoTA) sequences via supervised learning on synthetic data. TACO typically executes tools within single process, contrasting with our RL-based approach and distributed architecture (Ma et al., 2024). (3) Qwen-Instruct: We implement two instruct version models without any tool usage, employing basic prompt formats for robust evaluation. (4) Qwen-SAT-SFT: Two models after supervised finetuning with the SAT training set (13k samples) as strong baseline. We also include an enhanced SFT baseline with rephrased answers generated by the model itself as reported by previous research (Wang et al., 2024b). (5) Qwen-SFT (cold start): The model state after cold-start training. (6) Text-based RL: An RL agent trained similarly to ReV PT but without any visual feedback from tools. This follows the native GRPO training in VisualThinker-R1-Zero (Zhou et al., 2025). Evaluation. We select 8 multi-modal benchmarks covering diverse capabilities with focus on visual perception and reasoning tasks. This includes perception-centric benchmarks CV-Bench (Tong et al., 2024a), Blink (Fu et al., 2024), MMVP (Tong et al., 2024b), Blink-Hard (Bigverdi et al., 2024). We also include MMStar (Chen et al., 2024), MMMU-val (Yue et al., 2024) and MMBench (Liu et al., 2024b) for general capabilties assessment. Reasoning benchmark MathVista (Lu et al., 2024) is also included. We choose subset Technical Report: Reinforced Visual Perception with Tools Data Construction Hint Avg. CV-Bench BLINK Tool-usage ene al Count Relation Depth Distance Avg. Count Depth Relation MMVP MMSTAR BLINK-HARD 3 4 82.50 87.69 78.28 66.62 82.33 88.62 77.26 65.74 78.97 68.15 87.23 80.67 80.86 66.24 91.54 87.17 77.75 65.00 73.39 83.92 76.83 81.12 76.76 60.00 79.03 72.83 80.33 78.46 65.83 72.58 81.82 79.17 80.17 65.83 79.84 81. 68.67 69.33 69.00 72.67 60.40 58.33 58.33 61.80 65.32 54.84 51.61 61.29 54.03 59.68 63.71 50.81 41.94 68.55 55.65 60.48 Table 3: Ablation study on Re T-7B SFT data construction demonstrates that synthesized tool-usage data and general data from TACO collaboratively contribute to an effective cold-start process, preserving the models general capabilities while enabling visual tool usage. Specifically, training with synthesized data where the model provides 1-2 sentence reasoning about task suitability for visual tools proves beneficial for this dual objective. with only one image as input. All results are reported under average of three runs. Experiment Details. We conduct model training using configurations with 8 NVIDIA A800 GPUs. We leverage LLaMA-Factory (Zheng et al., 2024) as SFT platform and Verl as visual tool-base RL platform (Sheng et al., 2024). The training process involved two phases: (1) Cold-start Period: Models are trained for 2 epochs with learning rate of 1e-5 and global batch size of 64. (2) RL Period: For this phase, models are trained for 200 steps to reach reward converge. We provide training loss and reward curve in Appendix. 3.2. Experiment Results Re outperform supervised finetuning and text-based RL on visual perception tasks. As shown in Table 2, our method significantly outperforms the original model on both CV-Bench and BlinkHard benchmarks, while also demonstrating improvements over SFT cold start models. The performance gains are particularly pronounced in depth and distance tasks, with improvements of 9.5% and 13.23% respectively compared to the base model, indicating that reinforcement learning effectively teaches the model to utilize tools and interpret tool-generated information. The consistent improvements across ReV PT-3B (+5.81%) and Re T-7B (+5.73%), with striking gains on perception-heavy benchmarks like CV-Bench (+8.65% and +9.82%), demonstrate that reinforcement learning unlocks latent capabilities inaccessible through conventional training. This ceiling elevation stems from RLs unique ability to enable exploratory tool selection and adaptive reasoning during inference, allowing models to dynamically adjust problem-solving strategies rather than being constrained to pre-defined patterns from static supervised trajectories. The fact that Re substantially outperforms not only base instruct models but also specialized baselines suggests that reward-driven optimization creates synergistic learning dynamic that transcends single training methodologies, indicating the true potential of visual perception capabilities may be significantly higher than previously demonstrated. Performance tradeoffs in perception-specialized training. Our training on the curated perception subset from SAT and TACO resulted in slight reduction of the models general performance capabilities. Direct supervised fine-tuning on the SAT dataset (SAT-SFT) revealed substantial degradation across several capabilities required for BLINK and MMStar evaluations. These findings underscore the critical relationship between cold-start sample distribution and the resulting model capabilities. Our key insight is that developing effective visual tool-using agents requires not only sophisticated interaction environments but also carefully balanced, comprehensive, and high-quality data construction that preserves general capabilities while enhancing targeted skills. 8 Technical Report: Reinforced Visual Perception with Tools Figure 5: Re T-3B effectively learns tool utilization after cold-start process. ReV PT-3B shows bias toward object detection and depth estimation over zoom and edge detection tools due to cold-start data construction. Future work will address data balance and generalized perception objectives. Figure 6: Re T-3B learns to improve accuracy by changing its tool usage after reinforcement learning. This strategic shift results in higher proportion of correct answers, demonstrating that the RL phase optimizes models tool selection. Visual Tools RL CV-Bench BLINK Obj ect Dep th Count Relation Depth Distance Avg. Count Depth Relation 70.05 68.53 66.62 76.22 80.40 60.00 70.16 77.67 78.62 55.83 69.35 84.62 71.33 74.83 76.99 59.17 73.39 69.67 70.43 92.62 87.50 78.33 81.58 60.83 75.81 81. 85.17 86.33 86.50 91.08 90.46 87.54 MMVP MMSTAR BLINK-HARD 3 5 63.67 63.33 51.00 70.33 54.53 53.93 53.47 53.87 66.13 57.26 58.06 62.10 58.87 58.87 61.29 55.65 54.03 66.13 57.26 60.48 Table 4: Ablation study on visual tools and RL phase showing that each component within ReV PT-3B collaboratively contribute to our outstanding results on vision perception tasks. 3.3. In-Depth Analysis Ablation study on how to introduce tool-use capability to current models. To address the above mentioned challenge, we conducted an ablation study on cold-start process (shown in Table 3) to determine how to introduce tool-use agent capabilities to the current model without compromising its general performance. Our empirical experiments demonstrate that synthesizing hints about whether and why to use tools can drastically preserve the models general capabilities. Moreover, including general data from TACO also contributes to maintaining general model capability, while incorporating tool-use data enhances the models tool-using performance. Ablation study on visual tools. We conduct ablation studies on the two most frequently used visual toolsobject detection and depth estimationby removing these tools from both data and instructions to validate their importance as core components of ReV PT. The experiments follow the same cold-start and RL training procedures, with results shown in Table 4. When removing object detection, performance declines significantly across most metrics: BLINK relation dropped by 5 points (8.2% decrease) from 60.83 to 55.83, and MMVP scores fall by 12.33 points (9.6% decrease) from 70.33 to 63.33. This confirms that visual tool usage is fundamental to ReV PTs effectiveness. 9 Technical Report: Reinforced Visual Perception with Tools Figure 7: Case studies illustrating various failure modes of ReV PT when using visual tools.Examples include incorrect tool output (top left) , misinterpretation of correct tool output (top right) , inappropriate tool usage that interferes with perception (bottom left) , and the selection of an unhelpful tool for the given task (bottom right). Tool usage analysis. We analyze the frequency of tool usage by cold-start and RL phase models across different benchmarks, as shown in Figure 5. On perception-intensive benchmarks like CV-Bench and MMVP, models exhibit high tool usage frequency, while on the more general MMSTAR benchmark, tool usage frequency decreases substantially. Furthermore, we observe that object detection and edge detection usage decline significantly after RL training, resulting in an overall reduction in tool usage frequency. To figure out how vision tools contribute to models improvement, we examine the changing portion (Figure 6) between ReV PT-3B and base model. Across 4 benchmarks, we find that ReV PT-3B achieves higher accuracy when using tools, indicating that visual tool-usage provide more information and select more appropriate tools for visual perception tasks. Limitations of visual tools. During our experimental process, we observe that these visual tools often hinder rather than enhance model performance, sometimes degrading or interfering with the models reasoning capabilities. In some cases, the model needed to correct erroneous tool outputs to arrive at accurate answers. For instance, in Figure 7 (top left), the object detection model misclassify mattress as pillow, resulting in an incorrect response despite the models inherent capability to solve the problem correctly. Case study in Figure 7 (top right) shows that although the depth estimation tool returned color-coded depth map, the model do not acknowledge this information and use it during reasoning, ultimately producing an incorrect answer. We attribute this phenomenon to the models original perception capabilities constraining its ability to effectively and accurately utilize external tools, limiting tool-use efficacy. 4. Discussion and Future Works Do we really need visual tool specialist? The utility of visual tool specialists follows non-monotonic relationship with model capability. For resource-constrained models like our 3B variants, visual tools provide 10 Technical Report: Reinforced Visual Perception with Tools compelling shortcut to enhanced perception, with ReV PT achieving over 9% improvements on perceptionheavy benchmarks. This aligns with findings from TACO (Ma et al., 2024), where chain-of-thought-and-action approaches enabled performance gains of 30-50% compared to few-shot prompting. However, as models scale to moderate sizes, the marginal benefit diminisheslarger vision encoders reduce dependency on external tools, and our analysis reveals cases where tool outputs contradict correct model intuitions, leading to degraded performance. Paradoxically, the most advanced models like GPT-o3 (OpenAI, 2025b) may experience tool renaissance through dynamic code generation, representing shift from using pre-defined tools to creating sophisticated reasoning chains. This suggests tool integration is worthwhile across the capability spectrum, with beneficial tools evolving from compensating for perceptual deficits in small models to enabling sophisticated multi-step reasoning in large ones. Which visual tools should we select for tool-use training? The selection of visual tools fundamentally shapes model capabilities and constrains downstream performance. Our domain-specific approach prioritized tools aligned with perceptual challenges: object detection, depth estimation, edge detection, and zoom-in functionality. While this reflects broader patterns where tool selection is guided by target applications, it reveals fundamental limitations in current approaches that constrain generalizability. The core challenge lies in the cold-start nature of tool learning, which necessitates committing to specific tool repertoire during initial training (Zhang et al., 2025b, Wang et al., 2025c). Our experiments initially included broader tools (e.g., auxiliary line drawing, highlighting, rotation) but found extremely low utilization rates, underscoring that smaller models struggle to learn multiple tools simultaneously due to limited world knowledge. The constraint is data-drivenachieving proficiency across diverse tool sets requires comprehensive training datasets. Our reliance on SAT and TACO datasets lacks the breadth for more specialized tools, suggesting that advancing visual tool learning requires systematic development of richer, more diverse training ecosystems. The Bitter Lesson of Tool Use: Human Intuitions vs. Computational Discovery. fundamental tension exists in tool-using AI systems: while any approach producing correct solutions should be considered valid regardless of human expectations, current training paradigms systematically inject human biases about tool selection. This occurs inevitably through cold-start phases where synthetic data generation relies on humandesigned demonstrations, creating imitation learning dynamics that persist even after reinforcement learning begins. The history of AI suggests that methods leveraging greater computation for autonomous discovery will ultimately outperform those constrained by human intuitions about appropriate tool usage. However, practical limitations force this bias injectionopen-source agents employing cold-start supervised fine-tuning require initial human demonstrations because smaller models lack the capability to explore effectively from scratch. Approaches like process reward models (Lightman et al., 2023, Wang et al., 2025b) likely amplify rather than resolve these human-centric biases. This creates computational scaling challenge: as systems grow more capable, they should increasingly discover tool-use strategies aligned with their computational strengths rather than human cognitive patterns, yet current training paradigms embed human preferences that may limit this autonomous discovery. 5. Related Work Multimodal language models. Recently, there have been many advances on open-source multimodal models Awadalla et al. (2023), Chen et al. (2023), Liu et al. (2023b,a, 2024a), Dai et al. (2024), Li et al. (2022, 2023), Deitke et al. (2024). These efforts include training multimodal models to take in multiple images, engage in multi-turn conversations, and even understand videos Liu et al. (2024a), Jiang et al. (2024), 11 Technical Report: Reinforced Visual Perception with Tools Li et al. (2024). For example, LLaVA-Next achieves strong multi-image understanding through large-scale interleaved visual instruction tuning with M4-Instruct Liu et al. (2024a). Similarly, Mantis introduces new large-scale multi-image instruction tuning dataset Mantis-Instruct for multi-image training Jiang et al. (2024). These efforts pave the foundation for our work on learning vision-language models with image-text interleaved reasoning traces. over perceptual outputs from vision specialists. Multimodal tool-use. Recently, there has been increasing interest in enhancing multimodal language models with improved tool-use capabilities Liu et al. (2023c), Qi et al. (2024), Shao et al. (2024a). LLaVa-Plus demonstrated the feasibility of training such models to utilize visual tools Liu et al. (2023c). Visual Program Distillation transfers tool-use and reasoning skills into multimodal model using chain-of-thought (CoT) data derived from programs Hu et al. (2024b). Similarly, Visual CoT introduces synthetic CoT dataset aimed at boosting the reasoning abilities of multimodal models Shao et al. (2024a). More recently, LLaVa-CoT incorporates both perception and reasoning capabilities inspired by GPT-4o Xu et al. (2025). CogCoM identifies six key manipulation strategies and trains multimodal models with synthetic chain-of-manipulation (CoM) data Qi et al. (2024), while TACO contributes 273K multimodal reasoning traces derived from the outputs of 15 visual tools Ma et al. (2024). Recent works also extent multimodal tool-use with RL, enhancing general reasoning capability (Wang et al., 2025c, Zhang et al., 2025b, Zhao et al., 2025) and visual search capabiltiy (Zheng et al., 2025, Chen et al., 2025b, Wang et al., 2025a). Multimodal language model reasoning with RL. DeepSeek-R1 (Guo et al., 2025) has demonstrated that simple rule-based reinforcement learning can effectively induce strong reasoning behaviors. This R1-style reasoning paradigm has shown success in multimodal language models (Shen et al., 2025, Li et al., 2025, Chen et al., 2025c). VLM-R1 (Shen et al., 2025) applies reward-driven fine-tuning to improve visual reasoning and generalization beyond supervised baselines. Vision-R1 (Zhan et al., 2025) introduces human-free alignment through vision-guided rewards, while another Vision-R1 (Huang et al., 2025) leverages CoT-style tasks and reward shaping to boost multi-step reasoning. UI-R1 (Lu et al., 2025) enhances action prediction in GUI agents via GRPO-based RL, achieving strong performance with compact models. In the video domain, Video-R1 (Feng et al., 2025) and VideoChat-R1 (Li et al., 2025) improves temporal reasoning through T-GRPO and mixed-modality rewards across diverse video types. 6. Conclusion In this work, we explore tool-usage reinforcement learning to enable models to utilize external vision tools for test-time scaling. We propose ReV PT, training suite designed to integrate tool-usage with RL, enabling models to optimize tool selection and interleaved text-image-tool reasoning through direct interaction and reward feedback. Our experiments across multiple benchmarks demonstrate that Re successfully raises performance beyond base model, achieving results significantly higher than those obtained through supervised fine-tuning and text-only RL. We hope that Re T, along with our fully open-sourced code, dataset, and platform, will serve as valuable resource for the community."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Jieyu Zhang, Weikai Huang, and Xirui Li for their insightful feedback and support. 12 Technical Report: Reinforced Visual Perception with Tools"
        },
        {
            "title": "References",
            "content": "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models, 2023. Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. arXiv preprint arXiv:2412.03548, 2024. Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3D: large benchmark and model for 3D object detection in the wild. In CVPR, Vancouver, Canada, June 2023. IEEE. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. Yang Chen, Yufan Shen, Wenxuan Huang, Shen Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Botian Shi, and Yu Qiao. Learning only with images: Visual reinforcement learning with reasoning, rendering, and visual feedback. arXiv preprint arXiv:2507.20766, 2025b. Zhangquan Chen, Xufang Luo, and Dongsheng Li. Visrl: Intention-driven visual perception via reinforced reasoning. arXiv preprint arXiv:2503.07523, 2025c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. arXiv preprint arXiv:2501.18954, 2025. 13 Technical Report: Reinforced Visual Perception with Tools Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. Google. Gemini 2.5 flash. https://deepmind.google/technologies/gemini/flash/, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. ArXiv, abs/2211.11559, 2022. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models, 2024a. URL https://arxiv.org/abs/2406.09403. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into visionlanguage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95909601, 2024b. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Dongfu Jiang, Xuan He, Huaye Zeng, Con Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312. 14 Technical Report: Reinforced Visual Perception with Tools Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://llava-vl.github. io/blog/2024-01-30-llava-next/. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li. Llava-plus: Learning to use tools for creating multimodal agents, 2023c. URL https://arxiv.org/abs/2311.05437. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023d. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Uir1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, et al. Taco: Learning multi-modal action models with synthetic chains-of-thought-and-action. arXiv preprint arXiv:2412.05479, 2024. OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, 2025a. Large language model. Accessed: 2025-05-11. OpenAI. Openai o3-mini, 2025b. URL https://openai.com/index/openai-o3-mini/. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, and Jie Tang. Cogcom: Train large vision-language models diving into details through chain of manipulations, 2024. URL https://arxiv.org/abs/2402.04236. Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 3, 2024. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models, 2024a. 15 Technical Report: Reinforced Visual Perception with Tools Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms, 2024b. Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025b. Ye Wang, Qianglong Chen, Zejun Li, Siyuan Wang, Shijie Guo, Zhirui Zhang, and Zhongyu Wei. Simple o3: Towards interleaved vision-language reasoning. arXiv preprint arXiv:2508.12109, 2025c. Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang. theoretical understanding of self-correction through in-context alignment. arXiv preprint arXiv:2405.18634, 2024b. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2025. URL https://arxiv.org/abs/2411.10440. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, 16 Technical Report: Reinforced Visual Perception with Tools Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv preprint arXiv:2503.18013, 2025. Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025a. Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025b. Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. arXiv preprint arXiv:2306.03514, 2023. Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: Agentic vision with dynamic tooling. arXiv preprint arXiv:2507.07998, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model, 2025. URL https://arxiv.org/abs/2503. 05132. 17 Technical Report: Reinforced Visual Perception with Tools A. Dataset Construction Details We leverage the SAT dataset and the CoTA dataset from TACO for our training. The SAT (Ray et al., 2024) dataset is synthetic VQA dataset designed to enhance the spatial reasoning capabilities of MLLMs. The CoTA dataset from TACO (Ma et al., 2024) is synthetic dataset comprising Chain-of-Thought-Action data generated by GPT-4o. However, we leverage it solely as VQA filter for potential tool utilization. The CoTA data comprises trajectories of thought, action, and observation. We derive QA pairs from this data using Qwen3-30B-A3B. For open-ended questions within this set, we employ Qwen2.5-VL-32B to synthesize multiple-choice options, thereby transforming the entirety of the CoTA data into MCQA format. Subsequently, we task Qwen2.5-VL-7B with answering all the questions and retain only those that are answered incorrectly. These erroneous examples are more likely to exhibit both correct and incorrect responses during the sampling of rollouts in GRPO training. The resulting dataset is then randomly split into cold-start dataset and RL dataset. For the cold-start dataset, we utilize GPT-4.1 as the agent to synthesize tool-integrated reasoning rollouts, filtering out any rollouts that lead to incorrect answers. The SAT data is inherently in MCQA format. We randomly shuffled the answer options and subsequently filtered out data instances for which Qwen2.5-VL-7B provided an incorrect response. B. Experiment Setup Details B.1. Benchmark and Dataset Details In this paper, we evaluate five multi-modal benchmarks covering diverse visual reasoning capabilities: CV-Bench (Tong et al., 2024a), Blink (Fu et al., 2024), Blink-Hard (Bigverdi et al., 2024), MMVP (Tong et al., 2024b), and 1,000 randomly selected subset from SAT (Ray et al., 2024). Our dataset construction incorporates single-image samples from both SAT (Ray et al., 2024) and Taco (Ma et al., 2024). The following sections provide detailed descriptions of these benchmarks and datasets: CV-Bench (Tong et al., 2024a): vision-centric benchmark containing 2,638 manually-inspected examples for evaluating MLLMs. It repurposes standard vision benchmarks (ADE20K (Zhou et al., 2017), COCO (Lin et al., 2014), Omni3D (Brazil et al., 2023)) to assess fundamental 2D and 3D understanding capabilities including spatial relationships, object counting, depth ordering, and relative distance estimation. Questions are programmatically constructed and manually verified for clarity and accuracy. Blink (Fu et al., 2024): benchmark with 3,807 multiple-choice questions reformatting 14 classic computer vision tasks to test fundamental visual perception abilities. Despite humans achieving 95.70% accuracy, even top models like GPT-4V and Gemini achieve only 51.26% and 45.72% accuracy respectively. Blink highlights significant gaps between human-level visual perception and current MLLM capabilities. Specifically, we experiment with relative depth, count and spatial reasoning tasks in Blink. Blink-Hard (Bigverdi et al., 2024): challenging benchmark focused on visual perception tasks requiring deeper 3D understanding and spatial reasoning. It evaluates whether MLMs can produce and reason with intermediate representations like depth maps and bounding boxes. The benchmark serves as testbed for approaches like Perception Tokens that assist models in tackling complex visual reasoning problems. MMVP (Tong et al., 2024b): Contains 300 questions based on 150 pairs of CLIP-blind images that appear 18 Technical Report: Reinforced Visual Perception with Tools similar to CLIP models despite clear visual differences. The benchmark categorizes questions across nine visual patterns and reveals systematic shortcomings in MLLMs visual perception abilities, with even top models showing performance gaps of over 50% compared to humans. MMMU (Yue et al., 2024): massive, multi-discipline benchmark requiring expert-level knowledge to evaluate advanced MLLMs. It consists of 11.5K questions spanning six core disciplines from college-level exams and textbooks. MMMU tests perception and reasoning abilities on complex visual materials like diagrams, charts, and chemical formulas. MMBench (Liu et al., 2024b): comprehensive benchmark designed to evaluate MLLMs on wide range of capabilities, from basic perception to advanced reasoning. It features carefully curated set of multiple-choice questions covering 20 skill areas. MMBench employs circular evaluation strategy on held-out test set to provide robust assessment of model performance while preventing overfitting. MathVista (Lu et al., 2024): benchmark focused on visual mathematical reasoning, designed to test MLLMs on diverse set of 6,141 problems. Sourced from 28 existing math datasets and 3 newly created ones, its questions cover wide range of tasks, including geometric problems, statistical chart interpretation, and scientific puzzles. MathVista requires models to integrate visual perception, mathematical logic, and multi-step reasoning. SAT (Ray et al., 2024): Contains 218K question-answer pairs covering 22K synthetic scenes testing both static and dynamic spatial reasoning. Unlike benchmarks focused on static relationships, SAT (Ray et al., 2024) incorporates perspective-taking and egocentric action recognition. Generated using photo-realistic physics engine, it can be scaled and extended to include new scenarios. Taco (Ma et al., 2024): framework and dataset with over 1 million synthetic chains-of-thought-andaction traces (filtered to 293K high-quality examples). TACO (Ma et al., 2024) enables models to perform step-by-step reasoning while invoking external tools (OCR, depth estimation, calculators) to solve complex visual problems, showing performance improvements of up to 15% on challenging multimodal tasks. B.2. Training Details We fine-tune Qwen2.5-VL-3B and Qwen2.5-VL-7B on eight A800 GPUs. The detailed training parameters for cold-start and reinforcement learning are listed in Table 5 and Table 6. The reward curve of our method REVPT compared to text RL training is shown in Figure 8. Table 5: Training hyperparameters Name Value True bf16 True tf32 per_device_train_batch_size 4 gradient_accumulation_steps 2 lr weight_decay warmup_ratio lr_scheduler_type max_seq_len 1e-5 0 0.1 cosine 16384 Table 6: Training hyperparameters Name Value True True bf16 tf32 per_device_train_batch_size 8 ppo_mini_batch_size num_generation kl_loss_coef lr weight_decay warmup_ratio lr_scheduler_type max_seq_len 128 8 1e-3 2e-6 0.01 0.03 cosine 16384 19 Technical Report: Reinforced Visual Perception with Tools Figure 8: Our reward gradually upgrade and reach converge. The hyperparameters used for the agent are detailed in Table 7. During training with the veRL framework, we observed degradation in the response quality when batch-inferencing with Qwen2.5-VL by vllm as the batch size increased. Consequently, we imposed limit on the batch size used for each inference step. Table 7: Agent config Name Value max_turns max_tokens_per_turn 1024 inference_batch_size 8 C. Case Study We show some case studies that our model successfully using tools in Figure 9 ,10 ,11 and 12. We also include some failure cases that visual tools return wrong output and disturb models reasoning chain in 13, 14 and 15. 20 Technical Report: Reinforced Visual Perception with Tools Question: Considering the relative positions of the bottle (annotated by the red box) and the fork in the image provided, where is the bottle (annotated by the red box) located with respect to the fork? (A) left (B) right Answer: (B) Assistant: <think> bottle (red box) is located on the table near the center of the image. The fork is also visible, likely placed on the table as well. To determine their relative positions, should analyze the locations of both objects. The bottle appears to be more towards the middle-right side of the table, while the fork is closer to the center or slightly left. To confirm, will use the edge detection tool to better distinguish the boundaries and positions of these objects.</think> <tool_call>{\"name\": \"edge_detection\", \"arguments\": {\"image_id\": 0}}</tool_call> User: <image> <result>The edge map for image 0.</result> Assistant: <think>From the edge detection result, it is clear that the bottle (in the red box) is positioned more towards the center-right area of the table. The fork is visible beside the bottle, further to the right. This confirms that the bottle is to the right of the fork.</think> <answer>Based on the image and the edge detection result, the bottle (red box) is located to the right of the fork. Therefore, the answer is boxedB.</answer> Figure 9: Case study for the tool type: Edge Detection. 21 Technical Report: Reinforced Visual Perception with Tools Question: bounding box is an annotated rectangle surrounding an object. The edges of bounding boxes should touch the outermost pixels of the object that is being labeled. Given the two bounding boxes on the image, labeled by and B, which bounding box more accurately localizes and encloses the knife? Select from the following options. (A) Box (B) Box Answer: (B) Assistant: <think>Let≈õ carefully examine the positions of the two bounding boxes, and B. Box is around what appears to be book or stack of papers, while Box is around silver object that resembles knife. The knife is clearly visible and positioned in Box B, whereas Box does not contain any knife-like object. To confirm, Iƒ∫l double-check the boundaries of both boxes and their contents.</think> <tool_call>{\"name\": \"zoom_in\", \"arguments\": {\"image_id\": 0, \"bbox\": [200, 490, 480, 720], \"factor\": 1.5}}</tool_call> User: <image> <result>Zoomed image 0 on [200, 490, 480, 720] with 1.5x magnification.</result> Assistant: <think>The zoomed-in view shows the region where both bounding boxes and overlap. By closely inspecting this area, can see that Box encompasses the handle and blade of knife, which is not present in Box A. This confirms my initial assessment. Therefore, Box is the one that correctly encloses the knife.</think> <answer>The bounding box labeled more accurately localizes and encloses the knife, as it includes the knifes handle and blade. Therefore, the correct answer is boxed{B}.</answer> Figure 10: Case study for the tool type: Zoom In. 22 Technical Report: Reinforced Visual Perception with Tools Question:How many ties are in the image? Select from the following choices. (A) 6 (B) 5 (C) 3 (D) 2 (E) 4 (F) 0 Answer: (E) Assistant: <think>To accurately determine the number of ties in the image, its best to use the object detection tool targeting \"tie\" to ensure all visible ties are counted, even those partially obscured or at different angles.</think> <tool_call>{\"name\": \"object_detection\", \"arguments\": {\"image_id\": 0, \"objects\": [\"tie\"]}}</tool_call> User: <image> <result>Image 1:Detected 4 object(s) in image 0: 1. tie(0.76): [87, 144, 108, 223] 2. tie(0.46): [370, 117, 387, 153] 3. tie(0.72): [247, 115, 262, 153] 4. tie(0.66): [505, 116, 517, 138]</result> Assistant: <think>The object detection tool has identified four distinct \"ties\" in the image, each marked with bounding box around the neck area. The bounding boxes correspond to the visible neckwear worn by each individual in the image. Since the tools output matches the visible evidence and there are no other neck accessories present, can confidently answer the question based on this information.<//think> <answer>Based on the object detection results and visual confirmation, there are 4 ties in the image. The tool detected one tie for the man on the left (wearing suit), and three ties for the three men in military uniforms. boxed{B}.</answer> Figure 11: Case study for the tool type: Object Detection. 23 Technical Report: Reinforced Visual Perception with Tools Question:Which object is closer to the camera taking this photo, the table (highlighted by red box) or the television (highlighted by blue box)? Select from the following choices. (A) table (B) television Answer: (E) Assistant: <think>To determine which object is closer to the camera.the table (red box) or the television (blue box).its best to use depth estimation. This will allow me to visually compare the relative distances of these two objects from the camera.<//think> <tool_call>{\"name\": \"depth_estimation\", \"arguments\": {\"image_id\": 0}}</tool_call> User: <image> <result>Image 1:The colored depth map for image 0.</result> Assistant: <think>In the depth map, warmer colors indicate areas closer to the camera, while cooler colors represent regions farther away. The area corresponding to the table (red box in the original image) appears much warmer compared to the region where the television (blue box) is located. This suggests that the table is closer to the camera than the television. No further tools are needed as the depth map provides clear evidence.<//think> <answer>By analyzing the depth map, it is evident that the table (red box) is closer to the camera than the television (blue box). The tables region shows significantly warmer colors, indicating proximity, while the televisions area is cooler, signifying it is farther away. //boxed{A}. <//answer> Figure 12: Case study for the tool type: Depth Estimation. 24 Technical Report: Reinforced Visual Perception with Tools Question:How many pictures are in the image?? Select from the following choices. (A) 0 (B) 5 (C) 2 (D) 1 (E) 3 (F) 4 Answer: (E) Assistant: <think> To accurately determine the number of pictures in the image, should use the object detection tool to identify all the pictures present. This will help ensure dont miss any that may not be immediately obvious.<//think> <tool_call>{\"name\": </tool_call> User: <image> <result>Image 1:Detected 3 object(s) in image 0: 1. picture(0.76): [483, 144, 570, 279] 2. picture(0.72): [115, 158, 188, 247] 3. picture(0.42): [747, 194, 781, 280] </result> \"object_detection\", \"arguments\": {\"image_id\": 0, \"objects\": [\"picture\"]}} Assistant: <think>The object detection tool has identified three pictures in the image, each marked with bounding box and label. The bounding boxes correspond to the framed objects on the wall: one on the left above the bookshelf, one in the middle above the cabinet, and one on the right wall. Since the detection confidence is high and matches the visible frames, there is no need for further tool usage. am ready to answer the question based on this information.</think> <answer>Based on the object detection results, there are three pictures in the image. The bounding boxes and labels confirm this count. boxed{C}. </answer> Figure 13: Case study for the error:Models Misunderstanding of Tool Results (Object Detection). Technical Report: Reinforced Visual Perception with Tools Question: Two points are circled on the image, labeled by and beside each circle. Which point is closer to the camera? Select from the following choices. (A) is closer (B) is closer Answer: (B) Assistant: <think>Looking at the image, both points and are marked near the face of the doll. To determine which one is closer to the camera, need to analyze their positions relative to the viewer≈õ perspective. Point appears lower on the doll≈õ face compared to point B, which is higher up on the forehead. However, this could be due to the camera angle or perspective. To be more certain, should use depth estimation to compare the actual spatial distances of points and from the camera.</think> <tool_call>{\"name\": \"depth_estimation\", \"arguments\": {\"image_id\": 0}}</tool_call> User: <image> <result>Image 1:The colored depth map for image 0.</result> Assistant: <think>In the depth map, warmer colors indicate areas that are closer to the camera, while cooler colors represent regions farther away. Observing the map, the region around point (near the lower part of the face) appears much warmer compared to the area around point (higher on the forehead). This confirms that point is indeed closer to the camera than point B.</think> <answer>Based on the depth map, point is closer to the camera than point B. The warm color at point supports this conclusion. boxed{A}</answer> Figure 14: Case study for the error: Models Misunderstanding of Tool Results (Depth Estimation). Technical Report: Reinforced Visual Perception with Tools Question: How many pillows are in the image? Select from the following choices. (A) 2 (B) 0 (C) 1 (D) 3 Answer: (B) Assistant: <think>To accurately determine the number of pillows in the image, its best to use object detection targeting pillow√∂r similar objects. This will help ensure dont miss any pillows that may not be immediately obvious.</think> <tool_call>{\"name\": [\"pillow\"]}}</tool_call> User: <result>No objects matching pillow. detected in image 0.</result> \"object_detection\", \"arguments\": {\"image_id\": \"objects\": 0, Assistant: <think>The object detection tool did not find any pillows, but it is not always reliable. Visually inspecting the image, there appears to be one pillow visible on the bed in the background. The rest of the items look like bags, clothes, or bedding. Since the detection tool missed it, and only one pillow is clearly visible, it seems reasonable to conclude that there is only one pillow in the image.</think> <answer>After careful inspection and considering the output of the object detection tool, there is only one visible pillow in the image.boxed{C}</answer> Figure 15: Case study for the error: Flawed Tool Results (Object Detection)."
        }
    ],
    "affiliations": [
        "ONE Lab, HUST",
        "University of Maryland",
        "University of Washington",
        "Zhejiang University"
    ]
}