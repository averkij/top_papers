{
    "paper_title": "Adversarial Video Promotion Against Text-to-Video Retrieval",
    "authors": [
        "Qiwei Tian",
        "Chenhao Lin",
        "Zhengyu Zhao",
        "Qian Li",
        "Shuai Liu",
        "Chao Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Thanks to the development of cross-modal models, text-to-video retrieval (T2VR) is advancing rapidly, but its robustness remains largely unexamined. Existing attacks against T2VR are designed to push videos away from queries, i.e., suppressing the ranks of videos, while the attacks that pull videos towards selected queries, i.e., promoting the ranks of videos, remain largely unexplored. These attacks can be more impactful as attackers may gain more views/clicks for financial benefits and widespread (mis)information. To this end, we pioneer the first attack against T2VR to promote videos adversarially, dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement (MoRe) to capture the finer-grained, intricate interaction between visual and textual modalities to enhance black-box transferability. Comprehensive experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing datasets with over 10k videos, evaluated under 3 scenarios. All experiments are conducted in a multi-target setting to reflect realistic scenarios where attackers seek to promote the video regarding multiple queries simultaneously. We also evaluated our attacks for defences and imperceptibility. Overall, ViPro surpasses other baselines by over $30/10/4\\%$ for white/grey/black-box settings on average. Our work highlights an overlooked vulnerability, provides a qualitative analysis on the upper/lower bound of our attacks, and offers insights into potential counterplays. Code will be publicly available at https://github.com/michaeltian108/ViPro."
        },
        {
            "title": "Start",
            "content": "Adversarial Video Promotion Against Text-to-Video Retrieval"
        },
        {
            "title": "Shuai Liu Qian Li Chao Shen",
            "content": "Xian Jiaotong University michaeltqw@stu.xjtu.edu.cn, {linchenhao, zhengyu.zhao, qianlix}@xjtu.edu.cn, {sh liu, chaoshen}@mail.xjtu.edu.cn 5 2 0 2 2 1 ] . [ 2 4 6 9 6 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Thanks to the development of cross-modal models, text-tovideo retrieval (T2VR) is advancing rapidly, but its robustness remains largely unexamined. Existing attacks against T2VR are designed to push videos away from queries, i.e., suppressing the ranks of videos, while the attacks that pull videos towards selected queries, i.e., promoting the ranks of videos, remain largely unexplored. These attacks can be more impactful as attackers may gain more views/clicks for financial benefits and widespread (mis)information. To this end, we pioneer the first attack against T2VR to promote videos adversarially, dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement (MoRe) to capture the finer-grained, intricate interaction between visual and textual modalities to enhance blackbox transferability. Comprehensive experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing datasets with over 10k videos, evaluated under 3 scenarios. All experiments are conducted in multi-target setting to reflect realistic scenarios where attackers seek to promote the video regarding multiple queries simultaneously. We also evaluated our attacks for defences and imperceptibility. Overall, ViPro surpasses other baselines by over 30/10/4% for white/grey/black-box settings on average. Our work highlights an overlooked vulnerability, provides qualitative analysis on the upper/lower bound of our attacks, and offers insights into potential counterplays. Our manually trained weights are available at https://github.com/michaeltian108/ViPro. We will release full code after acceptance. 1. Introduction Text-to-video retrieval (T2VR) has thrived in recent years, driven by advances in visionlanguage models (VLM) [11, 23, 37, 44], making it vital tool for users to search, discover, and consume video content across. While many T2VR models [18, 21, 40, 41] have achieved strong performance on prevailing datasets such as MSR-VTT [43], Figure 1. simplified illustration for video suppression (left) and video promotion (right). The circle represents the retrieval boundary in which videos can be retrieved by the query (text). The yellow shape indicates the targeted area in which the perturbed videos must reach for successful attacks. Video promotion is more challenging, as it seeks an intersection of the retrieval boundaries for all target queries, while video suppression only pushes videos outside the retrieval boundary. DiDeMo [14], and ActivityNet [48], their robustness to adversarial attacks remains largely unexplored. For example, as the only existing attack against T2VR, Yang et al. [45] proposed an adversarial attack against T2VR models to suppress video ranks by pushing videos away from queries and vice versa under black-box and whitebox settings. However, attacks that promote videos to top positions can be more hazardous, as they allow attackers to gain more views/clicks for financial benefits and widespread (mis)information. Additionally, such manipulation can trigger snowball effect: once the manipulated videos gain initial popularity through adversarial promotion, the platforms recommendation/retrieval algorithm may further amplify their exposure. Recognizing this overlooked threat, we investigate this vulnerability to video promotion attacks as critical step towards building robust and trustworthy T2VR systems. To this end, we propose the first attack against T2VR 1 to promote videos adversarially for the selected queries, dubbed the Video Promotion attack (ViPro). Furthermore, to reflect realistic user scenarios, we restrict the attack to candidate videos only, without modifying the text queries, as attackers cannot interfere with queries from other users. In Figure. 1, we provide hypothesized illustration to highlight the difference between existing attacks (video suppression) and ours (video promotion) intuitively. In the left figure, video suppression attacks succeed if the perturbation δ pushes the video out of the retrieval boundary of q1 (depicted as the blue circle), which is determined by the retrieval list length and the videos scattered around the query. Thus, the desired perturbation can be obtained by simply maximizing the distance between the video and the query q1, as illustrated by the red arrows (δ1, δ2, δ3). In contrast, video promotion attacks (right figure) aim to push into the small overlapping area (yellow shape) of the retrieval boundaries for all target queries (e.g., q1, q2, q3). Thus, finding the optimal ˆδ requires pre-knowledge of model embeddings and the distribution of target queries. on experiments conduct leading models 3 for MSR-VTT-1k To overcome the above challenges for better transferability, we propose Modality Refinement (MoRe) to guide perturbation towards target areas through finer-grained optimization. MoRe first conducts Temporal Clipping to group temporally similar frames into clips, and then applies Semantic Weighting to resolve potential gradient conflicts for intra-modality and inter-modality through frameto-frame and frame-to-query similarities, respectively. Consequently, MoRe provides focused guidance to push the video towards the targeted queries for better transferability. We validate the effectiveness of MoRe through ablation studies in Section.4.3. Overall, we opensourced [43], DRL [40], Cap4Video [41], and Singularity [21] (ranked #11/#17/#301). We define the white-box, grey-box, and black-box scenarios for ViPro and evaluate its effectiveness and transferability across all three models. For dataset-wise evaluations, we test on three prevailing datasets using the Singularity modelActivityNet [48], DiDeMo [14], and MSR-VTT. As for baselines, we adopt Co-Attack[49] and SGA[29] from the image-to-text domain for comparison, due to the lack of reproducible T2VR-specific attacks. Lastly, we further assess robustness under defences using JPEG compression [10] (image-based) and Temporal Shuffling [16] (video-based). user study with 17 experts on 43 groups of videos is also conducted for human imperceptibility. Across all baselines across all datasets, models, and settings, ViPro consistently outperforms other baselines. Finally, we validate our theoretical motivation through thorough experiments and provide detailed analysis in Section 4.5. 1Using the leaderboard from paperwithcode.com. 2 In sum, ViPro exhibits superior effectiveness, generalizability, and transferability over existing baselines, achieving an averaged 30/10/4% lead for white/grey/black-box settings. Our main contributions are listed as follows: We introduce realistic and impactful attacking paradigm for text-to-video retrieval (T2VR) models where attackers promote video ranks for selected queries adversarially. We thus pioneer the Video Promotion Attack (ViPro) as the first attack targeting such vulnerability. We propose Modality Refinement (MoRe) to group temporally similar frames into clips and apply semantical weighting using frame-to-frame and frame-to-query similarities for enhanced black-box transferability. We conduct thorough experiments and validate the superiority of ViPro over baselines and provide qualitative analysis of the upper and lower bounds for all attacks. 2. Related Work Vision-Language Models. Vision-Language models (VLMs), as suggested by the name, require training in both textual and visual encoders. Textual encoders have witnessed the advancement in natural language processing, which progressed from Word2Vec[34] to Bert-base [19, 28] and GPT-based[36] encoders. Besides, visual encoders[1, 6, 7, 9, 24, 30, 39] also advance significantly, surpassing CNN on numerous tasks in the image domain. In light of that, pre-trained VLMs[3, 22, 37, 38, 42, 53] have achieved promising performance on various multi-modal tasks with proper fine-tuning. Text-to-Video Retrieval. Following the paradigm of VLMs, text-to-video retrieval (T2VR) models also use various off-the-shelf encoders to extract features from videos and their captions[17, 26, 27, 32, 35]. In recent works, CLIP Radford et al. [37] proposes contrastive learning to align visual representations with textual features to achieve an impressive cross-modal retrieval performance. Following this work, many T2VR models have adopted contrastive learning. For example, CLIP-ViP [44] uses pre-trained image-text models and proposes an Omnisource Crossmodal Learning method to contrastively align videos to captions and subtitles. In sum, the architecture of all existing T2VR models can be modelled similarly as two-plus-one: two encoders to encode visual and textual inputs, plus one module responsible for cross-modal interaction. All prevailing models can be modelled into such paradigm, such as Singularity[21] DRL[40], and Cap4Video[41]. Despite the flexibility of such paradigms, the adoption of pre-trained encoders induces risks of adversarial attacks from knowledgeable attackers. Our work shows that ViPro remains effective with the absence of cross-modal modules, even for completely black-box scenarios. Adversarial attacks on Unimodal Retrieval. Despite numerous explorations on uni-modal and multi-modal retrieval [5, 25, 29, 45, 49], existing works largely focus on the untargeted paradigm, i.e., supressing the ranks of videos regarding their corresponding queries. For example, Li et al. [25] proposed novel query-based adversarial attack to completely inversed retrieval results for image retrieval. Chen et al. [5] proposed query-efficient adversarial attack that efficiently suppresses the ranks of manipulated images. Among them, very limited literature has focused on targeted attacks against retrieval models. Specifically, Zhou et al. [52] proposed an Order Attack against image retrieval that manipulates the order of the retrieved results by manipulating the query images. Adversarial Attacks on Multimodal Retrieval Research on adversarial attacks on multimodal retrieval has primarily focused on the image domain[4, 20, 46, 50, 51]. Recent studies [13, 29, 47, 49] have demonstrated transferable cross-modal adversarial attacks against pre-trained VLM on tasks including text-to-image retrieval, VQA, etc. Yang et al. [45] has proposed an adversarial attack against T2VR models such as Clip4Clip[31], Frozen[2], and BridgeFormer[12]), using the MSR-VTT[43] and DiDeMo[14] datasets. However, this work mainly focused on the untargeted setting to suppress video ranks w.r.t. their corresponding queries. Another work was conducted by Hu et al. [15]. Specifically, the author proposed Trojan-horse attack (THA) against text-to-image retrieval that was the first rank promotion adversarial targeted on multiple modalities. Specifically, THA added adversarial QR patches into images to target text queries. promote the rank of the images w.r.t. However, this work only focused on text-to-image retrieval and used an adversarial QR patch as perturbations, which are too obvious and not applicable to videos. Lastly, THA is only targeted at 1 query per image, which is much less challenging than our ViPro, which targets multiple queries simultaneously. Consequently, we raise the importance of investigating targeted attacks against T2VR and propose ViPro as realistic and impactful attacking paradigm. 3. Methodology 3.1. Threat Model As illustrated in Table.1, we first define attacking scenarios by categorizing victim models as white-box, grey-box, and black-box. Specifically, the grey-box setting accounts for the situation when the cross-modal interaction module remains unknown to the attackers. For attackers, we hypothesize that attackers prudently target only semantically relevant queries to avoid obvious misalignment with unrelated queries, e.g., food video occurs in sports-related query. Similar to image classification, ViPro can be regarded as targeted attack focusing on multiple semantically related queries, as we depicted in Table 1. Settings for attackers per-knowledge in different scenarios. Attackers Pre-knowledge Attacking Caption/Category Unimodel Encoders Cross-modal Interaction Scenarios Caption Caption Category White-box Grey-box Black-box Figure.1. For queries, we follow existing works[21, 31, 40, 41] and use the paired caption or category of the video as its query. For white-box and grey-box scenarios, attackers can access subset of queries through the subtitles/descriptions of videos. (See details in Sec.4.1 and the Supplementary.) Specifically, for black-box attacks, attackers can only access the categories (domains) of videos and evaluate the attack accordingly. Lastly, we formalize the victim model as an open T2VR platform, allowing users to query or upload their videos (e.g., YouTube). Attackers can only launch attacks by uploading manipulated videos to acquire more views/clicks for potential financial gains and widespread (mis)information. 3.2. Video Promotion Attack (ViPro) 3.2.1. Preliminaries For T2VR model, we define and as the visual and text encoders, respectively, with being the cross-modality interaction, which could be dot product, an MLP layer, or transformer, etc. Formally, for an input query q, the T2VR task is to rank all videos by their similarities w.r.t. and output video list Xq,L that contains the top relevant video candidates Xi for the query: (cid:16) (q), V(D) (cid:17) Xq,L := [X1, X2, ..., XL]q (1) The performance of model can be evaluated based on the number of corresponding videos within the retrieval list Xq,L. Retrieval Boundary. We now formally define the mathematical definition of retrieval boundaries. Based on our hypothesis in Fig.1, for perturbed video to appear in the retrieval list Xq,L, the video must cross the retrieval boundary of the chosen query. Such boundary is determined by the length of the list and the spatial distribution of videos in the proximity. We use = 1 to consider top-1 videos only for simplification. Denoting the retrieval boundary of query as hypersphere Φq of radius r, is thus the distance towards its closest video around, denoted Xq. Once the manipulated video enters Φq, the video becomes closer than Xq, replacing it as the new top-1 video. Thus, the radius of Φq, r, is inversely proportional to the similarity of 3 Figure 2. An illustration of our ViPro with MoRe. (1) Temporal Clipping: Video frames are clustered into video clips = [C1, ..., CM ] based on frame-to-frame similarity WX. (2) Semantical Weighting: For each clip and each query, we calculate its frame-to-frame similarity WCi and frame-to-query similarity WCi,q using cosine similarity between all frames xj Ci and all query tokens ti q. Frames and queries with low similarity are suppressed by their corresponding weights during optimization. 3) Clip-wise Optimization: Perturbation are optimized as per clip before outputting the final δCi for adversarial video X. the closest video nearby. In other words, larger top-1 similarity means smaller r, making it harder for attacks to cross Φq, or vice versa. 3.2.2. Attack Objectives Our goal is to push the candidate video such that appears at the top-1 position for all selected queries: Xq,L, (2) where := + δ, with δ being the adversarial perturbations. Similar to other adversarial attacks, ViPro also requires visual constraints to keep the attack imperceptible to human eyes, i.e., δp ϵ, where ϵ is the threshold for the lp norm. Note that only visual modality is attacked. 3.2.3. Loss Function Due to the discreteness of Xq,L, Eq.2 is not directly differentiable. We thus apply our retrieval boundary hypothesis above, which translates our objective into pushing towards the overlap of the retrieval boundaries for all queries in Q, i.e., Φq1 Φq2... ΦqN . The optimal objective to achieve this is to increase the similarity between the candidate video to exceed the similarity for all top-1 videos Xq XQ. However, this optimal objective requires access to XQ, which is unrealistic. An approximation is to increase the similarity for all queries. Formally, denoting the vision/text feature as FX and Fq, we define their crossmodality similarity is defined below: = (cid:88) Sim(cid:0)Fq, FX (cid:1) (3) where Sim() refers to the cross-modal interaction to calculate the similarities between videos and queries. For whitebox, Sim() will be the cross-modal module of victim models, while for grey-box attacks, it will be the cosine similarity. FX, Fq refer to video and queries features. naive solution is to use the negative similarity scores as the loss function directly: Lneg(S) = (4) However, Lneg will lead to suboptimal results, especially for multi-target optimization, because it provides identical gradients (i.e., 1) for all targets. Thus, we propose to use exponential loss for optimal results: Lexp(S) = exp(S) (5) Lexp could provide adaptive gradients for different targets, i.e., larger gradients for lower (farther targets) and lower gradients for larger (nearer targets). Experimental results also validate the effectiveness of Lexp. 3.3. Modality Refinement (MoRe) An overview of MoRe is presented in Figure. 2. For chosen video = [x1, x2, ..., xT ] with frames and the target queries = [q1, q2, ..., qN ], MoRe handles the finer-grained interaction between intra-modality and intermodality in temporal (intra) and semantical (inter) respective. Specifically, we first conduct temporal clipping to group temporally related frames, and then perform semantical weighting to guide optimization using frame-totoken similarity for each query. In this way, MoRe will aid the optimization by guiding it towards target queries for enhanced transferability. Finally, each clip will be optimized separately and concatenated as the manipulated video X. Algorithm 1 Pseudo-code for ViPro. MoRe will be enabled for generating black-box attacks and disabled otherwise. Require: Video X, target queries Q, visual encoder V, textual encoder , maximum PGD steps K, PGD step size α, perturbation bound ϵ Ensure: Adversarial Video Clip video when MoRe Enabled = [C1, C2, ..., CM ] TEMPCLIP(X) for 1 to do ATTACK(Cm, Q) Clip-wise Attack 1: if MoRe then 2: 3: 4: 5: end for CONCAT(C ATTACK(X, Q) 6: 7: else 8: 9: end if 10: function ATTACK(X, Q) 11: X: input video FX V(X), FQ (Q) Get features if MoRe then Get weights when MoRe enabled WX COSSIM(FX,FX) Frame-to-frame WX,Q COSSIM(FX, FQ) Frame-to-query 1, 2, ..., M ) Concat clips 0 end if Initialize for 1 to do X 0 X, δX k1 + δX k) V(X FX Sim(cid:0)FQ, FX if MoRe then Lexp WX (cid:16) k1 (cid:1) Get sim using Eq.3 Calculate Lmore using Eq.8 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)WX,Q (cid:12) (cid:17) Lexp(S) Calculate Lexp in Eq.5 PROJ(αL, ϵ) PGD 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: else 24: end if 25: δX 26: end for 27: return 28: 29: end function Temporal Clipping. To accommodate temporal abruptness among video frames (intra-modality), perturbation using averaged gradients from all frames could hinder the convergence of losses and lead to sub-par transferability. An intuitive solution is to perform per-frame optimization, but this would significantly increase computational complexity and break temporal information. Thus, we propose Temporal Clipping to identify temporally outlying frames and group frames into clips. Specifically, for each input video X, we first calculate its frame-wise cosine similarity WX RT as follows: (cid:88) WX = CosSim(xi, xi), xi (6) We then calculate the similarity difference WX as the temporal shifts between frames, where larger value indi5 Figure 3. An illustration of the effectiveness of MoRe in guiding optimization. Due to the conflicting gradients, δ in the left figure leads to suboptimal results without entering any boundary. After applying MoRe, the anomalous query (q3) is suppressed, yielding more focused perturbation δ to push videos towards the target area (yellow shape). cates an outlying frame with palpable temporal abruptness. Subsequently, we loop through all frames and clip them when exceeding the threshold γ. (See detailed pseudo-code in the Supplementary.) Temporal Clipping will output the clipped videos = [C1, ..., CM ] for subsequent clip-toquery alignment. Semantical Weighting. the cross-modality alignment between clips and queries may vary significantly because of the semantic difference. Pushing videos towards them simultaneously may also lead to looser convergence, i.e., failing to enter the retrieval boundary due to conflicting gradient, as shown in the left graph of Figure.3. To avoid this, given query and video clip C, we calculate the frame-to-query weights WC,q using the mean of frame-to-token cosine similarity: Similarly, WC,q = (cid:80)N CosSim(xi, tj) , xi (7) WC,q is subsequently used for weighting the corresponding frames and query for optimal perturbations. In essence, semantic weighting suppresses anomalous queries during optimization to enable tighter convergence towards other queries, as shown in the right graph of Figure.3. The clipwise temporal similarity WC is similarly used as intramodality semantical weighting to guide frame-level optimization, resolving potential conflicts among gradients from different frames. Clip-wise Perturbation. Lastly, we optimize the video in clip-wise manner and present the overall learning objectives of ViPro with MoRe as follows: Lmore = Lexp (cid:16) WC (cid:12) (cid:12) (cid:17) (cid:12) (cid:12) (cid:12)WCi,q (cid:12) , Ci (8) where WCi, WCiq, are all matrices with dimensions of 1 TCi, TCi and TCi . TCi is the number of frames within the clip Ci. In summary, we incorporate MoRe for finer-grained alignment of intra-/inter-modalities. Temporal Clipping helps identify and clip temporally outlying frames, while query weighting enables guided optimization towards target queries by suppressing conflicting gradients. The two modules work cooperatively to push videos further towards target queries for enhanced transferability, which is comprehensively validated through experiments in Section.4.3. Overall, ViPro is used for white-box and grey-box attacks, while MoRe is enabled to generate transferable black-box attacks. We provide pseudo-code for ViPro and MoRe in Algorithm.1. 4. Experiments 4.1. Experimental Settings Datasets. We test our attacks on the three prevailing datasets: MSR-VTT-1K (MSR-VTT)[43] contains 1K testset of YouTube videos, DiDeMo[14] contains testset of 1065 Flickr videos, and AtivityNet [48] contains 4.9K testset sampled from YouTube. To ensure equal testset sizes, we randomly sample 1K subset for all datasets. We use MSR-VTT for model-wise evaluations. User Queries. As mentioned in Table.1, attackers could access captions for white-/grey-box settings and categories for the black-box setting. The acquisition of user queries is identical for all scenarios: assuming that all videos come with captions (categories) on T2VR platforms such as YouTube, attackers first use the captions of their own video to query the victim model for the top-K retrieved videos and the corresponding captions (categories). These captions (categories) are regarded as the relevant queries of this video. In our experiments, we use = 20 queries and randomly divide them evenly into train/test sets for the harshest scenario where users use different similar but different queries than the ones used for training attacks. All captions/categories used in our experiments are original from the datasets. See details of data construction in the Supplementary. Victim Models. We choose open-sourced leading T2VR models for MSR-VTT on paperwithcode, including Singularity-17M (Sing)[21], DRL-32B (DRL) [40], and Cap4Video-32B (C4V)[41]. DRL and C4V are manually trained on MSR-VTT using the released code. All models take an input size of 3 224 224. As for input frames, Sing uses 4 frames per video, while DRL and C4V take 12 frames. For unimodal encoders, Sing uses BEiT [3] pre-trained on ImgageNet-21K [8] for the vision encoder and the first 9 layers of BERT[19] model for the text encoder. DRL and C4V use Bert-based textual encoder and ViT as its visual encoder. For cross-modal interaction, Sing uses crossencoder, and DRL uses weight-based token interaction for cross-modality interaction. C4V uses similar token interaction as DRL and introduces sequence transformer as the video aggregator. Baselines Due to the lack of directly comparable baselines, we use Co-Attack[49] and SGA[29], which are targeted at text-to-image retrieval (T2IR), as two baselines for comparison. (Attacks introduced by Yang et al. [45] are not included due to the lack of reproducible code.) We then apply the necessary and minimal adaptation to ensure that the two baselines comply with our settings. For both baselines, textual attacks are not implemented since attackers can only manipulate their updated videos and cannot interfere with user queries. And we reverse the original attacking objectives from suppressing ranks to promoting ranks. For SGA and ViPro, we use the white-box text-to-video similarities and cosine similarity for white-box and grey-box attacks, respectively. For whitebox Co-Attack, we apply its strongest multimodal embedding setting, i.e., minimizing the distance between the perturbed videos and the multimodal embedding of the videotext pairs, denoted as EMBMul. For C4V/DRL, which do not feature multimodality embeddings, we modify the objectives to maximize the white-box text-to-video similarity, denoted as SIMT2V. For grey-box Co-attack, we adopt the same cosine similarity for all models. Evaluation Metrics. We use the difference between vanilla and attacked R@1/5 as evaluation metrics for attacking performance, denoted as R@1/5. Results are presented in the average over the 1K testset for all models/datasets. Higher values are preferable as they indicate efficient attacks. Hyperparameters. For all attacks, we adopt PGD [33] using linf norm with ϵ = 16 255 , η = 27, and the step size α = 1/255. Ablation studies for hyperparameters are given in Section.4.3. The random seed is fixed to 42. For the temporal clipping threshold γ, we empirically use 1 to indicate outlying frames, i.e., frame vectors deviating from the previous one by θ π/6. 3 2 4.2. Main Results We first provide white-box results to demonstrate the effectiveness of ViPro as an approximate upper bound. Afterwards, we present grey-box results to show the generalizability of ViPro under restricted knowledge. Finally, we 6 2. Table (ActivityNet/DiDeMo/MSR-VTT). The model ity. Changes in R@K are presented in the parentheses. White-box results on datasets all is Singular4. datasets Table (ActivityNet/DiDeMo/MSR-VTT). The model is Sing. Changes in R@K are presented in the parentheses. Grey-box results on all Dataset Method - R@1(%) R@5(%) Average(%) Dataset Method R@1(%) R@5(%) Average(%) 4.78 24.44 14.61 No Attack 4. 24.44 14.61 ActivityNet[48] Co-Attack[49] (EMBMul) SGA[29] ViPro 3.96 (-0.82) 7.99 (+3.20) 46.66 (+41.88) 26.34 (+1.90) 38.44 (+14.00) 76.25 (+51.81) 15.15 (+0.54) 23.22 (+8.61) 61.46 (+46.85) ViPro w/o ϵ 76.22 (+71.44) 90.77 (+66.33) 83.50 (+ 68.89) - 4.90 24.71 14.78 DiDeMo[14] Co-Attack[49] (EMBMul) SGA[29] ViPro 5.42 (+0.52) 9.58 (+4.68) 40.54 (+35.64) 28.09 (+3.38) 35.08 (+10.37) 69.56 (+44.85) 16.76 (+1.95) 22.33 (+7.53) 55.03 (+40.25) ActivityNet[48] Co-Attack[49] SGA[29] ViPro 4.66 (-0.12) 0.81 (-3.97) 16.19 (+11.41) 25.52 (+1.90) 8.24 (-16.20) 57.84 (+33.40) 15.09 (+0.48) 4.53 (-10.09) 37.02 (+22.41) No Attack 4.90 24.71 14.78 DiDeMo[14] Co-Attack[49] SGA[29] ViPro 4.48 (-0.42) 1.23 (-3.67) 8.92 (+4.02) 22.49 (-2.22) 7.54 (-17.17) 38.15 (+13.44) 13.49 (-1.32) 4.39 (-10.42) 23.51 (+8.73) ViPro w/o ϵ 66.63 (+61.73) 84.90 (+60.19) 75.74 (+60.96) No Attack 4.87 22.94 13. ViPro w/o ϵ No Attack 66.85 (+61.98) 82.53 (+59.59) 74.69 (+60.79) DRL[40] 4.29 15.76 10.03 - 4.87 22. 13.91 MSR-VTT[43] Co-Attack[49] (EMBMul) SGA[29] ViPro 9.51 (+4.64) 19.30 (+14.43) 47.81 (+42.94) 25.12 (+2.18) 46.77 (+23.83) 73.66 (+50.72) 17.32 (+3.41) 33.04 (+19.13) 60.74 (+46.83) ViPro w/o ϵ 66.85 (+61.98) 82.53 (+59.59) 74.69 (+60.79) Table 3. White-box results on all models (Sing/DRL/C4V) using MSR-VTT. Changes in R@K are presented in the parentheses. Models Sing[21] Method No Attack R@1(%) R@5(%) Average(%) 4.87 22.94 13.91 Co-Attack[49] EMBMul) SGA[29] ViPro 9.51 (+4.64) 19.30 (+14.43) 47.81 (+42.94) 25.12 (+2.18) 46.77 (+23.83) 73.66 (+50.72) 17.32 (+3.41) 33.04 (+19.13) 60.74 (+46.83) DRL[40] Co-Attack[49] (SIMT2V) SGA[29] ViPro 61.07 (+56.78) 53.53 (+49.24) 61.24 (+56.95) 83.89 (+68.13) 80.37 (+64.61) 84.62 (+68.86) 72.48 (+62.46) 66.95 (+56.93) 72.93 (+62.91) ViPro w/o ϵ No Attack 87.37 (+83.08) 97.09 (+81.33) 92.23 (+82.21) 3. 13.42 8.59 C4V[41] Co-Attack[49] (SIMT2V) SGA[29] ViPro 75.04 (+71.28) 72.13 (+68.37) 75.20 (+71.44) 88.57 (+75.15) 86.99 (+73.57) 88.91 (+75.49) 81.81 (+73.22) 79.56 (+70.97) 82.06(+73.47) ViPro w/o ϵ 93.61 (+89.85) 98.56 (+85.14) 96.09 (+87.50) conduct cross-model attacks for the transferability of all attacks. White-Box. We present white-box results on all datasets in Table.2 and all models in Table.3 to demonstrate the effectiveness of ViPro. Overall, all white-box attacks significantly promote video ranks to much higher R@1/5 for all datasets/models, showing the impact of our proposed attacking paradigm across datasets and models. Among them, ViPro achieves the best results over Co-Attack and SGA for all datasets and all models. For dataset-wise comparison, ViPro has consistent lead for 30% on R@1 over all datasets, especially on ActivityNet, which surpasses CoAttack and SGA by 43/38%, respectively. ViPro also achieves SOTA performance for model-wise comparison. For Co-Attack, it constantly performs the worst on all datasets on the Sing model because it applies the KL divergence to optimize using the implicit multimodal embedding to promote videos, which is inefficient in guiding videos towards queries. Once adapted to white-box similarity on DRL and C4V, Co-Attacks performance benefits signifiMSR-VTT[43] Co-Attack[49] SGA[29] ViPro 4.66 (-0.21) 0.98 (-3.98) 8.71 (+3.84) 20.86 (-2.08) 4.05 (-18.89) 28.9 (+5.85) 12.76 (-1.15) 2.52 (-11.39) 18.76 (+4.85) Table 5. Grey-box results on all models (Sing/DRL/C4V) using MSR-VTT. Changes in R@K are presented in the parentheses. Models Method R@1(%) R@5(%) Average(%) No Attack 4.87 22.94 13.91 Sing[21] Co-Attack[49] SGA[29] ViPro 4.66 (-0.21) 0.98 (-3.98) 8.02 (+3.15) 20.86 (-2.08) 4.05 (-18.89) 28.31 (+5.37) 12.76 (-1.15) 2.52 (-11.39) 18.17 (+4.26) No Attack 4.29 15. 10.03 Co-Attack[49] SGA[29] ViPro 38.61 (+34.328) 39.41 (+35.12) 42.35 (+38.06) 76.09 (+60.33) 76.46 (+60.70) 78.39 (+62.63) 57.35 (+47.33) 57.97 (+47.91) 60.37 (+50.35) No Attack 3.76 13.42 8.59 C4V[41] Co-Attack[49] SGA[29] ViPro 0.93 (-2.83) 1.28 (-2.48) 1.99 (-1.77) 5.07 (-8.35) 7.69 (-5.73) 10.53 (-2.90) 3.00 (-5.59) 4.49 (-4.11) 6.26 (-2.34) cantly. As for SGA, although it directly utilizes the whitebox similarity, it adopts the Lneg defined in Eq.4 plus an aggressive augmentation that includes 4 augmented images. The former leads to suboptimal results since Lneg treats all targets identically, while the latter harms the optimization by augmenting semantically irrelevant videos into training. Specifically, such augmentation could be more damaging on sensitive models such as Sing, which is expected to have smaller retrieval boundaries, making it easier for augmented videos to corrupt the semantic correlations between videos and queries. In contrast, DRL and C4V are expected to be less sensitive as it has larger retrieval boundaries since SGA shows comparable performance on these models. Lastly, we present results without perturbation bounds ϵ to demonstrate the experimental upper bound for ViPro attacks to validate our hypothesis on retrieval boundaries. Results on all datasets/models w/o ϵ show that, while gaining certain enhancements, ViPro cannot hit 100% in R@1 and R@5, implying that the distribution of datasets and model embeddings determines the upper bound of ViPro attacks. We will further elaborate on this with an in-depth analysis in Section.4.5. Grey-Box. We then present the results of all attacks un7 Table 6. Black-Box results of all attacks on MSR-VTT on all models. Results are evaluated using categories. Source Model Method Sing DRL C4V R@1(%) R@5(%) R@1(%) R@5(%) R@1(%) R@5(%) Average(%) - No Attack 5.21 9.79 1. 6.32 0.79 3.00 - Sing[21] DRL[40] C4V[41] Co-Attack [49] SGA [29] ViPro + MoRe Co-Attack [49] SGA [29] ViPro + MoRe Co-Attack [49] SGA [29] ViPro + MoRe 0.63 (-0.79) 0.70 (-0.72) 1.58 (+0.16) 5.53 (-0.79) 6.00 (-0.32) 8.53 (+2.21) 0.16 (-0.63) 0.32 (-0.47) 0.32 (-0.47) 1.74 (-1.26) 1.74 (-1.26) 3.32 (+0.32) 7.74 (+2.53) 7.58 (+2.37) 7.74 (+2.53) 5.69 (+0.48) 6.95 (+1.74) 7.63 (+2.42) 15.01 (+5.22) 15.32 (+5.53) 16.11 (+6.32) 13.90 (+4.11) 13.74 (+3.95) 13.99 (+4.20) 17.69 (+16.90) 22.59 (+21.80) 26.05 (+25.26) 33.33 (+30.33) 40.13 (+37.13) 45.31 (+42.31) 5.21 (+3.79) 7.11 (+5.69) 26.05 (+24.63) 20.54 (+14.22) 23.70 (+17.38) 46.31 (+39.99) 2.02 (-0.87) 2.19 (-0.69) 3.44 (+0.56) 18.44 (+13.75) 21.41 (+16.71) 23.80 (+19.11) 11.34 (+5.65) 12.88 (+7.19) 23.50 +(17.81) der the grey-box setting for all datasets/models. In Table.4 and Table.5, SGA and Co-attack fail to promote the R@1 of perturbed videos for most cases, incuring negative promotion after attacks. In contrast, our ViPro manages to generalize well and retain its advantages over other baselines for all datasets. For example, on DiDeMo and MSR-VTT, our ViPro is the only method that successfully promotes manipulated video, while other baselines are shown to be inefficient for grey-box attacks. For model-wise comparison, ViPro also achieves the SOTA results for all models. Specifically, SGA experienced large performance drops on all datasets, echoing our conclusion on the damage brought by augmentation on sensitive models. These leads further validate the effectiveness of our proposed ViPro over SGA and Co-Attack for promoting videos. Compared to white-box settings, all grey-box attacks experience performance downgrade. Specifically, we observe inconsistency in performance drops compared to their white-box counterparts. For example, attacks on DRL experienced the minimum degradations by only 15/9/12% for Co-Attack/SGA/ViPro, respectively. On the other hand, attacks on C4V decrease significantly by over 60% on average for all attacks, which is caused by the introduction of an extra video aggregator. We provide an ablation to validate this in the Supplementary, where all attacks show significant performance boosts with access to the aggregator. We will discuss these findings in Section. 4.5. Black-Box. We finally evaluate cross-model transferability in the black-box settings following the definition in Table.1, where attackers can only access the categorical information from the video. Thus, black-box attackers could only target the annotated category to promote its rank under this category. We use the categories provided in the original paper in [43]. Specifically, we use the 10 out of 20 categories with the best zero-shot R-Precision on all models. Details on evaluations for all models and categories are provided in the Supplementary. Results are given in Table.6. Overall, ViPro retains its lead over all baselines for all black-box models, leading Co-Attack and SGA by 6% and 4% on average. Specifically, when using C4V as the source model, ViPro significantly outperforms Co-Attack and SGA by 21/26% and 19/23% for R@1/5 when attacking DRL models. ViPro also surpasses the other two baselines by 5% and 2% on DRL-generated attacks, yielding maximum 25.26% promotion on R@1 and 42.31% on R@5 when attacking C4V. As for the attacks optimized using Sing, our ViPro is the only method that achieves improved overall average R@1/5 among all methods, yielding 0.5% promotion over CoAttack (-0.87%) and SGA (-0.69%). The results validate the advantages of ViPro + MoRe for transferability. Besides, comparing attacks generated by different models, we find that attacks from Sing exhibit the least transferability than those generated by DRL and C4V, while DRL and C4V show better transferability to the others. This observation aligns with the consensus that models exhibit better/worse transferability on similar/dissimilar models: C4V and DRL models adopt the same ViT encoders, while Sing uses BEiT. 4.3. Ablation Study In this section, we first demonstrate the effectiveness of MoRe in improving transferability, with specific comparison between temporal clipping and random clipping. Afterwards, we conduct qualitative evaluation on the impact of the number of training targets on optimizing ViPro, yielding counter-intuitive yet inspiring results. We finally provide an ablation study on the hyperparameters used in PGD. Modality Refinement. To validate the viability of Temporal Clipping, we provide results without MoRe and MoRe with random clipping (denoted as MoRe R) for comparison. Results are presented as the average results of each source model for clarity, as shown in Table.7. First of all, we observe sub-par or the worst performance for the random clipping, i.e., MoRe R. For example, on the Sing model, random clipping leads to downgraded performance 8 Table 7. Ablation studies on the effectiveness of MoRe to boost transferability. denotes random clipping. Results are given as the average of each source model. MoRe achieves the best overall black-box performances for ALL models. Source Model Method R@1(%) R@5(%) Average (%) Sing[21] DRL[40] C4V[41] ViPro ViPro + MoRe ViPro + MoRe 0.87(-0.24) 0.79 (-0.32) 0.95 (-0.16) ViPro 14.06 (+11.06) ViPro + MoRe 15.49 (+12.49) 16.90 (+13.90) ViPro + MoRe ViPro 15.33 (+12.01) ViPro + MoRe 12.00 (+8.69) 16.84 (+13.53) ViPro + MoRe 5.92 (+1.26) 5.37 (+0.71) 5.93 (+1.27) 27.25 (+20.86) 27.57 (+21.17) 30.71(+24.32) 25.68 (+17.62) 26.74 (+18.60) 30.15 (+22.10) 3.39 (+0.51) 3.08 (+0.20) 3.44 (+0.56) 20.66 (+15.96) 21.53 (+16.83) 23.80 (+19.11) 20.50 (+14.82) 19.37 (+13.69) 23.50 (+17.81) for both R@1 and R@5, yielding the worst results for the model. Moreover, the R@1 also drops significantly by over 3% on attacks generated by C4V, implying that randomly clipping the video would potentially incur worse attacking performance. On the other hand, our ViPro + MoRe consistently achieves the best performance over its counterparts with random clipping and ViPro itself across models. For example, our MoRe brings 2.9/3% boost over ViPro on DRL regarding R@1/5, and 1.5/5% boost on C4V regarding R@1/5. For the most challenging source model, our ViPro + MoRe also boosts the R@1/5 considerably. This validates the effectiveness of our MoRe to tighten the convergence of losses during the optimization of perturbations by clustering temporally similar frames and guiding optimization with query weighting. Number of Training Queries. Intuitively, the number of training queries for optimizing ViPro could significantly influence attack performance. Intuition may be that ViPro would benefit from larger numbers of training queries as it acquires more knowledge about the victim datasets. To this end, we trial different numbers of training queries on all datasets and models, ranging from 10 (experiment setting) to 500 (half of the testset). As shown in Figure.4, we find counter-intuitive results as the number of training queries increase for both white- &grey-box attacks: for dataset-wise comparison (top row), the attacking performance suffers from constant drop, yielding an over 40% degradation as the number reaches 500; for model-wise comparison (bottom row), the results on DRL and C4V exhibits rise-to-pleatu trend, with minor drop in the late stage. Specifically, for DRL and C4V, using only 10 training queries can effciently achieve more than 85% of the max performance when using more queries, where as for Sing, using 10 already achieves the best results over others. Overall, we can draw the following conclusion: Using more queries does not necessarily boost attacking performance, while small number of queries is sufficient to generate effective attacks for both white-box and grey-box ViPro. Hyperparameters. Ablations on PGD hyperparameters on Figure 4. The impact of varying numbers of training queries for white-box (solid lines) and grey-box (dashed lines) ViPro on all datasets (top row) and all models (bottom row). We present R@1 (red) and R@5 (orange). all datasets are presented in Figure.5, showing the results for max steps η, step size α, and perturbation bound ϵ. We use white-box results on the Sing model for all datasets and present R@1 changes to demonstrate attacking performance. For the max PGD step size η, we observe monotonic increase as it increases from 24 to 29 for all datasets. Considering the computational cost of perturbing videos, we choose 27 for balanced results between effectiveness and efficiency. Although further increasing η would boost attacking performance, such boosts come at the cost of doubled or more training time. For the step size α, we test values ranging from 1 to 4. Consistent with our previous speculation, ViPro suffers from steady performance drop as the step sizes increase. This echoes with our illustration in Fig.1 that video promotion is an intricate attack that requires sophisticated optimization for optimal results. Finally, similar trend as η is found for the perturbation budget ϵ. Thus, we empirically choose ϵ = 16 for better effectiveness while maintaining imperceptibility. We also use user study to examine the stealthiness of our ViPro over Co-Attack and SGA under the same ϵ in Sec.4.4. 4.4. Evaluation under Realistic Scenarios In reality, attacks against T2VR may undergo defenses such as JPEG compression during upload. Besides, it is also vital that manipulated videos do not contain artifacts or visual clues that are obvious to human users. Consequently, to comprehensively evaluate the performance of all attacks under realistic scenarios, we perform evaluations for all attacks regarding both robustness against defences and human evaluations. Robustness against Defences. To verify the robustness of all attacks under defence, we adopt JPEG Compression (JC) 9 Table 8. White-box defense evaluations using Temporal Shuffling and JPEG Compression on Sing, DRL, and C4V. Changes after attacks are presented in parentheses. Our ViPro retains its superiority under both defences. Defense Attack Sing DRL C4V R@1(%) R@5(%) R@1(%) R@5(%) R@1(%) R@5(%) Average(%) - No Attack 4.87 22.94 4.29 15.76 3. 13.73 - Temporal Shuffling[16] JPEG Compressio[10] (q = 75) Co-Attack [49] SGA [29] ViPro Co-Attack [49] SGA [29] ViPro 8.33 (+3.46) 17.11 (+12.24) 38.39 (+33.52) 8.49 (+3.62) 19.21 (+14.34) 34.85 (+29.98) 21.48(-1.46) 41.83 (+18.89) 64.67 (+41.73) 23.18(+0.24) 46.59 (+23.65) 63.67 (+40.73) 38.82 (+34.53) 36.05 (+31.76) 39.32 (+35.03) 53.46 (+49.17) 52.93 (+48.64) 53.57 (+49.28) 62.95 (+47.19) 60.42 (+44.66) 63.95 (+48.19) 77.55 (+61.79) 77.80 (+62.04) 78.00 (+62.24) 55.74 (+51.95) 53.20 (+49.41) 55.51 (+51.72) 67.55 (+63.76) 67.82 (+64.03) 67.76 (+66.97) 73.58 (+59.85) 71.24 (+57.51) 73.28 (+59.55) 84.24 (+70.51) 84.20 (+70.47) 84.26 (+70.53) 43.48 (+32.59) 46.64 (+35.75) 55.85 (+44.96) 52.41 (+41.52) 58.09 (+47.20) 63.69 (+52.79) Table 9. Grey-Box defense evaluations using Temporal Shuffling and JPEG Compression on Sing, DRL, and C4V. Changes after attacks are presented in parentheses. Our ViPro retains its superiority under both defences. Defense Attack Sing DRL C4V R@1(%) R@5(%) R@1(%) R@5(%) R@1(%) R@5(%) Average(%) - No Attack 4. 22.94 4.29 15.76 3.79 13.73 - Temporal Shuffling[16] JPEG Compression[10] (q = 75) Co-Attack [49] SGA [29] ViPro Co-Attack [49] SGA [29] ViPro 4.65 (-0.22) 0.89 (-3.98) 8.44 (+3.57) 5.06 (+0.19) 1.14 (-3.73) 7.80 (+2.93) 19.88 (-3.06) 3.93 (-19.01) 27.44 (+4.50) 20.53(-2.41) 4.52 (-18.42) 28.26 (+5.32) 21.08 (+16.79) 24.56 (+20.27) 37.84 (+33.55) 30.94 (+26.65) 36.51 (+32.22) 46.26 (+41.97) 53.31 (+37.55) 60.58 (+44.82) 72.09 (+56.33) 67.22 (+51.46) 73.45 (+57.69) 79.79 (+64.03) 0.47 (-3.32) 1.19 (-2.60) 1.83 (-1.96) 0.22 (-3.57) 1.27 (-2.52) 1.57 (-2.22) 3.73 (-10.00) 7.32 (-6.41) 10.15 (-3.58) 1.92 (-11.81) 6.91 (-6.82) 8.37 (-5.36) 17.19 (+6.29) 16.41 (+5.52) 26.30 (+15.40) 20.98 (+10.09) 20.63 (+9.74) 28.68 (+17.78) Figure 5. Ablation studies on PGD hyperparameters, showing results for max PGD steps η, step size α, and perturbation bound ϵ, respectively. Results are presented in R@1 changes after attacks, denoted as R@1. R@5 results are not presented for better visibility. [10] and Temporal Shuffling (TS) [16], two well-recognized image-based and video-based defences, to evaluate all included attacks on all models for white-box and grey-box scenarios. Following the settings in the original papers, we use JPEG with = 75, and for TS, we use h1 = 2, h2 = 1 for Sing (uses 4 frames), and h1 = 4, h2 = 2 for DRL and C4V (use 12 frames). Results are presented in Table.8 and Table.9. Our ViPro shows the best overall robustness against both image-based and video-based defences, leading Co-Attack by 10/9% and SGA by 7/9% for white-/greybox settings, respectively. Imperceptibility for Human. We finally provide evaluations on the visual imperceptibility of all attacks. We first present visualization of all attacks in Fig. 6, from which we find SGA as the most suspicious compared to Co-Attack Figure 6. Visualization of original frames and manipulated frames for all attacks (white-box). From top to bottom: Clean, SGA, Co-Attack, ViPro (ours). SGA has the most obvious patterns and artifacts. Co-Attack has on-par stealthiness as our ViPro with minor visual clues, while our ViPro remains stealthy with the best performance. and ViPro. We attribute this to its more aggressive data augmentation strategy that leads to diverse but perceptible 10 Figure 7. Visualization of white-box top 1 text-to-video similarity scores for all datasets. All results are normalized between 0 to 1 for better visibility. Figure 8. Visualization of white-box top 1 text-to-video similarity scores for all models. Because the embeddings vary significantly across models, we only present normalized top 1 scores for better visibility. perturbations. We further conducted user study with 43 shuffled groups of videos for each attack. Based on results from 17 human experts (i.e., equipped with background knowledge about adversarial attacks), ViPro is chosen as the stealthiest with 42.69% of cases, beating both Co-Attack 39.47% and SGA 17.84%, proving ViPros stealthiness. 4.5. Discussion In this section, we will elaborate on the mechanism behind the ViPro attack, including qualitative analysis on the determinants for upper bound and the lower bound of video promotion attacks. We then investigate the potential defensive practices against ViPro attacks. Finally, we discuss the limitations and future works of our paper. Upper Bound. The upper bound refers to the theoretically achievable maxima on victim model for all attacks, which is jointly determined by the distribution of model embeddings and the distribution of the dataset. The results w/o ϵ across datasets and models in Table.2 and Table.3 can also support the previous conclusion. For example, ActivityNet has the highest R@1 and R@5 than MSR-VTT and DiDeMo, while the results on Sing show the lowest R@1/5 over DRL and C4V. Following our previous theory on retrieval region, ActivityNet is supposed to have the largest retrieval region for all datasets, while Sing has the tightest (smallest) region for all models. This translates to the smallest average text-video similarities for ActivityNet and the largest ones for Sing. To validate this, we plot histograms of text-video similarities for all datasets and models. As shown in Figure.7 and Figure.8, the distribution and mean of the similarities for all datasets and models align with our previous speculation. The results also echo our previous speculation on model sensitivity: With the largest average top-1 similarity, Sing is the most sensitive model to variation in the embedding, making SGA the least effective attack due to augmentation, and vice versa for C4V and DRL. This verifies our theory on the two determinants: the model distribution of model embeddings and the dataset. Lower Bound. The lower bound refers to the generalization ability of attacks to maintain their effectiveness across settings. We hypothesize the algorithms of attacks and the architecture of victim models as the two determining factors. For algothrims, we show in the main results and ablation studies that ViPro consistently outperforms its counterparts using KL divergence (Co-Attack) and naive negative loss (SGA). Besides, the incorporation of MoRe also implies that the explicit design in refining temporal and semantical interaction further improves the ability to generalize across models. As for model architecture, we find that models with structural cross-modal interaction exhibit stronger resistance to non white-box attacks. For example, as shown in Table.5, DRL experiences the least performance drop compared to white-box results, given that it is the only model using algorithmic cross-modal interaction. C4V, in contrast, shows the strongest resistance because of its video aggregator within the cross-modal module. Furthermore, according to our ablation study in the Supplementary, the inclusion of access to the video aggregator would significantly boost all attacks compared to grey-box settings. These findings shed light on possible ways in building robut T2VR models, especially when using open-sourced pre-trained encoders. Potential Defense against ViPro. As presented in Table.8 and Table.9, ViPro remains effective under popular image and video defences. To better protect T2VR models from such attacks, popular methods such as adversarial fine-tuning and purification could also be an efficient protection. However, the former may cause large performance degradation, while the latter requires computational optimization for faster video purification. Thus, we suggest using more information-rich modalities, such as audio, for multimodal fusion. From our observation for the attack lower bound, the inclusion of encoders for more modalities would increase the robustness of models and weaken the dependence of models on visual information. Limitations & Future Work. Due to the computational overhead, we cannot test our attacks on larger VLMs but use deployable open-sourced models for evaluations. Thus, we cannot launch effective attacks on larger commercial T2VR platforms such as YouTube. Besides, the transferability can be further boosted using more accurate temporal clipping, i.e., model-aided clustering, and/or model ensemble. Lastly, temporal constraints can also be applied for better stealthiness of manipulated videos to avoid visual clues and artifacts. These directions are valuable for future exploration. 5. Conclusion We explore the overlooked vulnerability against textto-video retrieval (T2VR) and propose new attacking paradigm to promote video ranks adversarially. Accordingly, we pioneer the Video Promotion attack (ViPro) as the first attack targeting such vulnerability. We further propose Modality Refinement (MoRe) to capture the intricate modality interaction to enhance black-box transferability. Comprehensive experiments include 2 existing baselines, 3 leading T2VR models, 3 prevailing datasets with over 10k videos, evaluated under 3 scenarios. All experiments are conducted in multi-target setting to reflect realistic scenarios where attackers seek to promote the video regarding multiple queries simultaneously. We also provide evaluations for defences and imperceptibility. In sum, ViPro consistently shows the best performance on all settings over exisiting baselines, implying the superiority of our method. Our work provides qualitative analysis on the upper and lower bounds of ViPro, highlights an overlooked vulnerability, and offers insights into potential counterplays."
        },
        {
            "title": "References",
            "content": "[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 60776086, 2018. 2 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval, 2022. 3 [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. 2, 6 [4] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Show-and-fool: Crafting adversarial examples for neural image captioning. arXiv preprint arXiv:1712.02051, 2, 2017. 3 [5] Mingyang Chen, Junda Lu, Yi Wang, Jianbin Qin, and Wei Wang. Dair: query-efficient decision-based attack on imIn Proceedings of the 44th Internaage retrieval systems. tional ACM SIGIR Conference on Research and Development in Information Retrieval, pages 10641073, 2021. 3 [6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. 2019. 2 [7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. 2019. 2 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 2 [10] Gintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M. Roy. study of the effect of jpg compression on adversarial images, 2016. 2, 10 [11] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 1(2):6, 2021. 1 [12] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions, 2022. 3 [13] Bangyan He, Xiaojun Jia, Siyuan Liang, Tianrui Lou, Yang Liu, and Xiaochun Cao. Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation, 2023. [14] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 1, 2, 3, 6, 7 [15] Fan Hu, Aozhu Chen, and Xirong Li. Towards making In ICASSP trojan-horse attack on text-to-image retrieval. 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 3 [16] Jaehui Hwang, Huan Zhang, Jun-Ho Choi, Cho-Jui Hsieh, and Jong-Seok Lee. Temporal shuffling for defending deep action recognition models against adversarial attacks. Neural Networks, 169:388397, 2024. 2, 10 [17] Peng Jin, Jinfa Huang, Fenglin Liu, Xian Wu, Shen Ge, Guoli Song, David Clifton, and Jie Chen. Expectationmaximization contrastive learning for compact video-andlanguage representations. Advances in Neural Information Processing Systems, 35:3029130306, 2022. 2 [18] Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, and Jie Chen. Videotext as game players: Hierarchical banzhaf interaction for In Proceedings of cross-modal representation learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24722482, 2023. 1 [19] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transIn Proceedings of formers for language understanding. naacL-HLT, page 2, 2019. 2, 6 [20] Raz Lapid and Moshe Sipper. see dead people: Gray-box adversarial attack on image-to-text models, 2023. 3 [21] Jie Lei, Tamara Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 487507, Toronto, Canada, 2023. Association for Computational Linguistics. 1, 2, 3, 6, 7, 8, 9 [22] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation, 2021. 2 [23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the 39th International Conference on Machine Learning, pages 1288812900. PMLR, 2022. [24] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXX 16, pages 121137. Springer, 2020. 2 [25] Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui Wang, Hang Su, and Hui Xue. Qair: Practical query-efficient black-box attacks for image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33303339, 2021. 3 [26] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019. 2 [27] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019. 2 [28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 2 [29] Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, and Feng Zheng. Set-level guidance attack: Boosting adversarial transferability of vision-language In Proceedings of the IEEE/CVF Inpre-training models. ternational Conference on Computer Vision (ICCV), pages 102111, 2023. 2, 3, 6, 7, 8, [30] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. 2 [31] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval, 2021. 3 [32] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293304, 2022. 2 [33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 6 [34] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [35] Jesus Andres Portillo-Quintero, Jose Carlos Ortiz-Bayliss, and Hugo Terashima-Marın. straightforward framework for video retrieval using clip. In Mexican Conference on Pattern Recognition, pages 312. Springer, 2021. 2 [36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2 [38] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: joint model for video In Proceedings of and language representation learning. the IEEE/CVF international conference on computer vision, pages 74647473, 2019. 2 [39] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019. 2 [40] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and XianSheng Hua. Disentangled representation learning for textvideo retrieval. arXiv:2203.07111, 2022. 1, 2, 3, 6, 7, 8, [41] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do In Proceedings of the IEEE/CVF for text-video retrieval? Conference on Computer Vision and Pattern Recognition (CVPR), pages 1070410713, 2023. 1, 2, 3, 6, 7, 8, 9 [42] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training arXiv preprint for zero-shot video-text understanding. arXiv:2109.14084, 2021. 2 [43] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 1, 2, 3, 6, 7, 8 [44] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pretrained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430, 2022. 1, 2 [45] Haozhe Yang, Yuhan Xiang, Ke Sun, Jianlong Hu, and Xianming Lin. Towards video-text retrieval adversarial attack. 13 In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 65006504, 2024. 1, 3, 6 [46] Karren Yang, Wan-Yi Lin, Manash Barman, Filipe Condessa, and Zico Kolter. Defending multimodal fusion modIn Proceedings of els against single-source adversaries. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33403349, 2021. 3 [47] Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, and Fenglong Ma. Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models. In Advances in Neural Information Processing Systems, pages 5293652956. Curran Associates, Inc., 2023. [48] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 1, 2, 6, 7 [49] Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversarial attack on vision-language pre-training models. In Proceedings of the 30th ACM International Conference on Multimedia, page 50055013, New York, NY, USA, 2022. Association for Computing Machinery. 2, 3, 6, 7, 8, 10 [50] Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversarial attack on vision-language pre-training models. In Proceedings of the 30th ACM International Conference on Multimedia, page 50055013, New York, NY, USA, 2022. Association for Computing Machinery. 3 [51] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. arXiv preprint arXiv:2305.16934, 2023. 3 [52] Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning Zheng, and Gang Hua. Practical relative order attack in deep ranking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16413 16422, 2021. 3 [53] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87468755, 2020. 14 Adversarial Video Promotion Against Text-to-Video Retrieval"
        },
        {
            "title": "Supplementary Material",
            "content": "Algorithm 2 Pseudo-code for Temporal Clipping. Require: Clean video = [x1, ..., xT ], video features FX threshold for outlying frames γ Ensure: Clipped Videos 1: function TEMPCLIP(WX, X) 2: WX COSSIM(FX, FX) similarity temporal row 0, WX GETDIFF(WX) while row < do col row while col < do WX[row, col] if γ then and col /4 Clip the similarity WX and the video Break else col col + 1 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end if end while row row + 1 14: 15: 16: 17: 18: end function end while return Clipped Videos Data Construction. As mentioned in the main paper, we find targeted queries of the ViPro attack by conducting candidate-wise sortation, as shown in Fig. 9. Specifically, these texts are re-ranked by using them to re-query the model and check the rank of the candidate under these texts. If the candidates are within the top-20 results, the corresponding captions are categorized as targeted queries. The reranked 20 texts are then shuffled and evenly divided into trainsets and testsets. Re-rank are conducted using the Singularity-17M model. Pseudo-code for Temporal Clipping (TC). We provide pseudo-code for Temproal Clipping in Alg.2. TC first calculates the frame-to-frame correlation using cosine similarity to get WX, and then calculates the temporal difference between frames WX . Frames that exceed γ will be marked as temporally outlying frames and clipped if the clip length is longer than /4, which ensures that the video is not too fragmented to preserve temporal information. We also provide an illustration on the result of TC, as shown in Fig.10. Categorical Performance. As discussed in the main paper, we use the categorical information provided in [43] and choose the top 10 categories with the best R-Precisions. We provide the R-Precisions for all models on all categories as follows. Interestingly, we find that all categories with low R-Precision are very non-specific words, such as documentary, how-to and people, with which VLMs might find it difficult to relate visual contents. On the other hands, categories with higher precisions are generally ones with concrete implications, such as animation, sports, and gaming. For the distribution of data in each categoriy, please refer to the original paper[43]. Based on the R-Precision, we chose the 10 categories with the highest average performance: Sports, Movie, Animation, Music, Animal, Vehicles, Gaming, News, Education, Cooking. Figure 9. An illustration of data stealing: i. Attackers can obtain diverse subset of training videos and corresponding texts by querying the victim model. ii. Attackers can perform candidatewise sortation for each candidate video by querying the candidate video with the texts from step i, checking the rank of the candidate video, and categorizing the texts into Relevant and Irrelevant. 1 Table 10. R-Precision for first 10 categories on all models. The datset is MSR-VTT-1k. Chosen categories are highlighted in bold. Models Sing DRL C4V R-Precision (%) Music People Gaming Sports News Education TV shows Moive Animation Vehicles 35. 44.59 35.14 0.00 0.00 0.00 24. 37.74 35.85 42.11 64.21 46.32 13. 29.31 53.45 15.38 34.62 42.31 20. 16.67 26.67 31.15 49.18 44.26 26. 36.84 57.89 25.00 51.32 23.68 Table 11. R-Precision for last 10 categories on all models. The dataset is MSR-VTT-1k. Chosen categories are highlighted in bold. Models Sing DRL C4V R-Precision (%) How-to Travel Science Animal Kids Documentary Cooking 13.04 13.04 13.04 3.45 24.14 31. 11.63 27.91 34.88 12.07 55.17 37. 10.64 25.53 8.51 0.00 0.00 0. 12.50 37.50 15.62 food 9.09 45. 33.33 Beauty Advertisement 11.32 11.32 32. 12.50 16.67 20.83 Table 12. An ablation study on the influence of the access to the video aggregating transformer within the C4V model. All attacks have gained performance boost with access to the aggregator. Our ViPro maintains its lead in either scenario."
        },
        {
            "title": "Access to Video Aggregator",
            "content": "R@1(%) R@5(%) Average (%) Co-Attack"
        },
        {
            "title": "ViPro",
            "content": "4.66 (-0.21) 20.86 (-2.08) 12.76 (-1.15) 26.48 (+22.72) 53.11 (+39.69) 43.05 (+32.21) 0.98 (-3.98) 4.05 (-18.89) 2.52 (-11.39) 5.20 (+1.44) 18.98 (+5.56) 12.09 (+3.50) 8.71 (+3.84) 28.9 (+5.85) 18.76 (+4.85) 77.10 (+73.34) 90.65 (+77.23) 83.88 (+75.29) 2 Figure 10. An illustration of the clipped video frames through temporal clipping. All temporally related frames are grouped into clips without creating too fragmented videos, i.e., only 1-2 frames per clip."
        }
    ],
    "affiliations": [
        "Xian Jiaotong University"
    ]
}