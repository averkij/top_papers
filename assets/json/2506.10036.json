{
    "paper_title": "Token Perturbation Guidance for Diffusion Models",
    "authors": [
        "Javad Rajabi",
        "Soroush Mehraban",
        "Seyedmorteza Sadat",
        "Babak Taati"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2$\\times$ improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance"
        },
        {
            "title": "Start",
            "content": "Javad Rajabi1,2 Soroush Mehraban1,2,3 Seyedmorteza Sadat4 Babak Taati1,2,3 1University of Toronto 3KITE Research Institute 2Vector Institute for Artificial Intelligence 4ETH Zürich 5 2 0 2 J 0 1 ] . [ 1 6 3 0 0 1 . 6 0 5 2 : r {rajabi, taati}@cs.toronto.edu soroush.mehraban@mail.utoronto.ca seyedmorteza.sadat@inf.ethz.ch"
        },
        {
            "title": "Abstract",
            "content": "Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly 2 improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as general, condition-agnostic guidance method that brings CFG-like benefits to broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance"
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [1, 2, 3] have emerged as the main methodology behind many successful generative models for images [4, 5], videos [6, 7, 8, 9, 10], audio [11, 12], and 3D objects [13, 14, 15]. Despite their theoretical capacity to produce high-fidelity data, unguided diffusion models often suffer from poor sample quality, manifesting as visual artifacts, lack of semantic consistency, and insufficient sharp details [16]. To mitigate these issues, classifier-free guidance (CFG) [17] has become the de facto approach to steer the generation process toward higher quality and more semantically aligned outputs. However, CFG is inherently limited to conditional generation and requires specific training strategy that randomly replaces the input condition with null condition. In response to these constraints, several alternative guidance techniques have emerged, aiming to extend CFG-like benefits to broader settings [18, 16, 19, 20, 21]. These approaches often manipulate components of the denoiser network, such as attention layers, to construct effective guidance signals. However, they either require additional specialized training or offer limited improvements in prompt alignment and generation quality (particularly w.r.t. unconditional generation). Accordingly, there remains need for training-free guidance mechanism that works across both conditional and unconditional settings while improving generation quality and semantic alignment similar to CFG. In this paper, we revisit existing attention-based guidance techniques and aim to bridge the gap between the effectiveness of CFG and that of training-free, condition-agnostic methods. We observe that CFG effectively recovers global structure and coarse details during the early denoising steps, Figure 1: Visualization of the denoising process over time for different guidance strategies: CFG [17], PAG [19], SEG [18], and our proposed TPG. Each row shows generated images at various denoising time steps, from = 981 (left) to = 1 (right). The red box highlights the early-to-middle denoising stage (t = 821 to = 741), where CFG and TPG demonstrate clearer structure (e.g. horse face) and consistency. The text prompt used is \"a female in black jacket is riding brown and white horse\". whereas existing training-free methods tend to produce over-smoothed results at the same stage (see highlighted regions in Figure 1). This is problematic because early denoising steps are critical for both image quality and prompt alignment, as they establish global structure, major shapes, and coarse semantics before the network begins refining fine details [22, 23]. If the model fails to capture correct semantics, object placement, or overall composition at this stage, it may never fully recover from that high-level mismatch in later refinements. This lack of sufficient early-step guidance likely explains why existing methods often yield only marginal improvements in prompt alignment and generation quality compared to CFG. Motivated by these insights, we introduce Token Perturbation Guidance (TPG), novel method that directly perturbs intermediate token representations within the diffusion network, without requiring additional training or architectural changes. TPG employs token shuffling as the core operation to provide effective guidance signals. Specifically, token shuffling is (i) linear, (ii) preserves token norms, and (iii) disrupts local structure while maintaining global statistics. As shown in Figure 1, TPG exhibits behavior similar to CFG and, compared to other training-free methods, more faithfully recovers both global structure and fine details at early denoising stages. We evaluate the effectiveness of TPG on both conditional and unconditional image generation using SDXL [5] and Stable Diffusion 2.1 [24]. Our results show that TPG achieves nearly 2 improvement in FID for unconditional generation compared to the SDXL baseline and also significantly outperforms existing perturbation-based guidance techniques. Furthermore, we observe that TPG closely mirrors CFG in terms of alignment and its effect on the sampling process, i.e., positively aligning with ground-truth noise in low-frequency bands, remaining largely orthogonal at other frequencies, and following similar norm profile throughout denoising. These findings establish TPG as general, condition-agnostic guidance method that extends CFG-like benefits to broader class of diffusion models, including those for unconditional generation."
        },
        {
            "title": "2 Related work",
            "content": "Score-based diffusion models [25, 1, 26, 3] reverse forward noising process by learning the score function, the gradient of the log data density, at multiple noise levels to progressively transform pure Gaussian noise into realistic samples [26]. This principled estimator of the data distribution has outperformed previous generative modeling methods in both fidelity and mode coverage [23]. Since the score represents an explicit gradient field, the sampling trajectory can be guided by incorporating auxiliary gradients, leading to powerful guidance techniques such as classifier guidance (CG) [23] and classifier-free guidance (CFG) [17]. These guidance methods significantly enhance image quality and prompt alignment, albeit at the cost of oversaturation [27] and reduced diversity [17, 28]. 2 Although CFG improves image fidelity and alignment with the input condition, it is inherently restricted to conditional generation. Moreover, since its guidance signal is defined as the difference between conditional and unconditional denoising outputs, CFG requires specific training procedures and its sampling trajectory can overshoot the desired conditional distribution, leading to skewed or oversimplified images [29]. Autoguidance [16] builds on CFG by introducing deliberately weaker, also known as the bad version of the noise predictor, i.e., less-trained denoiser network, to produce the guidance signal. While this avoids CFGs reliance on the unconditional score, identifying an effective bad version is non-trivial and still requires training and careful tuning of the model. By contrast, our method requires no additional training or architectural changes. Recently, attention-based perturbation methods have shown promising results in improving the quality of generated images by leveraging or modifying the attention maps within the diffusion models attention blocks. Self-Attention Guidance (SAG)[20] uses the models own attention map to blur the denoiser input. Perturbed Attention Guidance (PAG)[19] replaces the attention map with an identity matrix to form the guidance signal, while Smoothed Energy Guidance (SEG) [18] applies Gaussian blurring to the attention maps. Although these techniques enhance image quality without additional training or auxiliary models, their impact on image quality and prompt alignment remains limited compared to CFG. In contrast, we show that TPG provides stronger guidance signal, bridging the gap between the effectiveness of CFG and that of training-free guidance methodsboth in terms of image quality and alignment with the input condition."
        },
        {
            "title": "3 Background",
            "content": "Diffusion Models Denoising diffusion models generate samples from data distribution pdata(x) by reversing gradual noising process [1, 25]. In the forward process, clean sample x0 pdata is progressively corrupted into noise xT through stochastic differential equation (SDE): dx = (x, t)dt + g(t)dw, (1) where (x, t) and g(t) are predefined functions representing the drift and diffusion coefficients, and dw is an increment of the standard Wiener process. widely adopted formulation is the variancepreserving (VP) diffusion process [3], where xt gradually approaches an isotropic Gaussian as . 2 β(t)x and g(t) = (cid:112)β(t) where β(t) defines the noise typical parameterization is (x, t) = 1 schedule. With this formulation, the denoising process is given by the following reverse-time SDE: (cid:20) dx = 1 2 (cid:21) β(t)x β(t)x log pt(x) dt + (cid:112)β(t)d (2) where log pt(x) is the score function, representing the gradient of the log-density of the noisy data at time t. This gradient indicates the direction in which sample should be updated to increase its likelihood w.r.t. the noisy data distribution. The unkown score function is typically approximated by neural network sθ(x, t), trained using denoising score matching [30]. Given noisy sample xt at time t, the network predicts the score of the marginal distribution pt. Training is performed by minimizing weighted denoising score matching loss: L(θ) = 1 2 (cid:90) 0 β(t)Ex0p0 Extpt0(xtx0) (cid:13) (cid:13)sθ (cid:0)xt, t(cid:1) xt log pt0 (cid:0)xt x0 (cid:1)(cid:13) 2 2dt, (cid:13) (3) where pt0(xt x0) is the known Gaussian transition kernel of the forward SDE. Once trained, samples are generated by integrating the reverse SDE using sθ(x, t) log pt(x) as an approximation to the score function. Conditional generation is enabled by training score network that receives additional conditioning signals, e.g., class labels or text prompts, as input. In this case, the score network sθ(x, t, c) approximates the conditional score log pt(x c). Guidance Although diffusion models trained via denoising score matching have strong theoretical foundations, the score approximations are often inaccurate due to limited model capacity. As result, unguided sampling using the learned score function tends to produce blurry and low-fidelity images, especially in complex tasks such as text-conditional generation. To address these limitations and improve generation quality, classifier-free guidance (CFG) [17] was introduced to steer the reverse diffusion trajectory toward higher quality outputs by linearly interpolating between conditional and unconditional score estimates. In general, guidance in diffusion model can be defined as: θ (xt, c, t) + γ[s+ θ (xt, c, t) sθ(xt, c, t) = s+ θ (xt, c, t)], (4) Algorithm 1 Token Perturbation Guidance (TPG) for Diffusion Models Require: Noisy input xT (0, III), shuffling matrices SSSk,t, set of perturbed layers L, score function (denoiser) sθ, guidance scale γ, total time steps 1: for = T, . . . , 1 do 2: 3: 4: 5: 6: 7: 8: // Forward pass without perturbation s+ θ (xt) sθ(xt, t) // Forward pass with perturbation Run the network sθ(xt) second time with the following modification to have for each layer do if then θ (xt): Apply token perturbation: HHH SSSk,tHHH // Apply token perturbation guidance sθ(xt, t) s+ θ (xt) // Update sample xt1 SolverStep(xt, sθ(xt, t)) θ (xt) + γ(s+ θ (xt)) 9: 10: 11: 12: 13: return x0 where s+ θ (xt, c, t) estimates the desired direction (typically the conditional score sθ(xt, c, t)), and θ (xt, c, t) acts as negative score. In such guidance methods, samples are effectively pushed more toward positive signal and away from negative signal. In CFG, by assigning θ (xt, c, t) = sθ(xt, , t), the samples are pushed away from the score of the unconditional data distribution and more strongly toward the given condition. Other guidance techniques implement θ (xt, c, t) with less-trained diffusion model [16], or by perturbing attention maps or input pixels [19, 18, 20]."
        },
        {
            "title": "4 Token perturbation guidance",
            "content": "We next introduce Token Perturbation Guidance (TPG), novel guidance method that directly perturbs token representations within the diffusion model. Unlike previous approaches that modify model weights or attention mechanisms, TPG operates on the intermediate token representations in the denoiser during inference. To improve sample quality, we use token shuffling as simple choice of the perturbation, which preserves global structure while disrupting local patterns. TPG is training-free and does not require any changes to the model architecture, effectively extending the benefits of CFG to broader class of diffusion models. Let HHH RBN denote the intermediate hidden representations, where is the batch size, is the number of tokens, and is the feature dimension per token. We apply the shuffling operator SSS RN along the token dimension to get HHH = SSSHHH. These shuffled tokens are used to define the negative score θ (xt, c, t) in TPG. The shuffling operation satisfies the following properties: Linearity: This ensures that the perturbations can be expressed as matrix multiplication, enabling efficient implementation within the denoising process. Thus, TPG does not add any noticeable overhead to the sampling process, and it has practically the same sampling cost as CFG. Norm preservation: The shuffling matrix is orthonormal, meaning it satisfies SSSSSS = III and acts as rigid rotation or permutation in the embedding space. This property guarantees that the preturbation preserves token norms, i.e., we have SSSHHH2 = HHH2. As result, the magnitude of the feature representations remains unchanged, which helps maintain their statistical properties and prevents internal covariate shift [31]. Algorithm 1 summarizes the inference procedure of TPG. At each timestep, two forward passes are performed: one standard, and one with token perturbations applied at selected layers via shuffling matrices SSSk,t, unique at each time step and each layer k. The outputs are combined to guide the denoising trajectory toward higher-quality samples. Accordingly, TPG can be applied to both conditional and unconditional models and does not require additional training of the base model. Figure 2: Analyzing the behavior of different guidance methods across denoising steps. (a) Cosine similarity between the added guidance term in eθ = eθ + γe and the true noise ϵ. SEG and PAG exhibit negative alignment at intermediate steps, while TPG and CFG maintain near-zero cosine values, indicating orthogonality to the noise. (b) Cosine similarity between the full guided score eθ and ϵ. Compared to SEG and PAG, TPG behaves more similarly to CFG across sampling. (c) ℓ2 norm of the guidance term e. TPG and CFG follow nearly identical trends, both starting around 40 and increasing steeply in the later denoising steps. In contrast, SEG and PAG maintain consistently low norms throughout."
        },
        {
            "title": "5 Comparing the behavior of TPG with other guidance methods",
            "content": "To better understand how different guidance strategies influence denoising, we analyzed their interaction with the true noise signal across various steps of the denoising process. In this experiment, we selected 1,000 images from the MS-COCO 2014 validation set [32]. Instead of starting from pure random noise, we corrupted each image with noise corresponding to specific time step to generate the noisy input. This noisy image was then passed through the denoiser to produce the guided output for various methods (e.g., SEG, CFG, and TPG). We analyzed the angle between the predicted and ground-truth noise and examined the frequency components by partitioning the spectrum (up to radius 0.7) into 29 bins. Importantly, we did not use the denoiser output to progress to the next step; rather, for each time step, we reapplied noise directly to the clean image. As shown in Figure 2, TPG and CFG produce guidance vectors that are nearly orthogonal to the ground-truth noise throughout the trajectory, as indicated by cosine values close to zero. In contrast, PAG and SEG exhibit strong negative alignment during the middle steps, suggesting that they temporarily oppose the denoising direction. Figure 2 (b) further shows that, compared to SEG and PAG, TPG more closely mirrors the behavior of CFG in terms of alignment between the predicted and ground-truth noise. Additionally, Figure 2 (c) shows that TPG and CFG exhibit similar guidance magnitudes across all denoising steps, while SEG and PAG show substantially lower norms. This indicates that the update terms in CFG and TPG are more influential throughout the sampling process compared to that in PAG and SEG. Figure 3 complements our step-wise analysis by illustrating how each guidance method behaves in the frequency domain. TPG and CFG remain almost perfectly orthogonal to the ground-truth noise across all frequencies and time steps, except for slight positive tilt in the lowest frequency bands. SEG, however, exhibits clear negative stripe at medium frequencies during intermediate steps, confirming that its correction momentarily opposes the desired direction. The norm heatmaps further reveal that CFG and TPG inject strong low-frequency signal in the early steps, while high-frequency modifications primarily occur in the later denoising steps. In contrast, SEG operates with less energy and displays markedly different norm pattern when modifying high-frequency content, indicating relatively weaker approach to detail refinement. This behavior aligns with Figure 1, where the reduced energy across frequency bands leads to overly smooth generations in the initial steps. In summary, these results show that the TPG update closely mirrors the behavior of CFG both in direction and frequency content across different denoising steps, suggesting more effective performance compared to previous training-free guidance methods like PAG and SEG."
        },
        {
            "title": "6 Experiments",
            "content": "Setup All experiments are conducted on RTX6000 GPUs via official implementations and pretrained checkpoints provided by publicly available repositories. We build upon the current opensource state-of-the-art Stable Diffusion XL (SDXL) [5] as our primary baseline, and include the 5 Figure 3: Frequency analysis of guidance residuals throughout sampling. Each heatmap shows either the cosine similarity between the guidance term and the ground-truth noise ϵ (top row), or the ℓ2 norm of the guidance term (bottom row), as function of frequency bin (horizontal axis) and denoising step (vertical axis; 1000 1). Top: For both CFG and TPG, the guidance term remains almost orthogonal to the noise across all frequencies, with mild positive bump in the lowest bands. In contrast, SEG transitions from weak positive alignment in the early steps to pronounced negative stripe centered at medium frequencies. Bottom: CFG and TPG concentrate most of their energy in the lowest frequency bin and inject significantly larger magnitudes than SEG, whose energy remains up to two orders of magnitude smaller throughout the denoising process. Stable Diffusion 2.1 (SD 2.1) [33] to show the generality of our method and analysis. Moreover, TPG is easy to implement, and it can be added into existing diffusion models as plug-and-play module with just few lines of additional code. In all experiments, the compared methods are evaluated using their original configurations and default guidance scales. The guidance scale for TPG is fixed at 3.0. Metrics We adopt Fréchet Inception Distance (FID) [34] as our principal metric because it correlates well with human preferences and jointly captures image quality and diversity. To quantify prompt alignment, we report the average CLIP Score [35] between sampled images and their corresponding prompts. Overall perceptual quality is further evaluated with the Inception Score and an Aesthetic Score [36]. Moreover, all experiments are conducted by generating 30k samples for each method (unless stated otherwise) and evaluated on the MS-COCO 2014 validation set [32]. 6.1 Quantitative results Table 1 presents quantitative comparison of TPG against vanilla SDXL, PAG, SEG, and CFG for both unconditional and conditional image generation tasks. In unconditional generation, TPG outperforms all compared methods by achieving notably lower FID and sFID scores (69.31 and 44.18, respectively), indicating better overall image quality and diversity. Additionally, TPG achieves the highest Inception Score (17.99) while maintaining competitive Aesthetic Score, further reflecting the perceptual quality of TPG outputs. In conditional generation, CFG achieves the best results overall, with TPG closely following and consistently outperforming vanilla SDXL, PAG, and SEG in FID, sFID, and CLIP Score metrics. Table 2 further compares TPG against vanilla Stable Diffusion 2.1, PAG, and SEG in conditional image generation. TPG consistently achieves the best performance, significantly outperforming the other methods with the lowest FID score (16.69) and highest Inception Score (36.28). It also 6 Table 1: Quantitative comparison of TPG against vanilla SDXL [5], PAG [19], SEG [18], and CFG [17] for unconditional and conditional image generation. Lower FID and sFID scores indicate superior image quality and diversity, while higher Inception, Aesthetic, and CLIP scores indicate enhanced perceptual quality and semantic alignment. TPG achieves the best metrics in unconditional generation and closely matches the performance of CFG in conditional generation. Setting Metric Vanilla SDXL [5] PAG [19] SEG [18] CFG [17] TPG (Ours) Unconditional Conditional FID sFID Inception Score Aesthetic Score FID sFID Inception Score Aesthetic Score CLIP Score 124.04 78.91 9.19 5.02 48.97 43.71 22.10 5.37 27.47 98.83 94.71 13.74 5.94 20.49 28.78 34.66 6.11 29.67 82.64 74.98 13.22 6. 23.94 31.50 30.29 6.18 29.49 - - - - 12.79 23.31 42.75 6.20 32.03 69.31 44.18 17.99 6.14 17.77 24.32 34.89 6.12 30.15 Table 2: Quantitative comparison of TPG with vanilla Stable Diffusion 2.1 [24], PAG [19], and SEG [18] for conditional generation. TPG outperforms other baselines in all evaluated metrics. Metric Vanilla SD [24] PAG [19] SEG [18] TPG (Ours) FID Inception Score Aesthetic Score Clip Score 25.24 24.59 5.07 27.74 21.30 28.80 5.93 29.03 20.98 25.15 5.83 28.53 16.69 36.28 5.97 29.30 maintains competitive results in terms of Aesthetic Score (5.97) and CLIP Score (29.30), highlighting TPGs strength in producing high-quality and semantically aligned images. 6.2 Qualitative results Figure 4 highlights the differences in unconditional generations produced by various guidance methods. Vanilla SDXL, PAG, and SEG often generate abstract or repetitive textures lacking clear semantic structure, while TPG consistently produces well-structured and realistic scenes. Across various initial seeds, TPG is less prone to generating abstract patterns and more likely to form coherent spatial layouts with identifiable objects. Moreover, visual artifacts such as unnatural textures or distortions are more noticeable in outputs from baseline methods, whereas TPG reduces such artifacts, resulting in cleaner and more realistic generations. We also show qualitative comparison of conditional image generation across different guidance methods in Figure 5. As can be seen, CFG and TPG consistently produce the highest-quality images, with sharp details and strong alignment to the text prompts. In contrast, other baselines such as SEG and PAG often generate outputs that deviate from the given condition, leading to less faithful semantic content. Furthermore, visual artifactssuch as distorted shapes or inconsistent texturesare more frequently observed in SEG and PAG outputs, while TPG exhibits improved robustness, generating cleaner and more semantically accurate images with fewer artifacts. 6.3 Ablating other norm-preserving perturbation methods To assess the importance of shuffling during guidance, Table 3 reports the results of various normpreserving perturbation methods compared to the vanilla baseline. In addition to the shuffling strategy used in TPG, we evaluated Sign Flip, Hadamard, and Haar transforms, all described in Appendix A. While these three alternatives provide slight improvements in generation quality over the baseline, their gains are modest. In contrast, shuffling yields substantially larger improvement in both FID and Inception Score, indicating significantly better image quality and diversity. Although all perturbations are orthogonal and preserve token norms, they influence the features differently. Shuffling randomly reorders tokens, disrupting local patterns while preserving recoverable global structure, which facilitates stronger guidance during inference. By comparison, Hadamard and Haar transformations mix all tokens together, potentially distorting useful information and weakening the 7 Figure 4: Qualitative comparison of unconditional generations produced by Vanilla SDXL [5], PAG [19], SEG [18], and our method (TPG). TPG achieves more realistic generations compared to other training-free guidance methods. Table 3: Comparison of different token perturbation methods evaluated using 5K samples. Metric Vanilla Sign Flip Hadamard FID Inception Score 131.57 9.21 119.23 10.98 120.54 10.34 Haar 118.47 10.75 Shuffling 78.43 18.26 guidance signal. Sign Flip merely alters the sign of each token, which may not offer strong enough signal to steer samples effectively toward desirable regions of the data distribution."
        },
        {
            "title": "7 Conclusion and discussion",
            "content": "In this paper, we introduced Token Perturbation Guidance (TPG), novel, training-free method for enhancing the quality of diffusion models by directly perturbing intermediate token representations. TPG employs token shuffling to define an effective guidance signal, extending the benefits of classifierfree guidance to broader range of models. Through extensive experiments, we demonstrated that 8 Figure 5: Qualitative comparison of conditional generations produced by Vanilla SDXL [5], CFG [17], PAG [19], SEG [18], and our method (TPG). TPG is able to achieve good quality and prompt alignment compared to other baselines such as PAG and SEG. TPG improves the quality of both conditional and unconditional generation, while also enhancing prompt alignment in conditional setups. Our analysis further showed that, unlike existing attentionbased perturbation methods such as SEG and PAG, the behavior of TPG closely resembles that of CFG in terms of the direction and frequency content of the guidance term. We thus consider TPG simple, plug-and-play method that effectively bridges the gap between existing training-free guidance methods and CFG both in quality and prompt alignment. Despite these strengths, as with CFG itself, TPG still requires two forward passes through the diffusion network, leading to increased sampling time compared to the unguided case. Additionally, although TPG significantly improves quality in most situations, the guidance term may remain limited in extreme out-of-distribution scenarios that are not captured by the learned distribution of the base model. We consider addressing these limitations an interesting direction for future research."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2, 3 [2] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 1 [3] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 2, 3 [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [5] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 2, 5, 7, 8, 9, 16, 17, 18 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [7] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 1 [8] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1 [9] Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, and Anima Anandkumar. Efficient video diffusion models via content-frame motion-latent decomposition. arXiv preprint arXiv:2403.14148, 2024. 1 [10] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di ZHANG, et al. Videotetris: Towards compositional text-to-video generation. Advances in Neural Information Processing Systems, 37:2948929513, 2024. [11] Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. AudioX: Diffusion transformer for anything-to-audio generation. arXiv preprint arXiv:2503.10522, 2025. 1 [12] Jinlong Xue, Yayue Deng, Yingming Gao, and Ya Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 1 [13] Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, et al. NeuSDFusion: spatial-aware generative model for 3d shape completion, reconstruction, and generation. In European Conference on Computer Vision, pages 118. Springer, 2024. 1 [14] Ying-Tian Liu, Yuan-Chen Guo, Guan Luo, Heyi Sun, Wei Yin, and Song-Hai Zhang. PI3D: Efficient text-to-3d generation with pseudo-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1991519924, 2024. 1 [15] Zizheng Yan, Jiapeng Zhou, Fanpeng Meng, Yushuang Wu, Lingteng Qiu, Zisheng Ye, Shuguang Cui, Guanying Chen, and Xiaoguang Han. DreamDissector: Learning disentangled text-to-3d generation from 2d diffusion priors. In European Conference on Computer Vision, pages 124141. Springer, 2024. 1 [16] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 1, 3, [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2, 3, 7, 9, 13, 16 10 [18] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. arXiv preprint arXiv:2408.00760, 2024. 1, 2, 3, 4, 7, 8, 9, 13, 14, 15, 16, 17, 18 [19] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pages 117. Springer, 2024. 1, 2, 3, 4, 7, 8, 9, 13, 14, 15, 16, 17, [20] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74627471, 2023. 1, 3, 4 [21] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024. 1 [22] Binxu Wang and John Vastola. Diffusion models generate images like painters: an analytical theory of outline first, details later. arXiv preprint arXiv:2303.02490, 2023. 2 [23] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 7, 14, 15 [25] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. 2, 3 [26] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 2 [27] Seyedmorteza Sadat, Otmar Hilliges, and Romann M. Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 2 [28] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M. Weber. CADS: In The Twelfth Unleashing the diversity of diffusion models through condition-annealed sampling. International Conference on Learning Representations, 2024. 2 [29] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 3 [30] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. 3 [31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448456. pmlr, 2015. 4 [32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 5, [33] CompVis. stable-diffusion-v1-2. https://huggingface.co/CompVis/stable-diffusion-v1-2, 2022. 6 [34] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [36] Christoph Schuhmann. Improved Aesthetic Predictor: CLIP+MLP Aesthetic Score Predictor, 2022. Accessed: 2025-06-06."
        },
        {
            "title": "A Orthogonal token perturbation matrix designs",
            "content": "TPG directly applies structured perturbations to the intermediate token embeddings within the denoiser during inference. Specifically, consider the intermediate hidden-state activations of the denoiser at given layer, represented as tensor HHH RBN C, where denotes the batch size, the number of tokens, and the dimension of each tokens feature vector. At each denoiser layer and diffusion time step i, we apply the perturbation by multiplying the token-embedding with an orthogonal (or approximately orthogonal) matrix PPP k,i RN along the token dimension: HHH = PPPHHH. The primary goal of these perturbations is to preserve global information flow while disrupting local correlations that may lead to overfitting or artifacts. To achieve this, we investigated four distinct perturbation methods for PPP k,i: Token Shuffling. Represented by permutation matrix SSSk,i RN , where denotes the denoisers block index and the time step. The permutation matrix rearranges the tokens by selecting exactly one token from each position and assigning it to new position; mathematically, this means that each row and column contains exactly one entry of \"1\", while all other entries are zeros. It simply changes the order of tokens without altering their magnitude or norm, satisfying: SSS k,iSSSk,i = I. Random Sign Flipping. This perturbation method is defined using diagonal matrix DDDk,i RN , whose diagonal entries dj are drawn independently and identically from {+1, 1}. Each tokens embedding is thus flipped in sign independently. By construction, the matrix DDDk,i ensures that the ℓ2-norm of every token embedding is preserved, by satisfying the orthogonality condition: DDD k,iDDDk,i = I. WalshHadamard Transform (WHT). The WHT uses normalized Hadamard matrix WWW and which satisfies WWW WWW = IN . When is power of two RN , whose entries are 1/ (i.e. = 2m), we compute the transform of an token matrix in = log2(N ) iterative stages. We begin with WWW (0) = X, and for each stage = 1, . . . , m, update WWW (s) j,: = WWW (s1) j+2s1,: = WWW (s1) j,: + WWW (s1) j,: WWW (s1) j+2s1,:, j+2s1,:, WWW (s) = 1, 3, . . . , 2s1 + 1. After stages, the result WWW (m) equals X. This structured, deterministic mixing redistributes each tokens information uniformly across all others while preserving every tokens ℓ2-norm. Haar-Random Orthogonal Perturbation. At each denoiser block and diffusion time step i, we generate dense orthogonal matrix QQQk,i RN by first sampling AAA (0, 1) and then computing its QR decomposition AAA = QQQ RRR. We set QQQk,i = QQQ. Since the entries of AAA are i.i.d. Gaussian, the orthogonal factor QQQ is distributed uniformly (with respect to the Haar measure) over the orthogonal group O(N ), and by construction k,iQQQk,i = I. This yields an isotropic rotation in the -dimensional token-index space, mixing all token positions globally without changing their ℓ2-norm. QQQ Each method preserves the overall norm and energy of the embeddings but changes their local structure in uniquely effective ways. Empirically, these operations break up small-scale noise patterns that the denoiser might overfit, while still carrying global structure for high-quality sample generation. Among these methods, token shuffling typically provides the best overall performance: it is straightforward to implement, has minimal computational overhead, and consistently achieves significant improvements in both diversity and fidelity of generated samples."
        },
        {
            "title": "B Societal impact",
            "content": "Generative modeling, particularly in the domains of images and videos, holds immense potential for misuse, raising important ethical concerns. While advancements in sample quality, such as those achieved through our method, can make generated content more realistic and convincing, this heightened believability can unfortunately facilitate the spread of disinformation. Such misuse may have far-reaching negative effects on society, including the amplification of existing stereotypes and the inadvertent reinforcement of harmful biases. Although our improvements do not introduce entirely new uses for the technology, they may nonetheless increase the risk of these unintended consequences. It is therefore crucial to remain vigilant and consider the broader societal impacts that enhancements in generative modeling capabilities might entail."
        },
        {
            "title": "C Additional qualitative results",
            "content": "In this section, we provide additional qualitative results to showcase the effectiveness and adaptability of our Token Perturbation Guidance (TPG) method across different generation tasks and to compare its performance with other existing methods. Figure 6: Visualization of the denoising process over time for different guidance strategies: CFG [17], PAG [19], SEG [18], and our proposed TPG. Each row shows generated images at various denoising time steps, from = 981 (left) to = 1 (right). The text prompt used is \"A red stop sign underneath green street signs\". Figure 7: Analysis of guidance behavior across denoising steps in unconditional setting. (a) Cosine similarity between the added guidance term and the true noise ϵ. (b) Cosine similarity between the full guided score eθ and ϵ. (c) ℓ2 norm of the guidance term e. 13 Figure 8: Frequencystep analysis of guidance residuals in unconditional setting. Each heat-map plots either the cosine similarity between the guidance term and the ground-truth noise ϵ (top row) or the ℓ2-norm of the guidance term (bottom row) as function of frequency bin (horizontal axis) and denoising step (vertical axis; 1000 1). Figure 9: Qualitative comparison of conditional generation based on Stable Diffusion 2.1 [24] produced by PAG [19], SEG [18], and our TPG. 14 Figure 10: Qualitative comparison of unconditional generation based on Stable Diffusion 2.1 [24] produced by PAG [19], SEG [18], and our TPG. Figure 11: Qualitative comparison of face images based on Stable Diffusion 2.1 [24] generated by SEG [18] and by our TPG under both conditional and unconditional settings. SEG [18] clearly produces unrealistic patterns in the generated faces. 15 Figure 12: Qualitative comparison of conditional generations produced by Vanilla SDXL [5], CFG [17], PAG [19], SEG [18], and our TPG. Figure 13: Qualitative comparison of unconditional generations produced by Vanilla SDXL [5], PAG [19], SEG [18], and TPG. 17 Figure 14: More qualitative comparison of unconditional generations produced by Vanilla SDXL [5], PAG [19], SEG [18], and TPG."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "KITE Research Institute",
        "University of Toronto",
        "Vector Institute for Artificial Intelligence"
    ]
}