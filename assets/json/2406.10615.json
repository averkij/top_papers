{
    "paper_title": "Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation",
    "authors": [
        "Tong Zhang",
        "Yingdong Hu",
        "Jiacheng You",
        "Yang Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias-action locality, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2's success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform a variety of tasks at a markedly higher success rate compared to baseline models. Project website: http://sgrv2-robot.github.io"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 2 5 1 6 0 1 . 6 0 4 2 : r a"
        },
        {
            "title": "Leveraging Locality to Boost Sample Efficiency in\nRobotic Manipulation",
            "content": "Jiacheng You1 Yang Gao1,2,3 Tong Zhang1,2,3 Yingdong Hu1,2,3 1Tsinghua University 2Shanghai Qi Zhi Institute 3Shanghai Artificial Intelligence Laboratory {zhangton20,huyd21,yjc23}@mails.tsinghua.edu.cn, gaoyangiiis@mail.tsinghua.edu.cn Abstract: Given the high cost of collecting robotic data in the real world, sample efficiency is consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of critical inductive biasaction locality, which posits that robots actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2s success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform variety of tasks at markedly higher success rate compared to baseline models. Project website: sgrv2-robot.github.io. Keywords: Robotic Manipulation, Sample Efficiency"
        },
        {
            "title": "Introduction",
            "content": "The creation of versatile, general-purpose robot has long captivated the robotics community. Recent advances in imitation learning (IL) [1, 2, 3] have enabled robots to exhibit increasingly complex manipulation skills in unstructured environments. However, prevailing imitation learning techniques frequently require an abundance of high-quality demonstrations, the acquisition of which incurs substantial costs. This contrasts markedly with disciplines such as computer vision (CV) and natural language processing (NLP), wherein vast repositories of internet data are readily available for utilization. In this paper, we investigate methods to boost sample efficiency in robotic manipulation by developing improved visual and action representations. In machine learning, introducing inductive bias is standard strategy to enhance sample efficiency. For instance, CNNs [4, 5] inherently embed spatial hierarchies and translation equivariance in each layer, while RNNs [6] and LSTMs [7] incorporate temporal dependencies in their architecture. In the realm of robotic manipulation, critical inductive bias is action locality, which posits that robots actions are predominantly determined by the target object and its relationship with the surrounding local environment. However, previous studies on representation learning for robotic manipulation have not effectively leveraged this bias. Typically, these studies [8, 9, 10, 11] aim to develop global representation that encapsulates the entire scene, which is then directly employed to predict robot actions. These approaches have demonstrated notably low sample efficiency. As depicted in Figure 1, reducing the number of demonstrations from 100 to 5 leads to substantial decrease in the performance of previous works such as SGR [10], which seeks to capture both semantic and geometric information in global vector. Corresponding author 8th Conference on Robot Learning (CoRL 2024), Munich, Germany. Figure 1: Left: Sample efficiency of SGRv2. We evaluate SGR and SGRv2 on 26 RLBench tasks, with demonstration numbers ranging from 100 to 5. Results indicate that, owing to the locality of SGRv2, it exhibits exceptional sample efficiency, with its success rate declining by only about 10%. Top Right: Overview of simulation results. We test SGRv2 on 3 benchmarks, consistently outperforming the baselines. Bottom Right: Tasks of the 3 simulation benchmarks. To effectively utilize the inductive bias of action locality, we introduce SGRv2, systematic framework of visuomotor policy that considers both visual and action representations. As shown in Figure 2, SGRv2 builds on the foundation of SGR but integrates action locality throughout the entire framework. SGRv2 demonstrates exceptional sample efficiency and consistently outperforms its predecessor across various demonstration sizes, achieving remarkable results with as few as 5 demonstrations, compared to SGRs performance with 100 demonstrations. The key algorithmic designs that lead to this achievement include: (i) an encoder-decoder architecture for extracting point-wise features, (ii) strategy for predicting the relative target position to ensure translation equivariance, (iii) the application of point-wise weights to highlight critical local regions, and (iv) dense supervision to enhance learning efficiency. We conduct an extensive evaluation of SGRv2 through behavior cloning across three benchmarks: RLBench [12], where keyframe control is utilized, and ManiSkill2 [13] and MimicGen [14], where dense control is applied (refer to Section 3.1 for an discussion on keyframe versus dense control). SGRv2 significantly surpasses both SGR [10] and PointNeXt [15] across these benchmarks and consistently outperforms baselines, including R3M [9], PerAct [16], and RVT [17] on RLBench. To confirm the necessity of our locality design, we conduct series of ablation studies. Additionally, real-world experiments with Franka Emika Panda robot demonstrate SGRv2s capability to complete complex long-horizon tasks across 10 sub-tasks, validating its effectiveness. Further experiments on real-world generalization underscore SGRv2s remarkable ability to generalize."
        },
        {
            "title": "2 Related Work",
            "content": "Semantic Representation Learning for Robotics. Numerous studies have focused on learning visual representations from in-domain data, tailored specifically to the relevant environment and task [18, 19, 20, 21, 22, 23, 24]. However, the efficacy of these methods is limited by the availability of robot data. Consequently, various efforts have been made to pre-train on large-scale out-ofdomain data for visuo-motor control [25, 26, 27, 8, 28, 29, 30, 31, 32]. Notably, R3M [9] has demonstrated that models pre-trained on human video data can serve as frozen perception module, facilitating downstream policy learning. Nonetheless, these approaches predominantly prioritize the pre-training of 2D visual representations, thus overlooking critical 3D geometric information, which is essential for enhancing spatial manipulation skills in robotics. 3D Representation Learning for Robotics. Recent works have increasingly explored the 3D representations in robotics. Studies such as C2F-ARM [33], PerAct [16], and GNFactor [34] employ voxelized observations to derive representations. However, the creation and reasoning over voxels entail high computational burden. In contrast, RVT [17] and some researches [35, 36, 37] leverage projection techniques to generate multi-view images, extracting representation in 2D spaces 2 and thereby reducing computational demands. Nevertheless, these methods do not incorporate 3D geometry in the process of representation extraction, consequently limiting their capacity for 3D spatial reasoning. Point-based models, such as PointNet++ [38] and PointNeXt [15], efficiently conserve computational resources while directly processing 3D information. These models serve as the foundation for numerous robotics studies [39, 40, 41, 42, 43, 44, 45, 46, 47]. Specifically, SGR [10] utilizes point-based models to extract 3D geometric information and employs 2D foundational models for semantic understanding, integrating both to enrich representations for downstream tasks. However, the approach of SGR to extract actions from global vector does not effectively harness locality information, thereby leading to suboptimal sample efficiency. Incorporating Inductive Biases into Robot Learning. Incorporating inductive biases is an essential strategy for enhancing sample efficiency in neural network designs. Several studies have sought to improve sample efficiency by designing networks that adhere to equivariance properties. For example, Act3D [45] employs translation-equivariant neural network architecture, while USEEK [48] utilizes an SE(3)-equivariant keypoint detector for one-shot imitation learning. However, these approaches are based on global equivariance [49], which may not be effective when there are relative movements between the object and the environment, which is prevalent in robot manipulation. To tackle this issue, NDFs [50] and EquivAct [51] segment the manipulated object before utilizing the equivariant models. However, given their reliance on well-segmented point cloud, these methods are ineffective when the object is required to interact with its surroundings. Some studies integrate locality into their models. L-NDF [52] incorporates locality through voxel [49] and RiEMann [53] achieves both equivariance and locality via local partitioning, EDFs message-passing mechanisms in SE(3)-Transformers [54]. Nonetheless, these methods require some hyperparameters to specify proper size of receptive field, which limits their flexibility and extensibility. It also incurs tradeoff between locality and expressiveness. Another line of work, such as Transporter [55], PerAct [16], and RVT [17], integrates locality by modeling action as the maximizer of scores on predefined grid of locations. However, these designs render voluminous action representation and suffer from quantization artifacts [56]. In contrast, our approach not only satisfies translation equivariance without relying on highly non-local centroid subtraction [50, 57] but also adaptively determines the local scope required for the task through learnable weight, without sacrificing expressiveness or introducing grid."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present detailed description of the methodologies employed in SGRv2. Initially, we present an overview of SGR, keyframe and dense control, along with the problem formulation, as outlined in Section 3.1. Subsequently, we explore how to leverage the inductive bias of locality to enhance sample efficiency in robotic manipulation learning, as discussed in Section 3.2. Finally, we describe the training approaches under different control modes, detailed in Section 3.3. 3.1 Background [10]. SGR is composed of the semantic branch, Semantic-Geometric Representation (SGR) geometric branch and fusion network. In the semantic branch, the RGB part of RGB-D images are fed into CLIP [58] image encoder, and the resulting semantic features are then back-projected onto the 3D points. The geometric branch and fusion network split PointNeXt encoder into two stages, with semantic features injected at the interface between them. SGR models the action by solely relying on the global features extracted by the PointNeXt [15] encoder, without utilizing locality. Refer to Appendix B.2 for more details. Keyframe and Dense Control. In the field of robotic manipulation, keyframe control [33, 16, 59] and dense control [13, 60] are two prevalent control modes. Keyframe control outputs few sparse target poses, which are then executed through motion planner. It exhibits reduced compounding errors [59], and enhanced suitability for visuomotor tasks that leverage visual priors [33, 16, 17] at the cost of inferior flexibility and expressiveness. Dense control, on the other hand, generates 3 Figure 2: SGRv2 Architecture. Built upon SGR, SGRv2 integrates locality into its framework through four primary designs: an encoder-decoder architecture for point-wise features, strategy for predicting relative target position, weighted average for focusing on critical local regions, and dense supervision strategy (not shown in the figure). This illustration specifically represents the water plants task. For simplicity in the visualization, we omit the proprioceptive data that is concatenated with the RGB of the point cloud before being fed into the geometric branch. dense sequence with hundreds of actions to control the robot directly. It is applicable across wide range of robotic scenarios but faces significant challenges with compounding errors. Given that the two models are complementary, it is highly compelling to construct framework that supports both simultaneously. However, previous research has predominantly focused on single control mode [16, 17, 61]. In contrast, our framework can support both modes seamlessly. Problem Formulation. We frame our task as vision-based robot manipulation problem. At each timestep, the robot receives an observation comprising single or multi-view RGB-D images {Ik}K k=1 and proprioceptive data z. For keyframe control, following the setup in PerAct [16], an action consists of the position, rotation, gripper open state, and collision indicator: akeyframe = {apos, arot, aopen, acollide}. For dense actions, as illustrated in ManiSkill2 [13] and robosuite[60], an action consists of the delta position, delta rotation, and gripper open state [13, 60, 14]: adense = {apos, arot, aopen}. We assume we are given expert demonstration trajectories = {τi}N i=1. Each trajectory τi is sequence of observation-action pairs (o1, a1, . . . , aT 1, oT ). The robot is then trained using the Behavioral Cloning (BC) algorithm with these demonstrations. 3.2 Locality Aware Action Modeling To develop sample-efficient framework for robotic manipulation that is effective in both keyframe and dense control scenarios, we capitalize on the inductive bias that actions exhibit locality properties and build our locality aware action modeling on the top of SGR. In SGRv2, we achieve locality through 4 primary designs: (1) an encoder-decoder architecture, (2) strategy for predicting pointwise relative position formulation with (3) learned weight, and (4) dense supervision strategy. Refer to Figure 2 for an overview of our designs, Table 3 for ablation studies, and Appendix B.1 for architecture details. Encoder-Decoder Architecture. In contrast to the encoder-only architecture used by SGR [10], we employ the encoder-decoder architecture of PointNeXt [15], which is U-Net like architecture that excels in dense prediction tasks (e.g. segmentation). This architecture can yield feature enriched with both global and local information for each point, namely fi RC for the i-th point, where is the dimension of the feature. Note that as designed in PointNeXt [15], the output features are solely dependent on relative coordinates, ensuring that the point-wise features fi remain invariant to translational transformations of the input coordinates. This point-wise features serve as the cornerstone of our locality aware action modeling. Relative Position Predictions. With the point-wise features, we can predict an action at each point. Our key insight is that the end-effector usually moves towards target close to specific object within each execution stage. Thus, it is natural to predict the displacement of the target relative to each point. Driven by this insight, for keyframe, we represent the position component of keyframe action apos by pi+p(fi) for the i-th point, where pi and fi are the coordinate and point-wise feature of the i-th point respectively, and is Multilayer Perceptrons (MLP). For dense control, we can 4 predict the delta position component apos of dense action by modeling its direction (towards target) and magnitude separately. Concretely, we predict the direction and the magnitude apos2 by m(fi), where is another MLP. In dense control, given that we employ the end-effector coordinate frame for the point cloud input2, pi + p(fi) can be interpreted as the can represent the direction target position relative to the end-effector. Consequently, of movement. The utilization of relative position leverages the locality information and achieves translation equivariance without depending on the extensive non-local centroid subtraction [50, 57]. This greatly aids in enhancing sample efficiency. pi+p(fi) pi+p(fi)2 pi+p(fi) pi+p(fi)2 apos apos2 by For other action components, we directly predict rotation by r(fi), gripper open state by o(fi) and collision indicator by c(fi), where r, o, are MLPs. Weighted Average Actions. After obtaining the point-wise action predictions, we need to integrate these predictions into an aggregated action prediction. We adopt simple yet effective strategy, namely weighted average. For each component, including position (which is broken down into direction and magnitude in dense control), rotation, gripper open state, and collision indicators, we employ learned weight w(fi) for each point. Here, denotes separate MLPs (softmax is applied for normalization) for each component. The motivation behind this design is that only few regions within the point cloud are crucial for accomplishing the task. For instance, in tasks such as picking up cube, points located on the cube itself are more informative than those on the surrounding table. By learning these weights, we enable the aggregated prediction to concentrate on the most predictive local regions, thereby enhancing both the overall accuracy and sample efficiency. Dense Supervision. To enhance the learning efficiency of local features, we adopt dense supervision strategy. This approach integrates both aggregated action predictions and point-wise action predictions into the loss function, expressed as L* = Laggregated and Lpoints , we adopt the same loss formulation with the same ground-truth labels. Dense supervision * provides feedback for all points, enabling models to learn local features more efficiently. . To compute Laggregated + Lpoints * * * 3.3 Training Smoothness Regularization for Dense Control. As illustrated in point-wise relative predictions, we predict the direction and the magnitude of the delta position component apos separately. Thus, the position loss is accordingly decomposed into Ldir and Lmag, i.e. Lpos = Ldir + Lmag. We predict , which poses an underdetermined problem and allows to output the direction by spuriously large values. To mitigate this issue, we incorporate smoothness regularization loss Lreg = 1 2. This loss enforces the consistency between 2 pi + p(fi) and pj + p(fj) for any two points and j. i,j (pi + p(fi)) (pj + p(fj))2 pi+p(fi) pi+p(fi)2 (cid:80) Overview of Losses. In keyframe control, following SGR [10], position is represented by continuous 3D vector: apos R3. For rotations, the ground-truth action is represented as one-hot vector per rotation axis with rotation bins: arot R(360/R)3 (R = 5 degrees in our implementation). Open and collide actions are binary one-hot vectors: aopen R2, acollide R2. In this way, our loss objective is as follows: Lkeyframe = α1Lpos + α2Lrot + α3Lopen + α4Lcollide, (1) where Lpos is L1 loss, and Lrot, Lopen, and Lcollide are cross-entropy losses. In dense control, following ManiSkill2 [13] and robosuite [60], both delta position and delta rotation (in the form of axis-angle coordinates) are represented by continuous 3D vectors: apos R3, arot R3, while open actions are still binary one-hot vectors: aopen R2. Our loss objective is: Ldense = β1(Ldir + Lmag) + β2Lrot + β3Lopen + β4Lreg, (2) where Ldir, Lmag and Lrot are MSE losses, Lopen is cross-entropy loss, and Lreg is smoothness regularization loss. For more training details, we refer to Appendix B.3. 2Motivated by FrameMiner [44], in dense control, we transform the point coordinates into the end-effector frame, positioning the end-effector at the origin of coordinates for easier computation and performance benefit. 5 Avg. Avg. Open Open Water Toilet Phone Put Take Out Open Open Slide Sweep To Meat Off Method R3M PointNeXt PerAct SGR RVT SGRv2 (ours) Method R3M PointNeXt PerAct SGR RVT SGRv2 (ours) Success Rank Microwave Door Plants Seat Up On Base Books Umbrella Fridge Drawer Block Dustpan 4.7 25.3 22.3 23.6 40.4 53.2 Turn Tap 26.1 48.7 8.0 34.4 38.4 87.9 5.8 3.4 4.1 4.1 2.2 1.2 Put In Drawer 0.0 17.1 0.1 8.3 19.6 75.9 0.9 7.1 4.3 6.4 18.3 27.2 Close Jar 0.0 36.0 0.5 13.3 25.2 25.6 2.9 36.4 5.6 60.9 28.5 59.6 24.9 55.3 34.8 71.2 38.0 76.8 Drag Stack Stick Blocks 0.3 18.5 10.3 64.4 45.7 94.9 0.0 1.9 1.7 0.0 8.8 17. 3.2 9.2 3.1 7.1 24.0 13.2 Sort 0.0 21.7 56.4 31.9 75.1 81.3 Push 0.5 57.5 25.1 29.3 46.5 63.7 Place 0.0 15.5 46.4 49.9 0.0 69.3 47.2 30.7 62.3 47.6 89.6 84.1 Screw Put In Bulb 0.0 4.1 4.4 0.9 24.0 24.1 5.2 37.5 75.9 36.3 85.3 74.5 Put In Safe Wine Cupboard Shape Buttons 0.0 0.3 3.3 12.1 0.4 0.9 0.1 16.9 5.6 30.7 20.3 55.6 6.8 22.0 83.1 12.0 90.4 93. 0.4 31.5 8.7 24.7 92.7 53.1 0.0 0.4 0.4 0.1 1.6 1.9 24.0 59.5 47.5 72.0 85.1 100.0 Insert Peg 0.0 0.1 1.9 0.1 4.0 4.1 0.4 42.0 2.8 43.6 19.6 61.5 Stack Cups 0.0 4.4 0.1 0.0 3.1 21.3 Grill 0.1 59.9 85.9 52.7 90.5 96.5 Place Cups 0.0 0.4 0.7 1.1 1.2 1.6 Table 1: Performance on RLBench with 5 Demonstrations. All numbers represent percentage success rates averaged over 3 seeds. See Appendix for standard deviation. SGRv2 outperforms the most competitive baseline RVT on 23/26 tasks, with an average improvement of 1.32. 4 Experiments Our experiments are designed to answer the following questions: (1) How does SGRv2 perform when locality is incorporated into designs, especially in data-limited scenarios, compared to various 2D and 3D representations? (2) Can SGRv2 consistently demonstrate advantages across different control modes? (3) What are the contributions of the key components of SGRv2s locality design to its overall performance? (4) How does SGRv2 perform in real-robot tasks, and does it possess the ability to generalize in the real world? 4.1 Simulation Setup Environment and Tasks. The simulation experiments are conducted on 3 robot learning benchmarks: RLBench [12], ManiSkill2 [13], and MimicGen [14]. RLBench is large-scale benchmark designed for vision-guided manipulation. Following previous works [33, 16, 17] we use keyframe control on RLBench. ManiSkill2 is comprehensive benchmark for manipulation skills, enhancing diversity with object-level variations. MimicGen generates large-scale robot learning datasets from human demonstrations. On ManiSkill2 and MimicGen, following prior works [61, 14], we use dense control. On RLBench, we use 4 RGB-D cameras positioned at the front, left shoulder, right shoulder, and wrist of Franka Emika Panda, while on ManiSkill2 and MimicGen, we use frontview and wrist-view RGB-D camera. We use 26 RLBench tasks with 5 demonstrations per task, and 4 ManiSkill2 tasks and 7 MimicGen tasks with 50 demonstrations per task (except for PickSingleYCB, where 50 demonstrations per object are used). On MimicGen, we use D1 initial distribution, which presents broader and more challenging range. See Appendix for more details. Evaluation. The evaluation approach is designed to minimize variance in our results. On RLBench, we train an agent for 20,000 iterations and save checkpoints every 800 iterations, while in ManiSkill2 and MimicGen, we train for 100,000 iterations and save checkpoints every 4,000 iterations. Then we evaluate the last 5 checkpoints for 50 episodes and get the average success rates 3. Finally, we conduct the experiments with 3 seeds and report the average results. Baselines. We compare SGRv2 against the following baselines: (1) R3M [9] is 2D visual representation designed for robotic manipulation, which is pre-trained on large-scale human video datasets. For fair comparisons, we utilize frozen R3M to process RGB images, employ separate 2D CNN to process depth images, and subsequently fuse the two resulting features. (2) PointNeXt [15] is an enhanced version of the classic PointNet++ architecture for point cloud processing. We employ the encoder of PointNeXt to obtain the 3D representation. (3) PerAct [16] is 3D representation that voxelizes the workspace and utilizes Perceiver Transformer [62] to process voxelized observations. (4) SGR [10] is representation that integrates both high-level 2D semantic understanding and low-level 3D spatial reasoning. (5) RVT [17] is 3D representation that utilizes multi-view transformer to predict actions and integrates these predictions into 3D space through back-projection from multiple viewpoints. 3MimicGen [14] reports maximum results across different checkpoints, while we provide average results, offering more robust and realistic measure of model performance. 6 Avg. Success Avg. Rank Method PointNeXt SGR SGRv2 (ours) Method PointNeXt SGR SGRv2 (ours) 16.8 14.9 55.8 Avg. Success Avg. Rank 13.6 14.2 26. 2.9 2.0 1.0 2.5 2.5 1.0 Stack 56.1 50.8 81.2 LiftCube 50.8 26.9 80.5 StackThree 3.7 5.6 37.9 Square 0.9 1.3 2.8 PickCube 4.7 12.2 72.9 Threading 3.6 4.0 6. StackCube 10.6 3.5 27.7 PickSingleYCB 1.1 17.0 42.2 Coffee HammerCleanup MugCleanup 12.0 14.1 27.9 11.7 14.1 16.1 7.1 9.7 9.7 Table 2: Performance on ManiSkill2 (top) and MimicGen (bottom) with 50 Demonstrations. We report success rates averaged over 3 seeds. See Appendix for standard deviation. We observe that SGRv2 consistently outperforms baselines like SGR and PointNeXt. 4.2 Simulation Results RLBench Performance. Figure 1 depicts the performance of SGR and SGRv2 across differing number of demonstrations. Initially, as the availability of data decreases, we note significant decline in SGRs performance compared to SGRv2, highlighting the difficulties in maintaining model performances without the inductive bias towards locality awareness. Additionally, Table 1 provides comparison of success rates obtained with only 5 demonstrations using various representations. We observe that the absence of 3D geometric information, as demonstrated by R3M using 2D representation, results in markedly low performance, highlighting the critical role of 3D priors in robotic manipulation under data-constrained scenarios. Finally, SGRv2 is demonstrated to be the superior representation, achieving 53.2% average success rate with merely 5 demonstrations and significantly outperforming the most competitive baseline, RVT, with an average improvement factor of 1.32 and achieving enhanced performance in 23 out of 26 tasks. The results underscore the advantages of incorporating 3D geometry and well-designed locality to enhance sample efficiency. ManiSkill2 and MimicGen Performance. To evaluate the performance of SGRv2 in dense control scenarios, we conduct comparisons with several baselines on ManiSkill2 and MimicGen. Our experimental results, summarized in Table 2, demonstrate that SGRv2 significantly outperforms the baselines. In particular, thanks to our tailored approach for dense control, SGRv2 exhibits superior performance in tasks where the objects location consistently aligns with the direction of the delta actions, such as Pick Cube and Stack Three. These findings confirm that SGRv2 acts as sample-efficient, universal representation adept at handling both keyframe and dense control scenarios. See Appendix for results on different numbers of demonstrations and additional baselines. Method Avg. success SGRv2 SGRv2 w/o decoder SGRv2 w/ absolute pos prediction SGRv2 w/ uniform point weight SGRv2 w/o dense supervision Ablations. As shown in Table 3, we conduct ablations to assess the locality design choices of SGRv2. (1) Decoder architecture is the cornerstone of our locality designs. Omitting the decoder from the SGRv2 would prevent the application of other locality designs, forcing us to rely solely on the global representation from the encoder. This would lead to significant decrease in performance. (2) Predicting absolute positions, results in markedly poorer performance. This underscores that relative position predictions are the key insight of the locality design. (3) Substituting point-wise weights with uniform weights reduces performance, confirming the role of point-wise weighting in focusing on predictive local regions. (4) Eliminating dense supervision leads to decline in overall performance, illustrating that dense supervision enhances the models learning efficacy. Table 3: Ablations. Average success rate of 26 RLBench tasks with 5 demonstrations after ablating key components of locality design. 53.2 21.3 21.0 44.2 40.1 Emergent Capabilities. In our study, we visualize the point-wise weights of SGRv2 detailed in Section 3.2. As depicted in Figure 3, we sequentially visualize point clouds with RGB and pointwise weights. Surprisingly, the results consistently demonstrate an alignment between points with higher weights (in red) and the object affordances, which denote the functional areas of objects. This observation highlights the capability of SGRv2 to precisely identify and emphasize critical local regions on objects. Motivated by this, we conduct experiments on visual distractors in Appendix E. 7 Figure 3: Emergent Capabilities. We visualize the point-specific weights and find that the points with high weights (in red) align with the objects affordances. Left: toilet seat up. Right: open microwave. Task Sub-task PerAct RVT SGRv2 Tidy Up the Table Make Coffee Avg. Success Rate Put trash in trash can Put socks in box Put marker in pen holder Open drawer Put lollipop in drawer Close drawer Turn on coffee machine Put funnel onto carafe Pour powder into funnel Pour water 50 60 10 20 10 40 100 0 0 10 30 50 80 10 40 10 100 20 10 30 41 80 90 30 60 30 80 100 80 10 70 63 Figure 4: Left: Real-robot long-horizon tasks. Right: Success rate (%) of multi-task agents on real-robot tasks. We collect 8 demonstrations and evaluate 10 episodes for each task. 4.3 Real-Robot Results To evaluate the effectiveness of SGRv2 in real-robot setting, we conduct experiments using the Franka Emika Panda across two long-horizon tasks (a total of 10 sub-tasks), and generalization task. Refer to Appendix for more details on robot setup, task designs and failure cases discussions. Long-horizon Tasks. We collect 8 demonstrations per task and train multi-task agents. Each subtask is tested across 10 episodes. We present comparative results between PerAct, RVT, and SGRv2 in Figure 4, where SGRv2 demonstrates substantial performance advantage. common issue with PerAct is that it tends to bias towards occupied voxels, frequently causing collisions. For RVT, typical failures stem from large position errors along the cameras viewing direction, potentially caused from the inaccuracies in the virtual images rendered orthogonal to the real cameras perspective. Generalization Task. We assess the generalization capability of SGRv2 by employing 6 cups of different colors. Each scene involves one target cup and two distractor cups of different colors. We collect 5 demonstrations for each of the 4 colors and test the model on both 4 seen and 2 unseen scenarios. Each color is tested across 10 episodes. As indicated in the right table, compared with PerAct and RVT, SGRv2 exhibits the ability to generalize across color variations. The results show SGRv2 without the semantic branch achieves zero performance on unseen colors, underlining the critical role of semantic awareness in enhancing generalization. 100 PerAct RVT 100 SGRv2 w/o sem. 100 100 SGRv2 Seen Unseen 10 5 0 70 Method"
        },
        {
            "title": "5 Discussion",
            "content": "Limitations. Our models are currently trained using vanilla BC objective. promising direction involves integrating the Diffusion Policy [2], which excels in dealing with multimodal and heterogeneous trajectories, with our locality framework to further enhance performance in real world. Additionally, due to our focus on sample efficiency, the evaluations on generalization are currently insufficient. We are eager to expand this work to include broader range of generalization aspects, such as object shapes, camera positions, and background textures. Conclusion. We present SGRv2, systematic framework of visuomotor policy that considers both visual and action representations. Built upon the SGR, SGRv2 integrates action locality across its entire framework. Through comprehensive evaluations across diverse range of tasks in multiple simulated and real-world environments with limited data availability, SGRv2 exhibits superior performance and outstanding sample efficiency. 8 Acknowledgments We thank Haoxu Huang and Fanqi Lin for their assistance in conducting real-robot experiments. This work is supported by the Ministry of Science and Technology of the Peoples Republic of China, the 2030 Innovation Megaprojects Program on New Generation Artificial Intelligence (Grant No. 2021AAA0150000). This work is also supported by the National Key R&D Program of China (2022ZD0161700)."
        },
        {
            "title": "References",
            "content": "[1] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. survey of robot learning from demonstration. Robotics and autonomous systems, 57(5):469483, 2009. [2] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023. [3] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. [4] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541 551, 1989. [5] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. [6] J. L. Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990. [7] A. Graves and A. Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 3745, 2012. [8] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022. [9] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. [10] T. Zhang, Y. Hu, H. Cui, H. Zhao, and Y. Gao. universal semantic-geometric representation for robotic manipulation. In Conference on Robot Learning, pages 33423363. PMLR, 2023. [11] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu. 3d diffusion policy. arXiv preprint arXiv:2403.03954, 2024. [12] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):30193026, 2020. [13] J. Gu, F. Xiang, X. Li, Z. Ling, X. Liu, T. Mu, Y. Tang, S. Tao, X. Wei, Y. Yao, et al. Maniskill2: unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659, 2023. [14] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu, and D. Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596, 2023. [15] G. Qian, Y. Li, H. Peng, J. Mai, H. A. A. K. Hammoud, M. Elhoseiny, and B. Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. arXiv preprint arXiv:2206.04670, 2022. 9 [16] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: multi-task transformer for robotic manipulation. arXiv preprint arXiv:2209.05451, 2022. [17] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694710. PMLR, 2023. [18] M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 56395650. PMLR, 2020. [19] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with augmented data. Advances in neural information processing systems, 33:1988419895, 2020. [20] I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. [21] C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning In International Conference on continuous latent space models for representation learning. Machine Learning, pages 21702179. PMLR, 2019. [22] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. [23] R. Jonschkowski and O. Brock. Learning state representations with robotic priors. Autonomous Robots, 39:407428, 2015. [24] A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant representations for reinforcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020. [25] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot learning with masked visual pre-training. arXiv preprint arXiv:2210.03109, 2022. [26] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023. [27] Y. Hu, R. Wang, L. E. Li, and Y. Gao. For pre-trained vision models in motor control, not all policy learning methods are created equal. arXiv preprint arXiv:2304.04591, 2023. [28] R. Shah and V. Kumar. Rrl: Resnet as representation for reinforcement learning. arXiv preprint arXiv:2107.03380, 2021. [29] C. Wen, X. Lin, J. So, K. Chen, Q. Dou, Y. Gao, and P. Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. [30] Z. Yuan, Z. Xue, B. Yuan, X. Wang, Y. Wu, Y. Gao, and H. Xu. Pre-trained image encoder for generalizable visual reinforcement learning. arXiv preprint arXiv:2212.08860, 2022. [31] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. [32] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of pre-trained vision models for control. In International Conference on Machine Learning, pages 1735917371. PMLR, 2022. [33] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1373913748, 2022. 10 [34] Y. Ze, G. Yan, Y.-H. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, and X. Wang. Gnfactor: Multi-task real robot learning with generalizable neural feature fields. In Conference on Robot Learning, pages 284301. PMLR, 2023. [35] H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained vision-language models. arXiv preprint arXiv:2210.13431, 2022. [36] P.-L. Guhur, S. Chen, R. Garcia, M. Tapaswi, I. Laptev, and C. Schmid. Instruction-driven history-aware policies for robotic manipulations. arXiv preprint arXiv:2209.04899, 2022. [37] Y. Seo, J. Kim, S. James, K. Lee, J. Shin, and P. Abbeel. Multi-view masked world models for visual robotic manipulation. arXiv preprint arXiv:2302.02408, 2023. [38] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. [39] H.-S. Fang, C. Wang, M. Gou, and C. Lu. Graspnet-1billion: large-scale benchmark for general object grasping. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1144411453, 2020. [40] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 1343813444. IEEE, 2021. [41] W. Huang, I. Mordatch, P. Abbeel, and D. Pathak. Generalization in dexterous manipulation via geometry-aware multi-task learning. arXiv preprint arXiv:2111.03062, 2021. [42] Y.-H. Wu, J. Wang, and X. Wang. Learning generalizable dexterous manipulation from human grasp affordance. arXiv preprint arXiv:2204.02320, 2022. [43] Y. Qin, B. Huang, Z.-H. Yin, H. Su, and X. Wang. Dexpoint: Generalizable point cloud reinforcement learning for sim-to-real dexterous manipulation. arXiv preprint arXiv:2211.09423, 2022. [44] M. Liu, X. Li, Z. Ling, Y. Li, and H. Su. Frame mining: free lunch for learning robotic manipulation from 3d point clouds. arXiv preprint arXiv:2210.07442, 2022. [45] T. Gervet, Z. Xian, N. Gkanatsios, and K. Fragkiadaki. Act3d: 3d feature field transformers for multi-task robotic manipulation. In 7th Annual Conference on Robot Learning, 2023. [46] S. Chen, R. Garcia, I. Laptev, and C. Schmid. Sugar: Pre-training 3d visual representations for robotics. arXiv preprint arXiv:2404.01491, 2024. [47] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. arXiv preprint arXiv:2402.10885, 2024. [48] Z. Xue, Z. Yuan, J. Wang, X. Wang, Y. Gao, and H. Xu. Useek: Unsupervised se (3)- In 2023 IEEE International Conequivariant 3d keypoints for generalizable manipulation. ference on Robotics and Automation (ICRA), pages 17151722. IEEE, 2023. [49] H. Ryu, H.-i. Lee, J.-H. Lee, and J. Choi. Equivariant descriptor fields: Se (3)-equivariant energy-based models for end-to-end visual robotic manipulation learning. arXiv preprint arXiv:2206.08321, 2022. [50] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitzmann. Neural descriptor fields: Se (3)-equivariant object representations for manipulation. In 2022 International Conference on Robotics and Automation (ICRA), pages 63946400. IEEE, 2022. [51] J. Yang, C. Deng, J. Wu, R. Antonova, L. Guibas, and J. Bohg. Equivact: Sim (3)-equivariant visuomotor policies beyond rigid object manipulation. arXiv preprint arXiv:2310.16050, 2023. [52] E. Chun, Y. Du, A. Simeonov, T. Lozano-Perez, and L. Kaelbling. Local neural descriptor fields: Locally conditioned object representations for manipulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 18301836. IEEE, 2023. [53] C. Gao, Z. Xue, S. Deng, T. Liang, S. Yang, L. Shao, and H. Xu. Riemann: Near realtime se (3)-equivariant robot manipulation without point cloud segmentation. arXiv preprint arXiv:2403.19460, 2024. [54] F. Fuchs, D. Worrall, V. Fischer, and M. Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:1970 1981, 2020. [55] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages 726747. PMLR, 2021. [56] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652660, 2017. [57] A. Simeonov, Y. Du, Y.-C. Lin, A. R. Garcia, L. P. Kaelbling, T. Lozano-Perez, and P. Agrawal. In Conference on Se (3)-equivariant relational rearrangement with neural descriptor fields. Robot Learning, pages 835846. PMLR, 2023. [58] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [59] L. X. Shi, A. Sharma, T. Z. Zhao, and C. Finn. Waypoint-based imitation learning for robotic manipulation. In Conference on Robot Learning, pages 21952209. PMLR, 2023. [60] Y. Zhu, J. Wong, A. Mandlekar, R. Martın-Martın, A. Joshi, S. Nasiriany, and Y. Zhu. robosuite: modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020. [61] N. Hansen, H. Su, and X. Wang. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828, 2023. [62] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021. [63] B. Calli, A. Singh, A. Walsman, S. Srinivasa, P. Abbeel, and A. M. Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pages 510517. IEEE, 2015. [64] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021."
        },
        {
            "title": "A Simulation Task Details",
            "content": "Figure 5: Simulation Tasks. Our simulation experiments encompass 26 tasks (1-26) from RLBench, 4 tasks (27-37, where 30-37 are 8 different YCB [63] objects of task Pick Single YCB) from ManiSkill2, and 7 tasks (38-44) from MimicGen. Our simulation experiments are conducted on 3 robot learning benchmarks: RLBench [12], ManiSkill2 [13], and MimicGen [14]. See Figure 5 for an overview of the simulation tasks. In these 13 simulations, all cameras have resolution of 128 128. In the following, we will provide detailed examination of tasks from the three benchmarks. A.1 RLBench Tasks We utilize 26 RLBench tasks, including 8 tasks used in SGR [10] and 18 tasks used in PerAct [16] and RVT [17]. For tasks with multiple variations, we use the first variation. In RLBench, we use 5 demonstrations per task, unless specified otherwise. Given that SGR and PerAct provide detailed descriptions of these RLBench tasks, we omit these details here for simplicity. A.2 ManiSkill2 Tasks We utilize 4 ManiSkill2 tasks, each described in detail as follows. (1) Lift Cube: Pick up red cube and lift it to specified height. (2) Pick Cube: Pick up red cube and move it to target position. (3) Stack Cube: Pick up red cube and place it onto green cube. (4) Pick Single YCB: Pick up YCB [63] object and move it to the target position. In our experiments, we use 8 YCB objects (excluding those that are too difficult to pick up): 002 master chef can, 004 sugar box, 005 tomato soup can, 006 mustard bottle, 007 tuna fish can, 008 pudding box, 010 potted meat can, 011 banana. For the first three tasks, we utilize 50 demonstrations per task, while for the last one (Pick Single YCB), we employ 50 demonstrations per YCB object. A.3 MimicGen Tasks We utilize 7 MimicGen tasks with 50 demonstrations per task, all employing the initial distribution D1, which presents broader and more challenging range. The details are as follows: (1) Stack: Stack red block on green one. (2) Stack Three: Similar to Stack, but with an additional step of stacking blue block on the red one. (3) Square: Pick up square nut and place it on peg. (4) Threading: Pick up needle and thread it through hole in tripod. (5) Coffee: Pick up coffee pod, insert it into the coffee machine, and close the machine hinge. (6) Hammer Cleanup: Open drawer, pick up hammer, place it back into the drawer, and close the drawer. (7) Mug Cleanup: Similar to Hammer Cleanup, but with mug. SGRv2 Details B.1 Architecture Details Input Data. The SGRv2 model takes as input RGB images {Ik}K k=1 of size and corresponding depth images of the same size from multiple camera views. Point clouds are generated from these depth images using known camera extrinsics and intrinsics. crucial aspect is the alignment of the RGB images with the point clouds, ensuring precise one-to-one correspondence between elements in the two data forms. For keyframe control, the point cloud is represented in the robots base frame. In contrast, for dense controlinspired by FrameMiner [44]the point cloud is transformed into the end-effector frame to simplify computation and enhance performance. For keyframe control, the model additionally receives proprioceptive data z, which includes four scalar values: gripper open state, left finger joint position, right finger joint position, and action sequence timestep. In dense control, proprioceptive data is not utilized. Additionally, following SGR [10], if task comes with language instruction S, this also forms part of the models input. Other Details. Following SGR [10], we use CLIP-ResNet-50 as the image encoder for the semantic branch. For the 3D encoder-decoder, we employ PointNeXt-XL. The output from the encoderi RC for the i-th point, where the feature dimension decoder is point-wise feature, denoted as raw is 64. We apply linear layer followed by ReLU activation to produce processed point-wise feature fi, increasing the feature dimension to 256. We then predict the relative position p(fi), magnitude m(fi) (for dense control), rotation r(fi), gripper open state o(fi), and collision indicator c(fi) (for keyframe control) of the i-th point, where p, m, r, o, are 3-layer MLPs. Note that when 14 representing the ground-truth actions as one-hot vectorssuch as rotation, gripper open state, and collision indicators in keyframe controlthe action predictions correspond to the output probabilities following the softmax layer. Finally, for each action component, we assign learned weight w(fi) to each point, where represents separate 3-layer MLPs with softmax normalization across the points dimension. B.2 SGR Details SGRv2 is built upon SGR [10], which we briefly introduced in Section 3.1. Here, we provide detailed description of SGRs three components: semantic branch, geometric branch, and fusion network. Semantic Branch. Using collection of RGB images {Ik}K k=1 from calibrated cameras, they initially apply frozen pre-trained 2D model G, such as CLIPs visual encoder, to extract multi-view image features {G(Ik)}K k=1. When language instruction accompanies task, they utilize pretrained language model H, like CLIPs language encoder, to generate the language features H(S). They align these image features G(Ik) with the language features H(S) using visual grounding module, producing {Mk}K k=1. Subsequently, they rescale the visual or aligned feature maps to the dimensions of the original images through bilinear interpolation and reduce their channels by 1 1 convolution, generating set of features {Fk}K k=1, where each Fk RHW C1. These high-level semantic features are then back-projected into 3D space to form point-wise features for the point cloud, expressed as Fsem RN C1 , where = . Geometric Branch. They construct the initial point cloud coordinates = {pi}N i=1 RN 3 and RGB features Fc RN 3 using multi-view RGB-D images and camera parameters (i.e., camera intrinsics and extrinsics). Optionally, they append D-dimensional vector, derived from robot proprioceptive data via linear layer, to each point feature. They then process the point cloud coordinates and features Fc through hierarchical PointNeXt encoder, extracting compact geometric RM C2 (M < ). coordinates RM 3 and features Fusion Network. To merge the two complementary branches, they first subsample the point-wise semantic features Fsem using the same point subsampling procedure as in the geometric branch, sem RM C1. They then perform channel-wise concatenation of the semantic and resulting in c) RM (C1+C2). Finally, the fused features sem, geometric features to form Ffuse = Concat(F are processed through several set abstraction blocks [38, 15], enabling cohesive modeling of the cross-modal interaction between 2D semantics and 3D geometric information. B.3 Training Details Losses. As illustrated in Section 3.3, in keyframe control, our loss objective is as follows: Lkeyframe = α1Lpos + α2Lrot + α3Lopen + α4Lcollide, (3) where Lpos is L1 loss, and Lrot, Lopen, and Lcollide are cross-entropy losses. In our experiments, we set α1 = 300 and α2 = α3 = α4 = 1. In dense control, our loss objective is: Ldense = β1(Ldir + Lmag) + β2Lrot + β3Lopen + β4Lreg, (4) where Ldir, Lmag and Lrot are MSE losses, Lopen is cross-entropy loss, and Lreg is smoothness regularization loss. In our experiments, we set β1 = 10, β2 = β3 = 1 and β4 = 0.3. Data Augmentation. (1) Translation and rotation perturbations: in keyframe control, the training samples is augmented with 0.125 translation perturbations and 45 yaw rotation perturbations. (2) Color drop is to randomly replace colors with zero values. This technique serves as powerful augmentation for PointNeXt [15], leading to significant enhancements in the performance of tasks where color information is available. (3) Feature drop: Color drop randomly replaces colors with zero values, which results in both the RGB and semantic features becoming constant. However, there are certain tasks where colors play crucial role, and disregarding color information in these tasks would make them unsolvable. To address this issue, we propose feature drop. Specifically, this involves randomly replacing the semantic features with zero values, while keeping the RGB values unchanged. (4) Point resampling is widely used technique in point cloud data processing that adjusts the density of the point cloud. It involves selecting subset of points from the original dataset to create new dataset with modified density. Firstly, we filter out points outside the workspace. Then in keyframe control, we resample 4096 points from the point cloud using farthest point sampling (FPS), while in dense control, we resample 1200 points using the same method. (5) Demo augmentation [33] [16], used in keyframe control, captures transitions from intermediate points along trajectory to keyframe states, rather than from the initial state to the keyframe state. This approach significantly increases the volume of the training data. Hyperparameters. The configuration of hyperparameters applied in our studies are shown in Table 4. For each task, the experiments are conducted on single NVIDIA GeForce RTX 3090 GPU. Table 4: Hyper-parameters used in our simulation experiments. Keyframe Control Dense Control Config Training iterations Leraning rate Batch size Optimizer Lr Scheduler Warmup step Weight decay Color drop Feature drop Number of input points 20, 000 0.003 16 AdamW Cosine 200 1 106 0.4 0 4096 100, 000 0.0003 16 AdamW Cosine 0 1 106 0 0.4 B.4 Training and Inference Speed We test the training and inference speed of SGRv2 on NVIDIA 3090 GPU. For keyframe control, training takes approximately 5 hours for 20k steps, with an inference speed of 10 FPS. For dense control, training requires around 7 hours for 100k steps, with an inference speed of 30 FPS. Real-Robot Details C.1 Real-Robot Setup For our real-robot experiments, we use Franka Emika Panda manipulator equipped with parallel gripper. We utilize keyframe control, and the motion planning is executed through MoveIt 4. Perception is achieved through an Intel RealSense L515 camera, positioned in front of the scene. The camera generates RGB-D images with resolution of 1280720. We leverage the realsense-ros5 to align depth images with color images. The extrinsic calibration between the camera frame and robot base frame is carried out using the MoveIt calibration package. When preprocessing the RGB-D images, we resize the 1280 720 images to 256 256 using nearest-neighbor interpolation. We choose this interpolation method instead of others, like bilinear interpolation, because the latter can introduce artifacts into the depth map, resulting in noisy point cloud. Following these steps enables us to process RGB-D images as we do in our simulation experiments. It is essential to adjust the cameras intrinsic parameters appropriately after resizing the images. We train SGRv2 for 40,000 training steps and use the final checkpoint for evaluation. 16 Figure 6: Real-robot generalization task. C.2 Real-Robot Tasks Our real-robot experiments involve three tasks: Tidy Up the Table, Make Coffee, and Move Color Cup to Target. The first two are long-horizon tasks, while the last is generalization task. We provide details of the task design as follows. Tidy Up the Table (as shown in Figure 4 Top Left) is to place the clutter on the table in its appropriate locations. The task consists of 6 sub-tasks, each detailed as follows: (1) Put trash in trash can: Pick up the trash and place it in the trash can. (2) Put socks in box: Pick up the socks and place them in the box. (3) Put marker in pen holder: Pick up the marker and place it in the pen holder. (4) Open drawer: Grasp the drawer handle and pull it open. (5) Put lollipop in drawer: Pick up the lollipop and place it into the drawer. (6) Close drawer: Push the drawer closed. Make Coffee (as shown in Figure 4 Bottom Left) is to make pour-over coffee. This task is composed of 4 sub-tasks, each described in detail as follows: (1) Turn on coffee machine: Press the button on the coffee machine to activate it. (2) Put funnel onto carafe: Pick up the funnel and place it onto the carafe. (3) Pour powder into funnel: Pick up the powder holder and pour the powder into the funnel. (4) Pour water: Pick up the kettle and pour the water onto the powder in the funnel. Move Color Cup to Target (as shown in Figure 6) is to select the target color cup from three cups and move it to the white area. The target color is indicated through language instructions. We have 6 cups of different colors: white, red, yellow, orange, black, and green. Each scenario involves one target cup and two distractor cups of different colors. We collect five demonstrations for each of the first 4 colors and test the model on both 4 seen and 2 unseen scenarios. C.3 Discussions of Failure Cases In the real-robot experiments, the primary failure cases of SGRv2 include: (1) For smaller objects like lollipops, the model sometimes struggles to detect them, especially when they are farther away from the camera. (2) For tasks that are sensitive to grasping position or angle, such as grasping markers or funnels, slight deviations can lead to unsuccessful attempts or cause the object to slip. (3) In keyframe control, where few sparse target poses are output and then executed through motion planner, failures can occur due to collisions during the motion planning phase."
        },
        {
            "title": "D Additional Results",
            "content": "Our primary objective of Section 4 is to demonstrate the sample efficiency of SGRv2. This is why we initially choose to evaluate our method using only 5% of the data employed in the baselines original settings. However, to make our experiments more complete, we conduct additional experiments to provide more comprehensive evaluation on more different numbers of demonstrations and more baselines. Note that all our results are the average success rates of the last 5 checkpoints. 4https://moveit.ros.org 5https://github.com/IntelRealSense/realsense-ros 17 Method PointNeXt PerAct SGR RVT SGRv2 (ours) Method PointNeXt PerAct SGR RVT SGRv2 (ours) Method PointNeXt PerAct SGR RVT SGRv2 (ours) Avg. Phone Water Plants 14.8 12.0 24.4 11.2 17.6 Open Open Success Microwave Door 61.6 13.6 78.0 9.2 76.8 46.0 79.2 19.2 68.4 86.0 Slide Block 83.6 97.3 89.2 71.2 94.4 Put In Safe 7.2 7.9 27.6 67.2 59.2 33.1 36.7 47.8 52.1 63.3 Open Drawer 63.6 89.8 75.6 70.0 92.8 Screw Bulb 21.6 18.2 17.6 43.2 68.4 Toilet Put Take Out Open Seat Up On Base Books Umbrella Fridge 16.4 48.0 57.2 14.4 18.8 0.0 26.4 92.0 82.8 18.4 63.6 78.4 19.2 85.6 69.2 Stack Put In Close Blocks Drawer 8.8 1.6 43.4 4.4 0.0 22.8 18.8 84.4 52.0 80.8 Place Push Cups 0.0 1.2 0.8 0.4 0. 64.4 83.6 59.6 62.0 69.2 Turn Tap 84.8 5.5 94.8 73.6 95.2 Sort Cupboard Shape Buttons 100.0 2.8 82.0 2.2 84.8 2.8 100.0 6.4 99.2 6.4 83.6 91.6 90.0 97.2 95.6 Drag Stick 0.0 75.3 80.8 100.0 94.8 Stack Cups 6.0 7.7 6.0 22.8 70.4 Jar 35.6 23.2 36.4 35.2 32.4 Insert Peg 1.2 8.9 2.0 12.8 8.0 Sweep To Meat Off Dustpan 52.4 32.9 63.2 18.0 64.4 Place Wine 13.6 39.7 35.6 92.0 68.0 Grill 0.0 98.2 93.6 92.0 97.6 Put In 18.0 7.9 12.4 17.6 50. Table 5: Performance on RLBench with 100 demonstrations. #Demonstrations RVT SGRv2 (ours) 100 52.1 63.3 46.3 62.4 20 43.3 61.9 10 42.3 56.0 40.4 53.2 Table 6: Average performance of 26 RLBench tasks with varying number of demonstrations. RLBench with 100 Demonstrations. We test SGRv2 and the baseline methods (RVT, SGR, PerAct, and PointNeXt) using 100 demonstrations. Refer to Table 5 for the results. RVT with Varying Demonstrations. We test RVT (the most competitive baseline in RLBench) on 26 RLBench tasks, with demonstration numbers ranging from 100 to 5. Table 6 shows the average results of 26 tasks compared with results of SGRv2. MimicGen with 1000 Demonstrations. We evaluate SGRv2 against SGR, PointNeXt, 2D BCRNN (used in MimicGen [14] and robomimic [64]), and 2D BC (used in robomimic [64]). The latter two baselines are trained and tested using the official codes of MimicGen 6 and robomimic 7, but with different evaluation metrics-we report the average results of the last 5 checkpoints instead of the maximum of 30 checkpoints, as explained in Section 4.1. Table 7 shows the results. More Baselines in MimicGen with 50 Demonstrations. In addition to the baselines included in the Table 2, we evaluated SGRv2 against the 2D BC-RNN (used in MimicGen [14] and robomimic [64]), 2D BC (used in robomimic [64]), and R3M [9] baselines with 50 demonstrations. Table 8 shows the results. MimicGen with 200 Demonstrations. We further compared SGRv2 against 2D BC-RNN (used in MimicGen [14] and robomimic [64]) and 2D BC (used in robomimic [64]) using 200 demonstrations. Table 9 shows the results. The results from these extended experiments show that SGRv2 consistently outperforms the baselines across all settings. While its noteworthy that BC-RNN achieves comparable performance to SGRv2 when trained on 1000 demonstrations, it falls short when the number of demonstrations is reduced to 50 or 200. This highlights the superior sample efficiency of SGRv2. Additionally, we 6https://github.com/NVlabs/mimicgen 7https://github.com/ARISE-Initiative/robomimic 18 Method SGR PointNeXt 2D BC 2D BC-RNN SGRv2 (ours) Avg. Success 42.1 42.3 32.3 63.2 66. Stack 84.4 90.4 84.4 96.0 96.4 StackThree 54.0 72.4 54.8 74.4 84.2 Square 26.4 12.4 35.6 56.8 56.4 Threading 11.6 12.8 13.2 34.8 56.0 Coffee HammerCleanup MugCleanup 41.6 36.4 6.8 82.8 86.0 38.4 38.0 4.8 51.6 38. 38.4 33.6 26.8 46.0 46.2 Table 7: Performance on MimicGen with 1000 demonstrations. Method R3M 2D BC 2D BC-RNN SGRv2 (ours) Avg. Success 5.3 10.6 10.0 26.0 Stack 34.5 31.2 30.0 81.2 StackThree 0.3 3.6 3.2 37. Square 0.0 0.4 0.0 2.8 Threading 0.5 4.4 0.8 6.7 Coffee HammerCleanup MugCleanup 1.2 22.8 24.0 27.9 0.3 8.0 4.0 16.1 0.0 3.6 8.0 9. Table 8: Performance of additional baselines on MimicGen with 50 demonstrations. recognize that SGRv2 and methods like BC-RNN, which model temporal dependencies, are complementary. Integrating temporal dependencies into SGRv2 presents promising avenue for future research."
        },
        {
            "title": "E Robustness to Visual Distractors",
            "content": "As illustrated in Section 4.2, the predicted per-point weights of SGRv2 effectively focus on locations that align well with object affordances. This raises the question: does this emergent capability make the model robust to distractors in the scene that it has never seen before? In Section 4.3, we present experiments involving distractor cups of different colors; nonetheless, it remains to be seen how SGRv2 performs in the presence of completely unseen objects and whether it will disregard them. To address this question, we conduct additional experiments where we randomly introduce taskirrelevant objects (such as YCB [63] objects, basketballs, etc.) as visual distractors into the RLBench environments for 3 tasks (meat off grill, phone on base, and push buttons). Refer to Figure 7 for visualization. We then test the SGRv2 and RVT models, which were previously trained on data without distractors, in these modified environments. The results, as shown in Table 10, indicate that SGRv2 is more robust to these distractors compared to RVT. This suggests that SGRv2 can effectively focus on relevant areas even in the presence of unseen objects, demonstrating robustness to visual distractors. Figure 7: RLBench tasks with visual distractors."
        },
        {
            "title": "F Detailed Results",
            "content": "For our simulation experiments using the SGRv2 on RLBench with 5 demonstrations (mentioned in Table 1) and on ManiSkill2 and MimicGen with 50 demonstrations (mentioned in Table 2), we employed 3 random seeds to ensure the reliability of our results. In the main body of the paper, we present averaged results for clarity. Here we include both the mean and standard deviation derived from our simulation results. The results for RLBench are shown in Table 11, and the results for ManiSkill2 and MimicGen are presented in Table 12. We also report the ablations mentioned in Table 3 for each task in Table 13. 19 Method 2D BC 2D BC-RNN SGRv2 (ours) Avg. Success 21.9 41.1 55.8 Stack 62.8 84.0 95. StackThree 23.6 51.6 80.4 Square 14.8 15.2 32.4 Threading 14.4 16.8 42.2 Coffee HammerCleanup MugCleanup 4.8 69.6 74.4 24.8 22.4 38. 8.4 28.4 28.2 Table 9: Performance on MimicGen with 200 demonstrations."
        },
        {
            "title": "Push Buttons",
            "content": "RVT on env w/o distractors RVT on env w/ distractors SGRv2 on env w/o distractors SGRv2 on env w/ distractors 90.5 65.0 96.5 92.4 62.3 2.5 84.1 80.4 90.4 67.5 93.2 81.7 Table 10: Performance evaluation in environments with and without visual distractors. Put Avg. Open Success Microwave Take Out Books Umbrella 5.2 8.7 0.5 0.9 Toilet Phone Seat Up On Base 0.0 0.0 15.5 2.1 Method R3M PointNeXt PerAct SGR RVT SGRv2 (ours) Water Open Open Plants Fridge Door 3.2 1.4 2.9 3.7 36.4 3.7 5.6 4.6 49.9 14.7 46.4 4.9 57.5 8.2 37.5 2.3 9.2 4.0 60.9 5.2 59.6 16.0 28.5 3.1 69.3 11.1 0.0 0.0 25.1 4.4 75.9 7.0 3.1 1.3 55.3 3.7 24.9 8.2 30.7 9.2 47.2 1.4 29.3 5.2 36.3 6.4 7.1 1.5 71.2 2.8 34.8 3.3 47.6 6.7 62.3 1.4 46.5 10.9 85.3 4.5 24.0 4.2 76.8 8.0 38.0 1.7 89.6 2.8 84.1 4.5 63.7 11.8 74.5 5.5 13.2 3.4 Put In Sweep To Meat Off Drawer Dustpan 0.0 0.0 0.4 0.4 4.7 25.3 22.3 23.6 40.4 53.2 Stack Open Blocks Drawer Method 0.0 0.0 0.0 0.0 R3M 21.7 20.4 59.5 22.1 42.0 34.7 59.9 17.8 48.7 13.4 17.1 27.8 36.0 4.6 18.5 32.1 1.9 1.6 PointNeXt 10.3 6.4 1.7 0.6 8.0 7.5 2.8 0.4 56.4 18.0 47.5 24.3 0.5 0.6 PerAct 31.9 6.2 72.0 27.1 43.6 8.4 52.7 5.1 34.4 7.4 13.3 5.6 64.4 11.4 0.0 0.0 SGR RVT 85.1 2.2 19.6 17.4 90.5 2.2 38.4 5.4 19.6 5.5 25.2 3.6 45.7 10.9 8.8 4.2 75.1 2.6 SGRv2 (ours) 81.3 3.1 100.0 0.0 61.5 7.2 96.5 3.9 87.9 6.9 75.9 3.6 25.6 2.2 94.9 0.6 17.5 3.0 0.9 0.6 7.1 6.3 4.3 7.0 6.4 2.2 18.3 1.8 27.2 2.0 Slide Block 24.0 8.8 Turn Tap 26.1 7.2 Drag Stick 0.3 0.5 Close Jar 0.0 0. Grill 0.1 0.2 0.1 0.2 8.3 9.2 85.9 6.9 Screw Bulb Method 0.0 0.0 R3M 4.1 1.5 PointNeXt 4.4 5.2 PerAct 0.9 0.8 SGR RVT 24.0 3.8 SGRv2 (ours) 24.1 0.6 Put In Safe 0.3 0.2 12.1 4.2 0.9 0.9 16.9 2.2 30.7 4.9 55.6 8.0 Put In Place Cupboard Wine 0.0 0.0 0.4 0.4 3.3 0.6 31.5 4.5 0.4 0.0 8.7 1.7 0.1 0.2 24.7 5.8 92.7 0.6 5.6 2.1 53.1 7.4 20.3 9. Place Stack Insert Push Sort Cups Cups Peg Buttons Shape 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.8 3.7 0.4 0.4 4.4 3.8 0.4 0.4 22.0 38.1 0.1 0.2 0.7 0.6 0.1 0.2 1.9 1.2 83.1 5.3 0.4 0.4 1.1 0.9 0.0 0.0 0.1 0.2 12.0 1.4 0.1 0.2 4.0 0.0 90.4 2.9 1.6 0.7 1.2 0.7 3.1 0.6 4.1 1.4 21.3 11.8 1.6 0.7 93.2 5.3 1.9 0.6 Table 11: RLBench results (%) on 5 demonstrations with mean and standard deviation. Method PointNeXt SGR SGRv2 (ours) Method PointNeXt SGR SGRv2 (ours) Avg. Success Avg. Rank LiftCube 50.8 15.2 26.9 4.0 80.5 7.3 16.8 14.9 55.8 Avg. Success Avg. Rank 13.6 14.2 26. 2.9 2.0 1.0 StackThree 3.7 1.4 5.6 1.7 37.9 1.5 Square 0.9 0.5 1.3 0.5 2.8 0.7 2.5 2.5 1.0 Stack 56.1 6.4 50.8 7.7 81.2 4.4 PickCube 4.7 0.4 12.2 3.1 72.9 4.1 Threading 3.6 2.2 4.0 0.8 6.7 2.0 StackCube PickSingleYCB 10.6 4.3 3.5 2.2 27.7 4. 1.1 0.1 17.0 0.2 42.2 2.3 Coffee HammerCleanup MugCleanup 12.0 5.2 14.1 2.0 27.9 7.0 11.7 2.8 14.1 1.7 16.1 3.9 7.1 0.9 9.7 2.4 9.7 2.7 Table 12: ManiSkill2 and MimicGen results (%) on 50 demonstrations with mean and standard deviation. 20 Method SGRv2 SGRv2 w/o decoder SGRv2 w/ absolute pos prediction SGRv2 w/ uniform point weight SGRv2 w/o dense supervision Method SGRv2 SGRv2 w/o decoder SGRv2 w/ absolute pos prediction SGRv2 w/ uniform point weight SGRv2 w/o dense supervision Method SGRv2 SGRv2 w/o decoder SGRv2 w/ absolute pos prediction SGRv2 w/ uniform point weight SGRv2 w/o dense supervision Avg. Phone Water Plants 38.0 12.4 21.2 28.8 6.8 Open Open Success Microwave Door 76.8 27.2 68.4 4.4 57.6 8.0 82.4 21.6 6.4 59.2 Slide Block 100.0 78.8 99.2 100.0 92.8 Put In Safe 55.6 16.4 1.6 32.8 30.4 53.2 21.3 21.0 44.2 40.1 Open Drawer 81.3 14.0 20.4 92.8 75.2 Screw Bulb 24.1 3.2 0.4 1.6 0.0 Toilet Put Take Out Open Seat Up On Base Books Umbrella Fridge 13.2 63.7 84.1 6.8 27.2 32.8 4.4 21.2 14.8 14.0 68.0 60.4 6.0 78.4 60.8 Stack Put In Close Blocks Drawer 17.5 75.9 0.0 4.0 1.2 4.0 1.6 74.0 43.2 42.0 Place Push Cups 1.6 1.6 0.0 0.8 2.4 89.6 38.8 17.2 32.0 54.4 Turn Tap 87.9 46.8 68.8 67.6 84.0 Sort Cupboard Shape Buttons 1.9 0.0 0.0 0.8 0.0 74.5 35.6 44.4 44.0 68.0 Drag Stick 94.9 0.8 54.8 97.6 59.6 Stack Cups 21.3 0.8 0.0 5.2 0. Jar 25.6 18.8 6.4 43.6 28.8 Insert Peg 4.1 0.4 1.6 0.0 0.0 Sweep To Meat Off Dustpan 61.5 15.6 50.8 72.8 19.2 Place Wine 53.1 27.6 3.2 50.4 52.0 Grill 96.5 40.8 10.8 90.8 72.0 Put In 93.2 56.8 35.2 64.4 100.0 20.3 0.0 0.0 1.2 0.4 Table 13: Ablations results (%) for SGRv2 on RLBench with metrics for each task."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Qi Zhi Institute",
        "Tsinghua University"
    ]
}