{
    "paper_title": "It Takes Two: Your GRPO Is Secretly DPO",
    "authors": [
        "Yihong Wu",
        "Liheng Ma",
        "Lei Ding",
        "Muzhi Li",
        "Xinyu Wang",
        "Kejia Chen",
        "Zhan Su",
        "Zhanguang Zhang",
        "Chenyang Huang",
        "Yingxue Zhang",
        "Mark Coates",
        "Jian-Yun Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 7 9 0 0 . 0 1 5 2 : r IT TAKES TWO: YOUR GRPO IS SECRETLY DPO Yihong Wu 1, Liheng Ma2,3, Lei Ding4, Muzhi Li5, Xinyu Wang2, Kejia Chen6, Zhan Su1, Zhanguang Zhang7, Chenyang Huang7,8,9, Yingxue Zhang7, Mark Coates2,3, Jian-Yun Nie1 1Universite de Montreal 2McGill University 3Mila - Quebec AI Institute 4University of Manitoba 5The Chinese University of Hong Kong 6Zhejiang University 7Huawei Noahs Ark Lab 8University of Alberta 9Alberta Machine Intelligence Institute (Amii)"
        },
        {
            "title": "ABSTRACT",
            "content": "Group Relative Policy Optimization (GRPO) is prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as form of contrastive learning, which reveals fundamental connection to Direct Preference Optimization (DPO). Motivated by DPOs empirical success, we investigate the minimal two-rollout case (2-GRPO)a configuration previously deemed infeasible. We provide rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) is now central paradigm for post-training Large Language Models (LLMs), aligning preference through RL with Human Feedback (RLHF) (Ouyang et al., 2022) and incentivizing reasoning capability through RL with Verifiable Rewards (RLVR) (Shao et al., 2024; Guo et al., 2025). Among recent advances, Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025), has emerged as powerful variant of Proximal Policy Optimization (PPO) (Schulman et al., 2017). Unlike PPO, which relies on value networks to stabilize rewards, GRPO samples multiple responses (rollouts) per prompt and normalizes their rewards within each group. This simple and effective strategy achieves state-of-the-art performance on various tasks while reducing significant computational resources. Despite GRPOs strong empirical performance, its theory remains largely unexplored. In this work, we revisit GRPO through the lens of contrastive learning (Wang & Isola, 2020; Chen et al., 2020; He et al., 2020; Wu et al., 2024). From this viewpoint, the GRPO objective naturally resembles contrastive loss: its intra-group normalization implicitly divides responses into positive and negative samples, encouraging positive responses while suppressing negative ones. This perspective reveals key conceptual link between GRPO and Direct Preference Optimization (DPO) (Rafailov et al., 2023), prominent alignment algorithm in RLHF. Both approaches optimize policies based on preference signals, though under different settings. Building on this connection, we introduce 2-GRPO, DPO-inspired variant of GRPO with response group size of two. Despite its simplicity, 2-GRPO preserves unbiased gradient estimation while offering greater efficiency. Conventional viewpoint attributes GRPOs empirical success to its stable group normalization, which relies on large group sizes for accurate statistical estimation. However, generating many rollouts per prompt leads to substantial computational and time costs. Our proposed 2-GRPO algorithm tackles this inefficiency head-on by reducing the group size to 2. At first glance, this design might violate the principle of GRPO, yet our theoretical analysis and experiments reveal the opposite. Specifically, we show that: (i) 2-GRPO preserves an implicit form of advantage estimation; (ii) the potential increase in gradient variance can be mitigated by larger batch size; and (iii) 2GRPO does not have less positive signals compared to its large-group counterpart. Empirically, Equal contribution. yihong.wu@umontreal.ca, liheng.ma@mail.mcgill.ca. 1 2-GRPO achieves performance on par with standard GRPO while reducing computational overhead and training time significantly. Our findings challenge the prevailing assumption that large group sizes are essential for the performance of GRPO. By demonstrating that 2-GRPO is competitive and substantially more efficient alternative, we offer new direction for designing resource-efficient RL algorithms for LLM posttraining. Our main contributions are: Contrastive Reinterpretation of GRPO. We formalize GRPO as contrastive objective distinguishing positive from negative rollouts via group-normalized advantages. This reframing clarifies its conceptual connection to preference-based methods like DPO. Theoretical Guarantees for the Pairwise Setting. In the context of RLVR, we prove that pairwise grouping is sufficient. Our analysis shows that 2-GRPO not only preserves the contrastive optimization behavior of standard GRPO but also provides unbiased gradient estimates, dispelling the notion that large groups are necessary for stable learning. Empirical Validation. Across multiple language models and reasoning datasets, we show that 2-GRPO matches the performance of standard GRPO while significantly reducing training time and computational resource usage. The rest of the paper is organized as follows. We begin with brief review of RL for LLM posttraining and summarize commonly used algorithms. Next, we present theoretical analysis that connects GRPO and DPO through the lens of contrastive learning via gradient analysis. We then analyze the properties of 2-GRPO in depth, demonstrating that it yields unbiased gradients and preserves the key characteristics of standard GRPO despite its reduced group size. Finally, we validate our approach through extensive experiments across diverse datasets and model scales."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Our work focuses on RL-based post-training of pre-trained LLMs to improve their reasoning capabilities, with particular emphasis on settings where responses can be automatically verified as correct or incorrect, i.e., the RLVR setting. Let πθ denote the policy network, i.e., the LLM parameterized by θ. Given an input prompt q, the model generates response oi = (oi,1, . . . , oi,T ), where oi,t is the token generated at step [0, ] and oi,<t denotes the sequence of preceding tokens. We let be the set of prompts, each consisting of question and any necessary instructions1. trajectory τ is defined as pair consisting of prompt and its corresponding LLMgenerated response sequence o, i.e., τ = (q, o). In RL-based post-training, the reward function is typically defined at the trajectory level, i.e., : R. The learning objective is to maximize the expected reward over the space of trajectories: (θ) = EqQEoπθ(q)[r(τ )] . (1) Vanilla Policy Gradient (VPG) (Williams, 1992): VPG (a.k.a. REINFORCE) aims to maximize the reward with gradient ascent: θJ (θ) = EqQEoiπθ(q) ri oi (cid:88) t= θπθ(oi,toi,<t, q) . (2) where ri is the reward of (q, oi). Proximity Policy Optimization (PPO) (Schulman et al., 2017): VPG might suffer from high variance and instability (Schulman et al., 2015). To reduce the variance and instability, PPO intro1Throughout this paper, we use the terms prompt and question interchangeably. 2 duces importance sampling, clipping, and value function for computing advantage: JPPO(θ) = qQ oiπθold"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 min (cid:20) πθ(oi,toi,<t, q) πθold(oi,toi,<t, q) Ai,t, clip (cid:18) πθ(oi,toi,<t, q) πθold(oi,toi,<t, q) (cid:19) (cid:21) , 1 ϵ, 1 + ϵ Ai,t , (3) where πθold is the policy which generates the sequences, πθ is the policy to update, ϵ is hyperparameter for clipping, and Ai,t is the advantage, which is computed from ri by subtracting value baseline. Here, the baseline is provided by value function, which is usually parameterized as another LLM in LLM post-training. Direct Preference Optimization (DPO) (Rafailov et al., 2023): DPO is proposed for RLHF, which is usually trained with offline human-annotated preference data (q, o+, o) DDPO. The loss function of DPO is LDPO = E(q,o+,o)DDPO (cid:20) (cid:18) log σ β log πθ(o+q) πref(o+q) β log πθ(oq) πref(oq) (cid:19)(cid:21) , (4) where o+ and denote preferred (positive) and dispreferred (negative) response, respectively; β is parameter controlling the deviation from the base reference policy πref. Group Relative Policy Optimization (GRPO) (Shao et al., 2024): GRPO the RL algorithm behind the success of DeepSeek-R1 (Guo et al., 2025) has become one of the most widely used RL algorithms for LLM post-training. Instead of maintaining value network like PPO, GRPO generates group of trajectories for each prompt (usually referred to as rollouts), and normalizes the corresponding rewards within each group to compute the advantages: JGRPO(θ) = 1 (cid:88) i= 1 oi oi (cid:88) t=1 min (cid:20) πθ(oi,toi,<t, q) πθold (oi,toi,<t, q) Ai,t, clip (cid:18) πθ(oi,toi,<t, q) πθold (oi,toi,<t, q) (cid:19) (cid:21) , 1 ϵ, 1 + ϵ Ai,t , qQ oiπθold (5) (6) where 2, oi denotes the i-th trajectory, and Ai,t denotes the corresponding advantage. The token-level advantage is given by the intra-group normalization: Ai,t = ri mean(r) std(r) + ϵ , where ri is the reward of the rollout, and ϵ is small constant added to avoid division by zero. In the degenerate cases, where all generated trajectories receive identical rewards (all correct or all incorrect), we have Ai,t = 0 for all i, t, leading to zero gradient for the parameter update. In practice, the choice of is typically relatively large, e.g., 16, in order to perform proper normalization."
        },
        {
            "title": "3 BRIDGING GRPO AND DPO WITH CONTRASTIVE LEARNING",
            "content": "In this section, we bridge GRPO and DPO from the perspective of contrastive learning via gradient analysis. This perspective not only clarifies the underlying mechanism of GRPO but also motivates deeper investigation into how group structures in GRPO can be more effectively designed. The key insight is that advantage values are inherently signed quantities: they are either positive or negative. This observation naturally leads to contrastive interpretation: trajectories with positive advantages can be viewed as positive examples, while those with negative advantages correspond to negative examples. This mirrors the core principle of contrastive learning, which seeks to increase the likelihood of positive samples (given an anchor) while decreasing that of negative ones. Although various contrastive loss functions and settings existranging from 1-vs-1 (one positive sample and one negative sample) (Rendle et al., 2009) and 1-vs-n (Oord et al., 2018) to n-vs-n (Frosst et al., 2019)we aim to unify them under general framework. To this end, we define general form of contrastive loss inspired by the analysis of Tao et al. (2022): 3 Definition 3.1 (General contrastive loss). Let πθ be probabilistic model, an arbitrary data distribution, be an anchor, and D+( x) and D( x) be the positive and negative distributions conditioned on x. We call y+ D+ the positive sample and D the negative sample w.r.t. x. We say differentiable loss function is contrastive if its gradient has the following form: θL = Ex,y+,y (cid:2)a(x, y+, D)θπθ(y+x) b(x, y, D+)θπθ(yx)(cid:3) , (7) where a, are arbitrary coefficient functions that weight positive and negative contributions. practice we only have access to empirical gradients: given groups {y+ }) and ˆb(x, y, {y+ coefficients ˆa(x, y+, {y In } we use empirical }) in place of a, b. } and {y Let be prompt sample from and {oi}G i=1 be group of trajectories drawn i.i.d. from the policy πθ( q). Let > 1 denote the group size of GRPO (i.e., the number of rollouts/trajectories generated per prompt). Given the prompt and the policy πθ, we let G+ denote the numbers of correct and incorrect trajectories, respectively, in the sampled trajectories. ˆpθ,q = G+ /G is the proportion of correct trajectories in the sampled trajectories, which approximates the probability of correct pθ,q given the policy πθ,q on the prompt q. In the following discussion, we drop the subscript θ for simplicity. and In the analysis, we can assume that clipping is not triggered, since the gradient will be zero outside the clipping range. Then we have the following equation for the GRPO objective function: JGRPO(θ, G) = (cid:113) qQ oπθ(q) (cid:100)VarG(q) 1 G+ G+ (cid:88) j=1 1 oj oj (cid:88) t=1 πθ(oj,toj,<t, q) 1 G (cid:88) k=1 1 ok ok (cid:88) t=1 πθ(ok,tok,<t, q) , (8) where (cid:100)VarG(q) = (1 ˆpq)ˆpq, is the empirical standard deviation of group of samples from Bernoulli(pq), which is the distribution of rewards in the verifiable setting. Regarding the group of sampled trajectories, the empirical objective Eq. (8) is approximating the true objective: JGRPO(θ) = qQ (cid:112)Var(q) oj π+ θ (q) 1 oj oj (cid:88) t= πθ(oj,toj,<t, q) okπ θ (q) 1 ok ok (cid:88) t=1 πθ(ok,tok,<t, q) . (9) where Var(q) = (1 pq)pq is the variance of the Bernoulli(pq). For simplicity, we use π+ θ (q) and π θ (q) to denote the corresponding positive and negative subdistribution, respectively. The detailed derivation is provided in Appendix A.1. In the theoretical analysis that follows, we center our attention on the true objective Eq. (9), since the empirical version Eq. (8) merely serves as an approximation derived from finite samples. For each prompt q, the GRPO objective Eq. (9) can be interpreted as an intra-group contrastive loss: it increases the likelihood of positive trajectories while suppressing the likelihood of negative ones. Importantly, each prompt is weighted by the standard deviation of the reward distribution, Bernoulli(pq), which quantifies the uncertainty of the policy πθ under that prompt. As result, Eq. (9) naturally emphasizes prompts where the policy exhibits higher uncertainty. This observation leads to the following proposition: the GRPO objective is, in essence, form of contrastive loss. Proposition 3.2. The GRPO objective is contrastive loss. Proof of Proposition 3.2. Eq. (9) has the following derivatives: (cid:32) (cid:33) θJGRPO = qQ (cid:112)Var(q) oj π+ θ (q) θπGRPO θ (ojq) okπ θ (q) θπGRPO θ (okq) , (10) where we denote πGRPO t=1 πθ(oi,toi,<t, q) (see Appendix A.2 for further discussion). Let = = (cid:112)Var(q). Under this choice, the expectation in Eq. (10) can be factored out (oiq) = 1 oi θ (cid:80)oi 4 of the parentheses, aligning the expression with the form of Eq. (7). Therefore, by Definition 3.1, the GRPO objective satisfies the definition of contrastive loss, which completes the proof. Proposition 3.3. The DPO objective is contrastive loss. Proof of Proposition 3.3. The DPO objective has the following derivatives: (cid:2)σ (cid:0)θ log πθ(o+q) θ log πθ(oq)(cid:1)(cid:3) θLDPO = βE(q,o+,o)DDPO σ πref(o+q) σ πref(oq) = βEq,o+,o θπθ(o+q) θπθ(oq) (cid:18) (cid:19) (11) (12) , where ˆrθ = β(x, y) log πθ(yx) πref(o+q) and = πref(oq) . Eq. (11) aligns with the form of Eq. (7), which indicates the DPO objective is contrastive loss. πref(yx) and σ = σ(ˆrθ(q, o) ˆrθ(q, o+)). Let = βσ βσ According to Proposition 3.2 and Proposition 3.3, both the GRPO and DPO objectives can be interpreted as contrastive losses."
        },
        {
            "title": "4 RETHINKING GROUP SIZE IN GRPO FROM DPO: WHY 2 IS ENOUGH",
            "content": "Building on our contrastive interpretation of GRPO, we are naturally led to 2-GRPO. At first glance, using only two rollouts per prompt may seem insufficient, since one might intuitively expect poor reward normalization and less positive signals. In this section, we analyze it from multiple perspectives and show that large groups are not strictly necessary for effective learning. In standard GRPO, each training step generates trajectories per prompt. reward function partitions these trajectories into positive and negative groups, which are then used to compute gradients for policy updates. Crucially, the generation phase is the dominant computational bottleneck, accounting for up to 70% of total training time (Liu et al., 2025). Reducing the group size G, therefore, offers direct path to higher training throughput via more frequent updates. Our gradient analysis in Sec. 3 shows that GRPO estimates expectations over positive and negative trajectories, which closely mirrors the formulation of DPO, where only single positivenegative pair is used. This connection raises natural question: if DPO succeeds with just one pair, could GRPO perform well with minimal group size? To push the limit, we introduce 2-GRPO, i.e., GRPO with group size = 2, which has unbiased gradient estimation and is substantially more efficient: J2-GRPO = EqEo+Eo 1 (cid:0)πGRPO θ (o+q) πGRPO θ (oq)(cid:1) . (13) This expression is obtained by replacing (cid:112)Var(q) in Eq. (9) with the constant 1/2. 4.1 ADVANTAGE ESTIMATE The first concern lies in the spurious lack of advantage to stabilize rewards. In 2-GRPO, the advantage computation is straightforward: A+ = 1, = 1 for positive-negative pair and A+ = = 0 otherwise. It seems 2-GRPO simply shifts the reward from 0/1 to 1/1 and lacks any normalization effect. However, the following proposition exposes that 2-GRPO does implicitly perform the normalization. Proposition 4.1. Given constant (0, 1) and small positive constant ϵ, we consider two scenarios below: Case 1: Consider X1, , X2N (cid:80)2N (cid:113) ˆσ = 1 2N i=1 (Xi ˆµ)2. Then, it follows that i.i.d. Bernoulli(p). Let Yi = Xiˆµ ˆσ+ϵ , where ˆµ = 1 2N lim ϵ lim E[YiXi = x] = (cid:112)p(1 p) . 5 (cid:80)2N i=1 Xi and (14) Case 2: Consider pairs of (Xi,1, Xi,2) with each Xi,j where ˆµi = 1 2 (Xi,1 + Xi,2) and ˆσi = (cid:80) j=1(Xi,j ˆµi)2. Then, it follows that (cid:113) 1 2 i.i.d. Bernoulli(p). Let Yi,j = Xi,j ˆµi ˆσi+ϵ , E[Yi,jXi,j = x] = p. lim ϵ (15) The limϵ0 E[Yi,jXi,j = x] differs from limϵ0 limN E[YiXi = x] by scaling factor 1 . p(1p) In Proposition 4.1, Case 1 corresponds to regular GRPO with sufficiently large group size; in this case E[YiXi = 1] and E[YiXi = 0] are, respectively, the advantage estimates of positive and negative trajectories given prompt. large will lead to smaller bias, and thus better estimation of the advantages. Case 2 corresponds to 2-GRPO, where E[Yi,jXi,j = 1] and E[Yi,jXi,j = 0] are unbiased advantage estimates. These advantage estimates differ from the ones of regular GRPO merely by scaling factor. This indicates that, even though the possible advantage values in 2-GRPO are only 1, 0, 1, the advantage estimates are still proportional to across training steps, suggesting the rationale behind the optimization. The proof is in Appendix A.3. 4.2 GRADIENT ESTIMATE second concern is that decreasing the group size increases gradient variance. We first provide formal definition of gradient variance, followed by lemma for empirical gradient estimation. Definition 4.2 (Gradient Variance). Let {xi}B i=1 be training batch of size B, where xi are sampled from the same distribution D, and let gi = θLθ(xi) denote the gradient of Lθ(xi) w.r.t. θ. Define the empirical batch gradient ˆg(ξB) = 1 i=1 gi, where ξB denote the randomness from sampling samples from the distribution and the expectation of gradient = ExiD[gi]. The variance of the gradient estimate over the batch is then defined as: (cid:80)B Var(ˆg) = Eξ(ˆg g)2. i=1, {xi}B2 Lemma 4.3. Let {xi}B1 i=1 be two training batches of batch size B1 and B2, respectively. Assume all data are i.i.d. sampled from the same distribution and the gradient of each data point has the same variance σg. Let ˆgB1, ˆgB2 denote the average of gradient of batch B1 and B2, respectively. If B1 < B2, then Var[ˆgB1] > Var[ˆgB1]. Proof of Lemma 4.3. Var(ˆgB) = Var (cid:32) 1 (cid:88) (cid:33) gi = 1 B2 (cid:32) (cid:88) (cid:33) Var(gi) = σ2 , (16) where the second equation is obtained by the fact that the covariance between i.i.d. data is zero. By the above equation, increasing decreases Var. At first glance, decreasing the group size in Eq. (10) seems to increase the variance of the gradient by Lemma 4.3. However, we have omitted the fact that the actual gradient calculation is obtained across different prompts. The actual calculation is described by the empirical GRPO objective: (cid:98)JGRPO(θ, G, Q) = 1 QG (cid:88) (cid:88) j= i=1 AijπGRPO θ (oijqj), (17) where is the number of prompts in the mini-batch, and the batch size of training is = QG rollouts. When we decrease G, we can increase to compensate. Since the total number of questions in the dataset is fixed, increasing will not affect the overall computational burden. Note that we are not arguing that we must pursue low variance or that high variance must necessarily lead to poor training outcomes. In fact, there are works showing that moderate variance can benefit the model generalization (Zhou et al., 2020). Therefore, the goal here is to use reduced group size to improve efficiency while controlling variance by adjusting Q."
        },
        {
            "title": "4.3 EXPLORATION ON HARD QUESTIONS",
            "content": "Another common concern with using small group size (e.g., = 2) is that it may perform poorly on difficult prompts, where multiple attempts are often needed to produce correct answer. The intuition is that smaller group provides fewer opportunities to sample correct response within single batch, potentially slowing down learning. However, under fixed computational budget where the dominant cost is rollout generation 2-GRPO and 16-GRPO explore approximately the same total number of rollouts across all training epochs. Consequently, the overall probability of sampling correct answer under = 2 is comparable to that under = 16. Proposition 4.4. Let pi [0, 1] denote the probability that single rollout under the policy πi produces correct answer. Then: 1. The probability of obtaining at least one correct answer in 2m independent rollouts with policy π0 is P2m = 1 (1 p0)2m. (18) 2. The probability of obtaining at least one correct answer when performing consecutive trials of 2 independent rollouts each, with the corresponding policy [π0, π1, , πm1] is Pm2 = 1 (cid:89) (1 pi)2 1 (1 p0)2m = P2m (19) when we have pi p0, > 0. i=0,m1 Note that the assumption pi p0, > 0 is prevailing, as we assume that the reasoning ability of LLM can be improved by RL post-training. Proposition 4.4 indicates that for hard questions, 2-GRPO will not breakdown compared to 16GRPO, given the same total rollouts traversed. It is worth mentioning that, due to its greater number of policy updates, 2-GRPO may have higher probability of getting correct output for difficult question and is more adaptive to capture more nuanced update requirements for different questions."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENT DETAILS Tasks and Training Framework Following prior studies, we consider mathematical tasks as representative instances of RLVR to verify our hypothesis, given their demonstrated transferability to broad range of other tasks (Yu et al., 2025). For training, we adopt the verl framework (Sheng et al., 2025) and utilize the built-in implementation of GRPO (Shao et al., 2024) as the baseline algorithm. Dataset and Baselines Following prior work (Chu et al., 2025), we employ Qwen-2.5-Math1.5B (Qwen-1.5B) and Qwen-2.5-Math-7B (Qwen-7B) (Yang et al., 2025) as base models. Both models are post-trained via RL on the MATH (Hendrycks et al., 2021b) and DAPO-Math-17k (Yu et al., 2025) datasets, and evaluated on MATH-500 (Hendrycks et al., 2021a), AMC23, Minerva Math (Lewkowycz et al., 2022), AIME-2025, and OlympiadBench (Huang et al., 2024). For DAPOMath-17k dataset, we randomly sample 7.5k questions from the original data to form subset for training in order to align with the size of MATH. In addition, we assess the proposed method on DeepSeek-R1-Distill-Qwen-1.5B (DS-1.5B) (DeepSeek-AI, 2025), which is post-trained on MATH. Owing to computational constraints, we do not extend its post-training to DAPO-Math-17k. All 1.5B models are trained on 4 GPUs. Qwen-7B is trained on 8 GPUs. We evaluate model performance using two metrics: Mean@32, the average accuracy across 32 i.i.d. samples, and Pass@32, which measures whether problem is solved in at least one of those 32 attempts. Hyper-parameters We mainly follow the default configuration of the verl framework. For sampling parameters in training generation, we set temperature to 1, top-p to 1 to encourage exploration, sequence length to 4096 for Qwen-series model and 8192 for DS-1.5B. For sampling parameters in 7 test generation, we set temperature to 0.7, top-p to 0.8, top-k to 20 and sequence length to 4096 for all models. For optimization, training employs the Adam optimizer (Kingma, 2014) with constant learning rate and linear warm-up over the first 10 steps. For GRPO hyper-parameters, All models are trained for 10 epochs. The baseline method, 16-GRPO, is trained with batch sizes of 32 (32 prompts and 16 rollouts per prompt) and learning rate 1 106. As discussed in Sec. 4.2, we trained 2-GRPO with larger batch size of 256 (256 prompts and 2 rollouts per prompt). Both case will have 512 rollouts in each mini-batch of training. Since we have fewer update steps due to the larger batch size, we adjust the learning rate of 2-GRPO to 8 106 based on the linear relationship of learning rate and batch size (Goyal et al., 2017). Goal of Experiment Building on the theoretical justification for 2-GRPO, we seek to empirically assess its validity in RLVR. We anticipated that 2-GRPO will exhibit better efficiencywith respect to computational resources and/or wall-clock timewhile maintaining the same performance as regular GRPO (16-GRPO)."
        },
        {
            "title": "5.2 MAIN EXPERIMENTS",
            "content": "Table 1: 2-GRPO v.s. 16-GRPO: post-trained on MATH/DAPO-Math-Sub and evaluated on five mathematical reasoning benchmarks. M/P@32 stands for Mean@32 and Pass@32. is the group size. denotes the difference 16 2. M/P@32 Time (h) MATH-500 AMC 2023 Minerva Math AIME 2025 Olympiad Bench Qwen-1.5B Qwen-7B DS-1.5B Qwen-1.5B Qwen-7B w/o 2 w/o 2 16 w/o 2 16 w/o 2 w/o 2 16 Post-training on MATH dataset - 2.05 8.53 31.83 / 81.92 34.30 / 79.23 5.33 / 28. 3.64 / 22.31 15.40 / 37.16 69.28 / 87.43 49.53 / 81.76 16.25 / 33.26 9.48 / 32. 22.31 / 37.24 70.24 / 87.24 51.25 / 83.46 16.84 / 33.46 10.10 / 35.82 23.11 / 37. -75.96% -0.96 / +0.19 -1.71 / -1.70 -0.59 / -0.19 -0.62 / -2.94 -0.80 / -0. - 2.43 9.30 47.16 / 85.95 38.36 / 85.29 5.99 / 31. 5.00 / 25.17 9.83 / 34.30 75.23 / 89.77 64.60 / 81.53 23.13 / 38.45 12.81 / 38. 26.39 / 40.20 75.90 / 88.24 61.79 / 80.77 22.81 / 37.68 13.23 / 34.22 25.99 / 40. -73.87% -0.67 / +1.53 +2.81 / +0.76 +0.32 / +0.77 -0.42 / +4.63 +0.40 / 0. - 7.07 38.40 -81.6% - 2. 13.30 65.11 / 84.90 44.14 / 73.86 14.64 / 32.80 22.40 / 42.79 20.07 / 33. 74.36 / 88.85 56.95 / 88.63 21.28 / 38.34 24.89 / 46.79 33.69 / 45.86 75.98 / 89. 58.91 / 87.26 21.76 / 38.29 26.97 / 56.36 35.39 / 47.05 -1.62 / -0.31 -0.48 / -0.05 Post-training on DAPO-Math-Sub dataset -1.96 / +1.38 -2.08 / -9.56 -1.70 / -1.19 31.83 / 81.92 34.30 / 79.23 5.33 / 28. 3.64 / 22.31 15.40 / 37.16 68.81 / 87.36 52.19 / 85.77 16.79 / 33/61 8.13 / 29. 23.52 / 39.29 70.66 / 87.04 56.56 / 85.54 18.00 / 34.16 9.58 / 32.31 24.56 / 39. -84.06% -1.85 / +0.32 -4.37 / +0.23 -1.21 / +0.71 -2.50 / -2.98 -1.04 / +0. - 3.63 17.68 47.16 / 85.95 38.36 / 85.29 5.99 / 31. 5.00 / 25.17 9.83 / 34.30 77.43 / 90.51 64.84 / 91.59 21.95 / 38.05 14.58 / 33. 29.86 / 45.24 77.35 / 88.79 69.69 / 87.31 24.45 / 40.04 14.27 / 33.73 28.86 / 39. -79.47% +0.08 / 1.72 -4.85 / +4.28 -2.50 / -1.99 +0.31 / -0.70 +1.00 / +5. As shown in Table 1, 2-GRPO requires at least 70% less wall-clock time than 16-GRPO while achieving comparable performance. The models are post-trained on the MATH and DAPO-MathSub datasets and evaluated on five widely-used mathematical reasoning benchmarks, representing an out-of-distribution evaluation. This setting imposes stringent requirements on the generalization ability of the post-trained models. Notably, 2-GRPO is optimized with only 0.15 million generated rollouts just 12.5% of the 1.2 million rollouts utilized by 16-GRPO. 2 These results provide strong corroboration of our theoretical finding that reducing group size preserves performance while 2Appendix B.1 discusses the relationship between the total number of rollouts and computational cost. 8 substantially improving efficiency. To further support this statement, we conduct ablation study on various k-GRPO (k = 4, 8) in Appendix B.2."
        },
        {
            "title": "5.3 VISUALIZATION",
            "content": "In Sec. 5.2, we present empirical results comparing 2-GRPO and 16-GRPO. However, the out-ofdistribution evaluation setting may not fully reflect the post-training with 2-GRPO, as the distribution shift could obscure the underlying performance differences. Therefore, in this section, we visualize the reward and evaluation scores on the MATH dataset to demonstrate the in-distribution generalization of the post-trained models using 2-GRPO in comparison to 16-GRPO. 3 The figures presented in Fig. 1 and Fig. 2 illustrate the performance of Qwen-2.5-Math-1.5B and Qwen-2.5-Math-7B, respectively. As depicted, the reward and evaluation scores for 2-GRPO are comparable to those of 16-GRPO, indicating that the in-distribution generalization of the posttrained models using 2-GRPO is on par with that of 16-GRPO. (a) Curve of rewards. (b) Curve of eval-scores. Figure 1: Qwen-1.5B: Visualization of reward and evaluation scores on the MATH dataset. (a) Curve of rewards. (b) Curve of eval-scores. Figure 2: Qwen-7B: Visualization of reward and evaluation scores on the MATH dataset."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Stronger Efficiency There remains potential for further enhancements of 2-GRPO in efficiency. In 2-GRPO, many rollouts generated are ultimately assigned zero advantage, which actually do not demand the computation of gradients. Consequently, more advanced implementation could optimize these computations during the training phase. It is important to note that, as discussed in Sec. 4.1, these zero-advantage rollouts are still necessary for accurate advantage estimation. Therefore, we must simulate the contributions of these zero-advantage rollouts during the training phase rather than simply discarding them after the inference phase. 2-GRPO is Quantization of GRPO An alternative perspective on 2-GRPO is that it serves as quantization of standard GRPO, wherein the candidate values for advantages are discretized to 1, 0, 1. Nevertheless, due to the stochastic nature of neural network optimization, 2-GRPO is capable of approximating continuous advantage values effectively, provided that sufficiently large number of training steps are employed. 3The DAPO dataset does not provide test set. 9 Data Efficiency The quantized nature of 2-GRPO inherently results in the rejection of number of generated rollouts. While this characteristic enhances computational efficiency, it may concurrently compromise data efficiency numerous rollouts are discarded when the policy exhibits either exceptionally poor or exceptionally strong performance. This limitation in data efficiency could impede the ability of the policy post-trained by 2-GRPO to attain near-optimal performance. This observation motivates the design of adaptive adjustments to the group size, aiming to strike balance between computational and data efficiency, where we leave this direction to future exploration. Conclusion In this work, we present theoretical analysis of GRPO from contrastive learning perspective, establishing key conceptual connection between GRPO and DPO and offering new lens for understanding GRPO. Building on this insight, we propose 2-GRPO, DPO-inspired variant with group size of two, which achieves significant efficiency gains while maintaining comparable performance."
        },
        {
            "title": "REFERENCES",
            "content": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 15971607. PmLR, 2020. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations with the soft nearest neighbor loss. In International conference on machine learning, pp. 2012 2020. PMLR, 2019. Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on unsupervised visual representation learning. computer vision and pattern recognition, pp. 97299738, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. In Advances in Neural Information Processing Systems, 2024. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. In Advances in neural information processing systems, 2022. Liyuan Liu, Feng Yao, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Flashrl: 8bit rollouts, full power rl, August 2025. URL https://fengyao.notion.site/ flash-rl. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Lei Pang and Ruinan Jin. On the theory and practice of grpo: trajectory-corrected approach with fast convergence. arXiv preprint arXiv:2508.02833, 2025. 11 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 452461, 2009. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Chenxin Tao, Honghui Wang, Xizhou Zhu, Jiahua Dong, Shiji Song, Gao Huang, and Jifeng Dai. Exploring the equivalence of siamese self-supervised learning via unified gradient framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1443114440, 2022. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pp. 99299939. PMLR, 2020. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Yihong Wu, Le Zhang, Fengran Mo, Tianyu Zhu, Weizhi Ma, and Jian-Yun Nie. Unifying graph convolution and contrastive learning in collaborative filtering. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 34253436, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, January 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically understanding why sgd generalizes better than adam in deep learning. Advances in Neural Information Processing Systems, 33:2128521296, 2020."
        },
        {
            "title": "A THEOREMS",
            "content": "A.1 REVEAL GRPO AS CONTRASTIVE Details of Sec. 3. In the RLVR setting, rewards are binary, which leads to binary advantages given prompt. Let A+ denote the positive and negative advantage, respectively. From Eq. (6), we can have , A+ = 1 ˆpq (cid:112)ˆpq(1 ˆpq) = (cid:115) 1 ˆpq ˆpq , = 0 ˆpq (cid:112)ˆpq(1 ˆpq) (cid:115) = ˆpq 1 ˆpq . (20) In Eq. (5), the clipping function can be considered as applying an indicator function to the token, which does not affect trajectory-level behavior. The omission of the clipping function does not affect the analysis, as the out of range will lead to zero gradient. The key derivation is as follows: JGRPO(θ) = qQ oiπθold (q) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 πθ(oi,toi,<t, q) πθold(oi,toi,<t, q) Ai,t , 1 oi oi (cid:88) t=1 πθ(oi,toi,<t, q)Ai,t , G+ (cid:88) j=1 1 oj oj (cid:88) t=1 A+ πθ(oj,toj,<t, q) + (cid:88) k=1 1 ok ok (cid:88) t=1 πθ(ok,tok,<t, q) , A+ G+ 1 G+ G+ (cid:88) j=1 1 oj oj (cid:88) t=1 πθ(oj,toj,<t, q) + q 1 G (cid:88) k=1 1 ok ok (cid:88) t=1 πθ(ok,tok,<t, q) , (cid:113) (cid:100)VarG(q) 1 G+ G+ (cid:88) j=1 1 oj oj (cid:88) t= πθ(oj,toj,<t, q) 1 G (cid:88) k=1 1 ok ok (cid:88) t=1 πθ(ok,tok,<t, q) . (21) The first equation is obtained by omitting the clipping function. The second equation is obtained by the fact of important sampling that Eq[ p(x) q(x) (x)] = Ep[f (x)]. The third equation is obtained by dividing the trajectories into two groups: positive and negative. The fourth equation is obtained by the fact that all positive advantages are the same and that all negative advantages are the same. Since = (cid:112)(1 ˆp)ˆp, we obtain Eq. (8). When , A+ G+ ˆp ˆp = (cid:112)(1 ˆp)ˆp and G (cid:113) 1 ˆp = 13 (cid:88) i=1 = qQ oiπθ(q) 1 = qQ oj π+ okπ θ (q) θ (q) 1 = qQ oj π+ okπ θ (q) θ (q) = qQ oj π+ okπ θ (q) θ (q) we have the following facts: lim lim lim G+ = , = (cid:112)(1 ˆp)ˆp = (cid:112)(1 p)p , lim G+ lim G"
        },
        {
            "title": "1\nG−",
            "content": "G+ (cid:88) j=1 (cid:88) k=1 (oj) = oj O+ θ (oj) , (ok) = okO θ (ok) . (22) Based on the above facts, it is easy to derive Eq. (9). A.2 JUSTIFICATION OF PROPOSITION 3.2 Most of autoregressive LLMs adopt causal probability modelling that (cid:80) log πθ(oto<t, q) = log πθ(oq). Then we have the following equation to describe the gradient of trajectory probability and the gradient of token probabilities: θπθ(oq) = πθ(oq) (cid:88) 1 πθ(oto<t, q) πθ(oto<t, q) However, the original GRPO objective does not hold this property. Or one can consider GRPO using the mean-field assumption for probability modelling. Some papers believe that GRPO should be corrected by sequence level importance sampling (Zheng et al., 2025; Zhao et al., 2025; Pang & Jin, 2025). It is still an open question for the choice of important sampling for GRPO. To avoid overhead discussion, we keep the assumption implicit adopted by the original GRPO and denote πGRPO θ πθ(oto<t, q). = (cid:80) A.3 PROOF OF PROPOSITION 4. (cid:113) Proof. Case 1. Notice that ˆσ = k=1 Xk. Fix an index and condition on the event {Xi = x} with {0, 1}. In this case, by the strong law of large numbers and the continuous mapping theorem, we have ˆµ a.s. and ˆσ a.s. (cid:112)p(1 p). Thus, it follows that k=1(Xk ˆµ)2 = (cid:112)ˆµ(1 ˆµ) and ˆµ = 1 (cid:80)2N (cid:80)2N 1 2N 2N lim ϵ0 lim E[Yi Xi = x] = (cid:112)p(1 p) . Case 2. When Xi,1 = Xi,2, we have Xi,j = ˆµi and Yi,j = 0 for any {1, 2}. When Xi,1 = Xi,2, we have ˆµi = 0.5, ˆσi = 0.5, and Yi,j = 2Xi,j 1 1+2ϵ . By the law of total expectation, it follows that [Yi,j Xi,j = 1] = 1 1 + 2ϵ , [Yi,j Xi,j = 0] = 1 + 2ϵ . Thus, we have E[Yi,j Xi,j = x] = p. lim ϵ"
        },
        {
            "title": "B EXPERIMENTS",
            "content": "B.1 THE CONNECTION BETWEEN TRAINING ROLLOUTS AND COMPUTATIONAL COST In Sec. 5.2, the total number of rollouts generated and utilized during training is adopted as metric for comparing the computational cost of different methods. 14 The rationale for this choice is as follows. principled measure of computational cost in the context of RL post-training is the number of floating-point operations (FLOPs) performed. Unlike wallclock time, which is susceptible to variations arising from software implementation details (e.g., optimization of training libraries) and hardware characteristics (e.g., GPU/CPU architecture, I/O throughput), FLOPs provide more direct and stable measure of computational effort. For fixed base model and the same type of RL algorithm (GRPO in our case), the FLOPs required for single forward or backward pass with one input prompt can be considered constant, for both the generation and training phases. Accordingly, the total number of rollouts executed during training is directly proportional to the FLOPs executed, thereby serving as theoretically justified and consistent proxy for computational cost. B.2 ABLATION STUDY ON THE GROUP SIZE We conducted an ablation study on the effect of group size. In this experiment, the batch size was fixed at 32 and the learning rate at 1 106, following the configuration of the standard GRPO (16-GRPO). 4 Only the group size was varied in order to isolate and evaluate its impact. Table 2: Ablation study on group size G: post-trained on MATH and DAPO, respectively, and evaluated on five mathematical reasoning benchmarks. M/P@32 stands for Mean@32 and Pass@32. M/P@32 Time (h) MATH-500 AMC 2023 Minerva Math AIME 2025 Olympiad Bench Qwen-1.5B Qwen-7B Qwen-1.5B Qwen-7B w/o 2 8 16 w/o 2 4 16 w/o 2 4 8 w/o 2 4 8 16 Post-training on MATH dataset 31.83 / 81.92 34.30 / 79.23 5.33 / 28.91 3.64 / 22.31 15.40 / 37.16 67.73 / 87. 53.28 / 86.21 14.15 / 34.02 6.15 / 29.54 23.11 / 37.82 69.05 / 87.49 52.50 / 92. 15.29 / 33.57 8.33 / 27.13 23.08 / 38.99 69.34 / 86.05 51.64 / 83.96 14.60 / 32. 7.18 / 32.24 22.77 / 36.69 70.24 / 87.24 51.25 / 83.46 16.84 / 33.46 10.10 / 35. 22.30 / 38.33 47.16 / 85.95 38.36 / 85.29 5.99 / 31.10 5.00 / 25.17 9.83 / 34. 74.41 / 89.25 63.83 / 89.58 21.53 / 37.72 11.67 / 33.05 26.04 / 41.34 76.24 / 88. 63.51 / 84.97 23.09 / 41.03 10.83 / 32.42 26.25 / 40.78 75.12 / 89.53 64.38 / 88. 22.24 / 35.94 12.71 / 35.85 26.25 / 40.52 75.90 / 88.24 61.79 / 80.77 22.81 / 37. 13.23 / 34.22 25.99 / 40.11 Post-training on DAPO-Math-Sub dataset 31.83 / 81.92 34.30 / 79.23 5.33 / 28. 3.64 / 22.31 15.40 / 37.16 67.71 / 87.68 53.82 / 88.35 16.85 / 34.83 8.12 / 32. 23.21 / 39.26 69.14 / 87.78 54.69 / 86.88 17.53 / 35.74 8.43 / 36.18 23.30 / 39. 70.25 / 86.84 57.57 / 81.19 17.80 / 35.08 8.54 / 29.42 24.23 / 39.95 - 2.05 2.78 4.67 8.53 - 2. 3.48 5.48 9.30 - 3.63 4. 8.62 13.30 70.66 / 87.03 56.56 / 85.53 18.00 / 34.16 9.58 / 32. 24.55 / 39.19 - 3.43 5.39 9.18 47.16 / 85. 38.36 / 85.29 5.99 / 31.10 5.00 / 25.17 9.83 / 34.30 74.41 / 89.25 63.83 / 89. 21.53 / 37.72 11.67 / 33.05 26.04 / 41.34 76.24 / 88.16 63.51 / 84.97 23.09 / 41. 10.83 / 32.42 26.25 / 40.78 75.12 / 89.53 64.38 / 88.63 22.24 / 35.94 12.71 / 35. 26.25 / 40.52 17.68 75.90 / 88.24 61.79 / 80.77 22.81 / 37.68 13.23 / 34. 25.99 / 40.11 THE USE OF LARGE LANGUAGE MODELS (LLMS) We used LLMs to polish the writing. 4It is worth noting that the batch size and learning rate used for 2-GRPO in this ablation differ from those employed in the main experiment."
        }
    ],
    "affiliations": [
        "Alberta Machine Intelligence Institute (Amii)",
        "Huawei Noahs Ark Lab",
        "McGill University",
        "Mila - Quebec AI Institute",
        "The Chinese University of Hong Kong",
        "Universite de Montreal",
        "University of Alberta",
        "University of Manitoba",
        "Zhejiang University"
    ]
}