{
    "paper_title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation",
    "authors": [
        "Yupeng Hou",
        "Jianmo Ni",
        "Zhankui He",
        "Noveen Sachdeva",
        "Wang-Cheng Kang",
        "Ed H. Chi",
        "Julian McAuley",
        "Derek Zhiyuan Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@$10$ by $6.00\\%$ to $12.82\\%$."
        },
        {
            "title": "Start",
            "content": "ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Yupeng Hou 1 * Jianmo Ni 2 Zhankui He 2 Noveen Sachdeva 2 Wang-Cheng Kang 2 Ed H. Chi 2 Julian McAuley 1 Derek Zhiyuan Cheng"
        },
        {
            "title": "Abstract",
            "content": "Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@10 by 6.00% to 12.82%. 5 2 0 2 9 1 ] . [ 1 1 8 5 3 1 . 2 0 5 2 : r 1. Introduction Generative recommendation (GR) (Geng et al., 2022; Rajput et al., 2023; Zheng et al., 2024a; Zhai et al., 2024) is an emerging paradigm for the sequential recommendation task (Hidasi et al., 2016; Kang & McAuley, 2018). By tokenizing the user actions (typically represented by the interacted items) into discrete tokens, GR models learn to *Work done as student researcher at Google DeepMind. 1University of California, San Diego 2Google DeepMind. Correspondence to: Yupeng Hou <yphou@ucsd.edu>. Figure 1. Illustration of the tokenization process of ActionPiece. Each action is represented as an unordered feature set. This figure presents two possible tokenized sequences. The same action can be tokenized into different tokens depending on the surrounding context. detailed case study can be found in Section 4.5. autoregressively generate tokens, which are then parsed into recommended items. These tokens share compact vocabulary that does not scale with the item pool size, improving model scalability, memory efficiency, and recommendation performance (Rajput et al., 2023; Zhai et al., 2024). The input action sequence is vital in understanding user intentions (Li et al., 2017; Kang & McAuley, 2018), which organizes users historical interactions in chronological order. The same action (e.g., purchasing the same item) may have different meanings in different action sequences. Evidence of taking certain action can be found in the context, such as whether other items in the sequence share the same brand, color tone, or price range (Zhang et al., 2019; Zhou et al., 2020; Hou et al., 2022; 2023; Yuan et al., 2023). Despite the importance of contextual relations among actions, existing methods tokenize each action independently of its context (summarized in Table 1). The typical pipeline for tokenizing action sequences involves two steps: (1) Tokenizing each action/item individually into pattern of tokens; (2) Replacing each action in the input sequence with its corresponding token pattern. In this way, the tokens do not explicitly contain the context. Instead, they solely rely on the autoregressive models parameters being well-trained to generalize effectively in understanding the context, which challenges the capabilities of GR models. As comparison, tokenization in language modeling also originates from context-independent methods, such as word-level tokeniza1 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation tion (Sutskever et al., 2014; Bahdanau et al., 2015). decade of progress has led to most tokenization methods for modern large language models (LLMs) (OpenAI, 2022; Anil et al., 2023; Touvron et al., 2023; Zhao et al., 2023) adopting context-aware approaches, including BPE (Sennrich et al., 2016) and Unigram tokenization (Kudo, 2018), which tokenize the same characters along with their adjacent context into different tokens. In this work, we aim to make the first step towards contextaware tokenization for modeling action sequences. In analogy to how characters or bytes serve as the basic units in language modeling, we consider the associated features of an item as initial tokens. The idea is to iteratively find the most commonly co-occurring pairs of tokens among the training action sequences, then merge them into new tokens to represent segments of context. However, its non-trivial to achieve this. Unlike text, where characters naturally form sequence, the features associated with an action form an unordered set (Zhang et al., 2019; Zhou et al., 2020). Thus, the proposed tokenization algorithm should be applied on sequences of token sets. We need to carefully consider which pairs of tokens should be counted, whether within single set or between two adjacent sets, and how much weight should be given to these different types of relationships. To this end, we propose ActionPiece, which enables the same actions to be tokenized into different tokens based on their surrounding context. (1) Vocabulary construction: We first initialize the vocabulary to include every unique feature as initial tokens. The vocabulary is then constructed by iteratively learning merge rules. Each merge rule specifies that pair of tokens can be merged into new token. In each iteration, we enumerate the training corpus to count the co-occurrence of existing tokens. Considering the structural differences between token pairs, e.g., whether they occur within single set or between two adjacent sets, we assign different weights to different pairs during the counting process. (2) Segmentation: The next step is to segment action sequences into token sequences for GR model training and inference. To fully exploit the unordered nature of the feature set for each action, we introduce set permutation regularization. By randomly permuting the features within each set, we can produce multiple token sequences of single action sequence that preserve the same semantics. These variations act as natural augmentations for training data and enable inherent ensembling during model inference. 2. Related Work Generative recommendation. Conventional sequential recommendation models often relies on large embedding tables to store representations for all items, leading to significant engineering and optimization challenges (Hidasi et al., 2016; Kang & McAuley, 2018). Generative recommendaTable 1. Comparison of different action tokenization methods for generative recommendation. Contextual denotes whether the same actions can be tokenized into different tokens based on the surrounding context. Unordered denotes whether the item features or semantic IDs are used in an order-agnostic manner. Action Tokenization Example Contextual Unordered VQ-Rec (Hou et al., 2023) Product Quantization Hierarchical Clustering P5-CID (Hua et al., 2023) Residual Quantization TIGER (Rajput et al., 2023) LMIndexer (Jin et al., 2024) HSTU (Zhai et al., 2024) SPM-SID (Singh et al., 2024) Text Tokenization Raw Features SentencePiece ActionPiece Ours (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) tion (Deldjoo et al., 2024; Rajput et al., 2023; Zheng et al., 2024a) addresses these issues by tokenizing each item as tokens from shared table. By autoregressively generating the next tokens as recommendations, this generative paradigm offers benefits such as memory efficiency (Rajput et al., 2023; Yang et al., 2024; Ding et al., 2024), scalability (Zhai et al., 2024; Liu et al., 2024c), and easier alignment with LLMs (Zheng et al., 2024a; Jin et al., 2024; Tan et al., 2024; Li et al., 2025). Existing research has developed different action tokenization techniques, such as hierarchical clustering (Hua et al., 2023; Si et al., 2024), quantization (Rajput et al., 2023; Wang et al., 2024a; Zhu et al., 2024), or jointly training with recommendation models (Liu et al., 2024a). Other works incorporate additional modalities like collaborative filtering (Petrov & Macdonald, 2023; Wang et al., 2024c;b; Liu et al., 2024c), text (Zheng et al., 2024a; Jin et al., 2024; Hou et al., 2024; Zhang et al., 2025), and vision signals (Liu et al., 2024b). However, current methods tokenize each action independently, ignoring the surrounding context. In this work, we propose the first context-aware action tokenization method, where the same actions are tokenized differently in different action sequences. Tokenization for language modeling. Tokenization is the process of transforming raw text into discrete token sequences (Kudo & Richardson, 2018). Early word-level methods are context-independent and struggle to tokenize out-of-vocabulary words (Sutskever et al., 2014; Bahdanau et al., 2015). Consequently, subword-level tokenization has gradually become the more mainstream choice. The vocabularies of these subword-level tokenizers are constructed iteratively, either bottom-up (starting with small vocabulary and merging commonly occurring token pairs as new tokens) (Wu et al., 2016; Sennrich et al., 2016), or top-down (starting with large vocabulary and pruning tokens to minimize likelihood decrease) (Kudo, 2018; Yehezkel & Pinter, 2023). Once the vocabulary is built, the text can be segmented either using the same method employed during vocabulary construction or based on additional objectives (He et al., 2020; Provilkov et al., 2020; Hofmann et al., 2022; Schmidt et al., 2024). As an analogy, existing action tokeniz2 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation ers are context-independent and function like word-level language tokenizers. In this work, we take the first step toward context-aware subaction-level action tokenizer. 3. Method In this section, we present ActionPiece, context-aware method for tokenizing action sequences in generative recommendation. First, we formulate the task in Section 3.1. Then, we introduce the proposed tokenizer, covering vocabulary construction and sequence segmentation, in Section 3.2. Finally, we describe the model training and inference process using ActionPiece-tokenized sequences in Section 3.3. 3.1. Problem Formulation Given users historical actions = {i1, i2, . . . , it}, organized sequentially by their timestamps, the task is to predict the next item it+1 the user will interact with. Action as an unordered feature set. In the development of modern recommender systems, each item ij is usually associated with set of features Aj (Zhang et al., 2019; Zhou et al., 2020; Cheng et al., 2016). Assuming there are features per item, the k-th feature of item ij is denoted as fj,k Fk, where Fk is the collection of all possible choices for the k-th feature. Compared to representing actions using ordered semantic IDs (e.g., those produced by RQ-VAE (Rajput et al., 2023; Singh et al., 2024)), the unordered set setting offers two key advantages: (1) It does not require specific order among features, which aligns better with how items or actions are represented in most recommender systems; (2) It enables the inclusion of more general discrete and numeric features, such as category, brand, and price (Pazzani & Billsus, 2007; Juan et al., 2016). Action sequence as sequence of sets. Representing each item as an unordered set, the input action sequence can be written as = {A1, A2, . . . , At}, which is chronologically ordered sequence of sets. There is no order within each set, but there are orders between the features from different sets. The tokenizer design should account for the ordered and unordered relationships among features. Generative recommendation task. In this work, we aim to design tokenizer that maps an input action sequence to token sequence = {c1, c2, . . . , cl}, where denotes the number of tokens in the sequence. Note that is typically greater than the number of actions t. Next, we train GR model to autoregressively generate tokens {cl+1, . . . , cq}, which can be parsed as next-item predictions ˆit+1. 3.2. Contextual Action Sequence Tokenizer The proposed tokenizer is designed to transform action sequences (represented as sequences of feature sets) into token Algorithm 1 ActionPiece Vocabulary Construction input Sequence corpus , initial tokens V0, target size output Merge rules R, constructed vocabulary 1: Initialize vocabulary V0 # each initial token corresponds to one unique item feature # Count: accumulate weighted token co-occurrences count(, ) Count(S , V) # Algorithm 2 # Update: Merge frequent token pair into new token Select (cu, cv) arg max(ci,cj ) count(ci, cj) Update(S , {(cu, cv) cnew}) # Algorithm 3 2: 3: while < do 4: 5: 6: 7: 8: 9: {(cu, cv) cnew} # new merge rule 10: 11: end while return R, {cnew} # add new token to the vocabulary sequences. In the ActionPiece-tokenized sequences, each token corresponds to set containing varying numbers of features. For example, token can represent: (1) subset of features from one item; (2) set with single feature; (3) all features of one item; or (4) features from multiple items. We also label these four types of tokens in Figure 1. Below, we first describe how to construct the ActionPiece tokenizers vocabulary given corpus of action sequences (Section 3.2.1). Then, we introduce how to segment action sequences into new sequence of sets, where each set corresponds to token from the constructed vocabulary (Section 3.2.2). 3.2.1. VOCABULARY CONSTRUCTION ON ACTION SEQUENCE CORPUS Given corpus of action sequences , the goal of vocabulary construction is to create vocabulary of tokens. Each token represents combination of features that frequently occur in the corpus. Similar to BPE (Sennrich et al., 2016), we construct the vocabulary using bottom-up approach. The process starts with an initial vocabulary of tokens V0. The construction proceeds iteratively, adding one new token to the vocabulary at each iteration until the predefined target size is reached. Each iteration consists of two consecutive steps: count, where the most frequently occurring token pair is identified, and update, where the corpus is modified by merging the selected pair into new token. An algorithmic workflow is illustrated in Algorithm 1. Vocabulary initialization. In BPE, each token represents sequence of bytes. Thus, the most fundamental units the initial tokensare single bytes, which form the initial vocabulary of BPE. Similarly, each token in ActionPiece represents set of features. Therefore, we initialize ActionPiece with vocabulary in which each token represents set containing one unique item feature. Formally, we denote the initial vocabulary as V0 = {c = {f }f F1 . . . Fm}. After initializing the vocabulary, each action sequence (of 3 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Figure 2. Illustration of how weights of co-occurring token pairs are counted during vocabulary construction. In this example, two adjacent sets in the sequence are considered: one with 4 tokens (represented as ) and another with 3 tokens (represented as ). Token pairs are counted within single set (< , > and < , >) and across the two adjacent sets (< , >). feature sets) can be represented as sequence of token sets. Count: context-aware token co-occurrence counting. In each iteration of vocabulary construction, the first step is to count the co-occurrence of token pairs in the corpus. These pairs capture important feature combinations, which are encoded by creating new tokens. There are two types of token co-occurrence within sequence of sets: (1) two tokens exist within the same set, or (2) two tokens exist in adjacent sets in the sequence. Notably, the second type allows ActionPiece to explicitly include context information. Weighted co-occurrence counting. In one-dimensional token sequences (e.g., text), all token pairs are typically treated equally. However, in sequences of token sets, token pairs vary based on their types and the sizes of their respective sets. To account for these differences, we propose assigning different weights to token pairs. To determine the weight for each token pair, we relate sequences of token sets to token sequences by randomly permuting the tokens within each set and flattening them into single token sequence. Let (c, c) represent the expected probability that tokens and are adjacent in the flattened sequence. For two tokens from the same set, we have: (c1, c2) = (c2, c1) = Ai 1 (cid:0)Ai 2 (cid:1) = 2 Ai , c1, c2 Ai, (1) and for two tokens from adjacent sets, we have: (c1, c3) = 1 Ai Ai+1 , c1 Ai, c3 Ai+1. (2) By considering the probabilities of all adjacent token pairs in the flattened sequence as 1, the weights for token pairs in the original sequence of token sets correspond to the probabilities given in Equations (1) and (2). An illustration is shown in Figure 2. Accumulating co-occurrence weights. The weights described above are calculated based solely on the cooccurrence type and the set size. They do not take into account the specific tokens being analyzed. Tokens ci and cj might appear in the same set in one sequence but in two adjacent sets in another sequence. By iterating through the corpus, we sum up the weights for each token pair whenever they appear together multiple times. Update: corpus updating with action-intermediate nodes. The next step in each iteration is to merge the token pair with the highest accumulated co-occurrence weight. Since token merging may change the set size, we use double-ended linked list (Zouhar et al., 2023) to maintain the action sequences, where each node represents set of tokens. Merging tokens within the same set is straightforward, i.e., replacing the two tokens with new one. However, merging tokens from two adjacent sets is more complex, e.g., determining which set should include the new token. Intermediate Node. We introduce the concept of intermediate node to handle tokens that combine features from multiple sets. Initially, all nodes in the maintained linked list contain features specific to their corresponding actions. These nodes are referred to as action nodes. (1) When tokens from two adjacent action nodes are being merged, we insert new intermediate node between the two action nodes. The new token is stored in the intermediate node, and the merged tokens are removed from their respective action nodes; (2) When merging tokens from an action node and an intermediate node, the new token replaces the original token in the intermediate node. The reason is that this new token also combines features from multiple actions. After the merge, the token from the action node is removed. Following the above update rules ensures that there is at most one intermediate node between any two action nodes, and each intermediate node contains no more than one token. When calculating co-occurrence weights involving an intermediate node, it can simply be treated as set of size 1. Efficient implementation. Naively counting and updating the corpus requires total time complexity of O(QN Lm2), where is the target vocabulary size, is the number of action sequences in the training corpus, and is the average length of these sequences. However, it is unnecessary to count co-occurrences from scratch in each iteration. This is because only small portion of the maintained linked lists is modified compared to the previous iteration. 4 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Figure 3. Illustration of how the linked list, which maintains the action sequence, is updated when merging two tokens into new token. Three cases are considered: (1) both tokens are in the same action node; (2) the tokens are in two adjacent action nodes; (3) one token is in an action node, while the other is in an intermediate node. Data structures. To address this, we propose maintaining hash table to store the accumulated co-occurrences for each token pair. Additionally, we use inverted indices to map token pairs to all the linked lists that contain them. global heap is maintained to return the token pair with the highest accumulated co-occurrence. The key challenge lies in updating these data structures. We carefully compute the changes in accumulated co-occurrences and update both the hash table and the inverted indices. For the heap, we employ lazy-update strategy. We insert the latest weights with tag. When fetching value from the heap, we check the tag to verify if the value is up-to-date. If it is not, we discard the value and fetch the next one. Time complexity. Let = O(N Lm) represent the maximal heap size. Using the proposed algorithm, we successfully reduce the original time complexity to O(log log Lm2), achieving efficient vocabulary construction. In practice, the later iterations take significantly less time than the initial ones. This is expected and because tokens with higher accumulated co-occurrence weights typically appear frequently in the early stages. However, the overall construction time benefits from the reduced amortized complexity. Further details about the vocabulary construction algorithm are provided in Appendix C. 3.2.2. SEGMENTATION BY SET PERMUTATION REGULARIZATION Segmentation is to convert original action sequences into sequence of feature sets. Each set in the segmented sequence corresponds to token in the vocabulary. Naive segmentation. One segmentation strategy in ActionPiece involves applying the same technique used to construct the vocabulary. Specifically, this technique iteratively identifies token pairs with high priorities (represented by the IDs of tokens, where tokens added earlier may have higher priority). However, we observed that this strategy can lead to bias, where only subset of tokens in the vocabulary is frequently used (as shown empirically in Section 4.4.2). Set permutation regularization (SPR). To address this issue and account for the unordered nature of sets, we propose set permutation regularization, which generates multiple segmentations for each action sequence. The key idea is to avoid enumerating all possible pairs between tokens in set or adjacent sets. Instead, we generate random permutation of each set and treat it as one-dimensional sequence. By concatenating all the permutations, we create long token sequence. This sequence can then be segmented using traditional BPE segmentation methods (Sennrich et al., 2016). In this approach, different permutations can produce distinct segmented token sequences with the same semantics. These sequences serve as natural augmentations for model training (Section 3.3.1) and enable inherent ensembling during model inference (Section 3.3.2). 3.3. Generative Recommendation Models 3.3.1. TRAINING ON AUGMENTED TOKEN SEQUENCES For an action sequence and its ground-truth next action in the training corpus, we tokenize them into token sequences Cin and Cout, respectively. Taking Cin as input, we train Transformer encoder-decoder module (Raffel et al., 2020) to autoregressively generate Cout (e.g., next-token prediction objective (Rajput et al., 2023)). During training, we tokenize the action sequence using the set permutation regularization described in Section 3.2.2 in each epoch. This approach naturally augments the training sequences, which empirically improves model performance, as shown in Section 4.3. 3.3.2. INFERENCE-TIME ENSEMBLING During model inference, we tokenize each action sequence times using set permutation regularization. By passing these tokenized sequences through the model, we obtain 5 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Table 2. Performance comparison of different methods on the Amazon Reviews dataset (McAuley et al., 2015). The best and second-best performance is denoted in bold and underlined fonts. R@K and N@K are short for Recall@K and NDCG@K, respectively. Improv. denotes the percentage improvement of our method compared to the strongest baseline method. Datasets Metric ID-based Feature + ID Generative BERT4Rec SASRec FDSA S3-Rec VQ-Rec P5-CID TIGER LMIndexer HSTU SPM-SID ActionPiece Improv. Sports Beauty CDs R@5 N@5 R@10 N@ R@5 N@5 R@10 N@10 R@5 N@5 R@10 N@10 0.0115 0.0075 0.0191 0.0099 0.0203 0.0124 0.0347 0.0170 0.0326 0.0201 0.0547 0.0271 0.0233 0.0154 0.0350 0. 0.0387 0.0249 0.0605 0.0318 0.0351 0.0177 0.0619 0.0263 0.0182 0.0122 0.0288 0.0156 0.0267 0.0163 0.0407 0.0208 0.0226 0.0137 0.0378 0.0186 0.0251 0.0161 0.0385 0. 0.0387 0.0244 0.0647 0.0327 0.0213 0.0130 0.0375 0.0182 0.0181 0.0132 0.0251 0.0154 0.0434 0.0311 0.0741 0.0372 0.0314 0.0209 0.0485 0.0264 0.0287 0.0179 0.0426 0. 0.0468 0.0315 0.0701 0.0400 0.0505 0.0326 0.0785 0.0416 0.0264 0.0181 0.0400 0.0225 0.0454 0.0321 0.0648 0.0384 0.0492 0.0329 0.0748 0.0411 0.0222 0.0142 0.0415 0.0262 0.0258 0.0165 0.0414 0.0215 0.0469 0.0314 0.0704 0.0389 0.0417 0.0275 0.0638 0.0346 0.0280 0.0180 0.0446 0. 0.0475 0.0321 0.0714 0.0399 0.0509 0.0337 0.0778 0.0424 0.0316 0.0005 +12.86% 0.0205 0.0002 +11.71% 0.0500 0.0007 +12.11% 0.0264 0.0003 +12.82% 0.0511 0.0014 0.0340 0.0011 0.0775 0.0017 0.0424 0.0011 0.0544 0.0005 0.0359 0.0004 0.0830 0.0008 0.0451 0.0005 +7.58% +5.92% +4.59% +6.00% +6.88% +6.53% +5.73% +6.37% output ranking lists (e.g., using beam search for inference when TIGER (Rajput et al., 2023) is the GR backbone). We then combine these ranking lists by averaging the scores of each predicted item. This approach applies data-level ensembling, which has been shown to enhance recommendation performance, as discussed in Section 4.4.3. Evaluation settings. Following Rajput et al. (2023), we use Recall@K and NDCG@K as metrics to evaluate the methods, where {5, 10}. Model checkpoints with the best performance on the validation set are used for evaluation on the test set. We run the experiments with five random seeds and report the average metrics. 4. Experiments 4.1. Experimental Setup Datasets. We use three categories from the Amazon Reviews dataset (McAuley et al., 2015) for our experiments: Sports and Outdoors (Sports), Beauty (Beauty), and CDs and Vinyl (CDs). Each users historical reviews are considered actions and are sorted chronologically as action sequences, with earlier reviews appearing first. To evaluate the models, we adopt the widely used leave-lastout protocol (Kang & McAuley, 2018; Zhao et al., 2022; Rajput et al., 2023), where the last item and second-to-last item in each action sequence are used for testing and validation, respectively. More details about the datasets can be found in Appendix E. Compared methods. We compare the performance of ActionPiece with the following methods: (1) ID-based sequential recommendation methods, including BERT4Rec (Sun et al., 2019), and SASRec (Kang & McAuley, 2018); (2) feature-enhanced sequential recommendation methods, such as FDSA (Zhang et al., 2019), S3-Rec (Zhou et al., 2020), and VQ-Rec (Hou et al., 2023); and (3) generative recommendation methods, including P5-CID (Hua et al., 2023), TIGER (Rajput et al., 2023), LMIndexer (Jin et al., 2024), HSTU (Zhai et al., 2024), and SPM-SID (Singh et al., 2024), each representing different action tokenization method  (Table 1)  . detailed description of these baselines is provided in Appendix F. Implementation details. Please refer to Appendix for detailed implementation and hyperparameter settings. 4.2. Overall Performance We compare ActionPiece with sequential recommendation and generative recommendation baselines, which use various action tokenization methods, across three public datasets. The results are shown in Table 2. For the compared methods, we observe that those using item features generally outperform item ID-only methods. This indicates that incorporating features enhances recommendation performance. Among the methods leveraging item features (Feature + ID and Generative), generative recommendation models achieve better performance. These results further confirm that injecting semantics into item indexing and optimizing at sub-item level enables generative models to better use semantic information and improve recommendation performance. Among all the baselines, SPM-SID achieves the best results. By incorporating the SentencePiece model (Kudo & Richardson, 2018), SPMSID replaces popular semantic ID patterns within each item with new tokens, benefiting from larger vocabulary. Our proposed ActionPiece consistently outperforms all baselines across three datasets, achieving significant improvement in NDCG@10. It surpasses the best-performing baseline method by 6.00% to 12.82%. Unlike existing methods, ActionPiece is the first context-aware action sequence tokenizer, i.e., the same action can be tokenized into different 6 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Figure 4. Analysis of recommendation performance (NDCG@10, ) and average tokenized sequence length (NSL, ) w.r.t. vocabulary size across three datasets. N/A indicates that ActionPiece is not applied, i.e., action sequences are represented solely by initial tokens. Table 3. Ablation analysis of ActionPiece. The recommendation performance is measured using NDCG@10. The best performance is denoted in bold fonts. Variants Sports Beauty CDs TIGER with larger vocabularies (1.1) TIGER - 1k (4 28) (1.2) TIGER-49k (6 213) (1.3) TIGER-66k (4 214) 0.0225 0.0162 0.0194 0.0384 0.0317 N/A 0.0411 0.0338 0. Vocabulary construction (2.1) w/o tokenization (2.2) w/o context-aware (2.3) w/o weighted counting 0.0215 0.0258 0.0257 0.0389 0.0416 0.0412 0.0346 0.0429 0.0435 Set permutation regularization (3.1) only for inference (3.2) only for training 0.0192 0.0244 0.0316 0.0387 0.0329 0.0422 ActionPiece (40k) 0. 0.0424 0.0451 not applicable as 214 is larger than #items in Beauty. tokens depending on its surrounding context. This allows ActionPiece to capture important sequence-level feature patterns that enhance recommendation performance. 4.3. Ablation Study We conduct ablation analyses in Table 3 to study how each proposed technique contributes to ActionPiece. (1) We increase the vocabulary size of TIGER, to determine whether the performance gain of ActionPiece is solely due to scaling up the number of tokens in the vocabulary. By increasing the number of semantic ID digits per item (4 6) and the number of candidate semantic IDs per digit (28 213 or 214), we create two variants with vocabularies larger than ActionPiece. However, these TIGER variants perform worse than ActionPiece, and even the original TIGER with only 1024 tokens. The experimental results suggest that scaling up the vocabulary size for generative recommendation models is challenging, consistent with the observations from Zhang et al. (2024). 7 Figure 5. Analysis of token utilization rate (%) during model training w.r.t. segmentation strategy. (2) To evaluate the effectiveness of the proposed vocabulary construction techniques, we introduce the following variants: (2.1) w/o tokenization, which skips vocabulary construction, using item features directly as tokens; (2.2) w/o context-aware, which only considers co-occurrences and merges tokens within each action during vocabulary construction and segmentation; and (2.3) w/o weighted counting, which treats all token pairs equally rather than using the weights defined in Equations (1) and (2). The results indicate that removing any of these techniques reduces performance, demonstrating the importance of these methods for building context-aware tokenizer. (3) To evaluate the effectiveness of SPR, we revert to naive segmentation, as described in Section 3.2.2, during model training and inference, respectively. The results show that replacing SPR with naive segmentation in either training or inference degrades performance. 4.4. Further Analysis 4.4.1. PERFORMANCE AND EFFICIENCY W.R.T. VOCABULARY SIZE Vocabulary size is key hyperparameter for language tokenizers (Meta AI, 2024; Dagan et al., 2024). In this study, we investigate how adjusting vocabulary size affects the generative recommendation models. We use the normalized sequence length (NSL) (Dagan et al., 2024) to measure the length of tokenized sequences, where smaller NSL indicates fewer tokens per tokenized sequence. We experiment with vocabulary sizes in {N/A, 5k, 10k, 20k, 30k, 40k}, where N/A represents the direct use of item features as tokens. As shown in Figure 4, increasing the vocabulary size improves recommendation performance and reduces ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Each item in the action sequence is represented as feature set, with each item consisting of five features. The features within an item do not require specific order. The first step of tokenization leverages the unordered nature of the feature set and applies set permutation regularization (Section 3.2.2). This process arranges each feature set into specific permutation and iteratively groups features based on the constructed vocabulary (Section 3.2.1). This results in different segments that convey the same semantics. Each segment is represented as sequence of sets, where each set corresponds to token in the vocabulary. By examining the segments and their corresponding token sequences, we identify four types of tokens, as annotated in Figure 1: (1) subset of features from single item (token 14844 corresponds to features 747 and 923 of the T-shirt); (2) set containing single feature (feature 76 of the socks); (3) all features of single item (token 7995 corresponds to all features of the shorts); and (4) features from multiple items (e.g., token 8316 includes feature 923 from the Tshirt and feature 679 from the socks, while token 19895 includes feature 1100 from the socks as well as features 560 and 943 from the shorts). Notably, the fourth type of token demonstrates that the features of one action can be segmented and grouped with features from adjacent actions. This results in different tokens for the same action depending on the surrounding context, showcasing the context-aware tokenization process of ActionPiece. 5. Conclusion In this paper, we introduce ActionPiece, the first contextaware action sequence tokenizer for generative recommendation. By considering the surrounding context, the same action can be tokenized into different tokens in different sequences. We formulate generative recommendation as task on sequences of feature sets and merge important feature patterns into tokens. During vocabulary construction, we propose assigning weights to token pairs based on their structures, such as those within single set or across adjacent sets. To enable efficient vocabulary construction, we use double-ended linked lists to maintain the corpus and introduce intermediate nodes to store tokens that combine features across adjacent sets. Additionally, we propose set permutation regularization, which segments single action sequence into multiple token sequences with the same semantics. These segments serve as natural augmentations for training and as ensemble instances for inference. In the future, we plan to align user actions with other modalities by constructing instructions that combine ActionPiece tokens and other types of tokens. We also aim to extend the proposed tokenizer to other tasks that can be framed as set sequence modeling problems, including audio modeling, sequential decision-making, and time series forecasting. Figure 6. Analysis of performance (NDCG@10, ) w.r.t. the number of ensembled segments during model inference. the tokenized sequence length. Conversely, reducing the vocabulary size lowers the number of model parameters, improving memory efficiency. This analysis demonstrates that adjusting vocabulary size enables trade-off between model performance, sequence length, and memory efficiency. 4.4.2. TOKEN UTILIZATION RATE W.R.T. SEGMENTATION STRATEGY As described in Section 3.3.1, applying SPR augments the training corpus by producing multiple token sequences that In Table 3, we observe that share the same semantics. incorporating SPR significantly improves recommendation performance. One possible reason is that SPR increases token utilization rates. To validate this assumption, we segment the action sequences in each training epoch using two strategies: naive segmentation and SPR. As shown in Figure 5, naive segmentation uses only 56.89% of tokens for model training, limiting the models ability to generalize to unseen action sequences. In contrast, SPR achieves token utilization rate of 87.01% after the first training epoch, with further increases as training progresses. These results demonstrate that the proposed SPR segmentation strategy improves the utilization of ActionPiece tokens, enabling better generalization and enhanced performance. 4.4.3. PERFORMANCE W.R.T. INFERENCE-TIME ENSEMBLES As described in Section 3.3.2, ActionPiece supports inference-time ensembling by using SPR segmentation. We vary the number of ensembled segments, q, in {N/A, 1, 3, 5, 7}, where N/A indicates using naive segmentation during model inference. As shown in Figure 6, ensembling more tokenized sequences improves ActionPieces recommendation performance. However, the performance gains slow down as increases to 5 and 7. Since higher also increases the computational cost of inference, this creates trade-off between performance and computational budget in practice. 4.5. Case Study To understand how GR models benefit from the unordered feature setting and context-aware action sequence tokenization, we present an illustrative example in Figure 1. 8 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation"
        },
        {
            "title": "References",
            "content": "Anil, R., Borgeaud, S., Wu, Y., Alayrac, J., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., and et al. Gemini: family of highly capable multimodal models. arxiv:2312.11805, 2023. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., Anderson, G., Corrado, G., Chai, W., Ispir, M., Anil, R., Haque, Z., Hong, L., Jain, V., Liu, X., and Shah, H. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, pp. 710, 2016. Dagan, G., Synnaeve, G., and Roziere, B. Getting the most out of your tokenizer for pre-training and domain adaptation. In ICML, 2024. Deldjoo, Y., He, Z., McAuley, J., Korikov, A., Sanner, S., Ramisa, A., Vidal, R., Sathiamoorthy, M., Kasirzadeh, A., and Milano, S. review of modern recommender systems using generative models (gen-recsys). In KDD, pp. 64486458, 2024. Ding, Y., Hou, Y., Li, J., and McAuley, J. Inductive generative recommendation via retrieval-based speculation. arXiv preprint arXiv:2410.02939, 2024. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazare, P.-E., Lomeli, M., Hosseini, L., and Jegou, H. The faiss library. arXiv preprint arXiv:2401.08281, 2024. Ge, T., He, K., Ke, Q., and Sun, J. Optimized product quantization for approximate nearest neighbor search. In CVPR, pp. 29462953, 2013. Geng, S., Liu, S., Fu, Z., Ge, Y., and Zhang, Y. Recommendation as language processing (rlp): unified pretrain, personalized prompt & predict paradigm (p5). In RecSys, pp. 299315, 2022. He, X., Haffari, G., and Norouzi, M. Dynamic programming encoding for subword segmentation in neural machine translation. In ACL, pp. 30423051, 2020. Hidasi, B., Karatzoglou, A., Baltrunas, L., and Tikk, D. Session-based recommendations with recurrent neural networks. In ICLR, 2016. Hofmann, V., Schuetze, H., and Pierrehumbert, J. An embarrassingly simple method to mitigate undesirable properties of pretrained language model tokenizers. In ACL, 2022. Hou, Y., Mu, S., Zhao, W. X., Li, Y., Ding, B., and Wen, J.-R. Towards universal sequence representation learning for recommender systems. In KDD, pp. 585593, 2022. Hou, Y., He, Z., McAuley, J., and Zhao, W. X. Learning vector-quantized item representation for transferable sequential recommenders. In WWW, pp. 11621171, 2023. Hou, Y., Zhang, J., Lin, Z., Lu, H., Xie, R., McAuley, J., and Zhao, W. X. Large language models are zero-shot rankers for recommender systems. In ECIR, 2024. Hua, W., Xu, S., Ge, Y., and Zhang, Y. How to index item ids for recommendation foundation models. In SIGIR-AP, pp. 195204, 2023. Jin, B., Zeng, H., Wang, G., Chen, X., Wei, T., Li, R., Wang, Z., Li, Z., Li, Y., Lu, H., Wang, S., Han, J., and Tang, X. Language models as semantic indexers. In ICML, 2024. Juan, Y., Zhuang, Y., Chin, W.-S., and Lin, C.-J. Field-aware factorization machines for ctr prediction. In RecSys, pp. 4350, 2016. Kang, W.-C. and McAuley, J. Self-attentive sequential recommendation. In ICDM, pp. 197206, 2018. Kudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. In ACL, pp. 6675, 2018. Kudo, T. and Richardson, J. SentencePiece: simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP, 2018. Li, G., Zhang, X., Zhang, Y., Yin, Y., Yin, G., and Lin, W. Semantic convergence: Harmonizing recommender systems via two-stage alignment and behavioral semantic tokenization. In AAAI, 2025. Li, J., Ren, P., Chen, Z., Ren, Z., Lian, T., and Ma, J. Neural attentive session-based recommendation. In CIKM, pp. 14191428, 2017. Liu, E., Zheng, B., Ling, C., Hu, L., Li, H., and Zhao, W. X. End-to-end learnable item tokenization for generative recommendation. arXiv preprint arXiv:2409.05546, 2024a. 9 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Liu, H., Wei, Y., Song, X., Guan, W., Li, Y.-F., and Nie, L. Mmgrec: Multimodal generative recommendation with transformer model. arXiv preprint arXiv:2404.16555, 2024b. Liu, Z., Hou, Y., and McAuley, J. Multi-behavior generative recommendation. In CIKM, 2024c. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In ACL, pp. 17151725, 2016. Sheng, L., Zhang, A., Zhang, Y., Chen, Y., Wang, X., and Chua, T.-S. Language representations can be what recommenders need: Findings and potentials. In ICLR, 2025. McAuley, J., Targett, C., Shi, Q., and Van Den Hengel, A. Image-based recommendations on styles and substitutes. In SIGIR, pp. 4352, 2015. Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date, 2024. URL https:// ai.meta.com/blog/meta-llama-3/. Ni, J., Abrego, G. H., Constant, N., Ma, J., Hall, K., Cer, D., and Yang, Y. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of ACL, pp. 18641874, 2022. OpenAI. Introducing ChatGPT, 2022. URL https:// openai.com/index/chatgpt/. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 32, 2019. Pazzani, M. J. and Billsus, D. Content-based recommendation systems. In The adaptive web: methods and strategies of web personalization, pp. 325341. Springer, 2007. Petrov, A. V. and Macdonald, C. Generative sequenarXiv preprint tial recommendation with gptrec. arXiv:2306.11114, 2023. Si, Z., Sun, Z., Chen, J., Chen, G., Zang, X., Zheng, K., Song, Y., Zhang, X., Xu, J., and Gai, K. Generative retrieval with semantic tree-structured item identifiers via contrastive learning. In SIGIR-AP, 2024. Singh, A., Vu, T., Mehta, N., Keshavan, R., Sathiamoorthy, M., Zheng, Y., Hong, L., Heldt, L., Wei, L., Tandon, D., Chi, E. H., and Yi, X. Better generalization with semantic ids: case study in ranking for recommendations. In RecSys, 2024. Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM, pp. 14411450, 2019. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In NIPS, 2014. Tan, J., Xu, S., Hua, W., Ge, Y., Li, Z., and Zhang, Y. Idgenrec: Llm-recsys alignment with textual id learning. In SIGIR, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. Provilkov, I., Emelianenko, D., and Voita, E. Bpe-dropout: Simple and effective subword regularization. In ACL, pp. 18821892, 2020. Wang, W., Bao, H., Lin, X., Zhang, J., Li, Y., Feng, F., Ng, S.-K., and Chua, T.-S. Learnable tokenizer for llm-based generative recommendation. In CIKM, 2024a. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rajput, S., Mehta, N., Singh, A., Keshavan, R. H., Vu, T., Heldt, L., Hong, L., Tay, Y., Tran, V. Q., Samost, J., Kula, M., Chi, E. H., and Sathiamoorthy, M. Recommender systems with generative retrieval. In NeurIPS, 2023. Schmidt, C. W., Reddy, V., Zhang, H., Alameddine, A., Uzan, O., Pinter, Y., and Tanner, C. Tokenization is more than compression. In EMNLP, 2024. Wang, Y., Ren, Z., Sun, W., Yang, J., Liang, Z., Chen, X., Xie, R., Yan, S., Zhang, X., Ren, P., Chen, Z., and Xin, X. Content-based collaborative generation for recommender systems. In CIKM, 2024b. Wang, Y., Xun, J., Hong, M., Zhu, J., Jin, T., Lin, W., Li, H., Li, L., Xia, Y., Zhao, Z., and Dong, Z. Eager: Twostream generative recommender with behavior-semantic collaboration. In KDD, pp. 32453254, 2024c. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural language processing. In EMNLP, 2020. 10 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Łukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Yang, L., Paischer, F., Hassani, K., Li, J., Shao, S., Li, Z. G., He, Y., Feng, X., Noorshams, N., Park, S., Long, B., Nowak, R. D., Gao, X., and Eghbalzadeh, H. Unifying generative and dense retrieval for sequential recommendation. arXiv preprint arXiv:2411.18814, 2024. Yehezkel, S. and Pinter, Y. Incorporating context into subword vocabularies. In EACL, pp. 623635, 2023. Yuan, Z., Yuan, F., Song, Y., Li, Y., Fu, J., Yang, F., Pan, Y., and Ni, Y. Where to go next for recommender systems? id-vs. modality-based recommender models revisited. In SIGIR, pp. 26392649, 2023. Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. Zhai, J., Liao, L., Liu, X., Wang, Y., Li, R., Cao, X., Gao, L., Gong, Z., Gu, F., He, M., Lu, Y., and Shi, Y. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. In ICML, 2024. Zhang, J., Xie, R., Hou, Y., Zhao, W. X., Lin, L., and Wen, J.-R. Recommendation as instruction following: large language model empowered recommendation approach. ACM Trans. Inf. Syst., 2025. Zhang, T., Zhao, P., Liu, Y., Sheng, V. S., Xu, J., Wang, D., Liu, G., and Zhou, X. Feature-level deeper self-attention network for sequential recommendation. In IJCAI, pp. 43204326, 2019. Zhang, T., Pan, J., Wang, J., Zha, Y., Dai, T., Chen, B., Luo, R., Deng, X., Wang, Y., Yue, M., et al. Towards scalable semantic representation for recommendation. arXiv preprint arXiv:2410.09560, 2024. Zhao, W. X., Mu, S., Hou, Y., Lin, Z., Chen, Y., Pan, X., Li, K., Lu, Y., Wang, H., Tian, C., Min, Y., Feng, Z., Fan, X., Chen, X., Wang, P., Ji, W., Li, Y., Wang, X., and Wen, J.-R. Recbole: Towards unified, comprehensive and efficient framework for recommendation algorithms. In CIKM, 2021. Zhao, W. X., Lin, Z., Feng, Z., Wang, P., and Wen, J.-R. revisiting study of appropriate offline evaluation for top-n recommendation algorithms. ACM Transactions on Information Systems, 41(2):141, 2022. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J., and Wen, J. survey of large language models. arXiv:2303.18223, 2023. Zheng, B., Hou, Y., Lu, H., Chen, Y., Zhao, W. X., Chen, M., and Wen, J.-R. Adapting large language models by integrating collaborative semantics for recommendation. In ICDE, pp. 14351448, 2024a. Zheng, B., Zhang, J., Lu, H., Chen, Y., Chen, M., Zhao, W. X., and Wen, J.-R. Enhancing graph contrastive learning with reliable and informative augmentation for recommendation. arXiv preprint arXiv:2409.05633, 2024b. Zhou, K., Wang, H., Zhao, W. X., Zhu, Y., Wang, S., Zhang, F., Wang, Z., and Wen, J.-R. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In CIKM, pp. 18931902, 2020. Zhu, J., Jin, M., Liu, Q., Qiu, Z., Dong, Z., and Li, X. Cost: Contrastive quantization based semantic tokenization for generative recommendation. In RecSys, 2024. Zouhar, V., Meister, C., Gastaldi, J., Du, L., Vieira, T., Sachan, M., and Cotterell, R. formal perspective on byte-pair encoding. In Findings of ACL, pp. 598614, 2023. 11 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Notation Explaination Table 4. Notations and explanations. i, i1, ij it+1 ˆit+1 = {i1, i2, . . . , it} A, A1, Aj = Aj fj,k Fk = {A1, A2, . . . , At} c, c1, cj = {c1, c2, . . . , cl} {cl+1, . . . , cq} {(cu, cv) cnew} = (c, c) q item, item identifier, item ID the number of actions in the input action sequence; the timestamp when the model makes prediction the ground-truth next item the predicted next item the action sequence where each action is represented with the interacted item ID set of item features or tokens the number of features associated with each item the k-th feature of item ij the collection of all possible choices for the k-th feature the action sequence where each action is represented with set of item features input & generated tokens the number of tokens in the token sequence the token sequence tokenized from the input action sequence the tokens generated by the GR model vocabulary of ActionPiece tokenizer merge rules of ActionPiece tokenizer one merge rule indicating two adjacent tokens cu and cv can be replaced by token cnew size of ActionPiece vocabulary probability that tokens and are adjacent when flattening sequence of sets into token sequence the number of action sequences in the training corpus the average length of action sequences in the training corpus Maximal heap size, O(N Lm) The number of segmentations produced using set permutation regularization during inference"
        },
        {
            "title": "Appendices",
            "content": "A. Notations We summarize the notations used in this paper in Table 4. B. Algorithmic Details In this section, we provide detailed algorithms for vocabulary construction and segmentation. B.1. Vocabulary Construction Algorithm The overall procedure for vocabulary construction is illustrated in Algorithm 1. As described in Section 3.2.1, this process involves iterative Count (Algorithm 2) and Update (Algorithm 3) operations. B.2. Segmentation with Set Permutation Regularization Algorithm The detailed algorithm for segmenting action sequences into token sequences using set permutation regularization (SPR) is shown in Algorithm 4. In practice, we often run Algorithm 4 multiple times to augment the training corpus or ensemble recommendation outputs, as described in Sections 3.3.1 and 3.3.2. C. Efficient Vocabulary Construction Implementation To efficiently construct the ActionPiece vocabulary, we propose using data structures such as heaps with lazy update trick, linked lists, and inverted indices to speed up each iteration of the construction process. The key idea is to avoid recalculating token co-occurrences in every iteration and instead update the data structures. The pseudocode is shown in Figure 7. 12 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation count(ci, cj) 0 Algorithm 2 ActionPiece Vocabulary Construction Count (Figure 2) input Action sequence corpus , current vocabulary output Accumulated weighted token co-occurrences count(, ) 1: for 0 to V, 0 to do 2: 3: end for 4: for all sequence do 5: 6: 7: 8: 9: 10: 11: 12: Ak S[k] # current action node # Process all unordered token pairs within Ak for all ci, cj Ak, = do length(S) # number of action nodes in sequence for 0 to 1 do end for # Process all ordered token pairs between Ak and Ak+1 if < 1 then Ak+1 S[k + 1] for all ci Ak, cj Ak+1 do 13: 14: 15: 16: 17: 18: 19: 20: end for return count(, ) end if end for end for count(ci, cj) count(ci, cj) + 2/Ak # weight of tokens within single set (Equation (1)) count(cj, ci) count(cj, ci) + 2/Ak # symmetric update count(ci, cj) count(ci, cj) + 1/(Ak Ak+1) # weight of tokens from two adjacent sets (Equation (2)) C.1. Data Structures The data structures used in the proposed algorithm are carefully designed to optimize the efficiency of vocabulary construction. Here is detailed discussion of their roles and implementations: Linked list: Each action sequence in the training corpus is stored as linked list. This allows efficient local updates during token merging. When token pair (cu, cv) is replaced by new token cnew, only the affected nodes and their neighbors in the linked list need to be modified (as shown in Algorithm 3 and Figure 3). Heap with lazy update trick: max-heap prioritizes token pairs by their co-occurrences. Instead of recalculating the heap entirely in each iteration, lazy update strategy is employed: outdated entries (with mismatched co-occurrence counts) are retained but skipped during extraction. In the pseudocode, the loop checks if the top element is outdated via is outdated. Invalid entries are discarded, and only valid ones are processed. Updated co-occurrences are pushed as new entries (with negative counts for max-heap emulation). Inverted indices: The pair2head dictionary maps token pairs to the sequences containing them. When pair (cu, cv) is merged, the algorithm directly retrieves affected sequence IDs via pair2head[(c u, v)], avoiding full corpus scan. After merging, the inverted indices are incrementally updated: new token pairs (e.g., (cprev, cnew) and (cnew, cnext)) are added to pair2head, while obsolete pairs are removed. This enables targeted updates and ensures subsequent iterations efficiently access relevant sequences. These structures collectively reduce time complexity by focusing computation on dynamically changing parts of the corpus and avoiding redundant global operations. The linked list enables localized edits, the heap minimizes priority recalculation, and the inverted indices eliminate brute-force searches, making the algorithm scalable to large corpora. C.2. Time Complexity The time complexity of the efficient vocabulary construction algorithm can be analyzed through two main components: initialization and iterative merging. 13 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation"
        },
        {
            "title": "Replace cu and cv in Ak with cnew",
            "content": "t length(S) for 0 to 1 do Ak+1 S[k + 1] if cu Ak and cv Ak+1 then Ak S[k] # Merge tokens in one action node if cu Ak and cv Ak then end if # Merge tokens from two adjacent nodes if < 1 then Algorithm 3 ActionPiece Vocabulary Construction Update (Figure 3) input Action sequence corpus before updating, current merge rule {(cu, cv) cnew} output Updated action sequence corpus 1: for all sequence do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for return Create intermediate node between Ak and Ak+1 {cnew} # linked list: Ak Ak+1 Ak Ak cu Ak+1 Ak+1 cv Ak {cnew} Ak+1 Ak+1 cv if Ak, Ak+1 are both action nodes then else if Ak+1 is intermediate node then else if Ak is intermediate node then Ak Ak cu Ak+1 {cnew} end if end for end if end if Initialization phase involves building the initial max-heap to track co-occurrence frequencies. Given input sequences (each with an average length of L), we count co-occurrences for all O(m2) token pairs within each set of size m. This requires O(N Lm2) time. Iterative merging phase dynamically processes the involved sequences. The total number of such sequences across all iterations is approximately (cid:19) (cid:18) V0 + (cid:18) (cid:19) V0 + + + (cid:19) (cid:18) O(log QN ). For each sequence, updating the linked list requires O(Lm) time, counting co-occurrences takes O(Lm2) time, and inserting co-occurrences into the max-heap requires at most O(Lm2 log H) time. Here, represents the heap size, which is at most O(N Lm). Thus, the overall time complexity for iterative merging is O(log QN (Lm + Lm2 + Lm2 log H)) = O(log log Lm2). Therefore, the overall time complexity of our proposed vocabulary construction algorithm is O(log log Lm2), where the iterative merging phase dominates. This complexity is significantly better than the naive vocabulary construction complexity of O(QN Lm2). 14 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Generate random permutation of Ai as [c1, c2, . . . , cAi] Extend with [c1, c2, . . . , cAi] # concatenate permutations Algorithm 4 Segmentation via Set Permutation Regularization (SPR) (Section 3.2.2) input Action sequence S, merge rules output Segmented token sequences 1: [ ] # initialize permuted initial token sequence 2: for all token set Ai do 3: 4: 5: end for 6: 7: # Apply BPE (Sennrich et al., 2016) segmentation on permuted sequence 8: repeat 9: # candidate merge rules for 0 to 1 do 10: 11: 12: 13: 14: 15: 16: 17: until is return end if end for Select {(ck, ck+1) c} with the smallest index among all merge rules [c1, . . . , ck1, c, ck+2, . . . ] # replace (ck, ck+1) with new token if {(ci, ci+1) c} then R {(ci, ci+1) c} Table 5. Comparison between ActionPiece and BPE. Aspect BPE ActionPiece Data Type Token Initial Vocabulary Merging Unit Co-occurrence Weighting Segmentation Strategy Intermediate Structures text sequences byte sequence single bytes adjacent byte pairs raw frequency counting greedy fixed-order merging N/A action (unordered feature set) sequences feature set single-feature sets feature pairs within one set or between adjacent sets probabilistic weighting (Figure 2) set permutation regularization (Algorithm 4) intermediate nodes for cross-action merges Table 6. Statistics of the processed datasets. Avg. denotes the average number of actions in an action sequence. Datasets Sports Beauty CDs #Users 18,357 22,363 75,258 #Items 35,598 12,101 64,443 #Actions 260,739 176,139 1,022, Avg. 8.32 8.87 14.58 D. Discussion: Comparison Between ActionPiece and BPE While ActionPiece follows similar algorithmic framework as BPE, its design is fundamentally different because it is tailored for tokenizing action sequences. To clarify, we summarize the key differences in Table 5. E. Datasets Categories. Among all the datasets, Sports and Beauty are two widely used benchmarks for evaluating generative recommendation models (Rajput et al., 2023; Jin et al., 2024; Hua et al., 2023). We conduct experiments on these benchmarks to ensure fair comparisons with existing results. Additionally, we introduce CDs, which contains about 4 more interactions than Sports, making it larger dataset for evaluating the scalability of GR models. For CDs, we apply the same data processing strategy as the public benchmarks. The statistics of the processed datasets are shown in Table 6. Sequence truncation length. Following Rajput et al. (2023), we filter out users with fewer than 5 reviews and truncate 15 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation 1 def vocab_construction_iteration(max_heap, vocab, rules, pair2head): 2 \"\"\"Performs one iteration of efficient vocabulary construction. 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 Args: max_heap (PriorityQueue): Max-heap storing (co_occurrence, (c_u, c_v)) pairs. vocab (List[Tuple]): Current vocabulary with merge rules. rules (Dict): Merge rule {(c_u, c_v): c_new} mapping. pair2head (Dict): Inverted indices that store mappings from the token pair to all sequences that contain this token pair. \"\"\" # Get most frequent valid pair (c_u, c_v) from max-heap # Efficient version of \"Count\" in Algorithm 1 # Avoid recalculating co-occurrences for each iteration, by maintaining them in max-heap with the lazy update trick while not max_heap.empty(): co_occurrence, (c_u, c_v) = max_heap.get() if not is_outdated((c_u, c_v), co_occurrence): # outdated values are lazily removed. break # Create new token and update vocabulary c_new = len(vocab) vocab.append((c_u, c_v)) rules[(c_u, c_v)] = c_new # Update sequences containing (c_u, c_v) seq_ids = pair2head[(c_u, c_v)].copy() delta_counts = defaultdict(int) for sid in seq_ids: # IDs of affected sequences # Merge all (c_u, c_v) pairs in sequence new_seq = merge_sequence(seqs[sid], c_u, c_v, c_new) # replace pairs seqs[sid] = new_seq # Calculate pair co-occurrence changes new_freqs = count_pairs(new_seq) # get token co-occurrences of the updated sequence delta = diff_counts(new_freqs, old_freqs[sid]) # compute co-occurrence differences update_index(pair2head, delta, sid) old_counts[sid] = new_freqs # update inverted index # Accumulate global co-occurrence changes for p, cnt in delta.items(): delta_counts[p] += cnt # Lazy update max-heap with new co-occurrences for (c_u, c_v), delta in delta_counts.items(): if abs(delta) < eps: continue all_pair_freqs[(c_u, c_v)] += delta max_heap.put( (-all_pair_freqs[(c_u, c_v)], (c_u, c_v)) ) # eps: minimum update threshold # Global co-occurrences Figure 7. Pseudocode for single iteration of the efficient vocabulary construction algorithm, illustrating how max-heap with lazy updates is used to track and merge frequent token pairs. action sequences to maximum length of 20 for Generative methods, including ActionPiece. For ID-based and Feature + ID baselines, we set the maximum length to 50, as suggested in their original papers. Item text features. Following Rajput et al. (2023); Zheng et al. (2024a); Sheng et al. (2025), the first step for feature engineering is to combine multiple raw text features into single sentence for each item. Then, we use pretrained sentence embedding model to encode this sentence into vector representation. In all our implementations, we concatenate title, price, brand, feature, categories, and description, and use sentence-t5-base (Ni et al., 2022) as the sentence embedding model. 16 ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation The encoded sentence embeddings of 768 dimension are directly used as textual item representations for UniSRec. We quantize the sentence embeddings using residual quantization (RQ) (Rajput et al., 2023; Zeghidour et al., 2021; Zheng et al., 2024b) into three codes, each with 256 candidates. To prevent conflicts, we add an extra identification code. These four codes together serve as the RQ-based semantic IDs for TIGER and SPM-SID. For other baselines that require item features, such as FDSA, S3-Rec, VQ-Rec, HSTU, and our method, we follow Hou et al. (2023) and quantize the sentence embeddings using optimized product quantization (OPQ) (Ge et al., 2013). Except for VQ-Rec, where the sentence embeddings are quantized into 32 codes as suggested in the original paper, we quantize the sentence embeddings into 4 codes for all other methods to ensure fair comparison. The codebook size is 256 for each digit of code. For generative methods HSTU and ActionPiece, we also include an additional identification code to prevent conflicts. Note that, unlike RQ-based semantic IDs, features produced by product/vector quantization do not require specific order. F. Baselines We compare ActionPiece with the following representative baselines: F.1. ID-Based Sequential Recommendation Methods SASRec (Kang & McAuley, 2018) represents each item using its unique item ID. It encodes item ID sequences with self-attentive Transformer decoder. The model is trained by optimizing binary cross-entropy objective. BERT4Rec (Sun et al., 2019) also represents each item using its unique item ID. Unlike SASRec, it encodes sequences of item IDs with bidirectional Transformer encoder. The model is trained using masked prediction objective. F.2. Feature-Enhanced Sequential Recommendation Methods FDSA (Zhang et al., 2019) integrates item feature embeddings with vanilla attention layers to obtain feature representations. It then processes item ID sequences and feature sequences separately through self-attention blocks. S3-Rec (Zhou et al., 2020) first employs self-supervised pre-training to capture the correlations between item features and item IDs. Then the checkpoints are loaded and fine-tuned for next-item prediction, using only item IDs. VQ-Rec (Hou et al., 2023) encodes text features into dense vectors using pre-trained language models. It then applies product quantization to convert these dense vectors into semantic IDs. The semantic ID embeddings are pooled together to represent each item. Since the experiments are not performed in transfer learning setting, we omit the two-stage training strategy outlined in the original paper. Instead, we reuse the model architecture and train it from scratch using an in-batch contrastive loss with batch size of 256. F.3. Generative Recommendation Methods Each generative recommendation baseline corresponds to an action tokenization method described in Table 1. P5-CID (Hua et al., 2023) is an extension of P5 (Geng et al., 2022), which formulates recommendation tasks in text-to-text format. Building on P5, the authors explored several tokenization methods to index items for better recommendations. In this study, we use P5-CID as representative hierarchical clustering-based action tokenization method. It organizes the eigenvectors of the Laplacian matrix of user-item interactions into hierarchy and assigns cluster IDs at each level as item indices. When implementing this baseline method, we adopt the same model backbone as ActionPiece (encoder-decoder Transformers trained from scratch) and use the indices produced by P5-CID. TIGER (Rajput et al., 2023) encodes text features similarly to VQ-Rec but quantizes them into semantic IDs using RQ-VAE. The model is then trained to autoregressively predict the next semantic ID and employs beam search for inference. We use beam size of 50 in beam search to generate the top-K recommendations. LMIndexer (Jin et al., 2024) takes text as input and predicts semantic IDs. The text description of each item is first tokenized using text tokenizer. The resulting text tokens are then concatenated to form input action sequences. The ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation Table 7. Hyperparameter settings of ActionPiece for each dataset. Hyperparameter learning rate warmup steps dropout rate weight decay vocabulary size inference segments beam size num layers model ff num heads kv optimizer lr scheduler train batch size max epochs early stop patience Sports 0.005 10,000 0.1 0.15 40,000 5 50 4 128 1,024 6 64 adamw cosine 256 200 Beauty 0.001 10,000 0.1 0.15 40,000 5 50 4 128 1,024 6 64 adamw cosine 256 200 20 CDs 0.001 10,000 0.1 0.07 40,000 5 50 4 256 2,048 6 64 adamw cosine 256 200 20 model is trained with self-supervised objectives to learn the semantic IDs of target items. The reported results in Table 2 are taken from the original paper. We do not report the results of LMIndexer on the large dataset CDs because it does not converge under similar computing budget as the other methods. HSTU (Zhai et al., 2024) discretizes raw item features into tokens, treating them as input tokens for generative recommendation. The authors also propose lightweight Transformer layer that improves both performance and efficiency. For action tokenization, we use the same item features as our method and arrange them in specific order to form the tokenized tokens of each item. SPM-SID (Singh et al., 2024) first tokenizes each item into semantic IDs. It then uses the SentencePiece model (SPM) (Kudo & Richardson, 2018) to merge important semantic ID patterns within each item into new tokens in the vocabulary. While the original paper introduces this method for ranking models, we adapt it for the generative recommendation task. Specifically, we concatenate the SPM tokens as inputs, feed them into the T5 model, and autoregressively generate SPM tokens as recommendations. G. Implementation Details Baselines. The results of BERT4Rec, SASRec, FDSA, S3-Rec, TIGER, and LMIndexer on the Sports and Beauty benchmarks are taken directly from existing papers (Zhou et al., 2020; Rajput et al., 2023; Jin et al., 2024). For other results, we carefully implement the baselines and tune hyperparameters according to the suggestions in their original papers. We implement BERT4Rec, SASRec, FDSA, and S3-Rec using the open-source recommendation library RecBole (Zhao et al., 2021). For other methods, we implement them ourselves with HuggingFace Transformers (Wolf et al., 2020) and PyTorch (Paszke et al., 2019). We use FAISS (Douze et al., 2024) to quantize sentence representations. ActionPiece. We use an encoder-decoder Transformer architecture similar to T5 (Raffel et al., 2020). We use four layers for both the encoder and decoder. The multi-head attention module has six heads, each with dimension of 64. For the public benchmarks Sports and Beauty, we follow Rajput et al. (2023) and set the token embedding dimension to 128 and the intermediate feed-forward layer dimension to 1024. This results in total of 4.46M non-embedding parameters. For the larger CDs dataset, we use token embedding dimension of 256 and an intermediate feed-forward layer dimension of 2048, leading to 13.11M non-embedding parameters. For model inference, we use beam search with beam size of 50. Note that the baselines P5-CID, TIGER, and SPM-SID use the same model architecture, differing only in their action tokenization methods. For ActionPiece-specific hyperparameters, we set the number of segmentations produced using set permutation regularization during inference to = 5. We tune the vocabulary size in {5k, 10k, 20k, 30k, 40k}. Training. We train the GR models from scratch for up to 200 epochs, using early stopping if the model does not achieve better NDCG@10 on the validation set for 20 consecutive epochs. The training batch size is set to 256. The learning rate is ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation selected from {1 103, 3 103, 5 103} with warmup step of 10,000. We use dropout rate of 0.1 and tune the weight decay from {0.07, 0.1, 0.15, 0.2}. For all methods implemented by us, we conduct five repeated experiments using random seeds {2024, 2025, 2026, 2027, 2028}. The model checkpoints with the best average NDCG@10 on the validation set are selected for evaluation on the test set, and we report these results. Each model is trained on single 40G NVIDIA A100 GPU. H. Reproduction To improve reproducibility, we provide the algorithms of vocabulary construction and segmentation processes in Algorithms 1 and 2 to 4. We also provide the pseudocode for the efficient vocabulary construction implementation in Figure 7. In addition, we provide the best hyperparameters of ActionPiece for all experimental datasets in Table 7."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "University of California, San Diego"
    ]
}