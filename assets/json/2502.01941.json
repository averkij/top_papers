{
    "paper_title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
    "authors": [
        "Xiang Liu",
        "Zhenheng Tang",
        "Hong Chen",
        "Peijie Dong",
        "Zeyu Li",
        "Xiuze Zhou",
        "Bo Li",
        "Xuming Hu",
        "Xiaowen Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$ performance improvements on long-context generation tasks under aggressive compression ratios."
        },
        {
            "title": "Start",
            "content": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Xiang Liu 1 Zhenheng Tang 2 Hong Chen 1 Peijie Dong 1 Zeyu Li 1 Xiuze Zhou 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1 5 2 0 2 4 ] . [ 1 1 4 9 1 0 . 2 0 5 2 : r Abstract This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs fundamental capabilities. While existing methods achieve impressive compression ratios on longcontext benchmarks, their effects on core model capabilities remain understudied. We present comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instructiontuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios. 1. Introduction The emergence of Key-Value (KV) cache compression techniques has become crucial for efficient LLM deployment, 1The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China 2The Hong Kong University of Science and Technology, Hong Kong, China. Correspondence to: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu <xwchu@hkust-gz.edu.cn>. (a) KV cache compression methods on long-context and arithmetic benchmarks. (b) Attention heatmap on long-context and arithmetic benchmarks. Figure 1: KV cache compression methods on long-context and arithmetic benchmarks. (a) Arithmetic benchmark shows more performance degradation than long-context benchmark. (b) Long-Context benchmark shows more sparsity in attention heatmap. primarily due to the increasing demands of memory management during inference. This need arises from the significant evolution of Large Language Models (LLMs), which now excel at processing extensive documents spanning thousands of tokens for tasks like question answering and summarization (Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Tay et al., 2022; Touvron et al., 2023a;b). The ability to handle such lengthy inputs has been transformed by two concurrent developments: breakthroughs in ML system architectures for processing long sequences (Dao et al., 2022; Dao, 2024; Jacobs et al., 2023; Xiao et al., 2024), and innovations in model design (Chen et al., 2023a; Xiong et al., 2024; Chen et al., 2023b; Peng et al., 2024). However, this enhanced capability comes with significant 1 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? computational costs; as context lengths grow, the GPU memory requirements for inference operations increase substantially (AI21, 2024; X.AI, 2024; Reid et al., 2024; Anthropic, 2024; DeepSeek-AI, 2024; Liu et al., 2024a). This challenge has made the development of efficient KV cache compression strategies critical focus in the field of LLM deployment and optimization. Multiple studies have proposed innovative approaches to address this challenge through selective token retention strategies (Xiao et al., 2024; Zhang et al., 2023; Li et al., 2024b; Ge et al., 2023; Cai et al., 2024; Fu et al., 2024; Yang et al., 2024; Adnan et al., 2024; Liu et al., 2024e; Tang et al., 2024; Liu et al., 2025). Pioneering works such as H2O (Zhang et al., 2023) and SnapKV (Li et al., 2024b) demonstrate that retaining approximately 50% of KV cache entries can maintain model performance while achieving substantial memory efficiency. However, current research primarily evaluates these methods on long-context scenarios like LongBench (Bai et al., 2023; 2025) and Need-In-AHaystack (NIAH) (Kamradt, 2023), leaving their broader impact on fundamental LLM capabilities largely unexplored. Our preliminary analysis, as shown in Figure 1, reveals two critical findings: (a) arithmetic reasoning tasks suffer significantly higher performance degradation under compression compared to long-context tasks, and (b) attention patterns in long-context scenarios exhibit notably higher sparsity than arithmetic tasks. These observations suggest that current evaluation frameworks, which focus predominantly on long-context performance, may inadequately capture the full spectrum of impacts on model capabilities. Further analysis of attention patterns, visualized in Figure 2, reveals distinct task-specific behaviors: while world knowledge and commonsense reasoning tasks exhibit universal attention distributions, arithmetic reasoning and safety tasks demonstrate more specialized patterns. Specifically, arithmetic reasoning tasks display increased attention sparsity, indicating focused attention on individual prompt examples, while safety tasks show concentrated attention on the system prompt. In contrast, world knowledge and commonsense reasoning tasks demonstrate more uniform attention distribution across the entire prompt. These varying attention patternsvisualized through colored squares representing system prompts, shot examples, and questionsprovide insights into task-specific context utilization and motivate our investigation into how compression affects various factors including model size, prompt length, and task type. Motivated by these initial findings, we conduct systematic study to comprehensively evaluate KV cache compression methods across different tasks (detailed in Section 4). Our analysis reveals several key findings: (1) Performance degradation is highly task-dependent, with arithmetic reasoning tasks showing particular sensitivity to aggressive compression; (2) Multi-step reasoning LLMs (O1 and R1) Figure 2: Attention heatmap on different tasks. The heatmap is generated by the attention scores of the 15-th layer of the LLaMA-3.1-8B-Instruct attention head 10. demonstrate higher compression robustness compared to instruction-tuned models; (3) Shorter prompts are more vulnerable to compression effects; (4) Chunk-level compression strategies show superior performance on complex long-context reasoning tasks; (5) Tasks with larger promptbased performance gains exhibit higher compression sensitivity; and (6) Long-context generation tasks are particularly sensitive to compression. These findings provide valuable insights into the relationship between compression methods and model capabilities, motivating our development of ShotKV. We hope our work can provide the research community with insightful perspectives on KV cache compressions impact on LLMs. To the best of our knowledge, we are the first to comprehensively evaluate and analyze the impact of KV cache compression on LLMs fundamental abilities. Our main contributions are summarized as follows: Through comprehensive evaluation of KV cache compression methods across diverse tasks and compression ratios, we demonstrate that task-specific sensitivity to compression varies significantly, with performance degradation Can LLMs Maintain Fundamental Abilities under KV Cache Compression? ranging from 1% to 40% under aggressive compression settings and multi-step reasoning LLM (DeepSeek R1) is more robust than instruction-tuned LLM. Our systematic investigation reveals multiple critical factors influencing compression sensitivity, including model training dynamics, prompt length characteristics, taskspecific requirements, long-context reasoning and longcontext generation capabilities. These findings provide generalizable insights applicable across various compression methodologies and model architectures. We introduce ShotKV, an innovative compression framework that distinctively manages prefill and decoding phases while maintaining shot-level semantic integrity. This approach demonstrates marked improvements in performance, particularly on complex long-context arithmetic reasoning and long-context generation tasks. 2. Related Works 2.1. Keyvalue Cache Optimization Techniques KV cache is the core component in LLM inference, which avoids repetitive computations by caching Key and Value vectors. However, the cost of caching KV increases exponentially with the expansions of the model size and context length (Pope et al., 2023). Some approaches have been published to alleviate the issue. For instance, KV Compression designs efficient content selection strategies to filter and manage tokens (Zhang et al., 2023; Adnan et al., 2024). Some methods identify important tokens by focusing on high attention allocation (Li et al., 2024b), while others optimize token selection by combining attention scores with value vector norms to improve importance evaluation (Guo et al., 2024). Techniques like PyramidInfer reduce critical contexts layer-by-layer based on the distribution of attention scores (Yang et al., 2024), and StreamingLLM preserves attention sinks to maintain stable performance in extended sequences (Xiao et al., 2024). Researchers reduce storage costs by merging similar context representations and solving input disturbances caused by compression (Zhang et al.). For instance, CaM (Zhang et al.) works by integrating the KV cache to be dropped into the retained cache in proportion to the attention weight. In addition, Yao et al. (2024) proposes CacheBlend to achieve selective KV recompute. Only partial KVs of crucial tokens are updated to reduce the delay in the prefill stage and increase the throughput. Besides, the dynamic budget allocation method is also used to optimize the KV Cache, which adjusts the resource allocation in real-time according to the importance of the context, providing balance between performance and efficiency in multi-task inference scenarios (Cai et al., 2024; Feng et al., 2024; Liu et al., 2025). 2.2. Evaluation of LLMs Fundamental Abilities Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. The evaluation typically spans across several key dimensions: world knowledge tasks like MMLU (Hendrycks et al., 2020),BBH (Suzgun et al., 2022) assess models grasp of diverse domains through multiplechoice questions; commonsense reasoning tasks such as CSQA (Talmor et al., 2019) evaluate inference and context understanding abilities; arithmetic reasoning benchmarks like GSM8K (Cobbe et al., 2021) test mathematical problem-solving capabilities through step-by-step reasoning; code generation tasks including HumanEval (Chen et al., 2021) measure the ability to generate functionally correct code; and safety evaluations using benchmarks like JailBreakV (Luo et al., 2024) assess models robustness against harmful content generation. Additionally, longcontext benchmarks such as LongBench (Bai et al., 2023; 2025) and Need-In-A-Haystack (NIAH) (Kamradt, 2023) aiming to evaluate models long-context summarization and retrieval capabilities. Furthermore, LongGenBench (Liu et al., 2024d) evaluate models abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as longcontext reasoning paradigm (Agarwal et al., 2024), which considers the number of shots as critical factor in the performance of LLMs. While these tasks typically employ automatic evaluation metrics for standardization, KV cache compression may introduce unique challenges, particularly in tasks requiring complex reasoning chains or extensive knowledge retrieval. 3. Preliminary In this section, we provide comprehensive preliminaries of KV cache compression and LLM evaluation. Key-Value Cache in LLMs With the increasing longcontext capabilities of LLMs, the Key-Value (KV) cache has become crucial for improving inference efficiency. During LLM inference, the KV cache stores intermediate computation results to avoid redundant calculations. For given input sequence = (x1, x2, ..., xn), each transformer layer maintains its key cache = (kl n) and value Rd represent 2, ..., vl 1, vl cache = (vl the key and value vectors for token xi at layer l. 1, kl n), where kl 2, ..., kl i, vl KV Cache Compression KV cache compression aims to reduce memory usage by selectively storing or merging cached vectors. compression operation can be denoted as C(K, ) = (K , ), where and are compressed caches with size < n, where is the compression method, is the number of retained tokens, and is the original number of tokens. The core idea is token selection - identifying and retaining important tokens based on 3 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? attention patterns or other metrics while discarding less important ones. The compression ratio = m/n indicates how aggressively the cache is compressed, where smaller ratio means more aggressive compression. Evaluation Environment We use the lm-evaluationharness (Gao et al., 2023) library to load the models and evaluate the performance. The evaluation environment is NVIDIA A40 GPU server. Evaluation Protocol To thoroughly evaluate the impact of KV cache compression on LLMs capabilities, we assess five benchmark categories: world knowledge, commonsense reasoning, arithmetic reasoning, code generation, and safety. For each task category and compression method C, we calculate the relative performance change as follows: = PC Pbase Pbase (1) where PC and Pbase represent the performance scores with and without compression, respectively. 4. Experiments Design 4.1. Experimental Setups In this section, we will introduce the experimental setups, including the datasets, models, and evaluation environment. Datasets To evaluate the performance of KV cache compression on LLMs overarching capabilities, we assess five benchmark categories: World Knowledge using MMLU (Hendrycks et al., 2020), measured by accuracy; CommonSense Reasoning using CommonsenseQA (Talmor et al., 2019) , evaluated through multiple-choice accuracy; Arithmetic Reasoning using GSM8K (Cobbe et al., 2021), assessed by solve rate; Code Generation using HumanEval (Chen et al., 2021), measured by pass@1 rate on test cases; and Safety using JailBreakV (Luo et al., 2024), evaluated by attack success rate. Furthermore, we test the performance of KV cache compression on LongGenBench (Liu et al., 2024d), long-context generation benchmark. Models We conduct experiments on series of LLMs, including LLaMA-3.1-8B, LLaMA-3.1-8B-Instruct (Dubey et al., 2024), and multi-step reasoning LLM DeepSeek-R1Distill-Llama-8B (Guo et al., 2025). KV Cache Compression Methods To comprehensively impact on KV cache cominvestigate the potential pression methods, we select the following methods: StreamingLLM (Xiao et al., 2024), SnapKV (Li et al., 2024b), H2O (Zhang et al., 2023), PyramidKV (Cai et al., 2024), PyramidInfer (Yang et al., 2024), and ChunkKV (Liu et al., 2025). Hyperparameters For the experiments on observation 1, 2 we set the normal shot number as in the original paper. For the experiments on observation 3, 4, 5, the shot number are dependent on the experiment settings. More details are shown in Appendix B.1. 4.2. Results and Analysis In this section, we will present the results and analysis of the experiments. For comprehensive results, please refer to Appendix B.2. (a) Sensitivity Analysis of Different Benchmark Categories to KV Cache Compression (b) Performance Delta Lines with Baseline Figure 3: Sensitivity Analysis of Different Benchmark Categories to KV Cache Compression. The performance delta lines are calculated by Equation (1). Observation 1. KV cache compression methods show task-dependent performance degradation, with varying sensitivity thresholds across different benchmark categories. As demonstrated in Figure 3, all tasks maintain stable performance at compression ratios above 40%, but exhibit distinct degradation patterns below this threshold. GSM8K, HumanEval, and JailBreakV tasks demonstrate the highest compression sensitivity, characterized by precipitous performance declines. Figure 4 illustrates the detailed performance impact of various KV cache compression 4 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Figure 4: Performance Comparison of KV Cache Compression Methods Across Tasks. Note: The y-axis scales vary across different tasks. Results for R1-GSM8K (e) were obtained using the DeepSeek-R1-Distill-Llama-8B model. methods across different tasks. This degradation is most pronounced in GSM8K (d), where performance deteriorates significantly below 20% compression ratio, with accuracy dropping from approximately 0.75 to below 0.5. Among the evaluated methods, ChunkKV (Liu et al., 2025) and PyramidKV (Cai et al., 2024) consistently demonstrate superior stability across most tasks, while StreamingLLM (Xiao et al., 2024) exhibits heightened sensitivity to aggressive compression. Additionally, R1-GSM8K (e) indicates that R1 LLMs demonstrate enhanced robustness to KV cache compression. Observation 2. Multi-step reasoning LLMs are more robust to KV cache compression. Figure 5 presents comparative analysis of LLaMA-3.1-8B across its base (w/o instruct tuned), instruct-tuned, and DeepSeek-R1 distilled variants, illustrating their averaged performance across five compression methods with confidence intervals. Although all three variants exhibit performance degradation at low compression ratios, their degradation trajectories differ significantly. The R1 distilled model demonstrates superior stability, maintaining performance around 0.60 even at 10% compression ratio. While the instruct-tuned model achieves Figure 5: Performance Comparison of KV Cache Compression Methods on different training dynamics on GSM8K higher initial accuracy (0.8), it exhibits heightened compression sensitivity, with performance deterioration initiating at 30% compression ratio and declining sharply to approximately 0.5 at 10% ratio. These findings suggest that while multi-step reasoning LLMs demonstrate enhanced robustness to KV cache compression, and instruct-tuning improves overall model performance, the latter may inad5 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? vertently increase model vulnerability to aggressive compression, particularly at compression ratios below 30%. (a) Many-shot GSM8K on LLaMA3.1-8B-Instruct Figure 6: Average Performance Across Different Shot Numbers Observation 3. Short prompt length is more sensitive to KV cache compression. As shown in Figure 6, the impact of KV cache compression varies significantly with different prompt lengths (shot numbers). One-shot and two-shot scenarios demonstrate higher sensitivity to compression, with performance dropping more sharply below 30% compression ratio compared to scenarios with more shots (4-8). For instance, in 1-shot settings, performance decreases from 0.5 to 0.05 when compression ratio drops from 30% to 10%, while 8-shot settings show only drop from 0.75 to 0.5 reduction under the same conditions. This suggests that longer prompts provide more information that helps maintain model performance even under aggressive compression. The increased robustness with higher shot numbers may be attributed to the models ability to leverage multiple examples, making it less dependent on perfect preservation of each individual example in the compressed KV cache. Observation 4. Chunk-level compression is more effective for long-context arithmetic reasoning tasks. Inspired by Agarwal et al. (2024), we consider many-shot in-context learning as long-context reasoning task, which is more complex than existing long-context benchmarks, such as LongBench and NIAH. Figure 7 shows the performance of KV cache compression methods on 50-shot GSM8K task, where the prompt length exceeds 4K tokens. From the figure, we observe that ChunkKV (Liu et al., 2025) demonstrates the most stability when the compression ratio is below 10% on both LLaMA-3.1-8B-Instruct and DeepSeek-R1-DistillLlama-8B, indicating that in more complex long-context arithmetic reasoning tasks, chunk-level retention is more effective at preserving semantic information. Observation 5. Tasks with larger prompt-based performance gains show higher sensitivity to KV cache compression. As shown in Table 1, different tasks exhibit varying levels of performance improvement from zero-shot to (b) Many-shot GSM8K on DeepSeek-R1-Distill-Llama-8B Figure 7: Many-shot scenario on KV cache compression few-shot prompting. GSM8K shows dramatic improvement of 50.41%, while MMLU demonstrates more modest gain of 6.20%. From Figure 3, we find that tasks with larger prompt-based improvements, such as GSM8K, are more sensitive to KV cache compression. This suggests that when task heavily relies on in-context examples to achieve better performance, the compression of these crucial prompt elements has more substantial impact on model performance. In contrast, tasks like MMLU, where the performance gain from prompting is smaller, show more resilience to KV cache compression, likely because the model relies more on its inherent knowledge rather than the specific examples in the prompt. Table 1: Zero-shot vs Few-shot Performance Comparison Benchmark Zero-shot Few-shot Delta GSM8K MMLU 0.2904 0.6262 0.7945 0.6882 +0.5041 +0.0620 Observation 6. KV cache compression exhibits significant performance degradation on Long-Context Generation tasks. As demonstrated in Table 2, our evaluation of 6 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? three unified compression methodsStreamingLLM, H2O, and PyramidInferon LongGenBench-GSM8K reveals substantial performance limitations. On this arithmetic reasoning task with approximately 4k token generation length, the compression methods show notable deterioration, with performance declining by more than 20% at compression ratios below 30%. The ShotKV is our proposed method aiming to improve the performance of KV cache compression on Long-Context Generation tasks, details in Section 5. Table 2: KV cache compression methods performance on LongGenBench-GSM8K where si represents the i-th shot example containing ki tokens, and αl t,h denotes the attention score for token in head at layer l. Once the prefill phase KV cache is compressed, it remains fixed throughout the generation process. Given prefill compression ratio rp, we prioritize shots with higher scores while ensuring the total number of preserved tokens does not exceed the KV cache limit. Specifically, shots are ranked by their scores and selected in descending order until reaching the compression budget rp KVprefill. This shot-level selection strategy helps maintain the semantic coherence of important examples while adhering to memory constraints. Method FullKV 100% 40% 35% 30% 25% 46.00 - - - - StreamingLLM H2O PyramidInfer ShotKV(Ours) - - - - 39.50 32.66 38.33 47.33 28.67 25.17 27.67 41. 14.83 19.83 20.50 38.33 6.33 14.83 16.67 26.83 5. ShotKV Based on our comprehensive analysis, we find that current unified KV cache compression methods exhibit significant performance degradation on Long-Context Generation tasks. In this section, we introduce ShotKV, decoding-time compression method designed to mitigate this degradation. Our approach is motivated by two key insights: (1) Figure 2 demonstrates that n-shot example prompts receive substantial attention in reasoning benchmarks, and (2) Observation 4 reveals that chunk-level compression is particularly effective for long-context arithmetic reasoning tasks. Based on these findings, we hypothesize that each shot example represents coherent chunk of information, leading us to design ShotKV to preserve shot examples intact during decoding time. 5.1. Implementation The ShotKV (Prefill-Decoding Separated Shot-aware KV Cache Compression), which separates the compression strategy for prefill and decoding phases. The key insight is that the prefill phase KV cache, which contains crucial prompt information, should be compressed once and remain fixed, while the decoding phase KV cache can be dynamically compressed with different strategies. Given prompt with shots and generated tokens, we define: KVtotal = KVprefill KVdecoding For the prefill phase, we compute shot importance and preserve complete shot examples: (2) Scoreprefill(si) = 1 ki (cid:88) (cid:88) (cid:88) tsi h=1 l=1 αl t,h (3)"
        },
        {
            "title": "KV C",
            "content": "prefill = Compress({sisi preserved}) (4) where Spreserved = argmax S{s1,...,sn} (cid:88) siS Scoreprefill(si) (5) subject to: (cid:88) siS ki rp KVprefill (6) Here, KV prefill represents the compressed prefill KV cache, and Spreserved represents the optimal subset of shots to be preserved after compression. The first equation aims to maximize the total importance score of selected shots, where {s1, ..., sn} represents all available shots and Scoreprefill(si) is the importance score of shot si computed using attention weights as defined earlier. The second equation enforces the memory constraint: the total number of tokens (ki) in the selected shots must not exceed the allocated budget, which is determined by the prefill compression ratio rp multiplied by the original KV cache size. For the decoding phase, we compute importance scores only for the tokens generated during decoding: Scoredecoding(t) = (cid:88) (cid:88) h=1 l=1 αl t,h (7) Given decoding compression ratio rd, we select tokens with the highest scores to preserve. The compressed decoding KV cache KV decoding retains only the top-k tokens where = rd KVdecoding, effectively maintaining the most influential context tokens while reducing memory usage: KV decoding = TopK(KVdecoding, Scoredecoding, = rd KVdecoding) (8) Finally, we combine the compressed prefill and decoding KV caches to form the total compressed KV cache: KVtotal = KV prefill KV decoding (9) Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 5.2. Empirical Results In this section, we evaluate ShotKV under two scenarios: many-shot GSM8K with multiple KV cache compression methods, and LongGenBench-GSM8K with three unified compression methods that optimize the KV cache during generation. Baseline. For LongGenBench-GSM8K evaluation, we employ three state-of-the-art unified compression methods as baselines: StreamingLLM (Xiao et al., 2024), H2O (Zhang et al., 2023), and PyramidInfer (Yang et al., 2024). We conduct experiments using LLaMA-3-8B-Instruct (Dubey et al., 2024) on the LongGenBench-GSM8K benchmark (Liu et al., 2024d), maintaining consistent parameters with Observation 6 (K = 35, = 20). For many-shot GSM8K experiments, we follow the configuration detailed in Observation 4. Main results and analysis. From the Table 3, we can see that ShotKV achieves the best performance on LongGenBench-GSM8K, maintaining high performance at low compression ratios. Specifically, at compression ratio of 40%, ShotKV achieves 47.33% accuracy, surpassing the full kv cache baseline (46.00%) and showing substantial improvements over other methods (32.66%-39.50%). And Table 2 shows that ShotKV also achieves the best performance on many-shot GSM8K, maintaining high performance at low compression ratios. Even at aggressive compression ratios (25%-30%), ShotKV maintains relatively stable performance (26.83%-38.33%), while other methods experience more severe degradation (6.33%-16.67%). This superior performance can be attributed to two key design choices: (1) the preservation of complete shot examples during compression maintains the semantic coherence necessary for mathematical reasoning, and (2) the separation of prefill and decoding phase compression allows for more flexible and task-appropriate token retention strategies. These results suggest that our shot-aware compression strategy is particularly effective for long-context generation tasks that require maintaining complex reasoning chains, such as mathematical problem-solving. 6. Further Discussion Our comprehensive analysis of KV cache compression reveals several important implications and limitations that warrant further discussion: Table 3: KV cache compression methods performance on Many-shot GSM8K Method FullKV StreamingLLM H2O PyramidKV SnapKV ChunkKV ShotKV(Ours) 100% 40% 30% 20% 10% 0.8235 - - - - - - - - - - 0.8037 0.7832 0.7834 0.7935 0.7832 0.8107 0.7835 0.7932 0.7934 0.8038 0.7932 0.8082 0.7537 0.7428 0.7832 0.7934 0.7835 0.8057 0.7432 0.5127 0.7037 0.6827 0.7932 0. Implications for Model Design The higher sensitivity of instruct-tuned models to compression raises questions about the relationship between model training objectives and compression robustness. Future research might explore training techniques that explicitly optimize for compression resilience while maintaining instruction-following capabilities. Limitations Our study has several limitations: (1) The evaluation was conducted on specific benchmarks, and performance on other tasks may vary; (2) The computational resources required for comprehensive evaluation limited our ability to test on larger models and more diverse compression methods. 7. Conclusion This paper presents systematic study of KV cache compressions impact on LLMs core capabilities, revealing several key findings: (1) Performance degradation is highly task-dependent, with arithmetic reasoning tasks showing particular sensitivity to aggressive compression; (2) Instructtuned models demonstrate higher sensitivity to compression compared to their base counterparts; (3) Shorter prompts are more vulnerable to compression effects; (4) Chunklevel compression strategies show superior performance on complex long-context reasoning tasks; (5) Long-context generation tasks are more sensitive to compression than long-context reasoning tasks. Based on these insights, we proposed ShotKV, novel compression method that separately handles prefill and decoding phases while preserving shot-level semantic coherence. Our method demonstrates superior performance on long-context arithmetic reasoning tasks and long-context generation tasks, maintaining high accuracy even at low compression ratios. Trade-off between Memory Efficiency and Task Performance While KV cache compression methods effectively reduce memory usage, our findings highlight non-uniform impact across different tasks. This suggests that deployment strategies should carefully consider task-specific requirements when selecting compression ratios, potentially implementing adaptive compression based on the task type. These findings have important implications for the deployment of LLMs in resource-constrained environments and suggest several promising directions for future research, including: (1) Development of task-adaptive compression strategies; (2) Investigation of compression-aware training methods; and (3) Extension of compression techniques to other model architectures and modalities. 8 Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
        },
        {
            "title": "Impact Statement",
            "content": "This work advances the field of efficient large language model deployment through systematic analysis and improvement of KV cache compression techniques. Our research has several potential societal impacts: First, by enabling more efficient memory usage in LLMs while maintaining performance, our work contributes to reducing the computational resources and energy consumption required for AI deployment. This has positive environmental implications and makes AI technology more accessible to researchers and organizations with limited computing resources. Second, our proposed ShotKV method specifically improves performance on long-context arithmetic reasoning tasks, which could enhance the practical applications of LLMs in education, scientific computing, and other fields requiring complex mathematical reasoning. This could lead to more reliable AI-assisted learning and problem-solving tools. However, we acknowledge that making LLMs more efficient could accelerate their widespread adoption, potentially raising concerns about AIs impact on employment and privacy. While our work focuses on technical improvements, we encourage the research community to carefully consider these broader implications when deploying such technologies. We believe the benefits of more efficient and capable AI systems outweigh potential risks, particularly as our work promotes more sustainable and accessible AI development. Nevertheless, we emphasize the importance of responsible deployment and continued ethical consideration in the application of these technologies."
        },
        {
            "title": "References",
            "content": "Adnan, M., Arunkumar, A., Jain, G., Nair, P., Soloveychik, I., and Kamath, P. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Rosias, L., Chan, S., Zhang, B., Anand, A., Abbas, Z., Nova, A., et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. AI21. Introducing jamba: Ai21s groundbreaking ssmtransformer model, 2024. URL https://www.ai21. com/blog/announcing-jamba. An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. ArXiv preprint, abs/2307.11088, 2023. URL https://arxiv.org/ abs/2307.11088. Anthropic. 2024. news/claude-3-family. Introducing the next generation of claude, URL https://www.anthropic.com/ Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X., Cao, S., Xu, J., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks, 2025. URL https://arxiv.org/abs/2412.15204. Brandon, W., Mishra, M., Nrusimha, A., Panda, R., and Kelly, J. R. Reducing transformer key-value cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Cai, Z., Zhang, Y., Gao, B., Liu, Y., Liu, T., Lu, K., Xiong, W., Dong, Y., Chang, B., Hu, J., et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. ArXiv preprint, abs/2306.15595, 2023a. URL https://arxiv.org/abs/2306.15595. Amini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi, Y., and Hajishirzi, H. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context In The Twelfth International large language models. Conference on Learning Representations, 2023b. Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting In Bouamor, language models to compress contexts. H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL https:// aclanthology.org/2023.emnlp-main.232. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Deng, Y., Zhang, W., Pan, S. J., and Bing, L. Multilingual jailbreak challenges in large language models. arXiv preprint arXiv:2310.06474, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., and Han, W. Extending context window of large language models via semantic compression. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 51695181, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 306. URL https://aclanthology.org/2024. findings-acl.306. Feng, Y., Lv, J., Cao, Y., Xie, X., and Zhou, S. K. Adakv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. Fu, Q., Cho, M., Merth, T., Mehta, S., Rastegari, M., and Najibi, M. LazyLLM: Dynamic token pruning for efIn Workshop on ficient long context LLM inference. Efficient Systems for Foundation Models II @ ICML2024, 2024. URL https://openreview.net/forum? id=gGZD1dsJqZ. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. Model tells you what to discard: Adaptive kv cache compression for llms. ArXiv preprint, abs/2310.01801, 2023. URL https://arxiv.org/abs/2310.01801. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Guo, Z., Kamigaito, H., and Watanabe, T. Attention score is not all you need for token importance indicator in kv cache reduction: Value also matters. arXiv preprint arXiv:2406.12335, 2024. Hartvigsen, T., Gabriel, S., Palangi, H., Sap, M., Ray, D., and Kamar, E. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? ArXiv preprint, abs/2404.06654, 2024. URL https: //arxiv.org/abs/2404.06654. Jacobs, S. A. et al. DeepSpeed Ulysses: System optimizations for enabling training of extreme long sequence Transformer models. ArXiv preprint, abs/2309.14509, URL https://arxiv.org/abs/2309. 2023. 14509. 10 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. LLMLingua: Compressing prompts for accelerated inference of large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1335813376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. URL https:// aclanthology.org/2023.emnlp-main.825. Jiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16581677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.acl-long.91. Kamradt, G. Needle testing LLMs. sure https://github.com/gkamradt/LLMTest_ NeedleInAHaystack/tree/main. In Haystack - presGithub, URL 2023. Li, D., Shao, R., et al. How long can open-source LLMs truly promise on context length?, 2023. URL https: //lmsys.org/blog/2023-06-29-longchat. Li, Q., Liu, X., Tang, Z., Dong, P., Li, Z., Pan, X., and Chu, X. Should we really edit language models? on the evaluation of edited language models. arXiv preprint arXiv:2410.18785, 2024a. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. ArXiv preprint, abs/2404.14469, 2024b. URL https://arxiv.org/abs/2404.14469. Liang, P. P., Wu, C., Morency, L.-P., and Salakhutdinov, R. Towards understanding and mitigating social biases in language models. In International Conference on Machine Learning, pp. 65656576. PMLR, 2021. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B. Minicache: Kv cache compression in depth dimension for large language models. arXiv preprint arXiv:2405.14366, 2024b. 11 Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024c. doi: 10.1162/tacl 00638. URL https:// aclanthology.org/2024.tacl-1.9. Liu, X., Dong, P., Hu, X., and Chu, X. LongGenBench: Long-context generation benchmark. In AlOnaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 865883, Miami, Florida, USA, November 2024d. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.findings-emnlp.48. Liu, X., Tang, Z., Dong, P., Li, Z., Li, B., Hu, X., and Chu, X. Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference, 2025. URL https: //arxiv.org/abs/2502.00299. Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024e. Luo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=GC4mXVfquq. Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. ArXiv preprint, abs/2305.16300, 2023. URL https: //arxiv.org/abs/2305.16300. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=wHBfxhZu1u. Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5:606624, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. URL http://jmlr.org/papers/v21/20-074. html. Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy, O. ZeroSCROLLS: zero-shot benchmark for In Bouamor, H., Pino, J., long text understanding. and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 7977 7989, Singapore, 2023. Association for Computational doi: 10.18653/v1/2023.findings-emnlp. Linguistics. 536. URL https://aclanthology.org/2023. findings-emnlp.536. Shen, X., Chen, Z., Backes, M., Shen, Y., and Zhang, Y. do anything now: Characterizing and evaluating inthe-wild jailbreak prompts on large language models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, pp. 16711685, 2024. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Sun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma, S., Zhang, Q., Wang, J., and Wei, F. You only cache once: Decoder-decoder architectures for language models. arXiv preprint arXiv:2405.05254, 2024. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421/. Tang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han, S. Quest: Query-aware sparsity for efficient long-context llm inference. ArXiv preprint, abs/2406.10774, 2024. URL https://arxiv.org/abs/2406.10774. 12 Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : benchmark for efficient transIn 9th International Conference on Learnformers. ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https: //openreview.net/forum?id=qVyeW-grC2k. Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D., Schuster, T., Zheng, H. S., Houlsby, N., and Metzler, D. Unifying language learning paradigms. ArXiv preprint, abs/2205.05131, 2022. URL https://arxiv.org/ abs/2205.05131. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. URL https://arxiv.org/abs/2307.09288. Wang, Q., Ding, L., Cao, Y., Tian, Z., Wang, S., Tao, D., and Guo, L. Recursively summarizing enables long-term dialogue memory in large language models. arXiv preprint arXiv:2308.15022, 2023. Wingate, D., Shoeybi, M., and Sorensen, T. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 56215634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp. 412. URL https://aclanthology.org/2022. findings-emnlp.412. Wu, H. and Tu, K. Layer-condensed kv cache for efficient inference of large language models, 2024. URL https: //arxiv.org/abs/2405.10637. X.AI. Announcing grok-1.5, 2024. URL https://x. ai/blog/grok-1.5. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=NG7sS51zVF. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, Can LLMs Maintain Fundamental Abilities under KV Cache Compression? S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 46434663, Mexico City, Mexico, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. naacl-long.260. Yang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and Zhao, H. Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. arXiv preprint arXiv:2405.12532, 2024. Yao, J., Li, H., Liu, Y., Ray, S., Cheng, Y., Zhang, Q., Du, K., Lu, S., and Jiang, J. Cacheblend: Fast large language model serving with cached knowledge fusion. arXiv preprint arXiv:2405.16444, 2024. Yuan, J., Liu, H., Zhong, S., Chuang, Y.-N., Li, S., Wang, G., Le, D., Jin, H., Chaudhary, V., Xu, Z., et al. Kv cache compression, but what must we give in return? comprehensive benchmark of long context capable approaches. arXiv preprint arXiv:2407.01527, 2024. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K., Han, X., Thai, Z. L., Wang, S., Liu, Z., et al. -bench: Extending long context evaluation beyond 100k tokens. ArXiv preprint, abs/2402.13718, 2024. URL https: //arxiv.org/abs/2402.13718. Zhang, Y., Du, Y., Luo, G., Zhong, Y., Zhang, Z., Liu, S., and Ji, R. Cam: Cache merging for memory-efficient llms inference. In Forty-first International Conference on Machine Learning. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y., Cotterell, R., and Sachan, M. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023. Zhu, K., Wang, J., Zhou, J., Wang, Z., Chen, H., Wang, Y., Yang, L., Ye, W., Zhang, Y., Zhenqiang Gong, N., et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv e-prints, pp. arXiv2306, 2023. 13 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? A. Additional Related work KV cache sharing Recent work has explored various strategies for sharing KV caches across transformer layers. LayerCondensed KV Cache (LCKV) (Wu & Tu, 2024) computes KVs only for the top layer and pairs them with queries from all layers, while optionally retaining standard attention for few top and bottom layers to mitigate performance degradation. Similarly, You Only Cache Once (YOCO) (Sun et al., 2024) computes KVs exclusively for the top layer but pairs them with queries from only the top half of layers, employing efficient attention in the bottom layers to maintain constant cache size. In contrast, Cross-Layer Attention (CLA) (Brandon et al., 2024) divides layers into groups, pairing queries from all layers in each group with KVs from that groups bottom layer. MiniCache (Liu et al., 2024b) introduces novel method that merges layer-wise KV caches while enabling recovery during compute-in-place operations, optimizing KV cache size. These methods illustrate various trade-offs between computation, memory usage, and model performance when sharing KV caches across transformer layers. Prompting Compression Recent advances in prompt compression have yielded innovative approaches to information density optimization in natural language processing. Research by Wingate et al. (2022) demonstrates how soft prompting techniques can achieve higher information density per token. Building upon this foundation, AutoCompressor (Chevalier et al., 2023) leverages soft prompts to both condense input sequences and expand model context windows. Parallel developments by Zhou et al. (2023) and Wang et al. (2023) showcase iterative summarization strategies using LLMs, establishing persistent memory mechanisms particularly beneficial for narrative construction and conversational systems. The progressive development of the LLMLingua framework (Jiang et al., 2023; 2024; Fei et al., 2024) has advanced prompt compression capabilities across extended context processing, logical reasoning, and retrieval-augmented generation. Notable contributions from Fei et al. (2024) demonstrate effective context management through automated segmentation and semantic condensation using pre-trained language models. General Tasks General tasks refer to evaluating the overall performance of LLMs under mathematical inference, logic reasoning, and common knowledge GSM8K (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2020) are the representative tasks. The former focuses on the step-by-step reasoning ability of mathematical problem solving while the latter covers assessment of common sense and expertise in multiple areas. Besides, MATH (Hendrycks et al., 2021) spans various mathematical fields, ranging from elementary algebra to calculus, aiming to improve the mathematical problem-solving capabilities of LLMs. Meanwhile, MathQA (Amini et al., 2019) is large-scale dataset comprising approximately 37,000 multiple-choice questions with precise annotations, designed to enhance the interpretability and performance of LLMs. In addition, BBH (Suzgun et al., 2022), subset of BIG-Bench (Srivastava et al., 2022), focuses on challenging tasks. BBH includes multi-step reasoning problems, highlighting the importance of Chain-of-Thought prompting in LLMs. Similarly, CSQA (Talmor et al., 2019) is task that combines knowledge graph-based multi-step reasoning with conversational capabilities. CSQA emphasizes inference and context understanding grounded in knowledge graphs. Normally, the general tasks apply automatic evaluation metrics (e.g. multi-choice accuracy) to ensure comparability and standardization. However, optimization strategies like KV cache compression may introduce challenges in executing the mentioned tasks. Filtering and dropping of contexts are involved in the compression strategy which may lead to an intermediate inference steps missing. In addition, in tasks such as MMLU that are highly dependent on knowledge coverage, compression may weaken the models ability to capture long context or rare domain knowledge (Yuan et al., 2024). Security Tasks Security tasks focus on assessing the robustness and protections of LLMs against harmful content, including truthfulness (Lin et al., 2021), toxicity (Hartvigsen et al., 2022), and bias (Liang et al., 2021). Recently, researchers noticed the weakness of LLMs in adversarial prompts (Zhu et al., 2023), especially in generating illegal or inappropriate content under jailbreak prompts. Shen et al. (2024) analyze the jailbreak prompts in real cases to reveal the failure of model security mechanism under complex malicious input. Meanwhile, Deng et al. (2023) demonstrates the multilingual jailbreak makes model security in low-resource languages easier to bypass, significantly increasing the probability that users of low-resource languages will generate insecure content. Similar to general tasks, KV optimization techniques can cause the model to ignore potential security threats when dealing with jailbreak prompts, thereby improving the success rate of adversarial prompts (Li et al., 2024a). Code Generation Tasks Code generation tasks test the capacities of LLMs to generate code, which not only requires that the model can generate syntactic code based on natural language description but also has certain logical reasoning abilities. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are the commonly used benchmarks. They measure the 14 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? functional correctness of the model by testing the results of the codes execution. Long-context Tasks Recent developments in evaluating long-context models have produced comprehensive ecosystem of benchmarks, focusing on both comprehension depth and retrieval efficiency. In the comprehension domain, -Bench (Zhang et al., 2024) has established new standards by crafting evaluation scenarios exceeding 100,000 tokens, while LongBench (Bai et al., 2023; 2025) introduced multilingual assessment frameworks spanning document comprehension, text synthesis, and programming tasks. Further enriching this landscape, ZeroSCROLLS (Shaham et al., 2023) and L-Eval (An et al., 2023) have expanded evaluation criteria to encompass real-world applications, particularly in query-based content summarization. The emergence of many-shot learning as distinct paradigm for extended context processing (Agarwal et al., 2024) has added another dimension to this field. Notable contributions from LongGenBench (Liu et al., 2024d) have advanced evaluation methodologies by combining extensive response generation requirements with efficient, cost-effective quality metrics. The development of retrieval-focused benchmarks has taken distinct approach, predominantly utilizing constructed datasets that enable precise experimental control, particularly in managing input sequence lengths. This methodology helps neutralize variations in model performance stemming from differences in training approaches. Substantial research efforts have yielded specialized synthetic frameworks for assessing retrieval capabilities (Kamradt, 2023; Mohtashami & Jaggi, 2023; Li et al., 2023; Liu et al., 2024c; Hsieh et al., 2024), while concurrent investigations have revealed the broader implications of extended context processing for enhanced reasoning capabilities (Tay et al., 2021). B. Experiment Details B.1. Hyper-parameters The hyper-parameters for different observations are shown in Table 4. The temperature for the experiments are set to 0 for ensuring the deterministic results. Table 4: Hyperparameters for Different Observations"
        },
        {
            "title": "Benchmarks",
            "content": "Obs 1 Obs 2 Obs 3 Obs 4 Obs 5 Obs"
        },
        {
            "title": "K T",
            "content": "MMLU (Hendrycks et al., 2020) CommonsenseQA (Talmor et al., 2019) GSM8K (Cobbe et al., 2021) HumanEval (Chen et al., 2021) JailBreakV (Luo et al., 2024) LongGenBench-GSM8K (Liu et al., 2024d) 5 4 8 8 8 - 5 4 8 8 8 - - - 1-8 - - - - - 50 - - - 0,5 - 0,8 - - - - - - - - - - - - - 35 20 B.2. Detail Results This section provide the detailed results of experiments in this paper, the results are shown in the format of xy, where is the performance of the method and is the from the Equation (1). Observation 1. KV cache compression methods show task-dependent performance degradation, with varying sensitivity thresholds across different benchmark categories. The detailed results of different KV cache compression methods are shown in Table 6, different tasks exhibit notably varied sensitivities to KV cache compression, particularly under aggressive compression ratios. At 10% compression ratio, MMLU demonstrates remarkable resilience with less than 1% average performance degradation, while GSM8K experiences severe average performance drop exceeding 35%. Other tasks show moderate to significant degradation, ranging from 6.5% to 17.2%. This substantial variation in compression sensitivity across tasks suggests that the effectiveness of KV 15 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? cache compression is highly task-dependent, necessitating careful consideration of the specific task requirements when determining appropriate compression ratios. The Table 5 compares the performance of R1-Distill-Llama-8B and LLaMA-3.1-8B-Instruct under different compression ratios. R1-Distill-Llama-8B demonstrates more robust performance under compression compared to LLaMA-3.1-8BInstruct. While both models start with similar baseline performance (0.6938 vs 0.7945), R1-Distill shows significantly less performance degradation under aggressive compression. Specifically, at 30% compression ratio, R1-Distill maintains performance of 0.6407 (-7.66%), while LLaMA-3.1-8B-Instruct drops to 0.7469 (-6.00%). The difference becomes more pronounced at 10% compression ratio, where R1-Distill achieves 0.5840 (-15.82%) compared to LLaMA-3.1-8B-Instructs sharp decline to 0.5143 (-35.30%). This suggests that the multi-step reasoning capabilities of R1-Distill contribute to its resilience against aggressive KV cache compression, particularly in maintaining reasoning coherence under limited context conditions. Table 5: Performance Comparison of Different KV Cache Compression Methods on Instruction-Tuning Model and MultiStep Reasoning Model Benchmark Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV Average R1-GSM8K GSM8K Baseline R1-Distill-Llama-8B FullKV: 0.6938 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7167(+3.30%) 0.6867(1.02%) 0.6933(0.07%) 0.6833(1.51%) 0.6700(3.43%) 0.6767(2.47%) 0.6600(4.87%) 0.6200(10.64%) 0.5167(25.53%) 0.6900(0.55%) 0.6933(0.07%) 0.6633(4.40%) 0.6900(0.55%) 0.6967(+0.42%) 0.6800(1.99%) 0.5900(14.96%) 0.4933(28.90%) 0.5567(19.76%) 0.6933(0.07%) 0.6933(0.07%) 0.7100(+2.34%) 0.6900(0.55%) 0.7067(+1.86%) 0.5967(13.99%) 0.5833(15.93%) 0.5633(18.81%) 0.5767(16.88%) 0.7100(+2.34%) 0.7067(+1.86%) 0.7100(+2.34%) 0.7133(+2.81%) 0.7000(+0.89%) 0.6967(+0.42%) 0.6700(3.43%) 0.6833(1.51%) 0.6267(9.67%) 0.6867(1.02%) 0.6767(2.47%) 0.7000(+0.89%) 0.7067(+1.86%) 0.6867(1.02%) 0.7133(+2.81%) 0.7000(+0.89%) 0.6533(5.84%) 0.6433(7.28%) 0.6993(+0.79%) 0.6913(0.36%) 0.6953(+0.22%) 0.6967(+0.42%) 0.6920(0.26%) 0.6727(3.04%) 0.6407(7.66%) 0.6026(13.14%) 0.5840(15.82%) Baseline LLaMA-3.1-8B-Instruct FullKV: 0.7945 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7695(3.10%) 0.7642(3.80%) 0.7642(3.80%) 0.7650(3.70%) 0.7657(3.60%) 0.7491(5.70%) 0.7051(11.20%) 0.6384(19.70%) 0.4784(39.80%) 0.7923(0.30%) 0.7938(0.10%) 0.7900(0.60%) 0.7809(1.70%) 0.7854(1.10%) 0.7688(3.20%) 0.7225(9.10%) 0.6406(19.40%) 0.4503(43.30%) 0.7839(1.30%) 0.7824(1.50%) 0.7923(0.30%) 0.7885(0.80%) 0.7847(1.20%) 0.7756(2.40%) 0.7619(4.10%) 0.6884(13.40%) 0.5034(36.60%) 0.7854(1.10%) 0.7900(0.60%) 0.7983(+0.50%) 0.7923(0.30%) 0.7854(1.10%) 0.7839(1.30%) 0.7718(2.90%) 0.7142(10.10%) 0.4829(39.20%) 0.7824(1.50%) 0.7824(1.50%) 0.7809(1.70%) 0.7885(0.80%) 0.7824(1.50%) 0.7763(2.30%) 0.7733(2.70%) 0.7763(2.30%) 0.6566(17.40%) 0.7827(1.50%) 0.7826(1.50%) 0.7851(1.20%) 0.7830(1.50%) 0.7807(1.70%) 0.7707(3.00%) 0.7469(6.00%) 0.6916(13.00%) 0.5143(35.30%) Observation 2. Multi-step reasoning LLMs are more robust to KV cache compression. As shown in Table 7, while instruct-tuned models achieve superior baseline performance (0.7945 vs 0.5122), they demonstrate heightened sensitivity to KV cache compression. This sensitivity becomes particularly pronounced at aggressive compression ratios. At 10% compression ratio, instruct-tuned models suffer an average performance degradation of 35.3% (from 0.7945 to 0.5143), nearly double the degradation observed in non-instruct-tuned models which show 17.2% drop (from 0.5122 to 0.4244). In contrast, R1-Distill-Llama-8B shows better resilience to compression, with only 15.82% performance drop (from 0.6938 to 0.5840) at 10% compression ratio. This pattern suggests that while instruction tuning enhances model capabilities, it also makes the model more dependent on maintaining complete context information. However, models trained with multi-step reasoning capabilities like R1-Distill demonstrate better robustness against aggressive compression, likely due to their enhanced ability to maintain reasoning coherence even with limited context. Observation 3. Short prompt length is more sensitive to KV cache compression. As demonstrated in Table 8, the impact of KV cache compression varies significantly with the number of shots in the prompt. One-shot prompts show extreme vulnerability to aggressive compression, with performance plummeting from 0.7149 to 0.0452 (a 93.7% drop) at 10% compression ratio. This sensitivity gradually decreases as the number of shots increases. For instance, at the same compression ratio, 4-shot prompts show 46.2% performance drop (from 0.7597 to 0.4088), while 8-shot prompts demonstrate relatively better resilience with 35.3% reduction (from 0.7945 to 0.5143). This pattern suggests that longer 16 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Table 6: Performance Comparison of Different KV Cache Compression Methods on Multiple Benchmarks Benchmark Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV Average MMLU GSM8K CSQA JailBreakV HumanEval Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.6882(+0.00%) 0.6882(+0.00%) 0.6881(0.01%) 0.6881(0.01%) 0.6881(0.01%) 0.6879(0.04%) 0.6876(0.09%) 0.6859(0.33%) 0.6787(1.38%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6880(0.03%) 0.6878(0.06%) 0.6852(0.44%) 0.7695(3.10%) 0.7642(3.80%) 0.7642(3.80%) 0.7650(3.70%) 0.7657(3.60%) 0.7491(5.70%) 0.7051(11.20%) 0.6384(19.70%) 0.4784(39.80%) 0.7923(0.30%) 0.7938(0.10%) 0.7900(0.60%) 0.7809(1.70%) 0.7854(1.10%) 0.7688(3.20%) 0.7225(9.10%) 0.6406(19.40%) 0.4503(43.30%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7174(7.40%) 0.6806(12.20%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7510(3.10%) 0.8893(0.00%) 0.8878(0.20%) 0.8872(0.30%) 0.8845(0.60%) 0.8849(0.50%) 0.8734(1.80%) 0.8329(6.40%) 0.6501(26.90%) 0.5314(40.30%) 0.8890(0.10%) 0.8885(0.10%) 0.8879(0.20%) 0.8848(0.50%) 0.8749(1.60%) 0.8557(3.80%) 0.8015(9.90%) 0.7178(19.30%) 0.6544(26.40%) 0.5061(1.20%) 0.5061(1.20%) 0.5000(2.40%) 0.5061(1.20%) 0.4939(3.60%) 0.4817(6.00%) 0.4817(6.00%) 0.4634(9.50%) 0.3659(28.60%) 0.5183(+1.20%) 0.5183(+1.20%) 0.5244(+2.40%) 0.5366(+4.80%) 0.5427(+6.00%) 0.5427(+6.00%) 0.5305(+3.60%) 0.5061(1.20%) 0.4634(9.50%) FullKV: 0.6882 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6880(0.03%) 0.6880(0.03%) 0.6831(0.74%) FullKV: 0.7945 0.7839(1.30%) 0.7824(1.50%) 0.7923(0.30%) 0.7885(0.80%) 0.7847(1.20%) 0.7756(2.40%) 0.7619(4.10%) 0.6884(13.40%) 0.5034(36.60%) FullKV: 0. 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7740(0.10%) 0.7191(7.20%) FullKV: 0.8895 0.8894(0.00%) 0.8895(+0.00%) 0.8896(+0.00%) 0.8892(0.00%) 0.8886(0.10%) 0.8880(0.20%) 0.8858(0.40%) 0.8806(1.00%) 0.8434(5.20%) FullKV: 0.5122 0.5122(+0.00%) 0.5183(+1.20%) 0.5122(+0.00%) 0.5366(+4.80%) 0.5061(1.20%) 0.5244(+2.40%) 0.5000(2.40%) 0.4939(3.60%) 0.4268(16.70%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6842(0.58%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6882(+0.00%) 0.6881(0.01%) 0.6880(0.03%) 0.6876(0.08%) 0.6839(0.63%) 0.7854(1.10%) 0.7900(0.60%) 0.7983(+0.50%) 0.7923(0.30%) 0.7854(1.10%) 0.7839(1.30%) 0.7718(2.90%) 0.7142(10.10%) 0.4829(39.20%) 0.7824(1.50%) 0.7824(1.50%) 0.7809(1.70%) 0.7885(0.80%) 0.7824(1.50%) 0.7763(2.30%) 0.7733(2.70%) 0.7763(2.30%) 0.6566(17.40%) 0.7827(1.50%) 0.7826(1.50%) 0.7851(1.20%) 0.7830(1.50%) 0.7807(1.70%) 0.7707(3.00%) 0.7469(6.00%) 0.6916(13.00%) 0.5143(35.30%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7723(0.30%) 0.8893(0.00%) 0.8891(0.00%) 0.8889(0.10%) 0.8887(0.10%) 0.8884(0.10%) 0.8877(0.20%) 0.8899(+0.00%) 0.8751(1.60%) 0.8556(3.80%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7699(0.60%) 0.7002(9.60%) 0.8896(+0.00%) 0.8894(0.00%) 0.8895(+0.00%) 0.8899(+0.00%) 0.8894(0.00%) 0.8900(+0.10%) 0.8846(0.60%) 0.8902(+0.10%) 0.8799(1.10%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7748(+0.00%) 0.7622(1.60%) 0.7246(6.50%) 0.8893(0.00%) 0.8889(0.10%) 0.8886(0.10%) 0.8874(0.20%) 0.8852(0.50%) 0.8790(1.20%) 0.8589(3.50%) 0.8028(9.70%) 0.7529(15.40%) 0.5122(+0.00%) 0.5305(+3.60%) 0.5183(+1.20%) 0.5305(+3.60%) 0.4939(3.60%) 0.4939(3.60%) 0.4939(3.60%) 0.4695(8.30%) 0.4207(17.90%) 0.5122(+0.00%) 0.5061(1.20%) 0.5122(+0.00%) 0.5244(+2.40%) 0.4878(4.80%) 0.5000(2.40%) 0.4817(6.00%) 0.4878(4.80%) 0.4451(13.10%) 0.5122(+0.00%) 0.5159(+0.70%) 0.5134(+0.20%) 0.5268(+2.90%) 0.5049(1.40%) 0.5085(0.70%) 0.4976(2.90%) 0.4841(5.50%) 0.4244(17.20%) prompts with more examples provide redundancy that helps maintain model performance under compression, while shorter prompts lack this buffer against information loss. 17 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Table 7: KV Cache Compression Performance Comparison on GSM8K with Different Instruction TuningSettings Setting Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV Average w/ Instruct Tuning w/ R1 Distill w/o Instruct Tuning Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7695(3.10%) 0.7642(3.80%) 0.7642(3.80%) 0.7650(3.70%) 0.7657(3.60%) 0.7491(5.70%) 0.7051(11.20%) 0.6384(19.70%) 0.4784(39.80%) 0.7923(0.30%) 0.7938(0.10%) 0.7900(0.60%) 0.7809(1.70%) 0.7854(1.10%) 0.7688(3.20%) 0.7225(9.10%) 0.6406(19.40%) 0.4503(43.30%) FullKV: 0.7945 0.7839(1.30%) 0.7824(1.50%) 0.7923(0.30%) 0.7885(0.80%) 0.7847(1.20%) 0.7756(2.40%) 0.7619(4.10%) 0.6884(13.40%) 0.5034(36.60%) 0.7854(1.10%) 0.7900(0.60%) 0.7983(+0.50%) 0.7923(0.30%) 0.7854(1.10%) 0.7839(1.30%) 0.7718(2.90%) 0.7142(10.10%) 0.4829(39.20%) 0.7824(1.50%) 0.7824(1.50%) 0.7809(1.70%) 0.7885(0.80%) 0.7824(1.50%) 0.7763(2.30%) 0.7733(2.70%) 0.7763(2.30%) 0.6566(17.40%) 0.7827(1.50%) 0.7826(1.50%) 0.7851(1.20%) 0.7830(1.50%) 0.7807(1.70%) 0.7707(3.00%) 0.7469(6.00%) 0.6916(13.00%) 0.5143(35.30%) Baseline R1-Distill-Llama-8B FullKV: 0.6938 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7167(+3.30%) 0.6867(1.02%) 0.6933(0.07%) 0.6833(1.51%) 0.6700(3.43%) 0.6767(2.47%) 0.6600(4.87%) 0.6200(10.64%) 0.5167(25.53%) 0.6900(0.55%) 0.6933(0.07%) 0.6633(4.40%) 0.6900(0.55%) 0.6967(+0.42%) 0.6800(1.99%) 0.5900(14.96%) 0.4933(28.90%) 0.5567(19.76%) 0.5061(1.20%) 0.5061(1.20%) 0.5000(2.40%) 0.5061(1.20%) 0.4939(3.60%) 0.4817(6.00%) 0.4817(6.00%) 0.4634(9.50%) 0.3659(28.60%) 0.5183(+1.20%) 0.5183(+1.20%) 0.5244(+2.40%) 0.5366(+4.80%) 0.5427(+6.00%) 0.5427(+6.00%) 0.5305(+3.60%) 0.5061(1.20%) 0.4634(9.50%) 0.6933(0.07%) 0.6933(0.07%) 0.7100(+2.34%) 0.6900(0.55%) 0.7067(+1.86%) 0.5967(13.99%) 0.5833(15.93%) 0.5633(18.81%) 0.5767(16.88%) FullKV: 0.5122 0.5122(+0.00%) 0.5183(+1.20%) 0.5122(+0.00%) 0.5366(+4.80%) 0.5061(1.20%) 0.5244(+2.40%) 0.5000(2.40%) 0.4939(3.60%) 0.4268(16.70%) 0.7100(+2.34%) 0.7067(+1.86%) 0.7100(+2.34%) 0.7133(+2.81%) 0.7000(+0.89%) 0.6967(+0.42%) 0.6700(3.43%) 0.6833(1.51%) 0.6267(9.67%) 0.6867(1.02%) 0.6767(2.47%) 0.7000(+0.89%) 0.7067(+1.86%) 0.6867(1.02%) 0.7133(+2.81%) 0.7000(+0.89%) 0.6533(5.84%) 0.6433(7.28%) 0.6993(+0.79%) 0.6913(0.36%) 0.6953(+0.22%) 0.6967(+0.42%) 0.6920(0.26%) 0.6727(3.04%) 0.6407(7.66%) 0.6026(13.14%) 0.5840(15.82%) 0.5122(+0.00%) 0.5305(+3.60%) 0.5183(+1.20%) 0.5305(+3.60%) 0.4939(3.60%) 0.4939(3.60%) 0.4939(3.60%) 0.4695(8.30%) 0.4207(17.90%) 0.5122(+0.00%) 0.5061(1.20%) 0.5122(+0.00%) 0.5244(+2.40%) 0.4878(4.80%) 0.5000(2.40%) 0.4817(6.00%) 0.4878(4.80%) 0.4451(13.10%) 0.5122(+0.00%) 0.5159(+0.70%) 0.5134(+0.20%) 0.5268(+2.90%) 0.5049(1.40%) 0.5085(0.70%) 0.4976(2.90%) 0.4841(5.50%) 0.4244(17.20%) Observation 4. Chunk-level compression is more effective for long-context arithmetic reasoning tasks. As shown in Table 9, ChunkKV demonstrates superior robustness across different compression ratios, particularly under aggressive compression settings. While other methods show significant performance degradation at 10% compression ratio (StreamingLLM: -9.8%, H2O: -37.8%, SnapKV: -17.1%, PyramidKV: -14.6%), ChunkKV maintains relatively stable performance with only -3.7% drop. This stark contrast in performance suggests that chunk-level compression better preserves the essential contextual information needed for complex reasoning tasks. The methods effectiveness likely stems from its ability to maintain the structural integrity of related context segments, which is particularly crucial for tasks requiring extended logical reasoning and arithmetic operations. C. ShotKV This section provides the detailed description of ShotKV. C.1. Pseudocode The detailed algorithm of ShotKV is presented in Algorithm1. Our method consists of two main phases: prefill compression and decoding compression. During the prefill phase, we compute an importance score for each shot by averaging the attention weights across all tokens, heads, and layers within that shot. This score Scoreprefill(si) is normalized by the shot length ki to avoid bias towards longer shots. Shots are then sorted by their scores and preserved until reaching the specified prefill ratio rp. In the decoding phase, compression is performed dynamically at each step. For each token in the decoding KV cache, we calculate its importance score Scoredecoding(t) by summing attention weights across all heads and layers. The top-k tokens are retained based on the decoding ratio rd. Finally, the compressed KV cache is formed by combining both the preserved 18 Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Table 8: Performance Comparison of Different Shot Numbers on GSM8K Shot Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV Average 1-shot 2-shot 4-shot 6-shot 8-shot Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% Baseline 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7013(1.90%) 0.6892(3.60%) 0.6816(4.70%) 0.6884(3.70%) 0.6952(2.80%) 0.6657(6.90%) 0.5118(28.40%) 0.2320(67.50%) 0.0296(95.90%) 0.7172(+0.30%) 0.7089(0.80%) 0.6914(3.30%) 0.6831(4.40%) 0.6596(7.70%) 0.6202(13.20%) 0.5004(30.00%) 0.2714(62.00%) 0.0243(96.60%) 0.7544(0.40%) 0.7551(0.30%) 0.7521(0.70%) 0.7475(1.30%) 0.7460(1.50%) 0.7445(1.70%) 0.7506(0.90%) 0.6217(17.90%) 0.1516(80.00%) 0.7604(+0.40%) 0.7521(0.70%) 0.7453(1.60%) 0.7506(0.90%) 0.7437(1.80%) 0.7081(6.50%) 0.6133(19.00%) 0.4412(41.70%) 0.1759(76.80%) 0.7597(+0.00%) 0.7559(0.50%) 0.7597(+0.00%) 0.7369(3.00%) 0.7475(1.60%) 0.7165(5.70%) 0.6558(13.70%) 0.6224(18.10%) 0.4708(38.00%) 0.7604(+0.10%) 0.7688(+1.20%) 0.7695(+1.30%) 0.7726(+1.70%) 0.7612(+0.20%) 0.7339(3.40%) 0.6603(13.10%) 0.5625(26.00%) 0.3980(47.60%) 0.7551(1.70%) 0.7642(0.50%) 0.7513(2.20%) 0.7468(2.80%) 0.7407(3.60%) 0.7377(3.90%) 0.7058(8.10%) 0.5921(22.90%) 0.4572(40.50%) 0.7748(+0.90%) 0.7756(+1.00%) 0.7771(+1.20%) 0.7748(+0.90%) 0.7718(+0.50%) 0.7506(2.30%) 0.7255(5.50%) 0.6232(18.80%) 0.4481(41.60%) 0.7695(3.10%) 0.7642(3.80%) 0.7642(3.80%) 0.7650(3.70%) 0.7657(3.60%) 0.7491(5.70%) 0.7051(11.20%) 0.6384(19.70%) 0.4784(39.80%) 0.7923(0.30%) 0.7938(0.10%) 0.7900(0.60%) 0.7809(1.70%) 0.7854(1.10%) 0.7688(3.20%) 0.7225(9.10%) 0.6406(19.40%) 0.4503(43.30%) prefill and decoding caches. FullKV: 0.7149 0.7142(0.10%) 0.7066(1.20%) 0.6945(2.90%) 0.6914(3.30%) 0.6611(7.50%) 0.6065(15.20%) 0.5042(29.50%) 0.2654(62.90%) 0.0296(95.90%) FullKV: 0. 0.7574(+0.00%) 0.7559(0.20%) 0.7566(0.10%) 0.7521(0.70%) 0.7437(1.80%) 0.7202(4.90%) 0.6657(12.10%) 0.4936(34.80%) 0.1622(78.60%) FullKV: 0.7597 0.7650(+0.70%) 0.7695(+1.30%) 0.7680(+1.10%) 0.7688(+1.20%) 0.7619(+0.30%) 0.7377(2.90%) 0.7111(6.40%) 0.6065(20.20%) 0.3995(47.40%) FullKV: 0.7680 0.7839(+2.10%) 0.7809(+1.70%) 0.7809(+1.70%) 0.7733(+0.70%) 0.7718(+0.50%) 0.7771(+1.20%) 0.7392(3.70%) 0.6732(12.30%) 0.4958(35.40%) FullKV: 0. 0.7839(1.30%) 0.7824(1.50%) 0.7923(0.30%) 0.7885(0.80%) 0.7847(1.20%) 0.7756(2.40%) 0.7619(4.10%) 0.6884(13.40%) 0.5034(36.60%) 19 0.7020(1.80%) 0.6952(2.80%) 0.6884(3.70%) 0.6816(4.70%) 0.6717(6.00%) 0.6475(9.40%) 0.5898(17.50%) 0.3973(44.40%) 0.1236(82.70%) 0.7172(+0.30%) 0.7081(1.00%) 0.7127(0.30%) 0.6990(2.20%) 0.6732(5.80%) 0.6050(15.40%) 0.4011(43.90%) 0.1319(81.60%) 0.0190(97.30%) 0.7104(0.60%) 0.7016(1.90%) 0.6937(3.00%) 0.6887(3.70%) 0.6722(6.00%) 0.6290(12.00%) 0.5015(29.90%) 0.2596(63.70%) 0.0452(93.70%) 0.7612(+0.50%) 0.7559(0.20%) 0.7574(+0.00%) 0.7589(+0.20%) 0.7604(+0.40%) 0.7309(3.50%) 0.7036(7.10%) 0.5534(26.90%) 0.2244(70.40%) 0.7627(+0.70%) 0.7589(+0.20%) 0.7642(+0.90%) 0.7695(+1.60%) 0.7619(+0.60%) 0.7650(+1.00%) 0.7445(1.70%) 0.5368(29.10%) 0.0735(90.30%) 0.7592(+0.20%) 0.7556(0.20%) 0.7551(0.30%) 0.7557(0.20%) 0.7511(0.80%) 0.7337(3.10%) 0.6955(8.20%) 0.5293(30.10%) 0.1575(79.20%) 0.7642(+0.60%) 0.7680(+1.10%) 0.7710(+1.50%) 0.7635(+0.50%) 0.7665(+0.90%) 0.7483(1.50%) 0.7263(4.40%) 0.6543(13.90%) 0.4321(43.10%) 0.7657(+0.80%) 0.7642(+0.60%) 0.7726(+1.70%) 0.7718(+1.60%) 0.7635(+0.50%) 0.7612(+0.20%) 0.7597(+0.00%) 0.7468(1.70%) 0.3434(54.80%) 0.7630(+0.40%) 0.7653(+0.70%) 0.7682(+1.10%) 0.7627(+0.40%) 0.7601(+0.10%) 0.7395(2.70%) 0.7026(7.50%) 0.6385(16.00%) 0.4088(46.20%) 0.7794(+1.50%) 0.7741(+0.80%) 0.7771(+1.20%) 0.7771(+1.20%) 0.7771(+1.20%) 0.7688(+0.10%) 0.7491(2.50%) 0.6960(9.40%) 0.4458(41.90%) 0.7794(+1.50%) 0.7786(+1.40%) 0.7786(+1.40%) 0.7809(+1.70%) 0.7718(+0.50%) 0.7854(+2.30%) 0.7763(+1.10%) 0.7665(0.20%) 0.5565(27.50%) 0.7745(+0.90%) 0.7747(+0.90%) 0.7730(+0.70%) 0.7706(+0.30%) 0.7666(0.20%) 0.7639(0.50%) 0.7392(3.70%) 0.6702(12.70%) 0.4807(37.40%) 0.7854(1.10%) 0.7900(0.60%) 0.7983(+0.50%) 0.7923(0.30%) 0.7854(1.10%) 0.7839(1.30%) 0.7718(2.90%) 0.7142(10.10%) 0.4829(39.20%) 0.7824(1.50%) 0.7824(1.50%) 0.7809(1.70%) 0.7885(0.80%) 0.7824(1.50%) 0.7763(2.30%) 0.7733(2.70%) 0.7763(2.30%) 0.6566(17.40%) 0.7827(1.50%) 0.7826(1.50%) 0.7851(1.20%) 0.7830(1.50%) 0.7807(1.70%) 0.7707(3.00%) 0.7469(6.00%) 0.6916(13.00%) 0.5143(35.30%) Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Table 9: Performance Comparison of Different KV Cache Compression Methods on Many-shot GSM8K Benchmark Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV Average Many-shot GSM8K Baseline LLaMA-3.1-8B-Instruct FullKV: 0. 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7728(6.16%) 0.7935(3.64%) 0.8038(2.39%) 0.7932(3.68%) 0.7934(3.65%) 0.8037(2.40%) 0.7835(4.86%) 0.7537(8.47%) 0.7432(9.75%) 0.8142(1.13%) 0.8334(+1.20%) 0.8136(1.20%) 0.8142(1.13%) 0.8137(1.19%) 0.7832(4.89%) 0.7932(3.68%) 0.7428(9.80%) 0.5127(37.74%) 0.8137(1.19%) 0.8138(1.18%) 0.7832(4.89%) 0.8037(2.40%) 0.7932(3.68%) 0.7935(3.64%) 0.8038(2.39%) 0.7934(3.65%) 0.6827(17.10%) 0.7932(3.68%) 0.8037(2.40%) 0.7932(3.68%) 0.7935(3.64%) 0.7932(3.68%) 0.7834(4.87%) 0.7934(3.65%) 0.7832(4.89%) 0.7037(14.55%) 0.8233(0.02%) 0.7932(3.68%) 0.8037(2.40%) 0.8038(2.39%) 0.7835(4.86%) 0.7832(4.89%) 0.7932(3.68%) 0.7835(4.86%) 0.7932(3.68%) 0.8034(2.44%) 0.8075(1.94%) 0.7995(2.91%) 0.8017(2.65%) 0.7954(3.41%) 0.7894(4.14%) 0.7934(3.65%) 0.7713(6.34%) 0.6871(16.56%) Baseline R1-Distill-Llama-8B FullKV: 0.7123 90% 80% 70% 60% 50% 40% 30% 20% 10% 0.7123(+1.42%) 0.7234(+3.00%) 0.7412(+5.54%) 0.7423(+5.69%) 0.7234(+3.00%) 0.7123(+1.42%) 0.6523(7.12%) 0.6912(1.58%) 0.6323(9.97%) 0.6612(5.85%) 0.6534(6.96%) 0.6523(7.12%) 0.6912(1.58%) 0.7134(+1.58%) 0.6923(1.42%) 0.7312(+4.12%) 0.5834(16.93%) 0.5423(22.78%) 0.6534(6.96%) 0.7123(+1.42%) 0.7234(+3.00%) 0.6912(1.58%) 0.7312(+4.12%) 0.6923(1.42%) 0.6634(5.54%) 0.5123(27.05%) 0.5412(22.94%) 0.6912(1.58%) 0.6423(8.54%) 0.6923(1.42%) 0.6823(2.85%) 0.7123(+1.42%) 0.7023(+0.00%) 0.7423(+5.69%) 0.6823(2.85%) 0.5923(15.66%) 0.6923(1.42%) 0.7123(+1.42%) 0.7234(+3.00%) 0.6634(5.54%) 0.7123(+1.42%) 0.7234(+3.00%) 0.6912(1.58%) 0.6634(5.54%) 0.6823(2.85%) 0.6821(2.88%) 0.6887(1.94%) 0.7065(+0.60%) 0.6941(1.17%) 0.7185(+2.31%) 0.7045(+0.31%) 0.6961(0.88%) 0.6265(10.79%) 0.5981(14.84%) This two-phase approach allows for different compression strategies during prefill and decoding stages, recognizing their distinct roles in the inference process. The shot-aware design during prefill ensures that the most informative examples are preserved, while the token-level compression during decoding maintains essential recent context. D. Evaluation Benchmark D.1. Dataset Details Detailed statistics for each benchmark dataset are provided in Table 10."
        },
        {
            "title": "TASK TYPE",
            "content": "# TEST"
        },
        {
            "title": "EVALUATION METHOD",
            "content": "MMLU (Hendrycks et al., 2020) World Knowledge GSM8K (Cobbe et al., 2021) CSQA* (Talmor et al., 2019) HumanEval (Chen et al., 2021) JailBreakV (Luo et al., 2024)"
        },
        {
            "title": "Arithmetic\nCommonsense\nCode Generation\nSafety",
            "content": "14,079 1,319 1,221 164 Accuracy Exact match Accuracy Pass@1 rate 28,000 Attack success rate Generation-Based Generation-Based Generation-Based Generation-Based Generation-Based Table 10: The statistics of the datasets used in this paper. # TEST denote the number of training data and test data, respectively. Can LLMs Maintain Fundamental Abilities under KV Cache Compression? Algorithm 1 ShotKV: Shot-aware KV Cache Compression Require: Prompt with shots {s1, ..., sn}, prefill ratio rp, decoding ratio rd Ensure: Compressed KV cache KVtotal 1: // Phase 1: Prefill Compression (performed once) 2: for each shot si in {s1, ..., sn} do (cid:80) Compute Scoreprefill(si) = 1 3: ki 4: end for 5: Sort shots by Scoreprefill(si) in descending order 6: Spreserved Select shots until (cid:80) 7: KV 8: // Phase 2: Decoding Compression (performed dynamically) 9: for each decoding step do 10: prefill Compress({sisi Spreserved}) for each token in KVdecoding do ki rp KVprefill l=1 αl (cid:80)H (cid:80)L tsi h=1 t,h si Compute Scoredecoding(t) = (cid:80)H (cid:80)L l=1 αl t,h h= end for rd KVdecoding 11: 12: 13: 14: KV 15: end for return KV prefill KV decoding decoding TopK(KVdecoding, Scoredecoding, k)"
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China",
        "The Hong Kong University of Science and Technology, Hong Kong, China"
    ]
}