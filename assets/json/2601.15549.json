{
    "paper_title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "authors": [
        "Ryo Fujii",
        "Hideo Saito",
        "Ryo Hachiuma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 9 4 5 5 1 . 1 0 6 2 : r VIOLA: Towards Video In-Context Learning with Minimal Annotations Ryo Fujii1,2, Hideo Saito1,2, and Ryo Hachiuma3 1 Keio University 2 Keio AI Research Center 3 NVIDIA Abstract. Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs. Keywords: In-context learning Multimodal large language models Active learning Pseudo annotation"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Multimodal Large Language Models (MLLMs) [38,58,74] have notably enhanced video understanding capabilities. However, despite the growing power of these foundational models, novel domains, scenarios, and tasks continuously emerge in real-world applications [8, 10, 23, 42, 5456, 59]. Consequently, relying solely on pre-trained knowledge is insufficient; models must possess the capability to adapt to these unseen data distributions. While fine-tuning on 2 R. Fujii et al. Fig. 1: Problem setup and performance comparison. (a) Unlike methods requiring large labeled datasets, our approach, VIOLA, strategically selects minimal subset of informative samples for expert annotation while leveraging abundant unlabeled videos via pseudo-labeling. This constructs hybrid pool from which the model retrieves relevant demonstrations for inference. (b) Performance comparison with Naive ICL (which randomly selects samples for annotation), averaged across seven classification datasets. VIOLA achieves robust adaptation with significantly reduced annotation costs. target videos offers straightforward path to enhance performance, it is often impractical since it requires substantial training data to avoid overfitting and incurs high computational costs (e.g., GPU memory), making it difficult to scale across continuously emerging scenarios. In contrast, In-Context Learning (ICL) [13] has emerged as an efficient alternative. Originally pioneered in Large Language Models (LLMs), ICL allows models to adapt to new domains by simply providing example inputs (i.e., demonstrations) alongside test sample during inference, bypassing the need for weight parameter updates via backpropagation. This paradigm has demonstrated strong generalization on unseen tasks in language-only [14, 48, 68] and image-language domains [2, 33], and is now being extended to the video-language domain [30] as powerful strategy for low-resource adaptation. critical bottleneck for ICL in the video domain is the difficulty of obtaining high-quality labeled demonstrations. This issue is exacerbated in specialized or emerging domains such as industrial [47] and surgical [21, 27] footage, as interpreting such content demands high-level expert knowledge, substantially increasing annotation costs compared to general scenarios. Consequently, the standard assumption in existing ICL methods [30], that large pool of annotated examples is readily available for retrieval, is often untenable in these novel environments. The labor-intensive nature of video annotation, combined with the necessity for expert involvement, creates substantial barrier to deploying ICL in real-world video applications where rapid adaptation to new data is essential. To address this realistic and practical challenge, ICL research leverages unlabeled data via two different strategies: selective annotation [17, 43, 53, 76, 77], VIOLA 3 which curates specific examples for labeling from an unlabeled sample pool, and pseudo-annotation [11, 18, 25, 41], which uses model predictions to transform unlabeled data into demonstrations. While these strategies have proven effective individually, the synergistic potential of combining selective annotation with pseudo-annotation remains largely underexplored. In this work, we explore this avenue by integrating these sources into hybrid pool as shown in Figure 2 (a), developing label-efficient framework that bridges minimal human supervision with extensive unlabeled videos, achieving robust adaptation with significantly reduced annotation costs as demonstrated in Figure 2 (b). However, transposing these strategies to the video domain presents three unique challenges: Inefficacy of selection: Current selective annotation methods typically prioritize diversity [53, 78] or representativeness [17, 76]. Highlighting diversity in data selection is crucial for comprehensive coverage, but may sacrifice representativeness by overlooking exemplary data. This trade-off is particularly critical in the video domain. Unlike text, video features are prone to outliers due to high redundancy and task-irrelevant noise (e.g., background variations). Selecting such outliers merely for the sake of diversity results in demonstrations that contribute minimally to downstream performance, as they fail to provide valid guidance during inference. Consequently, diversity and representativeness need to be balanced carefully to avoid selecting outlier samples that are diverse but not representative. Indiscrimination in hybrid retrieval: To address data scarcity, our framework operates on hybrid pool comprising small set of expert-annotated ground truths and large volume of pseudo-labeled videos. However, standard retrieval mechanisms rely solely on similarity [2, 30, 39, 46, 80], implicitly assuming uniform label quality. This assumption is unrealistic because reliability varies significantly not only between ground truths and pseudo-labels but also within the pseudolabeled subset itself. Consequently, retrieval system driven purely by similarity is prone to selecting visually close instances that may carry erroneous labels, regardless of whether they originate from human experts or model predictions, or the associated confidence of those predictions. To mitigate this, robust in-context learning requires mechanism that strictly balances visual similarity with label reliability, ensuring that the limited context slots are occupied by demonstrations that are both semantically relevant and trustworthy. Sensitivity to pseudo-label quality: Unlike the text domain [11, 18, 25, 41], where models can leverage many-shot learning [1] to statistically mitigate label noise, video inputs incur prohibitive token costs. This constraint confines video ICL to strict few-shot regime. In this setting, the model becomes hypersensitive to the quality of each demonstration, where single erroneous pseudo-label can induce significant negative transfer and severely degrade reasoning performance. Thus, simply retrieving examples without explicitly modeling their uncertainty leads to brittle performance. To overcome these limitations, we propose VIOLA, label-efficient framework that assembles robust hybrid demonstration pool by integrating minimal human 4 R. Fujii et al. annotations with extensive unlabeled videos. First, to guarantee diversity while mitigating the risk of selecting semantic outliers, we introduce density-uncertaintyweighted selection. This strategy leverages Gaussian mixture models (GMM) to partition the semantic space into distinct clusters. Within each cluster, we identify the most valuable sample by maximizing selection score that weights model uncertainty with probability density. This ensures that the expert-annotated examples are simultaneously diverse, representative of their semantic group, and highly informative. These labeled samples are then combined with refined pseudo-labeled data to form the hybrid pool. Second, to resolve indiscrimination in using this mixed data, we propose confidence-aware retrieval. This mechanism retrieves demonstrations based on composite score of visual similarity and label confidence, ensuring that limited context slots are occupied by reliable instances rather than merely visually similar ones. Finally, addressing hypersensitivity to noise, we design confidence-aware prompting. We explicitly encode the source and confidence scores into the prompt, enabling the MLLM to discern the reliability of each demonstration and adaptively weight its reliance on ground-truth versus pseudo-labeled examples for robust reasoning. Our main contributions are summarized as follows: 1. We propose VIOLA, label-efficient framework for video in-context learning that bridges the gap between minimal expert supervision and abundant unlabeled data by constructing confidence-aware hybrid demonstration pool. 2. We introduce density-uncertainty-weighted selection, which integrates GMMbased clustering with density-weighted uncertainty. This strategy ensures diverse data coverage while filtering out semantic outliers, guaranteeing that the limited annotation budget is invested in samples that are simultaneously representative and highly informative. 3. We develop confidence-aware retrieval mechanism that optimizes the tradeoff between visual similarity and label reliability within the hybrid pool, preventing noisy pseudo-labels from degrading the quality of the retrieved context. 4. We design confidence-aware prompting strategy that explicitly embeds confidence scores into the input, enabling the MLLM to adaptively calibrate its reasoning based on the reliability of the provided demonstrations (ground truths vs. pseudo-labels)."
        },
        {
            "title": "2 Related Work",
            "content": "In-Context Learning. In-Context Learning (ICL) [14, 48, 68] empowers models to adapt to novel tasks by conditioning generation on few input-output demonstrations, thereby eliminating the need for parameter updates. This paradigm has been successfully extended to various Computer Vision (CV) domains, spanning images [2,6,7,9,29,32,45,57,60,61,75,79,80], video [2,30,33], point clouds [20,50], skeleton sequences [62], and pedestrian trajectories [22]. Notably, VideoICL [30] VIOLA 5 recently advanced out-of-distribution video understanding through similaritybased example selection and confidence-based iterative inference. However, fundamental limitation of standard video ICL frameworks [30] is their reliance on large-scale annotated pools for retrieval. Such prerequisite is impractical in specialized domains [21, 47], where the prohibitive cost of expert annotation renders the construction of extensive support sets infeasible. In contrast to prior works predicated on abundant supervision, we target the realistic label-efficient setting, optimizing performance under strict ground-truth expert annotation budget. Selective Annotation. Traditionally, Active Learning (AL) [19] aims to incrementally annotate samples that maximize model performance with minimal labeling costs. Decades of research have yielded wide array of strategies to select informative samples [3, 4, 35, 40, 44, 49, 69, 72]. In the context of ICL, this paradigm is adapted as selective annotation, optimizing the labeling budget by identifying the most valuable examples from an unlabeled pool. Prior works typically prioritize either diversity [17, 53, 78] or uncertainty [43, 76]. However, directly applying these strategies to the video domain is suboptimal due to the prevalence of noise. Methods maximizing diversity often inadvertently capture outliers, which are samples that are statistically distinct but semantically unrepresentative due to task-irrelevant noise (e.g., background clutter). Conversely, uncertainty-based selection assumes that high entropy implies informational value. In multimodal settings, however, this often conflates epistemic uncertainty (useful difficulty) with aleatoric noise (e.g., visual ambiguity), leading to the selection of low-quality demonstrations. To address this, we propose density-uncertaintyweighted sampling. By synergizing semantic density with model uncertainty, our method explicitly filters noisy outliers while targeting samples that are both representative and informative. Pseudo-Labeling. Pseudo-labeling [31] serves as cornerstone of semi-supervised learning [71]. While traditional Semi-Supervised Learning (SSL) relies on selftraining paradigms [12, 15, 16, 34, 52, 63, 65, 67, 70] leveraging high-confidence predictions, recent studies have adapted this principle to ICL to mitigate data scarcity. Specifically, these works employ pseudo-annotation to scale demonstrations via model predictions [11, 18, 25, 41]. Such methods show promise in text domains, where models can utilize many-shot learning [1] to statistically average out label noise. Conversely, video ICL operates under strict few-shot regime due to prohibitive token costs. In this setting, models become hypersensitive to noise, as single erroneous demonstration can induce significant negative transfer. Current retrieval mechanisms [37, 51], relying primarily on similarity [2, 30, 39, 46, 80], fail to distinguish between ground truths and potentially noisy pseudo-labels. In contrast, our framework introduces confidence-aware mechanisms spanning both retrieval and prompting to explicitly model label reliability, enabling robust inference even with noisy hybrid pools. Distinct from prior studies that treat selective annotation and pseudo-labeling in isolation, our approach is grounded in the Semi-Supervised Active Learning (SSAL) paradigm [24,26,28,36,66,73]. SSAL merges the strengths of AL and SSL, 6 R. Fujii et al. Fig. 2: Overview of our proposed framework. The pipeline consists of three stages: 1. Selective Annotation: We acquire expert labels for small, informative subset (DL) using density-uncertainty-weighted sampling. 2. Pseudo-Annotation: We generate highconfidence pseudo-labels via in-context pseudo-annotation to construct hybrid pool DH . 3. Inference: We predict final answers via confidence-aware retrieval and prompting during inference. aiming to significantly improve model performance under conditions of limited annotation. By adapting this paradigm to the video ICL context, we propose unified label-efficient framework that strategically synergizes minimal expert supervision with abundant unlabeled data via pseudo-annotation. This holistic and unified approach ensures robust adaptation and maximizes performance in realistic low-resource target domains."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "In the standard In-Context Learning (ICL) paradigm, pre-trained Multimodal Large Language Model (MLLM), denoted as M, and demonstration pool = {(xi, yi)}N i=1 are typically employed. Here, xi represents the multimodal input (video and text query) and yi is the corresponding ground-truth answer. Given test query xtest, the inference process involves retrieving specific context set comprising demonstrations that are semantically relevant to the query. Typically, this retrieval is based on visual similarity within the latent space [2, 30, 80]. The index set I, corresponding to the top-K samples with the VIOLA 7 highest similarity scores, is identified as: = arg top-K i{1,...,N } sim(xtest, xi), (1) where sim(, ) denotes the cosine similarity between visual embeddings. Based on these indices, the in-context prompts are constructed as = {(xi, yi) I}. Finally, the model predicts the target answer ˆy by conditioning on both the retrieved context and the test query: ˆy = M(xtest, C). (2) key assumption in this standard setting is that the pool is fully annotated and sufficiently large to cover the distribution of xtest."
        },
        {
            "title": "3.2 Overview",
            "content": "In this work, we address realistic and challenging scenario where expert annotations are scarce. To overcome this limitation, our framework operates on hybrid demonstration pool DH , constructed by integrating small set of expertannotated ground-truth samples with large volume of pseudo-labeled videos. We start with pool of unlabeled instances DU = {ui}N i=1, where each instance ui consists of video and corresponding task query. We assume strict annotation budget (B ), representing the maximum number of samples an expert can label. We assemble this hybrid pool DH by strategically allocating the budget and leveraging the remaining unlabeled data, ultimately enabling robust inference. As illustrated in Fig. 2, this process consists of three key stages. Selective Annotation Stage. We first aim to select an optimal subset of indices {1, . . . , } such that = B. An oracle (human expert) provides labels for these selected samples, forming the labeled set: DL = {(ui, yi) S}. (3) To maximize the efficiency of this budget, we propose density-uncertaintyweighted selection. As detailed in Sec. 3.3, this strategy synergizes semantic density with model uncertainty to identify samples that are diverse, representative, and informative. Pseudo Annotation Stage. To utilize the vast amount of remaining unlabeled data Drem = DU DL, we employ the model to generate pseudo-labels while utilizing DL as contexts. Crucially, since video ICL is sensitive to noise, we also estimate confidence score cj for each sample. This results in pseudo-labeled set: DP = {(uj, ˆyj, cj) uj Drem}, where (ˆyj, cj) M(uj, CDL). (4) We describe the specifics of this in-context pseudo-annotation process in Sec. 3.4. 8 R. Fujii et al. Inference Stage. Finally, the hybrid demonstration pool is defined as DH = DL DP . For test query xtest, we retrieve context set from DH to enable robust inference: ˆy = M(xtest, C). (5) To mitigate the impact of noisy pseudo-labels, we introduce confidence-aware retrieval and confidence-aware prompting, which are elaborated in Sec. 3.5."
        },
        {
            "title": "3.3 Selective Annotation via Density-Uncertainty-weighted Selection",
            "content": "In the first phase, our objective is to identify the optimal subset for expert annotation. To maximize the utility of the limited budget B, we propose density-uncertainty-weighted sampling. This strategy addresses the limitations of diversity-only or uncertainty-only methods by synergizing semantic density with model uncertainty to filter out outliers while targeting informative samples. Semantic Density Estimation. We first capture the underlying semantic structure of the unlabeled pool DU . We extract video embeddings zi and fit Gaussian Mixture Model (GMM) with components, where is set equal to the budget B. For each sample ui, we compute the posterior probability γik of belonging to component k: γik = πkN (zi µk, Σk) j=1 πjN (zi µj, Σj) (cid:80)K . (6) high γik indicates that the sample is representative of the k-th semantic cluster. Zero-Shot Uncertainty Estimation. Concurrently, we estimate prediction uncertainty to gauge the informativeness of each sample. We prompt the frozen MLLM in zero-shot manner. The confidence score czero is defined as the minimum token probability in the generated sequence: czero = min t=1 (wtui), (7) where wt denotes the t-th token in the generated sequence, and represents the total sequence length. Density-uncertainty-weighted Sampling. To ensure diverse coverage while targeting informative samples, we enforce stratified policy. For each cluster k, we define selection score that weights uncertainty with density: Sk(ui) = γ(1λ) ik (1 czero )λ, (8) where λ [0, 1] is hyperparameter that balances the trade-off between representativeness and informativeness. higher λ prioritizes uncertain samples (hard examples), while lower λ favors high-density samples (prototypical examples) to avoid outliers. We select exactly one instance from each cluster that maximizes this score to form the labeled set DL. This guarantees that the demonstrations are diverse (covering all modes), representative, and informative. VIOLA 9 3. In-Context Pseudo-Annotation Once the expert-labeled set DL is obtained, we proceed to the pseudo annotation phase to utilize the remaining unlabeled data Drem = DU DL. critical insight here is that while zero-shot signals were sufficient for selection, they are too noisy to serve as demonstrations. Therefore, we employ an in-context pseudo-annotation strategy. Pseudo Annotation. To generate high-quality pseudo-labels, we leverage the expert knowledge encapsulated in DL. For each unlabeled sample uj Drem, we retrieve local context set Cj DL. Since DL consists exclusively of expertannotated ground truths, we apply the standard retrieval mechanism based on visual similarity. We then perform few-shot inference to obtain refined label ˆyj and confidence score cj derived from the output token probability: (ˆyj, cj) M(uj, Cj). (9) Unlike the zero-shot estimation used in the selection phase, this step utilizes in-context examples to align the prediction with the specific domain semantics. To strictly control the quality of the generated data, we filter these predictions based on their confidence. Specifically, we retain only samples where cj exceeds the 95th percentile of the confidence distribution, ensuring that only the most reliable pseudo-labels are added to the pool. Hybrid Pool Construction. We aggregate these processed samples to form the pseudo-labeled pool DP = {(uj, ˆyj, cj)}. Finally, to enable the retrieval of both expert-verified and model-generated demonstrations during inference, we unite the two sources into single hybrid pool: DH = DL DP . (10) This hybrid pool serves as the foundation for the subsequent confidence-aware inference stage."
        },
        {
            "title": "3.5 Confidence-Aware Inference",
            "content": "In the final phase, we leverage the hybrid pool DH to generate answers for test queries. To mitigate the risk of noise propagation from pseudo-labels, we introduce two confidence-aware mechanisms. Confidence-Aware Retrieval. As discussed in Sec. 3.1, standard retrieval relies solely on visual similarity (arg top-K sim), implicitly treating expert annotations and pseudo-labels as equally reliable. To rectify this, we propose retrieval score that penalizes uncertain pseudo-labels. For candidate ui in the hybrid pool DH , the score is computed as: ri = sim(ui, xtest)(1τ ) (ci)τ , (11) where sim denotes cosine similarity and τ balances semantic relevance with reliability. For ground-truth samples (ui DL), we set ci = 1.0, ensuring they 10 R. Fujii et al. are prioritized if semantically relevant. Consequently, we retrieve the context indices by maximizing this composite score: = arg top-K uiDH ri. (12) This formulation ensures that the limited context slots are occupied by demonstrations that optimize the trade-off between visual similarity and label trustworthiness. Confidence-Aware Prompting. To explicitly communicate reliability, we apply formatting function Φ that adapts based on the demonstration source. For retrieved demonstration (uk, yk, ck) where I, the refined answer yk = Φ(yk, ck) is generated as: yk = The correct label is yk (Ground-truth). if uk DL, The predicted label is yk (Pseudo-label with ck 100% confidence). if uk DP (13) This differentiation acts as soft-gating mechanism, allowing the MLLM to attend strongly to verified patterns while cautiously integrating high-confidence pseudo-labels. Finally, we construct the context set by pairing the input instances with their formatted answers for all retrieved indices: = {(uk, yk) I}. With this refined context, we perform the final inference: ˆy = M(xtest, C). (14) (15)"
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Datasets. We validate our framework across two video-language tasks using nine diverse datasets spanning medical, industrial, ego-centric, and surveillance domains. For video classification, we employ accuracy as the evaluation metric across seven benchmarks: Drive&Act [42] for driver activity recognition; EgoPet [8] for animal ego-centric interaction recognition; EgoSurgery [21] for medical procedure analysis; ENIGMA [47] for industrial human-object interaction recognition; UCF-Crime [54] for surveillance anomaly detection; Xsports [56] for extreme sports recognition; and MammAlps [23] for animal behavior recognition. For video captioning, we adopt the ROUGE-L score using Bora [55] for the biomedical domain and CapERA [10] for aerial surveillance scenes. Specific details regarding prompts and class definitions are provided in the Supplementary Material. VIOLA 11 Baselines. (1) Zero-Shot provides only the task instruction to the MLLM, without any demonstrations. (2) Random performs random example selection for annotation. (3) Random+Pseudo Annotation uses the MLLM to generate pseudo-labels for the unlabeled instances to serve as additional annotated data. (4) Random+VideoICL [30] utilizes confidence-based iterative inference mechanism to ensure high-quality generation. Note that while the original method assumes fully labeled pool, we implement it using randomly selected pool in our experiments. (5) VoteK [53] selects representative examples from buckets stratified by the models confidence scores. Implementation Details. We evaluate our framework using several state-of-theart open-source MLLMs, specifically Qwen2-VL-7B [58], VideoLLaMA3-7B [74], and Qwen3-VL-8B [5], LLaVA-Video7B [38]. These models were selected for their superior performance on general video understanding benchmarks. For feature extraction, we utilize InternVideo2 [64] as the video encoder. Videos are sampled at 1 frame per second (FPS); for sequences exceeding 32 seconds, we uniformly downsample to 32 frames to maintain temporal consistency. In our in-context learning setup, we set the number of examples to = 8, where each demonstration video consists of 16 frames. Furthermore, we apply quality control step by filtering all generated pseudo-demonstrations using confidence threshold at the 95th percentile. Regarding prompt arrangement, pseudo-labeled demonstrations are positioned at the beginning of the context, followed by labeled demonstrations. Within each group, examples are sorted by their similarity to the test sample, ensuring that the most relevant ground truth demonstrations are placed immediately adjacent to the target query."
        },
        {
            "title": "4.2 Main Results",
            "content": "Tab. 1 presents comparison between our method and baseline approaches on nine diverse benchmarks, with an annotation budget of DL = 20. From the results, we observe the following: Substantial Improvement over Zero-Shot Baselines. Our method generally outperforms the zero-shot baseline across all evaluated datasets and architectures. We observe particularly significant gains in specialized domains using Qwen2-VL7B, such as +53.6% increase on the industrial ENIGMA dataset and +38.2% on the animal-centric EgoPet dataset. These substantial margins highlight that our framework effectively bridges the domain gap where standard zero-shot inference typically struggles, successfully adapting models to unseen distributions without parameter updates. Superiority over Selective Annotation and Pseudo Annotation Baselines. In our experimental setting, restricted to pool of only 20 labeled samples, the choice of selection strategy becomes critical. We observe that VideoICL, originally designed for full-dataset scenarios, exhibits sensitivity to this limited pool size, often underperforming the simple Random baseline (e.g., 11.2% vs. 20.9% on Drive&Act with Qwen2-VL-7B). Similarly, Pseudo Annotation frequently yields subpar results due to noisy supervision (e.g., dropping to 8.0% on Xsports). While VoteK offers more robust baseline, our method still achieves distinct 12 R. Fujii et al. Table 1: Main results on diverse video understanding benchmarks. We evaluate our framework against various baselines across classification and captioning tasks, using fixed pool of 20 labeled samples for each dataset. denotes the performance gain of Ours relative to the Zero-shot baseline. Note that VideoICL incurs up to 4 computational cost in the worst case. Methods Zero-shot Random +Pseudo Annotation +VideoICL [30] VoteK [53] Ours Zero-shot Random +Pseudo Annotation +VideoICL [30] VoteK [53] Ours Zero-shot Random +Pseudo Annotation +VideoICL [30] VoteK [53] Ours Zero-shot Random +Pseudo Annotation +VideoICL [30] VoteK [53] Ours - 2 Q 3 L d V - 3 Q - i a Drive&Act EgoPet EgoSurgery ENIGMA UCF-Crime Xsports MammAlps Bora CapERA Classification Captioning 7.2 20.9 15.4 11.2 18.1 26.4 +19.2 21.7 36.0 28.3 31.8 34.5 37.6 +15.9 22.1 36.8 26.6 36.9 35.5 37.1 +15.0 24.8 23.7 22.1 22.2 24.0 33.3 +8.5 14.7 19.0 32.4 16.2 21.2 52.9 +38. 10.7 16.1 14.1 15.8 12.9 50.7 +40.0 32.6 39.1 33.3 39.4 40.0 56.0 +23.4 15.2 18.8 36.8 16.3 25.7 45.3 +30.1 25.5 26.8 21.8 28.3 13.2 30.1 +4.6 24.0 38.3 23.9 35.5 33.3 48.3 +24.3 22.2 39.7 22.8 39.7 35.9 39.7 +15. 33.1 28.2 37.3 29.5 18.0 46.1 +13.0 4.9 58.8 58.2 57.5 54.8 58.5 +53.6 44.3 47.6 50.5 47.5 44.3 61.3 +17.0 64.4 71.7 66.6 71.5 68.1 75.5 +11.1 61.0 46.1 57.6 42.4 38.4 48.9 -12.1 31.5 37.9 39.3 34.7 36.3 41.1 +9. 31.7 34.0 27.4 34.5 33.3 40.5 +8.8 34.8 39.8 41.1 40.1 41.7 43.5 +8.7 34.1 33.7 38.1 31.9 31.0 39.3 +5.2 18.5 21.5 8.0 23.5 26.5 25.9 +7.4 29.1 19.9 22.8 23.4 31.3 33.0 +3.9 28.8 28.8 29.1 28.8 32.5 29.6 +0. 18.5 20.8 18.4 22.8 18.2 31.1 +12.6 0.274 0.261 43.8 0.378 0.338 64.2 0.312 0.257 46.4 0.335 0.330 63.2 0.382 0.359 64.4 65.0 0.393 0.365 +11.2 +0.104 +0.119 0.228 0.220 56.4 0.382 0.311 67.8 0.297 0.224 58.4 0.325 0.283 69.1 0.325 68.5 0.400 68.2 0.376 0.340 +11.8 +0.120 +0.148 0.233 0.243 53.0 0.276 59.7 0.367 0.244 0.239 54.9 0.336 0.253 59.7 0.365 0.265 61.6 0.333 63.7 0.330 +10.7 +0.087 +0.100 0.232 0.244 63.3 0.347 0.306 65.2 0.203 0.228 62.4 0.343 0.292 65.5 0.355 0.320 65.7 66.7 0.358 0.332 +3.4 +0.088 +0.126 margins over it in classification tasks (e.g., +34.8% on EgoPet). This advantage extends to captioning tasks in many cases. On the biomedical Bora dataset, our framework achieves an ROUGE-L score of 0.365 with Qwen2-VL, surpassing both the Random baseline of 0.338 and the competitive VoteK method of 0.359. Similarly, on CapERA, we consistently outperform baselines, demonstrating that our confidence-aware retrieval effectively filters noisy pseudo-labels that otherwise degrade the quality of generated descriptions. Consistent Efficacy across Models. The effectiveness of our framework is not limited to specific model. As shown in Tab. 1, we observe consistent performance gains when applying our method to VideoLLaMA3-7B and the larger Qwen3-VL-8B. Notably, on VideoLLaMA3-7B, our method secures the best performance in nearly all classification tasks, with gains of up to +40.0% on EgoPet. Even on the stronger Qwen3-VL-8B model, our approach continues to yield positive improvements. Similarly, on LLaVA-Video-7B, our method achieves improvements in most benchmarks, with ENIGMA being the sole outlier. This VIOLA 13 Fig. 3: Performance trends under varying oracle annotation budgets. We compare our framework against baselines using Qwen2-VL-7B, varying the labeled pool size from 20 to 100 samples. demonstrates that our framework generally complements capable models by providing high-quality, relevant in-context demonstrations."
        },
        {
            "title": "4.3 Performance under Varying Annotation Budgets",
            "content": "To investigate the scalability of our framework, we evaluate performance variations as the annotation budget increases from 20 to 100 samples. Fig. 3 illustrates the performance trends using Qwen2-VL. We observe that VideoICL consistently underperforms the Random baseline across most datasets, with the exceptions of EgoSurgery and Xsports, indicating its ineffectiveness within limited budget setting. In contrast, our method demonstrates remarkable stability, generally outperforming the Random and VoteK baselines regardless of the annotation budget size. Furthermore, our approach yields continuous gains without premature plateauing and maintains steady advantage in captioning tasks, confirming that our framework scales robustly as more annotations become available."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "Unless otherwise specified, all ablation studies are performed with an annotation budget of DL = 20, aligning with the main results in Sec. 4.2. Effectiveness of Density-Uncertainty-weighted Selection. Table 2 validates our balanced Selection strategy in Eq. (8). Relying solely on uncertainty (λ = 1) severely degrades performance (e.g., -21.2% on EgoSurgery) by selecting noisy outliers. Conversely, using only density (λ = 0) fails to capture informative signals required for complex reasoning. By synergizing both (λ = 0.5), our method consistently outperforms these extremes, confirming that demonstrations must be simultaneously representative and informative. 14 R. Fujii et al. Table 2: Ablation study of densityuncertainty-weighted Sampling (λ). Table 3: Ablation study of ConfidenceAware Inference mechanisms. Strategy (λ) Density only (λ = 0) Uncertainty only (λ = 1) Ours (λ = 0.5) ENIGMA EgoSurgery CapEra Confidence-Aware 60.3 58.5 62.5 38.1 30.7 51.9 0.393 0.383 0.393 Retrieval Prompting Drive&Act Xsports CapEra 16.1 17.6 24.3 26.4 16.5 15.0 22.2 25. 0.368 0.373 0.368 0.393 Table 4: Performance evaluation on the smaller Qwen2-VL-2B model. Methods Zero-shot Random +Pseudo Annotation +VideoICL VoteK Ours Qwen2-VL-2B VideoLLaMA3-2B Drive&Act Xsports CapEra Drive&Act Xsports CapEra 8.0 18.4 8.7 15.1 16.7 29.1 26.2 20.7 7.4 19.6 28.2 31. 0.280 0.355 0.303 0.321 0.380 0.364 +21.1 +5.1 +0.084 14.4 20.9 16.7 16.3 17.2 21.2 +6.8 27.4 20.2 25,4 19.7 23.9 27. 0.206 0.370 0.281 0.304 0.377 0.384 +0.2 +0.178 of pseudo5: Comparison Table annotation strategies. We assess the benefit of using in-context examples (In-Context) over raw model predictions (Zero-shot) for generating pseudo-labels. Pseudo-Annotation Strategy Xsports EgoSurgery Bora Zero-shot In-Context (Ours) 24.2 25.9 58.8 62.5 0.337 0.365 Effectiveness of Confidence-Aware Inference. Table 3 demonstrates the synergy between confidence-aware retrieval and prompting during inference. While individual components provide limited or unstable gains, combining them yields robust improvements across all datasets (e.g., +10.3% on Drive&Act). This confirms that effective video ICL requires both filtering the hybrid pool for reliability and explicitly guiding the model to distinguish between ground truths and pseudo-labels. Generality across Model Sizes. We verify scalability using the smaller Qwen2VL-2B and VideoLLaMA3-2B models. As shown in Tab. 4, our framework consistently outperforms baselines despite the limited capacity, achieving significant gains over random selection. This confirms that our VIOLA remains effective across varying model scales. Effectiveness of In-Context Pseudo-Annotation. Tab. 5 demonstrates the benefit of conditioning pseudo-label generation on expert examples. Unlike the zero-shot baseline, which struggles with domain nuances, our In-Context strategy leverages retrieved expert data to produce higher-quality labels. This significantly improves downstream performance (e.g., +3.7% on EgoSurgery), confirming that expert guidance is essential for constructing reliable hybrid pool."
        },
        {
            "title": "5 Conclusion",
            "content": "Conclusion. We present VIOLA, unified framework for label-efficient video In-Context Learning that bridges minimal expert supervision with abundant unlabeled data. To address the trade-off between diversity and representativeness VIOLA 15 in video selection, we propose density-uncertainty-weighted sampling, which effectively filters out semantic outliers while identifying the most informative samples for annotation. Then, to mitigate the indiscrimination and hypersensitivity to noise inherent in hybrid pools, we introduce in-context pseudo-annotation followed by confidence-aware retrieval and prompting. These mechanisms explicitly model reliability, ensuring that the model distinguishes between verified ground truths and potentially noisy pseudo-labels. Experiments on diverse benchmarks, including specialized medical and industrial domains, using four MLLMs demonstrate that our approach consistently outperforms state-of-the-art baselines, validating the necessity of synergizing strategic data selection with confidence-aware inference in low-resource adaptation. Limitations and Future Work. Our framework relies on pre-trained visual embeddings for data selection and retrieval. In highly specialized domains, significant distribution shifts may distort the semantic space, potentially compromising clustering and retrieval accuracy. Future work will explore integrating domainadapted encoders to enhance robustness in such scenarios."
        },
        {
            "title": "References",
            "content": "1. Agarwal, R., Singh, A., Zhang, L.M., Bohnet, B., Rosias, L., Chan, S.C., Zhang, B., Faust, A., Larochelle, H.: Many-shot In-Context Learning. In: ICML Workshop (2024) 3, 5 2. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J.L., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Bińkowski, M.a., Barreira, R., Vinyals, O., Zisserman, A., Simonyan, K.: Flamingo: Visual Language Model for Few-Shot Learning. In: NeurIPS (2022) 2, 3, 4, 5, 6 3. Ash, J.T., Goel, S., Krishnamurthy, A., Kakade, S.M.: Gone Fishing: Neural Active Learning with Fisher Embeddings. In: NeurIPS (2021) 5 4. Ash, J.T., Zhang, C., Krishnamurthy, A., Langford, J., Agarwal, A.: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. In: ICLR (2020) 5 5. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., Zhu, K.: Qwen3-VL Technical Report. arXiv preprint arXiv:2511.21631 (2025) 11 6. Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A.L., Darrell, T., Malik, J., Efros, A.A.: Sequential Modeling Enables Scalable Learning for Large Vision Models. In: CVPR (2024) 7. Baldassini, F.B., Shukor, M., Cord, M., Soulier, L., Piwowarski, B.: What Makes Multimodal In-Context Learning Work? In: CVPR Workshops (2024) 4 8. Bar, A., Bakhtiar, A., Tran, D., Loquercio, A., Rajasegaran, J., LeCun, Y., Globerson, A., Darrell, T.: EgoPet: Egomotion and Interaction Data from an Animals Perspective. In: ECCV (2024) 1, 10, 20 16 R. Fujii et al. 9. Bar, A., Gandelsman, Y., Darrell, T., Globerson, A., Efros, A.: Visual prompting via image inpainting. NeurIPS (2022) 4 10. Bashmal, L., Bazi, Y., Al Rahhal, M.M., Zuair, M., Melgani, F.: CapERA: Captioning Events in Aerial Videos. Remote Sensing 15(8) (2023) 1, 10, 21 11. Berger, U., Baumel, T., Stanovsky, G.: In-Context Learning on Budget: Case Study in Token Classification. In: NAACL Workshop (2025) 3, 5 12. Berthelot, D., Roelofs, R., Sohn, K., Carlini, N., Kurakin, A.: AdaMatch: Unified Approach to Semi-Supervised Learning and Domain Adaptation. In: ICLR (2022) 5 13. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. In: NeurIPS (2020) 14. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. NeurIPS (2020) 2, 4 15. Chen, B., Jiang, J., Wang, X., Wan, P., Wang, J., Long, M.: Debiased Self-Training for Semi-Supervised Learning. In: NeurIPS (2022) 5 16. Chen, H., Tao, R., Fan, Y., Wang, Y., Wang, J., Schiele, B., Xie, X., Raj, B., Savvides, M.: SoftMatch: Addressing the Quantity-Quality Tradeoff in Semi-supervised Learning. In: ICLR (2023) 5 17. Chen, Z., Wang, S., Shen, C., Li, J.: FastGAS: Fast Graph-based Annotation Selection for In-Context Learning. In: ACL (2024) 2, 3, 18. Chen, Z., Wang, S., Tan, Z., Li, J., Shen, C.: MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning. In: ICML (2025) 3, 5 19. Cohn, D., Ghahramani, Z., Jordan, M.: Active Learning with Statistical Models. In: NeurIPS (1994) 5 20. Fang, Z., Li, X., Li, X., Buhmann, J.M., Loy, C.C., Liu, M.: Explore In-Context Learning for 3D Point Cloud Understanding. NeurIPS (2024) 21. Fujii, R., Hatano, M., Saito, H., Kajita, H.: EgoSurgery-Phase: Dataset of Surgical Phase Recognition from Egocentric Open Surgery Videos. In: MICCAI (2024) 2, 5, 10, 20 22. Fujii, R., Saito, H., Hachiuma, R.: Towards Predicting Any Human Trajectory In Context. In: NeurIPS (2025) 4 23. Gabeff, V., Qi, H., Flaherty, B., Sumbul, G., Mathis, A., Tuia, D.: MammAlps: Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps. In: CVPR (2025) 1, 10, 21 24. Gao, M., Zhang, Z., Yu, G., Arık, S.Ö., Davis, L.S., Pfister, T.: Consistency-based semi-supervised active learning: Towards minimizing labeling cost. In: ECCV (2020) 5 25. Gu, Z., Zou, H.P., Liu, A., Chen, Y., Zhang, W., Yu, P.S.: Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations. In: ICML 2025 Workshop (2025) 3, 26. Guo, J., Kang, Y., Li, X., Zhang, W., Kuang, K., Sun, C., Tang, S., Wu, F.: Unleash the power of inconsistency-based semi-supervised active learning by dynamic programming of curriculum learning. IEEE Transactions on Knowledge and Data Engineering 36(11), 72687282 (2024) 5 27. He, R., Xu, M., Das, A., Z. Khan, D., Bano, S., J. Marcus, H., Stoyanov, D., J. Clarkson, M., Islam, M.: PitVQA: Image-grounded Text Embedding LLM for Visual Question Answering in Pituitary Surgery. In: MICCAI (2024) 2 VIOLA 17 28. Huang, S., Wang, T., Xiong, H., Huan, J., Dou, D.: Semi-Supervised Active Learning With Temporal Output Discrepancy. In: ICCV (2021) 29. Jiang, Y., Irvin, J.A., Wang, J.H., Chaudhry, M.A., Chen, J.H., Ng, A.Y.: ManyShot In-Context Learning in Multimodal Foundation Models. In: ICML Workshop (2024) 4 30. Kim, K., Park, G., Lee, Y., Yeo, W., Hwang, S.J.: VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding. In: CVPR (2025) 2, 3, 4, 5, 6, 11, 12 31. Lee, D.H.: Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks. ICML Workshop (07 2013) 5 32. Lee, S.H., Jiang, J., Xu, Y., Li, Z., Ke, J., Li, Y., He, J., Hickson, S., Datsenko, K., Kim, S., Yang, M.H., Essa, I., Yang, F.: Cropper: Vision-Language Model for Image Cropping through In-Context Learning. In: CVPR (2025) 4 33. Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Cahyono, J.A., Yang, J., Li, C., Liu, Z.: Otter: Multi-Modal Model With In-Context Instruction Tuning. IEEE TPAMI 47(9), 75437557 (2025) 2, 34. Li, J., Xiong, C., Hoi, S.C.: CoMatch: Semi-Supervised Learning With Contrastive Graph Regularization. In: ICCV (2021) 5 35. Li, X., Yang, P., Gu, Y., Zhan, X., Wang, T., Xu, M., Xu, C.: Deep active learning with noise stability. In: AAAI (2024) 5 36. Li, Y., Wang, Y., Yu, D.J., Ye, N., Hu, P., Zhao, R.: ASCENT: Active Supervision for Semi-Supervised Learning. IEEE Transactions on Knowledge and Data Engineering 32(5), 868882 (2020) 5 37. Li, Xiaonan and Qiu, Xipeng: Finding Support Examples for In-Context Learning. In: EMNLP (2023) 5 38. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., Yuan, L.: Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. In: EMNLP (2024) 1, 11 39. Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., Chen, W.: What Makes Good In-Context Examples for GPT-3? In: DeeLIO (2022) 3, 5 40. Mahmood, R., Fidler, S., Law, M.T.: Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. In: ICLR (2022) 41. Mamooler, S., Montariol, S., Mathis, A., Bosselut, A.: PICLe: Pseudo-annotations for In-Context Learning in Low-Resource Named Entity Detection. In: NAACL (2025) 3, 5 42. Martin, M., Roitberg, A., Haurilet, M., Horne, M., Reiß, S., Voit, M., Stiefelhagen, R.: Drive&Act: Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles. In: ICCV (2019) 1, 10, 20 43. Mavromatis, C., Srinivasan, B., Shen, Z., Zhang, J., Rangwala, H., Faloutsos, C., Karypis, G.: CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage. In: EMNLP (2024) 2, 5 44. Parvaneh, A., Abbasnejad, E., Teney, D., Haffari, R., Van Den Hengel, A., Shi, J.Q.: Active Learning by Feature Mixing. In: CVPR (2022) 5 45. Qin, C., Zhang, S., Yu, N., Feng, Y., Yang, X., Zhou, Y., Wang, H., Niebles, J.C., Xiong, C., Savarese, S., et al.: UniControl: Unified Diffusion Model for Controllable Visual Generation In the Wild. NeurIPS (2023) 46. Qin, C., Zhang, A., Chen, C., Dagar, A., Ye, W.: In-Context Learning with Iterative Demonstration Selection. In: EMNLP (2024) 3, 5 47. Ragusa, F., Leonardi, R., Mazzamuto, M., Bonanno, C., Scavo, R., Furnari, A., Farinella, G.M.: ENIGMA-51: Towards Fine-Grained Understanding of Human Behavior in Industrial Scenarios. In: WACV (2024) 2, 5, 10, 20 18 R. Fujii et al. 48. Rubin, O., Herzig, J., Berant, J.: Learning To Retrieve Prompts for In-Context Learning. In: NAACL (2022) 2, 4 49. Sener, O., Savarese, S.: Active Learning for Convolutional Neural Networks: Core-Set Approach. In: ICLR (2018) 5 50. Shao, F., Liu, P., Wang, Z., Luo, Y., Wang, H., Xiao, J.: MICAS: Multi-grained In-Context Adaptive Sampling for 3D Point Cloud Processing. In: CVPR (2025) 4 51. Shum, Kashun and Diao, Shizhe and Zhang, Tong: Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. In: EMNLP (2023) 5 52. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk, E.D., Kurakin, A., Li, C.L.: FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. In: NeurIPS (2020) 5 53. SU, H., Kasai, J., Wu, C.H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N.A., Yu, T.: Selective Annotation Makes Language Models Better Few-Shot Learners. In: ICLR (2023) 2, 3, 5, 11, 12 54. Sultani, W., Chen, C., Shah, M.: Real-world anomaly detection in surveillance videos. In: CVPR (2018) 1, 10, 20 55. Sun, W., You, X., Zheng, R., Yuan, Z., Li, X., He, L., Li, Q., Sun, L.: Bora: Biomedical Generalist Video Generation Model. arXiv preprint arXiv:2407.08944 (2024) 1, 10, 21 56. Suriya Singh and Chetan Arora and C.V. Jawahar: Trajectory Aligned Features For First Person Action Recognition. Pattern Recognition 62, 4555 (2017) 1, 10, 20 57. Wang, C., Li, X., Ding, H., Qi, L., Zhang, J., Tong, Y., Loy, C.C., Yan, S.: Explore In-Context Segmentation via Latent Diffusion Models. In: AAAI (2024) 4 58. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., Lin, J.: Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191 (2024) 1, 59. Wang, R., Chen, J., Ji, K., Cai, Z., Chen, S., Yang, Y., Wang, B.: MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos (2025) 1 60. Wang, X., Wang, W., Cao, Y., Shen, C., Huang, T.: Images Speak in Images: Generalist Painter for In-Context Visual Learning. In: CVPR (2023) 4 61. Wang, X., Zhang, X., Cao, Y., Wang, W., Shen, C., Huang, T.: SegGPT: Towards Segmenting Everything In Context. In: ICCV (2023) 4 62. Wang, X., Fang, Z., Li, X., Li, X., Chen, C., Liu, M.: Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning. In: CVPR (2024) 4 63. Wang, X., Wu, Z., Lian, L., Yu, S.X.: Debiased Learning From Naturally Imbalanced Pseudo-Labels. In: CVPR (2022) 5 64. Wang, Y., Li, K., Li, X., Yu, J., He, Y., Wang, C., Chen, G., Pei, B., Zheng, R., Xu, J., Wang, Z., et al.: Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377 (2024) 11 65. Wang, Y., Chen, H., Heng, Q., Hou, W., Fan, Y., Wu, Z., Wang, J., Savvides, M., Shinozaki, T., Raj, B., Schiele, B., Xie, X.: FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning. In: ICLR (2023) 5 66. Xie, M., Geng, Y., Zhang, W., Li, S., Dong, Y., Wu, Y., Tang, H., Hong, L.: Multi-Resolution Consistency Semi-Supervised Active Learning Framework for Histopathology Image Classification. Expert Systems with Applications 259, 125266 (2025) 5 67. Xie, Q., Dai, Z., Hovy, E., Luong, T., Le, Q.: Unsupervised Data Augmentation for Consistency Training. In: NeurIPS (2020) VIOLA 19 68. Xie, S.M., Raghunathan, A., Liang, P., Ma, T.: An Explanation of In-context Learning as Implicit Bayesian Inference. In: ICLR (2022) 2, 4 69. Xie, Y., Lu, H., Yan, J., Yang, X., Tomizuka, M., Zhan, W.: Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm. In: CVPR (2023) 5 70. Xu, Y., Shang, L., Ye, J., Qian, Q., Li, Y.F., Sun, B., Li, H., Jin, R.: Dash: Semi-supervised learning with dynamic thresholding. In: ICML (2021) 5 71. Yang, X., Song, Z., King, I., Xu, Z.: Survey on Deep Semi-Supervised Learning. IEEE Transactions on Knowledge and Data Engineering 35(9) (2023) 5 72. Yehuda, O., Dekel, A., Hacohen, G., Weinshall, D.: Active Learning Through Covering Lens. In: NeurIPS (2022) 5 73. Yin, T., Liu, N., Sun, H.: Towards Cost-Effective Learning: Synergy of SemiSupervised and Active Learning. In: CVPR (2025) 5 74. Zhang, B., Li, K., Cheng, Z., Hu, Z., Yuan, Y., Chen, G., Leng, S., Jiang, Y., Zhang, H., Li, X., Jin, P., Zhang, W., Wang, F., Bing, L., Zhao, D.: VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding. arXiv preprint arXiv:2501.13106 (2025) 1, 11 75. Zhang, J., Wang, B., Li, L., Nakashima, Y., Nagahara, H.: Instruct Me More! Random Prompting for Visual In-Context Learning. In: WACV (2024) 4 76. Zhang, S., Xia, X., Wang, Z., Chen, L.H., Liu, J., Wu, Q., Liu, T.: IDEAL: InfluenceDriven Selective Annotations Empower In-Context Learners in Large Language Models. In: ICLR (2024) 2, 3, 5 77. Zhang, Y., Feng, S., Tan, C.: Active Example Selection for In-Context Learning. In: EMNLP (2022) 2 78. Zhang, Z., Zhang, A., Li, M., Smola, A.: Automatic Chain of Thought Prompting in Large Language Models. In: ICLR (2023) 3, 5 79. Zhao, H., Cai, Z., Si, S., Ma, X., An, K., Chen, L., Liu, Z., Wang, S., Han, W., Chang, B.: MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. In: ICLR (2024) 4 80. Zhao, Z., Tang, J., Lin, C., Wu, B., Huang, C., Liu, H., Tan, X., Zhang, Z., Xie, Y.: Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer. In: CVPR (2024) 3, 4, 5, 6 R. Fujii et al."
        },
        {
            "title": "A Datasets",
            "content": "Drive&Act. Drive&Act [42] is used for classifying driver behaviors from KinectIR videos. We utilize the provided action segmentation labels to extract video segments and task the model with action recognition. The dataset provides three splits, each with approximately 2,000 training and 600 test examples. We report the average performance across the three official splits. EgoPet. EgoPet [8] is dataset capturing pet egocentric imagery that features simultaneous egomotion and multi-agent interactions. While the full collection comprises approximately 84 hours of video sourced from platforms like TikTok and YouTube, we specifically curate subset containing single interaction events for this experiment. The resulting split consists of 387 training samples and 448 test samples. We task the model with identifying the specific object the animal is interacting with, classifying it into one of 18 predefined categories (e.g., ball, person, other animal). EgoSurgery. EgoSurgery [21] serves as benchmark for surgical phase recognition in open surgery, captured via head-mounted cameras. It addresses the data scarcity in open surgery compared to minimally invasive procedures. The dataset includes 15 hours of footage covering 9 distinct phases: anesthesia, closure, design, disinfection, dissection, dressing, hemostasis, incision, and irrigation. For our experiments, we feed 10 consecutive frames as input and task the model with classifying the surgical phase of the middle (fifth) frame. The data split consists of 1,886 training samples and 501 test samples. ENIGMA. ENIGMA [47] is an egocentric dataset acquired in an industrial scenario, where 19 subjects repair electrical boards using various specialized tools. The dataset is designed to study human behavior in realistic industrial environments. In our experiments, we focus on recognizing human-object interactions, specifically tasking the model with identifying the object the operators right hand is predominantly interacting with. The model classifies the interaction into one of 18 categories, such as electric screwdriver, oscilloscope, and pliers. The split used in our evaluation comprises 644 training samples and 323 test samples. UCF-Crime. UCF-Crime [54] is employed for video classification, categorizing surveillance footage into 13 crime types plus normal events as negative examples. By including all categories in the prompt, we guide the model to classify the video content. We follow the official protocol comprising four splits (532 training / 168 test samples each) and report the average performance across all splits. Xsports. Xsports [56] is an egocentric dataset specifically curated for analyzing extreme sports activities. It features footage characterized by rapid ego-motion and dynamic viewpoints, capturing 18 distinct action categories such as climb, flip, jump, and vault. In our experiments, we task the model with classifying the specific extreme sport action performed in the video into one of the predefined categories. The dataset split comprises 1,191 training samples and 351 test samples. VIOLA 21 MammAlps. MammAlps [23] is multimodal camera trap dataset captured in the Swiss National Park, providing 8.5 hours of densely annotated footage across various wildlife species. It offers hierarchical label structure comprising high-level activities and low-level actions. In our experiments, we focus on the behavior recognition task, where the model classifies the animals activity into one of 11 categories, including foraging, vigilance, and amera reaction. The split employed for evaluation consists of 1,249 training samples and 466 test samples. Bora. Bora [55] is comprehensive biomedical video dataset spanning four domains: endoscopy, ultrasound, MRI, and cell tracking. While originally designed for text-to-video generation, the dataset features video clips paired with detailed, LLM-refined textual descriptions, making it highly suitable for video captioning. In our experiments, we repurpose this corpus to evaluate the models ability to generate accurate, domain-specific medical descriptions from visual inputs. The dataset split consists of 4,407 training samples and 490 test samples. CapERA. CapERA [10] builds upon the Event Recognition in Aerial Videos (ERA) dataset to enable aerial video captioning tasks. It comprises 2,864 videos captured by Unmanned Aerial Vehicles (UAVs), featuring diverse scenarios such as traffic, concerts, and harvesting viewed from an overhead perspective. Each video is annotated with descriptions detailing the main event, objects, and setting. In our experiments, we utilize the split of 1,473 training samples and 1,391 test samples to evaluate the models ability to generate accurate textual descriptions for these high-altitude viewpoints."
        },
        {
            "title": "B Prompt Details",
            "content": "To ensure the reproducibility of our experiments, we provide the exact text prompts used for each dataset in Table 6. For classification tasks, each prompt specifies the domain-specific question and provides the full list of candidate categories to constrain the search space. We consistently append the instruction \"Just answer the name of the category\" (or behavior ) for classification queries to ensure the model outputs valid class label. For captioning tasks, the prompt instructs the model to describe the events concisely."
        },
        {
            "title": "C Qualitative Results",
            "content": "Figure 4 compares qualitative results between the baseline and VIOLA. In Fig. 4(a) on UCF-Crimes, for Normal query, the baseline retrieves irrelevant crime clips (e.g., Robbery), leading to hallucinated Assault prediction. In contrast, VIOLA retrieves high-confidence Normal pseudo-labels. Despite minor noise in the retrieved context, the strong semantic signal guides the model to the correct classification, highlighting VIOLAs robustness in reducing false positives. In Fig. 4(b) on EgoSurgery, the baseline misclassifies Dissection query as Hemostasis by retrieving visually similar but semantically incorrect examples from the limited labeled pool. Conversely, VIOLA retrieves four correctly pseudolabeled Dissection videos. This accurate visual context enables the MLLLM 22 R. Fujii et al. Table 6: Prompt Details. Exact text prompts used for each dataset. The model is tasked to classify the video or generate caption based on these instructions. Dataset Input Prompt UCF-Crime Drive&Act Xsports EgoSurgery MammAlps EgoPet ENIGMA Classify the following video into one of the following categories: Abuse, Arrest, Arson, Assault, Burglary, Explosion, Fighting, Normal Event, Road Accident, Robbery, Shooting, Shoplifting, Stealing, or Vandalism. Just answer the name of the category. Classify the following video into one of the following categories: closing bottle, closing door inside, closing door outside, closing laptop, drinking, eating, entering car, exiting car, fastening seat belt, fetching an object, interacting with phone, looking or moving around, opening backpack, opening bottle, opening door inside, opening door outside, opening laptop, placing an object, preparing food, pressing automation button, putting laptop into backpack, putting on jacket, putting on sunglasses, reading magazine, reading newspaper, sitting still, taking laptop from backpack, taking off jacket, taking off sunglasses, talking on phone, unfastening seat belt, using multimedia display, working on laptop, writing. Just answer the name of the category. Classify the following video into one of the following categories: bumpy forward, climb, curve left, curve right, flip, fly, forward, jump, left, left right, lift, right, roll, run, slide stop, spin, vault, or walk. These represent extreme sports actions. Just answer the name of the category. Classify the following video into one of the following categories: anesthesia, closure, design, disinfection, dissection, dressing, hemostasis, incision, or irrigation in the middle of the video. Just answer the name of the category. Classify the animal behavior in the following video into one of the following categories: camera reaction, chasing, courtship, escaping, foraging, grooming, marking, playing, resting, unknown, or vigilance. Just answer the name of the behavior. What object is the animal wearing the camera interacting with? Choose from the following categories: ball, bench, bird, cat, dog, door, filament, floor, food, NONE, other, other animal, person, plant, plastic, toy, vehicle, or water. Just answer the name of the category. Which tool was the operators right hand predominantly interacting with in the video segment? Choose from the following categories: battery connector, electric screwdriver, electric screwdriver battery, high voltage board, low voltage board, low voltage board screen, oscilloscope, oscilloscope ground clip, oscilloscope probe tip, pliers, power supply, power supply cables, screwdriver, socket 1, socket 2, socket 3, welder probe tip, or welder station. Just answer the name of the category. Bora & CapERA Generate concise caption describing the events and objects in the video. to distinguish fine-grained tool-tissue interactions and correct the prediction, demonstrating the effectiveness of expanding the knowledge base. VIOLA Fig. 4: Qualitative results on UCF-Crimes and EgoSurgery."
        }
    ],
    "affiliations": [
        "Keio AI Research Center",
        "Keio University",
        "NVIDIA"
    ]
}