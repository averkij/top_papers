{
    "paper_title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
    "authors": [
        "Jaehyun Jeon",
        "Jang Han Yoon",
        "Min Soo Kim",
        "Sumin Shim",
        "Yejin Choi",
        "Hanbin Kim",
        "Youngjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 2 6 2 0 5 0 . 5 0 5 2 : r Preprint. Under review. G-FOCUS: Towards Robust Method for Assessing UI Design Persuasiveness Jang Han Yoon Min Soo Kim Sumin Shim Yejin Choi Jaehyun Jeon Hanbin Kim Youngjae Yu Yonsei University jaehyun.jeon@yonsei.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasivenessthe key factor in optimizing user interactions. To address this, we introduce WISERUI-BENCH, benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 realworld UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly."
        },
        {
            "title": "Introduction",
            "content": "User interface (UI) design is key process in application development. While basic aesthetic aspects are fundamental, strategically guiding user experience and interaction flows towards desired behavior is critical. For instance, removing one input field from sign-up page can significantly boost the user registration rate, leading to substantial increase in revenue. This converges with the field of Design Persuasiveness, which views digital interfaces as tools to persuade users to change their behaviors in targeted way (Fogg, 2002). To enhance such UIs persuasiveness, designers employ A/B testing methodology, statistical approach that systematically compares UI variants by randomly assigning users to different versions to identify more persuasive design in maximizing conversion rates or other desired user behaviors. (Kohavi et al., 2009). Advancements in Vision-Language Models (VLMs) have introduced promising avenues for automating UI generation (Si et al., 2025; Li et al., 2025) and evaluation tasks. Duan et al. (2024) leverages language models to evaluate single UI image based on basic design principles such as visual balance and text readability. Wu et al. (2024a) has demonstrated how pretrained multimodal models can effectively capture fundamental UI design attributes. However, these studies have yet tackled the crucial issue of VLM evaluating the persuasiveness of UI design and its impact on user behavior. (Fig. 1 (a)) In this paper, we address two key research questions. (RQ1) How can we quantitatively evaluate VLMs ability in assessing UI design persuasiveness? (RQ2) How can we mitigate biases and enhance the reliability of VLM-based UI design persuasiveness evaluation? 1 Preprint. Under review. Figure 1: Overview of our work. (a) Motivated by A/B testings goal of identifying UI that persuades users towards desired behaviors, our work tackles the challenge of evaluating UI design persuasiveness. (b) G-FOCUS shows unbiased and reliable performance on the pairwise image setting. (c) Our works potential in scalable preference verification of UI pairs aligned with humans. To addresss these challenges, we first introduce WISERUI-BENCH (pairWISE peRsuasive UI BENCHmark), novel benchmark designed for the task: Pairwise UI Design Persuasiveness Evaluation. WISERUI-BENCH consists of 300 real-world UI image pairs, each annotated with designated winner based on actual A/B test results, with expert-curated rationales. By shifting from single-instance evaluation to pairwise evaluation, WISERUI-BENCH enables VLMs to recognize hierarchical relationships in persuasive user interface design, systematically ranking UI variants based on their influence on user engagement. (3) Furthermore, we propose G-FOCUS (Goal-FOcused Contrastive Ui reaSoning), novel inference-time reasoning strategy for VLMs in pairwise UI evaluation tasks. (4) G-FOCUS aims to mitigate common challenges such as position biases and inconsistent rationales observed in existing VLM-based evaluators (Zhang et al., 2023; Ye et al., 2024). By generating explicit comparative rationales between UI variants, G-FOCUS enhances the robustness, exhibiting the most unbiased and reliable VLM-driven assessments compared to baselines, as shown in Figure 1 (b). (5) Our contributions, WISERUI-BENCH and G-FOCUS establish rigorous methodological framework for automated UI design persuasiveness evaluation, bridging the gap between empirical A/B testing and AI-driven design assessment. Furthermore, as shown in Figure 1 (c), the enhancement of VLMs in the automatic assessment of UI design persuasiveness has the potential to enable scalable preference labeling for UI generation, paving the way for preference-considered design automation. (6)"
        },
        {
            "title": "2 Related works",
            "content": "2.1 Visual reasoning Visual reasoning benchmarks for Vision-Language Models (VLMs), such as visual question answering (VQA), image-text retrieval, and visual common sense reasoning (VCR), evaluate 2 Preprint. Under review. general visual understanding through clear, objective tasks. Recent works like MMMU (Yue et al., 2024) and SEED (Li et al., 2024) extend these evaluations to multidisciplinary and video-based tasks, respectively, while benchmarks like Fu et al. (2023), Liu et al. (2024), and Hao et al. (2025) focus on factual and structured reasoning, including object identification. Additionally, the spatio-temporal reasoning capabilities of VLMs are explored in Lim et al. (2025). Despite these advancements, there is still gap in assessing more subjective, usercentered aspects of visual reasoning. Various methodologiessuch as IPVR (Chen et al., 2023), VAP (Xiao et al., 2024), and iterative visual prompting (Nasiriany et al., 2024)have explored ways to enhance VLMs reasoning using interactive, spatial, and visual cues. However, there remains significant potential to further guide VLMs toward emulating human-like reasoning, especially for more subjective tasks. 2.2 VLM-as-a-Judge VLMs have increasingly been employed as evaluators in various tasks, offering scalable and automated assessment capabilities (Lee et al., 2024; Zhang et al., 2023). However, their effectiveness is compromised by inherent biases, notably position biases in multiple images setting (Wang et al., 2024; Zhang et al., 2023), which leads to inconsistent evaluations, as models may prefer earlier or later options regardless of content quality. Beyond position biases, Ye et al. (2024); Saito et al. (2023); Koo et al. (2024) revealed that while advanced models like GPT-4 align well with human preferences overall, range of biases persist, affecting the reliability of LLMs as judges. Addressing these biases is crucial for enhancing the reliability of VLMs in evaluative roles. 2.3 UI design and evaluation Recent advances in LLMs and VLMs have significantly improved automated UI generation and evaluation. LLMs can generate UI code from textual descriptions (Laurencon et al., 2024), while approaches like Wu et al. (2024b) refine this process using automated feedback and multimodal integration. Additionally, benchmarks such as Li et al. (2025) and Si et al. (2025) assess VLMs ability to translate sketches or screenshots into UI code. In alignment with the UI generation trend, AI-driven UI evaluation is explored. Duan et al. (2024) explores multimodal LLMs for evaluating UI designs based on fundamental principles such as readability and layout coherence. However, UI evaluation often extends beyond these static principles to incorporate user preferences and subjective judgments. To address this, pairwise comparison methods have emerged as reliable approach for asessing user preferences in an objective manner (Abel et al., 2020; Bawabe et al., 2021). Given the increasing role of multimodal models in UI assessment, their ability to perform pairwise comparisons is crucial for refining evaluation processes and adapting to complex, usercentered design requirements."
        },
        {
            "title": "3 WISERUI-BENCH",
            "content": "Given that persuading users to behave in desired way is the fundamental essence of UI design, we open up new benchmark, WISERUI-BENCH. Our benchmark comprises real-world UI image pairs, each with definitive winner determined through actual A/B test outcomes. It is also augmented with detailed rationales from the analyses of data sources and UI/UX experts. 3.1 Benchmark construction To construct benchmark reflective of real-world persuasive design decisions, we aim to gather UI images from industry-validated A/B testing data, ensuring reliability and diversity. Figure 2 demonstrates the overall process of benchmark construction. 3 Preprint. Under review. Figure 2: Benchmark construction process. After collecting open raw A/B testing data from reliable platforms, (a) we preprocessed images and preliminary information. (b) Our UI/UX experts then conducted meticulous refinement. (c) Our benchmark includes UI image pairs with verified testing results, along with corresponding categories and rationales. Data sources We aggregate data from widely recognized A/B testing and analyzing platforms1. They provide authentic UI A/B test scenarios, with their results, deployed by globally leading enterprises in diverse industries. Our benchmark adopts real-world design decisions tested at scale, ensuring its credibility as robust evaluation benchmark for practical assessment on UI design persuasiveness. Data curation procedure Our curation process involves multiple stages to ensure structured, high-quality annotations. First, we extract raw UI images and textual descriptions with metadata from authentic A/B test cases sourced from our selected platforms. Next, we refined these raw images by eliminating visual indicators-such as circled regions and arrows-to preserve clean evaluation of visual reasoning. Then, we systematically generate preliminary category assignments-including page type, industry domain, and platform (web versus mobile) classification-and draft rationales from the text descriptions, using GPT-4o (Hurst et al., 2024). Finally, group of three UI/UX professionals performs rigorous manual verification and refinement, supported by robust heuristics, including the Nielsen Norman 10 Usability Heuristics (Nielsen, 1994) and 12 carefully selected laws of user experience deemed pragmatic for crafting substantive rationales explaining how specific UI elements contribute to enhanced persuasiveness. It ensures the reliability and interpretability of the benchmark. Refer to Appendix for more details. 3.2 Benchmark analysis Through comprehensive pipeline, we curated 300 UI image pairs, each annotated with expert-level rationales. We present thorough analysis of WISERUI-BENCH, highlighting its unique strengths and positioning it as pioneering benchmark for evaluating UI design persuasiveness. Benchmark statistics Our benchmark is categorized as shown in Appendix B.1. The diversity of benchmark is critical since effective UI design strategies for persuading users vary depending on the context (Oinas-Kukkonen & Harjumaa, 2018). First, our benchmark includes diverse page types such as landing pages, product/service detail pages, and 1https://vwo.com/, https://mobbin.com/, https://goodui.org/ 4 Preprint. Under review. checkout pages. These page types were identified as critical by our UI/UX experts to represent each part of the user interaction flow within digital applications. Next, we have categorized the industry domains of our UI sites into 11 groups based on RICO (Deka et al., 2017). The retail and e-commerce sector constituted large portion of our samples, primarily due to its propensity for conducting A/B testing to optimize sales performance. Finally, for platform categories, our benchmark consists of 252 web UIs and 48 mobile UIs. In addition, the key UI differences influencing persuasive performance varies widely, aligning with real-world A/B test scenarios that explore diverse UI components. Refer to Appendix B.2 for more details. Benchmark characteristics WISERUI-BENCH stands out with the following key attributes: Objectified persuasiveness: Our benchmark exclusively employs pairwise UI comparisons, mirroring A/B testing methodologies. This comparative approach is crucial for systematically transforming human preferences on subjective UI design persuasiveness into rigorous, measurable insights (Abel et al., 2020; Bawabe et al., 2021). Validated persuasiveness: Unlike manually-rated previous UI datasets (Duan et al., 2024), our benchmark is grounded in definitive A/B test outcomes validated by empirically tested large-scale user interactions. By leveraging these user behavioral data, we establish more reliable approach to assessing UI persuasiveness, ensuring our benchmark reflects high human correlation while surpassing the limitations of traditional heuristic evaluations (Kohavi et al., 2009). Although experts proposed insights into our benchmark, their interpretations serve to elucidate the empirical results rather than contest the primacy of the actual data."
        },
        {
            "title": "4 G-FOCUS",
            "content": "Our work focuses on the task of UI design persuasiveness evaluation using Vision-Language models (VLMs). Drawing inspiration from the module based approach of Chen et al. (2023), which addresses open-world knowledge-based visual reasoning, concept closely related to our task, we propose G-FOCUS, goal-oriented framework designed to assess the persuasiveness of UI designs comparatively. In this context, goal specifically refers to the persuasion goal, the underlying objective that the given UIs aim to achieve by persuading users to think and behave in certain way. Goal extraction is crucial to our methodology, as it involves uncovering the hidden implications embedded within the UI designs. G-FOCUS infers this goal from the input UI images, and this inferred goal guides and influences all subsequent modules. More specifically, G-FOCUS integrates comprehensive approach comprising four key modules as demonstrated in Figure 3: (1) persuasion goal extraction, (2) UI difference localization, (3) contrastive reasoning, and (4) rationale-based evaluation. These method collectively work to determine the most visually persuasive UI aligned with the identified persuasion goals. G-FOCUS operates through multi-phase process, which each phase serving distinct and critical role in the nuanced evaluation of UI designs (see prompt details in Appendix E): 1. Persuasion goal extraction module (Prior step) The process begins by extracting the persuasion goal from industry, which serves as the guiding prior for the entire evaluation. p(G UI1, UI2) where p(G UI1, UI2) represents the conditional probability distribution of the persuasion goal given the two UIs, UI1 and UI2. 2. UI difference localization module In the See phase (Chen et al., 2023), the differences between the two UIs UI1 and UI2 are localized, where UI represents the localized differences between the two UIs based on the extracted goal G: UI = LocalizeDifferences(UI1, UI2 G) 5 Preprint. Under review. Figure 3: G-FOCUS follows four steps: (1) extracting the persuasion goal, (2) localizing UI differences, (3) generating contrastive rationales, and (4) ranking them to identify the most persuasive UI. The goal-driven, modular approach aligns assessments with persuasion objectives, using contrastive reasoning and rationale ranking to provide interpretable insights into UI persuasiveness. 3. Contrastive UI reasoning module The Think phase (Chen et al., 2023) involves generating rationales for both UIs using contrastive reasoning. This step assumes that each UI could be more visually persuasive and evaluates them accordingly. The rationales, ˆr1 and ˆr2 for UI1 and UI2, are generated respectively, based on the goal and the localized differences UI. ˆri = Reason(UIi G, UI), {1, 2} This contrastive reasoning step ensures that each UI is evaluated on its own merit relative to the persuasion goal. 4. Evaluator module The final phase, Confirm,(Chen et al., 2023) involves using an Evaluator to rank the importance of the rationales for both UIs and determine the most visually persuasive UI. The Evaluator makes the final selection by comparing the rationales ˆr1 and ˆr2 and selecting the UI that best aligns with the persuasion goal: ˆUI = Evaluator(ˆr1, ˆr2 G) The output ˆUI is the final selected UI, determined by the Evaluator LLM based on the rationales ˆr1 and ˆr2, and the extracted goal G."
        },
        {
            "title": "5 Empirical study",
            "content": "5.1 Experiment setup In this experiment, we evaluate the performance of Visual Language Models (VLMs) with various inference-time prompting or self-reasoning models for two-stage task using our proposed WISERUI-BENCH. The two-step task involves (1) localizing and identifying differences between pairs of UI images and (2) reasoning to determine which UI is visually more persuasive to users, providing supporting rationale. To analyze potential ordering biases, each pair of UI images is presented twice per candidate, with the order of images reversed between trials. We quantitatively assessed performance using three metrics. The notation preliminaries are as follows: denotes the total number of image pairs, yi,j represents the models predicted 6 Preprint. Under review. output for the i-th image pair in the j-th order, ˆyi is the benchmark-defined correct answer for the i-th pair, and 1[] is an indicator function that evaluates to 1 if the condition inside the brackets is true, and 0 otherwise. Consistency (C): = 1 i=1 1[yi,1 = yi,2] Consistency is measure of unbiasedness. This metric quantifies position bias (Wang et al., 2024; Zhang et al., 2023), indicating the proportion of image pairs for which the VLM provided consistent answers regardless of presentation order. Consistent Accuracy (CA) (primary metric): CA = 1 i=1 1[yi,1 = yi,2 = ˆyi] Consistent Accuracy is measure of reliability. This core metric measures how often the model reliably identifies the correct persuasive UI across both ordering scenarios, reflecting robust persuasive reasoning capabilities without position bias. BERTScore (Zhang et al., 2019): BERTScore measures the semantic alignment between model-generated rationales and benchmark references, providing insights into reasoning coherence and interpretability. It is calculated for image pairs where the models prediction aligns with the ground truth in at least one position, with the final score obtained by averaging across all such cases. 5.2 Baselines Given the unique nature of our task, we extend prior multi-stage structure of visual reasoning frameworks (Chen et al., 2023; Zhang et al., 2024b; Lin et al., 2024) that integrate visual perception and reasoning methods, ensuring relevant comparison for our task. Moreover, we enhanced our baselines by applying OmniParser (Lu et al., 2024), state-of-the-art Set-of-Mark prompting (Yang et al., 2023) method for parsing UI screenshots into structured elements. These parsed UI image pairs were used as inputs for the baselines. The range of baseline approaches is categorized into three distinct groups based on their visual reasoning strategies. Since visual evaluation is inherently subset of visual reasoning, we align our baselines accordingly. The first group consists of models with self-reasoning capabilities, including o1 (Jaech et al., 2024) and LLaVA-CoT (Xu et al., 2025), which perform internal reasoning without explicit external feedback. The second group includes inference-time prompting techniques, such as zero-shot, Contrastive Chain-of-Thought (CoCoT) (Zhang et al., 2024a), Self-Refine (Madaan et al., 2023), Duty-Distinct Chain-of-Thought (DDCoT) (Zheng et al., 2023), and Multi-Agent Debate (MAD) (Liang et al., 2024; Du et al., 2023), with our method also falling within this category. For MAD, we conduct cases moderating the arguments on the first and third debate rounds. We performed Self-Consistency (Wang et al., 2022) over 3 iterative sampling for both the first and second category baselines to ensure the robustness and reliability of the visual reasoning and inference-time prompting strategies. Our evaluations span leading VLMs such as GPT-4o, Claude 3.5 Sonnet (Anthropic, 2024), and Llama3.290B-Vision (Grattafiori et al., 2024). The third group features single UI-based critique and evaluation systems, represented by UICrit (Duan et al., 2024), which conducts structured assessments of individual UIs. We modified this method to our pairwise evaluation task by computing scores for each UI and subsequently comparing them to identify the more persuasive design. Consistency and BERTScore cannot be measured in this instance, as the method does not provide insights into ordering variations or substantive rationales. 7 Preprint. Under review. Category Models Methods Self-reasoning models o1 LLaVA-CoT GPT-4o Inference-time visual reasoning methods Claude 3.5 Sonnet Llama-3.2-90B-Vision Zero-Shot CoCoT Self-Refine DDCoT MAD (Round 1) MAD (Round 3) Ours Zero-Shot CoCoT Self-Refine DDCoT MAD (Round 1) MAD (Round 3) Ours Zero-Shot CoCoT Self-Refine DDCoT MAD (Round 1) MAD (Round 3) Ours Single UI-based evaluation system GPT-4o Claude 3.5 Sonnet UICrit UICrit Answer CA 30. 20.00 38.67 19.67 35.67 36.67 44.67 42.00 48.67 42.00 60.33 30.00 28.33 59.67 31.00 53.67 49.00 65.41 32.67 31.67 46.33 27.33 37.00 31.67 49. 24.67 24.33 27.33 27.67 30.67 27.67 43.33 21.33 20.67 29.67 19.00 34.67 34.00 45.09 14.67 15.33 19.33 11.67 18.67 16.00 26.00 32. 38.00 Rationale BERTScore 60.31 55.46 61.58 61.93 61.70 59.66 61.65 61.57 60.79 63.99 63.94 63.31 63.17 63.47 63.45 63. 59.70 60.11 59.96 59.73 60.09 60.56 60.44 Table 1: Empirical study results, as detailed in Sec. 5.3. Best-performing values are in bold, while the second-best results are underlined for each column. The results show that G-FOCUS surpasses the baselines in both and CA, which = Consistency(%), CA = Consistent Accuracy(%) 5.3 Results Table 1 demonstrates that G-FOCUS outperforms all baselines in Consistency and Consistent Accuracy. With GPT-4o, G-FOCUS achieves +11.66% improvement in Consistency and +12.66% in Consistent Accuracy over the best baseline. For Claude 3.5 Sonnet, it surpasses the baseline by +5.74% in Consistency and +10.74% in Consistent Accuracy. Similarly, for Llama-3.2-90B-Vision, G-FOCUS shows gains of +2.67% and +6.67%, respectively. The results indicate that self-reasoning models underperformed, suggesting their limitations in evaluating UI design persuasiveness in an unbiased and reliable manner. The UICrit method, which assesses UIs individually, performed comparably to the baselines. BERTScore remains relatively similar across the candidates, indicating that rationales were comparable at surface level. Crucially, G-FOCUS higher Consistency and Consistent Accuracy suggest that it produces more accurate and unbiased assessments, reinforcing its effectiveness in contrastive UI reasoning compared to baseline methods. Refer to Appendix C, including qualitative results."
        },
        {
            "title": "6 Discussion and impact of G-FOCUS",
            "content": "Scalable preference verifier for human-preferred UI generation Methodologies for aligning AI models with human preferences, such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), rely on manual human annotations of preference pair data with limited scala8 Preprint. Under review. bility. This issue is resolved by RLAIF, which utilizes LLMs to generate preference labels, thereby facilitating scalable dataset construction. (Bai et al., 2022; Yuan et al., 2024) G-FOCUS advances this paradigm in the field of human-preferred UI generation by introducing visual verification layer that enables scalable and reliable preference labeling for machine-generated UI codes. The methods ability to evaluate UI design persuasiveness naturally aligns with human preferences for determining which UI more effectively persuades users to achieve certain objective of UI. To empirically validate the role of GFOCUS as scalable UI preference verifier, we conducted human study with 55 participants across 10 LLM-generated UI design pair candidates. We compared human assessments with predictions made by the VLM (GPT-4o) using G-FOCUS vs Baseline (MAD). Unlike traditional A/B testing where users are unaware of design objectives, our participants were explicitly instructed to assess which UI would be more persuasive in achieving the shared objective that guided both designs generation. We deliberately chose this approach to simulate UI operators perspectivesomeone seeking to design interfaces that effectively persuade users toward specific behaviors. Meanwhile, the VLMs evaluating these same designs were not provided with these objectives; they simply selected which UI appeared more persuasive, maintaining consistency with our works experimental setup. As shown in Figure 4, while baseline showed low preference alignment (40%), G-FOCUS exhibited superior alignment with human preferences, with 70% UI preference choice aligned with humans selection. Refer to Appendix for more details. This validates G-FOCUSs potential as scalable preference verification approach applicable to methods like DPO, marking step forward in AI-driven human-preferred UI generation. Figure 4: Human preference alignment. GFOCUS vs MAD (round 1). Green: aligned inconsistent (positionpredictions, Yellow: biased) evaluation, and Red: misalignment between predictions and human preferences. G-FOCUS is more aligned to human preferences, whereas the baseline presents significant misalignment. Details are in Appx. D."
        },
        {
            "title": "7 Limitation",
            "content": "Despite efforts to mitigate position bias, cultural biases in persuasiveness and user experience are inherently challenging to eliminate entirely. These biases are inevitable since user experience preferences differ across diverse cultural contexts and societal norms. (Karreman et al., 2016) Also, interactive UIs were not extensively explored in our work. Although the number of 300 UI pairs may limit full generalizability, generally low Consistent Accuracy of the baselines and G-FOCUS prove the valuable insights of our benchmark. Additionally, GFOCUS may not be optimal for accurately predicting UI design persuasiveness, particularly in dynamic or context-dependent interactions. Although G-FOCUS demonstrates relatively strong consistency and accuracy in pairwise UI assessment, we anticipate further refinement in visual reasoning methods to improve its applicability across diverse UI contexts."
        },
        {
            "title": "8 Conclusion",
            "content": "WISERUI-BENCH and G-FOCUS set new standard for automated UI design persuasiveness evaluation. By empowering Vision-Language Models (VLMs) to robustly assess UI designs in pairwise manner, we address key challenges in scaling AI-driven UI preference modeling. This approach not only complements traditional A/B testing but also supports progression for more efficient, scalable methods for evaluating and optimizing UI design. We anticipate that our contributions might be influential in terms of advancing techniques to align AI models with human preferences, fostering an incremental integration of AI 9 Preprint. Under review. into design automation, and supporting more reliable, data-driven approaches in user experience optimization."
        },
        {
            "title": "Ethics Statement",
            "content": "To ensure expert-level annotation quality, we recruited three annotators with professional UX backgrounds currently employed at global strategy consulting firm specializing in AI and user experience. Annotators were clearly informed of the academic nature of the task and voluntarily participated in the study. Each annotator received monetary compensation equivalent to approximately $225 USD per person, which reflects fair market payment for domain-specific consulting labor. Payments were issued within 48 hours of annotation completion. All annotators were instructed to focus solely on task-relevant elements and were not exposed to or asked to label any sensitive, offensive, or triggering content. The human study was conducted with strict ethical considerations. To minimize bias, we employed double-blind setup. All 55 participants voluntarily took part in the survey, fully informed that the study was for academic purposes. Additionally, the survey contained no explicit or problematic content, ensuring safe and respectful research environment."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank the three expert annotators with professional UX backgrounds for their valuable contributions. Their expertise ensured high-quality annotations, and they were fairly compensated for their efforts. We also appreciate the 55 participants who voluntarily took part in our human study. Their participation was essential to our research, which was conducted with strict ethical considerations in double-blind, academic setting."
        },
        {
            "title": "References",
            "content": "Edward Abel, Ixent Galpin, Norman Paton, and John Keane. Pairwise comparisons or constrained optimization? usability evaluation of techniques for eliciting decision priorities. International Transactions in Operational Research, 29, 11 2020. doi: 10.1111/itor. 12907. Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic. com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model Card Claude 3.pdf, 2024. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Sarah Bawabe, Laura Wilson, Tongyu Zhou, Ezra Marks, and Jeff Huang. The ux factor: Using comparative peer review to evaluate designs through user preferences. Proc. ACM Hum.-Comput. Interact., 5(CSCW2), October 2021. doi: 10.1145/3479863. URL https://doi.org/10.1145/3479863. Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. arXiv preprint arXiv:2301.05226, 2023. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pp. 845854, 2017. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. 10 Preprint. Under review. Peitong Duan, Chin-Yi Cheng, Gang Li, Bjoern Hartmann, and Yang Li. Uicrit: Enhancing automated design evaluation with ui critique dataset. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pp. 117, 2024. Hermann Ebbinghaus, Henry Ruger, and Clara Bussenius. Memory: contribution to experimental psychology. 1913. Paul Fitts and James Peterson. Information capacity of discrete motor responses. Journal of experimental psychology, 67(2):103, 1964. Brian Fogg. Persuasive technology: using computers to change what we think and do. Ubiquity, 2002(December):2, 2002. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. William Hick. On the rate of gain of information. Quarterly Journal of experimental psychology, 4(1):1126, 1952. Clark Hull. The goal-gradient hypothesis and maze learning. Psychological review, 39(1): 25, 1932. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Joyce Karreman, Pietro Romeo, and Qian Li. Cross-cultural hci and ux design: comparison of chinese and western user interfaces. Unpublished work, 2016. Kurt Koffka. Perception: an introduction to the gestalt-theorie. Psychological bulletin, 19(10): 531, 1922. Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal Henne. Controlled experiments on the web: survey and practical guide. Data mining and knowledge discovery, 18:140181, 2009. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarking cognitive biases in large language models as evaluators. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 517545, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.29. URL https://aclanthology.org/2024.findings-acl.29/. Hugo Laurencon, Leo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024. Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. Prometheusvision: Vision-language model as judge for fine-grained evaluation. In Findings of the Association for Computational Linguistics ACL 2024, pp. 1128611315, 2024. 11 Preprint. Under review. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329913308, 2024. Ryan Li, Yanzhe Zhang, and Diyi Yang. Sketch2Code: Evaluating vision-language models for interactive web design prototyping. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 3921 3955, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025.naacl-long.198/. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1788917904, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.992. URL https://aclanthology.org/ 2024.emnlp-main.992/. Seungwon Lim, Sungwoong Kim, Jihwan Yu, Sungjae Lee, Jiwan Chung, and Youngjae Yu. Visescape: benchmark for evaluating exploration-driven decision-making in virtual escape rooms. arXiv preprint arXiv:2503.14427, 2025. Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36: 4653446594, 2023. George Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81, 1956. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. Jakob Nielsen. Enhancing the explanatory power of usability heuristics. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pp. 152158, 1994. Jakob Nielsen. End of web design. Alertbox [en lınea] Disponible en: www. useit. com/alertbox/20000723. html, 23(06):2000, 2000. Harri Oinas-Kukkonen and Marja Harjumaa. Persuasive systems design: key issues, process model and system features 1. In Routledge handbook of policy design, pp. 87105. Routledge, 2018. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Vilfredo Pareto. Manuale di economia politica con una introduzione alla scienza sociale, volume 13. Societ`a editrice libraria, 1919. 12 Preprint. Under review. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982 3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410/. Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. Verbosity bias in preference labeling by large language models. arXiv preprint arXiv:2310.10076, 2023. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2Code: Benchmarking multimodal code generation for automated front-end engineering. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39563974, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025.naacl-long.199/. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Hedwig Von Restorff. Uber die wirkung von bereichsbildungen im spurenfeld. Psychologische Forschung, 18:299342, 1933. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham Kakade, Hao Peng, and Heng Ji. Eliminating position bias of language models: mechanistic approach. arXiv preprint arXiv:2407.01100, 2024. Jason Wu, Yi-Hao Peng, Xin Yue Amanda Li, Amanda Swearngin, Jeffrey Bigham, and Jeffrey Nichols. Uiclip: data-driven model for assessing user interface design. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pp. 116, 2024a. Jason Wu, Eldon Schoop, Alan Leung, Titus Barik, Jeffrey Bigham, and Jeffrey Nichols. UICoder: Finetuning large language models to generate user interface code through automated feedback. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 75117525, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.naacl-long.417. URL https://aclanthology.org/2024.naacl-long.417/. Ziyang Xiao, Dongxiang Zhang, Xiongwei Han, Xiaojin Fu, Wing Yin YU, Tao Zhong, Sai Wu, Yuan Jessica Wang, Jianwei Yin, and Gang Chen. Enhancing LLM reasoning via vision-augmented prompting. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=NGuGVT7ar2. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2025. URL https://arxiv.org/abs/2411.10440. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Setof-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 13 Preprint. Under review. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin, Yuan Yao, Mingkai Chen, and Jiebo Luo. Cocot: Contrastive chain-of-thought prompting for large multimodal models with multiple image inputs. arXiv preprint arXiv:2401.02582, 2024a. Mingyu Zhang, Jiting Cai, Mingyu Liu, Yue Xu, Cewu Lu, and Yong-Lu Li. Take step back: Rethinking the two stages in visual reasoning. In European Conference on Computer Vision, pp. 124141. Springer, 2024b. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as generalist evaluator for vision-language tasks. arXiv preprint arXiv:2311.01361, 2023. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191, 2023. WISERUI-BENCH curation details A.1 Image preprocess During our benchmark construction process, we observed that some UI images from data source websites contained manual indicators-such as circled regions, numbered markers, and arrows-highligting specific UI elements related to reasoning points in A/B test results (see Fig. 5). These indicators, while useful for human analysts, introduced noise that could potentially interfere with our experiments by creating artificial attention points. To address this, we developed systematic approach to clean the dataset using inpainting techniques. We identified common indicator patterns including: (1) circular or oval highlights typically drawn in semi-transparent colors, (2) numbered markers placed near UI elements, and (3) directional indicators such as arrows. Using combination of edge detection and color-based segmentation, we identified these artificial markers. We then applied state-ofthe-art inpainting algorithm and manually finalized the process to remove these annotations while preserving the underlying UI elements and maintaining visual consistency with the surrounding content. This cleaning step helped create more naturalistic dataset that better represents real-world UI comparisons, enabling fair measurement of model performance on visual reasoning. A.2 Rationale annotation As shown in Figure 6, UI/UX experts are presented with pair of UI images, one of which was designated as the winner based on A/B test results, along with source website link. We engaged experts for annotation since user decision-making is often subconscious and influenced by implicit behavioral cues, making manual evaluation by non-experts is 14 Preprint. Under review. Figure 5: Example of visual indicator removal in WISERUI-BENCH. Left: Original image with markers added by data source website. Right: After inpainting, with markers removed while preserving UI elements. inherently unreliable. Experts are instructed to carefully examine these materials and revise the category assignments as well as the rationales for why winner version succeeded, which was preliminarily drafted by GPT-4o, based on set of pre-defined UI design persuasivness evaluation guidelines. The rationales are structured to specify (1) which UI changes were made and (2) how these changes contributed to the winning versions success in the A/B test. Expert annotators selected two types of robust theoretical heuristics for persuasiveness: the Nielsen Norman 10 Usability Heuristics (Nielsen, 1994) as broad conceptual overall criteria and 12 curated laws still widely utilized in UX domain as practically applicable criteria, which covers diverse aspects of persuasiveness, as follows: 1. Law of Common Region states that elements sharing boundary or background are perceived as unified group. This can be visible border, colored box, or distinct region in the layout. (Koffka, 1922) 2. Law of Proximity states that items positioned close together are seen as belonging to the same group. This mental shortcut helps users quickly identify relationships between adjacent elements. (Koffka, 1922) 3. Law of Pragnanz states that humans naturally perceive and interpret complex images in their simplest forms. It reduces cognitive effort by helping us group and understand elements more quickly. (Koffka, 1922) 4. Law of Similarity states that people group together elements that look alike, assuming they serve similar purpose. Consistency in color, shape, or size leads the user to see these items as related. (Koffka, 1922) 5. Von Restorff Effect states that people remember distinct element more easily than others that blend in. This is also known as the Isolation Effect, emphasizing our attention on anything unusual. (Von Restorff, 1933) 6. Serial Position Effect states that users tend to remember the first and last items in series more than those in the middle. This influences how they recall sequential information or actions. (Ebbinghaus et al., 1913) 7. Millers Law states that on average, people can hold only about seven items in their working memory at once. Going beyond this limit causes cognitive overload. (Miller, 1956) 8. Hicks Law states that decision time rises with the number and complexity of choices. Too many options can overwhelm users and slow their actions. (Hick, 1952) 9. Pareto Principle states that roughly 80% of outcomes stem from 20% of causes. Focusing on the most impactful features or tasks delivers the greatest improvement. (Pareto, 1919) 15 Preprint. Under review. 10. Jakobs Law states that users expect new interfaces to operate similarly to those theyve used before. Leveraging established patterns lowers cognitive load and accelerates adoption. (Nielsen, 2000) 11. Fittss Law states that the time required to reach target depends on its size and distance. Larger, closer elements are easier to access quickly and accurately. (Fitts & Peterson, 1964) 12. Goal-Gradient Effect states that motivation increases as people get closer to finishing task. Providing clear progress markers encourages users to continue and complete it faster. (Hull, 1932) 16 Preprint. Under review. Figure 6: screenshot of the annotation interface for UI/UX experts to revise category assignments and rationales based on guidelines. Preprint. Under review. WISERUI-BENCH statistics details B.1 Categorization The distribution details of our benchmarks categorization are shown in Figure 7, 8, 9. B.2 UI difference diversity Figure 10 demonstrates the diversity of UI differences of WISERUI-BENCH, using the visualization of t-SNE (Van der Maaten & Hinton, 2008). All ui differences are embedded using SentenceBERT (Reimers & Gurevych, 2019) with 768-dimensional vectors. Prior to embedding, the text was lowercased and cleaned of special characters. We then applied t-SNE with perplexity of 30, 2000 iterations, and Euclidean distance to reduce the dimensionality for visualization."
        },
        {
            "title": "C Experiment details",
            "content": "C.1 Computational overhead Token count details are in Table 2. Models Methods Input Tokens Output Tokens Zero Shot GPT-4o Claude Zero Shot CoCoT Self-Refine DDCoT MAD (R1) MAD (R3) Ours Zero Shot CoCoT Self-Refine DDCoT MAD (R1) MAD (R3) Ours 4842.22 ( 2112.66) 4839.5 ( 2113.0) 4844.53 ( 2113.06) 6826.28 ( 2707.54) 4942.52 ( 2113.06) 8676.84 ( 3311.34) 15881.36 ( 5822.08) 7547.21 ( 2557.54) 5778.8 ( 3133.91) 5786.41 ( 3136.19) 8474.11 ( 4147.64) 5895.8 ( 3133.92) 10477.54 ( 5172.73) 19285.36 ( 9349.53) 8918.07 ( 4220.57) 1132.96 ( 397.23) 231.0 ( 55.36) 238.65 ( 58.73) 387.67 ( 69.97) 431.6 ( 78.79) 564.5 ( 74.49) 1281.68 ( 127.03) 519.37 ( 78.18) 432.2 ( 54.28) 478.16 ( 66.13) 683.94 ( 82.98) 797.85 ( 79.80) 923.22 ( 82.00) 1964.24 ( 135.70) 1023.52 ( 116.77) Table 2: Overview of Token Counts (µ σ) C.2 Qualitative examples Our experiment cases comparing the results from G-FOCUS and baselines are shown in Figure 11, 12, 13. 18 Preprint. Under review. Figure 7: Page type distribution of WISERUI-BENCH Figure 8: Industry distribution of WISERUI-BENCH Figure 9: Web vs. Mobile distribution of WISERUI-BENCH 19 Preprint. Under review. Figure 10: The visualization of WISERUI-BENCHs UI differences. 20 annotations shown in the figure, randomly selected to inspect cluster patterns and semantic groupings, show diversity of our benchmark. 20 Preprint. Under review. Figure 11: As shown in the figure, our method produces consistent responses across our benchmark, whereas one of the baselines, DDCoT, exhibits inconsistent behavior. 21 Preprint. Under review. Figure 12: The figure demonstrates that our method maintains consistency across the benchmark, while MAD (Round 1), one of the baseline methods, exhibits inconsistency in its responses. 22 Preprint. Under review. Figure 13: The figure illustrates that our method maintains consistent performance across the benchmark, while Self-Refine, baseline method, fails to provide consistent responses. 23 Preprint. Under review."
        },
        {
            "title": "D Human study on UI generation preference details",
            "content": "D.1 Results Table 3 presents the results of the user study, detailing the distribution of participant responses across 10 A/B questions. For each question, the option selected by the majority is highlighted in bold. The rows labeled G-FOCUS and MAD(R1) represent the answers from two different systems or annotation rounds, with bold text indicating alignment with the majority vote. G-FOCUS matched the majority choice in 7 out of 10 questions, while MAD achieved 4 matches. Instances labeled as in refer to inconsistent or ambiguous selections, where the position difference (e.g., (left, right) vs. (right, left)) led to differing choices. G-FOCUS had 1 inconsistent case, while MAD(R1) had 3, indicating potential bias in the assessment. Additionally, G-FOCUS had 2 mismatches, while MAD had 3 mismatches. Figure 14, 15, 16 show our qualitative results. Choice Human Choice (GT) G-FOCUS MAD (R1) Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 42 13 9 46 B 23 32 14 41 B in 25 30 40 15 A in 14 41 in 27 28 in 35 20 18 37 A 7 4 1 3 2 3 Table 3: Human study results. tent(biased) cases, and denotes Misaligned cases) (H denotes Human-Aligned cases, denotes InconsisD.2 Human labeling interface Figure 17 displays the interface used for human evaluation, where participants select which generated UI better adheres to the given instructions. 24 Preprint. Under review. Figure 14: The figure shows that our method produced consistent responses, whereas MAD failed to provide correct answers in both cases, indicating biased decision. Preprint. Under review. Figure 15: The figure shows that our method provided consistent answers regardless of the image order, while MAD produced inconsistent responses when the image positions were switched. 26 Preprint. Under review. Figure 16: The figure shows that while our method produced inconsistent responses, MAD provided correct answers in both cases. However, human vote distributions were nearly balanced between and B, suggesting that an inconsistent response in this case may actually be reasonable reflection of the ambiguity present in the data. Preprint. Under review. Figure 17: screenshot of user study interface 28 Preprint. Under review. G-FOCUS prompts (Part 1) Persuasion Goal Extraction Your task is to think about the goal of the site operator for the given UIs. The two screenshots show two different versions of the same page. FYI, the page is from the company called {company name} whose industry domain is industry domain}. The page type is {page type} on {web mobile} environment. Based on the two UIs and the industry and page type, what is the main goal of the site operator for this page? Answer strictly in the following format: [Goal] <goal of the site operator considering industry and page type> (Part 2) UI Difference Localization You are an expert in designing UI/UX for web/apps. The two screenshots show two different versions of the same page. FYI, the page is from the company called {company name} whose industry domain is industry domain}. The page type is {page type} on {web mobile} environment. Your task is to find the crucial and major UI differences between the two versions. To do that, (1) Reflect what would be the fundamental and crucial design priorities on the current page, if the goal of the site operator on the current page is as follows: {goal} (2) See the provided screenshots of the page, focusing on the key UI areas related to what you have reflected as your prioritized design principles. (3) For each key UI area, localize it by imagining patches and highlight main UI differences within the patches of the versions. Be aware: - Mention key differences in terms of the given goal and design priorities. - Do not mention differences in an abstract nor subjective way. (example to avoid: First UI has more prominent visual clutter. more prominent: subjective expression, visual clutter: abstract expression) - Avoid mentioning differences which can be placeholders that can differ by example cases. (example to avoid: First UI has product image of apple, Second UI has banana. product images can be just placeholders) - Prefer to infer comprehensive differences that can be inferred by combining multiple UI components from the images and given information. You must give the answer and give the answer strictly in the following format: [Design priorities] 1. <Key priority> 2. <Key priority> ... [UI areas to focus] Preprint. Under review. 1. <Key UI area> 2. <Key UI area> ... [Key UI differences] First UI <Key UI difference>, Second UI <Key UI difference>. First UI <Key UI difference>, Second UI <Key UI difference>. ... (Part 3) Contrastive UI Reasoning (Case assuming first UI more persuasive) You are an expert in designing UI/UX for web/apps. The two screenshots show two different versions of the same page. FYI, the page is from the company called {company name} whose industry domain is industry domain}. The page type is {page type} on {web mobile} environment. Assume that user is currently engaging with the page shown in the screenshot. Inferred main goal of the site operator is as follows: {goal} Already found key UI differences between the two versions are as follows: {ui diff} Then, assuming first version was more visually persuasive in terms of achieving the inferred goal, you should make reasonable reasons for such result. Your reply should strictly follow the format.: [Evaluation] 1. <Rationale of the evaluation in sentences> 2. <Rationale of the evaluation in sentences> ... (Part 4) Evaluator You are an expert in designing UI/UX for web/apps. The two screenshots show two different versions of the same page. FYI, the page is from the company called {company name} whose industry domain is industry domain}. The page type is {page type} on {web mobile} environment. Assume that user is currently engaging with the page shown in the screenshot. Inferred main goal of the site operator is as follows: {goal} Already found key UI differences between the two versions are as follows: {ui diff} Then, we have made possible reasons of why first version would have been more visually persuasive in terms of achieving the inferred goal: {first reason} We also made possible reasons of why second version would have been more visually persuasive in terms of achieving the inferred goal: 30 Preprint. Under review. {second reason} Considering all these possible reasons, your task is to conclude which version would be visually persuasive in terms of achieving the inferred goal. Reasons decisions of first/second version are all different, so you should think carefully reminding: (1) Reasons may contradict each other, then you should decide which is more reasonable. (2) Then make your own rankings between the evaluations based on importance for making final decision. (3) Based on the rankings, conclude which version would be visually persuasive in terms of achieving the inferred goal with precise overall rationale that only contains key points that you think are crucial. Here, such precise overall rationale should be consisted of key points mentioning: (1) which UI difference was key to the winning versions success and (2) how each UI difference affected the winning version positively Your reply should strictly follow the format: [Importance Ranking] 1. <First/Second + Reason # (e.g. First 2)> - <Why you think this reason is the most important> 2. <First/Second + Reason # (e.g. Second 3)> - <Why you think this reason is the second important> ... [Conclusion] Better version: <First/Second> Key Rationale: * <UI Difference>: <Positive effects> * ... (if multiple)"
        },
        {
            "title": "F Model configuration",
            "content": "Models that we have used are o1 (Jaech et al., 2024), LLaVA-CoT (Xu et al., 2025), GPT-4o (Hurst et al., 2024), Claude 3.5 Sonnet (Anthropic, 2024), Llama-3.2-90B-Vision (Grattafiori et al., 2024). Model Model ID Temperature o1 LLaVA-CoT GPT-4o Claude 3.5 Sonnet Llama-3.2-90B-Vision o1-2024-12-17 Llama-3.2V-11B-cot gpt-4o-2024-11-20 claude-3-5-sonnet-latest llama-3.2-90b-vision-preview 1 1 1 1 1 Table 4: Model configuration"
        },
        {
            "title": "G Icon attribution",
            "content": "Icons used in this paper are from Flaticon https://www.flaticon.com, and they are attributed to the respective authors as required by Flaticons license."
        }
    ],
    "affiliations": [
        "Yonsei University"
    ]
}