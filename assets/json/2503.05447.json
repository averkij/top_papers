{
    "paper_title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
    "authors": [
        "Weigao Sun",
        "Disen Lan",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE."
        },
        {
            "title": "Start",
            "content": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts Weigao Sun1, Disen Lan1,2, Tong Zhu3, Xiaoye Qu1, Yu Cheng4 1Shanghai AI Laboratory, 2South China University of Technology, 3Soochow University, 4The Chinese University of Hong Kong 5 2 0 2 7 ] . [ 1 7 4 4 5 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce LinearMoE, production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for LinearMoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as next-generation foundational model architecture. Code: https://github. com/OpenSparseLLMs/Linear-MoE."
        },
        {
            "title": "Introduction",
            "content": "Mixture-of-Experts (MoE) (Jacobs et al., 1991; Qu et al., 2024) architectures have gained widespread adoption in cutting-edge models within industry, with prominent examples including Gemini1.5 (Reid et al., 2024) and the reported use of MoE Project lead. Corresponding to Weigao Sun (sunweigao@outlook.com). Work done during Disen Lans internship at Shanghai AI Laboratory. 1 in GPT-4 (Chintala, 2023). Other notable large models incorporating MoE techniques include Mixtral (Jiang et al., 2024), DeepSeek V2 (Liu et al., 2024), Qwen2 (Yang et al., 2024a), JetMoE (Shen et al., 2024), Jamba (Team et al., 2024), and OLMoE (Muennighoff et al., 2024). Most advances on MoE studies primarily concentrate on modifying the routing mechanism or expert layers, while typically keeping the attention layers unchanged (Zhu et al., 2024b). These attention layers commonly rely on the softmax self-attention mechanism introduced in the Transformer architecture (Vaswani et al., 2017). The softmax-based self-attention has proven to be highly effective for sequence modeling tasks across various data types. However, significant limitation of this mechanism is its computational complexity, which grows quadratically with the input sequence length. This complexity can lead to substantial computational costs, especially during training, making it challenge for models need to handle long sequences efficiently. Linear sequence modeling (LSM) has recently gained significant attention due to its impressive efficiency in both training and inference. These methods function similarly to recurrent neural networks (RNNs) with matrix-valued hidden states, allowing them to achieve linear-time training and constantmemory inference. This efficiency is largely due to the fact that LSM techniques bypass the computation of attention scores and eliminate the need for maintaining key-value (KV) cache. There are three primary approaches to linear sequence modeling: linear attention (Katharopoulos et al., 2020), state space models (SSM) (Gu and Dao, 2023; Dao and Gu, 2024; Hu et al., 2024; Waleffe et al., 2024), and linear RNN (Peng et al., 2023, 2024; Qin et al., 2024d). Linear attention is variation of the traditional softmax attention mechanism, replacing the exponential kernel with simpler dot product between key and query vectors, which enables the use of the right-product kernel trick to reduce computational complexity. SSM approaches, such as Mamba and Mamba2, stem from control theory and represent sequence modeling as dynamic systems. Meanwhile, linear RNN methods address the limitations of traditional RNNs in modeling long contexts by enabling parallel training of RNN models. These different methods, linear attention, SSM, and linear RNN, share common mathematical foundation and exhibit similar performance on sequence modeling tasks (Dao and Gu, 2024; Peng et al., 2024; Qin et al., 2024b; Yang et al., 2024c). In fact, they all employ unified recurrence framework expressed as Ms = Ms1 + (cid:99)Ms, where Ms denotes the memory state and (cid:99)Ms represents the incremental memory update at the s-th token. In this paper, we introduce Linear-MoE, production-level system designed for modeling and training of large-scale MoE models with LSM modules integrated. The Linear-MoE system is composed of two key subsystems: Modeling and Training. The Modeling subsystem provides unified linear sequence modeling framework for LinearMoE models. It supports three main types of LSM methods: linear attention, state space model (SSM), and linear RNN. For each type, multiple instances are implemented under unified formulation. While the Training subsystem is designed to achieve efficient training of Linear-MoE models on modern accelerators. In addition to supporting state-of-the-art training techniques, we incorporate specialized Sequence Parallelism (SP) technique for LSM modules, which is particularly effective for handling extremely long input sequences on Linear-MoE architecture. Importantly, the system is designed to be extensible, enables more advanced sequence modeling methods or training techniques integrated in the future. Furthermore, we also explore efficient modeling and training for hybrid Linear-MoE models, which combine Linear-MoE layers with standard Transformer-MoE layers. For hybrid models, we introduce an SP method that employs distinct computational and communication strategies tailored to the different types of layers. Our contributions can be summarized as follows: Production-level System. We introduce LinearMoE, production-level system designing for efficient modeling and training of large-scale MoE models with LSM modules integrated. Modeling & Training Subsystems. The LinearMoE system is composed of two subsystems: Modeling and Training. We provide unified linear sequence modeling formulation to support various LSM modules with MoE layers, as well as state-of-the-art training techniques for efficient large-scale model training, especially on long-context inputs. Experimental Validation. In empirical studies, we pretrain two series of Linear-MoE models from scratch on the public SlimPajama corpus. Extensive experiments validate the training and inference efficiency of our system framework, as well as the performance of Linear-MoE architecture on various downstream tasks."
        },
        {
            "title": "2 Linear-MoE System",
            "content": "2.1 Modeling 2.1.1 Unified Linear Sequence Modeling The standard softmax attention (Vaswani et al., 2017), commonly used in transformer models, whose parallel computation form during training can typically be expressed as: = Softmax(QK)V. (1) Here, the matrices Q, K, V, RN correspond to the query, key, value, and output matrices, respectively. The matrices Q, K, and are linear projections of the input matrix RN d, defined as = XWQ, = XWK, and = XWV , where WQ, WK, WV Rdd are learnable weight matrices. Here, and represent the sequence length and hidden dimension. Linear Attention (Katharopoulos et al., 2020) as one of the representative LSM methods, has emerged as viable alternative to traditional softmax attention by implementing two primary modifications. First, it eliminates the Softmax() operation, instead embedding it within kernel feature map. Second, it leverages the associative property of matrix multiplication, reconfiguring (QK)V into Q(KV). These changes reduce both the computational and memory complexity from O(N 2d) to O(N d2). This approach is frequently referred to as the right-product kernel trick, as it prioritizes matrix product on the right side. While during inference, both softmax selfattention and linear attention handle single token at each iteration. Given the s-th token xs R1d, softmax self-attention computes requiring the storage of an expanding set of keys {k1, , ks} and values {v1, , vs} i.e., the KV cache, which leads 2 Table 1: Instances of Linear Sequence Modeling Methods. All instances listed follow the unified formulation in Eq. (5). Here, R, as R, as Rd, Rdd, As Rdd represents fixed constant, time-dependent scalar, time-dependent vector, time-independent matrix, and time-dependent matrix, respectively. Note that the same notation may denote different variables in different instances. LSM Method Instance Recurrent Update Rule Linear Attention Ms = Ms1 + vs BLA Lightning Attn Ms = aMs1 + vs Ms = aMs1 + vs RetNet Ms = diag{as}Ms1 + GLA Ms = (I ask DeltaNet Ms = Ms1 + ϕ(ks)vs Rebased Ms = As Ms1 + GFW Ms = As Ms1 + GateLoop Gated DeltaNet Ms = as(I TTT Titans vs vs ks)Ms1 + bsk Ms = Ms1 + bsl(Ms1; ks, vs) Ms = asMs1 + bsl(Ms1; ks, vs) ks)Ms1 + bsk vs vs vs SSM * Linear RNN S4 Mamba Mamba2 HGRN2 RWKV6 RWKV Ms = exp((a1)A) Ms1 + (a1)bvs Ms = exp((as1)As) Ms1 + (as1)k Ms = exp(abs) Ms1 + bsk vs Ms = diag{as}Ms1 + (1 as)vs Ms = diag{as}Ms1 + Ms = diag{as}Ms1 + l(Ms1; ks, vs) vs vs Parameter as Rd as, bs As Rdd As Rdd as, bs bs as, bs a, Rd, Rdd as Rd, As Rdd a, bs as Rd as Rd as Rd * For both S4 and Mamba, the Euler Discretization (Gu et al., 2020) is applied, such that = B, and the unprojected xs is denoted as vs for consistency with other formulas. to significant memory burden when dealing with long input sequences: qs, ks, vs = xsWQ, xsWK, xsWV , os = (cid:80)s )vi i=1 exp(qski (cid:80)s i=1 exp(qsk ) . (2) Linear attention replaces the term exp(qsk ) with kernel k(x, y) with an associated feature map ϕ, i.e., k(x, y) = ϕ(x), ϕ(y). This simplifies the calculation of os as os = (cid:80)s i=1 ϕ(qs)ϕ(ki)vi i=1 ϕ(qs)ϕ(ki) . (cid:80)s i=1 ϕ(ki)vi and zs = i=1 ϕ(ki) where Ms Rdd, zs Rd1, we Letting Ms = (cid:80)s (cid:80)s (3) can rewrite Eq. (3) as an RNN: Ms = Ms1 + ϕ(ks)vs, zs = zs1 + ϕ(ks), ϕ(qs)Ms ϕ(qs)zs os = . (4) Follow-up studies on SSM (e.g., Mamba2) and linear RNNs (e.g., RWKV6, HGRN2), have demonstrated their similarity with linear attention (Dao and Gu, 2024; Peng et al., 2024). In fact, recent 3 studies (Qin et al., 2024b; Yang et al., 2024c) have suggested that linear attention, state space, and linear RNN sequence modeling methods can be expressed within unified recurrence framework as: (cid:99)Ms = (k , vs), Ms = Θs Ms1 + (cid:99)Ms. (5) In this formulation, (cid:99)Ms Rdd represents the memory state corresponding to the s-th input, which is function of and vs. And Θs denotes coefficient matrix that may be time-varying or constant (and also can be vector or scalar). The operator \"\" can denote either standard matrix multiplication or Hadamard product. We collect recent LSM method instances which follow the unified formulation in Eq. (5) and list them in Table 1. 2.1.2 Linear-MoE Architecture The Linear-MoE architecture is relatively straightforward, consisting of stacked Linear-MoE blocks, as depicted in Fig. 1. Each Linear-MoE block includes an LSM layer and an MoE layer, with normalization layer preceding each. The LSM layer serves as generalized structure that supports various LSM methods, specifically, linear attention, SSM, and linear RNN, each encompassing multiple instance methods. Table 1 provides Figure 1: Linear-MoE Architecture. In each Linear-MoE block, there is both an LSM layer and an MoE layer, with each layer preceded by its own normalization layer. The LSM layer is designed as flexible abstraction of LSM methods, including: linear attention, SSM, and linear RNN, which follows unified recurrence framework. an overview of these LSM method instances, unified under common recurrence framework. This framework highlights key distinctions between various LSM instances, primarily in their handling of the prior-step memory state Ms1 and the computation of the incremental memory state (cid:99)Ms. For the MoE layers, we retain the standard mechanisms of sparse expert activation and routing, as employed in SOTA open-source MoE models. These mechanisms are essential for maintaining an optimal balance between model performance and computational efficiency. In this paper, we refer to models composed exclusively of Linear-MoE layers as pure LinearMoE models. These models achieve high efficiency during both training and inference, benefiting from the LSM modules embedded in each layer. However, despite these advantages, empirical research (Lieber et al., 2024; Ren et al., 2024; Waleffe et al., 2024) has shown that models relying solely on LSM modules tend to underperform on tasks requiring strong recall capabilities, such as in-context learning (e.g., five-shot MMLU (Hendrycks et al., 2020), Phone-book lookup (Jelassi et al., 2024), Needle In Haystack (Briakou et al., 2023)) and long-context reasoning. In such cases, hybrid architecture that interleaves linear transformer layers with standard transformer layers has proven effective in improving model performance on recallintensive tasks (Yang et al., 2024b; MiniMax et al., 2025; Lan et al., 2025). Based on this prior, we propose hybrid LinearMoE architecture that combines Linear-MoE layers with standard (MoE) transformer layers. practical approach for constructing these hybrid models is to periodically substitute certain LinearMoE layers with standard MoE transformer layers within the model. For instance, in an 4-layer hybrid Linear-MoE model, denoted by \"L\" for LinearMoE layers and \"N\" for normal transformer layers, configurations such as \"LLLL\" or \"LNLN\" may be used, depending on the desired ratio of normal transformer layers, which can be adjusted based on user preference. 2.2 Training 2.2.1 Sequence Parallelism on Linear-MoE The existing methods, LASP (Sun et al., 2024) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication. In this work, we extend the capabilities of LASP series to the Linear-MoE system, allowing for the efficient SP training on LSM modules, particularly when dealing with extremely long sequences across large-scale distributed clusters. This extension significantly enhances the scalability and efficiency of training large-scale Linear-MoE models with long-context sequences on extensive compute re4 Figure 2: Sequence Parallelism Approach on Hybrid Linear-MoE models. We exemplify the parallelism on the hybrid layers of LSM and standard attention with both TP and SP (both have dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reducescatter in backward, RS/AG: reduce-scatter in forward and all-gather in backward, AG/No: all-gather in forward and no-op in backward, No/AG: no-op in forward and all-gather in backward. Note that the SP communication operations for linear attention operate on the memory state Ms Rdd, while for standard attention, they operate on states Ks, Vs RCd. sources. detailed breakdown of the SP algorithm on Linear-MoE, with and without masking, is provided in Appendix A.3. 2.2.2 Hybrid Model Sequence Parallelism Hybrid linear sequence modeling models, which combine linear transformer layers (leveraging LSM methods for token mixing) with standard transformer layers (utilizing conventional selfattention for token mixing), have demonstrated substantial improvements in handling long-context tasks (Lieber et al., 2024; Ren et al., 2024; Waleffe et al., 2024). This hybrid model is particularly beneficial for tasks with high recall requirements, including five-shot MMLU (Hendrycks et al., 2020), Phone-book lookup (Jelassi et al., 2024), and Needle In Haystack (Briakou et al., 2023), etc.. Our proposed hybrid Linear-MoE models also aim to enhance performance in areas where pure LinearMoE models have shown limitations, specifically on tasks where recall precision is critical. Applying SP on pure Linear-MoE models is straightforward, as this form of SP operates exclusively on the LSM modules, leaving the MoE layers unaffected. In hybrid Linear-MoE models, however, implementing SP becomes more complex due to the interleaving of distinct sequence modeling layers. To effectively optimize SP for these hybrid models, we introduce an integrated approach that incorporates SP across both the linear-MoE and standard transformer layers, thus enhancing overall efficiency. We illustrate the approach in Fig. 2, and explain it as below: On LSM Module. The SP for LSM modules is implemented via single collective communication operation on the memory state Ms Rdd. This approach ensures that the communication complexity does not depend on either the sequence or sub-sequence length; rather, it scales only linearly with the SP size , thereby maintaining efficiency in distributed setups. On Standard Attention Module. Context parallelism (CP) is SP technique used in MegatronLM that divides input data and activations along the sequence dimension, specifically designed for standard softmax attention. Traditional CP implementations in Megatron-LM rely on ring-like communication-computation overlap (Liu et al., 2023). In contrast, our approach for standard attention modules adopts the all-gather-based strategy used in the pretraining of Llama3 (Dubey et al., 2024). Rather than utilizing ring strategy, we perform all-gather communication for Ks and Vs tensors across devices, followed by local computation of attention output on each devices chunk of Qs. While all-gather communication theoretically has higher latency than ring-based methods, it offers greater flexibility and adaptability for handling different attention masks, such as documentlevel masks, making it ideal for varying attention patterns. Moreover, the latency of all-gather is minimized since the Ks and Vs tensors are notably smaller than the Qs tensor, especially with grouped query attention (Ainslie et al., 2023). Consequently, the computational time for generating attention output significantly outweighs the cost of all-gather communication. 5 2.2.3 Hybrid Parallelism SP in Linear-MoE allows for flexible choice of sequence parallel size that can be set to any factor smaller than or divisible by the total number of distributed nodes (i.e., the world size). This flexibility enables splitting input data across both batch and sequence dimensions, creating combined approach known as data-sequence hybrid parallelism. Standard data parallelism techniques, such as Distributed Data Parallel (DDP) (Li et al., 2020), can integrate seamlessly with SP in Linear-MoE. Additionally, the sharded data parallelism method, like Distributed Optimizer (Korthikanti et al., 2022) in Megatron-Core, is also compatible. Furthermore, the system provides support for Tensor Parallelism (TP), Pipeline Parallelism (PP), and Expert Parallelism (EP) specifically tailored for Linear-MoE models. In the case of TP, its application to Linear-MoE models is direct and efficient, as detailed in A.2. Regarding PP and EP, these parallelism techniques operate on Linear-MoE in much the same way as their original versions since they are not involved in the inner computations of the LSM modules but rather work at the level of complete Linear-MoE blocks or MoE layers. Moreover, TP, PP, and EP can be combined with DP and SP as introduced earlier, enhancing flexibility and scalability for large distributed setups. 2.2.4 Variable Length During pretraining, batches generally consist of sequences with uniform length. However, in the finetuning phase or during inference, the model often encounters batches containing sequences of different lengths. common approach to handle this variation is to right-pad each sequence in the batch so that all match the length of the longest sequence in that batch. While straightforward, this padding strategy can lead to inefficiencies, particularly when sequence lengths vary greatly within batch. For standard transformers, more advanced methods have been introduced to address this issue. These methods include techniques like distributing workloads across GPUs to avoid padding altogether (Zeng et al., 2022; Zhai et al., 2023), or packing multiple sequences into single batch while adjusting the attention mask as needed (Ding et al., 2024; Pouransari et al., 2024). In Linear-MoE, handling variable-length sequences is simplified by processing the entire batch as one continuous long sequence, effectively managing varying sequence lengths without the need for padding. Figure 3: Linear-MoE System Implementation. The Linear-MoE system is composed of two main subsystems: Modeling and Training. It is developed in non-intrusive manner, utilizing the latest version of Megatron-Core. All components within the system are designed with extensibility in mind, encompassing the LSM modules, base models, examples, and training technologies. This design allows for future enhancements and extensions of the system. 2.3 Implementation The implementation of the Linear-MoE system is based on Megatron-Core, an open-source library developed on PyTorch that incorporates optimized GPU techniques and advanced system-level enhancements. As depicted in Fig. 3, the Linear-MoE system consists of both modeling and training subsystems, facilitating adaptable model building and efficient training specifically for Linear-MoE models. Leveraging the capabilities of Megatron-Core, the Linear-MoE library is fully compatible with all NVIDIA Tensor Core GPUs, including support for FP8 acceleration on NVIDIA Hopper architectures. The Linear-MoE design approach aims to minimize any invasive changes to Megatron-Cores source code. Rather than adding new modules directly, Linear-MoE operates independently, allowing users to benefit from the latest LLM practices without disruptions due to updates or changes within Megatron-Core. 2.3.1 Modeling Subsystem Linear-MoE abstracts its LSM modules into modular and composable APIs, providing model developers and researchers with extensive flexibility to design and train large-scale custom Linear-MoE models on accelerated computing platforms. The system includes essential building blocks, such as 6 core components for LSM mechanisms, MoE layers and Linear-MoE blocks, normalization techniques, and embedding methods. To enhance adaptability, LSM mechanisms are organized into three main categories: linear attention, SSM, and linear RNN, with multiple instances available in each. For linear attention, options include basic linear attention (BLA), Lightning Attention, Retention, GLA, DeltaNet, Based, and Rebased; for SSM, we provide Mamba2, the leading SSM model at present; and for linear RNN, options include HGRN2 and RWKV6. As LSM techniques evolve, Linear-MoE will continue to incorporate more LSM methods to ensure users have access to the latest advancements. Additionally, Linear-MoE offers vital components such as model library, tokenizers, model converters, usage examples, and set of supportive toolkits. The model library includes instances of Linear-MoE models that are adapted from stateof-the-art open-source MoE architectures, including Qwen2 MoE, DeepSeekV2 MoE, and Mixtral MoE. These adapted instances are designated as Linear-MoE-Qwen2, Linear-MoE-DeepSeekV2, and Linear-MoE-Mixtral, respectively. These models are implemented following Megatron-Core format, with the standard attention layers replaced by LSM-based token mixing layers, while maintaining the original embedding, normalization, and expert layers unchanged. 2.3.2 Training Subsystem Advanced parallelism techniques, encompassing tensor, sequence, pipeline, context, and MoE expert parallelism, are seamlessly incorporated into the Linear-MoE system through its design on top of the Megatron-Core library. This non-intrusive integration allows Linear-MoE to leverage the robust training capabilities of Megatron-Core, supporting large-scale model training across both standard attention layers and MoE expert layers. However, the inherent parallelism mechanisms, such as TP and SP, were not originally optimized for LSM modules. Additionally, Megatron-Core does not fully support efficient SP for hybrid models containing both LSM modules and standard attention layers. To address these gaps, we elaborate on our TP and SP approaches specifically designed for LSM modules and hybrid models, as discussed in 2.2. Further capabilities, including mixed precision, activation recomputation, distributed optimizer, distributed checkpointing, and CPU offloading, are also inherited from Megatron-Core, enhancing model training flexibility and efficiency. And Linear-MoE supports 8-bit floating point (FP8) precision on Hopper GPUs, benefiting from the integration of NVIDIAs Transformer Engine (Micikevicius et al., 2022). This feature optimizes memory usage and accelerates performance during both training and inference stages. To enhance the training speed of MoE layers, we incorporate MegaBlocks (Gale et al., 2023) into our Linear-MoE system. MegaBlocks is designed to optimize MoE training on GPUs by reconfiguring MoE computations using block-sparse operations and developing new block-sparse GPU kernels that effectively manage the inherent dynamism of MoE. In addition, we also integrate the Grouped GEMM library into Linear-MoE, which introduces grouped GEMM kernels in PyTorch, thereby accelerating the computational processes involved in training MoE models. 2.3.3 Evaluation Module In order to facilitate the evaluation on mainstream benchmarks, we have developed offline text generation of Linear-MoE models within the system. Based on this, mature evaluation frameworks such as OpenCompass (Contributors, 2023) and LMEvaluation-Harness (Gao et al., 2023), are readily available for conducting evaluation tasks on LinearMoE models. Furthermore, the system facilitates seamless bidirectional conversion between model weights from HuggingFace and Megatron-Core. This functionality enables users to easily leverage pretrained models from HuggingFace for continued pretraining or fine-tuning within the MegatronCore environment. Additionally, it allows for the assessment of model performance by using HuggingFaces evaluation and inference pipelines on models trained within the Megatron-Core framework."
        },
        {
            "title": "3 Empirical Study",
            "content": "3.1 Experiment Setup Models and Dataset. We conduct experiments on two Linear-MoE model series: A0.3B-2B and A1B-7B. A0.3B-2B denotes Linear-MoE model containing total of 2 billion parameters, with 0.3 billion parameters activated. The same applies for the A1B-7B model. Each series consists of several model instances, each incorporating distinct instance of the LSM module. The specific 7 Models A0.3B-2B A1B-7B Hidden Dimension FFN Dimension Num of Heads Num of Layers Num of Act Experts Num of Experts LR Minimum LR LR Scheduler Seq Length Training Tokens 1024 896 8 12 8 64 1e-4 1e-5 Cosine 2048 15B 2048 1024 16 16 8 64 1e-5 1e-6 Cosine 2048 100B Table 2: Linear-MoE Family Models and Training Configurations. A0.3B-2B indicates that the LinearMoE model has total of 2 billion parameters, with 0.3 billion parameters activated. The same for A1B-7B. Figure 4: Training Throughput (Tokens/s). As sequence length increases, the throughput of Baseline declines significantly, whereas LSM models maintain stable training efficiency. LSM module instances used in our experiments include: basic linear attention (BLA) (Katharopoulos et al., 2020), Retentive Network (Retention) (Sun et al., 2023), Gated Linear Attention (GLA) (Yang et al., 2023), DeltaNet (Schlag et al., 2021), Mamba2 (Dao and Gu, 2024), HGRN2 (Qin et al., 2024d), and RWKV6 (Peng et al., 2023, 2024), all implemented in Triton. These model instances are evaluated against models with standard attention implementation in Megatron-Core (referred to as Baseline) and the FlashAttention-2 (Dao, 2023) implemented in Transformer Engine (in CUDA). To implement the Linear-MoE model instances, we utilize the Qwen2 MoE architecture (Yang et al., 2024a) as the base model. All models are pretrained from scratch on portion of the SlimPajama dataset (Soboleva et al., 2023). This dataset originally contains 627 billion tokens, we restrict our experiments to the first two chunks of the dataset, totaling approximately 100 billion tokens. The 8 Qwen2 tokenizer is employed throughout the training processes. Training Configurations. Table 2 details the training configurations for both Linear-MoE model series. We employ the Adam optimizer (Kingma and Ba, 2014) along with parallelism techniques, including TP and EP. Each pretraining run is performed on node with eight A100 80G GPUs. 3.2 Training and Inference Efficiency We perform experiments to evaluate the training efficiency of the Linear-MoE system, focusing on throughput and GPU memory requirements using eight A100 GPUs. For training the sparse MoE models, we set the EP size to 8. During the experiments, we maintain total of 16K input tokens per iteration, while varying the input sequence lengths across {2K, 4K, 8K, 16K} with corresponding batch sizes of {8, 4, 2, 1}. As illustrated in Table 3 and Fig. 4, we observe that the standard attention Baseline shows significant quadratic increase in memory usage and decline in throughput as the input sequence lengths grow. FlashAttention-2 also demonstrates notable variations in both memory footprint and throughput, when the sequence length reaches 16K. In contrast, the Linear-MoE models, which incorporate LSM, exhibit relatively stable GPU memory consumption and consistent throughput when the sequence length increases, but number of input tokens remains fixed. We also perform experiments to compare the inference efficiency of Linear-MoE (using Basic LA) with the Baseline (using FlashAttention-2). The results, shown in Table 5, reveal that LinearMoE offers significant speed advantage when the decoding length exceeds 16K. Additionally, its memory usage remains constant, which is key benefit resulting from the adoption of LSM. Furthermore, to highlight the efficiency benefits of the Linear-MoE training subsystem, we conduct ablation studies on MoE optimization techniques and parallelism training methods. The results of these experiments are presented in Table 4. It is evident that the implementation of MoE optimization techniques, specifically Grouped GEMM and MegaBlocks, significantly reduces the elapsed time for each iteration. Additionally, the various parallelism training techniques each demonstrate their own advantages in terms of memory footprint and overall training efficiency. Seq Length Batch Size Baseline FlashAttn-2 Basic LA Retention GLA DeltaNet Mamba2 HGRN2 RWKV6 2K 8 4K 4 8K 16K 1 Mem. Thpt. Mem. Thpt. Mem. Thpt. Mem. Thpt. 40.74 38.96 42.69 42.71 43.87 43.33 45.63 46.03 47.11 102.14 103.91 115.16 117.85 113.29 116.95 105.99 92.58 137.62 41.42 39.10 43.85 42.66 43.73 43.34 45.94 46.14 47.12 88.60 101.78 119.72 119.11 118.77 120.27 108.13 95.74 136.73 42.93 39.57 42.71 42.73 43.63 43.31 47.16 45.56 47.11 66.17 105.08 112.66 119.16 116.34 117.43 102.51 97.98 135. 47.08 41.51 43.00 42.65 43.60 43.32 44.97 44.97 47.12 49.39 96.16 114.67 118.19 110.87 109.72 106.84 96.02 134.51 Table 3: Quantitative Training Efficiency Results. We experiment on 8 A100 GPUs and report the max allocated GPU memory (GB) and throughput (103 tokens/s) of A0.3B-2B model instances with varying input sequence lengthes and batch sizes. Figure 5: Inference Efficiency of A0.3B-2B Model Instances. We variate the decoding length from 1K to 128K with fixed batch size of 16 on single A800 80GB GPU to evaluate the Baseline w/ FlashAttention-2 and the Linear-MoE w/ Basic Linear Attention in terms of inference latency time and GPU memory usage. 3.3 Training Loss and Evaluation To evaluate the overall training performance of the Linear-MoE models, we pretrain the A0.3B-2B and A1B-7B model instances using 15B and 100B tokens, respectively. We test both pure and hybrid model configurations; for the hybrid models, we incorporate one quarter of standard transformer MoE layers throughout the architecture. For instance, in the 12-layer A0.3B-2B model, the hybrid configuration follows the pattern \"LLLNLLLNLLLN\", while the 16-layer A1B-7B model adopts the pattern \"LLLNLLLNLLLNLLLN\". The training loss curves for the A0.3B-2B model instances, which include both pure and hybrid Linear-MoE models, are presented in Fig. 6. The results demonstrate that the pure Linear-MoE architecture achieves competitive convergence performance compared to the standard attention Baseline. Moreover, the hybrid models exhibit more stable convergence and consistent performance when compared with the Baseline. Additional experiment results such as benchmark evaluations and training loss curves of A1B-7B models can be found in Appendix A.4. Both the A0.3B-2B and A1B-7B Linear-MoE model series show competitive performance on various benchmarks, and it is verified that hybrid models usually perform better than the pure linear models."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we introduced Linear-MoE, novel product-level system designed to integrate LSM with MoE, aiming to advance both the efficiency and scalability of existing large models. By combining linear-complexity sequence modeling capabilities of LSM with sparsely activated MoE layers, Linear-MoE achieves high performance while addressing computational and memory constraints common in large model training and deployment. The dual subsystems: Modeling and Training, provide flexible and extensible framework that supports diverse LSM methods and advanced parallelism techniques, including specific sequence parallelism for handling long input sequences efficiently. We also explored hybrid models that further enhance adaptability by incorporating stan9 Figure 6: Training Loss Curves of A0.3B-2B Model Instances. Left: pure Linear-MoE models; Right: hybrid Linear-MoE models. Linear-MoE shows competitive training convergence performance compared to the standard attention Baseline. MoE Optimization Memory (GB) Time/Iter (ms) Baseline Grouped GEMM MegaBlocks 35.28 35.01 36. 1565.6 455.4 348.8 EP TP PP Memory (GB) Time/Iter (ms) 1 8 1 1 2 1 1 8 1 2 1 1 1 8 35.28 22.98 10.04 8.89 12.90 1565.6 739.4 6879.0 1820.2 1684.9 Table 4: Above: MoE Optimization. Below: Distributed training efficiency under different parallelism settings. We report the memory usage per GPU (GB) and elapsed time per iteration (ms) while training the A0.3B-2B model with sequence length of 2048 and batch size of 4, using node equipped with 8 A100 GPUs. The Baseline refers to the MoE implementation in Megatron-Core, which is used without any optimizations. dard Transformer layers. Our experimental results demonstrate that Linear-MoE achieves significant efficiency gains while maintaining strong performance across various benchmarks. These findings highlight the potential of Linear-MoE as the next generation of foundation model architecture."
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results demonstrated in this paper, there are several limitations to the LinearMoE framework. First, while the system successfully combines LSM with MoE to enhance efficiency, the integration of different LSM methods and MoE layers may introduce complexity in hyperparameter tuning, which could impact model performance under certain configurations. Additionally, the scalability of Linear-MoE in extremely large-scale settings, such as beyond the model sizes tested in our experiments (A0.3B-2B and A1B-7B), remains an area for further investigation. Moreover, while the system supports various parallelism techniques, their effectiveness on diverse hardware architectures, particularly in resource-constrained environments, needs more comprehensive evaluation. Therefore, future work should focus on further optimizing the system for broader set of use cases and exploring additional hybrid modeling strategies."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In ThirtyFourth AAAI Conference on Artificial Intelligence. Eleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in haystack: On the role of incidental bilingualism in palms translation capability. arXiv preprint arXiv:2305.10266. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. survey on mixture of experts. arXiv preprint arXiv:2407.06204. Soumith Chintala. 2023. Gpt-4 moe. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. 10 OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Albert Gu, Karan Goel, and Christopher Ré. 2022b. Efficiently modeling long sequences with structured In The International Conference on state spaces. Learning Representations (ICLR). Tri Dao. 2023. Flashattention-2: Faster attention with arXiv better parallelism and work partitioning. preprint arXiv:2307.08691. Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060. Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:2298222994. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto. 2024. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830. Jiaxi Hu, Disen Lan, Ziyu Zhou, Qingsong Wen, and Yuxuan Liang. 2024. Time-ssm: Simplifying and unifying state space models for time series forecasting. Preprint, arXiv:2405.16312. Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. 2025. Mom: Linear sequence Preprint, modeling with mixture-of-memories. arXiv:2502.13685. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. 2023. MegaBlocks: Efficient Sparse Training with Mixture-of-Experts. Proceedings of Machine Learning and Systems, 5. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:1474 1487. Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. 2022a. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:3597135983. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive mixtures of local experts. Neural Computation, page 7987. Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 51565165. PMLR. Diederik Kingma and Jimmy Ba. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Reducing activation recomputation in large transformer models. Preprint, arXiv:2205.05198. Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, and Yu Cheng. 2025. Liger: Linearizing large language models to gated recurrent structures. Preprint, arXiv:2503.01496. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pages 62656274. PMLR. 11 Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023. Cmmlu: Measuring massive multitask language understanding in chinese. Preprint, arXiv:2306.09212. Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. 2020. Pytorch distributed: Experiences on accelerating data parallel training. Preprint, arXiv:2006.15704. Jhonathan Osin, Opher Lieber, Barak Lenz, Hofit Bata, Gal CoItay Dalmedigos, Erez hen, Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. 2024. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434. Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023. Ring attention with blockwise transformers for nearinfinite context. Preprint, arXiv:2310.01889. Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, and Yu Cheng. 2025. Twin-merging: Dynamic integration of modular expertise in model merging. Advances in Neural Information Processing Systems, 37:7890578935. Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. 2022. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433. MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, and Zijia Wu. 2025. Minimax-01: Scaling foundation models with lightning attention. Preprint, arXiv:2501.08313. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. 2024. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1404814077, Singapore. Association for Computational Linguistics. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. 2024. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892. Hadi Pouransari, Chun-Liang Li, Jen-Hao Rick Chang, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, and Oncel Tuzel. 2024. Dataset decomposition: Faster llm training with variable sequence length curriculum. arXiv preprint arXiv:2405.13226. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong. 2024a. Transnormerllm: faster and better large language model with improved transnormer. Preprint, arXiv:2307.14995. Zhen Qin, Xuyang Shen, Weigao Sun, Dong Li, Stan Birchfield, Richard Hartley, and Yiran Zhong. 2024b. Unlocking the secrets of linear complexity sequence model from unified perspective. arXiv preprint arXiv:2405.17383. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, WeixLightning uan Sun, and Yiran Zhong. 2024c. attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024d. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904. Zhen Qin, Songlin Yang, and Yiran Zhong. 2024e. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36. 12 Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, and Yu Cheng. 2024. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint arXiv:2411.15708. Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. 2024. Jamba-1.5: Hybrid transformer-mamba models at scale. arXiv preprint arXiv:2408.12570. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Samba: SimLiang, and Weizhu Chen. 2024. ple hybrid state space models for efficient unlimarXiv preprint ited context language modeling. arXiv:2406.07522. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. 2024. An empirical study of arXiv preprint mamba-based language models. arXiv:2406.07887. Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. 2021. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:1755517566. Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. 2024. Auxiliary-loss-free load balancing strategy for mixture-of-experts. Preprint, arXiv:2408.15664. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin. 2024. Jetmoe: Reaching llama2 performance with 0.1 dollars. arXiv preprint arXiv:2404.07413. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. 2024b. Gated delta networks: Improving mamba2 with delta rule. Preprint, arXiv:2412.06464. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2023. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635. Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, and Yu Cheng. 2025. Lasp-2: Rethinking sequence parallelism for linear attention and its hybrid. arXiv preprint arXiv:2502.07563. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2024c. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484. Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Linear atarXiv preprint Yu Qiao, and Yiran Zhong. 2024. tention sequence parallelism. arXiv:2404.02882. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Jinle Zeng, Min Li, Zhihua Wu, Jiaqi Liu, Yuang Liu, Dianhai Yu, and Yanjun Ma. 2022. Boosting distributed training performance of the unpadded bert model. arXiv preprint arXiv:2208.08124. 13 Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong Chen, Xin Liu, and Yibo Zhu. 2023. Bytetransformer: high-performance transformer boosted for variable-length inputs. In 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pages 344355. IEEE. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Wei Bi Freda Shi, Bailin Wang, Peng Zhou, and Guohong Fu. 2024. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. 2022. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114. Tong Zhu, Daize Dong, Xiaoye Qu, Jiacheng Ruan, Wenliang Chen, and Yu Cheng. 2024a. Dynamic data mixing maximizes instruction tuning for mixture-ofexperts. arXiv preprint arXiv:2406.11256. Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. 2024b. Llama-moe: Building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Related Work A.1.1 Mixture-of-Experts MoE (Jacobs et al., 1991; Cai et al., 2024; Lu et al., 2025) is gaining increasing attention in the development of large language models (LLMs) due to its ability to scale model size while maintaining computational efficiency. Its key strength lies in the sparse activation of experts and routing mechanisms, enabling better balance between model performance and training cost. The effectiveness of MoE in modern deep learning was first demonstrated in Shazeer et al. (2017), where an MoE layer was introduced between LSTM layers, resulting in state-of-the-art performance on language modeling and machine translation benchmarks. Following this, the MoE layer was incorporated into the Transformer architecture, replacing the feed-forward network (FFN) layers. GShard (Lepikhin et al., 2020) applied MoE to Transformers, significantly improving machine translation across 100 languages. Switch Transformers (Fedus et al., 2022) further scaled model size to trillions of parameters, using simplified and efficient MoE layer design. However, training MoE models often leads to load imbalance, where only few experts are heavily utilized, leaving others underutilized (Lewis et al., 2021; Wang et al., 2024; Zhu et al., 2024a; Du et al., 2025). To address this, several strategies have been developed to optimize MoE training. These include the BASE layer (Lewis et al., 2021), the HASH layer (Roller et al., 2021), and Expert Choice (Zhou et al., 2022), all of which aim to maximize model capacity utilization. MoE architectures have been widely adopted in industry-leading models, such as Gemini-1.5 (Reid et al., 2024) and reportedly GPT-4 (Chintala, 2023). Other notable examples of LLMs incorporating MoE techniques include Mixtral (Jiang et al., 2024), DeepSeek V2 (Liu et al., 2024), Qwen2 (Yang et al., 2024a), JetMoE (Shen et al., 2024), Jamba (Team et al., 2024), and OLMoE (Muennighoff et al., 2024). Despite the advances in MoE, most research has focused on improving FFN layers and routers, while attention mechanisms have remained largely unchanged. There is still much room for exploring how to enhance the efficiency of MoE models by evolving their attention layers. 14 A.1.2 Linear Sequence Modeling Linear Attention. Linear attention encompasses set of techniques aimed at calculating attention outputs using the \"right-product kernel trick,\" which first computes key-value products, thereby avoiding the quadratic complexity associated with query-key computations. Vanilla linear attention (Katharopoulos et al., 2020) replaces the Softmax attention (Vaswani et al., 2017) with kernel methods, reducing the computational complexity to linear in relation to sequence length. Building on this, various extensions of linear attention have emerged. For example, TransNormerLLM (Qin et al., 2024a) introduces Lightning Attention, an optimized linear attention mechanism that speeds up processing by enhancing IO operations. Lightning Attention-2 (Qin et al., 2024c) further improves this by separately handling interand intra-block computations to fully exploit the advantages of linear attention on autoregressive tasks. RetNet (Sun et al., 2023) combines retention mechanism with attention, offering both parallel training and linear-time inference. Gated Linear Attention (GLA) (Yang et al., 2023) introduces data-independent gating mechanism and presents hardware-efficient algorithm for training. DeltaNet (Schlag et al., 2021), along with its parallelized version (Yang et al., 2024c), applies delta-rule-like update to improve performance in long-context scenarios. More recently, Gated Slot Attention (GSA) (Zhang et al., 2024), inspired by GLA, introduces boundedmemory slot control mechanism within the gated linear attention framework, further boosting performance in tasks requiring strong recall abilities. State Space Model. SSM provides robust framework for capturing the behavior of sequence modeling within dynamic systems, and has demonstrated itself in the field of linear sequence modeling. Models such as S4 (Gu et al., 2022b) and its subsequent variants (Gu et al., 2022a; Gupta et al., 2022) have achieved notable success, particularly in long-range synthetic tasks. recent example is Mamba (Gu and Dao, 2023), representative SSM model that introduces state selection mechanism. Mamba addresses the limitation of static dynamics in previous methods, arguing that they do not account for input-specific context selection within the hidden state, which is critical for tasks like language modeling. Mamba has shown superior performance compared to Transformers across various model sizes and scales. Mamba has been further refined in its successor, Mamba2 (Dao and Gu, 2024), which integrates linear attentionlike mechanism that improves hardware efficiency during training. Similar to how linear attention uses outer products to expand the state, Mamba2 leverages state-space duality that enables parallel attention-style computation while maintaining recurrent inference capabilities. Linear RNN. Traditional RNNs struggle with long-context sequence modeling, largely due to their sequential nature during training, which limits their ability to benefit from scaling laws (Sun et al., 2023). To mitigate these issues, Linear RNNs introduce parallel training capabilities, achieving competitive performance with Transformers of comparable size. RWKV (Peng et al., 2023, 2024) is an example of large language model based on linear RNNs, designed to effectively manage long-term dependencies. Furthermore, HGRN (Qin et al., 2024e) emphasizes the importance of datadependent decay mechanisms in enhancing linear RNN performance, showing how tuning decay parameters can improve learning in long-context scenarios. The upgraded HGRN2 (Qin et al., 2024d) builds on this by introducing state expansion mechanism that leverages outer product operations, allowing for better scalability and improved sequence modeling over extended inputs. Both RWKV and HGRN models aim to address the limitations of traditional RNNs for efficient longsequence modeling. A.2 Tensor Parallelism on Linear-MoE The core computation mechanism of LSM modules can be abstracted in the following general form: = ϕ(Q)(ϕ(K)V), = XWQ, = XWK, = XWV , (6) where TP is applied by splitting the matrix multiplications as follows: = [ϕ(XW1 = [ϕ(XW1 = X[W1 = [O1, O2], Q), ϕ(XW2 K), ϕ(XW2 ], , W2 Q)], K)], (7) where the weight matrices Wq, Wk, and Wv are divided along their columns, producing an output matrix that is also split along columns. The split output [O1, O2] is then multiplied by an output linear weight matrix that is split along its 15 rows, resulting in: A.4 Additional Experiments = [O1, O2][W1 O, W2 O] + O2W2 O, = O1W1 (8) which produces unified output. As with TP in standard attention, TP for LSM modules introduces an all-reduce collective communication operation during both the forward and backward passes. In practical terms, this all-reduce operation is implemented via two separate steps: all-gather and reduce-scatter, which together functionally achieve the same result as single allreduce. A.3 Sequence Parallelism on Linear-MoE Algorithm 1 SP on Linear-MoE w/o Masking 1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute = [Xt]T 1 . 3: for chunk {1, , } on ranks {1, , } in parallel do 4: Calculate Qt = XtWQ, Kt = XtWK , Vt = XtWV . Compute Mt = Communicate [Mt] Compute M1:T = Sum([Mt]T Compute Ot = QtM1:T . 5: 6: 7: 8: 9: end for 10: return = [Ot]T 1 . Vt. 1 = AllGather([Mt] 1 ). 1 ). Algorithm 2 SP on Linear-MoE w/ Masking 1: Input: input sequence X, distributed world size , sequence parallel size = . 2: Distribute = [Xt]T 1 . 3: Initialize mask matrix Ψ, where Ψij = 1 if and Ψij = if < j. 4: for chunk {1, , } on ranks {1, , } in parallel do Calculate Qt = XtWQ, Kt = XtWK , Vt = 5: 6: 7: 8: 9: XtWV . Compute Mt = (Kt)Vt. Communicate [Mt] Compute Ot,intra = [(QtK Compute prefix sum 1 = AllGather([Mt] 1 ). ) Ψ]Vt. M1:t1 = PrefixSum([Mt]t1 1 ). Compute Ot,inter = QtM1:t1. Compute Ot = Ot,intra + Ot,inter. 10: 11: 12: end for 13: return = [Ot]T 1 . 16 Figure 7: Training Loss Curves of A1B-7B Model Instances. A.5 Datasets and Benchmarks We pretrain all the models on portion of the SlimPajama dataset which is sampled to approximately 100 billion tokens. SlimPajama (Soboleva et al., 2023) is high-quality, optimized subset of the RedPajama dataset, designed for large-scale language model training. It includes diverse text sources such as Common Crawl, Wikipedia, books, and GitHub code, with primary focus on English. The dataset is cleaned, deduplicated, and optimized for efficiency and performance. For the benchmark, we tested on these tasks: PiQA (Bisk et al., 2020): dataset focused on physical commonsense reasoning in English with 3084 test samples. The text consists of everyday tasks and scenarios, requiring models to determine the most practical way to perform an action. The data is sourced from crowdsourced descriptions, reflecting broad range of common human experiences. ARC-Easy & ARC-Challenge (Clark et al., 2018): set of multiple-choice science questions in English, sourced from standardized exams and educational materials with 2376 and 1172 test samples. The dataset represents the domain of elementary and high school science, with questions authored by Scale Model LSM Instance Baseline Attention A0.3B-2B 15B Tokens Pure Hybrid BLA Retention GLA Mamba2 HGRN2 BLA Retention GLA Mamba2 HGRN PIQA Hella. Wino. ARC-e ARC-c MMLU Avg. Avg. acc acc_norm acc 55.77 64.42 62.08 65.56 66.97 52. 66.76 66.21 67.71 66.38 66.27 27.10 33.41 29.14 35.29 37.79 26.37 37.16 36.06 38.62 38.81 36.79 50.83 49.01 50.75 50.67 50.20 49. 49.96 51.54 49.72 51.30 51.46 acc 33.04 48.15 42.72 47.81 49.12 24.83 49.62 47.18 50.51 50.17 48.82 acc_norm acc(5-shot) (no MMLU) 23.21 24.32 21.50 23.04 24.74 27.65 24.74 24.91 26.02 24.91 25. 23.24 26.32 23.12 24.85 25.85 25.10 25.64 23.71 25.05 24.61 23.19 35.53 40.94 39.60 41.20 42.45 34.24 42.31 41.60 42.94 42.70 41. 37.99 43.86 43.39 44.47 45.76 36.07 45.65 45.18 46.52 46.31 45.75 Table 5: A0.3B-2B Evaluation Results on Language Modeling Benchmarks (No Data Corruption). All models are pretrained from scratch on the same 15B subset of the SlimPajama dataset with the Qwen2 tokenizer. No benchmark data corruption in the pretraining dataset. The A0.3B-2B hybrid models have stack as \"LLLNLLLNLLLN\", where \"L\" represents the Linear-MoE layer, and \"N\" represents the normal MoE transformer layer. PIQA Hella. Wino. ARC-e ARC-c MMLU Avg. Avg. Scale Model LSM Instance acc acc_norm acc A1B-7B 100B Tokens Pure BLA GLA Mamba2 66.65 68.17 69.21 37.74 43.51 41.86 50.12 51.22 51.46 acc 50.80 52.48 52.86 acc_norm acc(5-shot) (no MMLU) 24.23 25.09 25. 23.71 24.83 23.66 42.21 44.22 44.04 45.91 48.09 48.11 Table 6: A1B-7B Evaluation Results on Language Modeling Benchmarks (No Data Corruption). All models are pretrained from scratch on the same 15B subset of the SlimPajama dataset with the Qwen2 tokenizer. No benchmark data corruption in the pretraining dataset. signed to evaluate AI models general knowledge across wide range of subjects and languages. It comprises 57 distinct categories, spanning elementary-level knowledge to advanced professional topics such as law, physics, history, and computer science. The dataset has been translated into 14 languages using professional human translators, ensuring high-quality and accurate translations. This multilingual approach aims to improve the inclusivity and effectiveness of AI models across different linguistic communities. All datasets used in this work are publicly available and have been released by their original creators, who are responsible for ensuring privacy protection. These datasets are used in accordance with their respective licenses and intended purposes. No modifications or derivative datasets have been created. educators and test designers. ARC-Easy includes straightforward questions, while ARCChallenge contains more difficult ones that require advanced reasoning. HellaSwag (Zellers et al., 2019): An Englishlanguage dataset designed for commonsense reasoning, where models must choose the most plausible continuation of sentence. The text is derived from activity descriptions (e.g., WikiHow), covering everyday scenarios. The dataset was constructed adversarially to be challenging for language models. It has 10003 test samples. WinoGrande (Sakaguchi et al., 2019): large-scale English dataset for commonsense reasoning, based on the Winograd Schema Challenge with 1267 test samples. It tests pronoun resolution in ambiguous contexts, with sentences sourced and refined through crowdsourcing. The dataset aims to reduce annotation biases by diversifying sentence structures and topics. MMLU (Li et al., 2023): The MMLU (Massive Multitask Language Understanding) dataset is comprehensive benchmark de-"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Soochow University",
        "South China University of Technology",
        "The Chinese University of Hong Kong"
    ]
}