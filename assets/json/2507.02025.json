{
    "paper_title": "IntFold: A Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction",
    "authors": [
        "The IntFold Team",
        "Leon Qiao",
        "Wayne Bai",
        "He Yan",
        "Gary Liu",
        "Nova Xi",
        "Xiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure prediction, IntFold can be adapted to predict allosteric states, constrained structures, and binding affinity through the use of individual adapters. Furthermore, we introduce a novel confidence head to estimate docking quality, offering a more nuanced assessment for challenging targets such as antibody-antigen complexes. Finally, we share insights gained during the training process of this computationally intensive model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . - [ 1 5 2 0 2 0 . 7 0 5 2 : r 2025-07-02 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction The IntFold Team1 1IntelliGen AI We introduce IntFold, controllable foundation model for general and specialized biomolecular structure prediction. Utilizing high-performance custom attention kernel, IntFold achieves accuracy comparable to the state-of-the-art AlphaFold 3 on comprehensive benchmark of diverse biomolecular structures, while also significantly outperforming other leading all-atom prediction approaches. The models key innovation is its controllability, enabling downstream applications critical for drug screening and design. Through specialized adapters, it can be precisely guided to predict complex allosteric states, apply userdefined structural constraints, and estimate binding affinity. Furthermore, we present training-free, similarity-based method for ranking predictions that improves success rates in model-agnostic manner. This report details these advancements and shares insights from the training and development of this large-scale model."
        },
        {
            "title": "Experimental highlights",
            "content": "State-of-the-Art Accuracy: On the comprehensive FoldBench benchmark, IntFolds performance is comparable to AlphaFold 3 and significantly exceeds other contemporary models (Boltz-1, 2, Chai-1, HelixFold 3 and Protenix) across diverse range of biomolecular interaction tasks. Versatile Model Adaptation with Simple Tuning: We control the models predictions towards tasks relevant to drug screening and design by inserting small, trainable adapters while the large base model remains frozen. This efficient method allows IntFold to solve challenges where general models fail: Allosteric States: Fine-tuning successfully enables the prediction of specific, functionally critical allosteric conformations, known difficulty for general-purpose models. Pocket and Epitope Guidance: The model can incorporate structural constraints for known binding pockets or epitopes, leading to more accurate predictions of protein-ligand and antibody-antigen interactions. High-Fidelity Binding Affinity Prediction: After fine-tuning the affinity data, the model accurately predicts binding affinity, demonstrating performance that surpasses Boltz-2 and significantly enhances virtual screening capabilities. Superior Attention Kernel Performance: Development and implementation of custom FlashAttentionPairBias kernel that outperforms standard industry kernels. It runs faster or with lower memory usage than those developed by DeepSpeed and NVIDIA. Novel Model-Agnostic Ranking Method: We developed model-agnostic ranking method based on structural similarity. By selecting the most self-consistent structure from diverse set of predictions, this training-free approach consistently improves the success rate over random selection. Deep Insights into Large-Scale Model Behavior: Building this model uncovered critical challenges in large-scale training. It includes data processing complications, model parametrization choices, and the origins of internal model instabilities, such as large activation magnitudes. Please send correspondence regarding this report to contact@intfold.com"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Benchmarking 2.1 Proteins and Antibody-Antigen Complexes 2.2 Protein-Ligand Interactions 2.3 Nucleic Acid Systems 3 Applications 3.1 Target-type Specific Modeling 3.2 Guided Folding with Structural Constraints 3.3 Predicting Protein-Ligand Binding Affinity 4 Data 4.1 Data Sources 4.2 MSAs and Templates 5 Modeling"
        },
        {
            "title": "5.4.1 Activation Explosion and Gradient Spikes\n5.4.2 Parametrization and Initialization\n5.4.3 Numerical Consideration",
            "content": "6 Conclusion 7 Limitations and Future Directions 8 Acknowledgments References 1. Introduction 3 4 5 6 6 7 8 9 10 10 11 11 11 12 12 12 12 13 13 14 14 14 The prediction of three-dimensional biomolecular structures is fundamental problem in biology and medicine. Accurate structure modeling is essential for understanding biological mechanisms, disease pathways, and accelerating therapeutic designs. This field was revolutionized by DeepMinds AlphaFold 2 [1], deep learning model with unprecedented accuracy in protein monomer structure prediction. Building on this progress, AlphaFold 3 [2] was introduced in 2024 with unified framework to predict interactions between proteins, nucleic acids, small molecules, ions, and modified residues, establishing new standard for modeling general biomolecular assemblies. While this new architecture inspired wave of research, independent reproductions have so far shown significant performance gap compared to the original AlphaFold 3, highlighting the difficulty of successfully training such model. While these advances in general-purpose prediction are transformative, significant challenges remain to efficiently adapt such large models for specialized applications. In fields like therapeutic design, researchers often need to test specific biological hypotheses or incorporate prior knowledge, such as the location of known binding pocket or antibody epitopes. Furthermore, accurately capturing distinct functional states, such as the subtle yet critical conformations of allosteric proteins, remains IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 1 Diagram of the models architecture. The model processes input sequences and optional MSAs/templates through an Embedding Trunk. Controllable modeling is achieved by inserting modular Adaptor into this trunk. Diffusion Block then iteratively generates structure samples, which are ranked by Confidence Head (pLDDT, pTM) to yield the final prediction. The bottom panel shows how adapters enable specialized downstream tasks like Guided Folding, Target-Specific Modeling, and Affinity Prediction. difficult task for general models that are not designed for such fine-grained control [3]. This creates critical need for new class of models that combine state-of-the-art accuracy with high degree of user-driven controllability. To address these challenges, we introduce IntFold1, controllable foundation model designed for both high-accuracy general prediction and specialized, user-guided tasks. IntFold achieves two primary goals: first, it provides predictive accuracy for wide range of biological interactions comparable to the state-of-the-art; second, it offers exceptional user control for specialized applications critical to drug screening and design. This is accomplished by inserting lightweight, trainable adapter modules while keeping the large base model frozen. In this report, we detail IntFolds key contributions. We first demonstrate its performance against leading methods on comprehensive benchmark. We then showcase its versatility in several applications critical for drug design, such as modeling allosteric states and predicting binding affinity. We also introduce two technical innovations: custom attention kernel that is more efficient than standard industry implementations and novel confidence score designed to provide more fine-grained and reliable quality assessment for challenging targets like antibody-antigen complexes. Finally, we share practical insights gained during development with respect to data curation, parametrization, and the root causes of training instability. 2. Benchmarking To comprehensively evaluate the performance of IntFold, we conducted rigorous evaluation on FoldBench [4]. We compared IntFold against several leading methods, including Boltz-1,2 [5, 6], Chai-1 [7], HelixFold 3 [8], Protenix [9] and AlphaFold 3 [2].2 The following sections detail the performance across different biomolecular categories. 1The IntFold server and source code are available at https://server.intfold.com/ and https://github.com/ IntelliGen-AI/IntFold, respectively. 2Due to license limitations, performance metrics for AlphaFold 3 are cited directly from the original benchmark. For other models, results are from FoldBench if available; otherwise, we reran them ourselves. 3 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 2 (a) Mean LDDT scores for protein monomer prediction on FoldBench. (b) Success rates for protein-protein interface prediction. (c) Success rates for antibody-antigen interface prediction. (d) Cumulative distribution of DockQ scores for antibody-antigen complexes, showing the fraction of targets predicted above given quality threshold. (e) Predictions of newly released protein targets: (left) previously unsolved yeast enzyme Agn1 Glucanase (8YFH); (center) an H5N1 hemagglutinin complex (8X2F); (right) novel HER3-targeting antibody-drug conjugate (8YRY). 2.1. Proteins and Antibody-Antigen Complexes We first evaluated the model on protein-related systems. For protein monomers, IntFold achieves mean LDDT score of 0.88, matching the performance of AlphaFold 3 and remaining highly competitive with the other tested methods, demonstrating strong single-chain folding ability (Figure 2a). For protein-protein interactions, IntFold reaches success rate of 72.9%, again matching AlphaFold 3 (72.9%) and significantly outperforming the next best contemporary method, Chai-1 (68.5%) (Figure 2b). key focus was on antibody-antigen (Ab-Ag) complexes, modality critical for immunology and therapeutics where previous models have often struggled. significant performance gap exists between AlphaFold 3 and other methods for this task. As shown in Figure 2c, the general IntFold model shows success rate of 37.6%, closing the gap to AlphaFold 3 (47.9%). Furthermore, an enhanced model, IntFold+, improves the success rate to 43.2%, reaching performance comparable to AlphaFold 3. Moreover, the cumulative distribution of DockQ scores shows that IntFolds advantage is 4 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction consistent across all quality thresholds, producing more high-quality predictions than other methods at every threshold (Figure 2d). To demonstrate its predictive power on prospective challenges, we validated IntFold on several complex targets newly released in 2025. As illustrated in Figure 2e, the model successfully predicted the structures of previously unsolved yeast enzyme (8YFH), an H5N1 hemagglutinin complex (8X2F), and novel HER3-targeting antibody (8YRY). 2.2. Protein-Ligand Interactions Figure 3 (a) Success rates for protein-ligand interface prediction on FoldBench. (b) Cumulative distribution of ligand RMSD scores, showing the fraction of targets predicted below given RMSD cutoff. (c) Success rate as function of ligand similarity to the training set, testing generalization to out-of-distribution molecules. (d) Success rate on the PoseBusters v2 benchmark, filtered for targets not seen during training. (e) Example of protein-ligand prediction for recently released RIP3 Kinase For protein-ligand interactions, IntFold establishes itself as highly competitive model on the corresponding FoldBench subset. As shown in Figure 3a, it achieves success rate of 58.5%, placing it as the leading model after AlphaFold 3 (64.9%) and substantially outperforming other methods like Boltz-1 (55.0%). Moreover, IntFold+ improves the success rate to 61.8%, further narrowing the gap to AlphaFold 3. more detailed assessment, based on the cumulative distribution of ligand RMSD, confirms IntFolds superior accuracy over these other competing models at every precision level (Figure 3b). We also assessed performance as function of ligand similarity to the training set, which shows that IntFold maintains significantly higher success rate than other methods for novel ligands, indicating strong generalization capabilities (Figure 3c). 5 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction We also benchmarked on the PoseBusters [10] benchmark. To avoid data leakage, we created test set by selecting targets from PoseBusters v2 that were deposited after the training data cutoff of 2021-09-30. As shown in Figure 3d, IntFold achieves success rate of 76.1%, outperforming the next best method, Protenix (72.6%). Additionally, Figure 3e shows high-quality prediction for receptor-interacting protein kinase 3 (RIPK3) with its novel inhibitor LK01003. 2.3. Nucleic Acid Systems Figure 4 (a) Success rates for protein-nucleic acid interface prediction on FoldBench for both protein-DNA and protein-RNA complexes. (b) Mean LDDT scores for DNA and RNA monomer structure prediction. Finally, we benchmarked IntFolds performance on systems involving nucleic acids, critical but often challenging modality (Figure 4). For protein-DNA interfaces, IntFolds success rate is 74.1%, outperforming the best contemporary method, Boltz-1 (71.0%), and demonstrating strong performance compared to AlphaFold 3 (79.18%). For protein-RNA interfaces, the model achieves success rate of 58.9%, again surpassing the next best method, Boltz-1 (56.9%), and approaching the performance of AlphaFold 3 (62.3%). The model also produces high-quality predictions for nucleic acid monomers. For RNA monomers, IntFolds LDDT score of 0.63 surpasses all other models, including AlphaFold 3 with an LDDT score of 0.61. For DNA monomers, IntFolds score of 0.50 is also highly competitive, exceeding other methods like Protenix (0.44) and approaching AlphaFold 3 (0.53). These results confirm IntFolds strength as general-purpose model with robust capabilities for nucleic acid systems. 3. Applications key advantage of IntFold is its ability to be efficiently specialized for range of downstream tasks. This is achieved using the modular adapters described in Section 5.3, which allow for targeted modifications without retraining the entire foundation model. In this section, we demonstrate the power of this approach with three key applications: improving predictions for specific target families, guiding folding with structural constraints, and predicting binding affinity. 6 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 5 (Top left) Structure of CDK2 showing the ATP site and an allosteric pocket. (Others) Comparison of predictions (LRMSD in parentheses) from the general model (Original) and the fine-tuned, specialized model for CDK2 with three different allosteric inhibitors (7RWF, 7S4T, 7RXO). 3.1. Target-type Specific Modeling While large models excel at general prediction, achieving high accuracy for specific, therapeutically relevant protein families often requires specialized knowledge. This is especially true for proteins with subtle conformational states that are critical for function. prime example is the kinase family, particularly proteins like Cyclin-dependent kinase 2 (CDK2), whose activity is modulated by distinct conformations. General-purpose models often fail to capture these inhibitor-induced structural shifts, significant hurdle for effective structure-based drug design, where modeling these specific states is essential. To address this challenge, we applied target-specific adapter to capture these distinct conformations. We used Low-Rank Adaptation (LoRA) architecture, an efficient method that introduces small number of trainable parameters while the large base model remains frozen. This adapter was trained on our curated CDK2 dataset, which contains multiple structures of CDK2 bound to diverse range of inhibitors (Section 4.1). The results show clear improvement in predictive accuracy for these specific states. As shown in Figure 5 and 12, the general model consistently defaults to predicting an open, inactive-like state for CDK2, even in the presence of allosteric inhibitors. In contrast, the fine-tuned model successfully captures the correct closed conformations (7RWF, 7S4T, and 7RXO). This success was quantified across test set of 40 CDK2 structures. On this set, the general model failed to predict any of the 5 allosteric (closed-state) conformations. The fine-tuned model, however, correctly identified 4 of the 5 allosteric cases, while also maintaining perfect accuracy on the 35 open, inactive-like state structures. This demonstrates that the specialized adapter can robustly identify rare conformational states without compromising its accuracy on more common ones. 7 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction 3.2. Guided Folding with Structural Constraints Figure 6 (a, b) Success rate comparison for IntFold with and without structural constraints on the PoseBusters dataset and antibody-antigen interfaces, respectively. (c) case study on the PD1 signaling receptor with FAB (9HK1), showing how adding constraint to the binding interface guides the model from an incorrect prediction to the correct docked conformation. Structural biologists often have prior knowledge about which residues form an interaction interface, such as known binding pocket for ligand or specific epitope for an antibody. The ability to incorporate this information as structural constraint is critical not only for improving predictive accuracy but also for experimentally testing specific biological hypotheses about how molecules interact. Providing these constraints allows researchers to guide the model towards desired configuration, focusing its predictive power on defined region of interest. We implemented this capability by training constraint-specific LoRA adapter. To encode this spatial information for the model, we added dedicated embedder to the LoRA setup, which processes the known interaction residues. This guided approach leads to dramatic improvements in success rates. For the PoseBusters dataset, applying constraints boosts the success rate from 79.5% to 89.7% (Figure 6a). The effect is even more significant for challenging antibody-antigen interfaces, where the success rate more than doubles from 37.6% to 69.0% (Figure 6b). The case study on the PD1 signaling receptor (9HK1) clearly demonstrates this effect: the unconstrained model produces physically implausible, incorrect docking pose, but providing the epitope constraint guides the model to the correct docked conformation, perfectly matching the ground truth (Figure 6c). 8 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction 3.3. Predicting Protein-Ligand Binding Affinity Figure 7 (a, b) Comparison of IntFold against other methods on the Davis and BindingDB benchmarks, as measured by area under the precision-recall curve (AUPR). (c) Correlation between predicted and experimental affinity for IntFold and Boltz-2 on the CASP16 benchmark targets L1000 and L300. Table 1 Performance of IntFold versus Boltz-2 on subset of the FoldBench benchmark. The test set includes only targets released after January 1, 2024, to avoid data leakage and provide fair evaluation of generalization performance. Interface (Success Rate %) Monomer (LDDT) ProtLig ProtProt AbAg ProtDNA ProtRNA Protein 58.17 53.90 72.13 70. 40.27 25.00 76.54 73.84 77.78 76.92 0.87 0.88 Model IntFold BoltzBinding affinity measures how tightly small molecule attaches to protein. This measure is critical in drug discovery, as it helps determine whether potential drug will act on its intended target and be potent enough to produce therapeutic effect. While structure-based virtual screening is promising method for discovering drugs against new targets, existing tools lack atomic-level precision and fail to predict binding fitness accurately. To address the challenge, we trained post-hoc prediction module on the curated affinity dataset from Section 4.1. The models performance was evaluated on the standard DAVIS [11] and BindingDB [12] benchmark datasets, with the area under the precision-recall curve (AUPR) as the primary metric. As presented in Figure 7a-b, IntFold significantly outperforms range of existing methods, including both structure-based predictors and various sequence-based approaches. To further evaluate its performance on novel targets, we tested the model on the recent CASP16 affinity track [13]. IntFolds predicted affinities show stronger correlation with experimental values (Pearson Correlation Coefficient of 0.53) compared to those of Boltz-2 (PCC of 0.47) on L1000 and comparable on L3000 (Figure 7c). This strong performance extends to broader head-to-head comparison against Boltz-2 [6] on FoldBench [4] targets released after the training cutoff of Boltz-2 (January 1, 2024). As shown in Table 1, IntFold consistently outperforms Boltz-2 on most interaction types-for example, reaching 58.17% success rate for protein-ligand interactions compared to Boltz2s 53.90%-while delivering similar performance for protein monomer folding. Specifically, IntFold shows protein-ligand success rate of 58.17% and outperforms Boltz-2s 53.90%. This consistent strong performance on recent and challenging targets matches the models reliability for affinity 9 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction prediction. 4. Data The performance of foundation model is fundamentally shaped by the scale, diversity, and quality of its training data. Our data strategy was designed to build comprehensive understanding of biomolecular interactions by integrating multiple sources. We combined high-resolution experimental structures from the Protein Data Bank (PDB) with large-scale, high-quality predicted structures in process known as distillation. This was supplemented by curating specialized datasets for critical downstream tasks, such as affinity prediction and modeling specific protein families. This multi-faceted approach ensures the model is both powerful generalist and an adaptable specialist. 4.1. Data Sources Protein Data Bank For structural data in PDB [14], we closely followed the processing procedure described in AlphaFold 3 [2]. Our weighted PDB dataset is inclusive of all mmCIF files that passed the filtering criteria, including very large or unconventional structures (Figure 10) and structures with identical sequences but distinct docking poses (Figure 11). Furthermore, we removed all the ions, unknown ligands, and ligands present in AlphaFold 3s exclusion list from the sampling entries. Distillation Datasets Beyond experimentally resolved structures in PDB, we used three additional distillation datasets. Protein Monomer Distillation: We utilized protein structures from the AlphaFold Database [15] (AFDB). Structures from AFDB were filtered based on 30% sequence similarity clustering and global pLDDT over 85. Following ESM3 [16], we removed structures with fewer than 0.5𝐿 long-range contacts in the chain (where 𝐿 is the chain length). This approach was chosen over AlphaFold 3s method of using AlphaFold 2 [1] predictions on MGnify [17] sequences. Disordered Protein PDB Distillation: We employed the disordered protein PDB distillation set, prepared using AlphaFold-Multimer v2.3 [18] predictions on PDB proteins with unresolved regions, consistent with the methodology described in AlphaFold 3. We only used assemblies with fewer than 2000 tokens for this dataset. Antibody-Antigen Distillation: We constructed dataset of antibody-antigen distillation from PLAbDab [19]. To ensure diversity, we clustered all sequences at 60% identity and sampled representative interfaces based on the inverse size of each interface cluster. On the resulting pairs, we performed large-scale structural predictions using IntFold. The distillation set was formed by filtering predictions to examples selected by our ranking method. Affinity Dataset We built proteinligand affinity set by merging activity data from ChEMBL [20], BindingDB [12], GalaxyDB [21], BioLip [22] and PubChem [23], then cleaning them with the ExCAPE-DB protocol [24]. The pipeline (i) retained compounds whose best IC50/Ki/Kd value was 10𝜇𝑀 while keeping matched inactives, (ii) discarded entries flagged as low-confidence or linked to ultra-rare chemotypes, (iii) back-filled missing affinity values from primary sources and filtered out assays that still lacked number, and (iv) removed proteinligand pairs whose reported affinities disagreed by more than one log unit across databases. The final dataset comprises roughly three million log-scaled activity measurements spanning 27,000 proteins and 1,000,000 distinct ligands. CDK2 Dataset For the CDK2 dataset, we selected all entries in the PDB that are explicitly associated with Cyclin-dependent kinase 2. All CDK2-annotated entries in our training set were first partitioned into 10 previously validated allosteric complexes and larger pool of PDB-derived candidates; after global alignment to reference orthosteric structure, we calculated the Euclidean distance between 10 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 8 (a) Latency comparison of our custom kernel against other implementations. (b) Memory usage comparison of our custom kernel against other implementations. each candidates dominant pocket centroid and those of the allosteric set, retaining only structures that exceeded our distance and geometry thresholds as orthosteric examples, yielding final dataset of roughly 200 complexes ( 10 allosteric + 190 orthosteric). The dataset was split at ligandsimilarity threshold of 0.5, and the resulting subset was used to train our model. 4.2. MSAs and Templates Genetic Search: For constructing the paired MSA (providing cross-chain genetic information), we used Jackhmmer [25] to search the UniProt [26] database and paired them based on species, following AlphaFold 3. For the main MSA (unpaired sequences), our approach differed from AlphaFold 3. We utilized the ColabFold [27] search pipeline (typically with MMseqs2 [28]) to generate the main MSAs for protein sequences. For RNA sequences, we did not use MSAs. Template Search: The template search pipeline, including the tools (hmmsearch), databases (PDB sequences), and parameters, was implemented as described in AlphaFold 3. 5. Modeling As illustrated in Figure 1, IntFold comprises four main components: an embedding trunk of sequence, diffusion module, confidence head, and optional adapters for specialized tasks. The overall model architecture and training procedures are based on the methods described in the AlphaFold 3 supplementary materials [2]. Below, we highlight our novel contributions, including the custom attention kernel, the confidence metric, and insights gained from the models development. 5.1. Flash Attention Pair Bias We developed FlashAttentionPairBias, custom kernel implemented in Triton based on FlashAttention 2 [29]. To handle the pair bias, our kernel avoids the memory bottleneck through on-the-fly bias broadcasting. Rather than materializing the full intermediate tensor, only the necessary slice of the bias is loaded from HBM into SRAM during the attention computation. This mechanism brings substantial reductions in peak memory usage and achieves lower forward pass latency than existing implementations, as shown in Figure 8. 11 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction 5.2. Model-Agnostic Ranking The stochastic nature of the diffusion process results in variations among predicted structures for given target. This variability often creates significant gap between randomly selected prediction and the best possible structure within set of generated decoys. While many models employ learned confidence metric like iPTM to rank predictions, these methods can be tricky to train and are not always reliable for complex interfaces. As complementary approach, we propose training-free, model-agnostic ranking method based on structural similarity. Our method operates on the hypothesis that while incorrect predictions may vary widely, correct predictions tend to be more similar, as they all approximate the ground truth structure. In practice, for given target, we first generate diverse set of 25 predictions (5 samples per seed across 5 seeds). We then perform an all-versus-all comparison within this set using DockQ as the pairwise similarity metric. The structure with the highest mean DockQ score against all others is selected as the top-ranked prediction. This similarity-based ranking method consistently improves performance. When applied to our antibody-antigen benchmark set, this ranking method improves the final success rate by approximately 3% compared to selecting random sample. This demonstrates that consensus-based scoring is an effective strategy to better capitalize on the diverse, high-quality structures generated by the model. 5.3. Modular Adapters We used modular adapters to equip the model with new knowledge and specialized capabilities. These adapters work by introducing small number of trainable parameters, while the main foundation model remains frozen. We developed two distinct adapter architectures for this purpose. Per-layer LoRA Adapters: The first architecture uses per-layer Low-Rank Adaptation [30]. This approach is for tasks that steer the model toward specific 3D structures, such as modeling allosteric states. For predictions that require structural constraints like known epitopes, we add an additional embedder to the LoRA setup for encoding this information for the model. Post-Hoc Downstream Module: The second architecture is separate post-hoc module designed for downstream tasks different from structure prediction. This external adapter takes the final representations from the models backbone trunk and processes them through its own parameters. Currently, we use an affinity prediction module composed of four additional Pairformer blocks to output protein-ligand fitness score. 5.4. Insights on Instability Training model of this scale and complexity revealed several factors that affect stability, ranging from architectural choices to numerical considerations. 5.4.1. Activation Explosion and Gradient Spikes The primary instability observed was repeated loss spikes, which were preceded by an exploding gradient norm and prevented the model from converging. We identified that these large gradients originated from the backpropagation of the diffusion loss, caused by an abnormally large single representation sent to the diffusion module. This activation explosion initially appears in the Transition module of later layers in the Pairformer Stack, and moves to earlier layers as the training progresses (Figure 9). To alleviate this, we implemented skip-and-recover mechanism. During training, sample is skipped if the magnitude of its single representation exceeds 40,000. Furthermore, if the gradient 12 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction norm of step surpasses predefined threshold, we will recover training from previous checkpoint with resampled data batches. The underlying cause is likely the models architecture. The pre-Layernorm [31] design of the Pairformer Stack permits value accumulation on the straight-through path, which is amplified by the Gated Linear Units [32]. This hypothesis is supported by experiments where introducing sandwich LayerNorm [33] and QK-normalization [34] significantly reduced the frequency of loss spikes. Figure 9 (a) Histogram of single representation magnitudes during training. (b) Single representation magnitude across Pairformer layers at different training steps. 5.4.2. Parametrization and Initialization We also addressed the instability related to parametrization and initialization. For embedding input features like reference atoms, the method described in AlphaFold 3s supplement involves first concatenating features, then applying single linear projection. We found that this approach can lead to poor single representations, affecting the performance of the model, especially the confidence module. This is because the initialization of linear layer is related to its input dimension, which becomes very large after concatenation. To solve this, we use separate linear layers for individual features and then add them up. This approach leads to more stable and reasonable weight initialization, despite being equivalent in forward computation. Furthermore, we adopted simplified weight initialization method compared to those described by AlphaFold 2 [1]. We use zero initialization [35] for the final layer in residue paths, Kaiming Normal [36] for layers with ReLU activation, and Lecun Normal [37] for all others. Additionally, for modules utilizing AdaZero [38], we choose to initialize the output projection of residual blocks with Lecun Normal instead of all zeros. 5.4.3. Numerical Consideration Numerical precision during training is critical as well. While the models backbone trains effectively and efficiently with Bfloat16 mixed precision, we found that the diffusion module required full Float32 precision for stable training. We also addressed instability in the loss calculations alignment step. For the Kabsch alignment, it is better to use the already-augmented ground truth coordinates when aligning with the models predictions. This is more stable than using the original coordinates, though the two methods are mathematically equivalent. To ensure this alignment is always well-posed, we required the data crops much contain at least four resolved residues, avoiding ambiguous rotational alignment. IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction 6. Conclusion In this report, we introduced IntFold, versatile and controllable foundation model for biomolecular structure prediction. We have demonstrated that IntFold achieves state-of-the-art predictive accuracy, matching the performance of AlphaFold 3 on critical tasks like protein monomer and protein-protein interaction prediction while significantly outperforming other contemporary methods. This high performance is driven by key technical innovations, including custom attention kernel that is faster, more memory-efficient than standard industry implementations, and model-agnostic, training-free, similarity-based ranking method that consistently improves prediction selection. Beyond its accuracy as general predictor, IntFolds primary innovation is its adaptability. Through the use of lightweight, modular adapters, the model can be efficiently specialized for range of downstream applications critical for drug discovery. We have shown this capability by accurately modeling the inhibitor-specific conformations of CDK2, significantly improving prediction success by incorporating structural constraints, and accurately predicting protein-ligand binding affinity, where it surpasses existing methods on standard benchmarks. 7. Limitations and Future Directions While IntFold represents significant step forward, key challenges remain that define our future work. Computational Complexity: Like other models in its class, IntFolds use of triangle attention presents computational bottleneck with complexity of approximately 𝑂(𝑁 3), limiting the crop size during training and its speed on very large assemblies. primary goal is to explore new architectures that can mitigate this complexity without sacrificing accuracy. Accuracy on Challenging Targets: There is still room to improve predictive accuracy, especially for the most challenging and therapeutically relevant targets like antibody-antigen complexes. While IntFold narrows the performance gap, pushing the boundaries of accuracy for these systems remains key focus. Expanding Functional Capabilities: The current work demonstrates several key adaptations. We aim to expand IntFolds capabilities to more downstream applications and into the realm of de novo protein design, further bridging the gap between predicting biomolecular structures and engineering novel functions. 8. Acknowledgments We thank our partner, Jinghe Cloud (Dajing Holding), for providing the stable and scalable computational resources essential for this project. We also thank our colleagues and partners in academia and industry for their invaluable discussions and unwavering support, with special thanks to Shuangjia Zheng and Tao Shen for their insightful comments on the Applications and Modeling sections."
        },
        {
            "title": "References",
            "content": "[1] [2] John Jumper et al. Highly accurate protein structure prediction with AlphaFold. In: nature 596.7873 (2021), pp. 583589. Josh Abramson et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3. In: Nature 630.8016 (2024), pp. 493500. 14 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction [3] Gustav Olanders et al. Challenge for Deep Learning: Protein Structure Prediction of LigandInduced Conformational Changes at Allosteric and Orthosteric Sites. In: Journal of Chemical Information and Modeling 64.22 (2024), pp. 84818494. [4] Sheng Xu et al. FoldBench: An All-atom Benchmark for Biomolecular Structure Prediction. [5] In: bioRxiv (2025), pp. 202505. Jeremy Wohlwend et al. Boltz-1: Democratizing Biomolecular Interaction Modeling. In: bioRxiv (2024), pp. 202411. [6] Saro Passaro et al. Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction. In: bioRxiv (2025), pp. 202506. [7] Chai Discovery team et al. Chai-1: Decoding the molecular interactions of life. In: BioRxiv (2024), pp. 202410. [8] Lihang Liu et al. Technical report of HelixFold3 for biomolecular structure prediction. In: arXiv preprint arXiv:2408.16975 (2024). [9] ByteDance AML AI4Science Team et al. Protenix-advancing structure prediction through comprehensive AlphaFold3 reproduction. In: bioRxiv (2025), pp. 202501. [10] Martin Buttenschoen, Garrett Morris, and Charlotte Deane. PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences. In: Chemical Science 15.9 (2024), pp. 31303139. [11] Mindy Davis et al. Comprehensive analysis of kinase inhibitor selectivity. In: Nature biotechnology 29.11 (2011), pp. 10461051. [12] Tiqing Liu et al. BindingDB: web-accessible database of experimentally determined protein ligand binding affinities. In: Nucleic acids research 35.suppl_1 (2007), pp. D198D201. [13] Michael Gilson et al. Assessment of pharmaceutical protein-ligand pose and affinity predictions in CASP16. In: Assessment 15 (2025), p. 4. [14] Helen Berman et al. The protein data bank. In: Biological Crystallography 58.6 (2002), pp. 899907. [15] Mihaly Varadi et al. AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. In: Nucleic acids research 50.D1 (2022), pp. D439D444. [16] Thomas Hayes et al. Simulating 500 million years of evolution with language model. In: Science (2025), eads0018. [17] Alex Mitchell et al. MGnify: the microbiome analysis resource in 2020. In: Nucleic acids research 48.D1 (2020), pp. D570D578. [18] Richard Evans et al. Protein complex prediction with AlphaFold-Multimer. In: biorxiv (2021), pp. 202110. [19] Brennan Abanades et al. The Patent and Literature Antibody Database (PLAbDab): an evolving reference set of functionally diverse, literature-annotated antibody sequences and structures. In: Nucleic Acids Research 52.D1 (2024), pp. D545D551. [20] Barbara Zdrazil et al. The ChEMBL Database in 2023: drug discovery platform spanning multiple bioactivity data types and time periods. In: Nucleic acids research 52.D1 (2024), pp. D1180D1192. [21] Penglei Wang et al. Structure-aware multimodal deep learning for drugprotein interaction prediction. In: Journal of chemical information and modeling 62.5 (2022), pp. 13081317. 15 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction [22] Jianyi Yang, Ambrish Roy, and Yang Zhang. BioLiP: semi-manually curated database for biologically relevant ligandprotein interactions. In: Nucleic acids research 41.D1 (2012), pp. D1096D1103. [23] Sunghwan Kim et al. PubChem 2023 update. In: Nucleic acids research 51.D1 (2023), pp. D1373D1380. [24] Jiangming Sun et al. ExCAPE-DB: an integrated large scale dataset facilitating Big Data analysis in chemogenomics. In: Journal of cheminformatics 9 (2017), pp. 19. [25] Steven Johnson, Sean Eddy, and Elon Portugaly. Hidden Markov model speed heuristic and iterative HMM search procedure. In: BMC bioinformatics 11 (2010), pp. 18. [26] The UniProt Consortium. UniProt: the Universal Protein Knowledgebase in 2023. In: Nucleic Acids Research 51.D1 (2022), pp. D523D531. sn: 0305-1048. oi: 10 . 1093 / nar / gkac1052. eprint: https://academic.oup.com/nar/articlepdf/51/D1/D523/ 48441158/gkac1052.pdf. url: https://doi.org/10.1093/nar/gkac1052. [27] Milot Mirdita et al. ColabFold: making protein folding accessible to all. In: Nature methods 19.6 (2022), pp. 679682. [28] Martin Steinegger and Johannes Söding. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. In: Nature biotechnology 35.11 (2017), pp. 1026 1028. [29] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In: The Twelfth International Conference on Learning Representations. 2024. [30] Edward Hu et al. Lora: Low-rank adaptation of large language models. In: ICLR 1.2 (2022), p. 3. [31] Ruibin Xiong et al. On layer normalization in the transformer architecture. In: International conference on machine learning. PMLR. 2020, pp. 1052410533. [32] Noam Shazeer. Glu variants improve transformer. In: arXiv preprint arXiv:2002.05202 (2020). [33] Ming Ding et al. Cogview: Mastering text-to-image generation via transformers. In: Advances in neural information processing systems 34 (2021), pp. 1982219835. [34] Alex Henry et al. Query-Key Normalization for Transformers. In: Findings of the Association for Computational Linguistics: EMNLP 2020. 2020, pp. 42464253. [35] Priya Goyal et al. Accurate, large minibatch sgd: Training imagenet in 1 hour. In: arXiv preprint arXiv:1706.02677 (2017). [36] Kaiming He et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: Proceedings of the IEEE international conference on computer vision. 2015, pp. 10261034. [37] Yann LeCun et al. Efficient backprop. In: Neural networks: Tricks of the trade. Springer, 2002, pp. 950. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. 2023, pp. 41954205. 16 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction"
        },
        {
            "title": "The IntFold Team",
            "content": "Leon Qiao, Wayne Bai, He Yan, Gary Liu, Nova Xi, Xiang Zhang IntFold+ To create IntFold+, model specialized for high-quality antibody-antigen and protein-ligand prediction, we performed dedicated fine-tuning procedure on the base IntFold model. For this stage, we augmented the training data by including our curated antibody-antigen distillation set 4.1. We also modified the training protocol by increasing the batch size, removing the MSA pairing information for antibody-antigen complexes to force greater reliance on structural features, adjusting the ratio of nucleic data, and masking regions with low pLDDT. This combination of data enrichment and specialized training strategy results in model highly optimized for the nuances of antibody-antigen docking and protein-ligand binding. While we have demonstrated the effectiveness of these components, detailed ablation study to quantify the individual contribution of each modificationsuch as the best data recipe, removal of MSA pairing, and pLDDT maskingremains an area for future work."
        },
        {
            "title": "Supplementary Figures",
            "content": "Figure 10 Examples of diverse PDB entries included in the training data. (A) 7ARY, large DNA-rich assembly. (B) 7ECY, large virus Icosahedral with FAB. (C) 5KHR, structure with primarily C-alpha resolved atoms. (D) 4V5Z, structure of 80S ribosome with primarily Phosphorus resolved for RNA. 17 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 11 Examples of PDB entry pairs with identical sequences but different experimentally determined docking poses, included in the training data. (a) The pair 3AUX and 3AUY. (b) The pair 2NZ5 and 2NZA. 18 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 12 Performance of other methods on CDK2 with three different allosteric inhibitors (7RWF, 7S4T, 7RXO) 19 IntFold: Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction Figure 13 Performance of other methods on PD1 signaling receptor-FAB complex (9HK1). Figure 14 Performance orthosteric sites of CDK2."
        }
    ],
    "affiliations": [
        "IntelliGen AI"
    ]
}