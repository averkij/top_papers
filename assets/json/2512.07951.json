{
    "paper_title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
    "authors": [
        "Zekai Luo",
        "Zongze Du",
        "Zhouhang Zhu",
        "Hao Zhong",
        "Muzhi Zhu",
        "Wen Wang",
        "Yuling Xi",
        "Chenchen Jing",
        "Hao Chen",
        "Chunhua Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap"
        },
        {
            "title": "Start",
            "content": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality Zekai Luo1 Zongze Du1 Zhouhang Zhu1 Hao Zhong1 Wen Wang1 Yuling Xi1 Chenchen Jing2 Hao Chen1, Chunhua Shen1,2, Muzhi Zhu1 1Zhejiang University 2Zhejiang University of Technology 5 2 0 D 8 ] . [ 1 1 5 9 7 0 . 2 1 5 2 : r Figure 1. Qualitative results of our proposed video reference guided face swapping model, LIVINGSWAP. Across challenging cinematic scenariosincluding long-take shots, complex illumination, exaggerated expressions, heavy facial makeup, and semi-transparent occlusionsour method consistently preserves target identity and fine-grained attributes with high fidelity, while maintaining robust visual realism."
        },
        {
            "title": "Abstract",
            "content": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LIVINGSWAP, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To Corresponding authors. address the scarcity of data for reference-guided training, we construct paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable groundtruth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source videos expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap. 1. Introduction Video face swapping holds significant value in the film and entertainment industries. However, existing methods fall short of meeting the stringent demands of high-quality cinematic production. For instance, GAN-based methods [4, 7, 23, 24, 32], which typically operate in frameby-frame manner (see Fig. 2a), have made considerable progress in injecting the target identity. Yet they often strugFigure 2. (a) GAN-based approaches process videos in frame-by-frame manner, and therefore often struggle with realism and suffer from temporal inconsistency. (b) Inpainting-based methods focus on generating the facial region based on sparse conditions, which inevitably leads to loss of fidelity and unnatural visual artifacts. (c) Recent reference-based generation methods enable faithful utilization of rich visual attributes contained in references and demonstrate remarkable capability in preserving them. gle with realism and suffer from temporal artifactssuch as flickering and jitterespecially in long sequences. Meanwhile, contemporary video diffusion models [5, 14, 36, 49], while achieving high visual quality and temporal consistency, often rely on sparse conditioning signals such as facial landmarks. This reliance makes it challenging to perfectly align the generated expressions, lighting, and subtle nuances with the source video, resulting in faces that may appear unnatural or lack lifelike vitality. Consequently, there is critical need for video face swapping model capable of directly leveraging the rich, detailed information from the source videos facial region. Achieving high degree of customization while preserving the integrity of the original content remains fundamental challenge in generative media [42]. Methods based on DDIM inversion [12, 20, 28] or Score Distillation Sampling (SDS) [16, 27] often struggle to strike an optimal balance between editability and fidelity. In the field of video editing, common strategy involves combining inpainting with structural guidance such as depth or keypoints [17, 19, 33] (see Fig. 2b). However, such approaches inherently discard the original pixel information within the edited region, leading to noticeable loss of fidelity in details. Recently, reference guided generation has demonstrated remarkable breakthroughs in image editing, successfully reconciling editing flexibility with high-fidelity reconstruction [8, 21, 38]. This approach directly guides the model using the reference images, enabling the faithful utilization of rich visual attributes contained in the references. Nevertheless, adapting these techniques to video face swapping presents unique challenges: (1) the difficulty of injecting stable and consistent identity condition throughout long and complex video sequences; and (2) the scarcity of paired training data for reference-guided video face swapping task. In this work, we address these challenges by introducing LIVINGSWAP, the first video editing model for face swapping that directly references the source videos details. To facilitate this, we decompose the challenging task of long-video face swapping into highly controllable pipeline comprising keyframe identity injection, video reference completion, and temporal stitching. This pipeline not only enables flexible identity guidance using high-quality image swapping results, but also mitigates the accumulation of errors that typically arises in long videos.Furthermore, we construct Face2Face, the first-of-its-kind dataset specifically curated for video reference-guided face swapping. To ensure reliable ground-truth supervision, we reverse each data pair by using the generated results as inputs and the original data as the ground truth. To further validate the effectiveness of LIVINGSWAP, we collect set of cinematic video clips featuring wide range of challenging conditions and construct the CineFaceBench. Benefiting from its highly controllable pipeline and superior generation quality, LIVINGSWAP seamlessly integrates the target identity with the high-definition details of the source video, faithfully preserving original expressions, lighting conditions, and other key facial attributes. Our contributions are as follows: We introduce LIVINGSWAP, stable video faceswapping solution with controllable pipeline that reduces the need for frame-by-frame human editing by factor of 40, making it particularly well-suited for the professional film and television industry. We introduce Face2Face, paired dataset designed to address the scarcity of training data for video referenceguided face swapping. Moreover, by reverse data pairs and leveraging strong priors from pretrained video models, our approach is able to surpass the limitations of the dataset itself and achieve superior performance. We propose CineFaceBench, cinematic scenarios benchmark that facilitates reliable comparison of video face-swapping models tailored to industrial scenarios. 2. Related Work However, Video face swapping. The task of video face swapping is to replace the identity in video while preserving attributes such as pose, expression, illumination, and background. GAN-based approaches [4, 7, 23, 24, 32], which typically process videos frame-by-frame, have made notable progress in injecting target identity through encoderdecoder pipelines. they often suffer from temporal inconsistenciessuch as flickering and jitterespecially in long sequences. Recently, video diffusion models are used in video face swapping. Diffusionbased methods [5, 14, 36, 49] demonstrate stronger generative power and achieve higher visual quality and temporal consistency. They treat face swapping as inpainting by masking the original face and regenerating it conditioned on background frames and auxiliary attributes [5, 36]. This often leads to the loss of fine-grained details and introduces inconsistencies with the models pretrained priors, thereby degrading generation quality. In this work, we tackle these challenges by directly leveraging detailed source video references for face swapping, combined with carefully curated dataset Face2Face and an reversing data strategy to provide high-fidelity supervision. Diffusion-based Video Editing. With the rapid development of diffusion models, wide range of customization and editing techniques [3, 31, 40, 41, 48, 50, 51] have emerged. From methodological perspective, these approaches can be broadly categorized into inversion-based, inpainting-based, and reference-guided methods. Inversionbased methods [12, 16, 20, 27, 28] reconstruct the original video trajectory in the diffusion process to enable editing, but they often struggle to balance editability and fidelity. Inpainting-based approaches [17, 19, 33] edit masked regions with structural guidance such as optical flow, depth, or keypoints, achieving temporal coherence but usually at the cost of losing fine-grained details. Recently, referenceguided methods [6, 8, 18, 21, 38] have shown strong potential by leveraging reference images to combine flexible editing with high-fidelity reconstruction. Nonetheless, extending this paradigm to long video sequences remains challenging due to the scarcity of paired data and the difficulty of maintaining consistent identity or attributes over time. In this work, we use reference-guided generation for face video swapping, enabling controllable identity transfer while preserving temporal coherence and visual fidelity across long sequences. 3. Preliminary: Video Generation with DiT and Rectified Flow Recent advancements in diffusion-based video generation leverage the Diffusion Transformer (DiT) architecture combined with continuous-time training objectives such as Rectified Flow (RF) [10] to achieve high-quality and temporally coherent synthesis. DiT extends traditional UNet-based diffusion backbones with transformer blocks, enabling more flexible and scalable modeling of high-dimensional video data. In this framework, the model learns continuous denoising process by predicting the velocity between pair of latent points. Given ground-truth sample x1, and standard Gaussian noise x0 (0, I), linearly interpolated latent xt is constructed as: xt = tx1 + (1 t)x0, (1) where [0, 1] is timestep sampled from predefined distribution. The target velocity is defined as the derivative of xt with respect to time, yielding: vt = dxt dt = x1 x0. (2) The DiT model is trained to estimate this velocity given the latent xt, the conditioning signal c, and the timestep t. Let u(xt, c, t; θ) be the models predicted velocity, where θ denotes the model parameters. The training objective is to minimize the mean squared error (MSE) between the predicted and ground-truth velocities: = Ex0,x1,c,t u(xt, c, t; θ) vt2 . (3) This training formulation enables high-quality results with significantly fewer steps and greater computational efficiency in video generation. 4. Method In video face swapping tasks, the input typically consists of source video Vs = {ft [1, ]} to be modified, mask sequence = {mt [1, ]} specifying the target regions for editing, and target identity image Itar. The overall framework of LIVINGSWAP is illustrated in Fig. 3. In the following sections, we describe the design of LIVINGSWAP across four essential components of video face swapping: (1) target identity injection, (2) preservation of source-video attributes, (3) consistent long-video generation, and (4) construction of paired training dataset. 4.1. Keyframes Identity Injection Maintaining stable target identity across long and dynamic video sequences remains fundamental challenge in video face swapping. Compared to video-level approaches, image-level face swapping methods often provide stronger and more precise identity injection. Motivated by the complementary strengths of image-based face swapping and video interpolation paradigms [13, 37], we introduce keyframe-based identity injection strategy that delivers robust and consistent identity conditioning for long-sequence video generation. Figure 3. Overview of the proposed LIVINGSWAP framework for video face swapping. (1) Keyframes are used as temporal anchors to ensure consistent identity injection across long sequences. (2) We feed the source video as reference, enabling high-fidelity reconstruction of non-identity attributes such as lighting and expressions. (3) By sequentially generating chunks and propagating the final frame of the previous chunk as guidance, LIVINGSWAP achieves seamless transitions in long videos. (4) We use Per-frame Edit method to generate the data and reverse data roles to construct paired samples, ensuring reliable and artifact-free learning. As illustrated in Fig. 3 Part 1, we begin by selecting set of representative frames from the input video as keyframes, denoted as swap-in key = swap-in ki ki K, [1, ]. (4) These keyframes are chosen at moments exhibiting significant variations in pose, expression, or illumination, ensuring that the major appearance changes across the video are well captured. Next, we perform Per-frame Edit on these keyframes using high-quality image-level method [15]. This process can be optionally followed by manual refinement using tools such as Adobe Photoshop, which aligns with industrial workflows requiring both high-quality results and flexible editability. After obtaining high-quality edited keyframes, we use each neighboring keyframe pair {f swap-in ki , swap-in ki+1 } (5) serves as temporal boundary condition that guides the diffusion model during sequence completion. Compared to per-frame processing commonly adopted in industrial pipelines, our keyframe scheme requires modifying only small set of boundary keyframes, which preserves the visual quality of frame-by-frame editing while substantially improving efficiency. 4.2. Video Reference Completion Beyond injecting the target identity, it is equally crucial to preserve non-identity attributes within the edited region and maintain the integrity of unmodified content in the source video. However, existing inpainting-based approaches typically discard original pixels and rely on additional structured inputs, which often degrade visual fidelity and weaken the generation prior. Inspired by the reference-guided architecture of the video editing foundation model VACE [19], we extend this paradigm to video face swapping to achieve high-fidelity reconstruction. As illustrated in Fig. 3 Part 2, instead of masking the facial area in the source video and depending on external pretrained encoders, we directly feed the complete source video segment [ki:ki+1] = {ft [ki, ki+1]} (6) as visual reference. This design preserves fine-grained visual cuessuch as illumination, subtle expressions, and background detailswithout information degradation. Furthermore, we incorporate an optional target identity image Itar to enhance identity fidelity in the first or last frame, particularly when the source video contains occlusions or low-quality instances (e.g., closed eyes). As demonstrated in our ablation study (Tab. 4), this additional identity cue is not strictly necessary for all scenarios, but it consistently improves identity in challenging cases. To integrate identity and appearance signals, we encode each input using the VAE encoder Eϕ() and concatenate the resulting latent tokens in temporally aligned order [47]: Zc = Concattoken (cid:0) Eϕ(Itar), Eϕ(f swap-in Eϕ(V [ki:ki+1] ), ki ), Eϕ(f swap-in ki+1 (7) )(cid:1). where Zc denotes the aggregated latent conditioning tokens. The ordering naturally aligns with the temporal modeling of video diffusion models, enabling the generator to leverage temporal priors. Additionally, we construct binary mask sequence that marks the editable region via black-filled tokens and concatenate it with Zc along the channel dimension to provide explicit spatial localization. To support adaptive feature injection, we introduce an attribute encoder composed of DiT blocks, mirroring the architecture of the diffusion backbone and initialized with matching pretrained weights [19, 46]. At each layer, the output of the attribute encoder is injected into the corresponding layer of the backbone via element-wise addition, enabling hierarchical and fine-grained conditioning in latent space. Formally, the injection process is defined as: (cid:16) (l+1) = D(l) θ (l) + A(h) ψ (Z (h) (cid:17) , ) , (8) where (l) is the hidden representation of the l-th layer of the diffusion backbone Dθ, and A(h) ψ denotes the h-th block of the attribute encoder. This design preserves the pretrained generative prior while enabling flexible and adaptive conditioning, thereby effectively capturing pixel-level details from the source video. 4.3. Temporal Stitching To accommodate industrial video face swapping requirements on variable-length inputs and to address the generation-length limitation of existing video diffusion backbones, we partition long videos into multiple fixedlength chunks for sequential processing. critical question that follows is how to properly divide these chunks. Through extensive experiments, we find that introducing an overlap between adjacent chunks is essential. When each chunk is generated independently without temporal overlap, noticeable frame discontinuities and temporal jumps often emerge at the boundaries. Fortunately, benefiting from the synergy between keyframe design and reference-guided generation, our method enables efficient temporal stitching across chunks, ensuring coherent transitions in the final output. Specifically, as illustrated in Fig. 3 Part 3, we process the chunks in chronological order. To mitigate the accumulation of cross-chunk errors, we adopt the following effective strategy. For the first chunk, both the start and end guidance frames are taken directly from the corresponding keyframes. For subsequent chunks, we use the final output frame of the previous chunk, swap-out , as the startframe guidance, while the end-frame guidance remains the keyframe swap-in . Formally, the generation of each chunk is defined as: ki+1 ki = Dθ,ψ }ki+1 t=ki {f swap-out , [ki:ki+1] , swap-in ki+1 (cid:0)f swap-out ki , Itar, (cid:1). (9) In addition, to flexibly position keyframes under the constraint of fixed inference length, we employ several auxiliary engineering techniques, including frame interpolation, temporal reverse playback, frame skipping, and multi-pass inference. These strategies allow the model to adapt to diverse video rhythms and temporal structures. Finally, given that the reference model introduced in Sec. 4.2 produces fixed 81-frame output per inference, while our method typically requires manual editing only for the first and last frames of each chunk, this design yields approximately 40 reduction in manual labor, making the approach highly practical for industrial deployment. 4.4. Dataset Construction Face video datasets typically contain only the videos of individuals and lack paired sourcetarget samples required for video reference face swapping. To enable effective training of LIVINGSWAP, we construct paired dataset, Face2Face, by combining Per-frame Edit procedure with role-reversing strategy to form sourcetarget pairs. As discussed in Sec. 4.1, the Per-frame Edit process provides high-quality single-frame face-swapped results. This industrial approach, often based on GAN-based face swapping models such as Inswapper [15], achieve strong fidelity to the source video by leveraging the full-pixel source frame as input. However, such results struggle with realism and suffer from temporal inconsistencies and degraded visual quality, including artifacts and distortions  (Fig. 5)  . To overcome these challenges, as illustrated in Fig. 3 Part 4, we reverse the data pair roles when constructing training samples: the GAN-generated swapped video is used as the model input Vs, while the original unedited video provides the keyframe inputs swap-in , the target identity image Itar, and the ground-truth supervision. This design ensures that the reference frames and ground-truth frames share the same identity, providing artifact-free, highquality, and temporally consistent supervision signals. key Benefiting from the prior knowledge inherited from the pretrained model and our role-reversing strategy, LIVINGSWAP exhibits strong robustness to noisy training samples, effectively going beyond the limitations of training data quality, as further demonstrated in Sec. 5.3 and Sec. A. Finally, leveraging Face2Face, we apply the rectified flow loss (detailed in Sec. 3) to supervise LIVINGSWAP in preserving source-video attributes with high fidelity. Figure 4. Qualitative comparison with state-of-the-art face-swapping methods. LIVINGSWAP achieves the best overall performance, outperforming both GAN-based and diffusion-based approaches in video consistency, visual fidelity, and identity similarity. Although our keyframes are generated using Inswapper, the final results produced by LIVINGSWAP are more stable and better preserve source attributes, even in challenging scenarios such as side profiles, occlusions, facial makeup, and complex lighting. 5. Experiments 5.1. Experimental Setup Dataset. For training, we construct our dataset Face2Face using CelebV-Text [45] and VFHQ [39]. CelebV-Text is large-scale videotext dataset containing approximately 70,000 in-the-wild facial video clips, totaling around 279 hours of footage. VFHQ comprises over 16,000 highresolution facial video clips, covering diverse identities and scenarios. Based on these two datasets, we employ Inswapper [15] to generate paired face-swapping data and then reverse the Input-GroundTruth roles to construct our final training dataset, Face2Face. The construction details of dataset please refer to Sec. 4.4 and Sec. Benchmark. For evaluation, we adopt FaceForensics++ (FF++) [29], widely used benchmark for video face manipulation analysis. However, FF++ primarily consists of interview-style or livestream videos, which do not adequately reflect the challenging conditions commonly encountered in cinematic productions. To properly assess model performance in real film-like environments, we construct new benchmark, CineFaceBench. CineFaceBench contains 400 targetsource test pairs. The curated video clips span wide range of challenging cinematic scenarios, including long-take shots, complex lighting conditions, exaggerated expressions, heavy facial makeup, and semitransparent occlusions. To further assess robustness under different levels of identity similarity, each video is paired with two target imagesan easy and hard caseselected based on identity similarity scores. The construction details of CineFaceBench please refer to Sec. H. Metrics. To comprehensively evaluate the faceswapping performance, we employ both image-level and video-level metrics to assess the quality of the generated results. Following prior work [4, 5, 36], we randomly sample 10 frames from each face-swapped video to compute Table 1. Quantitative comparison CineFaceBench across multiple metrics. Higher is better for ID Similarity and Gaze; lower is better for Expression, Lighting, Pose, and FVD. Best values are in bold and second best are underlined. Methods ID Sim. Expr. Light Gaze Pose FVD easy hard easy hard easy hard easy hard easy hard easy hard Avg. Rank SimSwap BlendFace CanonSwap Face-Adapter Inswapper LivingSwap (Ours) 0.506 0.482 0.506 0.270 0.567 0.532 0.385 0.315 0.365 0.107 0.422 0.367 2.217 1.919 1.935 2.208 2.081 1.943 2.683 2.285 2.382 2.495 2.607 2.471 0.214 0.245 0.223 0.291 0.189 0. 0.240 0.271 0.255 0.319 0.243 0.238 0.722 0.751 0.671 0.685 0.734 0.752 0.712 0.726 0.684 0.692 0.741 0.755 4.623 4.450 3.297 5.643 3.421 3.108 4.820 4.520 3.492 6.423 3.916 3.399 74.63 100.28 104.19 176.96 66.62 54. 75.33 106.58 111.81 182.25 73.48 63.97 3.917 3.583 3.750 5.583 2.500 1.667 Table 2. Quantitative comparison with SOTA methods on FF++. Methods ID Sim. Expr. Light Gaze Pose FVD Avg. Rank Deepfakes FaceShifter InfoSwap SimSwap BlendFace CanonSwap DiffSwap Face-Adapter Inswapper LivingSwap (Ours) 0.432 0.485 0.542 0.562 0.480 0.523 0.261 0.247 0.636 0. 2.941 2.451 2.868 2.674 2.256 2.307 1.912 2.564 2.536 2.466 0.340 0.225 0.290 0.221 0.228 0.205 0.199 0.259 0.214 0.584 0.690 0.586 0.720 0.717 0.685 0.687 0.641 0.704 4.662 2.696 2.962 2.977 2.196 1.782 2.277 3.608 2.464 47.54 18.73 47.28 33.97 21.96 30.30 83.98 36.83 20. 0.211 0.706 2.336 19.29 9.50 4.67 7.67 5.17 4.00 3.83 5.00 8.17 3.83 3. image-level evaluation metrics, including ID Similarity, Expression Error, Lighting Error, Gaze Error, and Face Pose Error. ID Similarity is measured by encoding both the face-swapped result and the target image into identity vectors using pre-trained ID encoder [35], followed by computing the cosine similarity between them. In addition to identity-related metrics, we calculate Expression and Lighting Errors by extracting their respective coefficients using 3DMM-based face reconstruction method [9] and computing the L2 distance between the source and swapped results. Similarly, we use gaze estimation model [1] with cosine similarity and head pose estimation model [30] with L2 distance to quantify the gaze and head pose changes. For video-level evaluation, we use Frechet Video Distance (FVD) [34] to assess the overall quality of generated videos. Additionally, we compute the average ranking of each method across all metrics to provide comprehensive assessment of model performance. Implementation Details. We initialize LIVINGSWAP with the 14B pretrained weights of VACE [19]. Specifically, we train the model for 10,000 steps using the AdamW optimizer, with learning rate of 1e-5 and batch size of 16. The input resolution is set to 640, and the number of frames is set to 81, following the original VACE configuration. The final model is trained on 8 NVIDIA H200 GPUs for 14 days. During inference, we first detect faces using face detection model, then crop the detected regions and perform face swapping on the cropped sequences. The swapped regions are subsequently pasted back into their original positions within the frames. As face-swapping model commonly used in industrial scenarios, we employ Inswapper [15] as Per-frame Edit method for processing keyframes. In the ablation study, all models are trained for 2000 steps with the same hyperparameters. Moreover, we select the 100 most challenging cases in FF++ for evaluation, ensuring fair comparison and clearer demonstration of each ablated models effectiveness. 5.2. Comparisons with Existing Methods In this section, we compare our model against several state-of-the-art face-swapping approaches, including SimSwap [4], InfoSwap [11], BlendSwap [32], CanonSwap [24], DiffSwap [49], FaceAdapter [14], our baseline model VACE [19], and the widely used industrial system Inswapper [15], which is also employed for our keyframe generation and dataset construction. As shown in Tab. 2, on FF++which mainly contains relatively simple scenarios such as interviews and livestreamsLivingSwap achieves the best overall ranking. We further select the top-performing five methods on FF++ for additional evaluation on CineFaceBench. As shown in Tab. 1, LivingSwap achieves state-of-the-art performance across multiple metrics and average ranks on both the easy and hard identity settings of our CineFaceBench, demonstrating strong robustness under varying identity similarity levels. Compared to our keyframe generation system, Inswapper, although our keyframes are generated from its outputs, our model exhibits significantly superior temporal consistency, better preservation of source-video attributes, and more stable face-swapping behavior in difficult scenarios such as side views and occlusions (see Fig. 4). This also indicates that our approach is robust to imperfect or problematic keyframes, as further discussed in the Sec. C. 5.3. Ablation Studies We conduct ablation studies on synthetic data quality, model design, keyframe quality, identity difference, and source video variation, with the results of the latter three experiments provided in the Supplementary Material. Ablation of Synthetic Data Quality. As discussed in Sec. 4.4, we construct paired dataset, Face2Face, using state-of-the-art single-frame face swapping model [15] to investigate the impact of per-frame errors on data quality. In challenging scenarios, such models often introduce flickering artifacts, identity inconsistencies, and various visual Figure 5. Visualization of the Face2Face dataset. The central plot shows the distribution of identity similarity scores between each swapped video and its corresponding original video, with the lowest 30% (red) and highest 30% (blue) highlighted. Low-similarity pairs often contain artifacts and distortions as significant identity discrepancies (left), while high-similarity pairs may contain failed swap frames, causing identity inconsistencies and flickering (right). Table 3. Ablation on generated data quality. Results show that using the full dataset achieves better fidelity (e.g., lighting, pose) due to greater sample diversity and demonstrate the models robustness to failed noisy train data (also demonstrated in Fig. 6). Table 4. Ablation of key components. Our method achieves more balanced trade-off between id preservation and fidelity. Additionally, the w/o Target Image variant remains viable option in scenarios where identity similarity is less critical. Methods ID Sim. Expr. Light Gaze Pose Methods ID Sim. Expr. Light Gaze Pose LivingSwap VACE Using Upper Data Using Lower Data 0.536 0.313 0.532 0. 2.84 3.08 2.82 2.83 0.285 0.451 0.355 0.299 0.289 0.484 0.288 0.488 2.84 6.42 2.89 2.87 LivingSwap VACE w/o Target Image w/o Keyframe Inpainting 0.536 0.313 0.515 0.281 0.519 2.84 3.08 2.74 2.47 2. 0.285 0.451 0.355 0.299 0.279 0.537 0.249 0.502 0.292 0.491 2.84 6.42 2.80 2.84 2.87 distortions (see Fig. 5). On the other hand, these challenging cases also enhance the diversity of the dataset. To this end, we conduct detailed analysis of how the distribution of simple versus complex scenarios affects overall data quality. Specifically, we filter the data pairs based on identity variation between the original and synthetic videos and categorize them into three groups for comparison: the entire dataset, the first 70% of the data, and the last 70% of the data. Experimental results presented in Tab. 3 show that neither ID-similar data nor data with significant identity discrepancies substantially improves the models idenInstead, we observe that using the full tity performance. dataset leads to better results in terms of lighting, pose. This suggests that the diversity of samples within the full dataset plays an indispensable role in enhancing model performance. Meanwhile, the results also demonstrate the models robustness to noisy training data, enabling it to ultimately go beyond the limitations of training data quality (Sec. A). Therefore, in the subsequent experiments, we adopt all available data pairs for model training. Ablation of Model Design. We conduct ablation studies on three key components of our model design: video reference, keyframe guidance, and target image reference. As shown in Tab. 4, when we replace the video reference with the traditional inpainting approach, the model exhibits decline in fidelity metrics. Regarding identity injection, when we remove the keyframe guidance and rely solely on the target image, we observe significant drop in identity similarity as well as noticeable degradation in temporal consistency. Conversely, when we ablate the identity information provided by the target image, we still observe decline in identity similarity. This is due to the limitations of keyframes in certain scenariossuch as occlusion, extreme angles, or closed eyeswhich may result in the loss of critical identity features. 6. Conclusion that the first video This work presented LIVINGSWAP, reference-guided face swapping model leverages keyframes as conditioning signals to enhance both fidelity and temporal coherence in video face swapping. By combining keyframe conditioning with video reference guidance, our approach ensures stable identity preservation and high-fidelity reconstruction across long video sequences. We propose novel paired dataset, Face2Face, along with role-reversing strategy that provides reliable ground-truth supervision and tackles the challenge of scarce data for reference-guided training. Extensive experiments demonstrate that LIVINGSWAP seamlessly integrating target identities with source video attributes such as expressions, lighting, and exhibiting strong performance in complex face-swapping scenarios. Our model significantly reduces manual effort in production workflows, enabling more efficient and flexible video editing in film and entertainment."
        },
        {
            "title": "References",
            "content": "[1] Ahmed Abdelrahman, Thorsten Hempel, Aly Khalifa, Ayoub Al-Hamadi, and Laslo Dinges. L2cs-net: Fine-grained gaze estimation in unconstrained environments. In 2023 8th International Conference on Frontiers of Signal Processing (ICFSP), pages 98102. IEEE, 2023. 7 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 7 [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 3 [4] Renwang Chen, Xuanhong Chen, Bingbing Ni, and Yanhao Ge. Simswap: An efficient framework for high fidelity face In Proceedings of the 28th ACM International swapping. Conference on Multimedia, 2020. 1, 3, 6, 7 [5] Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, and Chengjie Wang. Hifivfs: High fidelity video face swapping. arXiv preprint arXiv:2411.18293, 2024. 2, 3, 6, 7, 9 [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3 [7] DeepFakes. https://github.com/deepfakes/faceswap. Accessed: 2020-12-20, 2020. 1, 3 [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 3 [9] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019. 7 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [11] Gege Gao, Huaibo Huang, Chaoyou Fu, Zhaoyang Li, and Ran He. Information bottleneck disentanglement for identity swapping. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 7 [12] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 2, 3 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Chenlin Meng, Omer Bar-Tal, Shuangrui Ding, Maneesh Agrawala, Dahua Lin, and Bo Dai. Keyframe-guided creative video inpainting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1300913020, 2025. [14] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face adapter for pre-trained diffusion models with fine-grained id and attribute control. arXiv preprint arXiv:2405.12970, 2024. 2, 3, 7, 9 [15] Ruhs Henry. Facefusion: Industry leading face manipulation platform, 2025. Accessed: 2025-09-23. 4, 5, 6, 7, 2 [16] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23282337, 2023. 2, 3 [17] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image arXiv preprint animation with environment affordance. arXiv:2502.06145, 2025. 2, 3 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3, 4, 5, 7 [20] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. 2, 3 [21] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 2, 3 [22] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77527762, 2025. 7 [23] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high fidelity and occlusion aware face swapping. arXiv preprint arXiv:1912.13457, 2019. 1, 3 [24] Xiangyang Luo, Ye Zhu, Yunfei Liu, Lijian Lin, Cong Wan, Zijian Cai, Shao-Lun Huang, and Yu Li. Canonswap: Highfidelity and consistent video face swapping via canonical space modulation. arXiv preprint arXiv:2507.02691, 2025. 1, 3, 7 [25] Pexels. Pexels: Free stock photos and videos, 2025. Accessed: 2025-11-20. [26] Pixabay. Pixabay: Free images and videos, 2025. Accessed: 2025-11-20. 7 [27] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2, 3 [28] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. 2, 3 [29] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE International Conference on Computer Vision, 2019. 6 [30] Nataniel Ruiz, Eunji Chong, and James Rehg. Finegrained head pose estimation without keypoints. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2018. [31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [32] Kaede Shiohara, Xingchao Yang, and Takafumi Taketomi. Blendface: Re-designing identity encoders for faceIn Proceedings of the IEEE/CVF International swapping. Conference on Computer Vision, 2023. 1, 3, 7 [33] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video obIn Proceedings ject insertion with precise motion control. of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 2, 3 [34] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [35] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018. 7 [36] Runqi Wang, Sijie Xu, Tianyao He, Yang Chen, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, and Yao Hu. Dynamicface: High-quality and consistent video face swaparXiv preprint ping using composable 3d facial priors. arXiv:2501.08553, 2025. 2, 3, 6 [37] Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. Framer: Interactive frame interpolation. arXiv preprint arXiv:2410.18978, 2024. [38] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 2, 3 [39] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchIn Proceedings of mark for video face super-resolution. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 6, 5 [40] Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, et al. Lora-composer: Leveraging low-rank adaptation for multi-concept customization in training-free diffusion models. arXiv preprint arXiv:2403.11627, 2024. 3 [41] Yang Yang, Siming Zheng, Jinwei Chen, Boxi Wu, Xiaofei He, Deng Cai, Bo Li, and Peng-Tao Jiang. Any-to-bokeh: One-step video bokeh via multi-plane image guided diffusion. arXiv preprint arXiv:2505.21593, 2025. 3 [42] Zhen Yang, Ganggui Ding, Wen Wang, Hao Chen, Bohan Zhuang, and Chunhua Shen. Object-aware inverarXiv preprint sion and reassembly for image editing. arXiv:2310.12149, 2023. 2 [43] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. 5 [44] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation In Proceednetwork for real-time semantic segmentation. ings of the European conference on computer vision (ECCV), 2018. [45] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 6, 5 [46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 5 [47] Canyu Zhao, Yanlong Sun, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, and Chunhua Shen. Diception: generalist diffusion model for visual perceptual tasks, 2025. 5 [48] Ruisi Zhao, Zechuan Zhang, Zongxin Yang, and Yi Yang. 3d object manipulation in single image using generative models. arXiv preprint arXiv:2501.12935, 2025. 3 [49] Wenliang Zhao, Yongming Rao, Weikang Shi, Zuyan Liu, Jie Zhou, and Jiwen Lu. Diffswap: High-fidelity and controllable face swapping via 3d-aware masked diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 3, 7 [50] Yibo Zhao, Liang Peng, Yang Yang, Zekai Luo, Hengjia Li, Yao Chen, Zheng Yang, Xiaofei He, Wei Zhao, Qinglin Lu, et al. Local conditional controlling for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1049210500, 2025. [51] Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, and Chunhua Shen. Unleashing the potential of the diffusion model in few-shot semantic segmentation. Advances in Neural Information Processing Systems, 37:4267242695, 2024. 3 Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality"
        },
        {
            "title": "Supplementary Material",
            "content": "Considering the space constraints of the main paper, this supplementary material provides additional experimental results and presents the construction details of Face2Face and CineFaceBench. The content is organized as follows: Sec. A: Generalization Beyond Train Data Quality. Sec. B: Keyframe Identity Injection for Accumulated Identity Errors. Sec. C: Robustness to Keyframe Quality. Sec. D: Robustness to Identity Differences. Sec. E: Robustness to Attribute Variations in Source Video. Sec. F: Grayscale Keyframe Guidance for Robust Color Learning. Sec. G: Face2Face Construction Details. Sec. H: CineFaceBench Construction Details. Sec. I: Comparison with Close-Source Methods. A. Generalization Beyond Train Data Quality In Sec. 5.3, we analyzed the robustness of LIVINGSWAP under varying levels of data quality, demonstrating the of our model to failed noisy train data. To further investigate whether LIVINGSWAP is fundamentally constrained by the quality of the training dataset itself, we conduct an additional experiment. We manually select several noisy sourcetarget pairs from Face2Face. These pairs contain various types of degradation, including local failure cases caused by failed swaps (e.g., residual beards), artifacts and misaligned expressions arising from large identity gaps or occlusions, as shown in Fig. 5. We then run LIVINGSWAP on these noisy pairs using the exact same inference process as Inswapper, which was used to construct the dataset. As illustrated in Fig. 6, LIVINGSWAP consistently surpasses the quality of the original dataset pairs, producing results with improved expression alignment, visual realism, and significantly fewer local artifacts. We attribute this improvement to two key design choices. (1) Reversing the role of data when constructing training pairs, which ensures reliable ground-truth supervision even when the original swaps contain noise. (2) Strong priors in the pretrained model, which enable the system to robustly correct misaligned or corrupted supervision. Together, these factors allow LIVINGSWAP to generalize beyond the limitations of the training data and deliver high-quality, noise-resistant face swapping results. B. Keyframe Identity Injection for Accumulated Identity Errors As demonstrated in Sec. 5.3, keyframes play crucial role in maintaining identity consistency within the video reference paradigm. The design of keyframes not only mitigates the interference caused by the source videos identity but also plays pivotal role in resolving accumulated ID errors. As shown in Fig. 7, when using only the first frame as guidance and combining it with temporal stitching (as detailed in Sec. 4.3) for long video generation, ID errors gradually accumulate as the video progresses, eventually leading to significant deviations from the target face. In contrast, by injecting keyframe identities alongside temporal stitching, our method ensures smooth connection with the previous video chunk while also providing correct ID guidance at the end of each chunk, preventing the accumulation of errors. This entire process is illustrated in the temporal stitching part of Fig. 3. C. Robustness to Keyframe Quality To examine the sensitivity of LIVINGSWAP to keyframe quality, we employ various image-level face swapping models as the Per-frame Edit module. As shown in Fig. 8, LIVINGSWAP demonstrates strong robustness against degraded or inconsistent keyframes. Even when the injected keyframes contain artifacts, expression misalignment, our method still produces results that remain well aligned with the source video and perceptually more faithful. We attribute this robustness to two key factors: (1) Directly referencing the source video, which enables the model to correct erroneous keyframe guidance by restoring the appropriate visual attributes; and (2) The strong generative prior of diffusion models, which further enforces temporal realism and semantic consistency throughout the video. D. Robustness to Identity Differences For the scenario of swapping different identities for the same source video, we conducted experiments with multiple videos and identities. As shown in Fig. 9, leveraging the advantages of keyframe identity injection, LIVINGSWAP achieves satisfactory results for the same video, regardless of whether the identity difference is large or small. We hypothesize that this robust of identity difference is due to the diversity of identities in our training data, as discussed in Sec. 5.3. Figure 6. Qualitative comparison between the data pairs in Face2Face (by Inswapper [15]) and corresponding results generated by LIVINGSWAP. Benefiting from reversing the role in data pair and strong priors in pretrained model, LIVINGSWAP surpasses the quality of its training data, achieving better expression consistency and overall realism. Unlike Inswapper-based results, our method avoids local failure casessuch as incomplete swaps, mismatched regions, and occlusion-induced artifactsdemonstrating its strong generalization beyond the training dataset. E. Robustness to Attribute Variations in"
        },
        {
            "title": "Source Video",
            "content": "outputs even when the keyframe model produces suboptimal results. To verify whether our reference-based video face swapping approach is robust to attribute variations in the source video, we selected diverse set of videos as source inputs and conducted experiments using the same target identity. As shown in Fig. 10, our model consistently produces highquality results across attributes in challenging scenarios, such as occlusions, side profiles, and complex lighting conditions. Furthermore, owing to the robustness of keyframe quality, our model is able to generate realistic, high-fidelity F. Grayscale Keyframe Guidance for Robust"
        },
        {
            "title": "Color Learning",
            "content": "As shown in Fig. 12, we observe that grayscale keyframe guidance significantly reduces color bleeding and flickering artifacts in challenging cases where the keyframe edits exhibit inconsistent or unrealistic colors, while maintaining comparable identity preservation and temporal consistency. In the main paper, LIVINGSWAP relies on RGB Figure 7. Keyframes Identity Injection for resolving Accumulated ID Errors. Keyframe Identity Injection for Resolving Accumulated ID Errors. When using only the first frame for ID injection, face-swapping results suffer from gradually accumulating ID errors over time. In contrast, with Keyframe Identity Injection, each video chunk is corrected individually by swapped keyframe, ensuring better ID consistency throughout the entire long video sequence. keyframes as temporal anchors for identity injection  (Fig. 3)  . While this design is effective for propagating identity information, we observe failure mode when the perframe edited keyframes contain imperfect color statistics, e.g., incorrect skin tone or illumination caused by upstream editing tools, as shown in Fig. 12. Because the keyframe tokens are directly concatenated with the video tokens, such color biases can be mistakenly treated as strong supervision signal, leading the diffusion model to reproduce the wrong colors in all synthesized frames. To mitigate this issue, we introduce simple yet effective modification: grayscale keyframe guidance. As shown in Fig. 11, given an edited keyframe, we convert it to singlechannel luminance image, and then replicate this channel to form three-channel grayscale keyframe before feeding it into the VAE encoder. The rest of the pipeline, including token concatenation and the DiT-based video generation, remains unchanged. This modification removes explicit chromatic information from the keyframe while preserving high-frequency structural cues such as facial identity, hairstyle, and local shading. Intuitively, grayscale keyframes encourage the model to use the keyframe primarily as structural and temporal anchor for stable identity injection, and to recover color statistics from the reference video branch in the Video Reference Completion module for fidelity. As result, LIVINGSWAP Figure 8. Qualitative comparison of using different image-level face swapping models as Per-frame Edit module. Injected keyframes often exhibit flaws including artifacts and expression misalignment. In contrast, by directly referencing the source video, LIVINGSWAP successfully refines these flaws using the corresponding source attributes, demonstrating strong robustness to imperfect or corrupted keyframes. Figure 9. Identity swapping results on the same source video with different target identities. Our method produces consistent and highfidelity face swaps regardless of large or small identity differences, demonstrating strong robustness to identity variations. becomes less sensitive to color artifacts in the keyframe edits. We finetune final checkpoint of LivingSwap in 5000 steps to adapt it to grayscale pipeline. As shown in Fig. 12, we observe that grayscale keyframe guidance significantly reduces color bleeding and flickering artifacts in challenging cases where the keyframe edits exhibit inconsistent or unrealistic colors, while maintaining comparable identity preservation and temporal consistency. G. Face2Face Construction Details We construct our dataset Face2FaceSwap based on CelebVText [45] and VFHQ [39]. First, we perform crop, resize, and clipping operations on the dataset to ensure the resolution is 640640 pixels and the video length is approximately 200 frames. We then randomly pair the data and extract the first frame from the target video as the target face image. Next, we apply Inswapper [15] to perform face-swapping on the entire dataset. The process is conducted using 8 NVIDIA H100 GPUs over duration of 120 hours. Additionally, we use the face-parsing model [44] to generate the face mask video. For the ablation study on the inpainting paradigm, we also use the pose estimation model [43] to generate the corresponding pose video. After filtering out the failed samples from the processing steps, our dataset Face2Face contains total of 152,221 video samples, with cumulative duration exceeding 300 hours. Finally, we reverse the roles in each training pair: the swapped video is used as the source video (model input), while the original video serves as the target video (ground-truth supervision). Figure 10. Face swapping results on diverse source videos with the same target identity. Our method consistently preserves target identity and produces high-fidelity outputs across challenging conditions, including occlusions, side profiles, and complex lighting. Figure 11. Grayscale keyframe guidance. To avoid incorrect color propagation from imperfect edited keyframes, we modify Video Reference Completion module and convert each keyframe to grayscale image before VAE encoding. This preserves structural cues (identity, pose, shading) while removing misleading chromatic information, allowing the model to recover accurate colors from the reference video. H. CineFaceBench Construction Details Due to the fact that most scenes in FaceForensics++ [29] consist of relatively simple settings such as interviews, hosts, or live broadcasts, it does not evaluate critical aspects often required in film scenarios, such as the models ability to preserve facial expressions, lighting, makeup, and overall fidelity after face-swapping, as well as the stability of long video clips longer than 30 seconds. To address the limitations of the aforementioned benchmark, we have Figure 12. Compared with the original LIVINGSWAP, using grayscale keyframes effectively suppresses color bleeding (e.g., the blue tint near the ear in the first example) and reduces temporal flickering artifacts (e.g., the dark patches on the head in the second example), leading to more stable and faithful video face swapping results. constructed film scene face-swapping benchmark, CineFaceBench. CineFaceBench consists of 200 video clips, paired with easy and hard target face images, resulting in 400 data pairs. Specifically, we downloaded and selected 100 video clips from two free video websites, Pixabay [26] and Pexels [25]. Additionally, we selected 100 video clips from the OpenHumanVid dataset [22] and preprocessed them, resulting in 200 video samples used for evaluation. As shown in Fig. 13, these 200 clips include challenging examples from film scenes, featuring difficult scenarios such as unique lighting, exaggerated expressions, micro-expressions, special makeup, occlusions, and even facial deformations. In addition, there are several video cases that are longer than 1 minute. On the other hand, we randomly selected 1,000 faces from the FFHQ dataset as target face images. By calculating the ID similarity (refer to Sec. 5.1) between these faces and the source video, we selected two samples with the most similar and least similar IDs to the source video, representing the easy and hard cases, respectively. This setup allows for better evaluation of the models robustness to ID differences. As shown in the Tab. 1, Fig. 4 and Fig. 13, our model demonstrates impressive performance in the challenging film scene scenarios. I. Comparison with Close-Source Methods Recently, several inpainting-based video face swapping methods using the Stable Video Diffusion model [2] are proposed, such as HiFiVFS [5] and FaceAdapter [14]. However, these methods are not open-source. To enable comparison with them, we captured several demos from their project websites and conducted tests using the same target face image. The comparative results are shown in the Fig. 14. Our approach better preserves the original video attributes such as lighting and expression, and also demonstrates strong stability in occluded cases. Figure 13. Additional Qualitative Comparison of Different Methods on CineFaceBench. LIVINGSWAP produces results with higher fidelity and realism compared to other methods. Figure 14. Qualitative comparison with recent inpainting-based video face swapping methods [5, 14] shows that our approach better preserves source video attributes (e.g., lighting and expression) and achieves greater stability under occlusions."
        }
    ],
    "affiliations": [
        "Zhejiang University",
        "Zhejiang University of Technology"
    ]
}