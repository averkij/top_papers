{
    "paper_title": "How to Teach Large Multimodal Models New Skills",
    "authors": [
        "Zhen Zhu",
        "Yiming Gong",
        "Yao Xiao",
        "Yaoyao Liu",
        "Derek Hoiem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent \"forgetting\" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 6 5 8 0 . 0 1 5 2 : r HOW TO TEACH LARGE MULTIMODAL MODELS NEW SKILLS? Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu & Derek Hoiem University of Illinois Urbana-Champaign Champaign, IL, USA {zhenzhu4,yimingg8,yaox11,lyy,dhoiem}@illinois.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent forgetting on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to measurable shift in the output token distribution, manifested through simple counting-bias probe that identifies the shift co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL.git."
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: Surprising Forgetting Behavior in LMMs: Left: When fine-tuning most components on one target task, we see major improvement in that task (Learning) but substantial drop in performance of other tasks (Forgetting, total across tasks shown here), as expected. But if we only tune self-attention projection layers (SA Proj.) in the language model, we still get substantial learning on the target task with minimal forgetting. Right: Even fine-tuning SA Proj. for multiple tasks sequentially, we see no forgetting. For others, we see large forgetting on the PixmoCount task, but the models somehow partly recover what they \"forgot\" in learning the next specialized task. Our paper documents and analyzes these and other interesting phenomena of learning and forgetting in LMMs, leading to simple and effective ways to teach LMMs new skills. Large multimodal models (LMMs), such as LLaVA (Liu et al., 2023b) and Qwen2.5-VL (Bai et al., 2025), are trained to generate natural language answers based on image(s) and natural language instruction. As such, these models can perform wide range of tasks. However, for many special domains, such as medical images, or skills, such as counting, the models do not perform as desired. How can we teach LMMs something new without degrading existing capabilities? Work done in UIUC 1 Training new LMM can cost millions of dollars, weeks of time, and emit hundreds of tons of CO2, so finding ways to more efficiently and effectively update existing models is pressing concern. One option is to simply fine-tune the model on the new task. However, at least for simpler models, fine-tuning is known to cause catastrophic forgetting, such that model previously proficient on many tasks becomes narrow expert on the new one. more reliable method is to completely retrain the model with an expanded training set, but this becomes increasingly impractical as the scale of training data continues to climb. Intuitively, an intelligent system should be able to add to its knowledge without repeating all of its learning. LMMs are sometimes trained in single epoch (Li et al., 2024c) raising pressing question: do LMMs suffer catastrophic forgetting? Recent works (Chen et al., 2024a; Yu et al., 2024; Zhu et al., 2024a) conclude yes, but our findings are more nuanced. We study continual learning in LMMs using controlled evaluation program. The target suite contains five practical skills that span different answer formats (fine-grained bird classification, counting, medical VQA, OCR reading, and time reading). The held-out suite contains eight widely used benchmarks for general visionlanguage ability. We evaluate learning as improvement on the target tasks and forgetting as the average drop on held-out tasks. Our first goal is to identify tunable parts that deliver high target performance with minimal forgetting. We compare full-model fine-tuning to tuning each major component (vision encoder, projector, LLM) and then open the LLM into its two essential blocksself-attention projections (SA Proj.) and the feed-forward network (MLP). Early experiments on LLaVA-OneVision  (Fig. 1)  reveal two surprising results: 1) tuning SA Proj. learns with little or no measurable forgetting across five-task sequence; and 2) what appears forgotten after one stage can be recovered by tuning another specialized task. These results lead us to ponder: why is SA Proj. so robust to forgetting, and how is forgotten knowledge recovered without rehearsing? Consider the roles of the two essential components in the transformer decoder: self-attention projection is data processing, applying an algorithm to the inputs (Elhage et al., 2021; Olsson et al., 2022), while MLPs perform external memory look up and produce the output distribution (Geva et al., 2021). We thus hypothesize that perhaps what looks like forgetting or interference after fine-tuning on narrow target task is actually bias in the output distribution due to the task distribution shift. Through in-depth analysis when tuning the counting task, we confirm this hypothesis: tuning the MLP increases target accuracy but also increases the likelihood of outputting numeric tokens and highly correlated drop in held-out task accuracy, while tuning the self-attention achieves the target learning without much bias toward numeric tokens and without losing held-out accuracy (Sec. 5.2). Guided by this result, we explore tuning recipes that preserve learning while limiting output shift. To avoid biasing the output distribution, we tune the MLP up/gating projections while keeping the down projection frozen, and find that it achieves similar learning to full MLP tuning with little forgetting. We experiment on LLaVA-OneVision (Li et al., 2024c) by training five target tasks sequentially, averaging over three sequence orders, measuring the learning and forgetting in target tasks and held-out tasks (Sec. 5.1). We then confirm that similar trends hold for LLaVA-NeXT (Li et al., 2024b) and Qwen2.5-VL (Bai et al., 2025) (Sec. 5.3). In summary, our work documents and analyzes several interesting phenomena of learning and forgetting in LMMs, leading to simple and effective ways to teach LMMs new tricks. The findings are: Tuning the LLM ( learning +31.8/ forgetting -23.3) is critical for learning new tasks, while tuning the vision encoder (+9.6/-10.8) brings little gain and harms general ability. Tuning only the self-attention projection weights (+24.9/-0.6) or the up layers of the MLP (+30.5/-2.1) provides excellent learning with limited forgetting, evaluated on five-target task sequence, eight held-out benchmarks, and three model families. Forgetting is largely manifestation of output distribution shift. We use simple countingbias probe to show that the rise in number-token likelihood grows with MLP tuning and remains near baseline for self-attention tuning; the magnitude of this shift co-varies with held-out drops. Therefore, forgetting can be recovered when subsequent tuning shifts back the output distribution, and methods that limit shift effectively mitigate forgetting, such as distillation to the previous checkpoint or freezing the MLP down projection while tuning the up&gate."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Continual learning for traditional vision. Continual learning, also known as lifelong learning (Aljundi et al., 2017; Chen & Liu, 2018; Chaudhry et al., 2019), aims to train models on sequence of tasks or data streams without forgetting previously acquired knowledge. Traditionally, it is mainly explored in closed-vocabulary image classification, and can be categorized into three main types: (1) regularization-based methods try to preserve the knowledge captured in previous version of the model by matching logits (Li & Hoiem, 2017; Rebuffi et al., 2017), feature maps (Douillard et al., 2020), or other information (Tao et al., 2020; Wang et al., 2022; Simon et al., 2021; Joseph et al., 2022; PourKeshavarzi et al., 2022; Liu et al., 2023c) in the new model; (2) exemplar replay methods build reservoir of samples from old training rounds (Prabhu et al., 2020; Liu et al., 2024b; Luo et al., 2023b; Liu et al., 2020; Rebuffi et al., 2017; Shin et al., 2017; Bang et al., 2021) and replay them in successive training phases as way of recalling past knowledge; and (3) network-architecture-based methods (Liu et al., 2021; Wang et al., 2022) expand the network capacity for new target data and freeze some network parameters to retain original knowledge. Recently, several studies (Jin et al., 2022; Khattak et al., 2023a;b; Smith et al., 2023) show prompt tuning as an effective strategy for continual learning, which freezes all weights but adds learnable prompts to optimize for new tasks. Vision-text contrastive models, such as CLIP (Radford et al., 2021), are trained to align images and texts for open-vocabulary image classification and retrieval. Pretrained CLIP models may outperform on fine-grained or specialized tasks (Radford et al., 2021; Zhu et al., 2024c), pressing the need for reliable continual learning approaches. Zhu et al. (2024c;b) propose learning to blend predictions from original and tuned image encoders, enabling fast online learning without forgetting for open vocabulary classification. Yu et al. (2024) adds parameter-efficient adapters to mixture-of-experts on frozen CLIP model to prevent forgetting. Zhou et al. (2025) design task-specific projection layers and cross-modal fusion modules for vision-language models in class-incremental learning. Liu et al. (2025) incorporate continual low-rank adaptation and knowledge consolidation to prevent forgetting. Zheng et al. (2023) use knowledge distillation (Li & Hoiem, 2017) on CLIP to maintain zero-shot performance. Large language models (LLMs). Studies of LLMs evaluate the learning-forgetting trade-off across various fine-tuning strategies on LLMs with billions of parameters, including full-model tuning, adapters, LoRA, and prompt tuning. Luo et al. (2023a) find that decoder-only models are more robust than encoder-decoder models. Lin et al. (2024) observe that models fine-tuned for narrow domains lose ability on general tasks, but method like weight interpolation (WiSE-FT) (Wortsman et al., 2022) helps maintain balance. Biderman et al. (2024) show that LoRA reduces learning and forgetting, compared to full fine-tuning. Li et al. (2025) propose dual-memory replay framework with interpolated LoRA. Huang et al. (2024) generate pseudo-data from the model itself to mitigate forgetting without requiring original training data. Xiang et al. (2023) propose to use regularization strategies such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and hierarchical importance-based penalties to preserve general knowledge by constraining updates to important parameters. Wang et al. (2023) propose learning orthogonal LoRA weights for new tasks to mitigate forgetting. Roles of attention and FFN in LLMs. Mechanistic studies of transformer blocks show division of labor. Attention heads act primarily as routing and retrieval mechanisms: they select where to read from using querykey patterns and then mix the corresponding values; this view is formalized in the Transformer Circuits framework and supported by analyses of induction heads, which implement simple copying algorithm and closely track the emergence of in-context learning during training (Olsson et al., 2022). In contrast, feed-forward (FFN/MLP) blocks behave like keyvalue memories: learned keys detect input patterns while values write features that align with groups of vocabulary items, thereby shifting the models output preferences (Geva et al., 2021). Meng et al. (2022) show that directly modifying MLP weights updates specific facts while preserving unrelated behavior, implying FFN as principal site where what to say is stored. Earlier analyses in BERT and NMT also found that minority of attention heads specialize into linguistically interpretable roles (e.g., syntax, coreference) while many heads are prunable with little loss, reinforcing the view of attention as selective routing rather than the main repository of lexical knowledge (Clark et al., 2019; Voita et al., 2019). This literature aligns with our empirical finding that self-attention updates tend to preserve global behavior while MLP updates are the main driver of output-distribution shift. 3 Figure 2: Architecture of our evaluated LMMs. The input contains visual inputs such as images or videos, which are converted to visual tokens by the vision encoder, and text input is processed by tokenizer containing visual placeholder token <image>. Visual tokens are converted by the projector and concatenated with text tokens as input for the language model. We visualize the architecture of the transformer decoder layer of the language model. \"LN\", \"MHA\", \"MLP\" represent layer norm, multi-head attention, and multi-layer perceptron, respectively. r(l) is the final output of layer l. Large multimodal models (LMMs). Relatively little work has investigated continual learning in LMMs, but there is growing interest. Chen et al. (2024a) find that LMMs suffer from catastrophic forgetting when learning sequence of new tasks. Most studies of LMMs more narrowly focus on visual question answering (VQA) (Zhang et al., 2023; Nikandrou et al., 2024; Lin et al., 2025; Marouf et al., 2025; Del Chiaro et al., 2020) or image captioning (Nguyen et al., 2019). Zhang et al. (2023) leverage both sample-specific and sample-invariant features to learn representations that are both discriminative and generalizable for VQA tasks. Nikandrou et al. (2024) propose to distill knowledge separately for each modality, ensuring that both image features and question features retain their relevant information when new tasks arrive. Lin et al. (2025) combine selective memory replay and knowledge distillation for VQA. Marouf et al. (2025) store only past questions from previous tasks as memory for rehearsal. For older models, Nguyen et al. (2019) integrate continual learning techniques, such as finetuning schemas and regularization, into the captioning pipeline to combat forgetting, and Del Chiaro et al. (2020) introduce an attention-based LSTM architecture. Our work complements these studies in several ways: (1) more diverse tasks, such as counting, clock reading, classification, OCR, and medical VQA, finding large differences in the extent of learning and forgetting in each and that what is forgotten by one task can be recovered by learning the next (see Figs. 1 and 5); (2) systematic analysis that forgetting is highly related to output token distribution shift and methods that prevents shifts mitigate forgetting (Sec. 5.2 and 5.4); (3) investigation into tuning different components, finding that tuning MLP Gate&Up and SA Proj. provide good balance of learning and forgetting."
        },
        {
            "title": "3 METHOD",
            "content": "Setting. We adapt pretrained large multimodal model on either single-target task or sequential stream of tasks. In the single-target case, given target dataset DT and held-out suite DH, the goal is to improve performance on DT while preserving performance on DH. In the sequential case, tasks {D(1) } arrive in stages; unless noted we update at each stage without rehearsal (no mixing of earlier tasks) and assess both the current target and the aggregated held-out suite after every stage. , . . . , D(K) 3.1 MODEL Overview of the LMM. Our evaluated LMMs have three major parts  (Fig. 2)  : vision encoder that turns an image into visual tokens, projector that maps those tokens to the language width d, and decoder-only transformer language model that produces next-token logits given the visual and text tokens. Vision encoder and projector. The vision encoder produces = fvis(I) RSvdv . The projector maps to the language representation width, where ψ are the projectors trainable parameters. xvis = gψ(v) RSvd, 4 Language model. The language model is pre-norm, decoder-only transformer with identical blocks. As illustrated in Fig. 2, the sublayer outputs and residual update at block are a(l) = MHA(l)(cid:0)LN(r(l1))(cid:1), (l) = MLP(l)(cid:0)LN(r(l1) + a(l))(cid:1), (1) where MHA and MLP denote the multi-head self-attention and feed-forward network and LN is layer normalization. r(l) = r(l1) + a(l) + (l), Self-attention. With input and per-head key width dk, {Q, K, } = {WQ, WK, WV } attention = softmax (cid:16) QK dk (cid:17) value mix + WO MHA(x) = (AV ) WO (2) Here WQ, WK set where to attend (routing), WV selects the content to mix, and WO is the matrix that writes the attention result back into the residual stream at model width d. Feed-forward. With input and gating nonlinearity ϕ = SiLU, MLP(x) = Wdown(ϕ(xWgate) (xWup)) , (3) where Wgate, Wup detect features (key-like pattern match), and Wdown writes those features back to the residual at width d. We use RdV for the LM head and denote the final block output as r(L), so logits are = r(L). Residual stream. Let xtext RStd be the text embeddings and xvis RSvd the projected visual tokens. The transformer input stacks them along the sequence: r(0) = (cid:2) xtext; xvis Unrolling the pre-norm recurrence over l=1:L yields the representation read by the LM head, (cid:3). r(L) = r(0) + (cid:88) a(l) + (cid:88) (l). (4) l=1 l=1 Eq. 4 shows the additive influences of attention and MLP outputs, but it does not imply disentangled influences: each a(l) and (l) is function of the shared stream r(l1), so changing self-attention alters the inputs that later MLPs receive (and vice versa). Combined with Eqs. 23, we can change what concepts are activated by modifying the self-attention (since it feeds into the MLP) or the MLP up/gate layers, and we can change what to write given the activated concepts by changing the down layers. WO determines the output of the attention layers, but the change in final model output is most influenced by the outputs of the final MLP layers, whichever parameters are tuned (Appendix Fig. 10). 3.2 WHICH PARTS TO TUNE? At the system level, one can update the vision encoder and the projector (which change r(0)), or the language model (which produces the layerwise increments that accumulate into r(L)). Because = r(L) and the sublayers are coupled through the residual, we focus on controlled updates inside the LM that probe routing versus writing without altering the readout: we keep the LM head , token embeddings, and layer-norm parameters fixed by default. Guided by Eqs. 23, we consider: SA Proj.: Update WQ, WK, WV , WO in all blocks (routing + write-back for attention). SA Proj. (QKV): Freeze WO to emphasize routing without directly modifying write-back. MLP: Update Wgate, Wup, Wdown (concept activation + write-back). MLP (Gate & Up): Update Wgate, Wup while freezing Wdown to regulate write-back. 3.3 TRAINING OBJECTIVE Target task loss. We use next-token cross-entropy on the current target dataset with teacher forcing. For batch D(k) at stage k, Ltask(θ) = E(I,y)B log pθ (cid:0)yt y<t, xvis (cid:1) , (cid:88) t=1 5 xvis = gψ(fvis(I)) . (5) Learning-without-Forgetting (optional). To explicitly curb the output-distribution drift, we can enforce KL-divergence constraint between the outputs of the current model at stage with frozen teacher model (checkpoint after stage k1). Let θk1 be the frozen teacher and θ the current model tuned on D(k) . The objective is L(θ) = Ltask(θ) + λ Ldistill(θ; θk1), with Ldistill(θ; θk1) = (I,y) τ 2 S(y) (cid:88) (cid:16) KL jS(y) softmax(cid:0) zθk1 ,j τ (cid:1) (cid:13) (cid:13) softmax(cid:0) zθ,j τ (6) (7) (cid:1)(cid:17) , where D(k) is target minibatch, τ is the distillation temperature, and S(y) is uniformly random subset of positions with S(y) = min(y, 1000) so we distill over many tokens while capping compute/memory. The coefficient λ balances fitting the new supervision against preserving the models earlier behavior."
        },
        {
            "title": "4 EXPERIMENT DESIGN",
            "content": "Our experiments are designed to answer four questions: (i) Where to tune?which components of an LMM can be updated to learn new skills while preserving prior abilities (Sec. 5.1); Why does forgetting occur?whether performance loss is tied to shift in the models output distribution (Sec. 5.2); (iii) How generalizable is our selective tuning strategywhether the same selective-tuning recipes (SA Proj., MLP Gate&Up) transfer across model families (Sec. 5.3); and (iv) How does our selective tuning compare to other simple forgetting mitigation approaches? (Sec. 5.4). Due to space limits, we provide more details about tasks and implementation in Appendix and B. 4.1 TASKS AND EVALUATION SUITES Target tasks. Our criteria for target task selection are: (1) prefer tasks that occur in daily experience, like counting and reading clocks preferred; (2) prefer tasks that LMMs are known to be typically weak, such as fine-grained species classification (Liu et al., 2024a); and (3) exclude tasks used to train the original LLaVA-OneVision model. Based on this, we select 5 target tasks and create default sequential-tuning task curriculum: 1. Bird classification from the CUB dataset (Wah et al., 2011) which contains 5,994 training samples. We reformat the dataset following the instructions of (Liu et al., 2024a) for training and evaluating LMMs. 2. Counting from the PixmoCount dataset (Deitke et al., 2025) which contains 36,140 training samples with object count labels. 3. Medical VQA from the PathVQA dataset (He et al., 2020) which contains 19,654 radiology question answer pairs. 4. OCR reading from the TextVQA dataset (Singh et al., 2019) which has 34,602 training samples. 5. Time reading from the TimeClock dataset (Gpiosenka) containing 11,520 training images of analogue clocks with ground truth times. In total, the curriculum contains 107,910 training samples, providing comprehensive stress test for forgetting and knowledge transfer. Held-out suite. To measure generalization beyond the training stream, we evaluate on eight held-out benchmarks: AI2D (Kembhavi et al., 2016), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022), RealWorldQA (visheratin, 2024), SeedBench (Li et al., 2023), ScienceQA (Lu et al., 2022), and MMStar (Chen et al., 2024b). InfoVQA and DocVQA use ANLS; since ANLS is in [0, 1], we average it with accuracies from the other held-out tasks when reporting the mean held-out score. 6 Table 1: Effect of tuning different components: learning, forgetting, and overall performance, averaged over three five-task sequences. Cells are colored using blue-orange colormap to show performance changes. Blue indicates positive change, where darker shade is better. Orange indicates negative change, where lighter shade is better. We underline numbers that do not reflect significantly different task-average distribution from the best, based on two-sided paired sample t-test. Method Baseline Full - Vision Encoder - Projector - LLM - SA Proj. - SA Proj. (QKV) - MLP - MLP (Gate&Up) Target Learning Target Forgetting Target Overall Held-out Forgetting 43. +29.9 +9.6 +2.3 +31.8 +24.9 +14.9 +31.1 +30.5 0.0 25.9 12.7 0.8 23.5 2.3 0.5 19.5 4.2 43.9 +9.2 0.5 +1.7 +13.0 +23.1 +14.5 +15.5 +27.1 76. 27.4 10.8 1.3 23.3 0.6 +0.2 15.7 2.1 4.2 SEQUENCE-LEVEL METRICS We summarize performance over the five-stage curriculum with four metrics computed for every method. Target Learning. At each stage, consider only the task being tuned and measure its improvement over the base model on that task. We then average these stage-wise gains across all stages. This captures how well method learns the task it is currently trained on. Target Forgetting. To measure forgetting on target tasks trained earlier in the sequence, we report the average difference between their accuracy immediately after they were trained and their accuracy at the end of the sequence. More negative means more forgetting. Target Overall. After training the full sequence, we compute the average performance change vs. the base model across all target tasks. This yields the net end-of-sequence effect on the target suite, combining the learned task and the previously learned targets. Held-out Forgetting. After training the full sequence of target tasks, we measure the average performance across all eight held-out benchmarks, in comparison to the base model. Negative values indicate forgetting on general visionlanguage ability; positive values indicate positive transfer."
        },
        {
            "title": "5 RESULTS",
            "content": "5.1 COMPONENT TUNING ON LLAVA-ONEVISION Fig. 1 previews learning (singletask tuning, left) and forgetting (held-out along the default sequence, right). Tab. 1 summarizes the four sequence-level metrics on LLaVA-OneVision for each component-tuning configuration, averaged over three five-task curricula. Entries are percentage-point deltas from the base model; the baseline row reports absolute scores. For each column, we run paired sample t-tests on the per-sequence/per-task averages to test whether tuning different components leads to significantly different per-task learning, forgetting, or overall performance. We underline numbers that are not significantly different (p > 0.1) than the best. Detailed single-task and sequential results, together with per-task performance tables, are provided in the Appendix. From the table, we find the following patterns: 1) Full-model tuning attains large learning but maximizes forgetting: Target Learning +29.9 is coupled with the worst Target/Held-out Forgetting (25.9/27.4). 2) Vision-side updates are weak or near-neutral: the vision encoder yields modest Target Learning +9.6 with negative Target Overall 0.5 and Held-out Forgetting 10.8; projector-only updates barely move any metric (smallest changes overall). 3) Language-model tuning has the best learning: LLM shows the strongest Target Learning +31.8 and solid Target Overall +13.0, but also substantial Target/Held-out Forgetting (23.5/23.3), even better than the Full model. 4) Self-attention projection is the most stable among LLM choices: SA Proj. achieves high Target Overall +23.1 with minimal forgetting (Target 2.3, Held-out 0.6); the conservative variant without WO further reduces forgetting (Target 0.5, Held-out +0.2) at the cost of learning (+14.5 Target Overall), indicating over-regularization when the attention write-back is frozen. 5) Regulating the MLP write-back offers the best balance: MLP (Gate&Up) delivers near-maximal Target Learning +30.5 and the highest Target Overall +27.1 while keeping forgetting small (Target 4.2, Held-out 2.1); by contrast, full-MLP pushes learning slightly higher on the current task +31.1 but increases forgetting (Target 19.5, Held-out 15.7). Overall, methods that mainly reroute evidence such as SA Proj. and SA Proj. (QKV), or constrain how activated concepts are written back, such as MLP Gate&Up, provide the most favorable learningstability trade-off on LLaVA-OneVision."
        },
        {
            "title": "5.2 OUTPUT-DISTRIBUTION PROBE (COUNTING BIAS)",
            "content": "Figure 3: Learningforgetting tracks outputdistribution shift. On LLaVA-OneVision tuned for counting, we plot five curves over log-spaced steps for LLM, SA Proj., MLP, MLP (Gate&Up) and MLP (LwF). The dashed line represents the base model. Left: PixmoCount accuracy rises for all methods. Middle: mean held-out performance drops sharply for LLM and MLP, remains nearly unchanged for SA Proj., and is preserved by MLP (LwF); Right: the expected likelihood of number tokens on non-counting captions (LCS-558K (Liu et al., 2023a)) surges for LLM and MLP, stays near baseline for SA Proj., and has little changes for MLP (LwF). To test whether forgetting is tied to global shift in token preferences, we track simple number-token bias (NTB) during counting adaptation: as training proceeds, how does the likelihood of outputting numeric token change for task that does not require counting? Let be fixed subset of vocabulary items (digits and common spelled numerals). For fixed held-out batch = {(I, y)} and token subset (digits and spelled numerals), at training step we generate ˆy with greedy decoding and, at each step j, read the next-token distribution before committing the token and take the maximum probability over C. Averaging first over positions and then over the batch gives NTBs = 1 (cid:88) (I,y)B 1 ˆy ˆy (cid:88) j=1 (cid:0)v ˆy<j, xvis pθs (cid:1) . max vC We plot absolute NTBs on log-spaced grid of steps (1, 10, 100, 1000) for SA Proj., MLP, MLP (Gate&Up), MLP (LwF), and full LLM. As seen in Fig. 3, full LLM/MLP sharply increase NTBs and their held-out accuracy drops in tandem; SA Proj. keeps NTBs near the baseline with an essentially flat held-out curve; constraining the MLP write-back (tuning only Gate&Up) or distilling to the baseline checkpoint (LwF) suppresses the rise in NTBs and correspondingly preserves held-out performance. In our setting, forgetting rises and falls with the magnitude of this shift: updates that mainly reroute evidence (SA Proj.) or regulate write-back (Gate&Up, LwF) learn the new skill while keeping drift small and thus interference small. We also create an analysis of per-layer contribution of SA Proj. and MLP to the output distribution shift in the Appendix ( Fig. 10), which shows MLP drives the major shift, regardless of what gets tuned. 8 Figure 4: Visualizations on counting and captioning examples after tuning tuning MLP and SA Proj. on the counting task. The counting and examples are sampled from the PixmoCount dataset and the LCS-558K (Liu et al., 2023a) dataset, respectively. Qualitative results on counting and captioning. Fig. 4 contrasts the output on counting and captioning tasks for models that have tuned MLP or SA Proj weights for 1K steps on PixmoCount. Both answer counting questions correctly, but the model with SA Proj tuning retains detailed captioning ability, while the MLP tuning leads to reframing captioning as counting statements, such as There are 2 photos in the photo (third row). This pattern suggests that MLP tuning does not erase visual understanding (objects and relations are still recognized) but biases the output distribution. 5.3 BEYOND LLAVA-ONEVISION: GENERALIZATION TO OTHER BACKBONES We repeat the default five-task curriculum on two additional backbones LLaVA-NeXT (LLaMA-3 8B) and Qwen2.5-VL (7B), using the same training protocol and sequence-level metrics as for LLaVA-OneVision. For vision-side updates, we tune the vision encoder and projector jointly, since they form single interface that produces the visual token sequence consumed by the language model. Across both backbones, the broad picture echoes LLaVA-OneVision: updating the language model is consistently effective for learning new skills; full-model and full-LLM tuning achieve large target task gains but come with the largest drops on held-out. Within the LM, two settings stand out as robust: self-attention projections deliver meaningful target learning with small held-out change, and MLP (Gate&Up) preserves most of the learning of full-MLP while limiting forgetting. There are, however, model-specific nuances worth noting. On LLaVA-NeXT, MLP achieves the strongest target-overall improvement but incurs noticeably larger held-out decrease than SA Proj. or Gate&Up, which remain the most stable choices. LLaVA-NeXT is much more susceptible to forgetting in general than the other models. On Qwen2.5-VL, SA Proj. is particularly stable: held-out performance is 9 Table 2: Component-level tuning experiments with LLaVA-NeXT and Qwen2.5-VL. \"T\" represents \"Target\" and \"H\" is for \"Held-out\". Underlined text of each column represents the best method in the corresponding metric. Method Baseline LLaVA-NeXT (LLaMA-3 8B) Qwen2.5-VL (7B) T. Learn T. Forget T. Overall H. Forget T. Learn T. Forget T. Overall H. Forget 31.5 0.0 31.5 59.9 52. 0.0 52.1 77.9 Full - Vision + Projector - LLM +31.7 20.3 1.8 +0.1 +36.2 21.2 - SA Proj. +28.3 7.9 - MLP +34.9 10.3 - MLP (Wgate, Wup) +28.0 8. +15.4 1.3 +19.3 +21.9 +26.6 +20.9 32.0 +17.3 5.2 13.4 +12.1 9.1 35.9 +16.8 5.9 7.7 +16.1 1.6 16.3 +17.7 4.8 +16.8 +0.4 8.7 +13.1 +4.9 +12.1 +14.9 +13.9 +17.1 17.5 6.2 24.6 +0.6 10.9 4.6 maintained or slightly improved; MLP (Gate&Up) attains the best target-overall score with near-zero target forgetting; vision + projector tuning also yields non-trivial target gains with moderate stability cost, in contrast to its weaker effect on LLaVA-NeXT. Though Qwen2.5-VL appears to learn less, it is worth noting that its baseline performance is much higher than that of other models. Overall, taking LLaVA-OneVision, LLaVA-NeXT, and Qwen2.5-VL together, the clearest cross-model takeaway is to prefer SA Proj. when stability on held-out is paramount and MLP (Gate&Up) when seeking near-maximal target learning with limited forgetting; projector-only updates are generally weak, and full-model / full-MLP tuning maximizes short-term gains at clear stability cost. 5.4 MITIGATING FORGETTING Figure 5: Comparison of different continual learning techniques in the default sequential task curriculum. For LwF, WiSE-FT, only the MLP layers are tuned. LoRA adapters are wrapped only on the MLP layers. MoE is also applied to the MLP layers. Figure 5 compares three selective tuning recipes, i.e. MLP, SA Proj. and MLP (Gate&Up), with common forgetting mitigation methods: Learning without Forgetting (LwF) (Li & Hoiem, 2017), LoRA, Mixture-of-Experts (MoE), and weight-space ensembling (WiSE-FT) (Wortsman et al., 2022). Refer to the Appendix Sec. for details of these methods. Two patterns emerge. First, SA Proj. and MLP (Gate&Up) provide the best learningstability trade-off: SA Proj. keeps held-out performance essentially flat while achieving meaningful target gains, and MLP (Gate&Up) delivers stronger target improvements with only small held-out change, being substantially more stable than MLP. Second, among the compared methods, WiSE-FT can preserve held-out accuracy better than LwF but requires careful selection of task-dependent blending coefficients; LwF reliably curbs forgetting yet may impact target task gains; MoE and LoRA do not match the learningstability balance of SA Proj. or MLP (Gate&Up), with LoRA often lagging behind on target performance. Overall, selectively tuning SA Proj. or the MLP Gate&Up pair matches or exceeds these mitigation methods while remaining simple (no extra modules, no replay, no per-stage weight blending)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We sought to answer how to teach large multimodal models new skills without erasing prior abilities, and studied this across five target skills, eight held-out benchmarks, and three backbones. Our results show that the apparent loss on held-out tasks after narrow fine-tuning is often temporary: performance that drops at one stage can recover later. We trace this behavior to measurable shift in the next-token distribution rather than the loss of concepts. simple counting-bias probe makes this drift visible, and layer-wise residual-to-logit analysis shows that most of the shift is written by late MLP blocks, not by self-attention. Guided by this, we find that tuning only the self-attention projection layers or only the MLP Gate&Up layers limits the bias, leading to good learning with limited forgetting across model families. Thus, our study helps to understand the learning and forgetting behavior of LMMs, and our recommendations, to limit which components are tuned, are broadly applicable. We hope this work leads to more stable and efficient continuous improvement of LMMs, reducing the environmental and financial cost of model adaptation. Limitations. Due to limited resources, we must leave exploration of many interesting aspects to future work, such as alternative architectures and longer sequences. Also, testing with much larger models, and additional modalities, such as audio, requires further study. Broader issues, such as privacy leakage, safety, and societal impact, remain open for future investigation."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work is supported in part by ONR award N00014-23-1-2383, and U.S. DARPA ECOLE Program No. #HR00112390060. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, ONR, or the U.S. Government."
        },
        {
            "title": "REFERENCES",
            "content": "Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with network of experts. In CVPR, pp. 33663375, 2017. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923, 2025. Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with memory of diverse samples. In CVPR, pp. 82188227, 2021. Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John Patrick Cunningham. LoRA learns less and forgets less. TMLR, 2024. Arslan Chaudhry, MarcAurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with A-GEM. In ICLR, 2019. Cheng Chen, Junchen Zhu, Xu Luo, Hengtao Shen, Jingkuan Song, and Lianli Gao. CoIN: Benchmark of Continual Instruction Tuning for Multimodel Large Language Models. In NeurIPS, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large visionlanguage models? In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), NIPS, 2024b. Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3):1207, 2018. 11 Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris CallisonBurch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In CVPR, pp. 91104, 2025. Riccardo Del Chiaro, Bartłomiej Twardowski, Andrew Bagdanov, and Joost Van de Weijer. Ratt: Recurrent attention to transient tasks for continual image captioning. In NeurIPS, volume 33, pp. 1673616748, 2020. Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In ECCV, pp. 86102, 2020. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), EMNLP, pp. 54845495, 2021. Gpiosenka. TIME - Image Dataset-Classification. https://www.kaggle.com/datasets/ gpiosenka/time-image-datasetclassification. Xuehai He, Yichen Zhang, Luntian Mou, Eric P. Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. CoRR, abs/2003.10286, 2020. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), ACL, pp. 14161428, 2024. Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), ACL, pp. 27632775, 2022. K. J. Joseph, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, and Vineeth Balasubramanian. Energy-based latent aligner for incremental learning. In CVPR, pp. 74527461, 2022. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), ECCV, volume 9908, pp. 235251. Springer, 2016. Muhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz, Salman H. Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In CVPR, 2023a. Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In ICCV, 2023b. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):35213526, 2017. 12 Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. instruction tuning beyond data?, May 2024a. URL https://llava-vl.github.io/blog/ 2024-05-25-llava-next-ablations/. Llava-next: What else influences visual Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024b. URL https://llava-vl.github.io/blog/ 2024-05-10-llava-next-stronger-llms/. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy Visual Task Transfer. arXiv preprint arXiv:2408.03326, 2024c. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. Xinlong Li, Weijieying Ren, Wei Qin, Lei Wang, Tianxiang Zhao, and Richang Hong. Analyzing and reducing catastrophic forgetting in parameter efficient tuning. In ICASSP, pp. 15. IEEE, 2025. Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 2017. Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. Mitigating the Alignment Tax of RLHF. In EMNLP, 2024. Yuxin Lin, Mengshi Qi, Liang Liu, and Huadong Ma. Vlm-assisted continual learning for visual question answering in self-driving. arXiv preprint arXiv:2502.00843, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36: 3489234916, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In NeurIPS, 2023b. Huan Liu, Lingyu Xiao, Jiangjiang Liu, Xiaofan Li, Ze Feng, Sen Yang, and Jingdong Wang. Revisiting mllms: An in-depth analysis of image classification abilities. arXiv preprint arXiv:2412.16418, 2024a. Wenzhuo Liu, Fei Zhu, Longhui Wei, and Qi Tian. C-clip: Multimodal continual learning for vision-language model. In ICLR, 2025. Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun. Mnemonics training: Multi-class incremental learning without forgetting. In CVPR, pp. 1224512254, 2020. Yaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive aggregation networks for class-incremental learning. In CVPR, pp. 25442553, 2021. Yaoyao Liu, Yingying Li, Bernt Schiele, and Qianru Sun. Online hyperparameter optimization for class-incremental learning. In AAAI, 2023c. Yaoyao Liu, Yingying Li, Bernt Schiele, and Qianru Sun. Wakening past concepts without past data: Class-incremental learning from online placebos. In WACV, pp. 22262235, 2024b. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), NIPS, 2022. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023a. 13 Zilin Luo, Yaoyao Liu, Bernt Schiele, and Qianru Sun. Class-incremental exemplar compression for class-incremental learning. In CVPR, pp. 1137111380, 2023b. Imad Eddine Marouf, Enzo Tartaglione, Stephane Lathuiliere, and Joost van de Weijer. No images, no problem: Retaining knowledge in continual vqa with questions-only memory. arXiv preprint arXiv:2502.04469, 2025. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), ACL, pp. 22632279, 2022. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for VQA on document images. In WACV, pp. 21992208. IEEE, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. Infographicvqa. In WACV, pp. 25822591. IEEE, 2022. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. NeurIPS, 36, 2022. arXiv:2202.05262, doi:10.48550/arXiv.2202.05262. Giang Nguyen, Tae Joon Jun, Trung Tran, Tolcha Yalew, and Daeyoung Kim. Contcap: scalable framework for continual image captioning. arXiv preprint arXiv:1909.08745, 2019. Malvina Nikandrou, Georgios Pantazopoulos, Ioannis Konstas, and Alessandro Suglia. Enhancing continual learning in visual question answering with modality-aware feature distillation. arXiv preprint arXiv:2406.19297, 2024. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Mozhgan PourKeshavarzi, Guoying Zhao, and Mohammad Sabokrou. Looking back on learned experiences for class/task incremental learning. In ICLR, 2022. Ameya Prabhu, Philip HS Torr, and Puneet Dokania. GDumb: simple approach that questions our progress in continual learning. In ECCV, pp. 524540, 2020. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph Lampert. iCARL: Incremental classifier and representation learning. In CVPR, 2017. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259. Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In NeurIPS, pp. 29902999, 2017. Christian Simon, Piotr Koniusz, and Mehrtash Harandi. On learning the geodesic path for incremental learning. In CVPR, pp. 15911600, 2021. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In CVPR, pp. 83178326, 2019. James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In CVPR, 2023. Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei, and Yihong Gong. Topology-preserving class-incremental learning. In ECCV, pp. 254270, 2020. 14 Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In CVPR, pp. 87698778, 2018. visheratin. RealWorldQA: Benchmark for real-world spatial understanding, 2024. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head selfattention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL, pp. 57975808, 2019. doi: 10.18653/v1/P19-1580. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. In ECCV, 2022. Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. Orthogonal subspace learning for language model continual learning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of EMNLP, 2023. Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei Lü, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Liang Zeng, Xiaokun Wang, Yutuan Ma, Rui Hu, Shuicheng Yan, Han Fang, and Yahui Zhou. Skywork-moe: deep dive into training techniques for mixture-of-experts language models. arXiv preprint arXiv:2406.06563, 2024. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In CVPR, 2022. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. In NeurIPS, volume 36, pp. 7539275412, 2023. Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters. In CVPR, 2024. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models. arXiv preprint arXiv:2407.12772, 2024. Xi Zhang, Feifei Zhang, and Changsheng Xu. Vqacl: novel visual question answering continual learning setting. In CVPR, pp. 1910219112, 2023. Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, and Yang You. Preventing zero-shot transfer degradation in continual learning of vision-language models. In CVPR, pp. 1912519136, 2023. Da-Wei Zhou, Yuanhan Zhang, Yan Wang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Learning without forgetting for vision-language models. TPAMI, 2025. Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Chao Wu, and Kun Kuang. Model tailor: Mitigating catastrophic forgetting in multi-modal large language models. In ICML, 2024a. Zhen Zhu, Yiming Gong, and Derek Hoiem. Anytime Continual Learning for Open Vocabulary Classification. In ECCV, volume 15064, 2024b. Zhen Zhu, Weijie Lyu, Yao Xiao, and Derek Hoiem. Continual learning in open-vocabulary classification with complementary memory systems. Trans. Mach. Learn. Res., 2024, 2024c."
        },
        {
            "title": "CONTENTS",
            "content": "A Task details A.1 On building target tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Task curriculum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation details B.1 Implementation details for LLaVA-OneVision . . . . . . . . . . . . . . . . . . . . B.2 Parameter count for LLaVA-OneVision . . . . . . . . . . . . . . . . . . . . . . . B.3 Implementation details for Qwen2.5-VL and LLaVA-NeXT (LLaMA 3) . . . . . . B.4 Implementation details for Sec. 5.2: Output-distribution probe (counting bias) . . . B.5 Numerical token list used for counting bias probe . . . . . . . . . . . . . . . . . . Discovery Process Use of Large Language Models (LLMs) More results E.1 Single task fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Sequential fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More analysis F.1 Composing stable tuning strategies: SA Proj. + MLP Gate&Up . . . . . . . . . . . F.2 Layer-wise residualtologit contribution analysis . . . . . . . . . . . . . . . . . . Forgetting mitigation approach descriptions Detailed task performance H.1 Forgetting mitigation methods sequential tuning tables . . . . . . . . . . . . . . . H.2 Sequential tuning detailed performance tables on LLaVA-OneVision . . . . . . . . H.3 Sequential tuning detailed performance tables on LLaVA-NeXT (LLaMA 3) . . . . H.4 Sequential tuning detailed performance tables on Qwen2.5-VL . . . . . . . . . . . 16 16 17 17 18 18 18 19 20 21 21 21 22 23 24 25 25"
        },
        {
            "title": "A TASK DETAILS",
            "content": "A.1 ON BUILDING TARGET TASKS Our training and evaluation scripts are built on the LLaVA-NeXT and lmms-eval public GitHub repositories. Since some of the target tasks are not supported by lmms-eval, we need to implement support for evaluation of target tasks. Details are as follows. Bird classification. We reformulate the bird classification dataset CUB200 (Wah et al., 2011) to multiple choice VQA task following (Liu et al., 2024a). Specifically, for <image> and <class 16 name> pair, we mix the correct label with 31 randomly chosen labels from the whole dataset and then compose question like: <image> What species is the bird in this photo? Answer with the options letter from the given choices directly. A.<class name A> B.<class name> Z.<class name Z> This task has 5,794 validation samples. The instruction prompt for this task is: Answer with the options letter from the given choices directly. And only exact match can be deemed as correct by lowercasing models output and compared to lowercased ground-truth answer. Counting. The original PixmoCount dataset provides download links rather than actual image files. By the time of downloading, not all links are valid. In the end, besides 36,140 training samples, we collected 535 and 536 validation and test samples. We use the validation set to report numbers in the paper, as done in the technical report of Pixmo dataset (Deitke et al., 2025). The instruction prompt for this task is Answer with integer and nothing else. For example, if the answer is 1, you should output 1.. We convert the output by the model to digits and then use exact match to compute accuracy. Medical VQA. We use the test split of the PathVQA dataset for evaluation, containing 6,719 samples. The instruction prompt for this task is For questions that can be answered with yes or no, just answer yes or no. Otherwise, provide an answer in the medical domain. We use the exact match score as the metric for this task, using the official evaluation algorithm. OCR reading. lmms-eval has support for TextVQA evaluation and we use the accuracy on the validation set as the performance for this task. Time reading. We evaluate on the validation split of the TimeClock dataset, which contains 1,440 samples. The instruction prompt is Answer with the time in HH:MM format. For example, if it is 3:45, output 3:45. To compute accuracy, we parse the models output to extract the hour and minute. prediction is marked correct only when both values match the ground truth. A.2 TASK CURRICULUM We provide three task sequences for sequential-tuning: 1. CUB200 PixmoCount PathVQA TextVQA TimeClock 2. PathVQA CUB200 TextVQA TimeClock PixmoCount 3. TimeClock TextVQA PathVQA PixmoCount CUB200 Unless otherwise stated, we use the first as the default sequential-tuning sequence."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 IMPLEMENTATION DETAILS FOR LLAVA-ONEVISION We adopt the 7B Qwen2 language model checkpoint for experiments on LLaVA-OneVision. Experiments run primarily on 4NVIDIA H100 GPUs with 1 sample per GPU. We use 8 gradient-accumulation steps (effective batch 32), learning rate of 5 106 with cosine decay, and 3% warm-up. Following the practice from (Li et al., 2024a), we use smaller learning rate at 2106 when tuning the vision encoder and the projector. Evaluation uses lmms-eval (Zhang et al., 2024) with added support for our targets. When we activate LwF, we set λ = 1 and τ = 2. For all the experiments, we perform single epoch training. 17 Table 3: Parameter groups and counts for LLaVA OneVision Qwen2-7B Group Components ΘVE SigLIP vision encoder ΘProj Multimodal projector ΘSA ΘMLP All blocks: WQ, WK , WV , WO All blocks: Wgate, Wup, Wdown #Params 400 20 822 5,703 Input token embeddings LM head ΘEmb ΘLM SigLIP So400M vision backbone, about 400M parameters. OneVision Stage 1 projector is about 20M parameters for the 7B class. Per layer counts with d=3584, dkv=512, L=28: SA 29,364,736, MLP up 135,790,592, MLP 203,685,888; totals multiply by L. Vocab size 152,128 and width d=3584 give 152,128 3584 = 545,226,752 parameters for embeddings and for the LM head (untied). 545 545 B.2 PARAMETER COUNT FOR LLAVA-ONEVISION Tab. 3 lists the parameter groups and counts of each part in the LLaVA-OneVision model. It can be seen that the language model takes large part of the total capacity of the model. Within the language model, MLP is the major consumer of parameters. B. IMPLEMENTATION DETAILS FOR QWEN2.5-VL AND LLAVA-NEXT (LLAMA 3) We adopt the 7B Qwen2.5 checkpoint for experiments on Qwen2.5-VL. For all experiments on Qwen2.5-VL, we use 4 H100 GPUs and set learning rate at 2e-5 for all components in the model. Per-GPU batch size is set to 4, with 4 gradient accumulation steps. Therefore, the effective batch size is 64. For experiments on LLaVA-NeXT (LLaMA-3), we use the 8B LLaMA-3 checkpoint. We adopt the same learning rate, warm-up ratio, batch size, gradient accumulation steps as tuning LLaVAOneVision. For all the relevant experiments, we perform single epoch training. B.4 IMPLEMENTATION DETAILS FOR SEC. 5.2: OUTPUT-DISTRIBUTION PROBE (COUNTING BIAS) Setup. Fix token subset (digits and common spelled numerals; exact list in the repo) and held-out batch ˆB = {(I, y)} of ˆB = 100 imagecaption pairs sampled once from LCS-558K (reused for all checkpoints and methods). For each checkpoint and each (I, y) ˆB, generate caption ˆy with deterministic greedy decoding using identical preprocessing and decoding settings across methods. Per-position score. At generation step j, before committing the token, read the models next-token probabilities and compute the per-position number tendency (cid:0)v (cid:12) pmax (θs; I, ˆy, j) = max vC (cid:12) ˆy<j, xvis pθs (cid:1). Per-example and batch aggregates. Summarize each example by the sequence average SeqAvgC(θs; I) = 1 ˆy ˆy (cid:88) j=1 pmax (θs; I, ˆy, j), and aggregate over the batch to obtain the number-token bias at checkpoint s: NTBs = 1 ˆB (cid:88) (I,y) ˆB SeqAvgC(θs; I). Table 4: Numeric token indices and their corresponding tokens. Index / Token Index / Token Index / Token Index / Token 15 19 0 4 8 16 20 24 1 5 9 3966 One Zero 17999 three 27856 Six 41460 Eight 59085 Twenty 75796 nine 93223 5225 ONE Three 19641 Ten 32687 six 50364 Seven 59528 seven 80185 twenty 93965 2 17 6 21 one 603 Two 11613 two 19789 four 34024 five 52670 67532 eight 83329 Nine 3 7 ten zero Four Five 18 22 1960 14154 26972 37020 58313 million ZERO 73956 Thirty 91602 B.5 NUMERICAL TOKEN LIST USED FOR COUNTING BIAS PROBE In Tab. 4, we list the total 38 numeric tokens we used for counting bias probe, and their indices in the tokenizer. The numeric tokens include numeric digits and words such as \"one\", \"ONE\", etc."
        },
        {
            "title": "C DISCOVERY PROCESS",
            "content": "In science, the ordering of observation, hypotheses, experimental results, and explanation is important to know whether claims are post-hoc rationalization of results or experiments are confirmation of hypotheses that were based on prior observations. Therefore, we wish to give full accounting. In beginning this research, we first sought to verify the problem of catastrophic forgetting in LMMs. While prior works had largely confirmed the forgetting, these works tending to involve limited range of tasks, so we created diverse set of target tasks, some of which we expected to be very hard for the LLaVA-OneVision model (e.g. counting and telling time), and others to be easy (e.g. bird identification and TextVQA). After confirming that typical tuning practices, such as tuning the vision component, LLM, or full model, led to substantial forgetting, we thought we would turn to mitigation strategies, such as experience replay, model expansion with mixture-of-experts, knowledge distillation, and weight averaging. We also noticed surprising result, that the model performance would drop significantly in held out benchmarks after training on the counting task, it would mostly recover on PathVQA, another specialized task that is not well represented in the benchmarks. Meanwhile, while performing the forgetting mitigation experiments, we also tried separately tuning only the self-attention projection (SA Proj) or MLP layers, motivated by the finding that tuning only the LLM was generally better than tuning the full model. This led to another very surprising result that tuning only self-attention projection layers led to very good learning of the target tasks with no drop in performance in held out tasks, even after training all five target tasks in sequence. This was surprising because we were not aware of other instances of strong learning without forgetting behavior, in the absence of model expansion, rehearsal, or strong regularization. third interesting result was that knowledge distillation turned out to be the most effective method for mitigating forgetting that we tried, outperforming e.g. replay of examples from earlier target tasks and mixture-of-experts scheme for model expansion. Initially, we sought to stress test these results. Indeed, we found that if we keep training new tasks, such as the large long-tailed task of iNaturalist (Van Horn et al., 2018) classification, we see little bit of forgetting. Also, in tuning other models, LLaVA-NeXT and Qwen2.5-VL  (Table 2)  , we do not see exactly the same numbers, of course, but the major trends hold. With Qwen2.5-VL, we actually get little forward transfer on the held-out tasks when tuning SA Proj. With LLaVA-Next, we get 7.7 point drop in held out tasks, but less than half the forgetting of tuning MLP layers and less than one-quarter as much as tuning the full LLM. We also tried other sequences of target task training and found that the post-counting recovery of forgetting was not fluke. For example, we see recovery from both PixmoCount and TimeClock when reversing the sequence order. By-and-large, the results held fine-tuning self-attention is remarkably robust to forgetting, what was forgotten can be recovered without rehearsal, and regularizing the outputs with knowledge distillation also is highly effective in mitigating forgetting when tuning the MLP. 19 We performed many other experiments, but our breakthrough in understanding came from reviewing the literature, particularly in work, such as Geva et al. (Geva et al., 2021) and Olsson et al. (Olsson et al., 2022), that experimentally explore the roles of transformer components. Their key results are that MLPs are responsible for storing and applying memories, with the up layer(s) looking up the memories (or activating concepts) and the down layers applying the activated concepts to modify the output token distribution. Attention, on the other hand, is responsible for processing and organizing the inputs. This led us to consider that model can adapt to task in many ways: acquiring skills to make better use of its inputs, acquiring new memories and concepts, better applying those concepts, or simply biasing toward the output distribution. We hypothesized that, when training the full LLM, the model is at least partially taking shortcut to bias toward the output distribution, rather than focusing on skill or memory improvement. This hypothesis could explain all three observed phenomena. The SA Proj is robust to forgetting because it does not directly tune the MLP layers that produce the output distribution. The forgetting is sometimes recoverable because subsequent training on task with more varied outputs reverses the narrow output distribution shift. Knowledge distillation directly penalizes shift in the output distribution. This led us to propose two experiments to test this hypothesis. First, we reasoned, we should see that, as the counting task is trained, the model becomes more predisposed to output numbers, since the counting task answers are always of the form, There are [number] [object(s)] in this image. We also should see some correlation between this bias toward numeric tokens and forgetting in held-out benchmarks. As we show in Fig. 3 and exemplify in Fig. 4, the results are quite striking with strong effect of the output distribution bias and strong correlation with forgetting. Second, we proposed, tuning the MLP except for the down layers that most directly modify the output distribution should mitigate the output bias and, therefore, reduce forgetting. Again, the confirmation was strong with LLaVA-OneVision, tuning only MLP up layers achieved the best overall target performance with only little more forgetting than tuning SA Proj. We believe our results are conclusive, especially given the observe-hypothesize-test-confirm pattern of our research. Further, in the past few days (at the time of this writing), another paper has come out with related findings. Shenfield et al. (Shenfeld et al., 2025) finds that the amount of forgetting is correlated to distributional shift between the base and tuned model, as measured by KL-divergence, and this explains why on-policy RL training is more robust to forgetting than SFT (supervised fine-tuning). Finally, we would like to stress that our experiments have been more thorough than we can relate in the main text. Just in generating the results of Table 1, we fine-tuned the 7B parameter model on 5 tasks 21 times (3 sequences, 7 components) and evaluated 8 broad benchmarks and 5 target tasks 105 times (after each target task was trained). That is 105 task trainings and 1365 task evaluations. We include many other experimental results in the main text and appendix below. While it is always possible to train more models, more component variations, more mitigation strategies, on more datasets and with more evaluations, we have pushed our resources to the brink and hope that the reader finds our claims sufficiently supported, as we do. USE OF LARGE LANGUAGE MODELS (LLMS) We used general-purpose large language model (LLM) as an assistive tool for writing and editing. Specifically, the LLM helped (i) refine phrasing for the abstract, introduction, method descriptions, and result summaries; (ii) advise on LaTeX syntax and commands, and help address compile errors; and (iii) brainstorm alternative framings and terminology for clarity and coherence. The LLM also proposed suggestions for paper organizations. The LLM did not design or run experiments, collect data, produce numerical results, or generate figures. All experimental protocols, scripts, hyperparameters, and evaluations were implemented by the authors; all numbers and tables in the paper are computed from our own training/evaluation runs. When the LLM proposed text for technical definitions or claims, we verified the statements against our code, logs, and checkpoints and revised as needed. All citations were added and checked by the authors. No private or sensitive data were shared with the LLM beyond draft text and public references. Final responsibility for the content rests with the authors."
        },
        {
            "title": "E MORE RESULTS",
            "content": "E.1 SINGLE TASK FINE-TUNING Table 5: Single task fine-tuning by component. Each individual target task is fine-tuned from the original model, and the performance for that task (Target) and average held-out performance (Held-out) is measured. Each row is for tuning different component or set of components: Proj., Vis. Enc., SA Proj., MLP, LM, Full represent tuning the projector, vision encoder, self-attention projections in the LLM, MLP in LLM, full LLM, and all parameters, respectively. + is an increment over the baseline (original LLaVA-OneVision-7B checkpoint), and - is decrease. CUB200 PixmoCount PathVQA TextVQA TimeClock Average Method Target Held-out Target Held-out Target Held-out Target Held-out Target Held-out Target Held-out Baseline 53. Proj. Vis. Enc. SA Proj. MLP LM Full +5.7 +16.1 +31.8 +36.4 +40.0 +37.0 76.4 -0.0 -0.8 +0.3 +0.3 -0.0 +0.1 52.4 +4.2 +11.6 +15.2 +17.8 +16.3 +19. 76.4 -0.1 -4.7 -0.2 -4.0 -7.7 -9.0 36.3 +0.6 +3.7 +14.4 +26.5 +26.8 +27.4 76.4 -0.1 -2.8 -0.3 -0.4 -0.7 -0. 76.0 +0.5 +1.0 +3.5 +3.8 +3.5 +3.4 76.4 +0.2 -0.7 +0.3 +0.0 -0.7 -0.7 1.1 +0.4 +12.7 +56.0 +73.3 +72.6 +79. 76.4 -0.5 -11.9 -0.1 -3.1 -4.6 -5.4 43.9 +2.3 +9.0 +24.2 +31.6 +31.8 +33.3 76.4 -0.1 -4.2 +0.0 -1.4 -2.8 -3. In Fig. 1, we show the learning and forgetting of tuning different components on one target task at time, and then recording the performance for that target task and the average held-out performance. In Tab. 5 we show the actual performance of each component by taking the delta based on the baseline (original model), ordered from least to most parameters. As general trend, tuning more parameters increases both learning (improvement in target task) and forgetting (decrease in average held-out tasks), with vision encoder and self-attention projection as the notable exceptions. Tuning the full network or only the language model yields the greatest learning (+33.3 and +31.8 percentage points, on average), yet these gains are accompanied by significant forgetting (-3.2 and -2.8 points). Adjusting the MLP of the LLM provides good trade-off, with similar learning (+31.6) and substantially lower forgetting (-1.4). Adjusting only the self-attention projection layers achieves respectable +24.2 in learning and, surprisingly, no measurable forgetting. The vision encoder and the projector offer relatively little gain, and the vision encoder has the most forgetting (-4.2), indicating that tuning the vision features is particularly disruptive. Now, consider the variations by task. There is only weak correlation between the amount learned and forgotten per task. For instance, CUB200 has the second-most learning (after TimeClock) but the least forgetting. Also, some tasks benefit from visual tuning while others do not. Fine-grained bird recognition (CUB200) and medical question answering (PathVQA) benefit almost exclusively from language model updates, gaining +40.0 and +26.8 points, respectively, with little or no benefit from additional vision tuning. Conversely, for PixmoCount and TimeClock, tuning the full model handily outperforms tuning only the LLM portion. E.2 SEQUENTIAL FINE-TUNING In Fig. 6, we display how sequentially tuning different components on the default sequence of all five tasks affects the average performance of all target tasks and held-out tasks. In this case, forgetting in later learning can affect the performance of target tasks learned earlier. Per-task performance is attached in the later sections. Updating the MLP (Gate&Up) gives the best target-task performance overall. Multiple methods have stable results on held-out tasks through out the whole sequence, such as SA Proj., SA Proj. (QKV), and MLP (Gate&Up). Another interesting phenomenon which is also mentioned in the main paper is that held-out performance does not continually drop as more tasks are trained, but rises and falls. For example, training on PixmoCount causes substantial loss in held out performance (0.76 to 0.63 for the full model), but the loss is largely recovered (to 0.74) by training the next task PathVQA. This means in forgetting, much of the information is not permanently lost but temporarily inaccessible. 21 Figure 6: Sequential fine-tuning by component. The target tasks in the x-axis are trained sequentially, from left to right. After training each task, the average performance of all target tasks (left) and all held-out tasks (right) are measured. Each line shows the performance after tuning different component or set of components: LLaVA-OneVision (Full, Vision Encoder, Projector, LLM, SA Proj., SA Proj. (QKV) MLP, MLP (Gate&Up)). The dashed horizontal gray line marks the average performance of the original model. Figure 7: Sequential fine-tuning by component. Tasks are arranged as TimeClock TextVQA PathVQA PixmoCount CUB200. In Figs. 8 and 7, we show the sequential tuning results by component on LLaVA-OneVision in the other two orders. Fig. 7 validates that forgetting recovery is not order-specific: methods that forget significantly on PixmoCount, rebound after tuning on CUB200. Both figures indicate the robustness of SA Proj., SA Proj. (QKV), and MLP (Gate&Up) on held-out tasks as they essentially keep flat throughout. Especially, MLP (Gate&Up) has huge benefit in target learning."
        },
        {
            "title": "F MORE ANALYSIS",
            "content": "F.1 COMPOSING STABLE TUNING STRATEGIES: SA PROJ. + MLP GATE&UP We asked whether combining the two most stable, high-learning settings from the main paper, i.e., SA Proj. and MLP (Gate&Up), has further benefits. We evaluate two compositions: SA Proj. + MLP (Gate&Up) and SA Proj. (QKV only) + MLP (Gate&Up) under the same five-task curriculum, reporting the same sequence-level metrics and counting-bias probe. In aggregate, the composed variants match or slightly underperform the two standalone settings on target learning while keeping held-out changes small; the QKV-only composition is better than the other composition in the held-out performance. But across tasks and checkpoints, neither composition consistently dominates MLP (Gate&Up) alone, indicating that most of the achievable learningstability trade-off is already realized by Gate&Up for LLaVA-OneVision. 22 Figure 8: Sequential fine-tuning by component. Tasks are arranged as PathVQA CUB200 TextVQA TimeClock PixmoCount. Figure 9: Composing stable updates. We compare SA Proj. and MLP (Gate&Up) to two compositions: SA Proj. + Gate&Up and SA Proj. (QKV only) + Gate&Up using the default five-task sequential tuning. F.2 LAYER-WISE RESIDUALTOLOGIT CONTRIBUTION ANALYSIS We quantify where (by depth) and how strongly (by pathway) adaptation perturbs the output distribution by comparing the logit-space effects of self-attention versus MLP residual updates across layers and training steps. We evaluate on fixed, held-out multimodal shard sampled once from LCS-558K and reuse it for all methods and checkpoints; unless noted, statistics are computed under teacher forcing over the assistant answer span (target tokens). For each tuning configuration (SA Proj., MLP, LLM, MLP (Gate&Up), and MLP (LwF)) we compare tuned checkpoints to the frozen stage-0 base model at log-spaced training steps (e.g., 1, 10, 100, 1000), excluding the combined SA Proj. + MLP (Gate&Up) condition. We register forward hooks on every decoder layers self-attention and MLP submodules in both the base and tuned models to capture their residual increments a(l) and (l) at each token j; with the LM head fixed, we form logit-space deltas by projecting the difference of residual contributions through : z(l) SA(j) = U(cid:0)a(l) tuned(j) a(l) base(j)(cid:1), z(l) MLP(j) = U(cid:0)f (l) tuned(j) (l) base(j)(cid:1). For each layer we aggregate token-wise vectors into scalar via the ℓ2 norm and then average across tokens and examples to obtain per-layer logit-space magnitudes: (cid:113) (cid:113) SA(l) = Ej (cid:2)z(l) SA(j)2 2 (cid:3), MLP(l) = Ej (cid:2)z(l) MLP(j)2 2 (cid:3). We report these two curves (dashed blue for self-attention, solid red for MLP) per checkpoint, sharing axes across panels for comparability. Key observations. (1) MLP dominates the shift. Across configurations and steps, MLP curves exceed self-attention curvesoften by >2 in later layersindicating that most logit-space change Figure 10: Layer-wise residual-delta magnitude across training iterations. Each subplot shows the average logit-space residual2 attributable to MLP (solid reds) and self-attention projections (dashed blues) versus transformer layer index. The five method configurations are: SA Proj., MLP, LLM, MLP (Gate & Up), and MLP (LwF). Color shade encodes checkpoint iteration (darker = later). The top-left panel is legend; other panels omit legends for clarity. comes from the MLP pathway. (2) Drift grows with training. For settings that forget (e.g., full LLM or full MLP), per-layer magnitudes increase monotonically with checkpoint step, mirroring the counting-bias rise and held-out decline. (3) Late layers drive the effect. The last 45 transformer layers account for the vast majority of the shift, with the final two layers contributing the largest deltas; early layers remain comparatively stable. (4) Regulating write-back reduces drift. MLP (Gate&Up) and MLP (LwF) substantially shrink late-layer MLP magnitudes relative to full MLP, aligning with their smaller held-out drops. (5) Self-attention changes are smaller and flatter. SA Proj. curves are consistently below the corresponding MLP curves and vary less across steps, indicating weaker and less step-sensitive contribution to the overall distribution shift."
        },
        {
            "title": "G FORGETTING MITIGATION APPROACH DESCRIPTIONS",
            "content": "Low-rank adaptation (LoRA). When memory or compute restricts full fine-tuning, LoRA (Hu et al., 2022) offers lightweight alternative. For weight matrix W0 Rdk, we introduce two trainable low-rank matrices Rrk and Rdr and model the update as = BA. After optimization, the effective weight becomes = W0 + α BA, where α is scalar scaling factor. Because only and are updated, the number of learned parameters per task drops from dk to r(d + k), which is substantial when min(d, k). In the continual-learning setting, we instantiate fresh pair (At, Bt) for each task Tt while keeping the backbone weights frozen. After completing task Tt, we merge the low-rank update into the backbone weight Wt Wt1 + α BtAt and then discard the adapters. This maintains constant parameter footprint across tasks and avoids accumulating growing set of task-specific modules. Weight-space interpolation Weight-space interpolation (Wortsman et al., 2022) forms an implicit ensemble by linearly combining the pretrained/base checkpoint with the fine-tuned checkpoint. Given the base weights Wbase (the original LLaVA-OneVision checkpoint) and the fine-tuned weights after stage t, FT , we build an interpolated model (8) The coefficient β trades off specialization on the current target task (larger β) against retention of general capabilities (smaller β). = (1 β) Wbase + β FT β [0, 1]. , (β) . Unless otherwise noted, optimization for the next stage continues from FT In our sequential setting, we apply Eq. equation 8 after finishing fine-tuning on task Tt and evaluate (β) (not from (β) ) to avoid repeatedly biasing training toward the base weights. We sweep β {0.1, 0.3, 0.5, 0.7, 0.9} and report β=0.3s result for comparison as it leads to better learning and forgetting tradeoff compared to results obtained from other β-s. t Mixture of Experts. We next leverage the Mixture of Experts (MoE) architecture to expand model capacity without overwriting knowledge learned during pretraining (Wei et al., 2024). An MoE layer combines set of specialist networks (experts) {Ei}N i=1 through learnable gating network that produces input dependent weights. The layer output is = (cid:88) i=1 gi(x) Ei(x), typically with sparsity constraint such as top-k gating so that only few experts are active per input. We follow standard practice and replace the feed-forward (MLP) submodule in every transformer decoder block of the language model with an MoE layer. At the start of continual training, each decoder block contains 1) the pretrained expert Ept that stores upstream knowledge and 2) new tuned expert Enew that is copy of Ept. The gating network is linear layer initialized to all zeros, which initially routes the entire token sequence through Ept. During task t, we freeze Ept and update only the parameters of Enew and the gate. Because Ept remains untouched, it acts as safeguard when the tuned expert fails, giving MoE an inherent resistance to forgetting. We repeat this procedure for every new task, always reusing the same pair (Ept, Enew) and thus adding no extra parameters beyond the current tuned expert and gate."
        },
        {
            "title": "H DETAILED TASK PERFORMANCE",
            "content": "H.1 FORGETTING MITIGATION METHODS SEQUENTIAL TUNING TABLES Tab. 7, Tab. 8, Tab. 9, and Tab. 6 are detailed sequential tuning tables for forgetting mitigation approaches tested in the paper, i.e., LoRA, LwF (Li & Hoiem, 2017), WiSE-FT (Wortsman et al., 2022), and MoE. H.2 SEQUENTIAL TUNING DETAILED PERFORMANCE TABLES ON LLAVA-ONEVISION We include the detailed task performances for sequential fine-tuning experiments on LLaVAOneVision here. Tab. 10, Tab. 11, Tab. 12, Tab. 13, Tab. 14, Tab. 15, Tab. 16, and Tab. 17 are detailed performance tables of sequentially fine-tuning the Full model, Vision Encoder, Projector, LLM, SA projection layers in LLM, SA Proj. (QKV), MLP layers in LLM, MLP (Gate&Up), respectively. H.3 SEQUENTIAL TUNING DETAILED PERFORMANCE TABLES ON LLAVA-NEXT (LLAMA 3) We include the detailed task performances for sequential fine-tuning experiments on LLaVA-NeXT (LLaMA 3) here. Tab. 18, Tab. 19, Tab. 20, Tab. 21, Tab. 22, and Tab. 23 are detailed performance tables of sequentially fine-tuning the Full model, Vision Encoder + Projector, LLM, SA projection layers in LLM, MLP layers in LLM, and MLP (Gate&Up), respectively. H.4 SEQUENTIAL TUNING DETAILED PERFORMANCE TABLES ON QWEN2.5-VL We include the detailed task performances for sequential fine-tuning experiments on Qwen2.5-VL here. Tab. 18, Tab. 19, Tab. 20, Tab. 21, Tab. 22, and Tab. 23 are detailed performance tables of 25 Table 6: Detailed performance of using MoE to mitigate forgetting by performing sequential finetuning on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72. 76.4 87.7 53.9 36.2 76.0 1.0 51.0 81.5 80.2 87.2 65.6 62.1 67.7 95.8 72.5 76.6 87.6 65.5 35.4 75.3 1. 53.1 81.2 79.6 85.2 64.0 62.6 62.2 95.9 72.2 75.4 87.4 68.2 61.3 75.3 1.2 58.7 80.5 79.6 85.8 64.8 62.6 63.9 95.7 71. 75.6 87.3 63.5 57.4 78.5 1.2 57.6 81.3 80.0 86.6 66.4 62.5 69.3 96.3 72.4 76.8 87.0 59.9 57.8 76.3 68. 69.8 80.8 76.1 84.5 64.4 61.4 68.0 96.0 72.1 75.4 Table 7: Detailed performance of using LoRA to mitigate forgetting by performing sequential fine-tuning on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72.4 76. 80.6 52.6 36.2 76.0 1.0 49.3 81.9 80.1 87.1 66.1 62.1 67.6 96.0 72.5 76.7 80.0 67.8 35.1 75.2 1.0 51. 80.8 79.2 85.0 63.9 62.2 66.4 95.4 72.2 75.6 78.0 64.2 58.1 75.2 1.0 55.3 79.7 78.1 83.4 62.5 60.5 65.9 95.2 71.9 74. 78.2 62.5 51.6 79.1 1.2 54.5 81.3 79.1 84.4 64.4 61.2 68.8 95.6 72.6 75.9 77.0 63.7 53.3 76.2 33.7 60. 79.7 71.7 74.1 59.3 60.3 66.3 93.5 72.4 72.2 sequentially fine-tuning the Full model, Vision Encoder + Projector, LLM, SA projection layers in LLM, MLP layers in LLM, and MLP (Gate&Up), respectively. 26 Table 8: Detailed performance of using LwF to mitigate forgetting by performing sequential finetuning on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72. 76.4 90.0 53.2 35.9 76.3 1.0 51.3 81.9 79.9 87.1 66.3 62.4 67.8 96.0 72.4 76.7 89.8 67.6 35.0 76.5 1. 54.1 81.6 79.9 86.6 66.0 63.2 59.6 93.0 72.5 75.3 89.6 67.8 61.1 76.6 1.0 59.2 81.6 79.9 86.5 65.8 62.6 64.4 95.8 72. 76.1 89.5 61.6 58.3 80.3 1.4 58.2 81.6 80.2 86.5 66.1 62.0 67.8 96.3 72.5 76.6 89.3 61.0 57.6 79.4 67. 71.0 81.7 78.0 85.6 65.2 61.6 66.9 96.0 72.5 75.9 Table 9: Detailed performance of using WiSE-FT using β = 0.3 to mitigate forgetting by performing sequential fine-tuning on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 89.5 68.0 59.1 76.1 1.0 58.7 81.6 80.2 86.7 65.3 61.6 68.4 96.3 72.6 76. 89.0 68.0 56.3 79.7 1.4 58.9 81.6 80.2 86.4 65.9 62.3 69.7 96.4 72.6 76.9 89.0 66.1 56.1 78.3 62.2 70. 81.8 78.3 85.1 64.1 60.9 67.8 96.3 72.6 75.9 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72.4 76. 89.9 53.7 35.9 76.4 1.0 51.4 81.8 80.1 87.3 65.7 61.9 67.7 96.3 72.6 76.7 89.2 69.7 34.2 74.2 1.8 53. 81.8 80.4 85.2 64.0 61.4 63.9 96.3 72.9 75.7 27 Table 10: Detailed performance of sequentially fine-tuning the full model on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72. 76.4 90.7 54.9 34.8 76.6 1.0 51.6 81.4 80.3 87.4 65.7 60.6 68.6 95.0 72.6 76.5 89.2 73.0 3.7 59.0 1. 45.3 57.9 63.8 74.1 54.2 59.6 44.2 76.0 71.7 62.7 88.1 64.6 63.6 74.6 1.2 58.4 80.4 77.9 83.1 62.5 58.9 63.4 94.6 70. 73.9 88.0 63.1 59.8 79.6 1.5 58.4 80.3 77.6 82.9 61.9 59.0 66.1 93.5 71.6 74.1 86.9 59.4 58.6 68.9 46. 64.1 74.7 66.9 68.6 50.3 53.9 55.8 87.9 65.9 65.5 Table 11: Detailed performance of sequentially fine-tuning the vision tower on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 58.8 44.0 37.0 70.7 0.8 42.3 78.9 74.3 75.1 56.3 56.4 61.3 90.5 69.6 70. 61.5 59.0 34.4 76.4 1.1 46.5 80.6 79.5 85.1 64.4 59.5 65.6 94.3 71.0 75.0 55.3 37.8 34.2 72.7 26.3 45. 77.3 76.0 78.9 59.6 55.6 61.8 89.4 69.5 71.0 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72.4 76. 69.9 22.1 35.4 75.5 1.0 40.8 81.3 80.0 85.8 63.6 61.4 66.3 94.6 71.5 75.6 57.5 64.2 31.8 72.2 1.0 45. 79.7 76.9 79.6 60.3 57.6 65.0 91.9 70.6 72.7 28 Table 12: Detailed performance of sequentially fine-tuning the projector in the LLM on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72. 76.4 59.4 53.2 36.1 76.1 1.1 45.2 81.4 79.9 87.3 66.1 62.1 66.1 95.9 72.6 76.4 57.7 56.6 36.0 76.3 1. 45.6 81.7 80.2 87.2 66.1 61.7 66.9 96.0 72.5 76.5 57.5 57.9 35.5 76.4 1.2 45.7 81.6 80.0 86.1 65.5 60.9 67.3 95.9 72. 76.2 58.2 58.2 36.4 77.0 1.0 46.2 81.8 80.1 86.3 66.3 61.0 68.0 95.8 72.5 76.5 57.4 57.7 35.9 76.9 1. 46.0 81.1 79.4 86.2 65.1 60.5 67.1 95.6 72.4 75.9 Table 13: Detailed performance of sequentially fine-tuning the LLM on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 89.6 62.2 63.2 74.7 1.0 58.1 80.7 78.4 84.2 62.8 58.9 63.5 94.7 71.0 74. 88.9 63.3 58.6 79.6 1.3 58.3 79.8 78.0 83.0 61.6 59.2 67.2 92.5 71.5 74.1 87.8 56.6 56.7 71.2 71.8 68. 75.2 68.6 72.0 51.8 53.4 59.0 90.0 65.3 66.9 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72.4 76. 90.7 54.3 35.2 76.6 1.0 51.6 81.2 80.4 87.3 65.8 60.8 67.5 94.8 72.7 76.3 89.4 70.2 4.4 60.5 1.4 45. 72.8 66.9 75.9 54.9 58.4 46.4 83.6 72.3 66.4 29 Table 14: Detailed performance of sequentially fine-tuning the SA projection layers in the LLM on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72. 76.4 85.5 53.9 35.7 76.1 1.0 50.4 82.0 80.0 87.2 66.0 62.4 68.0 95.7 72.3 76.7 85.1 67.8 35.0 76.4 1. 53.1 81.4 79.7 86.9 64.7 62.3 67.1 95.9 72.4 76.3 84.8 68.2 55.9 75.8 1.0 57.1 81.2 80.0 86.8 65.3 62.1 66.9 96.1 72. 76.3 84.4 64.8 52.6 79.3 1.2 56.5 81.9 80.6 86.3 66.0 62.4 69.2 96.3 72.5 76.9 84.0 66.3 51.4 78.9 52. 66.6 81.9 79.4 86.1 64.9 61.9 68.9 96.1 72.4 76.5 Table 15: Detailed performance of sequentially fine-tuning the SA Proj. (QKV) in the LLM on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 78.4 67.2 44.2 76.2 0.8 53.4 81.2 79.8 86.9 65.7 62.7 67.1 95.9 72.1 76. 78.2 62.4 42.6 78.6 1.1 52.6 82.0 80.4 86.7 65.9 62.3 68.4 96.3 72.3 76.8 77.6 65.4 43.0 78.3 32.2 59. 81.9 79.9 86.4 65.8 62.3 68.8 96.1 72.3 76.7 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72.4 76. 78.7 53.2 36.1 76.2 1.0 49.0 81.9 79.9 87.2 65.8 62.3 67.6 95.9 72.2 76.6 78.4 65.9 36.4 76.9 1.0 51. 81.7 79.9 87.3 65.4 62.5 67.6 95.8 72.4 76.6 30 Table 16: Detailed performance of sequentially fine-tuning the MLP in the LLM on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72. 76.4 90.1 54.1 35.6 76.6 1.0 51.5 81.7 80.4 87.2 65.8 61.5 68.0 96.1 72.7 76.7 89.5 71.5 17.0 66.2 1. 49.1 80.9 75.7 80.0 60.0 61.2 53.7 93.7 72.7 72.2 89.6 67.6 64.1 75.3 1.2 59.6 81.0 79.7 85.5 64.0 61.0 65.9 96.2 72. 75.7 89.3 68.0 60.9 79.8 1.6 59.9 80.5 79.9 84.5 63.7 60.7 68.1 95.9 72.3 75.7 88.9 62.0 60.9 74.0 74. 72.0 80.4 75.1 78.9 59.3 59.5 62.1 95.2 71.9 72.8 Table 17: Detailed performance of sequentially fine-tuning the MLP (Gate & Up) in the LLM on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 89.8 67.8 61.9 76.1 1.2 59.4 81.3 80.1 86.6 65.0 62.7 67.8 96.5 72.3 76. 89.6 68.4 58.5 80.0 1.9 59.7 81.2 80.7 85.9 65.3 62.5 69.5 96.2 72.6 76.7 89.5 67.2 58.9 79.3 72.2 73. 81.5 78.8 85.4 64.9 62.0 68.4 96.0 72.5 76.2 53.7 52.4 36.3 76.0 1.1 43.9 81.4 80.1 87.1 65.9 61.8 66.4 95.9 72.4 76. 90.2 53.4 36.1 76.4 0.9 51.4 81.7 80.1 87.0 66.1 62.2 67.7 96.3 72.4 76.7 89.8 71.5 35.0 75.5 1.8 54. 81.6 80.6 86.3 65.4 63.1 64.4 96.4 72.6 76.3 31 Table 18: Detailed performance of sequentially fine-tuning the full model of LLaVA-NeXT (LLaMA 3) on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 32.6 45.7 13.2 65.4 0.8 31.5 71.6 69.2 72.7 31.9 42.0 59.7 73.2 58. 59.8 84.8 37.6 24.8 52.2 0.3 39.9 54.0 54.3 40.4 23.4 43.9 55.3 63.3 56.8 48.9 76.8 63.3 0.7 31.0 0. 34.4 53.3 14.6 27.7 14.6 41.5 32.7 57.5 55.8 37.2 77.2 48.5 62.0 56.1 0.6 48.9 62.1 48.6 46.6 27.2 39.6 50.3 69.7 53. 49.7 76.6 32.4 55.6 72.9 0.5 47.6 58.2 51.0 59.2 33.9 42.4 53.6 66.4 56.6 52.7 69.6 44.0 45.2 42.8 33. 46.9 43.9 7.8 15.7 10.2 25.6 19.2 58.3 42.0 27.8 Table 19: Detailed performance of sequentially fine-tuning the vision encoder and projector of LLaVA-NeXT (LLaMA 3) on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 12.8 40.8 34.1 50.1 0.5 27.7 57.1 34.9 36.3 22.2 38.1 55.6 64.0 49.7 44. 22.1 48.9 34.1 69.9 0.7 35.1 64.9 55.3 59.2 29.9 42.3 59.7 69.9 58.7 55.0 13.4 40.3 33.1 59.3 5.2 30. 58.0 42.7 38.9 24.4 35.8 52.3 67.3 52.3 46.5 32.6 45.7 13.2 65.4 0.8 31.5 71.6 69.2 72.7 31.9 42.0 59.7 73.2 58.5 59. 4.2 0.6 0.3 0.9 0.0 1.2 13.8 0.1 0.6 0.3 9.9 14.4 11.3 15.1 8.2 4.2 44.9 1.8 0.9 0.6 10. 5.8 0.5 1.4 0.2 1.9 2.4 0.0 6.0 2.3 32 Table 20: Detailed performance of sequentially fine-tuning the LLM of LLaVA-NeXT (LLaMA 3) on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 32.6 45.7 13.2 65.4 0.8 31.5 71.6 69.2 72.7 31.9 42.0 59.7 73.2 58. 59.8 85.2 32.0 23.3 56.9 0.1 39.5 55.5 54.3 45.7 25.2 42.6 56.1 65.5 56.8 50.2 68.6 57.5 14.1 35.0 0. 35.0 54.4 20.0 31.6 11.5 40.8 11.8 24.3 55.4 31.2 72.0 55.1 62.7 57.8 0.8 49.7 62.7 49.4 49.5 27.4 40.8 50.8 70.8 54. 50.8 72.1 21.9 56.4 72.6 0.6 44.7 59.9 53.7 58.6 33.4 38.4 57.1 68.7 56.8 53.3 68.8 41.6 42.7 40.1 60. 50.8 35.1 4.3 21.5 8.7 16.3 16.1 52.2 37.2 23.9 Table 21: Detailed performance of sequentially fine-tuning the SA projection layers in the LLM of LLaVA-NeXT (LLaMA 3) on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 72.3 50.6 53.9 62.0 0.8 47.9 68.5 61.3 58.3 33.9 43.1 58.3 73.4 58.2 56. 70.6 9.9 43.9 73.5 0.6 39.7 67.4 63.8 63.6 35.8 42.5 63.3 72.6 59.9 58.6 70.3 54.7 44.3 64.9 32.9 53. 65.4 49.4 48.9 27.5 41.1 53.9 70.7 60.2 52.1 32.6 45.7 13.2 65.4 0.8 31.5 71.6 69.2 72.7 31.9 42.0 59.7 73.2 58.5 59. 78.1 38.8 28.7 64.3 0.7 42.1 67.6 60.4 60.8 29.5 46.9 58.6 72.2 59.6 57.0 68.2 60.5 28.4 62.9 0.8 44. 65.3 58.3 58.9 32.1 45.2 55.4 70.4 60.1 55.7 33 Table 22: Detailed performance of sequentially fine-tuning the MLP in the LLM of LLaVA-NeXT (LLaMA 3) on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 32.6 45.7 13.2 65.4 0.8 31.5 71.6 69.2 72.7 31.9 42.0 59.7 73.2 58. 59.8 84.3 34.6 28.5 61.5 0.5 41.9 65.3 59.8 54.1 29.7 44.9 58.3 71.7 59.5 55.4 78.7 58.6 26.3 54.6 0. 43.8 62.8 50.4 48.6 24.3 43.7 50.3 69.8 58.5 51.1 76.9 53.9 61.8 62.0 0.8 51.1 66.3 58.8 57.0 32.5 43.5 53.5 70.3 57. 54.9 76.9 35.8 56.2 73.2 0.7 48.6 62.1 57.6 61.2 35.3 41.2 57.3 62.8 58.8 54.5 72.0 52.8 52.7 59.2 54. 58.2 59.1 32.2 38.8 22.4 35.2 39.5 65.6 55.7 43.6 Table 23: Detailed performance of sequentially fine-tuning the MLP (Gate & Up) in the LLM of LLaVA-NeXT (LLaMA 3) on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 74.3 57.9 59.5 64.7 0.7 51.4 68.9 65.3 63.8 36.2 46.0 54.9 73.1 59.0 58. 71.5 29.8 51.6 74.0 0.8 45.5 66.7 64.0 64.1 36.5 43.6 59.0 71.8 60.0 58.2 71.9 44.8 54.0 63.9 27.6 52. 66.1 46.6 47.6 27.5 40.9 49.2 71.9 59.2 51.1 32.6 45.7 13.2 65.4 0.8 31.5 71.6 69.2 72.7 31.9 42.0 59.7 73.2 58.5 59. 78.4 32.4 27.9 63.5 0.8 40.6 67.6 60.6 59.7 32.3 45.8 60.4 72.3 60.1 57.4 73.9 58.4 27.9 63.7 0.6 44. 67.4 63.3 62.4 33.0 45.5 52.4 71.7 59.9 56.9 34 Table 24: Detailed performance of sequentially fine-tuning the full model of Qwen2.5-VL on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 81.4 58.6 29.2 83.0 8.2 52.1 82.9 83.2 94.4 80.3 62.6 68.6 76.7 74. 77.9 93.5 55.6 18.5 69.5 0.1 47.4 79.5 72.6 77.2 61.9 59.3 59.5 77.8 72.0 70.0 0.2 50.6 0.0 17.2 0. 13.6 0.1 54.2 30.2 33.6 0.0 3.7 0.4 0.0 15.3 12.1 48.3 60.8 73.3 0.0 38.9 64.0 72.1 76.1 64.8 34.0 27.5 43.4 24. 50.8 92.6 47.4 58.2 81.5 6.3 57.2 78.8 69.3 90.0 74.5 53.5 59.7 77.3 68.9 71.5 92.4 51.1 59.1 62.7 60. 65.2 72.5 62.7 66.8 47.1 46.9 51.5 71.6 63.7 60.4 Table 25: Detailed performance of sequentially fine-tuning the vision encoder and projector of Qwen2.5-VL on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 81.5 59.2 29.3 83.1 8.6 52.3 82.9 83.8 94.4 80.2 62.9 68.5 76.1 74.1 77. 81.5 58.2 30.2 83.1 8.8 52.4 83.1 83.9 94.5 80.3 63.4 69.9 76.2 74.1 78.2 88.0 33.1 35.4 71.0 57.5 57. 75.4 75.1 88.7 69.2 52.7 62.0 82.3 67.8 71.6 81.4 58.6 29.2 83.0 8.2 52.1 82.9 83.2 94.4 80.3 62.6 68.6 76.7 74.1 77. 92.3 56.4 30.3 82.5 8.5 54.0 83.0 83.8 94.4 79.5 62.3 67.6 76.6 73.7 77.6 81.4 59.0 29.1 83.2 8.4 52. 82.8 83.7 94.5 80.1 62.5 68.5 76.4 74.0 77.8 35 Table 26: Detailed performance of sequentially fine-tuning the LLM of Qwen2.5-VL on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 81.4 58.6 29.2 83.0 8.2 52.1 82.9 83.2 94.4 80.3 62.6 68.6 76.7 74. 77.9 93.8 55.8 4.9 47.9 0.0 40.5 77.4 41.9 49.6 41.6 59.7 56.7 77.6 71.7 59.5 0.0 47.0 0.0 11.6 0. 11.7 0.0 49.1 23.4 28.5 0.0 3.4 0.0 0.0 13.1 64.4 50.6 63.0 73.3 0.0 50.3 35.8 67.0 76.3 60.8 33.9 25.4 39.5 21. 45.1 67.6 41.0 59.7 82.1 4.6 51.0 75.6 65.1 89.9 74.5 52.4 51.8 69.9 61.8 67.6 91.5 49.1 60.2 61.8 58. 64.2 56.9 68.2 65.8 49.3 36.1 38.3 59.3 51.8 53.2 Table 27: Detailed performance of sequentially fine-tuning the SA projection layers in the LLM of Qwen2.5-VL on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 93.4 56.7 61.0 81.8 9.6 60.5 83.3 86.8 92.9 79.4 62.4 67.8 86.8 73.5 79. 93.4 53.6 57.3 83.6 9.9 59.6 82.4 86.2 93.8 78.9 61.4 69.2 86.5 73.3 79.0 93.2 53.4 58.3 80.6 49.4 67. 82.2 84.5 92.7 78.6 61.8 68.4 85.9 73.5 78.5 81.4 58.6 29.2 83.0 8.2 52.1 82.9 83.2 94.4 80.3 62.6 68.6 76.7 74.1 77. 93.7 59.9 35.7 77.8 10.5 55.5 83.3 84.4 85.6 74.9 63.6 70.1 85.9 73.9 77.7 93.6 53.4 35.3 77.4 9.1 53. 82.8 79.9 92.6 77.3 63.7 68.1 84.9 74.0 77.9 36 Table 28: Detailed performance of sequentially fine-tuning the MLP in the LLM of Qwen2.5-VL on each target task."
        },
        {
            "title": "Dataset",
            "content": "Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 81.4 58.6 29.2 83.0 8.2 52.1 82.9 83.2 94.4 80.3 62.6 68.6 76.7 74. 77.9 94.1 56.6 32.4 76.7 7.5 53.5 78.4 83.3 82.0 71.4 60.6 66.1 79.6 73.2 74.3 83.5 50.0 2.8 8.2 4. 29.9 0.1 0.0 3.0 2.4 29.0 4.4 0.1 20.5 7.4 93.1 53.4 61.5 78.9 6.0 58.6 81.0 74.2 89.3 75.4 58.8 61.4 80.3 72. 74.0 93.0 31.8 60.6 83.4 3.3 54.4 81.0 82.2 92.5 78.3 61.4 65.9 82.2 72.6 77.0 92.6 52.2 60.4 64.4 60. 66.0 74.0 74.8 72.7 59.4 55.9 50.7 77.0 70.9 66.9 Table 29: Detailed performance of sequentially fine-tuning the MLP (Gate & Up) in the LLM of Qwen2.5-VL on each target task. Dataset Target CUB200 PixmoCount PathVQA TextVQA TimeClock Average Held out AI2D ChartQA DocVQA InfoVQA MMStar RealWorldQA ScienceQA SeedBench Average Baseline Stage 1 CUB200 Stage 2 PixmoCount Stage 4 Stage 3 PathVQA TextVQA TimeClock Stage 5 93.8 50.0 61.5 81.1 7.6 58.8 69.0 84.1 91.3 78.4 56.0 56.3 56.5 68.2 70. 94.0 50.9 61.7 83.8 5.1 59.1 80.7 76.6 93.0 79.5 59.1 64.4 72.6 71.7 74.7 94.0 54.7 62.3 79.6 55.5 69. 75.5 80.5 90.4 78.4 58.7 66.3 65.3 71.1 73.3 81.4 58.6 29.2 83.0 8.2 52.1 82.9 83.2 94.4 80.3 62.6 68.6 76.7 74.1 77. 94.1 59.7 36.0 75.9 8.1 54.8 82.3 81.8 81.9 72.3 62.5 68.8 83.4 73.8 75.8 94.2 49.6 22.0 74.7 6.5 49. 76.6 81.9 81.2 68.3 47.6 34.5 78.7 60.1 66."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign Champaign, IL, USA"
    ]
}