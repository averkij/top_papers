{
    "paper_title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
    "authors": [
        "Zhangchen Xu",
        "Fengqing Jiang",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Radha Poovendran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines."
        },
        {
            "title": "Start",
            "content": "Zhangchen Xu Fengqing Jiang Luyao Niu Bill Yuchen Lin Radha Poovendran University of Washington Allen Institute for AI {zxu9,fqjiang,luyaoniu,rp3}@uw.edu, yuchenl@allenai.org 4 2 0 2 2 1 ] . [ 2 3 3 1 7 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop novel metric, named as CompatibilityAdjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines."
        },
        {
            "title": "Introduction",
            "content": "Instruction tuning (Figure 1) has been widely adopted to tailor the behavior of base Large Language Models (LLMs) to align with specific tasks and user intents (Zhang et al., 2023). This approach leverages instruction datasets, consisting of samples pairing an instruction with corresponding response. The success of instruction tuning depends on the availability of high-quality instruction datasets. Initially, constructing these datasets required large human effort in generating and curating instruction-response pairs (Databricks, 2023; 1 Figure 1: This figure demonstrates the process of instruction tuning and the scope of this paper. Zheng et al., 2024; Zhao et al., 2024), which is timeconsuming and labor-intensive (Liu et al., 2024b). To reduce the reliance on human-curated datasets, synthetic datasets generated by LLMs have surfaced as viable solution (Adler et al., 2024). Recent works, such as (Sun et al., 2023; Taori et al., 2023; Wang et al., 2023; Xu et al., 2024; Chen et al., 2024), have shown the strong potential of synthetic datasets in instruction tuning. While current research has primarily focused on using LLMs to create large, diverse, and highquality instructions (Liu et al., 2024b), the selection of appropriate LLMs for generating corresponding responses remains largely unexplored. The common approach is to employ top-performing models (e.g., those leading on benchmarks (Fourrier et al., 2024; Chiang et al., 2024)) for response generation in instruction tuning. For instance, Llama-3.2-3BInstruct uses responses generated by Llama-3.1405B-Instruct (i.e., the largest model in Llama3.1 family) for instruction tuning (Meta, 2024b). Additionally, most of the existing open synthetic datasets (Teknium, 2023; Xu et al., 2023a; Ding et al., 2023; Gallego, 2023; Chen et al., 2024) depend on expensive, closed-source models like GPT4 (Achiam et al., 2023) and Gemini (Google, 2024) to produce responses. Is it always better to use the larger or stronger models as teachers? In this paper, we investigate the choice of the teacher model that generate responses during synthetic dataset generation, which we refer to as response generators, influence the instruction-following performance of the instruction-tuned LLMs. Specifically, given base model and set of high-quality instructions, we investigate the following research questions: RQ1: Which models are the most effective response generators for instruction tuning? To answer RQ1, we conduct extensive experiments with five base models, and fine-tune them on datasets generated by 20 response generators across seven model families: Qwen2, Qwen2.5, Llama 3, Llama 3.1, Gemma 2, Phi-3, and GPT-4. Our findings challenge common assumptions in the field, revealing surprising result which we term the Larger Models Paradox: larger response generators (e.g., Llama-3.1-405B-Instruct) do not always enhance base models instruction-following capabilities compared to their smaller counterparts within the same model family (e.g. Llama-3.170B-Instruct). Moreover, we find that open-source models (e.g., Gemma-2-9b-it and Qwen2.5-72BInstruct) outperform GPT-4 as response generators. These findings question established practices and suggest more efficient and accessible approaches to create high-quality instruction datasets. To further explore the Larger Models Paradox, we investigate statistical metrics to reveal potential factors influencing the effectiveness of response generators given different base models. Here, we pose our second research question: RQ2: How can we determine the most effective response generators for certain base model without instruction tuning? This question is crucial due to the significant computational costs associated with instruction tuning across multiple datasets generated by diverse response generators. Our investigation reveals that existing metrics in alignment data selection, including quality (Dubey et al., 2024), difficulty (Li et al., 2024c), and response length (Liu et al., 2023), fail to consider the compatibility between the base model being fine-tuned and the response generator, thus results in their inability to explain the Larger Models Paradox. To bridge this gap, we formulate the task of finding the most effective response generators as risk-return problem. We solve this by calculating an Compatibility-Adjusted Reward (CAR), where compatibility serves as the risk factor. This compatibility is quantified by the average loss of responses on the base model being finetuned, with higher average loss indicating lower compatibility and thus higher risk. Our comparison of the proposed CAR with existing metrics demonstrates that it outperforms all baselines in predicting the effectiveness of response generators. We believe that our findings on the Larger Models Paradox and the proposed CAR can effectively guide future instruction tuning of LLMs. Instead of selecting response generators solely based on benchmark performance (e.g., GPT-4), practitioners should prioritize those with higher compatibility to better enhance the instruction-following capabilities of their LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Synthetic Data Generation for Instruction Tuning. While human-crafted instruction datasets (Databricks, 2023; Zheng et al., 2024; Zhao et al., 2024) have been used for LLM instruction tuning, they are time-consuming and labor-intensive. Consequently, synthetic dataset generation has emerged as promising alternative. Early approaches (Wang et al., 2023; Taori et al., 2023; Xu et al., 2023a,b; Wang et al., 2024b; Luo et al., 2023; Sun et al., 2023) focused on prompting LLMs to generate synthetic instructions, starting with small set of human-annotated seed instructions and expanding these through few-shot prompting (Li et al., 2024a). Another line of work (Ding et al., 2023; Li et al., 2024a) summarized world knowledge to generate more diverse synthetic datasets. Recent advancements (Xu et al., 2024; Chen et al., 2024) further simplified the process by leveraging single prompts to sample instructions directly from LLMs, requiring minimal human oversight. While existing work primarily focused on generating large, diverse, and high-quality instructions, the impact of response generators is often overlooked. Metrics for Data Selection. Instruction tuning data selection involves determining which instruction-response pairs to be included in the training dataset and how to sample them (Albalak et al., 2024). The most widely-used metric for selecting instruction data is quality, which is often assessed using LLM evaluators (Chen et al., 2023; Liu et al., 2024a), reward models (Dubey et al., 2024; Xu et al., 2024), gradient similarity search (Xia et al., 2024a), or combination of these methods (Cao et al., 2024). Another key metric is difficulty, where higher difficulty is considered more valuable for learning. For instance, Li et al. (2024c) introduces IFD, which measures the instruction2 following difficulty of specific instruction-response pairs. Li et al. (2024b) further refines IFD by utilizing GPT-2 for efficient estimation. Approaches like Deita (Liu et al., 2023) consider both quality and difficulty when selecting datasets. Token length is also adopted as metric, as discussed in (Xia et al., 2024b; Liu et al., 2023). Our investigation complements existing research on alignment data selection by shifting the focus to the response generation process itself, as illustrated in Figure 1. While prior studies have concentrated on selecting the most effective instruction-response pairs with an existing instruction dataset, we explore the crucial role that response generators play in influencing the quality of instruction tuning."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Instruction Datasets. An instruction dataset can be represented as = (xi, yi)D i=1, where each sample (xi, yi) consists of an instruction xi and its corresponding response yi. In this paper, we investigate how the response generator, denoted as M, impacts the instruction-following capabilities of models fined-tuned with with yi = M(xi). capabilities Supervised Fine-Tuning. Supervised finetuning (SFT) is widely adopted to enhance of LLMs. instruction-following The SFT process updates the parameters θ of pre-trained language model to minimize the negative log-likelihood loss over the instruction dataset D. The SFT loss can be formally expressed as: LSFT(θ) = 1 (cid:88) (xi,yi)D log pθ(yixi). (1)"
        },
        {
            "title": "3.2 Experimental Setup",
            "content": "Instruction Sets. To construct diverse and highquality instructions, we sample from the MagpieAir-3M dataset (Xu et al., 2024), and obtain subset of 100K high-quality instructions, denoted as Magpie-100K. detailed categorization of instruction tasks is provided in Appendix A.1. Additionally, we extracted another 100K high-quality instructions from multiple sources, including UltraFeedback (Cui et al., 2023), WildChat (Zhao et al., 2024), Lmsys-Chat-1M (Zheng et al., 2024), and Alpaca-GPT-4 (Gallego, 2023). This instruction Table 1: Overview of 20 response generators used in our study."
        },
        {
            "title": "Model ID",
            "content": "Qwen2 (Yang et al., 2024) Jun, 2024 Qwen2.5 (Team, 2024) Sept, 2024 Llama 3 (Meta, 2024c) Llama 3.1 (Meta, 2024c) Apr, 2024 Jul, 2024 Gemma 2 (Team et al., 2024) Jun, 2024 Phi-3 (Abdin et al., 2024) Jun, GPT-4 (Achiam et al., 2023) Since Mar, 2023 Qwen2-1.5B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Llama-3-8B-Instruct Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Gemma-2-2b-it Gemma-2-9b-it Gemma-2-27b-it Phi-3-mini-128k-instruct Phi-3-small-128k-instruct Phi-3-medium-128k-instruct"
        },
        {
            "title": "Size",
            "content": "1.5B 7B 72B 3B 7B 14B 32B 72B 8B 70B 8B 70B 405B 2B 9B 27B 3.8B 7B 14B GPT-4 & GPT-4 Turbo - set, denoted as Mix-100K, contains both humanwritten and synthetic instructions, ensuring comprehensive representation of instruction types. Response Generators. Our study considers 20 response generators across 7 model families for response generation. The model families include Qwen2 (Yang et al., 2024), Qwen2.5 (Team, 2024), Llama 3 (Meta, 2024c), Llama 3.1 (Meta, 2024c), Gemma 2 (Team et al., 2024), Phi-3 (Abdin et al., 2024), and GPT-4 (Achiam et al., 2023). comprehensive overview of the response generators is presented in Table 1. By combining the instructions with corresponding responses generated by these teacher models, we construct instructionresponse pairs for instruction-tuning. By default, we use greedy decoding to generate responses. The datasets used in our experiments can be found here1. Base Models. We consider five base language models from different developers of varying sizes as students, including Qwen2-1.5B (Yang et al., 2024), Gemma-2-2b (Team et al., 2024), Llama3.2-3B (Meta, 2024a), Qwen2.5-3B, (Team, 2024) and Llama-3.1-Minitron-4B-Width-Base (Llama3.1-Minitron-4B) (Muralidharan et al., 2024). Evaluation Benchmarks. To evaluate the instruction-following capabilities of the instructiontuned models, we use two widely-used instruction1https://huggingface.co/datasets/Magpie-Align/Magpie100K-Generator-Zoo 3 following benchmarks: AlpacaEval 2 (AE2) (Li et al., 2023) and Arena-Hard (AH) (Li et al., 2024d). Specifically, AE2 contains 805 representative instructions from real user interactions. AH contains 500 challenging user queries. AE2 and AH use GPT-4-Turbo (1106) and GPT-4-0314 as the baselines to assess the performance of Both instruction-tuned models, benchmarks compare responses generated by the model of interest with those generated by baselines, and employ GPT evaluators to automatically annotate which response is preferred. respectively. Evaluation Metrics. Similar to existing studies, we adopt two metrics to measure the performance of fine-tuned SLMs. The first metric, used by both benchmarks, is the win rate (WR), which calculates the fraction of responses that are favored by the GPT evaluator. The second metric, used by AE2, is the length-controlled win rate (LC) (Dubois et al., 2024). LC accounts for response length to reduce its impact on WR. Additionally, we report the Average Performance (AP), computed as the mean of AE2s LC and AHs WR. Instruction-Tuning and Evaluation Setup. We use SFT and implement cosine learning rate schedule with max learning rate of 2 105 to fine-tuning the base models for 2 epoches (Touvron et al., 2023). The detailed hyper-parameters and experimental platform can be found in Appendix A.2. We follow the official instruction templates of each model. To ensure reproducibility of our empirical analysis, we implement greedy decoding for both AE2 and AH benchmarks."
        },
        {
            "title": "3.3 Empirical Evaluation",
            "content": "This section evaluates the instruction-following capabilities of models fine-tuned over datasets whose responses are generated by various response generators. By default, we utilize the Magpie-100K dataset as our primary instruction set. Figure 2 provides comprehensive overview of the AP across different base models and response generators, and the detailed benchmark scores of AE2 and AH are deferred to Table 6 in Appendix B.1. We observe that the Gemma-2 and Qwen2 families consistently demonstrate superior performance across all base models evaluated. Notably, Gemma-2-9b-it and Qwen2.5-72B-Instruct emerge as the two best response generators, as evidenced by their consistently high AP scores. In addition, we report the following key findings. Figure 2: Average performance of five base models fine-tuned on various response generators across six model families. We use different colors to distinguish between model families, with darker bars indicating larger response generators within each family. Finding 1: [Larger Models Paradox] Larger = improved instructionresponse generators following capabilities. Our evaluation reveals counterintuitive finding: increasing the model size of response generators does not necessarily improve the instructionThis following capabilities of base models. finding is universal, evidenced across multiFor example, Gemma-2ple model families. 9b-it demonstrates superior performance compared to its larger counterpart, Gemma-2-27bit, in SFT across almost all base models examined. Similar observations are made in other model pairs: Phi-3-Small outperforms Phi-3Medium, Llama-3.1-70B-Instruct surpasses Llama3.1-405B-Instruct, Qwen2-7B-Instruct outperforms Qwen2-72B-Instruct, and Qwen2.5-7B-Instruct exceeds Qwen2.5-32B-Instruct. We refer to this finding as the Larger Models Paradox: larger language models, despite their superior performance, may not always generate better responses for finetuning smaller language models compared to responses generated by medium-sized models. We believe the key to explain this paradox is the compatibility between the response generators and base models. For example, high-quality textbook (responses from large size response generators) written for college students may be challenging for primary school students (smaller base models). We will investigate this paradox in Section 4 with more detailed statistics and metrics to evaluate the compatibility. Finding 2: [Familys Help] Learning from response generators within the same model family leads to higher performance. We observe higher AP when base models are fine-tuned using responses generated by models within the same family. This is evidenced when Qwen2-1.5B, Qwen2.5-3B, and Gemma 22B serve as base models. In these instances, the relative performance of using intra-family response generators surpasses that observed when tuning other base models. Furthermore, while not practically applicable, we observe significant performance boost when fine-tuning base model using responses generated from its own instruction-tuned version. prime example of this is the Gemma 2-2B base model, which achieves best performance when tuned with responses from Gemma-2-2b-it, outperforming all other response generators. These two phenomena underscore the importance of compatibility between the base model and the response generator in instruction tuning. Table 2: This table compares the performance of GPT-4 and other state-of-the-art open source LLMs as the response generator. All models are supervised-fine-tuned on the Llama-3.1-Minitron-4B base model."
        },
        {
            "title": "Response\nGenerator Model",
            "content": "AlpacaEval 2 LC (%) WR (%) Arena-Hard WR (%) AP (%) Gemma-2-9b-it Gemma-2-27b-it Llama-3-70b-Instruct Llama-3.1-70b-Instruct Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct GPT-4 16.09 13.93 10.55 9.52 13.50 19. 6.63 13.70 13.31 10.68 10.10 14.33 21.01 5.70 13.7 12.4 6.7 8.3 10.6 13.1 4.8 14.90 13.17 8.62 8.91 12.05 16. 5.72 [Open-Source > Close-Source] Finding 3: Open-source LLMs can outperform closesource LLMs as response generators. Table 2 compares the instruction-tuning performance when utilizing GPT-4 and open-source LLMs (e.g., Gemma 2, Llama 3, Llama 3.1 and Qwen2.5) as response generators. For this evaluation, we employ the Mix-100K dataset as our instruction source. Notably, our findings reveal that all open-source LLMs significantly outperform GPT-4. We hypothesize that this is because the response length of GPT-4 is less than open-source LLMs, thus less favored by the evaluators. These results suggest the potential for using cost-effective open-source LLMs for synthetic data generation in instruction-tuning tasks. Finding 4: Higher temperature and top-p enhance instruction-following capabilities. Figure 3 illustrates the effects of different sampling hyper-parameters when generating responses using Gemma-2-9b-it model. We observe that higher temperature and top-p value can lead to better performance in instruction following. We hypothesize that this enhancement in performance is because higher temperature and top-p values yield more diverse and contextually rich outputs. Finding 5: Reject sampling slightly increases instruction-tuning performance. Table 3 quantifies the impact of reject sampling on synthetic data generation using Gemma-2-9bit model. Specifically, we generate 5 responses per instruction with temperature = 0.8, evaluate them using the ArmoRM-Llama3-8B-v0.1 reward model (Wang et al., 2024a), and select the highest and lowest-rated responses to create two distinct"
        },
        {
            "title": "4 How can we determine the most",
            "content": "effective response generators without instruction tuning?"
        },
        {
            "title": "Generators",
            "content": "It is computationally expensive to brute-force all response generators to identify the most effective one for given base model. In this section, we investigate how to measure the effectiveness of response generators for given base model without training or fine-tuning. Specifically, we study the following research question: Definition 4.1 (Effectiveness Measure of Response Generators). Given base language model and set of synthetic instruction datasets D1, D2, ..., Dn, where each Di contains responses generated by distinct response generator Mi, measure the effectiveness of these response generators without performing the actual fine-tuning process. Evaluation Metric. To assess the accuracy when measuring effectiveness of response generators, we employ Spearmans rank correlation coefficient (ρ) (Zar, 2005). This coefficient evaluates the monotonic relationship between two ranking variables. In our context, we compute ρ between two ranks: the ground truth rank RAP , obtained by fine-tuning the model on each synthetic instruction dataset and measuring the Average Performance (AP), and an estimated rank REST , predicted without finetuning. Spearmans ρ is calculated as: ρ = 1 6 (cid:80) d2 n(n2 1) (2) where di is the difference between the two ranks for each observation and is the number of observations. ρ ranges from -1 to 1, with 1 indicating perfect positive correlation. Our objective is to maximize ρ, thereby achieving the closest prediction between predicted and actual performance rankings. We employ the empirical results obtained in Section 3 as the ground truth."
        },
        {
            "title": "4.2 Baseline Methods",
            "content": "In this section, we introduce commonly-used metrics for alignment data selection: quality, difficulty, and response length, for predicting the performance rank of instruction-tuned models. Response Quality. Following Meta (2024a); Xu et al. (2024), we assess response quality using reward models and calculate the Average Reward Figure 3: This figure demonstrates the impact of different sampling hyper-parameters when generating responses. We use Gemma-2-9b-it as the response generator. All models are supervised-fine-tuned on the Llama-3.1-Minitron-4B base model. Table 3: This table investigates the impact of reject sampling on model performance."
        },
        {
            "title": "Base Model Method",
            "content": "AlpacaEval 2 LC (%) WR (%) Arena-Hard WR (%) AP (%) Llama-3.1Minitron-4B Qwen2.53B-Instruct Best-of-N Worst-of-N Sampling Greedy Best-of-N Worst-of-N Sampling Greedy 15.94 13.02 15.71 16.13 13.83 12.37 13.43 13.78 15.14 12.66 14.81 14.51 13.57 12.54 13.29 13.57 11.9 11.0 11.8 11. 21.0 17.9 20.1 19.4 13.92 12.01 13.755 13.565 17.415 15.135 16.765 16.59 datasets: Best-of-N and Worst-of-N. We also compare them with responses sampled at = 0.8 and greedy decoding (T = 0). The results presented in Table 3 demonstrate slight improvement in performance when utilizing reject sampling compared to standard sampling techniques. In what follows, we summarize the conclusion for RQ1. RQ1. Which models are the most effective response generators for instruction tuning? A1. Gemma-2 and Qwen2 families consistently demonstrate superior performance across all base models evaluated, and even outperform GPT-4. Notably, Gemma-29b-it and Qwen2.5-72B-Instruct emerge as the two best response generators, as evidenced by their consistently high AP scores. We also found that larger models do not always generate responses for enhanced instruction-following capabilities. 6 (AR) of all responses. To mitigate potential selection bias, we employ three state-of-the-art reward models from RewardBench (Lambert et al., 2024): ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024a), Skywork-Reward-Llama-3.1-8B (Liu and Zeng, 2024), and Skywork-Reward-Gemma-2-27B (Liu and Zeng, 2024). Instruction-following Difficulty. Instructionfollowing difficulty is another widely-used metric in alignment data selection (Meta, 2024a; Liu et al., 2023; Li et al., 2024c,b; Xu et al., 2024). To assess the difficulty of responses, we employ the following two metrics: 1. Response Perplexity (PPL). For given the reinstruction-response pair (xi, yi), sponse perplexity is defined as: PPL(yixi) = (cid:88) exp("
        },
        {
            "title": "1\nN",
            "content": "j=1 log pθ(yi,jxi, yi,1:j1)), Figure 4: This figures demonstrates the response quality measured by three reward models. where is the token length of yi and yi,j is its j-th token, and θ is the parameter of the base model. We use GPT-2 model and each corresponding base model for evaluation, denoted as PPL-GPT2 and PPL-Self respectively. 2. Instruction Following Difficulty (IFD) (Li et al., 2024c). IFD is defined as: IFD(yixi) = PPL(yixi) PPL(yi) , where PPL(yi) is the unconditional perplexity of response yi. We follow Li et al. (2024b) and employ GPT-2 and the base model respectively, denoted as IFD-GPT2 and IFD-Self. For each metric, we compute the average value across the entire dataset Di. Response Length. According to Liu et al. (2023) and Xia et al. (2024b), the response length positively correlates with the final alignment performance. We use the tiktoken library (OpenAI, 2024) to count the number of response tokens for each pair, and report the average response length for each Di."
        },
        {
            "title": "4.3 Baseline Methods Fails to Measure the\nEffectiveness of Response Generators",
            "content": "In what follows, we demonstrate that the effectiveness of response generators indicated by baseline methods does not match the performance of models fine-tuned on various synthetic instruction datasets. As shown in Figure 4, AR consistently increases with model size within model families (except Phi3 family). However, this trend fails to explain the \"Larger Models Paradox\" discussed in Section 3. Notably, since AR measures human preference, this discrepancy suggests that responses preferred by humans are not necessarily optimal for aligning language models. Similarly, metrics representing instructionfollowing difficulty (IFD and Perplexity) and response length show no strong correlation with model instruction-following capabilities. We deferred the results and analysis of these metrics to Appendix B.2. These findings highlight the inadequacy of existing metrics in accurately measuring the effectiveness of response generators in enhancing performance of instruction-tuned models."
        },
        {
            "title": "Measure Effectiveness",
            "content": "In this section, we present new metric to measure the effectiveness of response generators, making the \"Larger Models Paradox\" explainable. Our key insight to capture the compatibility of response generators with base models. To reflect such compatibility, we use the loss of the response ri in the base model being fine-tuned as the key metric. Intuitively, lower loss of response yi on the 7 Table 4: Spearmans rank correlation coefficient (ρ) for different measurement metrics. Here RM1, RM2 , RM3 are reward models ArmoRM-Llama3-8B-v0.1, Skywork-Reward-Llama-3.1-8B, and Skywork-Reward-Gemma-2-27B respectively. We observe that our proposed CAR shows the highest correlation between the effectiveness of the response generator and the instruction-following capabilities of fine-tuned base models."
        },
        {
            "title": "Difficulty",
            "content": "RM1 RM2 RM3 IFD-GPT2 IFD-Self PPL-GPT2 PPL-Self Qwen2-1.5B Gemma 2-2B Qwen2.5-3B Llama 3.2-3B 0.5526 0.5526 0.4526 0.6088 Llama-3.1-Minitron-4B 0.6632 0.7895 0.7982 0.7351 0.8105 0. 0.8754 0.8842 0.7456 0.9088 0.9386 0.7088 0.8281 0.7386 0.7632 0."
        },
        {
            "title": "Average",
            "content": "0.5660 0.8039 0.8705 0.7575 0.7719 0.8930 0.8088 0.8579 0.8555 0. 0.1473 0.1614 0.0456 0.0456 0.1579 0.1116 0.5596 0.4351 -0.0614 0.6018 0."
        },
        {
            "title": "Response\nLength",
            "content": "0.5404 0.6298 0.6088 0.5877 0."
        },
        {
            "title": "CAR",
            "content": "0.8842 0.9000 0.8105 0.9053 0.9439 0.4323 0.5895 0.8888 base model indicates that the response aligns well with the base models existing knowledge and capabilities, thus is more learnable compared to the response with higher loss. While compatibility is crucial, it alone cannot fully measure effectiveness. Consider scenario where response generator consistently produces simple, low-quality responses for every question. In such cases, although these responses might be highly compatible with the base model, their overall quality and would be low. Therefore, to bridge this gap between quality and compatibility, we formulate the task of finding the most effective response generator as risk-return problem (Fama and MacBeth, 1973). We propose an adjusted reward value that incorporates both the potential benefit (return) and the compatibility risk. Specifically, we define our Compatibility-Adjusted Reward (CAR) as follows: CAR(Di, θ) = r(Di) 1 + β L(Di, θ) (3) (cid:80) where r(Di) is the average reward measured by the reward model, representing the potential return, and L(Di, θ) = 1 log pθ(yi) is the avDi erage loss for responses in Di on the base model parameterized by θ. β is tunable parameter that controls the impact of compatibility on the adjusted reward. yiDi"
        },
        {
            "title": "4.5 Experimental Results",
            "content": "Table 4 compares the Spearmans ρ correlation coefficient of baseline metrics with our CAR when using datasets generated by different response generators to fine-tune various base models. For CAR calculation, we employ Skywork-Reward-Gemma2-27B as the reward model and set β = 3. The results in Table 4 demonstrate that our proposed 8 CAR consistently outperforms other baseline metrics across almost all settings, indicating its potential to predict the effectiveness of different response generators without instruction tuning. RQ2. How can we determine the most effective response generators without instruction tuning? A2. Existing metrics in instruction data selection are inadequate for accurate prediction as they fail to consider the compatibility between the base model and the response generator. To address this limitation, we propose the Compatibility-Adjusted Reward (CAR), which achieves better performance in identifying effective response generators across various base models."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "This paper investigates the impact of response generators in synthetic dataset generation for instruction tuning. We uncovered the Larger Models Paradox, wherein larger response generators do not necessarily enhance base models instructionfollowing capabilities compared to their smaller counterparts within the same model family. To explain this phenomenon, we considered the compatibility between response generators and the base model, and proposed the Compatibility-Adjusted Reward (CAR). Our metric achieved better performance in identifying the effectiveness of different response generators without the need for fine-tuning, outperforming existing baselines in alignment dataset selection. We will explore several promising directions. First, efficiently transforming existing datasets to achieve better compatibility can lead to more effective use of available instruction tuning datasets. Second, investigating theoretical foundations of compatibility would enhance our understanding of the underlying mechanisms of instruction tuning. Lastly, studying the impact of different response generators for preference tuning may help aligning LLMs to better reflect human values."
        },
        {
            "title": "Limitations",
            "content": "While our study provides valuable insights into the effectiveness of response generators in instruction tuning, we acknowledge that our research primarily focuses on general instruction following tasks and does not extensively explore the synthesis of alignment datasets for specialized domains such as mathematics or complex reasoning. As result, the applicability of the Larger Models Paradox to these specific areas remains uncertain."
        },
        {
            "title": "Ethical Impact",
            "content": "This paper makes counterintuitive observation, referred to as the Larger Models Paradox, showing that stronger models are not stronger teachers for instruction tuning. We further propose new metric to measure the effectiveness of teachers when generating responses for instruction datasets. This metric informs the selection of response generators for future fine-tuning processes to enhance language models instruction-following capabilities. We do not identify potential misuse and ethical concerns in this paper."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is partially supported by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, the National Science Foundation (NSF) under grants IIS 2229876, and the Office of Naval Research under grant N0014-23-12386. This work is supported in part by funds provided by the National Science Foundation, by the Department of Homeland Security, and by IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or its federal agency and industry partners."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. 2024. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. 2024. survey on data selection for language models. arXiv preprint arXiv:2402.16827. Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. 2024. Instruction mining: Instruction data selection for tuning large language models. In First Conference on Language Modeling. Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchenbauer, Tianyi Zhou, and Tom Goldstein. 2024. Genqa: Generating millions of instructions from handful of prompts. arXiv preprint arXiv:2406.10323. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377. Databricks. 2023. Databricks dolly-15k. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Eugene Fama and James MacBeth. 1973. Risk, return, and equilibrium: Empirical tests. Journal of political economy, 81(3):607636. Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. 2024. Open https://huggingface. llm leaderboard v2. co/spaces/open-llm-leaderboard/open_llm_ leaderboard. Victor Gallego. 2023. alpaca-gpt4. Google. 2024. Our next-generation model: Gemini 1.5. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Rewardbench: Evaluating reward models for language modeling. Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, et al. 2024a. Synthetic data (almost) from scratch: Generalized instruction tuning for language models. arXiv preprint arXiv:2402.13064. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024b. Superfiltering: Weak-to-strong data filtering In Proceedings of the for fast instruction-tuning. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1425514273, Bangkok, Thailand. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024c. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 76027635, Mexico City, Mexico. Association for Computational Linguistics. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024d. From live data to high-quality benchmarks: The arena-hard pipeline. Chris Yuhao Liu and Liang Zeng. 2024. Skywork reward model series. https://huggingface.co/ Skywork. Liangxin Liu, Xuebo Liu, Derek Wong, Dongfang Li, Ziyi Wang, Baotian Hu, and Min Zhang. 2024a. Selectit: Selective instruction tuning for large language models via uncertainty-aware self-reflection. arXiv preprint arXiv:2402.16705. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. 2024b. Best practices and lessons learned on synthetic data for language models. arXiv preprint arXiv:2404.07503. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. arXiv preprint arXiv:2306.08568. Meta. 2024a. Llama-3.2-3b. https://huggingface. co/meta-llama/Llama-3.2-3B. Meta. 2024b. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta. 2024c. Meet llama 3.1. https://llama.meta. com. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679. OpenAI. 2024. Tiktoken. https://github.com/ openai/tiktoken. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven selfalignment of language models from scratch with minimal human supervision. Advances in Neural Information Processing Systems, 36. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Jerrold Zar. 2005. Spearman rank correlation. Encyclopedia of Biostatistics, 7. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. 2024. LMSYS-chat-1m: large-scale real-world LLM conversation dataset. In The Twelfth International Conference on Learning Representations. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Qwen Team. 2024. Qwen2.5: party of foundation models. Teknium. 2023. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024a. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. In EMNLP. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. Zifeng Wang, Chun-Liang Li, Vincent Perot, Long Le, Jin Miao, Zizhao Zhang, Chen-Yu Lee, and Tomas Pfister. 2024b. Codeclm: Aligning language models with tailored synthetic data. arXiv preprint arXiv:2404.05875. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024a. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333. Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, and Junyang Lin. 2024b. Rethinking data selection at scale: Random selection is almost all you need. Preprint, arXiv:2410.09335. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6268 6278, Singapore. Association for Computational Linguistics."
        },
        {
            "title": "A More on Experimental Setups",
            "content": "A."
        },
        {
            "title": "Instruction Set Details",
            "content": "Figure 5 demonstrates the task category of instructions in our sampled Magpie-100K. We follow (Xu et al., 2024) and use Llama-3-8B-Instruct to tag the task categories. We note that this instruction set covers wide range of instructions across different task categories. Figure 6: Average Output Length of synthetic datasets generated using different response generators (measured in Tokens). Figure 5: Task categories of the Magpie-100K instruction set used in our study. Figure 7: PPL-GPT2 and IFD-GPT2. A.2 Supervised Fine-Tuning Setups Table 5 demonstrates the detailed supervised finetuning (SFT) hyper-parameters. We perform experiments on server with four NVIDIA A100-SXM480GB GPUs, an AMD EPYC 7763 64-Core Processor, and 512 GB of RAM. These experiments were conducted using Axolotl2. Table 5: This table shows the hyper-parameters for supervised fine-tuning. Hyper-parameter Learning Rate Number of Epochs Number of Devices Per-device Batch Size Gradient Accumulation Steps Effective Batch Size Optimizer Learning Rate Scheduler Warmup Steps Max Sequence Length"
        },
        {
            "title": "Value",
            "content": "2 105 2 4 1 8 32 Adamw cosine 100 4096 2https://github.com/OpenAccess-AI-Collective/ axolotl"
        },
        {
            "title": "B More Experimental Results",
            "content": "B.1 Detailed Benchmark Scores of Instruction-Tuned LLMs Table 6 details the benchmark scores of AE2 and AH when tuning 5 base models with different response generators. These results complement the Average Performance shown in Figure 2. B.2 Visualization of baseline methods in measuring the effectiveness of response generators. Figure 6 presents the output length of synthetic datasets for each response generator. Figure 7 visualizes the PPL-GPT2 and IFD-GPT2 across different response generators. Figure 8 and 9 reports PPL-Self and IFD-Self, respectively. We observe that although PPL-Self and IFD-Self have higher correlation compared with measuring using GPT2, they still to fail to effectively predict the effectiveness of different response generators, with low Spearmans rank correlation coefficients demonstrated in Table 4. Table 6: This table details benchmark scores of AE2 and AH when tuning different base models with diverse response generators. Phi-3 Gemma 2 Llama 3 Llama 3.1 Qwen2 Qwen2. Base Model Metric Mini Small Medium Qwen2-1.5B Gemma 2-2B Qwen2.5-3B Llama-3.2-3B Llama-3.1Minitron-4B AE 2 WR AE 2 LC AH AE 2 WR AE 2 LC AH AE 2 WR AE 2 LC AH AE 2 WR AE 2 LC AH AE 2 WR AE 2 LC AH 3.65 2.85 1.8 6.60 5.90 3.3 8.19 7. 10.5 4.88 4.11 3.3 6.35 5.74 3.9 3.64 2. 1.8 6.54 5.89 4.1 7.79 7.29 11.0 3.54 2. 4.1 7.11 6.61 4.5 2.80 2.18 1.2 4.54 3. 2.6 5.97 5.49 8.3 3.05 2.37 2.6 4.83 4. 3.6 2B 5.34 4.16 4.4 16.88 12.93 12. 10.52 9.58 11.8 8.89 7.49 9.0 11.80 10.37 9B 6.13 5.60 5.2 27B 5.49 4.99 4.5 11.83 12. 12.09 13.09 9.3 9.9 13.57 13.78 10.01 10.18 19. 19.6 11.45 10.60 10.9 14.50 16.13 10.58 9.79 8. 11.90 12.34 10.7 11.0 11.9 8B 3.39 2. 1.9 7.09 5.70 5.2 8.07 7.85 9.7 4.67 3. 5.1 6.11 4.80 4.7 70B 3.74 3.10 2. 8.49 7.13 5.6 10.17 9.37 11.4 5.45 4.52 6. 9.87 8.93 6.0 8B 2.76 2.10 2.2 7.20 5. 4.9 7.91 7.22 10.9 4.26 3.17 3.6 8.24 6. 6.0 70B 3.49 2.74 2.8 9.45 7.32 5. 9.68 8.94 13.8 6.68 5.19 5.7 9.61 8.52 5. 405B 1.5B 3.09 2.36 2.4 8.92 7.11 5. 9.12 8.59 12.7 6.44 5.17 5.3 10.03 9.23 6. 2.83 2.68 1.0 2.14 1.91 0.9 2.98 2.54 2. 1.72 1.28 0.6 2.30 2.03 0.9 7B 4.09 3. 3.3 7.11 6.45 5.7 8.54 7.98 14.4 6.23 5. 5.6 7.84 7.31 6.4 72B 3.35 2.82 1. 6.07 5.46 3.4 6.86 6.59 10.6 5.13 4.49 4. 8.45 8.11 5.1 3B 5.60 4.50 2.6 7.91 6. 6.5 16.22 14.79 24.8 6.09 5.11 7.2 7B 6.84 5.66 4.3 12.00 10.94 7.1 12.76 11.89 20. 7.72 6.63 9.8 14B 5.13 4.38 4.4 8.07 7. 8.4 32B 5.65 4.96 3.7 9.19 8.77 6. 10.32 10.28 11.71 11.65 17.9 6.82 5.92 9.5 19. 7.10 6.32 8.9 10.27 9.17 12.05 11.12 11.30 10.89 11.65 11. 72B 7.03 5.83 4.8 16.68 13.85 9.6 18.42 16. 21.2 12.12 9.99 10.8 19.58 17.77 8.3 9. 11.1 10.2 12.2 13 Figure 8: PPL-Self of five base models. Figure 9: IFD-Self of five base models."
        }
    ],
    "affiliations": [
        "University of Washington",
        "Allen Institute for AI"
    ]
}