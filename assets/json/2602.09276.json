{
    "paper_title": "Effective Reasoning Chains Reduce Intrinsic Dimensionality",
    "authors": [
        "Archiki Prasad",
        "Mandar Joshi",
        "Kenton Lee",
        "Mohit Bansal",
        "Peter Shaw"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes."
        },
        {
            "title": "Start",
            "content": "Archiki Prasad* 1 Mandar Joshi 2 Kenton Lee 2 Mohit Bansal 1 Peter Shaw"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach given accuracy threshold on given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe strong inverse correlation between the intrinsic dimensionality of reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering new quantitative metric for analyzing reasoning processes. 6 2 0 2 9 ] . [ 1 6 7 2 9 0 . 2 0 6 2 : r 1. Introduction Chain-of-thought reasoning (CoT) whether through fewshot prompting (Wei et al., 2022), zero-shot prompting (Kojima et al., 2022), or various post-training methods (Zelikman et al., 2022; Chung et al., 2024) has substantially improved the performance of large language models (LLMs) on reasoning tasks by generating textual rationales before 1UNC Chapel Hill 2Google DeepMind. *Work partially done during an internship at Google DeepMind. Correspondence to: Archiki Prasad <archiki@cs.unc.edu>. Preprint. February 11, 2026. 1 final answers. Subsequent work has proposed numerous variations with different stylistic and strategic features, including code-based solutions (Gao et al., 2023; Chen et al., 2023), decomposition strategies (Zhou et al., 2023; Khot et al., 2023; Wang et al., 2023b), and extended reasoning with verification loops (Snell et al., 2024; Muennighoff et al., 2025). These variations represent different ways of communicating problem-solving strategies and structuring solutions analogous to how humans adapt their communication style to their interlocutor in dialogue (Pickering & Garrod, 2004; Giles et al., 1991). Empirical evidence shows different reasoning strategies yield varying performance across tasks (Zhou et al., 2024), consistent with the intuition that different solution approaches suit different problems or learners. Further, not all problems benefit from generating rationales prior to the answer (Sprague et al., 2025). This motivates an important research question: when and why is reasoning effective, and given different reasoning strategies, which is most effective for improving model performance? Existing explanations in prior work suffer from notable limitations. First, qualitative hypotheses about the importance of structure or relevance of reasoning chains are not quantifiable (Wang et al., 2023a; Li et al., 2025). Consequently, these hypotheses are subject to interpretation, limiting both their predictive capacity and the ability to offer theoretically grounded explanation for what makes reasoning effective. On the other hand, prevalent quantitative measures are often associated with conflicting evidence. For example, the relationship between the length of reasoning trajectories and the subsequent increased inference-time computational capacity remains unclear; while some works find clear gains (Muennighoff et al., 2025; Li et al., 2025), other work reports that shorter chains can be more effective and that continuing to extend reasoning (e.g., via wait tokens) can yield degradation in performance (Wu et al., 2025; Marjanovic et al., 2025). Current approaches such as process reward models or correctness-based classifiers also require subjective specifications of desirable properties and do not provide principled measure of effectiveness. reliable quantitative measure would have significant practical implications: it could inform how to annotate or collect reasoning data, how to align reasoning strategies to particular student models, and how to design better regularizers that avoid limiting exploration or reward models grounded Effective Reasoning Chains Reduce Intrinsic Dimensionality in generalization principles rather than subjective criteria. To address this gap, we draw on the long-standing literature that uses information-theoretic perspectives to explain the efficacy of neural networks. Foundational concepts such as the minimum description length principle (Rissanen, 1978; Grunwald, 2007) posit an inverse relationship between the capacity required to represent solution and its expected generalization. Building on this, the notion of intrinsic dimensionality (Li et al., 2018; Aghajanyan et al., 2021) applies these insights for overparameterized models, measuring the effective number of parameters needed to fit given task objective. Specifically, intrinsic dimensionality is function of both the model and the task. While prior work has typically fixed the data to analyze how different models vary in their intrinsic dimensionality, we instead fix the model and vary the training data by changing the reasoning strategy used to generate solutions. Although the underlying capability required (e.g., solving math problems) remains constant, different reasoning strategies change the supervision provided to the model during training. In this context, one might intuitively expect that requiring model to generate long reasoning chains alongside final answers would increase the complexity of the outputs, making the task harder to fit. However, we hypothesize the opposite for effective reasoning: if reasoning strategy effectively bridges the logical gap between input and answer, it should render the underlying mapping more compressible, requiring fewer degrees of freedom to learn, thereby resulting in lower intrinsic dimensionality. We demonstrate strong inverse correlation between intrinsic dimensionality and generalization performance across multiple chain-of-thought variants on GSM8K (Cobbe et al., 2021). These findings hold for both Gemma-3 1B and 4B models (Gemma Team et al., 2025) on in-distribution and out-of-distribution evaluations. We compare intrinsic dimensionality against alternative metrics based on length of trajectories and likelihood under the student model, finding that intrinsic dimensionality provides substantially stronger predictive power. These findings provide principled, quantitative explanation for why different reasoning strategies improve generalization, and offer potential guidance for data annotation, model alignment, and training optimization. 2. Intrinsic Dimensionality of Reasoning 2.1. Background on Intrinsic Dimension The concept of intrinsic dimensionality formalizes the observation that many tasks can be learned in lower-dimensional subspaces than the full parameter space of overparameterized neural networks. Following Li et al. (2018); Aghajanyan et al. (2021), we can express the models parameters θ RD as: θ = θ0 + (θd), where θ0 represents the pretrained model parameters, is the total number of parameters in the model, θd Rd is lower-dimensional parameter vector with D, and : Rd RD is projection operator. By training only in this d-dimensional subspace, we can identify the minimum dimensions required to achieve target performance, which defines the intrinsic dimension of the task under given model. 2.2. Lower-Dimension Projection for LLMs While the original formulation uses random projections applied globally to all parameters, optimizing in such randomly projected spaces can be challenging and sub-optimal for larger models (Aghajanyan et al., 2021; Hu et al., 2022). Instead, we adopt the Low-Rank Adaptation framework (LoRA; Hu et al., 2022), which was itself motivated by the intrinsic dimension findings of Li et al. (2018) and Aghajanyan et al. (2021) and has proven effective for fine-tuning LLMs. LoRA targets specific weight matrices in the transformer architecture and constrains their updates to low-rank subspaces. For pretrained weight matrix W0 Rmn, LoRA represents the weight update as: = W0 + BA where Rmr and Rrn are learned low-rank matrices with rank min(m, n). During training, W0 remains frozen while and are optimized. LoRA can be applied to different subsets of weight matrices, including attention modules (Wq, Wk, Wv, Wo), MLP layers, or all transformer layers. The total number of trainable parameters, which we denote as params(.), is determined by the rank and the number of weight matrices: params(r, LLoRA) = 2 LLoRA dmodel where LLoRA is the number of weight matrices LoRA is applied to, dmodel is the models hidden dimension, and is the LoRA rank. This formulation aligns with intrinsic dimensionality by using LoRA as structured lowdimensional projection, constraining trainable capacity in manner that is both architecturally informed and empirically effective for LLM fine-tuning. 2.3. Measuring Intrinsic Dimension of Reasoning We measure intrinsic dimensionality as the minimum number of trainable parameters required to reach specified performance threshold. Formally, for task with performance metric A, the intrinsic dimension dint is: dint = min{d : A(d) τ } where = params(r, LLoRA) is the total number of trainable parameters, A(d) is the training accuracy achieved with this configuration, and τ is the performance threshold. 2 Effective Reasoning Chains Reduce Intrinsic Dimensionality Figure 1. Overview. Middle (Green): We calculate the intrinsic dimension of reasoning strategy as described in Section 2.3, and then compare how well its predicts the generalization performance of models trained with different reasoning strategies (top; c.f. Section 3). On the right, we demonstrate strong inverse correlation between intrinsic dimensionality and generalization performance (Section 4). To compute dint, we conduct sweep of LoRA configurations where we vary both the rank and which weight matrices receive LoRA adaptations (controlled by LLoRA). The configurations are chosen such that the resulting parameter counts params(r, LLoRA) are uniformly distributed in log scale. The lower bound of our sweep applies LoRA of rank = 1 to only the query and value projection matrices in the attention modules (Wq and Wv). The upper bound applies full-rank LoRA (r = dmodel) to all layers (both attention and MLP modules), for additional details we refer readers to Appendix A.1. For each configuration, we train the model and record the training accuracy at the completion of training, then identify the minimum parameter count at which training accuracy exceeds the threshold τ . Choice of Performance Threshold. Prior work on intrinsic dimension measurement sets τ as percentage (e.g., 90%) of the validation performance achieved with full finetuning for fixed model and training data (Li et al., 2018; Aghajanyan et al., 2021). However, our work differs crucially: we fix the model but implicitly vary the data by changing the reasoning strategy (CoTs) in the output space, resulting in different training datasets, one for each CoT strategy applied to the same set of problems. Different reasoning strategies may differ in the maximum attainable performance, making percentage-based thresholds incomparable. Rather than focusing on the absolute value of intrinsic dimensionality, we argue that the relative ordering of intrinsic dimensions across different reasoning strategies is what matters for understanding their effectiveness. Therefore, we use common threshold τ across all strategies to ensure fair comparison. We set τ based on either: (1) fixed validation accuracy threshold representing strong reasoning performance, or (2) the full-capacity training accuracy after one epoch (which avoids overfitting contamination), allowing us to compute the intrinsic dimension and thereby, identifying effective reasoning strategies entirely from training curves alone. Unless stated otherwise, we set τ based on the latter, computed as the maximum training accuracy achieved by any strategy at full capacity after one epoch of training. We evaluate each strategys sweep against this same threshold, identifying the minimum parameter count needed to reach τ , as illustrated in Figure 1; thus measuring how much capacity each reasoning approach requires to achieve the same level of capability. In Section 4.3, we justify this choice and demonstrate that conclusions drawn from intrinsic dimensions are generally robust to the exact choice of threshold, holding across wide range of thresholds. 3. Experimental Setup Datasets. We use the training split of the well-studied GSM8K dataset (Cobbe et al., 2021) comprising gradeschool level math word problems. To measure models abilities at solving word problems in general, we evaluate the trained models on the (i) in-domain test split of GSM8K, as well as several stress test sets that measure out-of-domain generalization (ii) GSM-Symbolic (Mirzadeh et al., 2025), (iii) GSM-IC (Shi et al., 2023), and (iv) GSM-Hard (Gao et al., 2023). Mirzadeh et al. (2025) propose GSM-Symbolic to test robustness of models on diverse set of questions sampled from symbolic perturbations to the questions phras3 Effective Reasoning Chains Reduce Intrinsic Dimensionality ing and varying difficulty via three different splits. Shi et al. (2023) find that performance of models on math word problems is diminished in the presence of irrelevant sentences in the question which have no bearing on the solution. Finally, Gao et al. (2023) measure the numerical robustness and the ability to solve word problems involving more complex arithmetic. We use the test split of the GSM8K dataset to measure the in-distribution (ID) performance, and report the geometric mean of the 5 stress test sets as the out-ofdistribution (OOD) performance. The overall performance is computed as the geometric mean across all the 6 test splits. We enumerate the size of various test splits in Appendix B. Reasoning Strategies. We evaluate intrinsic dimensionality across diverse set of reasoning strategies that vary in length, structure, and generation method. Our simplest baselines are No CoT, which outputs direct answer without intermediate reasoning (Sprague et al., 2025), and No CoT with extra tokens, which appends filler text to isolate the effect of inference-time computation from reasoning quality. Using Gemma-3 27B, we generate three naturallanguage CoT variants: Very Short CoT, prompted for concise, equation-style reasoning (Nye et al., 2022); Short CoT, restricted to brief (12 sentence) explanations; and Gemma 27B CoT, which allows unconstrained reasoning. In contrast, Gemini CoT is produced by stronger teacher model known for longer solutions (Gemini 2.5 Flash; Comanici et al., 2025). To study robustness to irrelevant information, we include Short CoT with Distractors (n 2, 4, 8), where unrelated steps sampled from other problems are inserted before reaching the correct answer (Li et al., 2025). We additionally evaluate codeand structurebased approaches, including Executed PoT with actual program execution (Gao et al., 2023; Chen et al., 2023), Simulated PoT relying on internal code simulation (Sprague et al., 2025), and Plan and Solve following decomposition framework (Wang et al., 2023b). Finally, we evaluate Critical CoT (Zhou et al., 2024), reasoning structure associated with critical-thinking strategies, and High Review Ratio CoT (Feng et al., 2025), with higher occurrences of revision tokens for longer and verification-based reasoning. We provide examples of each strategy in Appendix C. All strategies except Gemini CoT are generated by prompting instruction-tuned Gemma-3 27B for each reasoning style and filtering out generations with incorrect final answers (see Appendix B). Together, these strategies span broad range of lengths and structural properties, enabling us to test whether intrinsic dimensionality explains reasoning effectiveness beyond metrics such as trajectory length. To measure generalization performance of each strategy, we curate training dataset for each reasoning strategy based on the train split of GSM8K (see details in Appendix B) and finetune full-capacity model to generate rationale (as per one of the above strategies) followed by the final answer. Baseline Metrics. In addition to intrinsic dimensionality (unless mentioned otherwise, computed with threshold of 90% of maximum training accuracy attained by any strategy after the first epoch), we compare against several baseline metrics that do not require test-time evaluation to assess their ability to predict generalization performance: 1. Trajectory Length: Since longer responses increase inference-time computation and often correlate with better reasoning (e.g., via backtracking or verification) (Snell et al., 2024; Guo et al., 2025; Marjanovic et al., 2025), we measure the average token length of CoTs to test if length alone predicts effectiveness. 2. Token Perplexity: Recent work (Yue et al., 2025; Karan & Du, 2025; Zhang et al., 2025) suggests that proximity or overlap between the pretrained distribution and finetuning data affects learning effectiveness, i.e., models learn more effectively from reasoning chains that are in-distribution to the base model (Agarwal et al., 2024; Yue et al., 2025). We compute the average token-level perplexity of CoT training data for each strategy relative to the pretrained student model. 3. Sequence KL Divergence: To account for full-sequence probability rather than per-token averages, we estimate the KL divergence between the empirical data distribution ˆπ (uniform over the training samples) and the student models distribution πθ. Empricially, this is computed as the average sequence-level negative loglikelihood: 1 i=1 log πθ(y(i)x(i)). Unlike token perplexity, this metric is not normalized by sequence length, making the two metrics complementary measures of distribution alignment. (cid:80)N LoRA Training Hyperparameters. We fine-tune Gemma-3 base models (Gemma Team et al., 2025) of two sizes: 1B and 4B parameters, using consistent hyperparameters across all reasoning strategies. We train the 1B model for 8,000 steps (learning rate 103) and the 4B model for 6,000 steps (104); these parameters were set based on preliminary full-capacity training runs to ensure training accuracy fully plateaus. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) with training batch size of 8, evaluating on the validation set to select checkpoints for ID and OOD performance reporting. We sweep across = 20 (1B) and = 30 (4B) LoRA configurations, with parameter counts distributed uniformly in log space from minimum (rank 1 applied to query and value projection matrices only) to maximum (full rank applied to all attention and MLP layers). The best configuration for each parameter target is selected by minimizing the absolute difference between the target and actual trainable parameter count (c.f. Appendix A.1). 4 Effective Reasoning Chains Reduce Intrinsic Dimensionality Table 1. Performance of Gemma-3 4B across reasoning strategies. ID: Accuracy on GSM8K Test. OOD: Geometric mean of 5 stress test sets (GSM-Symbolic, GSM-IC, GSM-Hard). Overall: Geometric mean across all 6 splits (refer to Table 4 for breakdown of accuracy across splits). Spearman correlations are computed between each metric and overall accuracy; for metrics marked with , +ve correlations indicate that lower values successfully predict higher accuracy. Note: denotes statistical significance (p < 0.01). CoT Strategy ID OOD Overall Intrinsic Dim. (M) KL Div Token PPL Length Baseline Strategies No CoT No CoT with extra tokens Short CoT Variants Very Short CoT Short CoT Short CoT with 2 Distractors Short CoT with 4 Distractors Short CoT with 8 Distractors 14.94 16. 6.20 7.15 7.18 8.22 44.58 58.98 50.11 41.32 22.97 19.22 28.99 23.09 18.74 10.21 Default CoTs Sampled from Teacher Model Gemma 27B CoT Gemini CoT 67.48 66. 38.24 35.46 Specific Reasoning Strategies from Prior Work Executed PoT Simulated PoT Plan Solve Critical CoT High Review Ratio CoT 62.77 64.75 64.75 63.84 67.63 43.40 35.13 34.33 33.74 40.10 Spearman Rank Correlation - - 22.11 32.63 26.27 21.38 11.69 42.04 39.40 46.15 38.90 38.16 37.52 43.75 - 5246.16 2729. 532.81 3.92 532.81 2729.64 1968.84 2.05 2.55 1.49 1.49 2.05 104.12 2.05 0.93 45.46 123.00 89.69 116.31 494.42 775.23 1315. 162.42 571.86 131.30 257.91 250.16 924.26 727.69 -0.17 91.34 59.06 6.84 2.73 3.43 3.12 2.86 1.84 1. 1.77 1.67 1.77 3.07 2.63 0.82 9.31 23.31 42.63 93.53 289.97 485.11 878.82 221.95 650.72 188.79 388.11 333.04 591.39 547. 0.31 4. Main Results and Analysis more compressible task representations. 4.1. Intrinsic Dimension with Gemma-3 4B Setup. The goal of our study is to measure the extent to which intrinsic dimensionality and other baselines are predictive of the generalization performance of Gemma-3 4B under different reasoning strategies. To this end, we compute the Spearman rank correlation between the average performance (including in-distribution as well as out-ofdistribution datasets) and each metric as reported in Table 1. For metrics where smaller values theoretically indicate better learnability (Intrinsic Dimension, KL Divergence, Token Perplexity), we compute correlation between increasing metric values and decreasing average accuracy, such that positive correlations indicate successful prediction in the theoretically expected direction; for instance, correlation of 0.93 for Intrinsic Dimension means strategies with lower intrinsic dimensionality achieve higher accuracy. For Length, we report standard correlation with average accuracy. Intrinsic Dimension Strongly Predicts Reasoning Effectiveness. Table 1 presents our main results across 14 reasoning strategies on the Gemma 4B model. Intrinsic dimensionality exhibits the strongest correlation with generalization performance, achieving Spearman rank correlation of 0.93 with average accuracy substantially higher than all baseline metrics. This demonstrates that effective reasoning chains require significantly fewer parameters to learn, supporting our hypothesis that such chains help models learn Length and KL Divergence Show Poor Predictive Power. In contrast, length shows weak correlation (0.31) with reasoning effectiveness, which is unsurprising given conflicting evidence in prior work (Feng et al., 2025; Snell et al., 2024; Wu et al., 2025) showing that depending on the task, there may be an optimal reasoning length beyond which performance worsens. Similarly, KL divergence exhibits weak or negative correlation (-0.17). This can be explained by noting that KL divergence directly measures the cost in bits to encode the training data (CoT + answer) using the model (Blier & Ollivier, 2018). While this metric is effective for comparing different models on the same task and output space, it is prohibitive for comparing effectiveness across reasoning strategies: irrespective of how easy the trajectory is to encode in terms of likelihood, its length unduly influences the divergence, preferring shorter lengths and leading to no meaningful correlation. Connection between Token Perplexity and Intrinsic Dimensionality. In Table 1, token perplexity achieves correlation of 0.82, though still lower than intrinsic dimensionalitys 0.93. One way of understanding this relationship is that the two metrics are potentially interrelated: reasoning chains that exhibit high likelihood and low surprisal under the base model are likely also readily compressible with fewer parameters that need to be altered. This is consistent with the findings in Yue et al. (2025), which shows that even after reinforcement-learning, the reasoning chains of 5 Effective Reasoning Chains Reduce Intrinsic Dimensionality Table 2. Performance of Gemma-3 1B across reasoning strategies. ID: Accuracy on GSM8K Test. OOD: Geometric mean of 5 stress test sets (GSM-Symbolic, GSM-IC, GSM-Hard). Overall: Geometric mean across all 6 splits (refer to Table 5 for breakdown of accuracy across splits). Spearman correlations are computed between each metric and overall accuracy; for metrics marked with , +ve correlations indicate that lower values successfully predict higher accuracy. Note: denotes statistical significance (p < 0.05). CoT Strategy ID OOD Overall Intrinsic Dim. (M) KL Div Token PPL Length Baseline Strategies No CoT No CoT with extra tokens Short CoT Variants Very Short CoT Short CoT Short CoT with 2 Distractors Short CoT with 4 Distractors Short CoT with 8 Distractors 3.56 5. 8.95 18.04 10.46 4.78 2.43 2.00 2.00 4.00 7.00 5.00 3.00 1.00 Default CoTs Sampled from Teacher Model Gemma 27B CoT Gemini CoT 20.40 20.55 7.00 8. Specific Reasoning Strategies from Prior Work Executed PoT Simulated PoT Plan Solve Critical CoT High Review Ratio CoT 20.24 20.85 21.53 17.51 22.60 11.00 8.00 10.00 8.00 9.00 Spearman Rank Correlation - - 1.84 2.38 4.39 8.68 5.44 2.91 1.57 8.27 9.73 11.76 8.98 11.24 9.11 10.57 - 119.38 83. 31.45 1.03 7.34 31.45 134.93 7.34 31.45 1.03 7.34 7.34 31.45 7.34 0.75 58.74 148.07 136.15 191.41 662.68 1028.70 1740. 286.24 854.37 247.16 431.72 432.53 1318.52 1042.57 -0.18 236.16 121.88 15.78 4.89 5.16 4.49 4.01 2.82 2. 2.83 2.31 2.63 4.91 3.95 0.63 9.31 23.31 42.63 93.53 289.97 485.11 878.82 221.95 650.72 188.79 388.11 333.04 591.39 547. 0.24 the trained model still exhibit decreased perplexity under the base model, suggesting that effective reasoning remains grounded in the base models distribution. Table 3. Robustness to threshold selection for computing intrinsic dimension using epoch 1 training accuracy at 70%, 80%, and 90% of the maximum achieved by any strategy, and 90% of validation accuracy by any strategy. 4.2. Intrinsic Dimension with Gemma-3 1B Table 2 presents results for the Gemma 1B model across the same 14 reasoning strategies. Despite the 1B model achieving substantially lower absolute performance than the 4B model, intrinsic dimensionality maintains strong correlation of 0.75 with generalization performance. This demonstrates that the predictive power of intrinsic dimensionality holds even when the performance ceiling is significantly lower, suggesting the metric captures fundamental properties of reasoning strategy effectiveness that are independent of model scale. Token perplexity remains the second-best predictor with correlation of 0.63, while length (0.24) and KL divergence (-0.18) continue to show poor predictive power. This is consistent with our findings on Gemma-3 4B that intrinsic dimensionality outperforms other baselines. 4.3. Robustness to Threshold Selection Setup. Recall that in Sections 2.3 and 3, we compute intrinsic dimensionality by identifying the minimum parameter count needed to reach common threshold τ across all reasoning strategies. For the main results presented in Tables 1 and 2, we use the maximum full-capacity training accuracy after one epoch across reasoning strategies at 90% as our threshold, following prior work in computThreshold Selection Correl: 1B Correl: 4B 90% of Epoch 1 Train Acc 80% of Epoch 1 Train Acc 70% of Epoch 1 Train Acc 90% of Val Acc 0.75 0.72 0.73 0.73 0.93 0.94 0.93 0. ing intrinsic dimension for other domains (Li et al., 2018; Aghajanyan et al., 2021). However, critical question remains: are our findings dependent on particular golden or ad hoc choice of threshold? To address this concern, we evaluate the robustness of intrinsic dimensionalitys predictive power across different threshold selection methods and threshold levels. Even if individual intrinsic dimension measurements become noisier with different thresholds, we expect the relative ordering of reasoning strategies, and thus, the correlation with performance, to remain stable if intrinsic dimensionality reliably measures the effectiveness of different reasoning strategies. Strong Correlations Persist Across Thresholds. Table 3 demonstrates that our findings are remarkably robust to threshold selection. Across both the 1B and 4B models, intrinsic dimensionality maintains strong correlations (ranging from 0.72 to 0.94) whether we use epoch 1 training accuracy at 70%, 80%, or 90% thresholds, or validation 6 Effective Reasoning Chains Reduce Intrinsic Dimensionality Figure 2. Visualization of intrinsic dimension computation for Gemma-3 4B showing select reasoning strategies. We plot the Pareto frontier of monotonic training accuracy versus trainable parameters (log scale). The dashed line indicates the threshold (τ = 63.0%); intrinsic dimension is the parameter count where each curve first crosses this threshold (vertical dotted lines). Strategies crossing earlier have lower intrinsic dimensionality and tend to yield higher overall performance (cf. Table 1). accuracy at 90%. The consistency across threshold levels confirms that the predictive power of intrinsic dimensionality is not an artifact of hyperparameter tuning. Notably, using epoch 1 training accuracy offers practical advantage: it enables computing intrinsic dimensionality entirely from training curves without requiring validation set evaluation, making the metric more accessible (e.g., when most of the data is used for training, or out-of-distribution testing is not feasible) and reducing computational overhead while maintaining strong predictive performance. Additionally, it avoids contamination in the threshold selection from overfitting or memorization that occurs at later training stages. 4.4. Additional Analysis As mentioned in Section 2.3, to compare different reasoning strategies fairly, we enforce common absolute accuracy threshold across all strategies. In Figures 2 and 3 this corresponds to τ = 63.0% for 4B and τ = 24.3% for 1B models . We derive τ from the maximum training accuracy achieved by any strategy after the first epoch. This choice is critical: while final training accuracy often conflates generalizable learning with rote memorization (overfitting), performance after the first epoch effectively isolates the learnability of the reasoning structure. Consequently, our reported thresholds represent realistic gauge of generalization capability, explaining the gap between the thresholds for 1B and 4B models. Executed PoT Achieves Lowest Intrinsic Dimensionality and Best OOD Performance. Figures 2 and 3 visualize the intrinsic dimensionality computation by plotting the Pareto frontier of training accuracy as function of parameter count for select reasoning strategies. Examining these curves and the results in Tables 1 and 2, we observe that Executed PoT consistently achieves both the lowest intrinsic dimensionality and the strongest out-of-distribution performance across both model sizes. For the 4B model (Figure 2), Exec PoT crosses the 63.0% threshold at only 1.49M parameters substantially lower than other strategies shown: Short CoT (3.92M), Gemma 27B CoT (2.05M), Very Short CoT (532.81M), and even the No CoT baseline (at full capacity or rank), while achieving the highest OOD accuracy of 43.40%  (Table 1)  . This finding aligns with Sprague et al. (2025), who demonstrate that python code + interpreter solutions often outperform CoT in zero-shot settings across multiple models. The combination of low intrinsic dimensionality and strong generalization suggests that code-based reasoning with execution provides particularly compressible and robust representation of our task. Larger Models Compress Effective Reasoning Strategies More Efficiently. Comparing intrinsic dimensionality across model sizes in Tables 1 and 2 reveals an interesting pattern: for effective reasoning strategies, the 4B model exhibits lower intrinsic dimensionality than the 1B model despite having larger parameter space. For instance, comparing the training accuracy curves in Figures 2 and 3, Executed PoT requires 1.49M parameters for the 4B model versus 1.03M for the 1B model, but critically, the 4B model achieves this at much higher absolute performance level (maximum accuracy of 46.15% vs 11.76%, and threshold of 63.0% vs 24.3%). Similarly, Gemma 27B CoT and other effective strategies show comparable or lower intrinsic dimensionality on the 4B model relative to their task complexity. This demonstrates that larger models, despite being more overparameterized, are better compressors of effective reasoning tasks consistent with findings in existing intrinsic Effective Reasoning Chains Reduce Intrinsic Dimensionality Figure 3. Visualization of intrinsic dimension computation for Gemma-3 1B showing select reasoning strategies. We plot the Pareto frontier of monotonic training accuracy versus trainable parameters (log scale). The dashed line indicates the threshold (τ = 24.3%); intrinsic dimension is the parameter count where each curve first crosses this threshold (vertical dotted lines). dimensionality work (Aghajanyan et al., 2021) showing that larger networks learn more efficient representations. Ineffective Reasoning Strategies Reveal Higher Intrinsic Dimensionality in Larger Models. Interestingly, the pattern reverses for less effective reasoning strategies. As shown in Tables 1 and 2, strategies like No CoT, No CoT with extra tokens, and Very Short CoT with multiple distractors exhibit considerably higher intrinsic dimensionality on the 4B model compared to the 1B model. For example, Very Short CoT requires over 500M parameters for Gemma-3 4B model but only 31M on the 1B model, as visible in the delayed threshold crossing in Figure 2 vs. Figure 3. This suggests that when reasoning chains provide little structure or contain substantial noise (as with distractors), larger models require disproportionately more capacity to fit these less compressible patterns, while smaller models may more readily resort to simpler memorization strategies that require lesser deviation from the base model. 5. Related Work Evaluation and Analysis of Reasoning. Recent work has aimed to disentangle the factors driving the efficacy of Chain-of-Thought (CoT) reasoning, revealing that performance gains are highly task-dependent often providing limited benefits for knowledge-intensive tasks (Sprague et al., 2025) and are driven more by the structural coherence of the reasoning template than by local numerical precision (Wang et al., 2023a; Li et al., 2025). However, many of these explanations are not quantifiable measures, and for those that are quantifiable, our meta-evaluation in Section 4 shows that intrinsic dimensionality offers stronger correlation and is more predictive of downstream gains in reasoning performance of the target model. parallel line of work has developed various proxies for selecting highquality reasoning chains, ranging from simple heuristics like ensemble-based agreement (Wang et al., 2023c), to those based on reasoning length and review tokens (Feng et al., 2025), and finer-grained linguistic or information-theoretic scores (Golovneva et al., 2023; Prasad et al., 2023). However, these metrics primarily serve as proxies for instancelevel correctness rather than explaining what makes reasoning strategy effectively learnable. In contrast, we show that intrinsic dimensionality grounded in concept of minimum description length provides principled explanation: effective reasoning chains reduce the capacity required to fit the task, enabling better generalization. Intrinsic Dimension of Neural Networks. In the context of deep learning, Li et al. (2018) and Aghajanyan et al. (2021) proposed the concept of intrinsic dimensionality, quantifying the minimum degrees of freedom required to optimize network for specific objective. This line of research demonstrated that large, pretrained models possess remarkably low intrinsic dimensions, which directly motivated parameter-efficient fine-tuning methods such as LoRA (Hu et al., 2022). In this work, we invert this experimental setup: rather than varying the model to study pre-training quality, we vary the LoRA rank to empirically measure the intrinsic dimension of different reasoning strategies. We distinguish our method from measuring the intrinsic dimension of the data manifold itself (Levina & Bickel, 2004; Tenenbaum et al., 2000). While the latter estimates 8 Effective Reasoning Chains Reduce Intrinsic Dimensionality the complexity of the training data and which scales with dataset size, we estimate the complexity of the learning objective itself, involving both the model and the reasoning task. We posit that effective reasoning strategies simplify the underlying rule connecting inputs to answers, enabling the model to fit the task within lower-dimensional subspace despite the increased length of the output. This notion of intrinsic dimensionality is related to the Minimum Description Length (MDL) principle (Rissanen, 1978; Grunwald, 2007; Hinton & Van Camp, 1993), which frames learning as data compression. If accuracy is held constant, then the MDL principle suggestions that the best model is the one with the shortest description length. 6. Discussion and Conclusion We establish that the generalization performance of model trained on given set of reasoning chains is correlated with the degree to which the reasoning chains reduce the intrinsic dimensionality of the given task. This offers new perspective on why chain-of-thought reasoning can improve generalization, grounded in information theory and the minimum description length (MDL) principle: effective reasoning chains reduce the conditional complexity of learning new task. This measure of intrinsic dimensionality correlates better with generalization performance than alternatives such as perplexity and length. Notably, while measures such as perplexity are computed over individual trajectories and then aggregated, intrinsic dimensionality potentially captures coherence across the complete set of trajectories, and future work could investigate the importance of this aspect further. While our study focuses on fine-tuning models given reasoning trajectories, future work could explore other post-training settings (e.g., Zelikman et al., 2022; Agarwal et al., 2024). From practical perspective, our method to estimate intrinsic dimensionality requires fine-tuning adapters of various sizes, and therefore this measure would be computationally expensive to optimize for directly. Future work could potentially build on these insights to explore more computationally tractable alternatives for identifying effective reasoning chains that enable greater generalization."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank Jacob Eisenstein, and Kristina Toutanova for their valuable feedback on early drafts of this work. Part of this work was done during an internship at Google DeepMind. This work was partially supported by NSFAI Engage Institute DRL2112635, NSF-CAREER Award 1846185, DARPA ECOLE Program No. HR00112390060, and an Apple PhD Fellowship. The views contained in this article are of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Garea, S. R., Geist, M., and Bachem, O. On-policy distillation of language models: Learning from self-generated mistakes. In The twelfth international conference on learning representations, 2024. Aghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: long papers), pp. 7319 7328, 2021. Blier, L. and Ollivier, Y. The description length of deep learning models. Advances in Neural Information Processing Systems, 31, 2018. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Feng, Y., Kempe, J., Zhang, C., Jain, P., and Hartshorn, A. What characterizes effective reasoning? revisiting length, review, and structure of cot. arXiv preprint arXiv:2509.19284, 2025. Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799. PMLR, 2023. 9 Effective Reasoning Chains Reduce Intrinsic Dimensionality Gemma Team, Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Giles, H., Coupland, N., and Coupland, J. Accommodation theory: Communication, context, and consequence. Contexts of accommodation: Developments in applied sociolinguistics, 1:168, 1991. Golovneva, O., Chen, M. P., Poff, S., Corredor, M., Zettlemoyer, L., Fazel-Zarandi, M., and Celikyilmaz, A. ROSCOE: suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. Grunwald, P. D. The minimum description length principle. MIT press, 2007. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hinton, G. E. and Van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 513, 1993. Hu, E. J. et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. Karan, A. and Du, Y. Reasoning with sampling: Your base model is smarter than you think. arXiv preprint arXiv:2510.14901, 2025. Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations, 2023. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Levina, E. and Bickel, P. Maximum likelihood estimation of intrinsic dimension. Advances in neural information processing systems, 17, 2004. Li, D., Cao, S., Griggs, T., Liu, S., Mo, X., Tang, E., Hegde, S., Hakhamaneshi, K., Patil, S. G., Zaharia, M., et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations, 2019. Marjanovic, S. V., Patel, A., Adlakha, V., Aghajohari, M., BehnamGhader, P., Bhatia, M., Khandelwal, A., Kraft, A., Krojer, B., L`u, X. H., et al. Deepseek-r1 thoughtology: Lets think about llm reasoning. arXiv preprint arXiv:2504.07128, 2025. Mirzadeh, S. I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S., and Farajtabar, M. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. In The Thirteenth International Conference on Learning Representations, 2025. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022. Pickering, M. J. and Garrod, S. Toward mechanistic psychology of dialogue. Behavioral and brain sciences, 27(2):169190, 2004. Prasad, A., Saha, S., Zhou, X., and Bansal, M. Receval: Evaluating reasoning chains via correctness and informativeness. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1006610086, 2023. Rissanen, J. Modeling by shortest data description. Automatica, 14(5):465471, 1978. Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Scharli, N., and Zhou, D. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210 31227. PMLR, 2023. Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 10 Effective Reasoning Chains Reduce Intrinsic Dimensionality International Conference on Learning Representations, 2023. Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le, Q. V., Chi, E., Zhou, D., Mishra, S., and Zheng, H. S. Selfdiscover: Large language models self-compose reasoning structures. Advances in Neural Information Processing Systems, 37:126032126058, 2024. Sprague, Z. R., Yin, F., Rodriguez, J. D., Jiang, D., Wadhwa, M., Singhal, P., Zhao, X., Ye, X., Mahowald, K., and Durrett, G. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. In The Thirteenth International Conference on Learning Representations, 2025. Tenenbaum, J. B., Silva, V. d., and Langford, J. C. global geometric framework for nonlinear dimensionality reduction. science, 290(5500):23192323, 2000. Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pp. 27172739, 2023a. Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26092634, 2023b. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023c. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, Y., Wang, Y., Ye, Z., Du, T., Jegelka, S., and Wang, Y. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zhang, C., Neubig, G., and Yue, X. On the interplay of pre-training, mid-training, and rl on reasoning language models. arXiv preprint arXiv:2512.07783, 2025. Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V., and Chi, E. H. Least-to-most prompting enables complex In The Eleventh reasoning in large language models. Effective Reasoning Chains Reduce Intrinsic Dimensionality A. Additional Details on Computing Intrinsic Dimension A.1. LoRA Sweeps for Computing Intrinsic Dimensions To compute intrinsic dimensionality, we conduct sweep over LoRA configurations with parameter counts uniformly distributed in log scale. The key challenge is that different combinations of rank and target modules LLoRA can yield similar parameter counts but potentially different performance. To address this, we employ greedy search procedure that selects the configuration minimizing the absolute error between the target and actual parameter count for each point in our sweep. We define four groups of target modules that represent increasing levels of model adaptation: attention v: Query and value projections only (Wq, Wv) attention all: All attention projections (Wq, Wv, Wk, Wo) mlp all: All MLP layers (Wgate, Wup, Wdown) all layers: All attention and MLP layers combined Configuration Selection Algorithm. Given set of target parameter counts {d1, d2, . . . , dk} distributed uniformly in log scale from dmin to dmax (c.f. Section 2.3), we select configurations as follows: 1. For each target parameter count di: (a) For each module group {attention v, attention all, mlp all, all layers}: Calculate parameters per rank: αg = params(1, g) Estimate required rank: rest = di/αg Clip rank to valid range: = max(1, min(rest, dmodel)) Calculate actual parameters: dactual = params(r, g) Compute error: ϵ = dactual di (b) Select configuration (r, g) that minimizes ϵ 2. Store configuration with actual parameter count dactual, this naturally handles collisions by keeping only one configuration per unique parameter count. Hyperparameter Selection and Convergence. To ensure the robustness of our training configuration, we conducted preliminary experiments to rigorously determine the optimal training duration and learning rate. We extended training runs up to 15,000 steps to empirically identify the point of convergence, observing that training accuracy and loss consistently plateaued well before our selected limits (8,000 steps for 1B and 6,000 steps for 4B). Additionally, we performed comprehensive learning rate sweep over logarithmic scale ranging from 1 102 to 1 106 (evaluating intermediate steps such as 1 102, 5 103, 1 103, . . . , 1 106). The final learning rates reported in the main text were selected based on the optimal balance of training stability and validation performance observed during this sweep. B. Size of Training and Test Splits Evaluation Splits. We evaluate all models on six test splits spanning both in-distribution (ID) and out-of-distribution (OOD) settings. The in-distribution evaluation uses the GSM8K test set, which contains 1.32K instances (Cobbe et al., 2021).1 Out-of-distribution evaluations include several GSM-based variants designed to stress different generalization axes: (i) GSM Symbolic (Main) (Mirzadeh et al., 2025),2 consisting of 5K instances generated from distinct symbolic template variations; (ii) GSM Symbolic P1 (Mirzadeh et al., 2025),3 higher-difficulty symbolic split with 5K instances; and (iii) GSM Symbolic P2 (Mirzadeh et al., 2025),4 the most challenging symbolic split, containing 2.5K instances. We additionally evaluate on GSM-IC (Shi et al., 2023),5 for which we sample 5K instances from the m-step dataset augmented with irrelevant contextual information, and on GSM-Hard (Gao et al., 2023),6 which contains 1.32K instances featuring more challenging arithmetic. Together, these splits enable systematic assessment of generalization across symbolic structure, difficulty, and robustness to distractors. 1Source: https://huggingface.co/datasets/openai/gsm8k/viewer/main/test 2Source: https://huggingface.co/datasets/apple/GSM-Symbolic/ 3Source: https://huggingface.co/datasets/apple/GSM-Symbolic/viewer/p1 4Source: https://huggingface.co/datasets/apple/GSM-Symbolic/viewer/p2 5Source: https://github.com/google-research-datasets/GSM-IC/blob/main/GSM-IC_mstep.json 6Source: https://huggingface.co/datasets/reasoning-machines/gsm-hard 12 Effective Reasoning Chains Reduce Intrinsic Dimensionality Training Splits across Reasoning Strategies. As stated in Section 3, we use the GSM8K (Cobbe et al., 2021) training set as the source of questions, which are solved using different reasoning strategies. We prompt teacher model (Gemini-2.5 or Gemma-3 27B) to generate these solutions using its instruction-following capabilities and filter the outputs for correctness. This yields between 6.2K and 7.5K valid training instances per strategy, with No CoT and No CoT with extra tokens retaining the full 7.5K examples, most reasoning-based CoT variants producing approximately 7.07.2K instances, and more constrained formats such as Very Short CoT and Executable PoT resulting in smaller splits (6.7K and 6.2K instances, respectively).Crucially, we find that the variation in training set size does not account for the performance differences between strategies. We observe no statistically significant correlation (Spearman Rank) between the number of training samples and downstream accuracy (ρ 0.11, > 0.7 for both 1B and 4B models). Notably, Executed PoT achieves the highest generalization performance despite having the fewest training examples (6.2K), while the No CoT baseline performs poorly despite maximizing the dataset size (7.5K). This confirms that the content of the reasoning data, rather than the number of training examples, are the primary drivers of learnability. C. Examples of Different Reasoning Strategies Below are examples of the reasoning strategies evaluated in this work. All examples use the same base question to illustrate the differences in generation structure. No CoT (Sprague et al., 2025) Outputs direct answer without intermediate reasoning. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Answer: 72."
        },
        {
            "title": "No CoT with Extra Tokens",
            "content": "Appends filler text to isolate the effect of inference-time computation from reasoning quality. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Thinking about the answer to this math word problem and formatting it ... Answer: 72. Very Short CoT (Nye et al., 2022) Prompted for concise, equation-style reasoning. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: April sales plus May sales (48 + 48/2) equals total sales. Answer: 72 13 Effective Reasoning Chains Reduce Intrinsic Dimensionality"
        },
        {
            "title": "Short CoT",
            "content": "Restricted to brief (12 sentence) explanations. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Natalia sold half of 48 clips in May, which is 48/2 = 24 clips, and then added that to the 48 clips sold in April for total of 48 + 24 = 72 clips. Answer: 72 Short CoT with = 4 Distractors (Li et al., 2025) Unrelated steps sampled from other problems are inserted before reaching the correct answer to test robustness. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Let Chases speed be c, Camerons speed be 2c, and Danielles speed be 3(2c) = 6c. Since time = distance/speed, and the distance is the same for all three, we can set up proportion: 30 minutes / (6c) = t/c, where is the time it takes Chase to travel the distance; solving for t, we get = 30 c/(6c) = 5 30 = 180 minutes. But wait, this is wrong. Let me try again. First, we find Joshua made 20 + 6 = 26 macarons, and Miles made 26 2 = 52 macarons; then, Renz made (3/4) 52 1 = 39 1 = 38 macarons, so the total number of macarons is 20 + 26 + 52 + 38 = 136, and 136/2 = 68 kids would receive 2 macarons each. But wait, this is wrong. Let me try again. First, subtract the unripe apples from the total to find the number of ripe apples: 34 6 = 28 ripe apples. Then, divide the number of ripe apples by the number of apples needed per pie to find the number of pies they can make: 28/4 = 7 pies. But wait, this is wrong. Let me try again. First, calculate the total points needed to tie the record: 287 points/player/round 4 players 10 rounds = 11,480 points. Then, determine the points needed in the final round: 11, 480 10, 440 = 1, 040 points, and divide by the number of players to find the required average: 1, 040/4 = 260 points/player. Finally, find the difference between the record average and the needed average: 287 260 = 27 points. But wait, this is wrong. Let me try again. Natalia sold half of 48 clips in May, which is 48/2 = 24 clips, and then added that to the 48 clips sold in April for total of 48 + 24 = 72 clips. Answer: 72 14 Effective Reasoning Chains Reduce Intrinsic Dimensionality Gemma 27B CoT Allows unconstrained reasoning (generated by Gemma-3 27B). Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Let the number of clips Natalia sold in April be A. Let the number of clips Natalia sold in May be . We are given that Natalia sold clips to 48 of her friends in April, so = 48. We are also given that she sold half as many clips in May as she did in April, so = 1 2 (48) = 24. The total number of clips Natalia sold in April and May is + = 48 + 24. 48 + 24 = 72. Therefore, Natalia sold 72 clips altogether in April and May. 2 A. Since = 48, we have = 1 Answer: 72 Plan and Solve (Wang et al., 2023b) Follows decomposition framework to plan before executing. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Heres step-by-step plan to solve the problem: 1. Find the number of clips sold in May: The problem states Natalia sold half as many clips in May as she did in April. We need to calculate half of the number of clips sold in April. 2. Find the total number of clips sold: Add the number of clips sold in April to the number of clips sold in May to find the total number of clips sold over the two months. Now, lets execute the plan: Step 1: Find the number of clips sold in May Natalia sold 48 clips in April. She sold half as many in May, so we need to calculate 48 / 2. 48/2 = 24 clips Step 2: Find the total number of clips sold Clips sold in April: 48 Clips sold in May: Total clips sold: 48 + 24 = 72 Answer: 72 15 Effective Reasoning Chains Reduce Intrinsic Dimensionality Executed PoT (Gao et al., 2023; Chen et al., 2023) Uses actual program execution to derive the answer. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: def solution(): # Natalia sold clips to 48 friends in April. april_sales = 48 # She sold half as many clips in May. may_sales = april_sales / 2 # Calculate the total sales in April and May. total_sales = april_sales + may_sales return int(total_sales) Simulated PoT (Sprague et al., 2025) Relies on internal code simulation (reasoning about code) rather than external execution. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: # Natalia sold clips to 48 friends in April. april_sales = 48 # She sold half as many clips in May. may_sales = april_sales / 2 # Calculate the total sales in April and May. total_sales = april_sales + may_sales # Print the total sales. print(total_sales) Simulation of Execution: 1. april sales is assigned the value 48. 2. may sales is calculated as april sales / 2, which is 48/2 = 24. 3. total sales is calculated as april sales + may sales, which is 48 + 24 = 72. 4. The value of total sales (72) is printed. Answer: 16 Effective Reasoning Chains Reduce Intrinsic Dimensionality High Review Ratio CoT (Feng et al., 2025) Features higher occurrences of revision tokens for longer and verification-based reasoning. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Okay, lets break down this problem step-by-step. Natalia sold clips to 48 friends in April. In May, she sold half as many. We need to find the total number of clips sold in both months. First, lets find out how many clips she sold in May. Half of 48 is 48/2. Let me double check that division. 48 divided by 2 is indeed 24. So, she sold 24 clips in May. Now, we need to find the total number of clips sold in April and May. That means we need to add the number of clips sold in April (48) to the number of clips sold in May (24). So, 48 + 24. Lets verify this addition. 48 + 20 = 68, and 68 + 4 = 72. Therefore, Natalia sold total of 72 clips in April and May. Wait, the problem states she sold clips to her friends. Does that mean each friend bought one clip? It seems reasonable to assume that each friend purchased one clip, so the number of friends is equal to the number of clips sold. Let me verify this assumption. The problem doesnt explicitly state this, but its the most logical interpretation. Lets go back and review the calculations. April: 48 clips. May: 48/2 = 24 clips. Total: 48 + 24 = 72 clips. should check the addition one more time to be absolutely sure. 48 + 24 = 72. Yes, thats correct. Answer: 72 Effective Reasoning Chains Reduce Intrinsic Dimensionality Critical CoT (Zhou et al., 2024) Uses reasoning structure associated with critical-thinking strategies (deconstruction, questioning assumptions). Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Okay, lets break down this problem using both critical and reflective thinking. 1. Understanding the Problem (Critical Thinking - Deconstruction) The problem states Natalia sold clips to specific number of friends in April (48). It then tells us she sold relative amount in May half as many as in April. The question asks for the total number of clips sold across both months. The key here is understanding half as many and recognizing that we need two steps: calculate Mays sales, then add April and Mays sales together. 2. Identifying Assumptions (Critical Thinking - Questioning) Assumption: The problem implies Natalia sold one clip to each friend. This is crucial assumption. If she sold multiple clips to each friend, the problem would be unsolvable without more information. Well proceed assuming one clip per friend. Assumption: The problem is dealing with whole numbers of clips. Its unlikely Natalia sold half clip. 3. Solving the Problem (Critical Thinking - Logical Reasoning) Step 1: Calculate Mays sales. Mays sales = Aprils sales / 2 Mays sales = 48/2 = Step 2: Calculate total sales. Total sales = Aprils sales + Mays sales Total sales = 48 + 24 = 72 4. Reflective Thinking - Checking for Reasonableness & Alternative Approaches Reasonableness Check: Does the answer make sense? Natalia sold almost 50 clips in April, and roughly half that in May. total of 72 seems plausible. Alternative Approach: We could have thought of Mays sales as fraction of the total. If May is 1/2 of April, then May is 1 part and April is 2 parts... total sales is 3 24 = 72. This confirms our initial approach. 5. Final Answer Answer: 72 18 Effective Reasoning Chains Reduce Intrinsic Dimensionality Gemini CoT (Gemini 2.5 Flash; Comanici et al., 2025) Produced by stronger teacher model known for longer solutions. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Model Output: Heres my thinking process to solve this problem: 1. Understand the Goal: The question asks for the total number of clips sold in April and May. 2. Break Down the Information: Clips sold in April: 48 Clips sold in May: Half as many as April. 3. Calculate Clips Sold in May: Half as many as April means Aprils number divided by 2. May clips = 48/2 May clips = 24 4. Calculate Total Clips Sold: Total = Clips in April + Clips in May Total = 48 + 24 5. Perform the Addition: 48 + 24 = 6. Formulate the Answer: Present the steps clearly and state the final answer in the requested format. Heres how to solve the problem: 1. Clips sold in April: Natalia sold 48 clips in April. 2. Clips sold in May: She sold half as many clips in May as in April. To find half, we divide by 2: 48 clips/2 = 24 clips 3. Total clips sold: To find the total, we add the clips sold in April and May: 48 clips (April) + 24 clips (May) = 72 clips Answer: 72 D. Detailed Results across all Test Splits 19 Effective Reasoning Chains Reduce Intrinsic Dimensionality Table 4. Detailed Performance of Gemma-3 4B across test splits. Symb: GSM-Symbolic; P1/P2: Symbolic Perturbations; IC: GSM-IC; Hard: GSM-Hard. OOD: Geometric Mean of the 5 stress tests. Overall: Geometric Mean of all 6 splits. CoT Strategy GSM8K Symb P1 P2 IC Hard OOD Overall ID Out-of-Distribution (Stress Tests) Aggregates Baseline Strategies No CoT No CoT with extra tokens Short CoT Variants Very Short CoT Short CoT Short CoT with 2 Distractors Short CoT with 4 Distractors Short CoT with 8 Distractors 14.94 16. 44.58 58.98 50.11 41.32 22.97 Default CoTs Sampled from Teacher Model Gemma 27B CoT Gemini CoT 67.48 66.72 7.76 7.64 3.70 4.16 2.25 2. 34.62 48.64 4.09 4.09 6.20 7.15 7.18 8.22 33.00 49.36 39.32 31.74 18.90 16.70 28.54 19.90 15.98 8. 6.12 10.80 7.24 5.48 2.64 53.64 66.70 66.72 56.28 33.02 14.48 20.17 17.36 14.78 8.19 19.22 28.99 23.09 18.74 10.21 22.11 32.63 26.27 21.38 11.69 56.82 56. 42.20 39.60 16.36 15.32 78.56 66.66 26.54 24.72 38.24 35.46 42.04 39. Specific Reasoning Strategies from Prior Work Executed PoT Simulated PoT Plan Solve Critical CoT High Review Ratio CoT 62.77 64.75 64.75 63.84 67.63 63.64 53.80 54.54 55.72 61.44 48.24 39.74 36.06 37.98 42.84 18.80 15.08 13.52 13.84 19.16 63.30 63.40 71.64 68.34 78. 42.15 26.16 25.02 21.83 26.23 43.40 35.13 34.33 33.74 40.10 46.15 38.90 38.16 37.52 43.75 Table 5. Detailed Performance of Gemma-3 1B across test splits. Symb: GSM-Symbolic; P1/P2: Symbolic Perturbations; IC: GSM-IC; Hard: GSM-Hard. OOD: Geometric Mean of the 5 stress tests. Overall: Geometric Mean of all 6 splits. ID Out-of-Distribution (Stress Tests) Aggregates CoT Strategy GSM8K Symb P1 P2 IC Hard OOD Overall Baseline Strategies No CoT No CoT with extra tokens Short CoT Variants Very Short CoT Short CoT Short CoT with 2 Distractors Short CoT with 4 Distractors Short CoT with 8 Distractors 3.56 5.31 8.95 18.04 10.46 4.78 2.43 0.90 1. 4.32 9.96 6.76 2.70 1.00 0.94 1.18 0.80 1.92 17.64 7.08 2.14 3.98 1.98 1.16 0.66 1.20 1.92 1.40 0.48 0. 39.82 72.08 79.04 65.04 45.60 Default CoTs Sampled from Teacher Model Gemma 27B CoT Gemini CoT 20.40 20.55 11.54 12.66 3.50 5.38 2.04 2. 44.74 64.90 0.91 1.44 1.82 4.32 1.67 1.29 0.23 4.25 4.40 2.00 2.00 4.00 7.00 5.00 3.00 1. 7.00 8.00 1.84 2.38 4.39 8.68 5.44 2.91 1.57 8.27 9.73 Specific Reasoning Strategies from Prior Work Executed PoT Simulated PoT Plan Solve Critical CoT High Review Ratio CoT 20.24 20.85 21.53 17.51 22. 18.54 9.74 13.86 11.98 14.30 8.80 5.64 6.78 4.62 5.84 1.96 1.84 2.88 2.28 2.20 38.28 59.58 74.86 57.96 66.28 10.69 4.17 4.62 4.47 5.08 11.00 8.00 10.00 8.00 9. 11.76 8.98 11.24 9.11 10."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "UNC Chapel Hill"
    ]
}