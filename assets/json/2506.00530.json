{
    "paper_title": "CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing",
    "authors": [
        "Tianhui Liu",
        "Jie Feng",
        "Hetian Pang",
        "Xin Zhang",
        "Tianjian Ouyang",
        "Zhiyuan Zhang",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce $\\textbf{CityLens}$, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 0 3 5 0 0 . 6 0 5 2 : r CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing Tianhui Liu, Jie Feng, Hetian Pang, Xin Zhang, Tianjian Ouyang, Zhiyuan Zhang, Yong Li School of Electronic and Information Engineering, Beijing Jiaotong University Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China"
        },
        {
            "title": "Abstract",
            "content": "Understanding urban socioeconomic conditions through visual data is challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct multi-modal dataset covering total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-theart LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens."
        },
        {
            "title": "Introduction",
            "content": "Understanding the socioeconomic characteristics of urban regions is fundamental to the planning, management, and sustainability of cities. Urban socioeconomic sensing, the process of quantifying indicators such as income, education, health, and transport conditions across spatial units, plays critical role in shaping how cities function and evolve. These indicators directly influence residents quality of life and are deeply intertwined with key aspects of urban inequality, mobility, and resource allocation. Moreover, urban socioeconomic data serves as cornerstone for measuring progress toward several United Nations Sustainable Development Goals (UN SDGs). Accurate and timely information on urban disparities is essential for tracking these goals and designing effective interventions. In practice, governments and urban planners rely on socioeconomic indicators to inform wide range of decisionsfrom zoning regulations and infrastructure investment to public health strategies. better understanding of spatially resolved urban indicators empowers decision-makers to allocate resources more equitably, respond to local needs, and promote inclusive urban development. growing body of work has explored the use of classical deep learning methods to predict urban socioeconomic indicators. Some approaches, such as [49], leverage knowledge graphs to infer socioeconomic indicators. In parallel, researchers have explored the use of urban imagery to understand cities through their visual appearance. Methods such as [23], [27], [25], and [44] employ contrastive learning to generate visual representations from street view or satellite images, while others like [14] apply basic computer vision models to extract visual features. However, the classical methods face several key limitations, including difficulty in handling unstructured or multi-modal data, the inability to work across multiple countries, and cannot interpret subjective and culturally significant aspects Preprint. Under review. of place. Nevertheless, Large Language-Vision Models are inherently equipped to address these challenges with their ability to integrate multiple modalities, generalize globally, and interpret cultural nuances. In recent years, researchers have begun to leverage LLVMs and large language models (LLMs) to address some of the limitations of classical approaches. For example, [43] and [17] employ LLVM to generate textual descriptions from urban imagery, effectively introducing textual modality to enrich visual understanding. Other studies, such as [29] and [28], explore the ability of LLMs to predict socioeconomic indicators directly through textual prompts, and further examine issues like geographical bias across different countries. Despite these promising advances, existing works still fall short in several key aspects. Most efforts are limited in terms of spatial coverage, indicator diversity, and multi-modal integration. Crucially, there remains lack of systematic and unified benchmark to comprehensively evaluate how LLVMs perform across tasks, regions, and modalities in the context of urban socioeconomic sensing. To address these limitations, we propose CityLens, comprehensive benchmark designed to evaluate the ability of large vision-language models to predict urban socioeconomic indicators using both street view and satellite imagery. CityLens spans total of 17 cities across multiple continents, covering 11 indicators across 6 socioeconomic domains, including economy, health, education, environment, transport, and crime. By integrating diverse data modalities and global geographic coverage, CityLens enables systematic, cross-task, and cross-region evaluation of LLVMs capabilities in urban perception, geo-visual reasoning, and numerical estimation. Overall, our contributions are summarized as follows: We introduce CityLens, multi-modal benchmark for urban socioeconomic sensing. It is built through scalable data collection and processing pipeline, covering 17 cities across different continents, 11 indicators in 6 socioeconomic domains, using both street view and satellite imagery. We conduct comprehensive evaluation of 17 state-of-the-art large vision-language models across diverse tasks and evaluation settings, systematically comparing three paradigms: Direct Metric Prediction, Normalized Estimation, and Feature-Based Regression. We provide detailed empirical analyses that offer new insights into how different prompting strategies, input modalities, and task types affect model performance, highlighting challenges, opportunities, and future directions for socioeconomic sensing with LLVMs."
        },
        {
            "title": "2 Methods",
            "content": "In this paper, we present CityLens, comprehensive benchmark designed to evaluate the capabilities of large language-vision models in predicting socioeconomic indicators from both satellite and street view imagery. As illustrated in Figure 1, CityLens spans 11 real-world indicators across 6 socioeconomic domains, covering 17 globally distributed cities with diverse urban forms and development levels. To systematically assess model performance, we evaluate 17 different LLVMs using 3 distinct evaluation paradigms."
        },
        {
            "title": "2.1 Dataset Construction",
            "content": "To support the evaluation of LLVMs across diverse socioeconomic indicators, as Figure 2 illustrates, we construct region-level dataset for indicators. Each region is represented by one satellite image and street view images, which are jointly provided as input to the LLVMs."
        },
        {
            "title": "2.1.1 Tasks and Sources",
            "content": "We provide detailed list of data sources for all indicators in the Appendix A.1; here, we briefly describe the collected indicators. Under the economy domain, we cover 7 critical indicators: Gross Domestic Product (GDP), house price, population, median household income, poverty 100%, poverty 200%, and income Gini coefficient. In the transport domain, we include seven indicators: PMT, VMT, PTRP, VTRP, walk and bike ratio, public transport ratio, and drive ratio. In the crime domain, we focus on two indicator: violent crime incidence and non-violent crime incidence, both defined as the number of crime occurrences per census tract. For the health domain, we include 9 kinds of indicators to capture different dimensions of urban health outcomes: obesity, diabetes, cancer, no leisre-time physical activity(LPA), mental health, physical health, depression rate, accessibility to 2 Figure 1: Framework of CityLens. healthcare, and life expectancy. In the environment domain, we consider two indicators: carbon emissions and building height. Building height is increasingly used as an explicit yet indirect indicator of urban socioeconomic development, population density, and land use intensity. Under the education domain, following [27], we use the bachelor ratio, defined as the proportion of residents holding bachelors degree or higher, as the target variable. These domains are selected to ensure balanced and holistic representation of urban conditions that are commonly studied in social science and urban planning."
        },
        {
            "title": "2.1.2 Data Collection",
            "content": "Since many ground-truth indicators are only available for specific countries (the US and the UK), we focus on region-level prediction tasks in three representative cities from each country. We choose New York, San Francisco, and Chicago in the US, and Leeds, Liverpool, and Birmingham in the UK. For globally available indicators, we expand coverage to cities across 6 continents, including: Cape Town, Nairobi, London, Paris, Beijing, Shanghai, Moscow, Mumbai, Tokyo, Sao Paulo, and Sydney, which ensures cross-regional evaluation diversity. Beyond ground-truth indicator data, we collect both satellite images and street view images for each task region. We obtain street view images for Beijing and Shanghai using the Baidu Maps API, while for other cities, we utilize the Google Street View API. Additionally, satellite images are downloaded from Esri World Imagery, providing zoom level 15 images for all cities in our study."
        },
        {
            "title": "2.1.3 Indicator Selection",
            "content": "We initially collect ground-truth data for 28 indicators spanning 6 domains. From these, as Figure 3a illustrates, we select 11 final indicators to construct prediction tasks. The selection followed two principles: First, we assess the perceptual relevance of indicators i.e., whether human could reasonably infer the variable from satellite and street view imagery. Indicators such as Estimated personal miles traveled on working weekday, which lack visible spatial cues, are excluded. Second, we conduct Pearson correlation analysis among semantically similar indicators in the same domain to remove redundancy. For example, in the health domain, we found high correlation between obesity and mental health (Pearsons = 0.7524), which is intuitively understandable that people experiencing psychological stress or poor mental well-being tend to overeat or engage in unhealthy eating behaviors. To avoid task redundancy, we retained only mental health in the final task list. 3 Figure 2: Benchmark Construction Pipeline, including data collection, indicator selection, task construction and evaluation method."
        },
        {
            "title": "2.1.4 Task Construction",
            "content": "For US-only indicators, we define census tract level prediction tasks. Each census tract is represented by one satellite image and 10 street view images, which are jointly input into the LLVM for prediction. While prior work such as [14] uses 20 or more street view images to represent each region, we found that this setup is often impractical for LLVMs. Specifically, we initially experiment with 20 street view images per region, but observe that this would significantly increase the computational cost, and exceed the input limits of models like Gemini, which can only process up to 10 images per inference, and also frequently hit the token length limit of other models. Therefore, we adopt compromise of 10 images per region to ensure compatibility across models while maintaining sufficient visual context. For UK-only indicators, tasks are constructed at the MSOA level using similar strategy. For global tasks, we first download multiple satellite images covering each citys spatial extent. Then, we randomly sample 30 geographic points within the satellite image coverage, downloading at least 10 street view images for those points. Each satellite image is paired with its local indicator value to form an evaluation unit. For the China house price task, we follow the global task methodology and select Beijing and Shanghai as target cities. Due to resource constraints, we randomly sample up to 500 cases per task for country-specific indicators, and up to 1000 cases per task for global indicators. In practice, some tasks contain fewer samples due to data availability limitations, but these values represent the maximum sample size allowed per task. The detailed statistics of available data before applying the sampling strategy are presented in Figure 3b. Task GDP Population House Price Public Transport Drive Ratio 4285 4517 769 631 Satellite Images Street View Images Scale Region Global Global Sat Sat US, UK, China CT, MSOA, Sat US US US Global UK US US Global CT CT CT Sat MSOA CT CT Sat 42842 45157 7770 6390 6390 6400 42837 1930 11438 3960 44505 (b) Mental Health 632 Accessibility to Health 4285 Life Expectancy 193 Bachelor Ratio Violent Crime Building Height 1135 389 4451 (a) Figure 3: (a) 11 indicators in CityLens benchmark and their case counts. (b) Statistics of the CityLens dataset, including urban images."
        },
        {
            "title": "2.2 Evaluation Methodologies",
            "content": "We design three distinct paradigms to explore the capability of LLVM for socioeconomic indicator prediction. Each paradigm is aimed at assessing different facets of how LLVM can be applied to socioeconomic indicator sensing."
        },
        {
            "title": "2.2.1 Direct Metric Prediction",
            "content": "Direct Metric Prediction refers to providing region-level urban imagery and directly querying the LLVM for the metric value, such as: \"Whats the percentage of the population commuting by public transit in this census tract?\" In addition, the prompt positions the model as an urban socioeconomic scientist in specific city. Despite this, the model faces significant challenges in accurately predicting the exact true values of these indicators."
        },
        {
            "title": "2.2.2 Normalized Metric Estimation",
            "content": "Given the difficulty of directly predicting precise indicator values, we adopt Normalized Metric Estimation approach inspired by GeoLLM [29]. Specifically, we transform all indicator values into normalized range from 0.0 to 9.9, discretized to one decimal place. The model is then prompted to estimate this normalized value based on the input images. This formulation aims to investigate whether the LLVM possesses coarse-grained spatial knowledge and the ability to associate visual cues with relative indicator levels."
        },
        {
            "title": "2.2.3 Feature-Based Regression",
            "content": "In the Feature-Based Regression approach, we first design structured prompt that guides the LLVM to evaluate each street view image along 13 predefined visual attributes, following the visual taxonomy proposed by [14]. These features capture key elements of the urban environment, such as greenery, vehicle, facade, and sidewalk. For each region, we represent its visual environment using 10 sampled street view images. For each visual feature, we compute the average score across these images, resulting in single feature vector that characterizes the region. These aggregated visual features are then used as inputs to LASSO regression model, which is trained to predict the corresponding ground-truth indicator values using 5-fold cross-validation setup."
        },
        {
            "title": "3.1.1 LLVMs",
            "content": "We consider diverse set of LLVMs as baselines to benchmark our proposed methods. The selected models include both open-source and proprietary systems, covering range of model sizes and capabilities. We choose Gemma3-4B/12B/27B [38], Qwen2.5VL-3B/7B/32B [8], Llama4Scout/Maverick [4], Mistral-small-3.1-24B [5], Phi-4-multimodal [2], MiniMax-01 [21], Gemini-2.0flash/Gemini-2.5-flash [13], GPT-4o-mini [3], GPT-4.1-mini/nano [32] and Amazon-Nova-Lite [6]. One thing to note is that models in the Gemini series can accept at most 10 images as input. Therefore, for this series, we use 1 satellite image and 9 street view images per region to stay within the models input constraints."
        },
        {
            "title": "3.1.2 Metrics",
            "content": "For evaluation, we adopt two commonly used metrics in socioeconomic prediction tasks: coefficient of determination (R2) and normalized root mean squared error (nRMSE). Higher R2indicates better performance, with 1.0 representing perfect prediction. Lower nRMSE values indicate more accurate predictions. 5 Table 1: Main results on the Feature-Based Regression method. The values in the table represent R2 scores. Mean denotes the average performance across tasks, and SD refers to the standard deviation. In each row, bold indicates the best result, and underline denotes the second-best. Domain Tasks Gemma3-4B Gemma3-12B Gemma3-27B Qwen2.5VL-3B Qwen2.5VL-7B Qwen2.5VL-32B Llama4-Scout Llama4-Maverick GDP 0.479 0.484 0.463 0.372 0.468 0.517 0.460 0.452 Mistral-small-3.1-24B 0.452 Phi-4-multimodal 0.190 Nova-lite-v1 0.466 Minimax-01 0. Gemini-2.0-Flash Gemini-2.5-Flash GPT-4o-mini GPT-4.1-mini GPT-4.1-nano 0.436 0.375 0.425 0.441 0.360 Econ. Pop. 0.252 0.280 0.324 0.157 0.304 0.347 0.264 0. 0.366 0.079 0.219 0.336 0.317 0.314 0.251 0.316 0.314 HP 0.036 0.136 0.141 0.169 0.104 0.067 0.164 0. 0.144 0.154 0.216 0.197 0.129 0.143 0.119 0.243 0.201 Crime VC Trans. PT DR Env. BH MH Health AH 0.103 0.063 0.077 0.029 0.053 0.067 0.090 0.110 0.062 0.038 0.007 0. 0.090 0.064 0.076 0.063 0.084 0.486 0.527 0.567 0.382 0.483 0.508 0.508 0.547 0.499 0.238 0.439 0.523 0.560 0.527 0.470 0.542 0. 0.365 0.448 0.525 0.262 0.308 0.427 0.479 0.447 0.393 0.224 0.359 0.448 0.490 0.500 0.253 0.444 0.198 0.585 0.588 0. 0.513 0.536 0.528 0.524 0.523 0.571 0.142 0.538 0.516 0.559 0.568 0.554 0.505 0.485 0.183 0.159 0.211 0.172 0.166 0. 0.168 0.229 0.159 0.096 0.222 0.113 0.222 0.251 0.239 0.151 0.175 0.294 0.266 0.283 0.247 0.261 0.261 0.280 0. 0.260 0.172 0.272 0.273 0.310 0.277 0.295 0.264 0.267 LF 0.148 0.263 0.245 0.006 0.119 0.193 0.155 0. 0.098 0.144 0.145 0.162 0.194 0.210 0.236 0.150 0.086 Edu. Overall BR Mean SD 0.290 0.202 0.297 0.001 0.195 0.311 0.197 0. 0.198 0.103 0.175 0.170 0.201 0.203 0.163 0.195 0.227 0.293 0.311 0.338 0.210 0.272 0.309 0.299 0.324 0.291 0.143 0.278 0. 0.319 0.312 0.280 0.301 0.251 0.165 0.166 0.166 0.158 0.157 0.164 0.155 0.139 0.166 0.059 0.150 0.159 0.161 0.156 0.141 0.153 0."
        },
        {
            "title": "3.2.1 Challenge of the Benchmark for LLVMs",
            "content": "As shown in Table 1, the overall performance suggests that our benchmark poses significant challenge for current large language-vision models. In particular, tasks such as Mental Health and Bachelor Ratio exhibit low R2 scores, in some cases even approaching zero, e.g., 0.001. This highlights the difficulty of CityLens in Feature-Based Regression method: even when leveraging visual features extracted by advanced LLVMs, the resulting representations often fail to capture the complex patterns required for accurate prediction of socioeconomic indicators, especially in domains like health and education, where visual cues are often subtle, noisy, or indirect."
        },
        {
            "title": "3.2.2 Performance Differences Across Models",
            "content": "We observe substantial performance differences across LLVMs, reflecting how model scale, architecture, and training design influence their ability to extract meaningful visual features for downstream prediction. Comparing models within the same series but at different scales, we find that increasing model size does not always guarantee better performance. For example, Gemma3-12B achieves the best score on GDP and Life Expectancy, yet the 27B variant, despite its larger size, performs worse in these two tasks, with relative drops of 4.3% and 6.8% respectively. This counterintuitive result may be attributed to the unique nature of socioeconomic sensing tasks, which requires the model to consistently extract and score predefined set of nuanced visual features from urban imagery. When comparing models from different series with similar parameter scales, clear differences emerge. For instance, Gemma3-4B significantly outperforms Qwen2.5VL-3B in nearly all tasks, with relative improvements ranging from 6.4% to 255% across different indicators, suggesting that Gemmas architecture or training process may enable more consistent and informative scoring of urban visual features, which in turn leads to better performance in downstream socioeconomic prediction."
        },
        {
            "title": "3.2.3 Variations Across Different Task Types",
            "content": "Performance also varies across task types. Tasks like Building Height, Public Transport, and GDP tend to have relatively higher values across models, with Building Height reaching an R2 of 0.59, suggesting that these indicators are associated with more observable visual cues that can be directly captured from street view images. For instance, Building Height is closely linked to the skyline and the vertical structure visible in images; Public Transport usage may be inferred from the presence of bus stops, transit signs, or road markings. In contrast, tasks such as Life Expectancy and Mental Health remain highly challenging, exhibiting low or near-zero predictive scores for many models. 6 These indicators are influenced by latent factors such as lifestyle, stress levels, or social cohesion, which do not have clear or direct visual manifestations in the urban environment. Even if certain proxies exist, such as the presence of graffiti or the amount of green space, they are often subtle or semantically ambiguous, making it difficult for current LLVMs to interpret them reliably and consistently."
        },
        {
            "title": "3.3.1 Overall Performance",
            "content": "We evaluate the performance of large language-vision models on all 11 tasks using both the Direct Metric Prediction and Normalized Estimation settings. To ensure robustness of the analysis, any data point for which either the simple or normalized R2 is less than or equal to 0.5 is excluded, as such values generally reflect weak predictive performance and are not analytically meaningful. The final comparison is visualized in Figure 4. few tasks such as House Price, Public Transport, and Building Height achieve relatively better R2 scores under certain models and settings, e.g., House Price consistently exceeds 0.2 under the Direct setting. These tasks are likely more visually grounded, with cues such as building density, road layout, and commercial signage that can be directly observed from urban imagery. This suggests that some socioeconomic indicators may be approximated more easily when the visual-structural link is strong. However, the majority of results fall into the low or even negative R2 range, indicating that the models predictions often fail to explain the variance in the ground-truth indicator values. This suggests that, while large language-vision models have made rapid progress in perception and reasoning, predicting regionlevel socioeconomic indicators remains highly challenging and underexplored task. The models may still lack the necessary numerical grounding, contextual interpretation, and semantic alignment required to associate urban visual content with structured socioeconomic quantities. Even with normalization, which alleviates the demand for precision by coarsening the prediction space, performance remains weak across most tasks. In many cases, the model predictions tend to collapse toward city-wide averages or exhibit narrow output range, suggesting lack of sensitivity to fine-grained regional variation. This behavior indicates that the models may struggle to differentiate subtle socio-spatial differences across urban regions, especially when visual cues are weak or ambiguous. These findings highlight the inherent difficulty of the CityLens benchmark; models struggle not only when asked to predict exact values, but also when the task is simplified into normalized estimation. We also incorporate the Feature-Based Regression method into unified evaluation. comprehensive comparison across all three paradigms is visualized in Appendix Figure 9. Figure 4: Comparison of task-wise R2 performance between Direct Metric Prediction and Normalized Estimation across 11 socioeconomic indicators in CityLens."
        },
        {
            "title": "3.3.2 Task Preference Between Direct and Normalized Estimation",
            "content": "In Figure 4, the diagonal line indicates equal performance under both methods; points above it suggest that the task benefits more from normalization, while points below indicate preference for direct estimation. This result highlights that different tasks tend to favor different estimation strategies, depending on the nature of the indicator and its visual and semantic properties. Specifically, tasks such as Violent Crime, GDP, and Population are more frequently observed above the diagonal, suggesting that these indicators with limited direct visual correspondence benefit from normalized formulation that emphasizes relative ranking rather than precise value prediction. These tasks are difficult to estimate accurately, but models may still capture coarse ordinal relationships across regions, aided by their global knowledge priors and implicit ranking sense. Conversely, tasks like Bachelor Ratio, House Price, Public Transport, and Accessibility to Health tend to fall below 7 (a) (b) (c) Figure 5: (a) shows the results of the GDP prediction task across 13 different cities. (b) presents the results showing that satellite imagery has limited impact on prediction. (c) demonstrates that increasing the number of street view images leads to progressive improvement in predictive performance. the diagonal, indicating better performance under the direct estimation setting. These tasks are often associated with clearer, more stable visual correlates, such as building types, infrastructure visibility, and environmental layout, which can support more precise image-to-value mappings. In addition, some indicators, e.g., Life Expectancy, exhibit narrower value ranges or lower variance, making them more amenable to direct value prediction. Moreover, for tasks like House Price and Bachelor Ratio, LLVMs may leverage latent knowledge about typical value scales across different cities, enabling surprisingly accurate numerical predictions. Taken together, these findings emphasize the importance of task-specific method selection in socioeconomic indicator prediction. Some indicators benefit from reducing output precision to focus on broader distinctions, while others retain enough visual signal to support direct prediction. The CityLens benchmark thus not only tests model capacity, but also reveals the nuanced interplay between task semantics and prediction strategy."
        },
        {
            "title": "3.4.1 City-Level Performance Variations",
            "content": "To better understand the variation in socioeconomic prediction outcomes across different cities, we conduct city-level analysis for the GDP task under the Feature-Based Regression paradigm. Each of the 14 cities is represented by 100 regions, with Gemma3-12B extracting 13 visual features per street view image. The GDP task is selected because it is global-scale indicator, available across the largest number of cities in CityLens. Among the 13 cities evaluated in the GDP prediction task, we observe considerable variation in model performance in Figure 5a. Cities such as Shanghai, San Francisco, and Sao Paulo achieve R2 scores above 0.43, indicating relatively strong predictive performance. One possible explanation for the strong performance in cities like Shanghai lies in their well-structured urban design and high alignment between street-level appearance and economic development. These cities tend to have clear visual stratification between affluent and less affluent areas, consistent architectural patterns and homogeneous zoning that make features more learnable and high quality, diverse street view coverage. In contrast, cities like Mumbai and Moscow yield near-zero or even negative R2, which may be attributed to two key factors. First, there may be weak alignment between street-level visuals and actual economic activity, especially in cities with spatially mixed development, where wealth and poverty coexist within the same region, blurring the visual economic signal. Second, the quality and coverage of street view images can be limiting factor. Inconsistent image sources, low resolution, or sparse sampling reduce the availability of reliable visual cues, hindering feature extraction and degrading downstream prediction."
        },
        {
            "title": "3.4.2 Impact of Satellite Imagery",
            "content": "In this part, we evaluate the impact of satellite imagery by comparing model performance in two configurations: with and without satellite imagery. We test 3 tasks: House price, Public transport, and Drive ratio using Gemini-2.0-Flash under the Direct Metric Prediction setting. As shown in Figure 5b, the results reveal minimal performance differences between the two configurations, which appears counterintuitive given the common assumption that satellite imagery can provide high-level spatial cues, such as regional development intensity, land use, and urban rural gradient, which are potentially informative for socioeconomic prediction. One possible explanation is that street view imagery 8 already provides sufficiently rich and fine-grained visual information, like building conditions and public infrastructure. These ground-level features may be more directly interpretable by current large language-vision models and more tightly coupled with the target indicators in these tasks. As result, the one additional satellite image may offer redundant or less impactful information, leading to negligible performance gains."
        },
        {
            "title": "3.4.3 Effect of Street View Image Quantity",
            "content": "To evaluate the impact of street view image quantity on prediction performance, we conduct experiments using Llama4-Maverick on the House Price task under the Direct Metric Prediction setting. In this experiment, each region is represented by one satellite image and varying number of street view images: 1, 5, 10, 15, or 20. The results are shown in Figure 5c. In addition, we test no-image baseline following the design in [29], where only the geographic coordinates and address are provided in the prompt without any visual input. In this setting, most responses from the model consist of refusals, indicating that the model lacks sufficient information to generate confident estimate. Interestingly, in several cases, the model proactively suggests alternatives, such as checking local housing websites, demonstrating the LLVMs conversational safety and reasoning abilities, but also its limitation in open-world knowledge retrieval. From the results, we observe clear trend: increasing the number of street view images consistently improves model performance. As more images are included, R2 increases and nRMSE decreases. This suggests that richer visual context helps the model form more accurate understanding of the regions socioeconomic condition."
        },
        {
            "title": "4.1 Urban Socioeconomic Sensing",
            "content": "A growing number of studies have attempted to predict socioeconomic indicators in urban environments, using diverse technical strategies. [49] and [27] employ knowledge graph based approaches to support socioeconomic inference. Li et al. [23] propose contrastive learning framework based on structural urban imagery to support socioeconomic indicator prediction. Fan et al. [14] extract urban visual features from street view imagery using computer vision model, applying them to predict socioeconomic indicators at scale across U.S. cities. Recent approaches have extended this paradigm to multi-modal vision-language settings. UrbanCLIP [43] and UrbanVLP [17] utilize contrastive learning on urban images while incorporating textual prompts generated by large language models. GeoLLM [29] extracts latent geospatial knowledge from large language models through fine-tuning and prompt design. [28] investigates geographical bias in LLMs via zero-shot socioeconomic prediction, while [24] evaluates the performance of LLMs on socioeconomic tasks across region-level and city-level granularity, comparing three different paradigms. Different from these works, we introduce CityLens, the first benchmark to systematically evaluate the ability of large vision-language models to predict socioeconomic indicators using both street view and satellite imagery. CityLens spans total of 17 cities from 6 continents, 11 indicators across 6 domains."
        },
        {
            "title": "4.2 Benchmarking LLM and LLVM",
            "content": "In recent years, LLMs have demonstrated strong commonsense and reasoning abilities, rapidly surpassing evaluation benchmarks across range of domains that were previously established. Thus, developing diverse and challenging benchmarks to systematically assess the capabilities of LLMs has become an emerging and highly active research direction. variety of benchmarks have been proposed, including conversational benchmarks such as Chatbot Arena [10] and MT-Bench [7], coding benchmarks such as SWE-Bench [20] and LiveCodeBench [19], mathematical benchmarks such as Gaokao-MathQA [47] and SciBench [40], as well as agent benchmarks like AgentBench [26] and ToolBench [35]. Furthermore, numerous multimodal benchmarks have also been introduced to evaluate the capabilities of Large Language-Vision Models, including comprehensive benchmarks like SEED-Bench [22] and MMMU [45], and domain-specific benchmarks such as CARES [42], UrBench [48], and DABench [18]. While there are some benchmarks include urban imagery (e.g., CityBench [15] and UrBench [48]), they are not specifically designed for urban socioeconomic sensing tasks and address only very limited subset of such tasks. Therefore, in this paper, after comprehensive review of previous works in urban sensing, we propose new benchmark to fill this gap and provide opportunities to bridge LLMs with urban sensing applications."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce CityLens, benchmark for evaluating the ability of large language-vision models to predict socioeconomic indicators from satellite and street view imagery. Through extensive experiments across 3 evaluation paradigms and 17 state-of-the-art models, we find that while current models exhibit promising perceptual abilities on certain visually grounded tasks, they still face major challenges in making accurate and generalizable predictions across domains and regions. CityLens provides foundation for analyzing these limitations and motivates further research into enhancing the capabilities of large language-vision models in urban socioeconomic sensing."
        },
        {
            "title": "6 Discussion",
            "content": "Our benchmark results highlight both the potential and limitations of using LLVMs to predict regionlevel socioeconomic indicators. While some tasks, particularly those with visually salient correlates such as building height achieve moderate performance, most indicators remain challenging to estimate accurately. Their outputs often converge around city-level averages, suggesting that the model lacks sensitivity to intra-city variation, and consequently exhibits limited geospatial grounding. In the two LLVM as predictor paradigms, we observe that different tasks tend to favor different strategies: some perform better under direct value prediction, while others benefit from normalized estimation. Overall, our results indicate that the Feature-Based Regression paradigm, where the LLM functions as feature enhancer, significantly outperforms the two predictor-based methods. These findings suggest several promising directions for future research. First, while the feature-based method relies on trained LASSO regressor, the predictor-based methods are evaluated in zero-shot setting. This highlights the potential benefit of fine-tuning LLVMs directly for socioeconomic indicator prediction tasks. Second, improvements in prompt engineering could enhance model performance. In particular, incorporating Chain-of-Thought (CoT) prompting may help guide the model through intermediate reasoning steps, yielding more reliable numerical estimates. Finally, we envision the development of domain-specific agent framework tailored to urban socioeconomic sensing, which could combine visual perception, geospatial knowledge, and reasoning modules to make robust and context-aware predictions in real-world scenarios."
        },
        {
            "title": "References",
            "content": "[1] Safegraph. http://https://www.safegraph.com/,. [2] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [3] Josh Achiam, Steven Adler, Sandhini Agarwal, and et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [4] Meta AI. llama4. https://www.llama.com/models/llama-4/, 2025. [5] Mistral AI. Mistral small 3.1. https://mistral.ai/news/mistral-small-3-1, 2025. [6] Amazon. Amazon nova. https://aws.amazon.com/cn/ai/generative-ai/nova/, 2025. [7] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024. [8] Shuai Bai, Keqin Chen, and Xuejing et al Liu. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [9] Centers for Disease Control and Prevention. Local data for better health. https://www.cdc. gov/places/, n.d. Accessed: 2025-05-11. 10 [10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. [11] City and County of San Francisco. Police department incident reports - historical 2003 to present. https://data.sfgov.org/Public-Safety/154-Police-Department-Incid ent-Reports-Historical-2003/tmnf-yvry, 2019. Accessed: 2021. [12] City of Chicago. Crimes - 2001 to present. https://data.cityofchicago.org/Public-S afety/Crimes-2019/w98m-zvie, 2019. Accessed: 2021. [13] Google DeepMind. Gemini2.0. https://cloud.google.com/vertex-ai/generative-a i/docs/gemini-v2, 2025. [14] Becky P. Y. Loo Fan Zhuangyuan, Zhang Fan and Carlo Ratti. Urban visual intelligence: Uncovering hidden city profiles with street view images. Proceedings of the National Academy of Sciences, 120(27):e2220417120, 2023. [15] Jie Feng, Jun Zhang, Junbo Yan, Xin Zhang, Tianjian Ouyang, Tianhui Liu, Yuwei Du, Siqi Guo, and Yong Li. Citybench: Evaluating the capabilities of large language model as world model. arXiv preprint arXiv:2406.13945, 2024. [16] Zhenyu Han, Tong Xia, Yanxin Xi, and Yong Li. Healthy cities: comprehensive dataset for environmental determinants of health in england cities. Scientific Data, 10(1):165, 2023. [17] Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, and Yuxuan Liang. Urbanvlp: Multi-granularity vision-language pretraining for urban socioeconomic indicator prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2806128069, 2025. [18] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, et al. Infiagent-dabench: Evaluating agents on data analysis tasks. arXiv preprint arXiv:2401.05507, 2024. [19] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [20] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [21] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. [22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [23] Tong Li, Shiduo Xin, Yanxin Xi, Sasu Tarkoma, Pan Hui, and Yong Li. Predicting multi-level socioeconomic indicators from structural urban imagery. In Proceedings of the 31st ACM international conference on information & knowledge management, pages 32823291, 2022. [24] Zhuoheng Li, Yaochen Wang, Zhixue Song, Yuqi Huang, Rui Bao, Guanjie Zheng, and Zhenhui Jessie Li. What can llm tell us about cities? arXiv preprint arXiv:2411.16791, 2024. [25] Yuming Lin, Xin Zhang, Yu Liu, Zhenyu Han, Qingmin Liao, and Yong Li. Long-term detection and monitory of chinese urban village using satellite imagery. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, 2024. [26] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. 11 [27] Yu Liu, Xin Zhang, Jingtao Ding, Yanxin Xi, and Yong Li. Knowledge-infused contrastive learning for urban imagery-based socioeconomic prediction, 2023. [28] Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, and Stefano Ermon. Large language models are geographically biased, 2024. [29] Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David B. Lobell, and Stefano Ermon. GeoLLM: Extracting geospatial knowledge from large language models. In The Twelfth International Conference on Learning Representations, 2024. [30] New York City Police Department. Citywide crime statistics - incident level data. https://www. nyc.gov/site/nypd/stats/crime-statistics/145-citywide-crime-stats.page, 2019. Accessed: 2021. [31] Tomohiro Oda and Shamil Maksyutov. ODIAC fossil fuel CO2 emissions dataset (ODIAC2022), 2015. [32] OpenAI. gpt-4.1. https://openai.com/index/gpt-4-1/, 2025. [33] Martino Pesaresi and Panagiotis Politis. GHS-BUILT-H R2022A - GHS building height, derived from AW3D30, SRTM30, and Sentinel2 composite (2018) - OBSOLETE RELEASE, 2022. Dataset. [34] Lianjia Real Estate Platform. Lianjia housing price data. https://lianjia.com/, 2020. [35] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. [36] 2017 National Household Travel Survey. 2017 national household travel survey. https: //www.bts.gov/, 2017. [37] Andrew Tatem. Worldpop, open data for spatial demography. Scientific Data, 4(1):14, 2017. [38] Gemma Team. Gemma 3. 2025. https://arxiv.org/abs/2503.19786. [39] Tingting Wang and Fubao Sun. Global gridded gdp data set consistent with the shared socioeconomic pathways. Scientific Data, 9:221, 2022. [40] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023. [41] Daniel J. Weiss, Andrew Nelson, Carlos A. Vargas-Ruiz, et al. Global maps of travel time to healthcare facilities. Nature Medicine, 26:18351838, 2020. [42] Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: comprehensive benchmark of trustworthiness in medical vision language models. Advances in Neural Information Processing Systems, 37:140334140365, 2024. [43] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, and Yuxuan Liang. Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web. In Proceedings of the ACM Web Conference 2024, pages 40064017, 2024. [44] Xixian Yong and Xiao Zhou. Musecl: Predicting urban socioeconomic indicators via multisemantic contrastive learning. arXiv preprint arXiv:2407.09523, 2024. [45] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 12 [46] Yunke Zhang, Ruolong Ma, Xin Zhang, and Yong Li. Perceiving urban inequality from imagery using visual language models with chain-of-thought reasoning. In Proceedings of the ACM on Web Conference 2025, page 53425351. Association for Computing Machinery, 2025. [47] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [48] Baichuan Zhou, Haote Yang, Dairong Chen, Junyan Ye, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, Conghui He, and Weijia Li. Urbench: comprehensive benchmark for evaluating large multimodal models in multi-view urban scenarios. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1070710715, 2025. [49] Zhilun Zhou, Yu Liu, Jingtao Ding, Depeng Jin, and Yong Li. Hierarchical knowledge graph learning enabled socioeconomic indicator prediction in location-based social network. In Proceedings of the ACM Web Conference 2023. Association for Computing Machinery, 2023. [50] Zillow. Housing data. https://www.zillow.com/research/data/, 2020."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Summary of Indicators in CityLens We provide an overview of all indicators considered in the construction of the CityLens benchmark. Table 2 lists all 28 collected indicators, along with their data sources and whether they are ultimately selected as prediction tasks. Economy Under the economy domain, we cover 7 critical indicators: Gross Domestic Product (GDP), house price, population, median household income, poverty 100%, poverty 200% and income gini coefficient. For GDP, we utilize global dataset that provides GDP estimates with spatial resolution of 1 km [39]. For population, we adopt estimates from WorldPop [37], global demographic dataset with 1 km spatial resolution that provides consistent population counts across countries. For house price, we collect data from multiple sources tailored to each countrys context: (1) For US cities, we use the Zillow Home Value Index (ZHVI) [50], available at the ZIP code level. We map these values to census tract boundaries using spatial overlays, enabling fine-grained local prediction. (2) For UK cities, we obtain house price data from [16], targeting the Middle Layer Super Output Area (MSOA) level. (3) For Chinese cities, we collect house price data from LianJia [34], one of Chinas largest online real estate platforms. For median household income, poverty 100%, and poverty 200%, we obtain the raw data from [1]. We obtain the ground-truth values for the income Gini coefficient from [46]. Transport In the transport domain, we include seven indicators: PMT, VMT, PTRP, VTRP, walk and bike ratio, public transport ratio, and drive ratio, following the design of [14]. The underlying data is sourced from [36], which provides commuting behavior statistics at the census tract level across the United States. Table 3 outlines the definitions of the seven transport related indicators. Crime In the crime domain, we focus on two indicators: violent crime incidence and non-violent crime incidence, both defined as the number of crime occurrences per census tract. The data is collected from the official websites of individual US cities [12, 30, 11], which publish annual crime reports and geolocated incident-level data. Health For the health domain, we include 9 kinds of indicators to capture different dimensions of urban health outcomes: obesity, diabetes, cancer, no leisre-time physical activity (LPA), mental health, physical health, depression rate, accessibility to healthcare, and life expectancy. The first 7 tasks focus on the United States only, using data from Local Data for Better Health [9]. The accessibility to healthcare task is defined globally, using dataset that quantifies walking-only travel time to the nearest healthcare facility [41]. The life expectancy task targets the United Kingdom, where we use data from [16] to obtain male life expectancy estimates at the MSOA level. Environment In the environment domain, we consider two indicators: carbon emissions and building height. For carbon, we use global estimates from [31]. We use global building height data obtained from [33], which provides global coverage at spatial resolution of 100 meters. Education Following [27], we use the bachelor ratio, defined as the proportion of residents holding bachelors degree or higher, as the target variable in the education domain. The ground-truth data for this indicator is obtained from SafeGraph [1], which provides fine-grained demographic datasets across the United States. A.2 Prompt Design and Case Analysis We provide additional insights into the design and behavior of LLVMs under the three evaluation methodologies. Table 4 summarizes the representative prompts used in the three paradigms, highlighting their structural differences and role-setting strategies. Figure 6 shows case in the Direct Metric Prediction setting, where the model refuses to estimate GDP due to insufficient information. Figure 7 shows an example under the Normalized Estimation setting, where the model is asked to predict the bachelor ratio based on regional images. Figure 8 presents an example from the Feature-Based 14 Table 2: Summary of 28 indicators in CityLens."
        },
        {
            "title": "Economy",
            "content": "GDP House Price Population Median Income Poverty 100% Poverty 200% Income Gini Coefficient"
        },
        {
            "title": "Health",
            "content": "Violent Non-Violent"
        },
        {
            "title": "Source",
            "content": "[39] [50, 16, 34] [37] [1] [1] [1] [46] [1] [12, 30, 11] [12, 30, 11] [36] [36] [36] [36] [36] [36] [36] [9] [9] [9] [9] [9] [9] [9] [16] [41] [31] [33] Selected Table 3: Definitions of the seven indicators in the Transport domain. Label"
        },
        {
            "title": "Transport",
            "content": "%Population(>16)commute by driving alone Estimated personal miles traveled on working weekday Estimated personal trips traveled on working weekday Estimated vehicle miles traveled on working weekday Estimated vehicle trips traveled on working weekday %Population(>16)commute by public transit %Population(>16)commute by walking and biking"
        },
        {
            "title": "Drive Ratio\nPMT\nPTRP\nVMT\nVTRP\nPublic Transit\nWalk and Bike",
            "content": "Regression method, where the model scores street view image along 13 predefined urban visual attributes to support downstream prediction. A.3 Supplementary Comparison of Three Evaluation Methodologies We present two supplementary figures in Figure 8: one comparing Feature-Based Regression with Direct Metric Prediction, and the other with Normalized Estimation. These visualizations enable side-by-side assessment of performance across all socioeconomic indicators. From the figures, it is evident that the Feature-Based approach, where the large language-vision model acts as feature enhancer, consistently outperforms the Direct and Normalized approaches, in which the model is expected to behave as numerical predictor. This suggests that current LLVMs, while powerful in perceptual and language tasks, are still more effective when used to extract structured visual representations rather than to directly generate precise socioeconomic estimates. Although large language-vision models have made impressive strides, accurately predicting fine15 Table 4: Prompt comparison across the three evaluation methodologies in transport domain tasks. Method"
        },
        {
            "title": "Normalized Metric Estimation",
            "content": "Feature-Based Regression Suppose you are professional transport data analyst in {city}, {country}. Based on the provided satellite imagery and several street view photos, please estimate the {indicator} in the census tract where these images are taken. Consider factors such as road infrastructure, visible traffic patterns, availability of public transport options, pedestrian walkways, and any other relevant details that might influence these transport behaviors in the area. Please provide single specific number (not range or approximate value) for {indicator}. No explanation is needed.Example answer: {example num}. Suppose you are professional transport data analyst in {city}, {country}. Based on the provided satellite imagery and several street view photos, please estimate the {indicator} in the census tract where these images are taken. Consider factors such as road infrastructure, visible traffic patterns, availability of public transport options, pedestrian walkways, and any other relevant details that might influence these transport behaviors in the area. Please provide single specific number for {indicator} (on scale from 0.0 to 9.9). No explanation is needed. Example answer: 8.8. Analyze the provided street view image. For each of the following 13 indicators, provide score from 0.0 to 9.9 representing its presence or prominence in the image. The output should only be the indicator name followed by its score, one indicator per line. No need for explanations or additional text. Indicators: Person; Bike; Heavy Vehicle; Light Vehicle; Faade; Window & Opening; Road; Sidewalk; Street Furniture; Greenery - Tree; Greenery - Grass & Shrubs; Sky; Nature Example: Person: 2.5; Bike: 0.0; . . . . . . grained, region-level socioeconomic indicators remains highly challenging, highlighting the need for further advancements. Figure 6: An example of model refusal in the Direct Metric Prediction setting. Figure 7: Prompt example for Normalized Metric Estimation. 17 Figure 8: Prompt template for guiding large language-vision models to extract 13 visual features from street view image. (a) (b) Figure 9: (a) shows the performance comparison between Feature-Based Regression and Direct Metric Prediction. (b) compares Feature-Based Regression with Normalized Estimation.."
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China",
        "School of Electronic and Information Engineering, Beijing Jiaotong University"
    ]
}