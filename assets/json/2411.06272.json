{
    "paper_title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models",
    "authors": [
        "Xiaojun Wu",
        "Junxi Liu",
        "Huanyi Su",
        "Zhouchi Lin",
        "Yiyan Qi",
        "Chengjin Xu",
        "Jiajun Su",
        "Jiajie Zhong",
        "Fuwei Wang",
        "Saizhuo Wang",
        "Fengrui Hua",
        "Jia Li",
        "Jian Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose \"Golden Touchstone\", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at \\url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area."
        },
        {
            "title": "Start",
            "content": "Golden Touchstone: Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models 4 2 0 2 9 ] . [ 1 2 7 2 6 0 . 1 1 4 2 : r Xiaojun Wu1,2, Junxi Liu1,5, Huanyi Su1,4, Zhouchi Lin1, Yiyan Qi1, Chengjin Xu1, Jiajun Su1, Jiajie Zhong1, Fuwei Wang1, Saizhuo Wang1,3, Fengrui Hua1,2, Jia Li2, Jian Guo1, 1IDEA Research, 2The Hong Kong University of Science and Technology (Guangzhou), 3The Hong Kong University of Science and Technology, 4Nanjing University, 5South China Normal University"
        },
        {
            "title": "Abstract",
            "content": "As large language models become increasingly prevalent in the financial sector, there is pressing need for standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose \"Golden Touchstone\", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes variety of financial tasks aimed at thoroughly assessing models language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o, Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area. *Equal Contribution. Work done during internship at IDEA Research. Corresponding Author. Correspondence to: Jian Guo <guojian@idea.edu.cn>. 1. Introduction The rapid development of both proprietary(Anthropic, 2024; Brown et al., 2020; OpenAI, 2023; Ouyang et al., 2022; Team et al., 2023) and open-source Large Language Models (LLMs) has led to their increasing importance and application across various fields(AI@Meta, 2024; Bai et al., 2023; Baichuan, 2023; DeepSeek-AI, 2024; Gan et al., 2023; Touvron et al., 2023a,b; Yang et al., 2024a; Young et al., 2024; Zeng et al., 2023; Zhang et al., 2022) , including finance(Lopez-Lira and Tang, 2023; Wu et al., 2023), healthcare Thirunavukarasu et al. (2023); Tian et al. (2023), and law Cui et al. (2023); Xiao et al. (2021). Among these, the financial sector stands out as one of the most important areas for LLM application, due to its rich textual information and high practical value. In recent years, variety of advanced financial large language models (FinLLMs) have emerged, capable of specialized tasks such as financial sentiment analysis, content summarization, stock movement prediction, and specialized question answering, as depicted in Figure.1. These models, including FinGPT (Yang et al., 2023), PIXIU Xie et al. (2023), CFGPT (Li et al., 2023), DISC-FinLLM (Chen et al., 2023), and XuanYuan (Zhang and Yang, 2023), leverage unique frameworks and tuning methods to enhance their performance on domain-specific benchmarks, offering robust solutions for real-world financial applications. Alongside the rapid emergence of FinLLMs, there has also been significant increase in financial benchmarks. In the English financial domain, FLUE(Shah et al., 2022) was the first publicly available benchmark for assessing English financial language understanding. Similarly, BBT(Lu et al., 2023a) was the pioneering open-source financial NLP benchmark in Chinese. As LLMs and financial applications have continued to develop, more financial benchmarks have been introduced(Lei et al., 2023; Xie et al., 2023, 2024; Yang et al., 2023; Zhang et al., 2023). While these benchmarks have provided valuable resources and evaluation criteria for assessing current LLMs in finance, there remain challenges, such as the presence of low-quality or unsuitable for LLMs datasets within certain tasks, leading to generally poor evaluation results across various models. Furthermore, Current benchmarks also suffer from insufficient language and task coverage, which prevents comprehensive evaluation of large financial language models. To address these gaps, we propose Golden Touchstone, bilingual financial benchmark that consolidates representative datasets across eight financial NLP tasks in both Chinese and English. Golden Touchstone provides high-quality datasets, task-aligned metrics, and instructional templates tailored to guide LLMs in generating task-appropriate responses. The Golden Touchstone organizes all task data into an instruction, input, and output format, facilitating the use and evaluation of different models. We evaluated current open-source general-purpose LLMs and FinLLMs using the Golden Touchstone benchmark. Results indicate that while GPT-4o, Qwen-2, Llama-3, and FinMA performed well across several tasks like financial sentiment analysis and entity extration, there remains considerable room for improvement in areas like credit card scoring dataset within classification tasks and stock movement prediction. Additionally, we have open-sourced Touchstone-GPT, model trained through domain-specific continual pre-training and financial instruction tuning. It achieved acceptable performance on the bilingual benchmark but showed limitations in tasks such as stock movement prediction and question answering. These findings underscore the need for more high-quality training data and potentially more suitable model architectures for finance-specific applications. Our main contributions are as follows: Introduction of Golden Touchstone, the first comprehensive bilingual benchmark for financial Large Language Models (LLMs), encompassing 22 datasets across eight tasks in both Chinese and English. Compared to previous FinLLMs benchmarks, our benchmark 2 Figure 1 Financial large language models are designed to perform specialized tasks such as financial sentiment analysis, content analysis, stock movement prediction, and financial analyst level question answering by interpreting and processing structured instructions and various input data to generate precise outputs. offers enhanced diversity, systematicity, and adaptability. thorough evaluation of state-of-the-art LLMs and FinLLMs, including GPT-4o, Qwen-2, Llama-3, FinGPT and FinMA and so on, on the Golden Touchstone. This evaluation highlights the key strengths and shortcomings in model performance across various tasks, and suggests directions for future research in financial LLMs. The open-source release of Touchstone-GPT, specialized financial LLM that has undergone domain-specific continuous pre-training and instruction tuning. This model serves as new baseline for subsequent FinLLMs research, fostering further advancements in Financial AI. 2. Related Works 2.1. Financial Large Language Models In recent years, large language models (LLMs) tailored for the financial domain have become increasingly important. Several significant FinLLMs have been developed: BloombertGPT (Wu et al., 2023) marks the beginning of the FinLLM era. FinGPT (Yang et al., 2023) is an opensource framework designed for financial LLMs. It focuses on data-centric approach, offering transparent resources for developing FinLLMs. The framework highlights the importance of an automated data curation pipeline and lightweight low-rank adaptation techniques. The follow-up work, FinGPT-Forecaster, continues to use LLM for stock price prediction, which is not ideal for numerical computation tasks. PIXIU (Xie et al., 2023) provides comprehensive framework that includes the first financial LLM fine-tuned on LLaMA with instruction data, 136K instruction dataset for fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets. This framework supports the evaluation and development of financial LLMs. CFGPT (Li et al., 2023) introduces Chinese Financial Generative Pre-trained Transformer framework. 3 It consists of CFData, dataset for pre-training and supervised fine-tuning; CFLLM, financial LLM capable of processing financial texts; and CFAPP, deployment framework for real-world financial applications. DISC-FinLLM Chen et al. (2023) is based on Multiple Experts Finetuning Framework. It enhances general LLMs by adding multi-turn question answering, domain text processing, mathematical computation, and retrieval-enhanced generation capabilities. XuanYuan 2.0 (Zhang and Yang, 2023) is the largest Chinese chat model to date, built on the BLOOM-176B architecture. It introduces novel training method called hybrid-tuning to address catastrophic forgetting, combining general-domain and domain-specific knowledge. FinMem (Yu et al., 2023), FinRobot (Yang et al., 2024c), and FinReport (Li et al., 2024) predict stock price trends using agents and tools, such as quantitative models that use multiple alpha factors and alternative factor models that analyze news and textual data. FinVis-GPT (Wang et al., 2023b) and FinTral Bhatia et al. (2024) develop multi-modal financial models, similar to other visual language models (VLM) (Liu et al., 2023; Lu et al., 2023b; Zhu et al., 2023). These models represent the cutting-edge in FinLLMs, each contributing unique approaches and advancements to the field of AI in finance. 2.2. Benchmarks for FinLLMs In the current landscape of open-source financial evaluation benchmarks, FLUE (Shah et al., 2022) was the first open-source benchmark introduced for financial language understanding in English. It includes five financial tasks: Financial Sentiment Analysis (Maia et al., 2018b; Malo et al., 2014), Financial News Classification (Sinha and Khandait, 2021), Structure Boundary Detection (FinSBD3, 2021), Named Entity Recognition (Alvarado et al., 2015), and Question Answering (Maia et al., 2018a). Building on FLUE, FinGPT (Yang et al., 2023) introduced the Financial Relation Extraction task with the addition of the FinRED dataset (Sharma et al., 2022), while also expanding the sentiment analysis task by incorporating TFNS (Magic, 2022) and NWGI (Yang, 2023) datasets. Additionally, FinGPT created prompt-based instructions across all datasets, culminating in the Financial Instruction Tuning Benchmark. Around the same time, other opensource benchmarks, such as PIXIU (Xie et al., 2023) and FinBen (Xie et al., 2024), were developed. PIXIU includes wide range of financial tasks and datasets, while FinBen, expanding on PIXIU, introduced numerous supplementary financial datasets to evaluate LLMs comprehensively and gauge their proficiency across various financial scenarios.In the Chinese domain, BBTBenchmark (Lu et al., 2023a) was the first Chinese financial benchmark, covering multiple financial NLP tasks, including sentiment analysis, news classification, summarization, relation extraction, and question answering. Later, FinEval (Zhang et al., 2023) benchmark emerged, providing an evaluation framework for LLMs in financial contexts, encompassing multiplechoice questions across finance, economics, accounting, and certification. DISC-FinLLM (Chen et al., 2023) evaluated financial LLMs on top of these benchmarks, introducing additional tasks focused on financial calculations and current events to assess LLMs in applied financial scenarios. The most recent Chinese Financial benchmark, CFBenchmark (Lei et al., 2023), introduced three key tasks like Financial Entity Recognition, expanding evaluation resources for Chinese financial tasks. The increasing number of benchmarks in both English and Chinese financial domains has established valuable evaluation frameworks for LLMs in finance, though challenges remain. For example, data quality across various English financial datasets is inconsistent, limiting LLMs from reaching their full potential, as seen in FinBen and PIXIU, where low-quality datasets yielded suboptimal performance metrics. In Chinese benchmarks, tasks are often skewed toward multiple-choice questions, with limited alignment with English financial tasks. Moreover, no bilingual benchmark currently exists for comprehensive LLM evaluation in financial NLP tasks 4 across English and Chinese. In this work, we integrate representative datasets from both languages, creating FinLLM-benchmark. We compile and align eight financial NLP tasks in English and Chinese, selecting high-quality datasets for each task and appropriate metrics to effectively assess model performance. 3. Benchmark Design 3.1. Current Benchmark Status In current open-source financial benchmarks, FLUE(Shah et al., 2022) is among the most widely used benchmarks for English financial NLP tasks, encompassing five financial tasks: financial sentiment analysis, news headline classification, named entity recognition, structural boundary detection, and question answering. FinGPT(Yang et al., 2023) extended this benchmark with additional financial tasks. PIXIU(Xie et al., 2023) proposed more comprehensive benchmark, covering more financial tasks and datasets than FLUE and introducing stock movement prediction task. Building on PIXIU, FinBen(Xie et al., 2024) added numerous financial datasets to thoroughly evaluate large language models (LLMs) and assess their proficiency across diverse financial scenarios. In the Chinese financial domain, BBT-Benchmark(Lu et al., 2023a) was the first Chinese benchmark for financial evaluation, covering financial news classification, summarization, relation extraction, question answering, negative news detection, and sentiment classification for financial social media texts. Additionally, the FinEval(Zhang et al., 2023) benchmark provides an evaluation framework for LLM knowledge in finance, including multiple-choice datasets across finance, economy, accounting, and certification fields. Subsequently, the FinanceIQ(Zhang and Yang, 2023) and CFBenchmark(Lei et al., 2023) benchmarks were introduced, with FinanceIQ including broader range of Chinese financial multiple-choice datasets, while CFBenchmark focuses on three major financial task categories: financial entity recognition, financial text classification, and financial content generation. However, data quality issues in certain tasks within these benchmarkscharacterized by inconsistent, unstructured input informationoften hinder the accurate comprehension and response generation of financial LLMs. Large language models currently face challenges in numerical understanding and computation, which may be attributed to multiple factors such as tokenizers, positional encoding, and others (Akhtar et al., 2023; Schwartz et al., 2024; Shen et al., 2023). Therefore, using specialized agents is often more ideal approach for handling these tasks rather than relying on vanilla large language models. This issue is especially evident in PIXIUs credit analysis, question answering, and stock movement prediction tasks, where model performance metrics are frequently low. Additionally, varying label formats across datasets for the same task make it challenging to match model outputs with labels directly. Notably, most current financial benchmarks are monolingual, lacking bilingual evaluation options to assess financial models comprehensively in both English and Chinese. Table 1 Diversity of Financial Analysis Tasks Across Different Financial Large Language Model Benchmarks Benchmarks FinGPT-Bench (Wang et al., 2023a) FinBen (Xie et al., 2024) BBT-Fin (Lu et al., 2023a) Fin-Eval (Zhang et al., 2023) FinanceIQ (Zhang and Yang, 2023) CFBenchmark (Lei et al., 2023) Golden-Touchstone Sent. Anal. Classif. Ent. Extr. Rel. Extr. Multi. Choice Summ. Quest. Ans. Stock Pred. 5 Table 2 Language Coverage, Systematicity, Adaptability, and Model Training Stage for Benchmarks. Systematicity refers to whether benchmarks are established according to comprehensive system standard. Adaptability indicates whether the tasks are suitable for large language models. Benchmarks Language Systematicity Adaptability Model Training EN CN Cont. Pre-train Instr. Tuning FinGPT-Bench (Wang et al., 2023a) FinBen (Xie et al., 2024) BBT-Fin (Lu et al., 2023a) Fin-Eval (Zhang et al., 2023) FinanceIQ (Zhang and Yang, 2023) CFBenchmark (Lei et al., 2023) Golden-Touchstone Medium High Medium High Medium High High High Medium High High High High High To address these limitations, we propose unified benchmark that incorporates representative financial datasets in both English and Chinese. By selecting high-quality, authoritative datasets and optimizing inputssuch as using news-based input for stock trend prediction rather than tabular datawe aim to make tasks more compatible with LLM assessment. This bilingual benchmark will facilitate model evaluation on both English and Chinese financial tasks, align financial NLP tasks across languages, and provide holistic measure of financial model performance across linguistic contexts. The financial sub-tasks included in previous financial benchmarks are shown in Table.1, which illustrates the diversity of financial analysis tasks across different benchmarks. Table.2 provides an overview of the language coverage, systematicity, adaptability, and model training of these benchmarks. In this context, systematicity refers to whether benchmarks are established according to comprehensive and standardized framework, while adaptability assesses whether the tasks are suitable for large language models. As seen in Table 1, the task coverage of earlier benchmarks is not fully comprehensive, with some financial tasks missing. To address this, we selected representative datasets from various open-source benchmarks based on the task categories in Table 1, ensuring broad diversity of tasks in the Golden-Touchstone benchmark. Therefore, Golden-Touchstone includes tasks that broadly cover those in previous benchmarks. From Table 2, it is evident that most current financial benchmarks are monolingual, with inconsistent levels of systematicity and adaptability. In response, we have compiled the first bilingual financial benchmark from open-source datasets, achieving high standards in terms of both systematicity and adaptability. 3.2. Golden Touchstone Benchmark Design Golden Touchstone benchmark redefines popular open-source financial benchmarks, as illustrated in Figure 2. We classify financial NLP tasks by typefinancial NLU (Natural Language Understanding) and financial NLG (Natural Language Generation)and by language, distinguishing between English and Chinese datasets. These dimensions guide the organization of high-quality open-source datasets. NLU tasks include Financial Sentiment Analysis, Financial Knowledge Examination, Financial Information Extraction, and Financial Text Understanding, while NLG tasks include Financial Summary Analysis and Financial Question Answering. Collectively, these cover eight sub-tasks: Sentiment Analysis, Classification, Entity Recognition, Relation Extraction, Multiple Choice, Summarization, Question Answering, and Stock Movement Prediction. For the English benchmark, tasks were selected primarily from the PIXIU(Xie et al., 2023) and FinBen(Xie et al., 2024) financial benchmarks, covering sentiment Figure 2 Financial NLP tasks are categorized along two dimensions: task types, divided into financial NLU (Natural Language Understanding) and financial NLG (Natural Language Generation), and language, categorized as English and Chinese. We organized the collected high-quality datasets along these axes. analysis, classification, entity recognition, question answering, stock movement prediction, summarization, and credit scoring. Due to low data quality and inconsistent labeling formats in some datasets, we excluded certain lower-quality data while retaining those with relatively higher quality. FinBens credit scoring task, binary classification task, was merged into the general classification task. Additionally, as the stock movement prediction task in FinBen uses time-series tabular data, which tends to be ineffective for prediction in large language models (LLMs) due to LLMs limited sensitivity to numerical data in inference contexts, we replaced this task with the DJIA (Aaron7Sun, 2016) dataset, using daily news to predict Dow Jones Index trends. To ensure alignment with the Chinese benchmark tasks, we also incorporated the Financial Relation Extraction task from FinGPT (Yang et al., 2023) and the Multiple Choice task from the CPA-Eval (Yang et al., 2024b) dataset, using the validation set as the test set due to the absence of labels in the test set. For the Chinese benchmark, tasks were chosen primarily from the BBT-benchmark (Lu et al., 2023a) and FinEval benchmark (Zhang et al., 2023) , including sentiment analysis, classification, entity extraction, question answering, summarization, relation extraction, and multiple-choice tasks. As test sets for these benchmarks lack labels, we used the validation set for evaluation. To expand the multiple-choice tasks, we added the CPA (Yang et al., 2024b) dataset, using its validation set as test set, also without labels. For alignment with the English benchmark tasks, we introduced stock movement prediction with the stockA (Zou et al., 2022) dataset, which includes individual stock news and stock data for five days post-publication. The price movement label is based on the difference between the final days closing price and the initial net asset value, enabling LLM-based predictions on stock trends informed by news. Dataset details are provided in Table.3 and Table.4. English Finance Benchmark: We constructed this benchmark primarily based on the PIXIU and FinBen Benchmark. For sentiment analysis, we selected the FPB (Malo et al., 2014) and 7 Table 3 Overview of English Finance Evaluation Datasets by Task Type, Sample Sizes (Training, Validation, Test), and Evaluation Metrics"
        },
        {
            "title": "FPB",
            "content": "3100 776 970 FiQA-SA"
        },
        {
            "title": "Headlines",
            "content": "71900"
        },
        {
            "title": "FOMC",
            "content": "1984 - 496 lendingclub 9417 2691 Weighted-F1 ACC Weighted-F1 ACC Weighted-F1 ACC Weighted-F1 ACC Weighted-F1 MCC"
        },
        {
            "title": "NER",
            "content": "408 103 98 Entity-F"
        },
        {
            "title": "FinRE",
            "content": "27558 - 5112 Relation-F"
        },
        {
            "title": "CFA",
            "content": "1884"
        },
        {
            "title": "Summarization",
            "content": "EDTSUM 8000 -"
        },
        {
            "title": "FinQa\nConvfinQa",
            "content": "6251 8890 883 2210"
        },
        {
            "title": "DJIA",
            "content": "1591 - 398 Weighted-F1 ACC"
        },
        {
            "title": "RMACC\nRMACC",
            "content": "Weighted-F1 ACC 8 Table 4 Overview of Chinese Finance Evaluation Datasets by Task Type, Sample Sizes (Training, Validation, Test), and Evaluation Metrics"
        },
        {
            "title": "Sentiment Analysis",
            "content": "FinFE-CN 16157 2020 2020 Weighted-F1 ACC"
        },
        {
            "title": "Classification",
            "content": "FinNL-CN 7071"
        },
        {
            "title": "Entity Extraction",
            "content": "FinESE-CN 14252"
        },
        {
            "title": "Relation Extraction",
            "content": "FinRE-CN 13486"
        },
        {
            "title": "FinEval",
            "content": "1071"
        },
        {
            "title": "CPA",
            "content": "6268"
        },
        {
            "title": "Summarization",
            "content": "FinNA-CN 28800"
        },
        {
            "title": "Question Answering",
            "content": "FinQa-CN 19906 FincQa-CN 21965 2469 2741"
        },
        {
            "title": "AStock",
            "content": "11815 1477 1477 Weighted-F1 ACC Weighted-F1 ACC"
        },
        {
            "title": "RMACC\nRMACC",
            "content": "Weighted-F1 ACC FiQA-SA (Maia et al., 2018b) datasets, evaluated using accuracy (ACC) and weighted F1 score (weighted-F1), two widely adopted classification metrics. For classification tasks, we used the Headlines (Sinha and Khandait, 2021), FOMC (Shah et al., 2023), and LendingClub (Feng et al., 2023) datasets. Evaluation of the Headlines and FOMC datasets was conducted with ACC and weighted-F1, while the LendingClub dataset was assessed using Matthews Correlation Coefficient (MCC) and accuracy to align with FinBen standards. For entity extraction, we used the NER (Alvarado et al., 2015) dataset, employing entity-level F1 score (Entity-F1) in line with the PIXIU framework. The EDTSUM (Zhou et al., 2021) dataset was selected for summarization tasks, evaluated using Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002) and ROUGE scores (Lin, 2004), which are standard metrics in text generation. For question answering, we chose the FinQA (Chen et al., 2021) and ConvFinQA (Chen et al., 2022) datasets, opting for Regex Match Accuracy (RMACC) over exact match accuracy, as RMACC better accommodates our general-purpose models that may not directly output exact answers. For relation extraction, we used the FinRED (Sharma et al., 2022) dataset, evaluated with the Relation-F1 metric, consistent with FinGPT. The CFA (Yang et al., 2024b) dataset was used for multiple-choice tasks, evaluated using weighted-F1 and ACC. Finally, we selected the DJIA (Aaron7Sun, 2016) dataset for stock movement prediction, evaluated using ACC and weightedF1. Unlike PIXIUs use of MCC for this task, we employed weighted-F1 due to generation models tendency to produce low-accuracy [unknown] labels. Chinese Finance Benchmark: We constructed this benchmark primarily based on the BBTBenchmark (Lu et al., 2023a) and FinEval Benchmark (Zhang et al., 2023). For sentiment analysis, we used the FinFE-CN dataset, evaluated by weighted-F1 and ACC. For question answering, we used the FinQA-CN and FinCQA-CN datasets, assessed using RMACC. Classification tasks employed FinNL-CN, requiring full matches for multi-answer questions and evaluated by Overall Regex Match Accuracy (ORMACC).For relation extraction, we used the FinRE-CN 9 dataset, also evaluated with RMACC. The entity extraction task employed the FinESE-CN dataset, evaluated using ORMACC due to the datasets lack of BIO labels for original text entities. Summarization tasks used the FinNA-CN dataset, evaluated with BLEU and ROUGE scores in line with the English benchmark. For multiple-choice tasks, we used FinEval and CPA (Yang et al., 2024b), with FinEval combining finance, economy, accounting, and certification questions, both evaluated with weighted-F1 and ACC. The AStock (Zou et al., 2022) dataset was used for stock movement prediction, assessed by ACC and weighted-F1. In tasks with [unknown] labels, such as classification, multiple choice, and stock prediction, we treated [unknown] labels as incorrect to maintain consistency. This adjustment proved more reliable than alternative methods like sample omission, random assignment, or fixed selection, which may introduce bias. 4. Experiments 4.1. Experimental Setup Baselines. We conducted an extensive experimental evaluation against the Golden Touchstone Benchmark, incorporating comprehensive array of models. For all models and inference tasks, we set the PyTorch and CUDA random seeds and configured the model with greedy decoding strategy. This ensures reproducibility of experimental results and eliminates the influence of sampling decoding strategies on the final generated outputs. This included cutting-edge commercial models such as GPT-4o (OpenAI, 2023), alongside prominent open-source alternatives like Meta Llama-3 (AI@Meta, 2024) and Alibaba Qwen-2 (Yang et al., 2024a). Additionally, we integrated the latest and most influential financial language models (FInLLMs), namely FinGPT (Yang et al., 2023), FinMA (Xie et al., 2024), CFGPT (Li et al., 2023), and DISC-FinLLM Chen et al. (2023). These models were meticulously selected to represent diverse spectrum of capabilities, ranging from general-purpose language understanding to specialized financial domain expertise. Our experiments aimed to rigorously assess the performance, robustness, and adaptability of each model within the context of financial data processing and analysis. The results provide valuable insights into the strengths and limitations of current state-of-the-art models, offering foundation for future advancements in financial language modeling. Touchstone-GPT Training. To further contribute to the research and development of FInLLMs and Financial benchmarks for LLMs, we have meticulously trained and open-sourced Touchstone-GPT model. This initiative aims to serve as valuable resource for advancing the field, providing robust and versatile model that can be utilized for wide range of financial language tasks. By making the Touchstone-GPT model publicly available, we aspire to foster collaboration and innovation within the research community, facilitating the development of more sophisticated and effective financial language models. Our hope is that this open-source contribution will not only enhance the current state of research but also inspire new methodologies and benchmarks that push the boundaries of what is possible in financial language modeling. We adopted two-stage training strategy comprising continuous pre-training and post-training, based on the Qwen-2 (Yang et al., 2024a) foundational model. During the continuous pre-training phase, we initially conducted pre-training on high-quality financial corpus containing 100 billions tokens, which included textbooks, encyclopedias, research reports, news articles, and real-time analysis, all meticulously cleaned. In the post-training phase, we employed standard instruction fine-tuning strategy, collecting, cleaning, and formatting high-quality dataset of 300,000 instruction-response pairs shown in Tabel.3 and Table.4. To avoid catastrophic forgetting in general tasks, we also incorporated general-domain pre-training corpora (Gan et al., 2023) 10 Figure 3 Comparison of different models performance across tasks in the Golden Touchstone benchmark, illustrating average performance for English and Chinese tasks respectively. and instruction-tuning corpora (Peng et al., 2023) into continuous pre-training and post-training. This culminated in the final Touchstone-GPT model. We utilized Megatron(Shoeybi et al., 2019) for continuous pre-training and LlamaFactory(Zheng et al., 2024) for instruction post-training as our training frameworks, respectively. In this study, we employ an advanced model training setup using the AdamW optimizer (Kingma and Ba, 2014) with learning rate of 1.0e-5, cosine annealing scheduler (Szegedy et al., 2016), and 10% warmup ratio (Goyal et al., 2017; He et al., 2016) to enhance training stability and convergence. We enable gradient accumulation (Shoeybi et al., 2019) and checkpointing (Chen et al., 2016) to simulate larger batch sizes and reduce memory footprint. Training is conducted in mixed bfloat16 precision (Micikevicius et al., 2017; Wang and Kanwar, 2019) with DeepSpeeds ZeRO-1 optimization (Rajbhandari et al., 2020), reducing memory consumption and allowing for larger model training. This comprehensive setup optimizes efficiency and performance, providing an effective solution for large-scale deep learning model training. Our training was conducted on 4 NVIDIA DGX servers, each equipped with 8 A100 GPUs, and spanned period of 4 weeks. Inference was performed on single NVIDIA DGX server with eight A100 GPUs, utilizing parallel batch inference. During the pre-training phase, we employed data packing strategy (Krell et al., 2021) and batch dynamic right padding strategy in the instruction tuning phase (Wolf et al., 2020), while the inference phase incorporated batch left padding strategy (Wolf et al., 2020). 4.2. Evaluation Results In this section, we provide detailed analysis of the evaluation and results for both the English and Chinese benchmarks. We discuss task-specific performances and identify key areas of strengths and weaknesses for each model. The following sections present insights for the English and Chinese benchmarks, each highlighting the differences in model capabilities across variety of NLP tasks. 11 4.2.1. Results of English Benchmark We analyze the English benchmark results shown in Table.5 across task categories shown in left column of Figure.3 such as Sentiment Analysis, Classification, and Summarization. We highlight specific strengths and weaknesses observed in each task and metric. GPT-4o consistently performs well in tasks like Sentiment Analysis with Weighted-F1 score of 0.8084 and an accuracy of 0.8093 on the FPB dataset and Multiple Choice, showcasing robustness in understanding sentiment-based and structured questions. However, it struggles significantly in Relation Extraction achieving Relation-F1 score of 0.1613 on the FinRE dataset and Entity Extraction with an Entity-F1 score of 0.1800 on the NER dataset, suggesting difficulties with detailed information extraction and handling relationships. FinMA-7B excels in Sentiment Analysis on the FPB dataset, achieving Weighted-F1 score of 0.9400 and an accuracy of 0.9402 but has difficulties in Classification on the FOMC dataset, with Weighted-F1 score of 0.3988 and an accuracy of 0.4274 and Summarization on the EDTSUM dataset, with Rouge-1, Rouge-2, and BLEU scores all below 0.16, indicating lack of versatility in broader NLP tasks despite its sentiment strengths. Qwen-2-7B-Instruct performs well in Sentiment Analysis with WeightedF1 score of 0.7965 on the FPB dataset but is weak in Summarization on the EDTSUM dataset, achieving Rouge-2 score of 0.0433 and Question Answering with an RMACC of 0.0270 on the Finqa dataset. Llama-3-8B-Instruct stands out in Stock Movement Prediction achieving Weighted-F1 score of 0.5116 but struggles in Entity Extraction achieving an Entity-F1 score of 0.2973, highlighting its capacity in structured tasks but limitations in text understanding. FinGPT-8B-lora shows consistent underperformance, especially in Relation Extraction (FinRE, Relation-F1: 0.0100) and Summarization (EDTSUM, BLEU: 0.0592), indicating significant gaps in training these complex financial tasks, necessitating targeted model improvements. The results of many models on lengdingclub are not good enough, this is because of the specialization of the dataset, if the model has not been trained on similar dataset, the results from zero-shot inference are definitely not good, which emphasizes the importance of instruction fine-tuning for large models in finance. Touchstone-GPT, built on the strong foundation of Qwen-2-7B, utilizes high-quality financial pre-training and instruction tuning datasets through continuous pre-training and post-training, resulting in competitive performance across most tasks. In summary, the analysis reveals that while GPT-4o and FinMA-7B perform well in sentiment tasks, Llama-3-8B-Instruct has strengths in stock movement prediction, but its still not at practical level. All models, including Touchstone-GPT, have limitations in handling broader NLP tasks. 4.2.2. Results of Chinese Benchmark We present an analysis of the Chinese benchmark results across tasks such as Sentiment Analysis, Entity Extraction, and Summarization. Specific task metrics are discussed to highlight strengths and weaknesses. GPT-4o excels in Question Answering with an RMACC of 0.6578 on the FinQa-CN dataset and Entity Extraction achieving an ORMACC of 0.6867 on the FinESE-CN dataset but struggles in Classification with score of 0.3303 on the FinNL-CN dataset and Relation Extraction with score of 0.2754 on the FinRE-CN dataset, showing strength in detailed extraction but limitations in handling complex relationships. Qwen-2-7B-Instruct shows moderate but balanced performance, with strengths in Multiple Choice achieving Weighted-F1 score of 0.7230 on the FinEval dataset but weaknesses in Relation Extraction achieving an ORMACC of 0.1330 and Entity Extraction achieving an ORMACC of 0.3678. Llama-3-8B-Instruct performs particularly 12 well in Stock Movement Prediction (Weighted-F1: 0.4929) but is weak in Sentiment Analysis with score of 0.4262 on the FinFe-CN dataset and Classification with score of 0.0747, indicating less capability in detailed financial text understanding. CFGPT1-7B-Full struggles in most tasks, particularly in Classification with score of 0.0894 and Relation Extraction (0.0678), with low scores across Summarization metrics with BLEU score of 0.0238 on the FinNA-CN dataset, implying need for enhanced adaptability and specialized training. DISC-FinLLM-Full shows moderate potential in Entity Extraction with 0.4346 ORMACC but lacks in Relation Extraction (0.1182) and Classification (0.0011), suggesting opportunities for targeted optimization to enhance broader capabilities. As mentioned aboveTouchstone-GPT, based on the Qwen-2-7B model, leverages carefully curated financial pre-training data and instruction tuning through continuous pre-training and post-training, achieving competitive results across most tasks. In conclusion, while GPT-4o and Llama-3-8B-Instruct have areas of strength in Entity Extraction and Stock Movement Prediction, the models overall, including Qwen-2-7B-Instruct, CFGPT1-7B-Full, and DISC-FinLLM-Full, exhibit considerable gaps in handling relational and classification tasks in Chinese, highlighting the challenges of processing complex Chinese financial text. The need for targeted model improvements remains evident for these tasks. 4.3. Analysis of Evaluation Results In this overall analysis, we synthesize insights from both the English and Chinese benchmarks by evaluating the strengths and weaknesses of each model, as well as examining the performance across different types of tasks and datasets. From the perspective of individual models, GPT-4o shows strong performance in sentiment analysis and structured tasks like multiple choice, indicating robust general language understanding capabilities. However, its weakness lies in relation extraction and detailed entity extraction, which require detailed understanding of complex financial relations. FinMA7B stands out in sentiment tasks but lacks versatility, especially in question answering and summarization, likely due to the absence of targeted training for diverse NLP challenges. Qwen2-7B-Instruct has balanced yet modest performance, doing well in sentiment analysis but struggling significantly in question answering and summarization, which suggests need for more specialized post-training. Llama-3-8B-Instruct excels in stock movement prediction, but shows limitations in tasks requiring deep linguistic understanding, such as entity and relation analysis. The metrics of FinGPT-8B-lora indicating that the current level of domain-specific tuning is insufficient for complex financial tasks. Finally, DISC-FinLLM-Full and CFGPT1-7B-Full demonstrate moderate strengths in entity extraction tasks but lack the robustness needed for broader NLP capabilities, revealing significant gaps in financial language comprehension. From task perspective, we observe that Sentiment Analysis generally yields high scores across most models, particularly for the English benchmark, indicating that sentiment understanding, even in financial contexts, is relatively well addressed by these models. In contrast, Relation Extraction and Question Answer in financial domain exhibit notably lower performance, especially for the Chinese benchmark. These results suggest that capturing financial relationships and classifying detailed financial statements pose greater challenges, requiring more sophisticated training datasets or better model architectures. The LendingClub dataset in Classification is specialized dataset in the field of risk control, requiring more targeted finetuning to achieve good results. Stock Movement Prediction also shows low performance across most models, with only few models such as Llama-3-8B-Instruct demonstrating relatively moderate performance, but it is still practically unusable, highlighting the inherent difficulty of this task. Market prediction relying solely on news information is likely insufficient; volume13 Table 5 Performance metrics of financial large language models across english tasks like Sentiment Analysis, Classification, and Summarization. Models include GPT-4o, Llama-3-8B, Qwen-2-7B, FinMA-7B, FinGPT-8B, and Touchstone-GPT. The best results of each dataset are marked in bold. Task Dataset Metrics GPT-4o FinMA-7B full Qwen-2-7B Instruct Llama-3-8B Instruct FinGPT-8B lora Touchstone GPT Sentiment Analysis FPB Fiqa-SA Headlines Classification FOMC lendingclub Weighted-F1 ACC Weighted-F1 ACC Weighted-F1 ACC Weighted-F1 ACC Weighted-F1 MCC 0.8084 0.8093 0.8106 0.7702 0.7857 0.7931 0.6603 0.6794 0.6730 0.1642 Entity Extraction Relation Extraction Multiple Choice NER Entity-F 0.1800 FinRE Relation-F1 0.1613 CFA Weighted-F1 ACC Rouge-1 Rouge-2 Rouge-L BLEU RMACC Convfinqa RMACC Finqa DJIA Weighted-F1 ACC 0.7700 0.7700 0.1675 0.0556 0.1069 0.1192 0.1037 0.2540 0.4241 0.4648 Summarization EDTSUM Question Answering Stock Movement Prediction 0.9400 0.9402 0.8370 0.8340 0.9739 0.9739 0.3988 0.4274 0.1477 -0.6218 0.6200 0.0054 0.2200 0.2400 0.1566 0.0491 0.1060 0.1361 0.0497 0.0953 0.3211 0.3291 0.7965 0.8000 0.6726 0.5957 0.7278 0.7252 0.6112 0.6210 0.5938 0.1714 0. 0.1083 0.6697 0.6700 0.1466 0.0433 0.0857 0.0999 0.0270 0.0644 0.2744 0.4372 0.7631 0.7660 0.7515 0.7064 0.7006 0.7004 0.4904 0.5625 0.5943 0.1670 0.2973 0.0540 0.5800 0.5800 0.1467 0.0429 0.0930 0.1085 0.0470 0.1477 0.5116 0. 0.2727 0.3072 0.5885 0.5872 0.4516 0.4331 0.2758 0.2702 0.5480 -0.1120 0.0231 0.0100 0.3993 0.3800 0.0622 0.0085 0.0412 0.0592 0.0110 0.0772 0.2171 0.2211 0.8576 0.8557 0.8591 0.8638 0.9866 0.9866 0.8788 0.8790 0.9783 0.9297 0. 0.5331 0.7497 0.7500 0.5254 0.3446 0.4705 0.4512 0.2258 0.5053 0.4396 0.4749 price data and factor analysis can provide more comprehensive information. However, current large language models are unable to process these inputs, which is significant area of future research. Summarization also stands out as weak area for most models, with consistently low BLEU and Rouge scores, reflecting the challenges in generating concise, coherent summaries of complex financial text. Overall, the insights suggest that while models like GPT-4o, FinMA-7B, and TouchstoneGPT have particular strengths in sentiment analysis and some structured tasks, the overall capability to handle comprehensive financial NLP tasks remains limited. Most models require targeted improvements, especially for relation extraction, summarization, question answering and stock movement prediction in both English and Chinese contexts. This calls for more domain-specific training and the development of specialized datasets that focus on capturing the detailed and often complex financial language, which is crucial for advancing the performance of financial large language models. Furthermore, while Touchstone-GPT demonstrates competitive performance across various tasks due to its robust pre-training and instruction tuning, ongoing refinements and specialized tuning efforts are needed to address specific deficiencies observed in tasks such as summarization, relation extraction, question answering and stock movement prediction. 5. Conclusion and Future Work In this study, we introduce the Golden Touchstone benchmark, the inaugural structured and comprehensive bilingual benchmark specifically designed for English-Chinese financial NLP. This benchmark encompasses wide array of financial NLP tasks, including Natural Language Un14 Table 6 Performance metrics of financial large language models across chinese tasks like Sentiment Analysis, Classification, and Summarization. Models include GPT-4o, Llama-3-8B, Qwen-2-7B, CFGPT-7B, DISC-FinLLM, and Touchstone-GPT. The best results of each dataset are marked in bold. Task Dataset Metrics GPT-4o Qwen-2-7B Instruct Llama-3-8B Instruct CFGPT1-7B Full DISC-FinLLM Full Touchstone GPT Sentiment Analysis FinFe-CN Weighted-F1 ACC 0.6593 0.6500 Classification FinNL-CN ORMACC 0.3303 0.6274 0. 0.0622 FinESE-CN ORMACC 0.6867 0.3678 FinRE-CN RMACC 0. Entity Extraction Relation Extraction Multiple Choice FinEval CPA Weighted-F1 ACC Weighted-F1 ACC Rouge-1 Rouge-2 Rouge-L BLEU Summarization FinNA-CN Question Answering Stock Movement Prediction FinQa-CN RMACC FinCQa-CN RMACC AStock Weighted-F1 ACC 0.7364 0.7353 0.6312 0.6309 0.3197 0.1434 0.2511 0.1423 0.6578 0.4765 0.5007 0.5017 0. 0.7230 0.7235 0.6957 0.6960 0.3326 0.1597 0.2644 0.1541 0.5043 0.3422 0.4906 0.4915 0.3633 0.4891 0.0747 0.3088 0.1296 0.4432 0.4471 0.3421 0.3504 0.3477 0.1702 0.2802 0.1672 0.4540 0.3787 0.4903 0. 0.2528 0.2732 0.0894 0.3863 0.0678 0.3543 0.3529 0.3543 0.3553 0.1018 0.0263 0.0650 0.0238 0.1126 0.2714 0.4631 0.4888 0.4177 0. 0.0011 0.4346 0.1182 0.4288 0.4294 0.3451 0.3518 0.3486 0.1678 0.2997 0.1885 0.3949 0.2134 0.4142 0.4144 0.7888 0.7936 0. 0.9074 0.6541 0.7361 0.7353 0.9238 0.9238 0.5526 0.3603 0.5214 0.3944 0.9214 0.8552 0.4003 0.5587 derstanding (NLU) and Natural Language Generation (NLG) across eight categories: Sentiment Analysis, Classification, Entity Extraction, Summarization, Stock Market Prediction, Question Answering, Relation Extraction, and Multiple Choice. By leveraging existing high-quality opensource financial datasets, we curated representative datasets and selected appropriate evaluation metrics for each task category. Utilizing these resources, we conducted extensive evaluations of current models such as GPT-4o and prominent open-source financial LLMs, including FinGPT and FinMA, thereby establishing performance benchmarks for financial LLMs within bilingual contexts. Moreover, we contributed to the community by open-sourcing Touchstone-GPT, robust financial LLM that employs two-stage training approach and has demonstrated superior input-based inference capabilities on the Golden Touchstone benchmark compared to GPT-4o. Our open-source initiative provides bilingual English-Chinese evaluation framework aimed at fostering the sustainable development of LLMs in multilingual financial environment. Despite these advancements, the benchmark currently exhibits certain limitations, including limited range of NLG tasks and focus solely on single-modality. Future enhancements will include the integration of additional NLG tasks, such as extended text generation for financial report analysis and more sophisticated sentiment assessments. Furthermore, we plan to expand the benchmark to cover other financial sectors such as insurance, cryptocurrency, and futures trading, thus broadening the scope and applicability of financial LLM assessments across diverse scenarios. Also, the performance of Touchstone-GPT on specific tasks within the Golden Touchstone benchmark, particularly in stock market prediction, requires further improvement. Our subsequent research will explore the incorporation of agent-based and retrieval-augmented generation (RAG) methods to augment the models capabilities in numerical computation and real-time news analysis. Additionally, we aim to venture into multimodal modeling, integrating visual data and time-series data for tasks such as financial time-series forecasting, financial chart analysis, and content generation."
        },
        {
            "title": "References",
            "content": "Aaron7Sun. Stocknews dataset. https://www.kaggle.com/datasets/aaron7sun/stoc knews, 2016. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/bl ob/main/MODEL_CARD.md. M. Akhtar, A. Shankarampeta, V. Gupta, A. Patil, O. Cocarascu, and E. Simperl. Exploring the numerical reasoning capabilities of language models: comprehensive analysis on tabular data. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1539115405, 2023. J. C. S. Alvarado, K. Verspoor, and T. Baldwin. Domain adaption of named entity recognition to support credit risk assessment. In Proceedings of the Australasian Language Technology Association Workshop 2015, pages 8490, 2015. A. Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305. G. Bhatia, E. M. B. Nagoudi, H. Cavusoglu, and M. Abdul-Mageed. Fintral: family of gpt-4 level multimodal financial large language models. arXiv preprint arXiv:2402.10986, 2024. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. W. Chen, Q. Wang, Z. Long, X. Zhang, Z. Lu, B. Li, S. Wang, J. Xu, X. Bai, X. Huang, et al. Disc-finllm: chinese financial large language model based on multiple experts fine-tuning. arXiv preprint arXiv:2310.15205, 2023. Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang, In B. R. Routledge, et al. Finqa: dataset of numerical reasoning over financial data. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, 2021. Z. Chen, S. Li, C. Smiley, Z. Ma, S. Shah, and W. Y. Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 62796292, 2022. J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan. Chatlaw: Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv:2306.16092, 2023. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. 16 D. Feng, Y. Dai, J. Huang, Y. Zhang, Q. Xie, W. Han, Z. Chen, A. Lopez-Lira, and H. Wang. Empowering many, biasing few: Generalist credit scoring through large language models. arXiv preprint arXiv:2310.00566, 2023. FinSBD3. Financial sbd 3. https://sites.google.com/nlg.csie.ntu.edu.tw/finweb2 021/shared-task-finsbd-3, 2021. R. Gan, Z. Wu, R. Sun, J. Lu, X. Wu, D. Zhang, K. Pan, P. Yang, Q. Yang, J. Zhang, et al. Ziya2: Data-centric learning is all llms need. arXiv preprint arXiv:2311.03301, 2023. P. Goyal, P. Dollr, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. D. P. Kingma and J. Ba. Adam: method for stochastic optimization. arXiv e-prints, pages arXiv1412, 2014. M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. arXiv preprint arXiv:2107.02027, 2021. URL http://arxiv.org/abs/2107.02027. Y. Lei, J. Li, M. Jiang, J. Hu, D. Cheng, Z. Ding, and C. Jiang. Cfbenchmark: Chinese financial assistant benchmark for large language model. arXiv preprint arXiv:2311.05812, 2023. J. Li, Y. Bian, G. Wang, Y. Lei, D. Cheng, Z. Ding, and C. Jiang. Cfgpt: Chinese financial assistant with large language model. arXiv preprint arXiv:2309.10654, 2023. X. Li, X. Shen, Y. Zeng, X. Xing, and J. Xu. Finreport: Explainable stock earnings forecasting via news factor analyzing model. In Companion Proceedings of the ACM on Web Conference 2024, pages 319327, 2024. C.-Y. Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. A. Lopez-Lira and Y. Tang. Can chatgpt forecast stock price movements? return predictability and large language models. arXiv preprint arXiv:2304.07619, 2023. D. Lu, H. Wu, J. Liang, Y. Xu, Q. He, Y. Geng, M. Han, Y. Xin, and Y. Xiao. Bbt-fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark. arXiv preprint arXiv:2302.09432, 2023a. J. Lu, D. Zhang, X. Wu, X. Gao, R. Gan, J. Zhang, Y. Song, and P. Zhang. Ziya-vl: Bilingual large vision-language model via multi-task instruction tuning. arXiv preprint arXiv:2310.08166, 2023b. N. Magic. Twitter financial news sentiment. Huggingface repository, 2022. M. Maia, S. Handschuh, A. Freitas, B. Davis, R. McDermott, M. Zarrouk, and A. Balahur. Www18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of the The Web Conference 2018, WWW 18, page 19411942, Republic and Canton of Geneva, CHE, 2018a. International World Wide Web Conferences Steering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3192301. URL https://doi.org/10.1145/3184558.3192301. M. Maia, S. Handschuh, A. Freitas, B. Davis, R. McDermott, M. Zarrouk, and A. Balahur. Www18 open challenge: Financial opinion mining and question answering. pages 19411942, 04 2018b. ISBN 9781450356404. doi: 10.1145/3184558.3192301. P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782796, 2014. P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. R. OpenAI. Gpt-4 technical report. arXiv, pages 230308774, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. E. Schwartz, L. Choshen, J. Shtok, S. Doveh, L. Karlinsky, and A. Arbelle. Numerologic: Number encoding for enhanced llms numerical reasoning. arXiv preprint arXiv:2404.00459, 2024. A. Shah, S. Paturi, and S. Chava. Trillion dollar words: new financial dataset, task & market analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66646679, 2023. R. S. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du, S. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang. When flue meets flang: Benchmarks and large pre-trained language model for financial domain. arXiv preprint arXiv:2211.00083, 2022. S. Sharma, T. Nayak, A. Bose, A. K. Meena, K. Dasgupta, N. Ganguly, and P. Goyal. Finred: dataset for relation extraction in financial domain. In Companion Proceedings of the Web Conference 2022, pages 595597, 2022. R. Shen, S. Bubeck, R. Eldan, Y. T. Lee, Y. Li, and Y. Zhang. Positional description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023. 18 M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. A. Sinha and T. Khandait. Impact of news on the commodity market: Dataset and results. In Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2, pages 589601. Springer, 2021. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28182826, 2016. G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large language models in medicine. Nature medicine, 29(8):19301940, 2023. Y. Tian, R. Gan, Y. Song, J. Zhang, and Y. Zhang. Chimed-gpt: chinese medical large language model with full training regime and better alignment to human preferences. arXiv preprint arXiv:2311.06025, 2023. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. N. Wang, H. Yang, and C. D. Wang. Fingpt: Instruction tuning benchmark for open-source large language models in financial datasets. arXiv preprint arXiv:2310.04793, 2023a. S. Wang and P. Kanwar. Bfloat16: The secret to high performance on cloud tpus. Google Cloud Blog, 4(1), 2019. Z. Wang, Y. Li, J. Wu, J. Soon, and X. Zhang. Finvis-gpt: multimodal large language model for financial chart analysis. arXiv preprint arXiv:2308.01430, 2023b. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, Oct. 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnl p-demos.6. S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564, 2023. C. Xiao, X. Hu, Z. Liu, C. Tu, and M. Sun. Lawformer: pre-trained language model for chinese legal long documents. AI Open, 2:7984, 2021. 19 Q. Xie, W. Han, X. Zhang, Y. Lai, M. Peng, A. Lopez-Lira, and J. Huang. Pixiu: large language model, instruction data and evaluation benchmark for finance. arXiv preprint arXiv:2306.05443, 2023. Q. Xie, W. Han, Z. Chen, R. Xiang, X. Zhang, Y. He, M. Xiao, D. Li, Y. Dai, D. Feng, et al. The finben: An holistic financial benchmark for large language models. arXiv preprint arXiv:2402.12659, 2024. A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. C. Yang, C. Xu, and Y. Qi. Financial knowledge large language model. arXiv preprint arXiv:2407.00365, 2024b. H. Yang. Data-centric fingpt. open-source for open finance. https://github.com/AI4Fina nce-Foundation/FinGPT, 2023. H. Yang, X.-Y. Liu, and C. D. Wang. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023. H. Yang, B. Zhang, N. Wang, C. Guo, X. Zhang, L. Lin, J. Wang, T. Zhou, M. Guan, R. Zhang, et al. Finrobot: An open-source ai agent platform for financial applications using large language models. arXiv preprint arXiv:2405.14767, 2024c. A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. Y. Yu, H. Li, Z. Chen, Y. Jiang, Y. Li, D. Zhang, R. Liu, J. W. Suchow, and K. Khashanah. Finme: performance-enhanced large language model trading agent with layered memory and character design. arXiv preprint arXiv:2311.13743, 2023. A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu, P. Zhang, Y. Dong, and J. Tang. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=-Aw0rrrPUF. J. Zhang, R. Gan, J. Wang, Y. Zhang, L. Zhang, P. Yang, X. Gao, Z. Wu, X. Dong, J. He, et al. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. arXiv preprint arXiv:2209.02970, 2022. L. Zhang, W. Cai, Z. Liu, Z. Yang, W. Dai, Y. Liao, Q. Qin, Y. Li, X. Liu, Z. Liu, et al. Fineval: chinese financial domain knowledge evaluation benchmark for large language models. arXiv preprint arXiv:2308.09975, 2023. X. Zhang and Q. Yang. Xuanyuan 2.0: large chinese financial chat model with hundreds of billions parameters. In Proceedings of the 32nd ACM international conference on information and knowledge management, pages 44354439, 2023. Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/ 2403.13372. 20 Z. Zhou, L. Ma, and H. Liu. Trade the event: Corporate events detection for news-based eventdriven trading. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 21142124, 2021. D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. J. Zou, H. Cao, L. Liu, Y. Lin, E. Abbasnejad, and J. Q. Shi. Astock: new dataset and automated stock trading based on stock-specific news analyzing model. In Proceedings of the Fourth Workshop on Financial Technology and Natural Language Processing (FinNLP), pages 178 186, 2022. 21 A. Examples of instruction construction This Table.7 provides detailed overview of the instruction construction examples for various financial NLP tasks, categorized by task type and language. Each example includes the instruction, input, and output components, demonstrating the specific requirements and expected responses for each task. For more comprehensive understanding of the prompt settings and additional details, please refer to our datasets open-source repository on Hugging Face: Golden-Touchstone Dataset. Table 7 Examples of Instruction Construction for Various Financial Language Tasks, Categorized by Task Type and Language Task Type Language Instruction Input Sentiment Analysis English Chinese Classification English Chinese Entity Recognition English Chinese Relation Extraction English Chinese What is the sentiment of the following financial post: Positive, Negative, or Neutral?  , ...... Review the sentence from central banks communiqu......   ...... RT @tomhend777 $MU needs to hold here - Broken for now. Needs big flush. Still not technically oversold so now big bounce yet T In their discussion of prices, participants indicated that data over the intermeeting period......        Affirm Holdings(AFRM.O)175127  In the sentences extracted from financial agreements in U.S. SEC filings...... T, S,T S...... There is default in any agreement to which Borrower or any Guarantor is party with third party or parties...... :  (300193)900 2:  Output neutral 1 neutral  Borrower, PER  What is the relationship between Ivan Glasenberg and Glencore in the context of the input sentence...... ,             ...... The persistent oversupply is \"damaging the credibility of the industry,\" Glencore CEO Ivan Glasenberg said in May. owner_of : ISIS : : ISIS <N> unknown 22 Task Type Language Instruction Input Stock Movement Prediction Multiple Choice English Chinese English Chinese Summarization English Please predict the next rise or fall of DJIA Adj based on the next input of the days 25 most popular news items......             ,              ...... Given text T, and several options, according to the question posed in the text T......      T,    ABCD,T  ...... You are given text that consists of multiple sentences...... Top1:WikiLeaks demands answers after Google hands staff emails to US government......  0.5-1.0...... Output 0 The inventory/sales ratio is most likely to be rising......  ...... A,B,D PORTLAND, Ore., 2021 /PRNewswire/ Allied Market Research published report, titled,\"Matcha Tea Market By Product Type...... Feb. 17, Chinese  ,  Question Answering English Please answer the given financial question based on the context...... Chinese                          ...... APP 13Air France-KLM(BA.N)  ...... on november 18 , 2014 , the company entered into collateralized reinsurance agreement with kilimanjaro......  109 ...... Matcha Tea Market to Reach $4.48 Bn, Globally, by 2027 at 7.1%......                 The answer is:0.26685 873361 B. Inference Template of Large Language Models The Table.8 showcases how inference templates vary across different models. It is crucial to select the appropriate template for constructing correct inputs when inferring on the test sets of datasets. An incorrect template can significantly impair the performance of model. We have observed that the underperformance of some large financial language models in some benchmarks is precisely due to not selecting the appropriate templates for evaluation. For more details of training and inference template, please refer to our open-source code repository on Github: Golden-Touchstone Code. C. Typical Case Study Analysis of Typical Financial NLP Tasks This appendix provides detailed case study analysis for some typical financial NLP tasks: financial sentiment analysis, text classification, entity extraction, and stock movement prediction. Each analysis is presented in separate table, categorizing data sets, instructions, inputs, labels, and predictions from multiple models. Financial Sentiment Analysis As demonstrated in Table ??, financial sentiment classification is one of the simpler tasks for benchmarks in financial NLP, resulting in high performance across all models tested. General-purpose models (GPT-4o, Qwen-2, Llama-3) provide not only the answer but also detailed analysis, despite not being specifically fine-tuned on the FiQA-SA dataset. In contrast, specialized models (FinGPT, FinMA, Touchstone-GPT) that have undergone instruction tuning deliver straightforward, direct responses, illustrating their efficiency and focus in domain-specific applications. 23 Table 8 Comparison of Inference Templates Across Different Models for Dataset Evaluation"
        },
        {
            "title": "Model",
            "content": "GPT-4o Qwen-2 Llama-"
        },
        {
            "title": "Template",
            "content": "\"<im_start>system{{system_prompt}}<im_end>n\" \"<im_start>user{{instruction}}{{input}}<im_end>n\" \"<im_start>assistantn\" \"<im_start>system{{system_prompt}}<im_end>n\" \"<im_start>user{{instruction}}{{input}}<im_end>n\" \"<im_start>assistantn\" \"<start_header_id>system<end_header_id>\" \"{{system_prompt}}<eot_id>n\" \"<start_header_id>user<end_header_id>\" \"{{instruction}}{{input}}<eot_id>n\" \"<start_header_id>assistant<end_header_id>n\" \"Instruction:{{instruction}}\" \"Input{{input}}nAnswer:\" \"Human:{{instruction}}{{input}}n\" \"Assistant:n\" \"{{instruction}}{{input}}n\" DISC-FinLLM \"<reserved_102> {{instruction}}{{input}}<reserved_103>\""
        },
        {
            "title": "Touchstone",
            "content": "\"<im_start>system{{system_prompt}}<im_end>n\" \"<im_start>user{{instruction}}{{input}}<im_end>n\" \"<im_start>assistantn\" 24 Table 9 Detailed Case Study Analysis of Financial Sentiment Analysis on the FiQA-SA dataset, Financial Text Classification on the LendingClub dataset, Financial Entity Extraction on NER dataset, Stock Movement Prediction on DJIA dataset. Dataset Instruction Input FiQA-SA What is the sentiment of the following financial post: Positive, Negative, or Neutral? @Joerazorback This time think $AAPL will miss again. Generally speaking market is near top when everyone thinks that market goes only up. Model GPT-4o Label Negative Qwen-2 Llama-3 FinGPT FinMA Touchstone GPT GPT-4o Qwen-2 Llama-3 FinGPT FinMA Touchstone GPT GPT-4o Qwen-2 Llama-3 FinGPT FinMA Touchstone GPT GPT-4o Qwen2 Llama-3 FinGPT FinMA Touchstone GPT LendingClub Assess the clients loan status based on the following loan records from Lending Club. Respond with only good or bad. The client has attributes as follows: ... The state of Employment Length is 2 years. The state of Home Ownership is RENT. The state of Verification Status is Verified. The state of Annual Income is 83000.0. Good NER In the sentences extracted from financial agreements in U.S. SEC filings, identify the named entities. WHEREAS: (A) The Borrower intends to develop plant in France for the production of solar grade silicon. Borrower, PER; France, LOC DJIA Predict the next movement of DJIA Adj based on the days top 25 news items. Top1: bHaitians are so desperate for food...The Japanese comic version of Das Kapital... 0 Prediction The sentiment of this financial post is Negative. The user expresses doubt about Apples performance, suggesting that they think the stock will miss expectations. Additionally, the phrase about the market being \"near top\" implies bearish outlook, indicating that the market may decline after its current uptrend. The sentiment of the given financial post is Negative. The user is predicting that Apples stock price ($AAPL) will miss or fail to meet expectations, and they also suggest that the market might be nearing its peak when everyone is optimistic about continuous upward movement. The sentiment of this post is Negative. The author is predicting that Apples stock (AAPL) will miss expectations, which is negative view. Additionally, the author is suggesting that the market is near top, implying that downturn is likely, which is also negative sentiment. Negative Negative Negative Good Good Good Bad Bad Good France, LOC Borrower, PER; France, LOC; solar grade silicon, ORG Borrower, PER; France, LOC The Borrower intends to develop plant in France for the production of solar grade silicon Borrower, PER; France, LOC Borrower, PER; France, LOC [Decrease 0] [Decrease 0] [Decrease 0] \"\" Rise 1 1 25 Credit Rating Analysis In the classification task using the LendingClub dataset, which poses challenging credit rating task, the models face complex array of professional financial information evident in the input fields. Consequently, most models do not perform optimally. Among general models, GPT-4o exhibits the best performance, demonstrating the capabilities of large-scale models. In the realm of specialized financial language models, Touchstone-GPT, with its high-quality instruction tuning, significantly outperforms FinMA and FinGPT, which are only minimally tuned with Lora. Financial NER In this information extraction task, most models demonstrated an understanding of the task intent and adhered to the instructions, signifying that even in the era of large language models, models like Qwen-2 and Llama-3 actually outperformed GPT-4o. In particular, specialized models such as FinMA and Touchstone-GPT, with more comprehensive instruction tuning, responded accurately and succinctly, highlighting their enhanced capability and focus on domain-specific tasks. Stock Movement Prediction The Stock Movement Prediction task is one of the most challenging tasks, as it requires models to predict the daily fluctuations of the DJIA based solely on the top 25 news items from social media. From the results in Table.3, it is evident that Llama-3 performed the best, yet it still falls short of practical utility. Even Touchstone-GPT, despite specialized instruction tuning, performed poorly. Our analysis suggests that the sentiment of news items may not reliably predict stock movements and that incorporating quantitative data is essential for achieving practical model performance. Similar conclusions were drawn from experiments with traditional machine learning methods like XGBoost. Nevertheless, aside from simpler tasks like financial sentiment analysis, we also pose challenging tasks such as stock prediction, which are closer to real-world applications, leaving more room for benchmark challenges and exploration. Multimodal fusion of news and quantitative data represents promising future direction, and we look forward to seeing models excel in these tasks."
        },
        {
            "title": "Due to the similar performance of models across corresponding task types on the Chinese",
            "content": "benchmark, we will not reiterate these comparisons and analysis here."
        }
    ],
    "affiliations": [
        "IDEA Research",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "The Hong Kong University of Science and Technology",
        "Nanjing University",
        "South China Normal University"
    ]
}