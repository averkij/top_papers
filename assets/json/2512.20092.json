{
    "paper_title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
    "authors": [
        "Yiming Du",
        "Baojun Wang",
        "Yifan Xiang",
        "Zhaowei Wang",
        "Wenyu Huang",
        "Boyang Xue",
        "Bin Liang",
        "Xingshan Zeng",
        "Fei Mi",
        "Haoli Bai",
        "Lifeng Shang",
        "Jeff Z. Pan",
        "Yuxin Jiang",
        "Kam-Fai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 9 0 0 2 . 2 1 5 2 : r MEMORY-T1: REINFORCEMENT LEARNING FOR TEMPORAL REASONING IN MULTI-SESSION AGENTS Yiming Du1, Baojun Wang2, Yifan Xiang1, Zhaowei Wang3, Wenyu Huang4, Boyang Xue1, Bin Liang1, Xingshan Zeng2, Fei Mi2, Haoli Bai2, Lifeng Shang2, Jeff Z. Pan4, Yuxin Jiang2, Kam-Fai Wong1 1The Chinese University of Hong Kong 2Huawei Technologies Co.,Ltd 3HKUST 4The University of Edinburgh {ydu, kfwong}@se.cuhk.edu.hk, jiang.yuxin2@huawei.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Temporal reasoning over long, multi-session dialogues is critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current longcontext models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce MEMORY-T1, framework that learns time-aware memory selection policy using reinforcement learning (RL). It employs coarse-to-fine strategy, first pruning the dialogue history into candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterancelevel (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts 7B model to an overall score of 67.0%, establishing new state-of-the-art performance for open-source models and outperforming 14B baseline by 10.2%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to 15.0% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in memory architectures and large language models (LLMs) have substantially improved the capabilities of conversational agents (Yu et al., 2025; Zhong et al., 2024; Xu et al., 2025). Increasingly, these agents are expected to support long-term multi-session interactions (Du et al., 2025b; Ge et al., 2025), where central challenge is understanding and reasoning about temporal relationships across dialogue histories (Wu et al., 2025; Maharana et al., 2024). Without this capability, agents may incorrectly order past events, conflate information from different sessions, and ultimately generate inconsistent or inaccurate answers. For example, as shown in Figure 1, correctly resolving query such as What time did Emi mention that some Suits characters were together at the Golden Globes? requires the agent to locate the relevant mention in the dialogue history, understand the key relative temporal expression (last night), and grounding it to the correct session date (10.01.2024) to infer the accurate date January 9, 2024. Ultimately, temporal reasoning is essential for factual consistency in long, noisy conversations. However, existing approaches (Yu et al., 2025; Xu et al., 2025) remain inadequate for temporal reasoning in conversation. General-purpose long-context models (Team, 2024; Guo et al., 2025b; Corresponding authors. 1 Figure 1: Multi-session QA with time-event annotations. Time range marks when an event or query occurs, either duration or an instantaneous point (start and end coincide). Event span highlights key evidence in the utterance. Wang et al., 2025) treat dialogue history as flat text and fail to locate or resolve temporal expressions, leading to steep performance degradation on noisy, extensive conversations (Wu et al., 2025; Maharana et al., 2024). Time-aware frameworks such as TReMu (Ge et al., 2025) handle explicit expressions but struggle with ambiguous ones like the week before that, and error accumulation from inferred event summaries undermines robustness. Reinforcement learning (RL) approaches such as Time-R1 (Liu et al., 2025) rely heavily on structured metadata, making them ineffective for unstructured multi-session dialogues. Thus, robust, scalable solution for temporal reasoning in dialogue remains an open challenge. To bridge this gap, we introduce Memory-T1, RL-based memory retrieval framework designed for temporal reasoning that combines coarse-to-fine retrieval strategy with multi-level reward design. In the Candidate Generation phase, the query temporal scope is predicted using an LLM to prune the dialogue history search space, which acts as hard filter to prune irrelevant sessions. This is followed by relevance-based retriever to produce small, high-recall candidate set of sessions. This phase efficiently narrows the vast memory pool to manageable context, setting the stage for more precise analysis. In the fine-grained selection phase, an RL agent identifies the precise evidence sessions. Training such an agent is challenging because answer-only supervision provides very sparse signals. To overcome this, we design dense, multi-level reward function. Beyond answer accuracy (Ra) and evidence grounding (Rg), we introduce novel temporal consistency reward (Rt) that explicitly evaluates (1) session-level chronological proximity and (2) utterancelevel temporal density. By rewarding temporally coherent and contextually concentrated evidence, this structured signal provides richer supervision, enabling the agent to resolve ambiguous temporal expressions and to generalize more robustly to noisy, long-context dialogues. We validate Memory-T1 on the Time-Dialog (Wei et al., 2025) and LoCoMo (Maharana et al., 2024) benchmarks. Results show that Memory-T1 achieves state-of-the-art temporal reasoning performance, substantially improving robustness on contexts up to 128k tokens. Notably, Memory-T1 enables 7B model to outperform 14B baseline, highlighting the effectiveness of temporal-aware retrieval and dense reward optimization. The key contributions are: (1) coarse-to-fine memory retrieval framework that efficiently narrows dialogue histories into high-quality candidates before fine-grained evidence selection. (2) novel dense reward design for RL-based retrieval introducing temporal consistency signals at both session and utterance levels, providing insights into training robust temporal-aware retrieval models by overcoming sparse reward limitations. (3) State-of-theart performance, with Memory-T1 achieving top results and maintaining accuracy under extremely long and noisy conversational contexts. 2 Figure 2: An overview of Memory-T1. The framework employs coarse-to-fine cascade to select time-consistent memories for multi-session temporal reasoning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Temporal Reasoning in LLMs. Temporal reasoning has become an active area of research for LLMs (Song et al., 2025; Wei et al., 2025; Liu et al., 2025). Benchmarks such as TimeBench (Chu et al., 2024) and TIME (Wei et al., 2025) reveal that even strong models struggle with temporal relationships, event ordering, factual consistency, and long-range reasoning. To fill these gaps, prior work has aligned knowledge with temporal contexts (Zhao et al., 2024), introduced specialized training such as Timo (Su et al., 2024) and TG-LLM (Xiong et al., 2024), or applied RL, as in DeepSeek-R1 (Guo et al., 2025b) and Time-R1 (Liu et al., 2025). However, these methods often depend on explicit supervision or handcrafted structures, limiting their applicability to multi-session dialogue. Furthermore, temporal reasoning has recently become an important problem in the memory of LLMs. TReMu (Ge et al., 2025) leverages memory via timeline summaries but relies on timestamp accuracy for temporal reasoning. few memory-related works (Mai et al., 2025; Du et al., 2025a) also highlight the importance of temporal reasoning in long-term memory modeling. Building on this perspective, our Memory-T1 framework directly learns implicit memory selection and temporal alignment through multi-level time consistency reward, enabling robust reasoning without external tools. Reinforcement Learning in Agents: Reinforcement learning is core technology driving breakthroughs in LLM reasoning, from early outcome-based optimization algorithms, such as PPO (Schulman et al., 2017), to recent variants for agent scenarios, such as GRPO (Zheng et al., 2025), DPO (Rafailov et al., 2023), and GSPO (Zheng et al., 2025). RL not only improves training stability but also efficiency, enabling reasoning-centric models like DeepSeek-R1 (Guo et al., 2025a) and Search-R1 (Jin et al., 2025). Beyond isolated reasoning, RL has been applied to agent settings involving tool use (Qian et al., 2025), multi-step planning (Jin et al., 2025), and long-term interaction (Yu et al., 2025). Recent studies further extend RL to diverse scenarios, including optimized tool integration (Li et al., 2025), emergent code execution under large-scale training (Mai et al., 2025), and generalized frameworks for retrieval and collaboration (Luo et al., 2025). However, temporal reasoning over multi-session dialogues remains an underexplored area, necessitating robust memory retrieval, chronological alignment of events, and reasoning with ambiguous supervision."
        },
        {
            "title": "3 MEMORY-T1",
            "content": "Temporal reasoning over extended, multi-session dialogues presents significant challenge in conversational AI. The task requires agents to navigate vast and noisy memory banks to retrieve tem3 porally accurate and contextually relevant information, process where existing models often fail. To address this, we propose Memory-T1, novel framework for temporal-aware memory retrieval. We proceed as follows: Section 3.1 provides formal problem definition, Section 3.2 details the Memory-T1 framework, and Section 3.3 describes the reward design used to train the agent."
        },
        {
            "title": "3.1 PROBLEM FORMULATION",
            "content": "Temporal reasoning in multi-session scenarios is formulated as QA task (Figure 1): given user query q, the goal is to produce an answer grounded in the dialogue history. The dialogue history is represented as memory bank [(τ1, S1), (τ2, S2), . . . , (τN , SN )], where each session Si is associated with timestamp τi and consists of sequence of utterances paired with referenced events: Si = {(ui1, Ei1), (ui2, Ei2), . . . , (uiLi, EiLi)}, (1) where uij denotes the j-th utterance in session i, and Eij = {e1, e2, . . . , eK} is the set of events mentioned in that utterance. Each event ek can be optionally annotate with semantic descriptor κk and temporal span (tstart ) (see Figure 1). These annotations are introduced solely for trainingtime reward computation and are never accessible during inference. Details of the annotation process are provided in the Appendix A. , tend 3.2 MEMORY-T1: TEMPORAL-AWARE MEMORY RETRIEVAL MEMORY-T1 is temporal-aware memory retrieval framework designed for multi-session dialogue agents. Its architecture follows coarse-to-fine filtering principle to efficiently identify relevant and temporally consistent memories from vast and noisy dialogue history. The process is organized into two main phases: Candidate Generation and Fine-grained Selection. Phase 1: Candidate Generation: This initial phase aims to rapidly prune the large-scale memory repository down to manageable set of high-recall candidates. It consists of two sequential filtering stages: 1. Temporal Filtering: Given user query q, an LLM first predicts its target temporal window (tstart, tend). This predicted scope acts as hard filter to discard all sessions whose timestamps do not overlap with this range, drastically reducing the search space and getting temporally-filtered sessions set Mtemp, which is subset of the given memory bank (Mtemp ). 2. Relevance Filtering: From the temporally-filtered sessions, we then use retriever to rank the remaining sessions by textual relevance to the query. This step further narrows the pool to manageable size that fits within the agents context budget, while preserving sessions that are both temporally and textually pertinent. The top-ranked sessions form the candidate pool C, formally defined as: = arg top-k (τi,Si) s.t. tstartτitend (Retriever(q, Si)) (2) Phase 2: Fine-grained Selection via Reinforcement Learning. While the candidate set is highly relevant, it may still contain temporally imprecise or misleading information. Reinforcement learning enables the agent to refine its evidence selection policy under reward signals that directly penalize incorrect or temporally inconsistent citations. In this way, RL encourages the model to disambiguate noisy candidates and learn robust mappings between cited evidence and generated answers. Therefore, after identifying the candidate pool in Phase 1, we employ an RL-finetuned model to perform the final evidence selection and answer generation in an end-to-end manner. The agent policy πθ takes the query and candidate pool as input and generates single, composite output string. This output is structured to explicitly cite the session IDs used as evidence, followed by the natural language answer. For example, valid generation would be: {selected_memory : [session_3, session_16]. answer : 19 days.} From this generated string, we can parse both the selected evidence subset (e.g., [session_3, session_16]) and the final answer a. This integrated action space allows the model to learn the direct link between the evidence it cites and the answer it produces. This agent learns policy πθ(S q, C) to select subset of evidence sessions from the candidate pool given the query q. 4 To train this policy, we employ Group Relative Policy Optimization (GRPO) (Zheng et al., 2025), an effective RL algorithm for LLM fine-tuning that mitigates high reward variance by using batchaverage baseline. Our overall objective is to maximize the following function: max θ JGRPO(θ) = E(q,C)D,{(Sj ,aj )}πref"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 (cid:16) min rj(θ) ˆAj, clip(rj(θ), 1 ϵ, 1 + ϵ) ˆAj βE(q,C)D [DKL (πθ((q, C)) πref((q, C)))] . (cid:17) (3) Following structure similar to PPO (Schulman et al., 2017), we first define probability ratio rk(θ) = πθ((Sj ,aj ))(q,C)) πref((Sj ,aj ))(q,C)) , where Sj and aj represent the evident session id set and answer in jth generated output. Here, ϵ is clipping hyperparameter that restricts the size of policy updates. The advantage estimate ˆAj corresponding to sampled generation that yields the pair (Sj, aj) is calculated against the batch-average reward: ˆA((q, C), (Sj, aj)) = R((q, C), (Sj, aj))"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 R((q, C), (Sj, aj)). (4) The reward is given by multi-level function in Section 3.3. The second term in Eq. (3) is KL divergence penalty regularizing the current policy πθ against frozen reference πref to ensure training stability. Algorithmic details are in Appendix B. 3.3 REWARD DESIGN In this section, we describe the design of our verifiable rewards. The core motivation of the multilevel reward is to address the limitation of sparse supervision. As shown in Table 1, models such as MemAgent (Yu et al., 2025), which are trained solely on answer accuracy (Ra), fail to develop effective temporal reasoning abilities. Thus, it is necessary to jointly optimize evidence grounding (Rg, ensuring the correct sessions are used) and temporal consistency (Rt, ensuring temporal alignment with query) to form dense, structured reward signal. Since all rewards assume that the model output can be successfully parsed into the required format (e.g., {selected_memory : .... answer : ....}), we assign fixed penalty of 0.5 if parsing fails.The overall reward is defined as: = (cid:40)waRa + wgRg + wtRt, if parsing succeeds, 0.5, otherwise, [1, 1]. (5) where wa, wg, wt are tunable weights with wa + wg + wt = 1. Exact values are in Appendix C.1, and sensitivity to different settings is analyzed in Appendix C.2. Accuracy Reward (Ra) This reward ensures that the final predicted answer is correct, providing the most direct supervision signal for the agents output quality. As tasks require different answer formats, Ra is multifaceted metric tailored to four main types, each with specialized evaluation function. For Option Answers (e.g., \"A\", \"A C\"), we use strict Exact Match (EM). For numerical answers involving dates or durations, we employ more flexible metrics: Timestamp answers (e.g., \"2024-09-01\") are assessed with Unit-aware Accuracy, while Time Interval answers (e.g., \"13 days\") use ϵ-Exact Match (ϵ-EM). Finally, for sequential answers like Event Order, we use Hamming Accuracy to credit partial correctness. The final reward Ra is normalized to the range [1, 1], with detailed formulations in Appendix C.3. Evidence Grounding Reward (Rg) This reward encourages the model to retrieve and utilize information from the correct dialogue session(s). Specifically, this reward is calculated by comparing the set of session IDs cited by the agent against the gold-standard evidence set, , provided in the dataset. The degree of match is quantified using the Jaccard Index, which measures similarity by dividing the size of the intersection of the two sets by the size of their union. This score is then scaled to range [1, 1] where perfect match (Jaccard Index of 1) corresponds to reward of +1, and complete mismatch (Jaccard Index of 0) results in reward of -1. Temporal Consistency Reward (Rt): This reward component enforces fine-grained temporal alignment between the selected sessions and the query. It is composed of two sub-rewards: chronological proximity (Rs) and temporal coverage (Rf ). Rt = αRs + βRf , (α + β = 1) (6) 1.Chronological Proximity (Rs, session-level): This reward measures the temporal distance between the selected session timestamp and the gold temporal range IQ of user query. Recognizing that hard-cutoff penalty is too rigid for the temporal ambiguities in real-world dialogues (e.g., timezone shifts, extended topics), we employ logistic function to create soft, differentiable penalty that better handles this imprecision. The reward is formulated as: Rs = 1 + exp(x) d, Rs (d, d], where the normalized distance is defined as: := gap(U, IQ) . (7) (8) Here, gap(U, IQ) is the minimum temporal distance (in days) between spans and IQ (zero if they overlap). The hyperparameters offer fine-grained control: the tolerance margin sets penalty-free grace period (e.g., 7 days), the scale factor controls the penalty curve sharpness, and the parameters c, scale the final reward magnitude (to range (0.5, 1]). This logistic approach ensures that sessions close to are highly rewarded while distant sessions are penalized. For detailed settings, please refer to Appendix C.1. 2. Chronological Fidelity (Rf , utterance-level). While Rs handles session-level relevance, Rf evaluates the fine-grained quality of events within each utterance. It rewards sessions dense with evidence that is temporally aligned with the time range of the query, IQ. First, we assign discrete score re to each event based on its temporal overlap with IQ: re(e, IQ) = +1, +0.5, 1, if the time range of event is fully within IQ, if partially overlaps with IQ, if no overlap with IQ. (9) The final fidelity reward Rf is then calculated by first averaging these event scores within each relevant utterance (u Urel), and then averaging the resulting utterance scores across the session: Rf (U, IQ) = (cid:32) (cid:88) uUrel 1 Eu (cid:88) eEu 1 Urel 0, (cid:33) re(e, IQ) , if Urel > 0, otherwise. (10) This reward structure effectively penalizes common failure mode: selecting session from the correct time period but grounding the answer in textually similar but temporally incorrect utterance from within it. It incentivizes the agent to select sessions that are not just broadly relevant but also densely packed with chronologically precise evidence. By combining these three reward signals (Ra, Rg, Rt), our multi-level reward structure guides the agent to develop robust, generalizable temporal reasoning policy that does not overfit to superficial cues."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 DATASETS Time-Dialog We use Time-Dialog as the core benchmark, extended from the dialogue portion of the existing Time dataset (Wei et al., 2025), containing 4,716 QA examples corresponding with the multi-session dialogue history as shown in Figure 1.o train robustly time-aware agent, we augment the dataset with fine-grained annotations for supervision, specifically annotating the target time range for each query, utterance-level events with their time spans, and the ground-truth session 6 IDs for ideal evidence retrieval. Further details are shown in Appendix A. Crucially, these finegrained annotations serve exclusively as ground-truth signal for computing our multi-level rewards during training. To ensure fair and realistic evaluation, this enriched information is withheld from all models during inference. The final dataset of 4,716 examples is partitioned into training (4,065), validation (451), and held-out test (200) sets. LoCoMo To assess the out-of-domain (OOD) generalization of our trained policy, we employ the LoCoMo benchmark (Maharana et al., 2024), an established testbed for multi-session conversational memory. LoCoMo is composed of five distinct subtasks, one of which is specifically designed to evaluate temporal reasoning. This makes it an ideal held-out test set to validate whether our model has learned generalizable temporal reasoning skill, rather than overfitting to the patterns of the Time-Dialog dataset."
        },
        {
            "title": "4.2 EXPERIMENTS SETUP",
            "content": "Baselines Our proposed method, MEMORY-T1, is built upon Qwen2.5-3B and Qwen2.5-7BInstruct (Team, 2024). We compare it against comprehensive suite of baselines, including standard methods like Full Context, which is evaluated across wide range of open-source models (Qwen2.5-3B/7B/14B, Gemma-4B-it (Team, 2025), LLaMa-3.1-8B-Instruct (Dubey et al., 2024)) and the closed-source GPT-4 (Achiam et al., 2023); standard Retrieval-Augmented Generation (RAG) (Lewis et al., 2020); the agentic ReAct framework using GPT-4 as its backbone (Yao et al., 2023); and Supervised Fine-Tuning (SFT) model fine-tuned from Qwen2.5-3B (Ouyang et al., 2022). Furthermore, we benchmark against two state-of-the-art specialized agents, MemAgent (Yu et al., 2025) and Time-R1 (Liu et al., 2025), by evaluating their public checkpoints in zero-shot setting. Finally, to isolate the benefits of our contributions, we include an RL (Task Reward Only) ablation baseline, which uses the same architecture as MEMORY-T1 but is trained only with task accuracy reward (Ra), omitting our proposed temporal consistency (Rt) and evidence grounding (Rg) rewards. Implementation All our experiments build upon Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct as the main models. We adopt BM25 as retriever model due to the efficiency. We adopt the GRPO training strategy within the VERL framework (Sheng et al., 2024). We implement our RL training with batch size of 32, learning rate of 1106, K=8 rollout responses per prompt, KL coefficient = 0.1, and maximum sequence length of 16k tokens. 4.3 RESULTS As shown in Table 1, MEMORY-T1 establishes new state-of-the-art, with our 3B and 7B models achieving top overall scores of 66.9% and 67.0%. This performance represents significant leap over diverse set of baselines. Compared to specialized SOTA models, our trained agent surpasses the zero-shot performance of both the temporal reasoning model Time-R1 (49.4%) and the memorybased framework MemAgent (49.9%) by over 17 absolute points, highlighting the necessity of targeted training for this complex task. Crucially, our approach proves superior to simply increasing model scale. Our 3B model not only consistently outperforms larger models from different families, including Gemma-4B (45.0%), Llama-3-8B (48.4%), and even the much larger Qwen2.5-14B (60.7%), but also performs nearly identically to our 7B variant. This strongly suggests that the performance gains stem primarily from our learned policy rather than the scale of the base model. Notably, MEMORY-T1 also outperforms standard GPT-4 configurations, surpassing both Full Prompt (64.8%) and ReAct (62.8%). While gap remains to the ideal GPT-4 (Oracle) score of 86.2%, this overall dominance confirms that our learned memory policy is both necessary and effective. This advantage is driven by our models particularly strong performance on complex reasoning tasks, such as order reasoning (OR) and range reasoning (RR), directly validating the effectiveness of its temporally grounded memory selection policy. 4.4 ABLATION STUDY Ablation Study on reward components. Our multi-component reward function is crucial for robust performance, as shown in Table 2. Training with only task accuracy (Ra) leads to catas7 Table 1: Performance comparison across different models and training strategies on temporal reasoning subtasks. Category As metrics include Location (Loc.), Duration Comparison (DC.), Comparison (Comp.), Order Comparison (OC.), and Extraction (Ext.). Category Bs metrics covers ER.=Event Reasoning, OR.=Order Reasoning, RR.=Range Reasoning. Category Cs metrics comprises CTF.=Contextual Temporal Filtering, Co-tmp.=Co-temporality, TL.=Timeline. Bold and underline denote column-wise best and second-best among non-GPT rows. Oracle setting using gold test evidence. Experiments Category Category Category Overall Loc. DC. Comp. OC. Ext. ER. OR. RR. CTF. Co-tmp. TL. GPT-4 (Oracle Evidence) GPT-4 (Full Prompt) GPT-4 (ReAct) Gemma-4B-it Time-R1 Qwen2.5-3B (Instruct) Qwen2.5-3B+SFT Memory-T1 (3B) Llama-3-8B (Instruct) MemAgent-7B Qwen2.5-7B (Instruct) Qwen2.5-14B (Instruct) MemoryT1 (7B) 88.9 78.3 61.1 87.0 66.7 43. 60.9 5.6 11.1 47.8 5.6 56.5 22.2 56.5 50.0 52.2 22.2 43.5 55.6 47.8 61.1 52.2 16.7 47.8 61.1 52.2 54.9 46.7 39.2 12.7 13.9 7.2 7.9 7.1 5.6 10.2 0.0 4.4 8.6 95.0 66.7 100.0 88.9 93.8 55.6 81.3 64.1 85.0 22.2 72.2 70.8 67.1 75.0 32. 55.0 33.3 65.0 55.6 70.0 44.4 65.0 44.4 75.0 55.6 75.0 33.3 55.0 40.7 75.0 55.6 70.0 55.6 65.0 56.7 62.2 76.9 66.7 66.7 82.1 79.5 61.5 63.3 84.6 71.8 38.9 50.0 27.8 44.4 33.3 56.3 33.3 56.3 66.7 87.5 27.8 56.3 38.9 62.5 38.9 54.2 66.7 75.0 83.3 87. 83.3 50.0 68.5 44.4 55.6 55.6 55.6 88.9 72.2 38.9 50.0 69.7 88.9 100.0 77.8 84.3 61.1 66.7 77.8 77.8 94.4 27.8 72.2 72.2 94.4 94. 35.4 27.1 29.2 15.3 25.0 12.5 16.7 12.4 14.6 27.1 16.7 20.8 27.1 86.2 64.8 62.8 45.0 49.4 49.4 50.6 66.9 48.4 49.9 53.2 60.7 67. Figure 3: Performance comparison between Memory-T1 (3B) and Qwen2.5-3B (Instruct) under different top-k values (bar charts represent overall F1 scores; line charts represent evidence session recall rate. Comparison conditions: With/without temporal filtering; Top-k refers to the number of sessions retrieved in the candidate generation phase.) trophic 22.4% drop in the overall score, with performance on complex reasoning (Category & C) collapsing. Removing the evidence grounding reward (w/oRg) significantly harms localization and extraction-based tasks (Category A, -17.4%), causing 9.1% overall performance drop and confirming its role in preventing distraction. The temporal consistency reward (Rt), composed of sequence (Rs) and fine-grained (Rf ) components, is vital for structured reasoning. Most revealingly, ablating only the sequence component (Rs) creates sharp trade-off: simpler tasks (Category A) unexpectedly improve by 23.4%, while complex reasoning (Category B) collapses by 56.2%. This highlights crucial synergy: Rg grounds the model in what evidence to use, while Rt teaches it how to reason with that evidence temporally. To clarify the non-monotonic effects of Rt components, CategoryA duration tasks rely on two complementary mechanisms: global timeline consistency (Rs) and content-level temporal relevance (Rf ). Removing only one leaves the other as compensatory constraint, improving simpler timestampor gap-based reasoning. Full removal of Rt eliminates both regulating factors, preventing correct event selection and temporal alignment, which explains the sharp performance drop. 4.5 MODEL ANALYSIS Out-of-Domain Generalization. Our model demonstrates strong out-of-domain (OOD) generalization on the LoCoMo benchmark (Table 3, Table 9 in Appendix C)). MEMORY-T1 achieves top score of 37.7%, significant improvement over the 33.5% from the base Qwen-2.5-3B model. This 8 Table 2: Ablation study on the reward function of Memory-T1 (3B). Relative changes compared to the full model are shown in parentheses. Model Category Category Category Overall Memory-T1 (3B) 49.5 79.5 80. w/o Rt remove Rs only remove Rf only w/o Rg Ra only 45.6 (-7.9%) 61.1 (+23.4%) 50.0 (+1.0%) 40.9 (-17.4%) 43.6 (-11.9%) 75.1 (-5.5%) 34.8 (-56.2%) 56.5 (-28.9%) 75.3 (-4.2%) 57.5 (-27.7%) 64.3 (-19.9%) 66.3 (-17.4%) 63.0 (-21.6%) 75.9 (-5.5%) 59.0 (-26.6%) 66.9 63.5 (-5.1%) 66.3 (-0.9%) 64.8 (-3.1%) 60.8 (-9.1%) 51.9 (-22.4%) Ablation Study on candidate generation phase. Figure 3 validates our coarse-to-fine candidate generation strategy. First, increasing retrieval depth top-k to 10 is essential to achieve high evidence recall ( 90%). Our temporal filter proves highly precise, as the overlapping recall lines show it removes distracting context without sacrificing this crucial evidence. Second, even with the same unfiltered context, our RLtuned MEMORY-T1 agent outperforms the base model (54.0% vs. 44.2%). The synergy of combining broad retrieval for high recall with sharp, evidence-preserving filtering creates an optimal candidate pool that enables the agent to achieve its final 66.9% score. Table 3: LoCoMo benchmark: Out-of-Domain evaluation of Qwen-2.5-3B-Instruct and MemoryT1 (3B) under RAG and Non-RAG settings. Values are shown as percentages; best results in each column are bolded. Overall shows improvement relative to Qwen-2.5-3B-Instruct (Non-RAG). Model Family Setting Single-Hop Multi-Hop Temporal Open-Domain Adversarial Overall Overall (%) Qwen-2.5-3B (Instruct) Memory-T1 (3B) Non-RAG RAG Non-RAG RAG 49.8 46.0 51.2 48.9 28.7 22. 30.2 25.8 24.5 27.3 31.5 30.7 13.5 11.4 15.8 14.6 16.6 19. 26.0 29.8 33.5 31.9 37.7 36.7 -1.6% +4.2% +3.2% Table 4: Robustness of Memory-T1 under increasing time label noise. Noise Level Category Category Category Overall Loc. DC. Comp. OC. Ext. ER. OR. RR. CTF. Co-tmp. TL. 20% 10% 5% 27.8 50.0 50.0 43.5 43.5 60.9 5.0 10.6 5.0 55.0 65.0 60.0 25.9 55.6 55. 76.9 74.4 82.0 72.2 67.4 77.8 81.2 81.2 87.5 94.4 88.9 94.4 94.4 94.4 88.9 16.7 18.8 16. 60.0 63.4 67.0 advantage is particularly consistent in the Non-RAG setting (31.9% 36.7%), driven by substantial gains in the Temporal and Adversarial subtasks. Intriguingly, MEMORY-T1 yields better performance in the Non-RAG setting compared to the RAG setting, suggesting it has learned superior internal memory management skill. The Adversarial subset is notable exception, which focuses on answerability detection (saying dont know when the information is missing). Without the RAG setting, the Post-filter candidate pool remains lengthy and is prone to lost in the middle effects and spurious snippets that encourage hallucination. With RAG, the condensed candidate pool prunes spurious in-dialog segments, preserving compact set lacking supporting evidence. This makes it easier for the RL policy to learn unanswerable behavior more effectively (26.0 29.8). It introduces mild distribution shift and loss of temporally key candidates on standard tasks but benefits adversarial detection by simplifying evidence incompleteness detection. Robustness in Long-Context Scenarios. To assess how models handle increasingly complex dialogues, we partition the test set by context length and evaluate performance on each bracket (Figure 4). As context length increases, the performance of baseline models collapses due to attentional dilution; the Qwen2.5-7B baseline, for instance, drops by over 30 F1 points. In contrast, MEMORYT1 maintains high and stable F1 score across all lengths. This creates performance gap that widens dramatically with context, growing from +9.8 point advantage to massive +25.0 point lead for MEMORY-T1 (7B) in the 64k-128k bracket. This resilience stems directly from our learned policy, which effectively filters context and shields the model from distraction, confirming its superiority for long-range reasoning. Further controlled experiments on lost-in-the-middle effects are provided in Appendix C.5. 9 Figure 4: Comparison of Qwen2.5 and Memory-T1 models on the test set, where examples are grouped by the length of each test example (tokens) (0k8k, 8k16k, 16k32k, 32k64k, 64k128k) to assess performance variation across lengths, along with overall evaluation. Table 5: Analysis of model performance and Retrieval-Augmented Generation (RAG) Latency (time in seconds) Model Num of Total Inf. Time Avg Latency Total Inf. Retrieval Time Time-R1 MemAgent Qwen2.5-3B (Instruct) Memory-T1 200 200 200 200 248.62 312.72 271.83 252.08 1.24 1.56 1.36 1. 256.35 320.47 279.74 259.81 0.01 0.01 0.01 0.01 Robustness under increasing time label noise. As shown in Table 4, with 5% noise (realistic error rate), overall F1 remains 67.0, and key temporal reasoning tasks such as Counterfactual (CTF.), Co-temporality (Co-tmp.), and Relative Reasoning (RR.) stay high at 94.4, 88.9, and 87.5, respectively. Increasing the noise to 10% and 20% leads to gradual but moderate degradation of the overall score to 63.4 and 60.0. Notably, the most temporally demanding tasks remain robust: CTF. and Co-tmp. stay above 88.9 F1 even at 20% noise. The main decline is concentrated in timespanrelated subtasks (such as Localization and Extract). This confirms the Memory-T1 is resilient to realistic label noise, supporting its practical applicability in real-world settings where time labels are imperfect. Efficiency Analysis. Memory-T1 incurs negligible additional inference latency  (Table 5)  . The average latency (1.26 seconds per query) is highly comparable to baselines such as Time-R1 (1.24 seconds) and Qwen2.5-3B (1.36 seconds). Crucially, the retrieval overhead (0.01 seconds) is insignificant relative to the total LLM generation latency, confirming that the framework achieves its improved performance with minimal computational cost. Qualitative Analysis. We focused our qualitative analysis on the six subtasks (ER., OR., RR., CTF., Co-tmp., and Loc.) where Memory-T1 exhibits the largest performance gains (Table 10 in Appendix). consistent pattern emerges across these subtasks: the base model often relies on semantic similarity rather than temporal correctness, which leads to systematic errors such as neglecting time constraints, confusing event order, overlooking co-temporal relations, and failing to incorporate counterfactual adjustments. Memory-T1 mitigates these issues through explicit timerange filtering and RL-based selection that enforces temporal consistency, yielding more accurate localization, ordering, and co-temporality. These qualitative observations align with and explain the performance improvements observed on the six subtasks."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce MEMORY-T1, novel reinforcement learning framework addressing the critical challenge of temporal reasoning over long, multi-session dialogues. The framework employs coarse-to-fine strategy, guided by multi-level reward function that incorporates answer accuracy, evidence grounding, and temporal consistency signal. This design provides the agent with dense supervision to effectively handle temporal ambiguities and noise. Experiments show that MEMORY-T1 achieves state-of-the-art performance on the Time-Dialog benchmark, enabling 3B 10 model to outperform 14B baseline and maintaining strong robustness in dialogue histories up to 128k tokens. This work demonstrates that selecting temporally consistent memory evidence is critical step toward building more reliable and factually consistent long-term conversational agents."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We are committed to ensuring the transparency and reproducibility of our research. To support this commitment, we will publicly release our annotated dataset and all source code, facilitating future extensions and community research. Comprehensive details of our methodology are provided throughout this paper: the annotation process and prompts are illustrated in Appendix A, Figures 21, 20, and 22; training and evaluation prompts are shown in Figure 23 and Figure 24, respectively. Furthermore, detailed algorithmic procedures can be found in Appendix B. We believe that releasing these assets will lower the barrier for replication, enable fair comparisons, and foster further exploration in this line of research."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The main artifact of this work is the annotated Time-Dialog dataset. To facilitate the process, we develop dedicated evidence-annotation website (Figure 5) and engage three experienced NLP researchers as annotators. Approximately 200 human hours are devoted to verifying GPT-4assisted annotations, categorizing error types, and refining the protocol through several iterations. All annotators are properly briefed and held regular discussions to resolve ambiguous cases. Model evaluations are conducted by three trained research assistants, each compensated at $20/hour, which is above the local average. Prior to release, all data underwent rigorous screening to ensure the exclusion of personally identifiable information and offensive content. Both the dataset and code will be publicly released under an MIT license to encourage transparency and community use."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, and Bing Qin. TimeBench: comprehensive evaluation of temporal reasoning abilities in large language In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd models. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12041228, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.66. URL https://aclanthology.org/2024.acl-long. 66/. Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, and Jeff Pan. Rethinking memory in ai: Taxonomy, operations, topics, and future directions. arXiv preprint arXiv:2505.00675, 2025a. Yiming Du, Bingbing Wang, Yang He, Bin Liang, Baojun Wang, Zhongyang Li, Lin Gui, Jeff Pan, Ruifeng Xu, and Kam-Fai Wong. Bridging the long-term gap: memory-active policy for multi-session task-oriented dialogue. arXiv preprint arXiv:2505.20231, 2025b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yubin Ge, Salvatore Romeo, Jason Cai, Raphael Shu, Monica Sunkara, Yassine Benajiba, and Yi Zhang. Tremu: Towards neuro-symbolic temporal reasoning for llm-agents with memory in multi-session dialogues. arXiv preprint arXiv:2502.01630, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, 11 Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 645(8081):633638, September 2025a. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL https://doi.org/10.1038/s41586-025-09422-z. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025b. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33: 94599474, 2020. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. doi: 10.1162/tacl_a_00638. URL https://aclanthology.org/2024.tacl-1.9/. Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, and Jiaxuan You. Time-r1: Towards comprehensive temporal reasoning in llms. arXiv preprint arXiv:2505.13508, 2025. Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning. arXiv preprint arXiv:2508.03680, 2025. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1385113870, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.747. URL https://aclanthology.org/2024.acl-long.747/. 12 Xinji Mai, Haotian Xu, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang, et al. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving. arXiv preprint arXiv:2505.07773, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Jiayu Song, Mahmud Elahi Akhter, Dana Atzil-Slonim, and Maria Liakata. Temporal reasoning for timeline summarisation in social media. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2808528101, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/ 2025.acl-long.1362. URL https://aclanthology.org/2025.acl-long.1362/. Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192, 2024. Gemma Team. Gemma 3. 2025. URL https://goo.gle/Gemma3Report. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Zhaowei Wang, Wenhao Yu, Xiyu Ren, Jipeng Zhang, Yu Zhao, Rohit Saxena, Liang Cheng, Ginny Wong, Simon See, Pasquale Minervini, et al. Mmlongbench: Benchmarking long-context visionlanguage models effectively and thoroughly. arXiv preprint arXiv:2505.10610, 2025. Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, and Houfeng Wang. Time: multi-level benchmark for temporal reasoning of llms in real-world scenarios. arXiv preprint arXiv:2505.12891, 2025. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. LongIn Proceedings of memeval: Benchmarking chat assistants on long-term interactive memory. the 2025 International Conference on Learning Representations (ICLR), 2025. URL https: //openreview.net/forum?id=pZiyCaVuti. Accepted at ICLR 2025. Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. Large language models can learn temporal reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1045210470, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.563. URL https://aclanthology. org/2024.acl-long.563/. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. 13 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and Noah Smith. Set the clock: Temporal alignment of pretrained language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1501515040, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.892. URL https://aclanthology.org/2024. findings-acl.892/. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1972419731, 2024."
        },
        {
            "title": "A DATASET AND ANNOTATIONS",
            "content": "Our experiments are conducted on the Time dataset, comprehensive benchmark for temporal reasoning over long-form dialogues. The dataset features complex dialogue histories and is structured into 3 levels of reasoning difficulty and 11 distinct QA subtasks. The distribution of these subtasks in the dataset is detailed in Table 6. Table 6: Distribution and characteristics of QA subtasks in the Time dataset, grouped by reasoning level. QA Subtask Format category # Samples Localization Duration_Compare Computation Order_Compare Extract Time Span Single Choice Time Span Single/Multi Choices Single/Multi Choices Explicit_Reasoning Order_Reasoning Relative_Reasoning"
        },
        {
            "title": "Single Choice\nSingle Choice\nSingle Choice",
            "content": "Counterfactual Co_temporality Timeline Single/Multi Choices Single/Multi Choices Event Order"
        },
        {
            "title": "B\nB\nB",
            "content": "C 381 385 390 380 197 363 381 393 398 397 390 While the Time dataset provides strong foundation, it lacks the fine-grained annotations necessary for our reward mechanisms and detailed analysis. To address this, we augmented the dataset with three additional layers of annotations. Our annotation process employed an iterative framework where GPT-4 performed an initial annotation pass, followed by human verification to identify systematic error patterns. These insights were then used to refine the prompts for final, improved annotation pass, achieving an overall accuracy of over 95%. 1. Question Temporal Range (IQ). First, for each question, we annotate its target temporal range (IQ). Many questions implicitly focus on specific period within the long dialogue history. We prompted GPT-4 to infer and extract this time range. For questions with no discernible temporal focus, we assigned default range starting from unknown to our annotation timestamp (e.g., 2025-07-17T11:46:32). As this timestamp is later than any event in the dataset, this default range effectively covers the entire dialogue history. The prompt can be found in Figure 2. Evidence Grounding (M). Second, we annotate the ground-truth evidence sessions (M) and utterances required to answer each question. The original datasets fact bank could not be reliably mapped to the dialogue text. We therefore used our iterative GPT-4 (Figure 22) and humanin-the-loop process (Figure 5) to perform this grounding. This resulted in session-level annotation accuracy of over 95% and an utterance-level accuracy of over 85%. To avoid introducing potential noise from less accurate annotations into our reinforcement learning process, we use the more reliable session-level annotations for calculating the Evidence Grounding Reward (Rg). 3. Utterance-level Event Times. Finally, to enable deeper temporal analysis, we performed utterance-level event extraction and temporal grounding for the entire dialogue history (Figure 20). This annotation is crucial because the timestamp of dialogue turn (when something was said) often differs from the timestamp of the event being discussed (when something happened). This distinction is the primary motivation for our chronological proximity (Rf ) reward. For each utterance, we prompted GPT-4 to extract key events and resolve their temporal scope based on the dialogue context. For instance, given dialogue turn on 2025-06-20, an utterance mentioning the meeting last week would be grounded to specific range like [2025-06-09, 2025-06-13]. For utterances without explicit temporal markers, we used grammatical tense to infer broad range (e.g., past tense implies range from the distant past up to the dialogue time, while future tense implies range from the dialogue time to the distant future). 15 Figure 5: An overview of website for human annotation."
        },
        {
            "title": "B ALGORITHM",
            "content": "The core of the framework is reinforcement learning agent trained with Group Relative Policy Optimization (GRPO) as shown in Algorithm 1. Our overall approach involves two-phase process: first, an efficient candidate generation stage to prune the search space, followed by reinforcement learning (RL) fine-tuning stage to train the policy model. Phase 1: Candidate Generation. Given query and the full dialogue memory M, we first generate small, highly relevant pool of candidate sessions C. This step, detailed in Algorithm 2, is crucial for making the subsequent selection process tractable and efficient. Phase 2: RL Fine-tuning. With the candidate set C, we perform an RL update. For each instance in the batch, we sample distinct outputs from the current policy πθ. Each output contains selected evidence set Sj and generated answer aj. multi-level reward Rj is then calculated for each of the samples by comparing it against the ground-truth labels (M, a, IQ). This reward, detailed in Algorithm 3, provides comprehensive signal reflecting accuracy, evidence grounding, and temporal consistency. To reduce the variance of the policy gradient estimate, we compute an advantage ˆAj for each sample. Following GRPO, we use simple yet effective batch-average baseline, where the advantage is the samples reward minus the average reward across all samples in the batch ( ˆAj = Rj R). Finally, the policy models parameters θ are updated using the GRPO objective function. This objective maximizes the advantage-weighted log-probability of the sampled outputs. Crucially, it also includes Kullback-Leibler (KL) divergence term, DKL(πθ πref), weighted by λ. This term regularizes the policy update, preventing the trained policy πθ from deviating too drastically from frozen reference policy πref, which is essential for maintaining training stability. 16 B.1 CANDIDATE GENERATION (ALGORITHM 2) The candidate generation process is critical filtering cascade designed to efficiently narrow down the vast memory repository to small set of promising candidates C. This is achieved through two-stage process: 1. Temporal Filtering. First, we leverage powerful LLM to perform zero-shot prediction of the likely temporal window (tstart, tend) relevant to the user query q. We then perform an initial broad-phase filtering by retaining only those sessions (τi, Si) from whose timestamps τi overlap with this predicted window. This step effectively prunes the majority of irrelevant sessions based on strong temporal heuristic. 2. Relevance Filtering. The temporally-filtered subset Mtemp is then passed to second filtering stage. Here, we use fast and effective lexical retrieval method, BM25, to rank all sessions in Mtemp based on their textual relevance to the query q. The final candidate pool is formed by selecting the top-ranked sessions from this list. This cascade approachusing temporal heuristic followed by lexical matchingallows for an efficient and effective reduction of the search space without relying on expensive semantic models at large scale. B.2 MULTI-LEVEL REWARD CALCULATION (ALGORITHM 3) To provide rich and informative learning signal for our policy, we designed multi-level reward function that captures three critical aspects of the task. The final reward is weighted sum of these components. 1. Task-level Accuracy Reward (Ra). This is sparse, binary reward that directly measures task success. It yields reward of 1 if the generated answer is correct with respect to the ground-truth answer a, and 0 otherwise. This component ensures the model is strongly incentivized to produce factually correct final answers. 2. Evidence Grounding Reward (Rg). This component evaluates the quality of the retrieved evidence. We calculate the F1-score between the set of session IDs in the predicted evidence set and the ground-truth evidence set M. This dense reward encourages the model to select the precise set of sessions required to formulate the answer, promoting better interpretability and faithfulness. 3. Temporal Consistency Reward (Rt). This novel reward component assesses the temporal quality of the selected evidence with respect to the ground-truth temporal range IQ. It is computed as the average of individual rewards over all selected sessions. For each session S, the reward is weighted sum of two sub-components: Chronological Proximity (Rs): This measures the temporal distance between the sessions timestamp and the gold range IQ. It uses logistic function to provide soft, differentiable penalty, rewarding close proximity and penalizing distant sessions. Chronological Fidelity (Rf ): This provides more fine-grained signal. Within given session , it assesses whether the events in the utterances are relevant to the query are themselves temporally aligned with the gold range IQ. It returns positive reward for relevant utterances inside IQ, smaller positive reward for those on the boundary, and negative penalty for those outside. The final reward is the weighted sum waRa + wgRg + wtRt, where the weights allow us to balance the relative importance of task accuracy, evidence quality, and temporal alignment. 17 Algorithm 1 MEMORY-T1 Training Procedure Require: Full dialogue memory repository , Training dataset = {(qi, the ground-truth answer, and IQ is the ground-truth temporal range Policy model to be trained πθ and frozen reference policy πref Hyperparameters: KL divergence weight λ, reward weights wa, wg, wt, group size i=1, where is the ground-truth evidence set, is , IQi)}N Ensure: Optimized policy model πθ 1: function TRAINMEMORY-T1(M, D, πθ, πref) 2: 3: 4: Initialize policy parameters θ for each training iteration do Sample batch (q, M, a, IQ) from Phase 1: Candidate Generation GenerateCandidates(q, M) Phase 2: RL Fine-tuning SampledOutputs [] for = 1 to do Call Algorithm 2 Sample outputs from the policy Generate an output string from πθ conditioned on (q, C) Parse the selected evidence set Sj and answer aj from the string Add (Sj, aj) to SampledOutputs end for Rewards [] for = 1 to do Rk CalculateReward((Sj, aj), (M, a, IQ)) Add Rj to Rewards Calculate reward for each sample Call Algorithm end for for = 1 to do ˆAj Rj 1 end for Policy Update (cid:80)g j=1 Rj Calculate Advantage GRPOs batch-average baseline Update model parameters θ using the GRPO objective: θJ(θ) (cid:80)G j=1 θ log πθ((Sj, aj) (q, C) ˆAj λθDKL(πθ πref) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end function end for return the trained policy model πθ"
        },
        {
            "title": "C REWARD SUPPLEMENTARY",
            "content": "C.1 HYBERPARAMETER DESIGN The Temporal Consistency Reward Rt = αRs + βRf and its component Rs are governed by set of hyperparameters (c, d, m, s, α, β). The function of each parameter group remains the same: Tolerance and Leniency (m, s) defines the \"softness\" of the temporal alignment. The tolerance margin (m) sets grace period, while the scale factor (s) control the sharpness of the penalty curve outside this margin. Incentive Scaling (c, d): controls the magnitude of the reward and penalty, allowing us to calibrate the strength of the positive and negative incentives. Component Weighting (α, β): balance the importance of chronological proximity (Rs) versus chronological fidelity (Rf ). C.2 SENSITIVE ANALYSIS To assess the sensitivity of the model to the composition of the reward function, we evaluate its performance under four different weight configurations for accuracy (wa), evidence grounding (wg), and temporal consistency (wt), with results in Figure 6. We find that optimal performance (67.0%) is achieved with the configuration (0.6, 0.2, 0.2). This result suggests that while task accuracy is the primary objective, substantial weights for both evidence grounding and temporal consistency are essential to guide the reasoning process of the agent effectively. Deviating from this balance leads 18 Algorithm 2 Candidate Generation Require: User query Full dialogue memory repository = [(τ1, S1), . . . , (τN , SN )] Ensure:"
        },
        {
            "title": "Candidate session pool C",
            "content": "1: function GENERATECANDIDATES(q, M) 1. Temporal Filtering Predict target temporal window (tstart, tend) for query using an LLM 2: 3: Mtemp 4: 5: 6: 7: 8: end if for each session (τi, Si) in do if timestamp τi overlaps with (tstart, tend) then Mtemp Mtemp {(τi, Si)} end for 2. Relevance Filtering Rank all sessions in Mtemp by textual relevance to query using BM25 9: Select top-ranked sessions from the sorted list 10: return 11: 12: end function Table 7: Heuristic configuration of hyperparameters for the temporal reward function. Parameter(s) Value Rationale c, 1.5, 0.5 7 (days) Normalizes the maximum reward (c d) to 1, bounding the reward Rs to the range (0.5, 1]. This provides strong positive signal for perfect match and moderate penalty for distant ones. Based on the domain knowledge that one-week window is reasonable span for contextual relevance in conversational data. 1 Set to default value to create standard and predictable logistic decay curve without excessive sharpness or leniency. α, β 0.5, 0.5 wa, wg, wt 0.6, 0.2, 0.2 Establishes robust baseline by giving equal importance to the two sub-rewards: chronological proximity (Rs) and chronological fidelity (Rf ). Selected based on extensive experiments to balance accuracy, evidence grounding, and temporal consistency. to clear degradation in performance. An accuracy-skewed weighting of (0.8, 0.1, 0.1) diminishes the influence of our guiding rewards, causing the score to drop to 64.0%. Similarly, uniform distribution ( 1 3 ) proves suboptimal (62.2%), likely because it fails to sufficiently prioritize the main task goal. These findings underscore that the reward components are synergistic; peak performance hinges on careful balance rather than maximizing any single objective in isolation. 3 , 1 3 , 1 C.3 ACCURACY REWARD (Ra) METRICS The Accuracy Reward (Ra) evaluates the correctness of the final answer, tailored for four main types. Each metric yields Score in the range [0, 1], which is then normalized to the final reward Ra [1, 1]. Option Answers (Exact Match, EM) For categorical answers (e.g., \"A\", \"A C\"), we use strict Exact Match. The score is defined as ScoreEM = I(Apred = Agold), where I() is the indicator 19 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: Algorithm 3 Multi-Level Reward Calculation Require: Generated evidence and answer (S, a) Ground-truth evidence, answer, and temporal range (M, a, IQ) Reward weights wa, wg, wt, α, β Ensure:"
        },
        {
            "title": "Total scalar reward R",
            "content": "1: function CALCULATEREWARD((S, a), (M, a, IQ)) 1. Task-level Accuracy Reward (Ra) Ra (1 if is correct w.r.t. else 0) 2. Evidence Grounding Reward (Rg) Rg F1_score(session_ids(S), session_ids(M)) 3. Temporal Consistency Reward (Rt) Rt,total 0 if is not empty then for each selected session do a. Chronological Proximity (Rs) (gap(U, IQ) m)/s Rs c/(1 + exp(κx)) b. Chronological Fidelity (Rf ) Urel {u text_similarity(u, q) > threshold} if Urel > 0 then (cid:80) Rf 1 Urel (cid:16) 1 Eu re(e, IQ) uUrel eEu (cid:80) (cid:17) re scores events (+1, +0.5, -1) based on overlap else Rf 0 end if Rt(U, IQ) αRs + βRf Rt,total Rt,total + Rt(U, IQ) end for Rt Rt,total/S else Rt 0 Average over all selected sessions end if Combine for final reward waRa + wgRg + wtRt 22: return 23: 24: end function function. For instance, if the gold answer Agold is \"B\", prediction of \"B\" scores 1, while \"C\" scores 0. Timestamp Answers (Unit-aware Accuracy) To handle various date/time formats, this metric compares canonical representations. Both prediction and ground truth are normalized via function () before comparison, making the score robust to format differences: ScoreUnitAware(Apred, Agold) = I(N (Apred) = (Agold)) For example, prediction of \"2025-09-24\" correctly matches the gold answer \"September 24, 2025,\" as both normalize to the same value, yielding score of 1. Time Interval Answers (ϵ-Exact Match, ϵ-EM) For numerical durations, this metric allows tolerance ϵ for minor calculation differences. The score is 1 if the absolute difference between the predicted value (Apred) and the gold value (Agold) is within this tolerance: Scoreϵ-EM(Apred, Agold) = I(V (Apred) (Agold) ϵ) For gold answer of \"13 days\" and with ϵ = 1, predictions from \"12\" to \"14 days\" are considered correct. 20 Table 8: PPO vs. GRPO: F1 performance on Memory-T1 models of different sizes. Model Category Category Category Overall 3B, GRPO 3B, PPO 3B, GRPO 3B, PPO 49.5 41.2 (-16.8%) 49.9 54.5 (+9.2%) 79.5 61.7 (-22.4%) 78.1 55.9 (-28.4%) 80.3 68.7 (-14.4%) 82.4 57.2 (-30.6%) 66.9 54.5 (-18.5%) 67.0 55.6 (-17.0%) Figure 6: Sensitive analysis. Heatmap of W1, W2, W3, level-wise performance. W4 com- =(0.6, 0.2, 0.2), (wa, wg, wt) bination (0.5, 0.25, 0.25), (0.8, 0.1, 0.1), ( 1 3 , 1 3 ). correspond to reward weights 3 , 1 Figure 7: The impact of memory context length on temporal reasoning: F1 performance comparison of Qwen2.5 models and Memory-T1 across context windows of 8k, 16k, 32k, 64k and 128k tokens (retaining the nearest context to the query). Sequential Answers (Hamming Accuracy) To award partial credit for ordered lists, we use Hamming Accuracy, which is the fraction of correctly positioned items. For prediction Apred = (p1, . . . , pL) and gold sequence Agold = (g1, . . . , gL) of length L, the score is: ScoreHamming(Apred, Agold) = 1 (cid:88) i=1 I(pi = gi) For example, if Agold is \"(1), (3), (2), (4)\" and Apred is \"(2), (3), (1), (4)\", the score is 1 second item and fourth item are correct. 2 , as only the Final Reward Normalization The final reward Ra is designed to strongly penalize completely incorrect answers while directly rewarding any degree of correctness. If an answer is entirely wrong (Score = 0), it receives reward of -1. For any partially or fully correct answer (Score > 0), the reward is equal to the score itself. This is formulated as: Ra = (cid:26)1 Score if Score = 0 if Score > 0 C.4 COMPARATIVE PERFORMANCE OF GRPO AND PPO Compared with GRPO, PPO generally underperformed across most categories (Figure 8). For the 3B models, PPO showed substantial declines relative to GRPO, with reductions of -16.8% in Category 21 Figure 8: Impact of text similarity filtering component in chronological fidelity reward. A, -22.4% in Category B, -14.4% in Category C, and -18.5% overall. For the 7B models, PPO achieved modest improvement in Category (+9.2%), but suffered marked decreases in Category (-28.4%) and Category (-30.6%), leading to -17.0% drop overall. These results indicate that GRPO provides more stable gains across categories, whereas PPO is less consistent, especially for more challenging tasks (Category and C). C.5 TEMPORAL REASONING UNDER CONTROLLED CONTEXT WINDOWS. To investigate how the lost-in-the-middle problem (Liu et al., 2024; Wang et al., 2025) affects temporal reasoning, we conduct controlled experiment truncating the input context at various window lengths (Figure 7). The baseline model peaks at sweet spot (e.g., 32k tokens) and then collapses as the context grows longer, failure caused by attentional dilution. In contrast, our MEMORY-T1 framework is completely resilient to this effect. This resilience stems from our coarse-to-fine candidate generation, which filters the noisy history down to concise and relevant evidence set. By shielding the final agent from irrelevant context, our framework maintains high and stable performance even when the original context exceeds 128k tokens. With this clean, highquality context, the specific impact of the RL-tuned agent becomes clear. The fine-tuning is highly targeted: it enables the agent to achieve near-perfect mastery on specific complex reasoning tasks, with MEMORY-T1(7B) exceeding 0.9 F1 scores on Order Reasoning (OR), Range Reasoning (RR), and Contextual Temporal Filtering (CTF). Conversely, the near-zero scores on Comparison (Comp) and Timeline (TL) highlight the limitations of the current agent paradigm on tasks requiring deeper compositional logic. Finally, we observe synergy between model scale and fine-tuning, with the RL policy acting as more powerful performance multiplier on the more capable 7B base model. C.6 IMPACT ON TEXT SIMILARITY IN REWARD Ablation results, as shown in Figure 8 clearly demonstrate the pivotal role of the text similarity reward. When this component is present, the model learns to filter out irrelevant dialogue history, thereby anchoring temporal spans more precisely and improving performance on duration-sensitive subtasks. Once the similarity reward is removed, performance on duration computation (DC) and compositional reasoning (Comp.) drops sharply, indicating that the model struggles to maintain accurate temporal spans without explicit guidance to suppress noise. Although slight gains appear in tasks such as Loc. and Ext., these are outweighed by the decline in precision-dependent metrics. This suggests that text similarity primarily functions as noise-reduction mechanism, ensuring that the reasoning process remains grounded in relevant context, which is especially critical for complex temporal reasoning tasks. 22 Table 9: MemAgent, Time-R1 model performance comparison: RAG vs. Non-RAG Settings Model Family Params Setting F1 Score Setting F1 Score Time-R1 MemAgent Memory-T 3B 7B 3B RAG RAG RAG 31.4 37.6 36.7 Non-RAG Non-RAG Non-RAG 29.2 40.2 37.7 C.7 SUPPLEMENTARY EXPERIMENTS: OUT-OF-DOMAIN GENERALIZATION Memory-T1 (3B) demonstrates strong OOD generalization  (Table 9)  , achieving 37.7% (Non-RAG), which is significant improvement over Time-R1 (29.2%) and nearly matches the larger MemAgent (7B) (40.2%). This high performance, particularly in the Non-RAG setting, suggests that MemoryT1s learned policy provides superior internal memory management and reasoning skill that is highly effective and robustly generalizable across domains, outweighing the benefit of RAG observed in other baselines."
        },
        {
            "title": "D LLM USAGE",
            "content": "We utilized large language models to support both manuscript polishing and data annotation. In particular, the GPT-4o API is employed to assist with the annotation of the Time-Dialog dataset. Further details of this process are provided in Appendix A. Localization Type: Localization Format: time_span Level: level_1 Question When is Debra Ryan working on starting her own business? Options: Answer: 8:35 pm, February 21, 2020 Figure 9: Localization subtask. Duration Comparison Type: Duration_Compare Format: single_choice Level: level_1 Question Which of the following two durations is longer? Options: A. Duration 1 is longer. B. Duration 2 is longer. C. The two durations are approximately the same length. Answer: Figure 10: Duration Comparison subtask."
        },
        {
            "title": "Computation",
            "content": "Type: Computation Format: time_span Level: level_1 Question How long was it between Debra Ryan going skydiving and India Brown attending street art fest in Brazil? Options: Answer: 19 days Figure 11: Computation subtask."
        },
        {
            "title": "Order Comparison",
            "content": "Type: Order_Compare Format: single_choice Level: level_1 Question For Fact1: India Brown became Queen fan. and Fact2: India Brown found flowers by lake in the park., which one happened earlier? Options: A. Fact 1 happened earlier. B. Fact 2 happened earlier. C. They happen at almost the same time. Answer: Figure 12: Order Comparison subtask. Extract Type: Extract Format: single_choice Level: level_1 Question Which of the following are time expressions mentioned in the context? Options: A. April 17, 2021 B. 2018 C. March 16, 2020 D. March 14, 2019 Answer: Figure 13: Extract subtask. Explicit Reasoning Type: Explicit_Reasoning Format: single_choice Level: level_2 Question What notable artistic or outdoor activities did India Brown participate in between April 1, 2020, and April 9, 2020? Options: A. India Brown attended street art fest in Brazil. B. India Brown took photo of feather and shells on beach. C. India Brown went hiking and sketching at nearby national park. D. India Brown received positive feedback on her artwork. Answer: Figure 14: Explicit reasoning subtask."
        },
        {
            "title": "Order Reasoning",
            "content": "Type: Order_Reasoning Format: single_choice Level: level_2 Question What was India Browns third teaching engagement in 2020? Options: A. Running painting workshop for kids. B. Teaching art at an orphanage in Cambodia. C. Conducting live demonstration for her college art club. D. Instructing pottery class at local studio. Answer: Figure 15: Order reasoning subtask."
        },
        {
            "title": "Relative Reasoning",
            "content": "Type: Relative_Reasoning Format: single_choice Level: level_2 Question What was India Browns most recent job before 12:00 am, March 09, 2020? Options: A. New series of abstract artworks. B. Travel guide based on her trip experiences. C. New painting technique from street art festival. D. Testing watercolors for her new series. Answer: Figure 16: Relative reasoning subtask. Counterfactual Type: Counterfactual Format: single_choice Level: level_3 Question What notable artistic or outdoor activities did India Brown participate in between April 1, 2020, and April 9, 2020, if she visited the Louvre in Paris in March 2020? Options: A. Mini soap sculpture. B. Photo of feather and shells. C. Photograph in Santorini, Greece. D. Sketched waterfall during hike. Answer: Figure 17: Counterfactual reasoning subtask. Co-temporality Type: Co_temporality Format: single_choice Level: level_3 Question At the same time as Debra Ryan is learning to play the guitar, what collection does India Brown have? Options: A. Soap sculptures. B. Watercolor paintings. C. CDs. D. Vinyl records. Answer: Figure 18: Co-temporality subtask. 25 Table 10: Qualitative analysis of subtasks showing significant improvement (over 10%) in MemoryT1. (e.g., Qwen2.5-3B Instruct Model: (Loc.): 0.278 0.500, (ER.): 0.692 0.821, (OR.): 0.333 0.667, (RR.): 0.563 0.875, (CTF.): 0.556 0.889, (Co-tmp.): 0.778 0.944) Subtask Question Options N/A Answer Qwen2.5-3B (Wrong) 9:32 pm, May 20, 2020 Answer MemoryT1 (Correct) pm, 8:35 February 21, Loc. ER. When is Debra Ryan starting her own business? What notable artistic or outdoor activities did India Brown participate in between April 1, 2020, and April 9, 2020? OR. What India was Browns third teaching engagement in 2020? RR. was India What Browns most recent job before 12:00 am, March 09, 2020? CTF. Cotmp. What notable artistic or outdoor activities did India Brown participate in between April 1, and April 9, 2020, she visited if 2020, the Louvre in Paris in March 2020? At the same time as Debra Ryan is learning to play the guitar, what collection does India Brown have? B B B A. India Brown attended street art fest in Brazil. B. India Brown took photo of feather and shells on beach. C. India Brown went hiking and sketching at nearby national park. D. India Brown received positive feedback on her artwork. A. Running teaching workshop for kids. B. Teaching art at an orphanage in Cambodia. C. Conducting live demonstration for her college art club. D. Instructing pottery class at local studio. A. India Brown is working on new series of abstract artworks based on her trip. B. India Brown is working as travel guide based on her trip experiences. C. India Brown is working on new painting technique learned at street art festival. D. India Brown is testing watercolors for her new series of abstract artworks. A. India Brown carved mini sculpture from soap bar. B. India Brown took photo of feather and shells on beach. C. India Brown took photograph in Santorini, Greece. D. India Brown sketched waterfall during hike. A. India Brown has collection of watercolor paintings. B. India Brown has collection of watercolor paintings. C. India Brown has collection of CDs. D. India Brown has collection of vinyl records. 26 Timeline Type: Timeline Format: event_order Level: level_3 Question Below are 8 facts. You need to sort these facts in chronological order. Options: (1) New painting technique. (4) Became Queen fan. (5) Invited to exhibit. (6) Beach photo. (7) Sketched waterfall. (8) Received feedback. Answer: (4)(5)(1)(7)(6)(2)(8)(3) (2) Shared mural image. (3) First art show. Figure 19: Timeline subtask."
        },
        {
            "title": "Prompt for Event Extraction and Time Coverage Annotation",
            "content": "You are precise temporal reasoner that analyzes utterances in multi-turn dialogue. Your goal is to analyze each individual utterance, based on its content and prior dialogue history, and extract: One or more events described in the utterance For each event: short summary of the event being described The estimated time range of that event The recurring pattern (if applicable) of that event Input: Session start time: {session_start_time} Dialogue history: {dialogue_history} Current utterance: {target_utterance} Current speaker: {speaker} Reasoning Rules: Event Time Range Estimation: Explicit date (e.g., August 14): use full-day range start: 00:00:00, end: 23:59:59 yesterday: the day before the utterance time last week: 7 days ending 1 day before the utterance time Past tense, no time mentioned: start = unknown, end = utterance time Future tense: start = utterance time, end = unknown Habitual/ongoing action: start = unknown, end = unknown, mark recurrence Recurring Field: choose from none (default), daily, weekly, monthly, yearly, habitual Output Format (JSON): [ { \"speaker\": \"Debra Ryan\", \"utterance\": \"I took this photo last week.\", \"event_summary\": \"Debra took photo\", \"event_time\":[\"2020-02-01T00:00:00\",\"2020-02-07T23:59:59\"], \"recurring\": \"none\" }, { \"speaker\": \"Debra Ryan\", \"utterance\":\"I met friend who was visiting from out of town.\", \"event_summary\": \"Debra met visiting friend\", \"event_time\": [\"2020-02-01T00:00:00\", \"2020-02-07T23:59:59\"], \"recurring\": \"none\" } ] Ensure your output is valid JSON. Only output the JSON, no extra text. Figure 20: Prompt used for event extraction and temporal coverage annotation. 28 Prompt for Question-based Event Reasoning You are precise temporal reasoner that analyzes users question. You are given the users question. Input: Users question: {user_question} Reasoning Rules: Event Time Range Estimation: Explicit date (e.g., August 14): use full-day range start: 00:00:00, end: 23:59: yesterday: the day before the utterance time last week: 7 days ending 1 day before the utterance time Past time mentioned: tense, no start = unknown, end = {current_time_str} Future tense: start = {current_time_str}, end = unknown Habitual/ongoing action: start = unknown, end = unknown, mark recurrence Recurring Field: choose from none (default), daily, weekly, monthly, yearly, habitual Output Format (JSON): { \"question\": \"What creative or social activities did India Brown participate in between April 16, 2020, at 06:22 and April 19, 2020, at 07:22?\", \"time_range\": [\"2020-04-16T06:22:00\", \"2020-04-19T07:22:00\"], \"recurring\": \"none\" } Ensure your output is valid JSON. Only output the JSON, no extra text. Figure 21: Prompt used for time range annotation over user questions. 29 Prompt for FactEvidence Alignment Your task: Determine which utterance contains the most relevant evidence that supports each of the given facts. Input: Facts: {facts_list} Sessions: {sessions_data} Output: Return the most relevant utterance for each fact using the following format: { \"fact_evidence\": [ \"fact_index\": 0, \"session_id\": \"session_id\", \"utterance_id\": \"id\" \"fact_index\": 1, \"session_id\": \"session_id\", \"utterance_id\": \"id\" { }, { }, ... ] } Example: { \"fact_evidence\": [ \"fact_index\": 0, \"session_id\": 2, \"utterance_id\": 3 { } ] } Constraints: Only include utterances that clearly support the fact (no hallucination or inference beyond whats stated). Select exactly ONE most relevant utterance per fact. If no utterance supports fact, return null for that fact. fact_index corresponds to the index in the facts list (0-based). session_id must be one of the provided session IDs. Figure 22: Prompt used for factevidence alignment in multi-session dialogues. 30 Prompt for Memory-T1 Training You are memory-aware reasoning assistant. Your task is to answer temporal questions based on multi-turn dialogue history. Carefully analyze the provided context, reason about time and events, and respond strictly in JSON format. The required JSON structure is: { selected_memory: [session_X, session_Y ], \"answer\": \"X\" } Answer Format Rules (by type): 1. Single choice: A, B, ... 2. Multiple choice: (space-separated) 3. Time: \"HH:MM:SS am/pm, Month DD, YYYY\" Example: \"02:30:00 pm, March 22, 2024\" 4. Sequence: (1)(3)(2)(4)(5)(6)(8)(7) Input format: < previous_memory >{dialogue_sessions}< /previous_memory > < question > ime : {current_time} Question : {question} < /question > Output example: { selected_memory : [session_1, session_7], answer : } Figure 23: Prompt used for training Memory-T1. Prompt for Evaluation You are presented with temporal question and previous memory, please answer the question with the correct format. The last line of your response should be of the form: Answer: $Answer, where $Answer is the answer to the problem. Output requirements: 1. Single choice: AB... (uppercase) 2. Multiple choice: (space-separated uppercase) 3. Time: HH:MM:SS am/pm, Month DD, YYYY Example: 10:45:41 pm, January 15, 2024 4. Sequence: (1)(3)(2)(4)(5)(6)(7)(8) Input format: < previous_memory >{dialogue_sessions}< /previous_memory > < question > ime : {current_time} Question : {question} < /question > Remember to put your answer on its own line after Answer: Figure 24: Prompt used for evaluation of temporal reasoning tasks."
        }
    ],
    "affiliations": [
        "HKUST",
        "Huawei Technologies Co.,Ltd",
        "The Chinese University of Hong Kong",
        "The University of Edinburgh"
    ]
}