{
    "paper_title": "Maya: An Instruction Finetuned Multilingual Multimodal Model",
    "authors": [
        "Nahid Alam",
        "Karthik Reddy Kanjula",
        "Surya Guthikonda",
        "Timothy Chung",
        "Bala Krishna S Vegesna",
        "Abhipsha Das",
        "Anthony Susevski",
        "Ryan Sze-Yin Chan",
        "S M Iftekhar Uddin",
        "Shayekh Bin Islam",
        "Roshan Santhosh",
        "Snegha A",
        "Drishti Sharma",
        "Chen Liu",
        "Isha Chaturvedi",
        "Genta Indra Winata",
        "Ashvanth. S",
        "Snehanshu Mukherjee",
        "Alham Fikri Aji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid development of large Vision-Language Models (VLMs) has led to impressive results on academic benchmarks, primarily in widely spoken languages. However, significant gaps remain in the ability of current VLMs to handle low-resource languages and varied cultural contexts, largely due to a lack of high-quality, diverse, and safety-vetted data. Consequently, these models often struggle to understand low-resource languages and cultural nuances in a manner free from toxicity. To address these limitations, we introduce Maya, an open-source Multimodal Multilingual model. Our contributions are threefold: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; 2) a thorough analysis of toxicity within the LLaVA dataset, followed by the creation of a novel toxicity-free version across eight languages; and 3) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya."
        },
        {
            "title": "Start",
            "content": "Maya: An Instruction Finetuned Multilingual Multimodal Model Nahid Alam1,2, Bala Krishna Vegesna5, Iftekhar Uddin2, Drishti Sharma2, Karthik Reddy Kanjula2, Abhipsha Das2, Shayekh Bin Islam7,2, Chen Liu10, Isha Chaturvedi11, Surya Guthikonda3,2, Anthony Susevski2, Timothy Chung4,2, Ryan Sze-Yin Chan6, Roshan Santhosh8, Snegha A9, Genta Indra Winata12, 4 2 0 2 0 1 ] . [ 1 2 1 1 7 0 . 2 1 4 2 : r Ashvanth.S2, Snehanshu Mukherjee13, Alham Fikri Aji14 1Cisco Meraki, 2Cohere For AI Community, 3Indiana University Bloomington, 4Imperial College London, 5Georgia Institute of Technology, 6The Alan Turing Institute, 7Bangladesh University of Engineering and Technology, 8University of Pennsylvania, 9IIT Bombay, 10TU Darmstadt, 11Articul8 AI, 12Capital One, 13IIT Dhanbad, 14MBZUAI nahid.m.alam@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "The rapid development of large Vision-Language Models (VLMs) has led to impressive results on academic benchmarks, primarily in widely spoken languages. However, significant gaps remain in the ability of current VLMs to handle low-resource languages and varied cultural contexts, largely due to lack of high-quality, diverse, and safetyvetted data. Consequently, these models often struggle to understand low-resource languages and cultural nuances in manner free from toxicity. To address these limitations, we introduce Maya, an open-source Multimodal Multilingual model. Our contributions are threefold: 1) multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; 2) thorough analysis of toxicity within the LLaVA dataset, followed by the creation of novel toxicity-free version across eight languages; and 3) multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya. 1. Introduction Vision Language Models (VLMs) have emerged as key technique in artificial intelligence, enabling machines to understand and reason about the visual world through natural language. Recent advancements in Large Language Models (LLMs) and general-purpose image encoders such as *Work does not relate to the authors positions. CLIP [44] and SigLIP [63] has significantly boosted VLM capabilities. Architectures such as Flamingo [5], LLaVA [34, 35], KOSMOS [40, 42], Florence-2 [59] and Molmo [16] demonstrate strong performance across tasks including image captioning, Visual Question Answering (VQA), and complex reasoning. Qwen2-VL [58] introduced Multimodal Rotary Position Embedding (M-RoPE) [50] and dynamic resolution techniques, while PaLIs joint modality scaling and cross-lingual learning [13, 14] have furthered vision-language understanding. Despite these advances, VLMs remain limited to high-resource languages. This creates disparity in accessibility, as current models struggle with cultural contexts and visual concepts in low-resource languages [26]. This gap is largely due to the lack of high-quality multilingual multimodal datasets. Pre-training datasets like COCO [31], Flickr30K [61], LAION [46], Visual Genome [27], and LLaVA [35] are predominantly in English, limiting cross-linguistic generalization. Existing multilingual datasets, such as Multi30k [18] and Crossmodal-3600 [56], are limited in scale and cultural diversity. Current datasets often contain toxic and culturally insensitive content [62], perpetuating biases and stereotypes. To our knowledge, no peer-reviewed research has systematically addressed the mitigation of toxicity and harmful content in imagetext datasets. This highlights the need for comprehensive, toxicity-mitigated datasets that support robust, multilingual training while capturing linguistic and cultural diversity. To address these challenges, we introduce Maya, an open-source Multilingual Multimodal Vision Language Model (mVLM) that expands multimodal capabilities to eight languages with an emphasis on data quality and cultural sensitivity. Built on the LLaVA framework, Maya includes newly created pre-training dataset designed to support multilingual and culturally aware VLM development. Our key contributions include: 1. novel multilingual image-text pretraining dataset consisting of 558,000 images for future development of mVLMs, 2. toxicity-free version of the dataset in 8 languages, and 3. new mVLM that demonstrates improved performance in understanding cultural and linguistic nuances compared to PALO-7B [37] on LLaVA-Bench-In-The-Wild [35], offering multilingual alternative to LLaVA [35]. 2. Related Work 2.1. Multilingual Large Language Models While open-source large language models have made notable strides, progress in multilingual LLMs (mLLMs) still lags. Many LLMs [1, 4, 17, 54] offer limited multilingual support, typically focusing on high-resource languages with reduced performance in others. Dedicated mLLMs [7, 9, 28, 30, 32, 57, 60], such as BLOOM [28] and Aya101 [57], extend language coverage to 43 and 101 languages respectively, improving performance across tasks like translation, summarization, and reasoning. 2.2. Multimodal Large Language Models Research in Multimodal Large Language Models (MLLMs) has advanced significantly in enabling models to process both images and text across diverse tasks, with progress along two main paths: proprietary closed-source systems [6, 39, 53] and open-source initiatives. While most of the initial efforts focused predominantly on Englishlanguage capabilities, such as LLaVA [34, 35], several recent works have attempted to close this gap by targeting non-English languages. PALO [37] extended LLaVA [35] to cover 10 major languages via semi-automated translations of the LLaVA-Instruct-150k dataset [35]. X-LLaVA [48] demonstrated strong Korean-English bilingual performance. Broader language coverage has been achieved by Llama 3.2 [17] supporting 8 languages and Qwen2VL [58] covering several European languages alongside Japanese, Korean, Arabic and Vietnamese. More recent developments have pushed multilingual boundaries through diverse approaches: PaliGemma [10] pretrained on variety of vision-language tasks upto 35 languages providing an open base MLLM but without instruction tuning, [19] mBLIP realigned English-tuned image encoder using machine-translated data for 95 languages, Pangea [62] is multilingual multimodal model trained on an instruction dataset spanning 39 languages, Parrot [51] addressed supervised fine-tuning imbalances across 6 languages through textual guidance for visual token alignment, while models like Phi-3.5-Vision [1], Pixtral [3] and Molmo [16] are mostly English-centric [62]. 2.3. Multimodal Multilingual Datasets Large-scale datasets for retrieval and captioning such as xFlickrCO [12], BLIP3-KALE [8] and WIT [49] have been developed to support broader multimodal research. For instruction tuning, PALO [37] built dataset of 150K images in 10 languages from subset of LLaVA image-text instruction tuning dataset on English text, while Pangea [62] demonstrated broader coverage with image-text pair dataset in 39 languages. 2.4. Evaluating Multimodal Models Evaluating multilingual multimodal LLMs [10, 58, 62] involves two main categories. The first assesses standard multimodal tasks, such as cross-lingual visual question answering (VQA) with xGQA [43] and image-text retrieval via xFlickrCO [12]. The second evaluates cultural understanding, using datasets like MaRVL [33] for visual reasoning, CVQA [45] for cultural context, and XM3600 [56] for cross-modal comprehension. Significant progress has been made in developing multilingual multimodal datasets for evaluating VLMs including XVNLI [12] for visual natural language inference, MTVQA [52] for temporal reasoning, M3Exam [64] for educational assessment, PangeaBench [62] for broad multilingual capabilities, MMMB [51] for cross-lingual understanding, and LLaVA-Bench-InThe-Wild [35, 37] for real-world scenarios. Notably, none of these datasets focus on evaluating toxicity. 3. Dataset Creation and Filtering 3.1. Dataset Creation Methodology Recently, PALO [37] and Pangea [62] have created multilingual image-text dataset for building multilingual multimodal models. However, these multilingual datasets often suffer from data quality issues and distribution biases across languages. For instance, in the PALO dataset, the distribution varies significantly between English and other languages [18, 24, 37]. To address these limitations, we present novel pre-training dataset tailored for LLaVAs architecture that is both multilingual and optimized for diverse language representation. Our dataset introduces rigorous processes for toxicity analysis, distribution balancing, and quality control, ensuring consistent and reliable performance across multiple languages and modalities as shown in Figure 1. We expand the original English LLaVA dataset, which contains 558,000 samples, to include seven additional languagesChinese, French, Spanish, Russian, Hindi, Japanese, and Arabicyielding total of 4.4 million samples, equally distributed across all eight languages. Figure 1. Pretrain Dataset Preparation Process Every sample in our dataset has undergone toxicity verification and filtering, ensuring safer training corpus than the original LLaVA pretraining dataset. Our approach encompasses three core components: 1) parallel dataset creation using hybrid translation method, 2) prompt engineering optimization, and 3) scalable generation of pre-training datasets. This pipeline integrates multiple language models, such as [6, 39, 53], alongside specialized multilingual models like Aya 35B [7], to ensure highquality, cross-lingual data suitable for multilingual applications. 3.1.1. Initial Dataset Processing Our pipeline is built on the LLaVA dataset [35], centered around image-text pairs and their corresponding GPT responses. We use stratified sampling to select 30 representative samples per language, optimizing for linguistic diversity through Length Analysis (LA), Flesch Reading Ease (FRE), and Flesch-Kincaid Grade Level (FKGL) metrics [55]. To ensure quality, we implement cascaded translation verification system. First, we use Google Translate to do initial translation. This is followed by back-translation verification. Finally human reviewers verify it with the help of [2, 6, 53]. 3.1.2. Prompt Engineering and Evaluation In the prompt engineering phase, we employ prompt evaluation process for each language. First, we create prompt evaluation dataset using similar process as Figure 1. We take 6 sample prompts and translate the English text of the prompt evaluation dataset to seven languages using Aya 35B. We then compare these translations with the reference translation of the prompt evaluation dataset using BLEU score [41] and N-gram score [11, 47]. In Figure 2, we plot the n-gram BLEU scores (averaged across 1-gram, 2-gram, 3-gram, and 4-gram) for each language and preamble type. We find that across all seven languages (Arabic, Chinese, French, Hindi, Japanese, Russian, and Spanish), Preamble 6 consistently achieves the highest N-gram BLEU scores, with most languages showing notable upward trend from Listing 1. Translation Instructions ## Instructions You are an expert in translations. Your job is to translate the input to Japanese in the given chat. Ensure that: {Specific Things to Consider while Translating} Note: {Extra Constraints on Output Generation} ## Examples ### Example 1 Input: {Input Sentence} Expected Output: {Translated Sentence} Type 5 to Type 6, reaching values around 0.4-0.5. The radar plot in Figure 3 provides complementary evidence by breaking down the N-gram analysis (1-gram through 4gram) averaged across all languages for each preamble type. It reveals that Preamble 6 maintains the largest area coverage, indicating superior performance across all N-gram levels compared to other preamble types. This is particularly significant as higher N-gram BLEU scores suggest better preservation of phrase structures and contextual meaning in the translations. The relatively balanced performance of Preamble 6 across different languages and across different N-gram levels indicates its robustness and languageagnostic effectiveness in the translation task. We find that preamble 6 is the best preamble across all the seven languages. The resulting prompt template is in listing 1. We use this prompt template in our translation framework. 3.1.3. Translation Framework Design Our translation framework uses the best preamble identified in the prompt engineering evaluation step. The prompt includes 1) standardized input-output formatting to maintain uniformity across languages, 2) example-driven instruction Figure 4. Image Toxicity Analysis with LLaVAGuard Figure 2. N-gram values by language and preamble type. These values are average of 1-gram, 2-gram, 3-gram and 4-gram Figure 5. Image Caption Toxicity Analysis with Toxic-BERT tecture enables efficient processing of the full 558K samples while maintaining translation quality. We implement version control for intermediate translations and maintain detailed debug logs, ensuring reproducibility and enabling systematic error analysis. 3.2. Dataset Toxicity Filtering In the original English LLaVA Pretrain dataset which has 558,000 image-caption pairs, we observed some toxic contents. To methodically identify toxic content, we used LLaVAGuard 7B [22] and Toxic-BERT [20]. For images, we relied on the LlavaGuard 7B framework to spot and categorize unsafe or toxic visuals based on set guidelines. For text, we used the Toxic-BERT model to scan captions and flag anything with offensive or harmful language. LLaVAGuard is specifically designed for in-depth toxicity analysis, assigning safety ratings, categories, and rationales to flagged images, producing structured data about each identified risk. This analysis led to the categorization of content across various types, including sexual content, hate speech, violence, and substance abuse, which are visually represented in the toxicity analysis chart as shown in Figure 4. Additionally, prompt optimization pipeline was introduced to ensure precise filtering, capturing potentially harmful content while reducing false positives. The overall process for creating toxicity free dataset is Figure 3. Radar chart of N-gram averages by preamble sets tailored for complex translations, and 3) integrated validation triggers to ensure quality control. Leveraging Aya 35B advanced translation capabilities, our framework achieves greater than 0.47 average BLEU scores across seven languages. 3.1.4. Scalable Dataset Generation For large-scale dataset generation, we implement batch processing pipeline integrated with Aya 35B API as shown in Figure 1. First, we extract GPT values with quality filter from the LLaVA pretrain dataset. These extracted GPT values are then passed through the Aya 35B batchoptimized API calling with intermediate translation checkpointing. The pipeline does necessary error handling and comprehensive logging for quality tracking. This archiMultilingual Language Response Xa Multilingual Language Model fϕ Projection Hv Multilingual Vision Encoder Zv Xv Image Hq Xq Multilingual Language Instruction Figure 7. Maya Architecture adapted from LLaVA [36] than CLIP [44], which is traditionally used in LLaVA. This choice is motivated by SigLIPs strong performance, multilingual adaptability, and capacity for variable-length patch sizes. Unlike CLIP, SigLIP supports scalable positional embeddings, enabling it to accept inputs of varying dimensions through positional embedding interpolation. This flexibility makes SigLIP particularly suitable for our purpose. For each input image Xv, we get the visual features from SigLIP, Zv = g(Xv). trainable projection matrix then converts the image features Zv into language features Hv. 4.2. Pretraining For image-text alignment, we used projection matrix that brings image features Xv closer to language features. This projection matrix is simple 2-layer MLP with GELU activation [23], as in LLaVA 1.5 [34]. Although we experimented with 4and 8-layer MLPs, the 2-layer configuration consistently achieved the lowest training loss. Advanced alignment techniques, such as gated soft-attention in Flamingo [5], Q-Former in BLIP-2 [29], or pooling from MM1 [38] as alternatives to the projection layer, are set aside for future work. For each image Xv, we used the multi-turn conversation data (X 1 , 1 a, . . . , , ) where is the total number of turns from LLaVA [35]. We translated this data to 7 languages. We pretrained Maya on this dataset and the toxicity free version of the same. Image inputs were cropped to 256x256 for compatibility with the SigLIP encoder. Training was conducted on 8xH100 GPUs with 80GB DRAM, using per-device batch size of 32 and global batch size of 256. learning rate of 1e-3 and cosine scheduler were applied during training. The pretraining process only trains the projection matrix and took about 20 hours. 4.3. Finetuning We instruction-finetuned our pretrained Maya model using the PALO 150K instruction-tuning dataset [37]. During finetuning, we observed that Low Rank Adaptation (LoRA) Figure 6. Dataset Toxicity Filtering Method shown in Figure 6. LLaVaGuard output provides rating, category, and rationale explaining why an image violates guidelines. We then refine this by identifying genuinely toxic images. This is done by developing an optimized prompt using the Cohere prompt tuner*. We use this prompt as preamble (System Prompt) and pair it with the previous informationrating, category, and rationaleassociated with each image flagged as unsafe by LLaVAGuard. Command R+ [15] then analyzes these results to identify the truly unsafe image IDs. In our analysis, LLaVAGuard identified 7,600 images as toxic and the final Command R+ output identified 7,111 images as unsafe. For identifying toxic captions, we used Toxic-BERT that categorized 892 image captions as toxic with greater than 80% confidence as shown in Figure 5. LLaVAGuard and Command R+ identified 7,111 images and Toxic-BERT identified 892 images, there are in total 7,531 unique toxic images. We then removed those 7,531 images from the pretraining dataset to create the toxicity free pretraining dataset. 4. Multilingual Multimodal Instruction Tuning 4.1. Model Architecture The Maya model architecture draws inspiration from LLaVA 1.5 [34]. Our goal is to leverage pretrained multilingual Aya model [7] and vision encoder with multilingual capabilities. Specifically, we employ the Aya-23 8B model as our LLM fϕ because of its multilingual capability. Aya23 has 8 billion parameters, an 8K context window, and is trained across 23 languages. Our dataset, however, includes 8 of these 23 languages, aligning with our objective of optimizing Maya for diverse yet focused linguistic range. For the vision encoder, we opted for SigLIP [63] rather *https://docs.cohere.com/v2/docs/prompt-tuner siglip-base-patch16-256-multilingual from https: //huggingface.co/google/siglipbasepatch16256multilingual [25] produced suboptimal results, particularly when both adapter matrices and were updated with the same learning rate [21]. Based on these findings, we opted against using LoRA and instead implemented full finetuning on 8xH100 GPUs. Our finetuning configuration used perdevice batch size of 16 and global batch size of 128. Finetuning process took about 48 hours. We kept both the vision encoder and the language encoders frozen during the training process. We did pretraining and finetuning for both versions of our dataset: the pretraining dataset translated into 7 languages, with and without toxicity filtering. This resulted in two variants of our model: Maya and Maya-Toxicity-Free. 5. Results We evaluate Maya on PALO multilingual evaluation set [37] as shown in Table 1. Although our pretraining dataset only contains eight languages, we finetuned Maya on PALO instruction tuning dataset that contains ten languages. Therefore in Table 1 Maya evaluation results are shown in all of those ten PALO instruction-tuning languages. Maya offers good performance across languages compared to the models within the size class of 7B models, and offers comparable performance to the 13B size models. On average across all languages, Maya outperforms LLaVA-13B and is only slightly worse than PALO-13B. Specifically, among the common eight languages, Maya performs better than PALO 7B in five languages. We think this is due to the fact that Maya pretrain dataset is multilingual in nature whereas PALO pretrain dataset is same as the English-only LLaVA pretrain dataset. Maya also beats PALO and LLaVA in Arabic language for both the size classes of 7B and 13B. The root based language system of Arabic along with the quality of translation due to the choice of preamble led to this result. Another observation we have with PALO multilingual evaluation is that the scores for Maya and Maya-Toxicity Free are exactly the same. We empirically conclude that this is because multilingual evaluation does not account for content toxicity. In Figure 8, we compare Maya responses with LLaVA and GPT4. We observe that Maya responses are very similar to LLaVA and lacks the similar level of details vs. GPT4. For example, in answering Question 1, both Maya and LLaVA lacks the details with respect to plastic box. When Maya is wrong, for example in Question 2, LLaVA is also wrong; although in different way. The right answer here is Fage from GPT4. Both Maya and LLaVA get it wrong saying Yoplait and Chobani respectively. Additional qualitative evaluation results are added in the supplemental materials. To compare the effect of filtering the pretraining dataset for toxicity, we evaluate Maya and Maya-Toxicity Free models across various English-only benchmarks as shown Close-Up of Multiple Everyday Objects Question 1: What fruit is in the left part of the fridge? Maya: There are strawberries in the left part of the fridge. LLaVA-7B: There are strawberries in the left part of the fridge. GPT4: In the left part of the fridge, there is plastic box of strawberries. Question 2: What is the brand of the yogurt flavored with blueberry? Maya: The brand of the yogurt flavored with blueberry is Yoplait. LLaVA-7B: The brand of the yogurt flavored with blueberry is Chobani. GPT4: The brand of the blueberry-flavored yogurt is Fage. Figure 8. Comparison of model responses to visual questions on everyday objects in Table 2. Both models have comparable accuracy across most of the benchmarks, with only slight variations. This suggests that removing toxic content from the training data does not significantly degrade overall performance for these tasks. Maya-Toxicity Free shows marginal gains in TextVQA, Text Translation, and Numerical Calculation benchmarks. Performance in Commonsense Reasoning and MM-VeT decreases, indicating that some complex reasoning tasks may benefit from the presence of diverse (and possibly toxic) content in training data. In Table 3, we show how Maya compares with its toxicity free variant in various VizWiz categories. MayaModel English Chinese French Spanish Russian Japanese Arabic Hindi Bengali Urdu Avg. Maya (8B) LLaVA-7B PALO-7B LLaVA-13B PALO-13B 61.5 67.9 64.2 -6.4 69.5 65. -8.0 61.7 55.7 55.7 +6.0 62.9 62.1 -1.2 61.0 62.4 58. -1.4 67.5 66.4 -6.5 60.4 64.5 61.0 -4.1 64.6 65. -5.5 62.2 55.3 57.4 +4.8 62.3 62.4 -0.2 63.7 59.2 57. +6.2 65.3 60.6 -1.6 63.4 38.9 57.8 +5.6 37.2 56. +6.5 64.0 29.4 57.6 +6.4 27.8 66.8 -2.8 50.8 13.9 51. -0.9 20.4 53.5 -2.7 55 21.8 55.3 -0.3 22.1 59. -4.6 60.4 46.9 57.7 +2.7 49.9 61.9 -1.5 Table 1. comparison of LLaVA and PALO with Maya on eight languages adapted from LLaVA-Bench (In-the-Wild). Values underlined indicate best performance within size class and values in bold indicate best performance across all models tested. We provide performance differences between Maya and the best competing model within the size classes where red indicates where Maya is performing better and blue indicates where Maya is performing worse than the best in the size class. Avg. represents the average over all the languages. Benchmarks Maya MayaToxicity Free 57.79% 57.84% 34.92% 34.98% 70.27% 69.51% 47.01% 48.56% 81.00% 80.97% 84.10% 82.84% 85.30% 84.27% 71.10% 71.12% 29.8 72.45% 71.90% 27.7 GQA VizWiz ScienceQA TextVQA POPE-adversarial POPE-popular POPE-random MMBench MM-VeT MME (Perception + Cognition) MME (Cognition): Commonsense Reasoning Numerical Calculation Text Translation Code Reasoning 75.71% 68.57% 47.50% 50.00% 52.50% 55.00% 60.00% 57.50% Table 2. Accuracy of Maya models on English Language across multiple benchmarks. VizWiz Overall other unanswerable yes/no number Maya Maya-Toxicity Free 34.92% 34.03% 30.88% 77.02% 24.63% 34.98% 33.66% 32.03% 77.08% 24.72% Table 3. Detailed accuracy results for VizWiz. Toxicity Free slightly outperforms Maya by 0.06%. This indicates that the removal of toxic content in training data has minimal positive impact on the overall performance of the VizWiz benchmark. Maya performs slightly better in the other category. This suggests that for questions that fall outside of well-defined categories, the unfiltered training data may offer slight advantage. Maya-Toxicity Free shows improved performance (32.03% vs. 30.88%), suggesting that cleaner training dataset helps the model better recognize when question cannot be answered. Both models perform almost identically in the yes/no category indicating that toxicity removal does not significantly affect binary question accuracy. In Table 4, we show detailed performance analysis of the Maya and Maya-Toxicity Free models on the MMVeT benchmark, breaking down their overall accuracy, individual capabilities, and more complex capability integrations. In categories like Spatial Awareness, OCR, Knowledge and Spatial Awareness, Language Generation, OCR, Maya-Toxicity Free shows zero performance, suggesting areas where toxicity removal could have eliminated beneficial patterns. For Spatial Awareness, OCR, Recognition, Math, both models perform poorly, indicating the difficulty of this capability combination, with no difference between the models. Overall, MMVeT results highlight that while Maya-Toxicity Free shows some improvements in specific areas, like language generation and certain integrations involving spatial awareness, it generally suffers performance decline, especially in math, OCR, and integrated recognition tasks. The impact of removing toxic content seems to weaken the models overall ability to perform complex reasoning and integrate multiple capabilities effectively. We leave out further investigation on this topic for future work. Figure 10 shows visual question output in different languages for figure 9. We asked Maya please describe the food in {language} in 1 sentence. We observe that Bengali reply in this case is more detailed than English, identifying meat on the bowl and wooden table. Spanish, French MMVeT Overall Capabilities Recognition OCR Knowledge Language Generation Spatial Awareness Math Maya MayaToxicity Free 29.8 27. 33.7 23.1 16.0 18.4 29.0 15.4 32.7 18.7 13.7 19.3 26.7 7.7 Capability Integrations Language Generation, Recognition, Knowledge Recognition Spatial Awareness, OCR Spatial Awareness, OCR, Math Spatial Awareness, Recognition OCR OCR, Math Recognition, Knowledge Spatial Awareness, Language Generation, OCR, Recognition Language Generation, OCR, Recognition, Knowledge Spatial Awareness, OCR, Recognition OCR, Recognition Spatial Awareness, OCR, Knowledge Spatial Awareness, Recognition, Knowledge Spatial Awareness, Language Generation, OCR Spatial Awareness, OCR, Recognition, Math 15.5 16. 69.2 25.8 28.6 41.7 25.8 0.0 16.7 44.2 68.9 25.4 14.3 50.0 18.3 0.0 6.0 49.8 15.2 9.0 14.3 14. 53.0 3.3 25.0 0.0 50.0 0.0 19.0 24. 0.0 0.0 Table 4. MMVeT results including individual and integrated capabilities. Values are mean of 5 runs with 0.2 standard deviation. and Hindi outputs are more detail than English as all three languages identify meat but fails to identify the wooden table as in Bengali. Chinese and Japanese outputs are similar to English. 6. Conclusion Maya drives high-quality AI content generation across languages and regions, leveraging multilingual, multimodal data to fill gaps, especially for low-resource languages. To ensure safe deployment, we rigorously curate data to Figure 9. Example image from LLaVA-Bench (In-the-Wild) [35]. Figure 10. Maya output for prompt (with image from Figure 9): Please describe the food in {language} in 1 sentence. remove harmful content, though some residual traces may persist. Future work will focus on refining Mayas adaptability, including testing alternative projection layers for improved cross-modal alignment and unfreezing decoder layers to optimize layer-specific fine-tuning. We plan to expand our pretrain dataset to include Bengali and Urdu translations and grow the instruction-tuning dataset to 665K examples, enhancing instruction accuracy across languages and modalities. To improve translation quality, we will tailor preambles per language. Additionally, we aim to benchmark rigorously on benchmarks such as PangeaBench [62], CVQA [45] etc. to ensure robust, diverse user support."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv preprint arXiv:2404.14219, 2024. 2 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. 3 [3] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amelie Heliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 2 [4] Mistral AI. Un Ministral, des Ministraux, 2024. 2 [5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 1, 5 [6] Anthropic. Introducing the next generation of Claude, 2024. 2, [7] Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, et al. Aya 23: Open Weight Releases to Further Multilingual Progress. arXiv preprint arXiv:2405.15032, 2024. 2, 3, 5 [8] Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, Jae Sung Park, Etash Guha, Silvio Savarese, Ludwig Schmidt, Yejin Choi, Caiming Xiong, and Ran Xu. Blip3kale: Knowledge augmented large-scale dense captions, 2024. 2 [9] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen Technical Report. arXiv preprint arXiv:2309.16609, 2023. 2 [10] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. PaliGemma: versatile 3B VLM for transfer. arXiv preprint arXiv:2407.07726, 2024. 2 [11] Eric Brill, Radu Florian, John C. Henderson, and Lidia Mangu. Beyond n-grams: Can linguistic sophistication imIn COLING 1998 Volume 1: prove language modeling? The 17th International Conference on Computational Linguistics, 1998. [12] Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, and Ivan Vulic. IGLUE: benchmark for transfer learning across In Proceedings of the modalities, tasks, and languages. 39th International Conference on Machine Learning, pages 23702392. PMLR, 2022. 2 [13] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: JointlyScaled Multilingual Language-Image Model. arXiv preprint arXiv:2209.06794, 2022. 1 [14] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. PaLI-X: On Scaling up Multilingual Vision and Language Model. arXiv preprint arXiv:2305.18565, 2023. 1 [15] Cohere. Command R. https://cohere.com/command, 2024. 5 [16] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and PixMo: Open Weights and Open Data for State-of-theArt Multimodal Models. arXiv preprint arXiv:2409.17146, 2024. 1, [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. 2 [18] Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30K: Multilingual English-German image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 7074, Berlin, Germany, 2016. Association for Computational Linguistics. 1, 2 [19] Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavaˇs. mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. arXiv preprint arXiv:2307.06930, 2023. 2 [20] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify, 2020. 4 [21] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. LoRA+: Efficient Low Rank Adaptation of Large Models. arXiv preprint arXiv:2402.12354, 2024. 6 [22] Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, and Patrick Schramowski. LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment. arXiv preprint arXiv:2406.05113, 2024. 4 [23] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016. 5 [24] Musashi Hinck, Carolin Holtermann, Matthew Lyle Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shaoyen Tseng, and Vasudev Lal. Why do LLaVA Vision-Language Models Reply to Images in English? arXiv preprint arXiv:2407.02333, 2024. 2 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685, 2021. [26] Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The State and Fate of Linguistic Diversity and Inclusion in the NLP World. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online, 2020. Association for Computational Linguistics. 1 [27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International journal of computer vision, 123:3273, 2017. 1 [28] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. BLOOM: 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100, 2023. 2 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. 5 [30] Peiqin Lin, Shaoxiong Ji, Jorg Tiedemann, Andre FT Martins, and Hinrich Schutze. MaLA-500: Massive Language arXiv preprint Adaptation of Large Language Models. arXiv:2401.13303, 2024. 2 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [32] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learnarXiv preprint ing with multilingual language models. arXiv:2112.10668, 2021. 2 [33] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. Visually Grounded Reasoning across Languages and Cultures. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1046710485, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 2 [34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning, 2023. 1, 2, 5 [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning, 2023. 1, 2, 3, 5, 8 [36] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. 5 [37] Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao Anwer, Tim Baldwin, Michael Felsberg, and Fahad Khan. PALO: Polyglot Large Multimodal Model for 5B People. arXiv preprint arXiv:2402.14818, 2024. 2, 5, 6 [38] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training. arXiv preprint arXiv:2403.09611, 2024. 5 [39] OpenAI. Hello GPT-4o, 2024. 2, 3 [40] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-G: Generating Images in Context with Multimodal Large Language Models. ArXiv, abs/2310.02992, 2023. 1 [41] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 3 [42] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding Multimodal Large Language Models to the World. arXiv preprint arXiv:2306.14824, 2023. 1 [43] Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanIvan Vulic, and Iryna Martin O. Steitz, Stefan Roth, Gurevych. xGQA: Cross-Lingual Visual Question Answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 24972511, Dublin, Ireland, 2022. Association for Computational Linguistics. [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 5 [45] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. CVQA: Culturally-diverse Multilingual arXiv preprint Visual Question Answering Benchmark. arXiv:2406.05967, 2024. 2, 8 [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1 [47] Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. 3 [48] Dongjae Shin, HyeonSeok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, and Kyungtae Lim. X-LLaVA: Optimizing BilinarXiv preprint gual Large Vision-Language Alignment. arXiv:2403.11399, 2024. 2 [49] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine LearnIn Proceedings of the 44th International ACM SIGIR ing. [61] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. [62] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. arXiv preprint arXiv:2410.16153, 2024. 1, 2, 8 [63] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Sigmoid Loss for Language Image PreLucas Beyer. In Proceedings of the IEEE/CVF International Training. Conference on Computer Vision, pages 1197511986, 2023. 1, 5 [64] Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3Exam: Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. Advances in Neural Information Processing Systems, 36:54845505, 2023. 2 Conference on Research and Development in Information Retrieval, page 24432449, New York, NY, USA, 2021. Association for Computing Machinery. 2 [50] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding, 2021. 1 [51] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, DeChuan Zhan, and Han-Jia Ye. Parrot: Multilingual Visual Instruction Tuning, 2024. [52] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering. arXiv preprint arXiv:2405.11985, 2024. 2 [53] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2023. 2, 3 [54] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 2 [55] Textstat Contributors. Textstat: An Easy to Use Library to Calculate Statistics from Text, 2024. Python package for calculating readability statistics. [56] Ashish Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522, 2022. 1, 2 [57] Ahmet Ustun, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827, 2024. 2 [58] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2 [59] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4818 4829, 2024. 1 [60] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: Massively Multilingual Pre-trained Text-toText Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483498, Online, 2021. Association for Computational Linguistics. 2 Maya: An Instruction Finetuned Multilingual Multimodal Model"
        },
        {
            "title": "Supplementary Material",
            "content": "Caption Generation in Multilingual. In Figure 15, we compare Mayas response in 10 languages in describing the image in Figure 14 in detail. Translating Hindi response to English: The image shows dining table with three bowls containing different Asian dishes. One of the bowls has noodles, which looks like delicious soup. The other two bowls contain different types of vegetables, making the meal healthy and vibrant. Apart from the main dishes, there are three cups on the table, probably for beverages. spoon is also visible, placed near the bowl of noodles, suggesting it will be used for serving the food. Overall, this scene captures delectable and welcoming Asian dining experience. In Chinese, Maya describes the scene as featuring few bowls of Asian noodles of varying kinds, along with identifying meat, vegetables, and spoon. It also notes the presence of three cups possibly containing side dishes or drinks, adding that the whole scene creates an inviting atmosphere for people to enjoy this delicious meal together. English translation of Spanish response: The image shows dining table with three bowls filled with different types of food, probably of Asian origin. One of the bowls appears to be bowl of noodles, while the others contain variety of vegetables. There are also three cups placed around the table, which adds to the variety of the meal. spoon can be seen resting on the table, ready to be used to enjoy the meal. The table is filled with delicious dishes, making it welcoming scene for those who want to enjoy meal together. Bengali response is similar to Spanish - they both fail to identify matcha, meat and does not identify the correct position of the spoon. 7. Additional Results We compare Mayas responses with those of LLaVA-7B and GPT4 using set of additional images. The objective is to qualitatively evaluate Mayas performance against model of similar scale (LLaVA-7B) and significantly larger model (GPT4, which is estimated to have trillions of parameters).The comparison includes examples of visual question answering and detailed caption generation to illustrate the strengths and differences among the models. Visual Question Answering. Figure 11 presents comparative analysis of model responses to visual question identifying the brand featured in an advertisement. The conversation box displays the outputs from three models: Maya, LLaVA-7B, and GPT4. All three models successfully identify the brand as Subway. Maya demonstrates consistent understanding of visual content and excels at extracting OCR information, particularly when the text is clearly visible and occupies significant portion of the image. Figure 12 presents comparative analysis of model responses in identifying well known landmark. While LLaVA-7B and GPT4 correctly identify the landmark as Diamond Head, volcanic crater in Hawaii, Maya incorrectly identifies it as the Seven Islands of Hawaii. This discrepancy illustrates differences in the models accuracy and contextual understanding. The correct responses from LLaVA-7B and GPT4 demonstrate superior alignment with the visual and textual context, whereas Mayas misidentification suggests potential limitations in training on geographically specific datasets. Caption Generation. Our example in Figure 13 illustrates how Maya generates description to highways-at-night scene. All models provide detailed description of the photo with different focuses. GPT4 provides precise information about the photo, including the number of lanes, the direction of the traffic, the density of the cars, the elevation of the four-lane highway, the illumination of the lights, and the presence of trees. Maya (similar to LLaVA-7B), while does not identify the number of lines, provides vivid but detailed description of the scene. Next, we focus on models ability to describe culturally diverse image in Figure 14. Here, GPT4 accurately identify the restaurant that serves this ramen, with detailed in the photo such as chashu in the ramen, the chopsticks in their paper wrap, the garnish, and the sides served with the ramen. It shows an understanding of the context. While Maya correctly identifies different elements on the table, the description is unfortunately lacks the specific culturally relevant details provided in the context. Subway Question: What brand is featured in this advertisement? Maya: The Subway brand is featured in this advertisement. LLaVA-7B: The brand featured in this advertisement is Subway. GPT4: The brand featured in this advertisement is Subway. Figure 11. Comparison of model responses to visual questions on brand recognition. Diamond Head in Hawaii Question: What is the name of this famous sight in the photo? Maya: The famous sight in the photo is the Seven Islands of Hawaii. LLaVA-7B: The famous sight in the photo is the Diamond Head, which is volcanic crater located in Hawaii. GPT4: The famous sight in the photo is Diamond Head. Figure 12. Comparison of model responses to visual questions on identifying famous landmark. Highways at Night Question: Describe this photo in detail. Maya: The image depicts large, busy highway at night, with heavy traffic in both directions. There are numerous cars and trucks on the road, occupying almost all available lanes. The traffic appears to be moving slowly, with vehicles occupying the entire length of the highway. The scene is illuminated by the bright lights of the vehicles, as well as by the streetlights, creating dynamic atmosphere. The highway extends into the distance, with no end in sight, highlighting the scale and congestion of this bustling transportation network. LLaVA-7B: The image depicts busy highway filled with heavy traffic, including numerous cars and trucks. The highway is bustling with activity, as vehicles are seen in various positions, some moving and others waiting in line. The traffic is dense, with cars and trucks occupying almost every lane, creating sense of congestion. The highway is surrounded by trees, adding touch of nature to the urban landscape. The scene captures the essence of typical day in large city, with people commuting and traveling on the busy roads. GPT4: This is top view photograph of highway at night. The highway is divided into four sections. On the left side, there are three lanes of traffic approaching the camera and four lanes of traffic moving away from the camera. On the right side, there are four lanes of traffic approaching the camera and three lanes of traffic moving away from the camera. Most cars in the four-lane section moving away from the camera have their brake lights on, indicating higher traffic density in the four-lane sections. Meanwhile, traffic appears to be lighter on the two three-lane sections. The four-lane highway is elevated compared to the three-lane highway, creating visually distinct separation between the two sections. The lights alongside the highway are illuminated, providing clear view of the road and cars. On the right side of the highway, there are trees, adding touch of nature to the scene. Figure 13. Comparison of model responses to visual questions on night highway traffic. Asian Food Question: Describe this photo in detail. Maya: The image features dining table with several bowls of food, including large bowl of noodles and few smaller bowls containing variety of dishes, possibly Asian cuisine. The noodles are served in bowl with vegetables, making it delicious and healthy meal. There are also couple of cups placed on the table, one near the top left corner and the other closer to the center. spoon can be seen resting in one of the bowls, ready to be used. The table is set up with variety of food options, making it an inviting scene for those who enjoy sharing and enjoying meal together. LLaVA-7B: The image features dining table with variety of food items, including large bowl of soup, plate of noodles, and bowl of vegetables. The soup is placed in the center of the table, while the noodles and vegetables are positioned around it. There are several bowls and cups scattered across the table, with some placed near the edges and others closer to the center. spoon is also visible on the table, likely used for eating the soup. The table setting creates an inviting atmosphere for meal. GPT4: This is close-up photo of meal at ICHIRAN. In the center, there is chashu ramen bowl with spoon. The ramen is seasoned with chili sauce and chopped scallions, and served with two pieces of chashu. To the right of the bowl, chopsticks are placed, still in their paper wrap, unopened. The ramen is accompanied by nori on the left side. On top, there are accompanying sides. From left to right, these include bowl of orange spice (possibly garlic sauce), plate of smoke-flavored stewed pork garnished with chopped scallions, and cup of matcha green tea. Figure 14. Comparison of model responses to visual questions on food details. Figure 15. Maya output for prompt (with image from Figure 9): Describe this photo in detail in {language}."
        }
    ],
    "affiliations": [
        "Articul8 AI",
        "Bangladesh University of Engineering and Technology",
        "Capital One",
        "Cisco Meraki",
        "Cohere For AI Community",
        "Georgia Institute of Technology",
        "IIT Bombay",
        "IIT Dhanbad",
        "Imperial College London",
        "Indiana University Bloomington",
        "MBZUAI",
        "TU Darmstadt",
        "The Alan Turing Institute",
        "University of Pennsylvania"
    ]
}