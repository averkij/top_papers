{
    "paper_title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
    "authors": [
        "Shwai He",
        "Chaorui Deng",
        "Ang Li",
        "Shen Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \\href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}."
        },
        {
            "title": "Start",
            "content": "Shwai He1,2 ,* Chaorui Deng1 Ang Li2 Shen Yan1 1ByteDance Seed 2University of Maryland, College Park shwai.he@bytedance.com, sheny@bytedance.com 5 2 0 2 2 ] . [ 1 1 5 3 2 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct systematic analysis of unified multimodal model components using training-free pruning as probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixtureof-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that fully trainable adaptation delivers additional gains. As result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at this link. 1. Introduction Large-scale multimodal models have recently achieved remarkable progress in both multimodal understanding [7, 24, 28, 31, 32] and generation [37, 39, 41]. Traditionally, these two tasks were studied in isolation, leading to distinct research trajectories and model families: understand- *Work done while the first author was an intern at ByteDance Seed. ing models for visionlanguage reasoning with textual outputs, and generative models designed for image synthesis. While effective for task-specific purposes, this separation stands in contrast to the broader pursuit [2, 49], which envisions unified model capable of both understanding and generating across modalities. Recent research has increasingly explored unified multimodal models [1, 4, 9, 25, 50], which integrate heterogeneous components such as vision encoders [10], language backbones [11, 54], and image decoders [1, 37]. These unified architectures can seamlessly support both understanding and generation within single system, marking promising step toward general-purpose multimodal intelligence. However, this unification comes at substantial cost in efficiency, as many tasks or input samples do not require the full knowledge or capacity of the unified model, but rather rely on much slimmer or more sparsely activated In addition, we identify three sub-architecture [13, 53]. complementary observations that shed light on the slimness and sparsity manifested in unified multimodal models: (1) Component-wise redundancy: the understanding and generation components follow distinct computation patterns and serve different functional roles, leading to different levels of redundancy. taskspecific inputs tend to activate only limited subset of parameters during inference, leaving much of the shared ca- (3) Input variability: even pacity underutilized [23, 43]. within the same task, different input queries can activate different portions of the model, further complicating efficient compute allocation [29, 33]. (2) Task-specific activation: Motivated by these factors, We systematically analyze the components of unified multimodal models and examine how they allocate and utilize parameters across tasks and inputs. Specifically, we first adopt training-free pruning as probing methodology [16, 18, 34, 35, 52], as it enables us to infer structural importance without retraining by removing structures and observing the resulting performance change. We begin by analyzing the understanding components, which serve as shared modules that process inputs from multiple modalities and produce language representations or embeddings that guide the generation components. Our results demonstrate that the understanding components are highly compressible in multimodal generation tasks but less so in understanding tasks. Furthermore, we observe clear task-specific activation patterns: understanding and generation tasks predominantly activate different model partitions, underscoring the necessity of dynamic activation for different testing tasks. In contrast to understanding components, generation components (e.g., image generators) are far less tolerant to static compression, where we find the activation patterns vary significantly across samples and timesteps. Consequently, applying static pruning leads to drastic drop in the generated image quality due to the inability to accommodate such dynamic activation shifts. To address this issue, we propose Mixture-of-Experts (MoE) Adaptation, where MLP neurons are partitioned into experts, allowing the model to selectively activate subsets of neurons to ensure efficient dynamic activation. To align the model with sparsely routed inference, we first introduce expert-frozen tuning, which freezes all experts and updates only the remaining parameters. Building on this initialization, we further perform fully end-to-end MoE training, yielding additional improvements, where the adapted BAGEL model [9] matches the performance of the full model while activating only about half of neurons. Our contributions are threefold: Our study demonstrates that the understanding component in unified multimodal models has high structural redundancy and can be compressed even more aggressively for generation tasks. We introduce training-free neuron partition analysis that uncovers task-specific activation patterns and the corresponding redundancy within the understanding component, offering insights into task-aware compression. To cope with the high compression sensitivity of the generation component, we design tailored MoE Adaptation that preserves generation quality while activating only about half of its parameters. 2. Related Works Multimodal Models for Understanding and Generation Early multimodal research typically treated understanding and generation as separate tasks, leading to two distinct architectural paradigms [7, 24, 57]. On the one hand, multimodal large language models (MLLMs) extend language models to handle input tokens from multiple modalities, such as LLaVA [28], which augments the LLaMA backbone [47] with visual tokens for visionlanguage understanding. On the other hand, multimodal generative models employ dedicated generators that can synthesize high-fidelity visual outputs [3, 40, 42]. For instance, recent diffusion-based approaches, including Diffusion Transformers (DiT) [37], further show that iterative denoising and latent refinement can be effectively leveraged to convert noise into images guided by natural language. More recently, several works aim to unify understanding and generation within single framework [4, 50, 56]. BAGEL [9] employs mixture-of-transformers design [25] to separate understanding and generation modules, while Ming-Omni [1] uses mixture-of-experts backbone with multimodal understanding and modality-specific decoders for generation. Although these unified models demonstrate strong versatility, their increased architectural complexity poses new challenges for efficiency, which remains underexplored. Model Compression toward Parameter Efficiency Despite the remarkable advances of scaling large language models, the continual growth in their size has introduced substantial redundancy and raised critical challenges for scalability. Network pruning [5, 30] has emerged as an effective technique to identify and remove redundant structures in neural networks, thereby improving parameter efficiency and reducing inference cost. For instance, [16] demonstrated that many deep layers in large language models are relatively unimportant, and that comparable performance can still be maintained after removing these redundant layers. Within single layers, width compression provides complementary approach to reduce parameter count by shrinking the intermediate or hidden dimensions of model components [34, 52]. While the uni-modal compression techniques can be transferred to Vision-Language models that take multi-modal inputs and output the language responses via language models [20, 26, 45], it is unclear whether such methods still work in unified models. We take the prior efforts to systematically explore and exploit redundancy in multimodal models, where heterogeneous components play distinct roles. This perspective enables us to design compression strategies better aligned with the unified nature of multimodal understanding and generation. 3. Preliminaries: Unified Multimodal Models Unified multimodal models are model architectures that integrate both multimodal understanding and generation within single framework. Let denote the multimodal input, and Understanding denote the corresponding output. The model predicts textual outputs in an auto-regressive manner: p(yund x; θund) = (cid:89) t=1 p(yt y<t, x; θund) , (1) where θund denotes the parameters of the understanding component, and yund denotes the sequence of output tokens, i.e., (y1, y2, . . . , yT ). input xinst (e.g., For generation tasks, Generation the unified model leverages the understanding component to process an instructional text prompt and reference images), producing conditional features fund(xinst; θund). The generative component, parameterized by θgen, synthesizes the output ygen, conditioned on the representation fund(xinst; θund) from the understanding component, together with an additional generative input (e.g., random noise in diffusion models): ygen p(ygen fund(xinst; θund), z; θgen) , (2) In unified multimodal models, the understanding component processes inputs from all modalities, while the generation component explicitly operates on non-text outputs such as image or audio synthesis. Due to their heterogeneous objectives and computational pathways, the understanding and generation components could exhibit different characteristics under compression. 4. Methodology In this section, we first introduce training-free pruning techniques to probe architectural redundancy, and then propose training-aware MoE adaptation that leverages dynamic sparsity to maintain performance under sparse activation. The proposed methods are illustrated in Figure 1. 4.1. Training-free Compression Strategies Depth Pruning via Layer Dropping Transformer-based large language models consist of multiple layers, and scaling their depth effectively improves performance. Nevertheless, excessive depth also introduces structral redundancy. Following [16, 18, 35], we measure the redundancy of single layer via: Sl = CosineSim(xl, yl), (3) where xl and yl correspond to the input and output of the l-th layer, respectively. The similarity provides measure of redundancy, with higher values implying that the layer contributes only marginal transformation. The metric has been shown to perform effectively in unimodal LLMs such as Mistral [22] and LLaMA [11, 48], and we next extend this metric to unified models. Width Reduction via Neuron Partition In addition to depth, scaling the width, particularly within MLP layers, has become prevalent strategy for enhancing model capability. In Transformer MLP layers, the input is expanded from dimension to dm, activating dm hidden neurons before being projected back to d. While this expansion increases model capacity, it also introduces redundancy among neurons. To mitigate this redundancy, we propose neuron partition that separates the hidden neurons into important and less important subsets, preserving the former while pruning the latter to achieve width reduction without significantly affecting performance. To measure neuron importance, we take inspiration from Wanda [44], which combines weight magnitudes and activation statistics as pruning criteria, and extend this idea from unstructured weight-level pruning to structured neuron-level metric. Given an input Rsd, in GateUp-Down MLP, the hidden activations Rsdm and output Rsd can be written as: )(cid:1) (xW ), = (cid:0)SiLU(xW = hW , (4) where Wg, Wu Rmdd are the up-projection matrix and gate-projection-matrix, Rnmd is the gated activation, Wd Rdmd is the down-projection matrix. The hidden activations consist of dm neurons, and the contribution of the i-th neuron to the final output is: yi = hiW d,i, (5) with Wd,i being the i-th column vector of Wd. If the ith neuron is pruned, the induced output error norm can be approximated by: y2 hiW d,i2. (6) Given all inputs from the calibration dataset D, the expected accumulated error of each neuron is used as its importance metric: si = ExD (cid:2)hi d,i (cid:3), (7) where hi measures the average activation magnitude of the i-th neuron, and Wd,i2 quantifies its amplification effect on the output. Therefore, neurons with larger scores play more critical roles, while those with smaller scores tend to be removed. Unlike unstructured pruning that zeroes individual weights, our approach enforces structured pruning by removing entire neurons, Concretely, this corresponds to removing column from Wd and row from both Wu and Wg, thereby ensuring hardware-friendly efficiency. Unified models unify diverse tasks within single architecture, and different tasks naturally activate different subsets of neurons. Figure 2 illustrates the distinct neuron partitions within the understanding component. We first identify the top 50% neurons according to their importance scores for understanding and for generation, respectively. Based on these two sets, we then partition neurons into three disjoint groups: those that lie only in the understanding top50% set, those that lie only in the generation top-50% set, and those that are shared by both. The relatively low overlap ratio suggests that different tasks activate distinct neuron subsets within the understanding component. Therefore, we propose to align the calibration samples with the target tasks before applying the neuron-level importance metric, ensuring more accurate identification of principal neurons. Figure 1. Overview of the proposed slimnessand sparsity-oriented techniques for efficient unified multimodal models. We introduce two complementary strategies: (1) Training-free compression, exemplified by neuron partitioning, which groups hidden neurons into subsets and prunes those less important for the target task; and (2) Training-aware MoE Adaptation, which dynamically activates neurons through Mixture-of-Experts design, where neurons are organized into shared experts (solid lines) and routed experts (dotted lines) controlled by router. Figure 2. Statistical analysis of high-importance neurons, quantifying those predominantly activated in understanding tasks, in generation tasks, and jointly across both. 4.2. Training-Aware MoE Adaptation Figure 3 visualizes the active neurons (those consistently ranked within the top 50% by activation scores) and inactive neurons (those never entering the top 50%) across layers of the generation component over multiple prompts and time steps. We observe that only small subset of neurons remain consistently active throughout inference, while most exhibit sample-dependent activation patterns, indicating dynamic specialization across inputs. This observation reveals dynamic activation phenomenon, where the subset of activated parameters depends on the input, aligning with the intuition behind the Mixture-of-Experts (MoE) architecture. To leverage this property, we incorporate an MoE mechanism into unified models through two key stages: Expert Partition and MoE Adaptation. Expert Partition To separate universal and samplespecific capacity [6, 17], we partition MLP neurons into shared and routed experts using cumulative importance across calibration samples. For each neuron, we compute its cumulative importance score using Equation 7. The neurons with the higher scores than the threshold are selected as shared experts Es, preserving features that consistently benefit multiple samples. The remaining neurons, whose relateive importance is more sample-dependent, are evenly allocated to routed experts E(1) by ranked imr portance. Specifically, neurons are sorted in descending or- , . . . , E(n) Figure 3. Visualization of the proportion of inactive and saturated neurons across layers. Active neurons are defined as those consistently ranked within the top 50% by activation scores, while inactive neurons never enter the top 50%. der of importance and sequentially assigned to experts in to E(n) alternating forward and reverse order (i.e., E(1) , then E(n) ), ensuring balanced total importance across experts. back to E(1) MoE Adaptation After expert partition, we insert router per layer to dynamically select routed experts for each input. In this case, the output of an MoE layer is formulated as follows: MoE(x) = fS (x) + (cid:88) jTop-k(G) Gj fRj (x), (8) where denotes the gating function, and fS and fR represent the transformations of the shared and routed experts, respectively. The original MLP layer can be regarded as special case of Equation 8, where all experts are selected and their gating scores are uniformly set to 1. To enable smooth transition from dense to sparse activation, we relax the constraint that the gating scores sum to 1 and reparameterize them as (1 + Router(x)), where the router networks gate is initialized to zero. To adapt the model to the converted sparse activation pattern, we perform Expert-Frozen Tuning as cold-start phase of MoE Adaptation, where experts remain fixed while the router and other parameters are trainable. This stage allows the model to leverage the pretrained knowledge of existing experts while establishing preliminary routing policy. Afterward, we release the freezing constraint to fully enable MoE Adaptation. 5. Experiments 5.1. Experimental Setup Models We focus on several mainstream open-source unified models, including BAGEL [9], Ming-Omni [1], and Qwen-Image [51]. All three adopt Qwen-Instruct [54] as the backbone for multimodal understanding. For the understanding component, BAGEL and Qwen-Image rely on VLM derived from Qwen-Instruct [54], whereas MingOmni employs an MoE-based backbone [46]. The generation components further diverge across models: BAGEL employs Mixture-of-Transformers (MoT) [25] design and reuses the Qwen-Instruct backbone for generation; QwenImage incorporates an MMDiT-based generator [12] and Ming-Omni adopts multi-scale DiT block architecture. Table 1 presents detailed comparison of these models. Table 1. Summary of evaluated unified models. Model Und. Component Und. Param. Gen. Component Gen. Param. Qwen-Image Ming-Omni BAGEL VLM MoE-VLM VLM 7.62B 17.12B 7.62B MMDiT MMDiT LLM 20.42B 2.51B 7.62B We systematically vary the number of experts per MoE layer (16, 32, 64). Following the design choices in [6, 8], we include shared experts that constitute one-sixteenth of the total number of experts. The overall activation ratio is set to 50% per layer. We exclude the first and last layers from MoE conversion, as they are essential for preserving input encoding and output generation quality [6]. Datasets For the calibration datasets used in training-free compression, we use small number of examples drawn directly from the target task, which is sufficient for trainingfree compression [14, 18, 27, 35]. Note that these samples are used solely to compute activation scores, so no ground-truth annotations are required and no label leakage occurs. For MoE adaptation, we additionally incorporate high-quality imagetext pairs, complemented by small amount of synthetic data generated by existing textto-image models. 5.2. Redundancy of Und. Component Depth Reduction works in Generation Tasks but Fails in Understanding Since the understanding component is not directly connected to the generation output, we first analyze its indirect influence on generation performance. Specifically, we remove transformer blocks, MLP layers, and attention layers, respectively. As shown in Figure 4, removing 50% of layers in the understanding component proves effective for BAGEL and Qwen-Image, but is less effective for Ming-Omni. We attribute this difference to architectural design: Ming-Omnis generation component is relatively smaller and thus depends more heavily on precise features encoded by the understanding component. However, depth pruning substantially deteriorates the models understanding capability. For instance, removing half of the MLP layers causes performance on MME [15] to drop from 1684.8 to 304.5 in perception and from 696.7 to 127.1 in cognition. These results suggest that depth reduction fails to preserve the performance of unified multimodal models on understanding tasks. Language-responseoriented understanding tasks rely on autoregressive decoding, which is inherently an error accumulation process, where the deviation of previous timesteps can propagate through subsequent decoding steps and ultimately cause the model to collapse within just few steps. Figure 4. Comparison of the overall performance of depth reduction on the GenEval, where the compression ratio is set as 50%. Table 2. Performance of neuron partitioning on understanding tasks at compression ratios of 25% and 50%. Model Sparsity MME-P MME-C MMMU MMBench MMVP Ming-Omni BAGEL 25% 50% 25% 50% 1584.3 1578.5 1269.0 1684.8 1558.1 1392. 670.4 560.4 317.9 696.7 681.7 528.9 66.7 56.7 51.7 65.0 60.1 56.7 86.7 81.2 81.0 88.1 85.7 79. 54.6 51.3 46.0 69.6 68.7 56.0 Neuron Partition on Understanding Components: Effective in Both Understanding and Generation We next evaluate the effectiveness of neuron partition on understanding components. Specifically, we compress the MLP layers to the target ratios using small set of calibration samples. As shown in Table 3, Ming-Omni and Qwen-Image largely maintain their performance even under aggressive compression ratios (i.e., 50% and 75%), whereas BAGEL exhibits greater loss in capability, likely due to its mixture-oftransformers architecture [25], in which components interact more frequently through cross-attention at every layer. Notably, as shown in Table 2, neuron partition consistently outperforms layer dropping across understanding Table 3. Performance on GenEval when applying training-free Neuron Partition to the understanding component. Since only the understanding component is compressed, the reported parameter counts correspond to this part rather than the full model size. Model Sparsity Params. Single Obj. Two Obj. Counting Colors Position Color Attri. Overall BAGEL Qwen-Image Ming-Omni 0% 25% 50% 0% 50% 70% 0% 50% 70% 7.62B 6.19B 4.76B 7.62B 4.76B 3.62B 17.12B 8.55B 5.61B 0.99 0.98 0.94 0.99 0.99 0.97 0.97 0.97 0.96 0.94 0.91 0.63 0.98 0.94 0.88 0.95 0.92 0. 0.81 0.81 0.62 0.91 0.94 0.85 0.67 0.66 0.58 0.95 0.94 0.77 0.94 0.93 0.91 0.92 0.89 0. 0.72 0.70 0.47 0.80 0.76 0.60 0.71 0.61 0.49 0.77 0.70 0.34 0.89 0.87 0.71 0.71 0.70 0. 0.86 0.84 0.63 0.92 0.90 0.82 0.82 0.79 0."
        },
        {
            "title": "Baseline",
            "content": "Comp. w/Gen. Comp. w/Und."
        },
        {
            "title": "Baseline",
            "content": "Comp. w/Gen. Comp. w/Und. (a) realistic broccoli sits upright on plain surface. (b) pair of scissors lies on flat surface. (c) dolphins swim through abandoned subway cars. (d) fruit bowl consisting of fruits and miniature planets. Figure 5. Impact of calibration data selection on neuron partition within the understanding component for generation tasks. Each triplet presents outputs from the unmodified model (left), the model after neuron partition using image-generation calibration (middle), and using understanding calibration (right). tasks. Although certain layers exhibit substantial redundancy, aggressively removing them would also eliminate the small subset of weights that are critical to task performance [55]. In contrast, neuron partition selectively preserves important neurons within each layer that are most relevant to the target task, thereby achieving more finegrained and reliable compression. Nevertheless, since the understanding component directly governs textual outputs, its compression ratio should remain more conservative in understanding tasks than in generation tasks. Calibration Data Affects the Activated Parameters Neuron partition leverages calibration samples to estimate neuron importance and prunes those that are less critical based on their activation behavior. Figure 2 shows that different neurons are activated for different tasks, suggesting that the choice of calibration data can lead to varying retained weights and thereby affect downstream performance. To assess how different calibration datasets affect parameter retention and downstream performance, we conduct an ablation study using samples from understanding tasks (MME) and generation tasks (GenEval). We find the alignment between calibration data and target tasks contributes to the performance. For instance, Using calibration samples from the understanding and generation tasks yields MMBench scores of 79.2 and 74.8, respectively. The generation results in Figure 5 further highlight this trend. When calibrated with image generation samples, the outputs remain faithful to the prompts, producing broccoli, scissors, dolphins, and fruit bowls with correct structures. In contrast, calibration with understanding samples introduces distortions and mismatches. This demonstrates Baseline (uncompressed) Compressed (50% width reduction ) Figure 6. Qualitative comparison between the baseline (uncompressed) model and the model with 50% width reduction in the generation component. The baseline model (left) is evaluated without compression, while the compressed model (right) reduces the generator width by 50%. Results are shown for the prompt The word START. Compression leads to noticeable degradation in fine details and semantic consistency. that task-aligned calibration data yields better performance, while mismatched data degrades generation quality. The effect is particularly critical for unified models, where both input and output types vary in different combination of modalities. 5.3. Dilemma of Gen. Component Compression We next investigate how compression influences generation quality by applying neuron partition to the generation components. While neuron partition demonstrates strong performance in compressing the understanding components, applying similar compression to the generation components presents clear dilemma. As illustrated in Figure 6, aggressive compression, such as 50% width reduction, substantially degrades the fidelity and coherence of generated outputs. Compressed models often produce distorted structures and unrealistic textures, deviating from the intended semantics. This highlights the contrasting compressibility between understanding and generation components: whereas understanding tasks remain robust under compression, generation components are highly sensitive, limiting the extent of feasible compression. 5.4. MoE Adaptation for Gen. Component Sparsity Static compression inherently conflicts with the dynamic activation patterns required across tasks and samples, leading to notable degradation in generation components. To mitigate this issue, we apply MoE Adaptation, enabling dynamic activation to restore representational capacity and overall performance. Warmup via Expert-Frozen Tuning MoE Adaptation begins with Expert-Frozen Tuning, which serves as cold start phase to train the model to effectively leverage the partitioned experts. This strategy mitigates catastrophic forgetting and encourages the model to learn effective expert selection while preserving pretrained knowledge [19, 21, 38]. Specifically, we examine different numbers of experts by comparing three configurations (16, 32, and 64), as shown in Figure 7 depicting the training loss curves. With few Figure 7. Training curves of expert-frozen tuning with only few training steps, where MoE layers are configured with different numbers of experts (i.e., 16, 32 and 64). steps of expert-frozen tuning, we observe substantial decline in loss values, indicating that the model effectively adapt to and exploit subset of experts to recover performance. Meanwhile, finer-grained expert partitioning enables more flexible activation combinations, leading to substantially lower training loss. For instance, the overall GenEval score improves from 0.58 to 0.78, reflecting more coherent and visually faithful outputs. As illustrated in Figure 8, before tuning, the model generates noisy, low-detail images that fail to capture fine-grained semantics. Expert-Frozen Tuning not only enhances image fidelity but also strengthens the alignment between the generated content and the given instructions. This demonstrates that certain subsets of parameters within the generation components, though difficult to compress, still retain the potential to produce high-quality images. Effectiveness of MoE Adaptation After few steps of Expert-Frozen Tuning as cold start, we lift the restriction on frozen expert parameters to further improve performance. Beyond applying MoE adaptation exclusively to the generation component, we also explore extending it to the understanding component to reduce the number of activated parameters while maintaining task effectiveness. To preserve fidelity in understanding tasks, the experts in the understanding component are kept frozen, remaining fully activated for understanding and only sparsely activated for generation, since generation tasks are more tolerant to sparTable 4. Performance comparison across stages of MoE Adaptation: Expert Partition (no training), Expert-Frozen Tuning, and full MoE Adaptation. We evaluate Expert Partition (without training), Expert-Frozen Tuning, and full MoE Adaptation across two configurations of adapted components (Gen. and Und. & Gen.). For reference, results from the dense model pruned with neuron partition and subsequent fine-tuning under an equivalent budget of activated parameters are also reported. Activated Params. denote the number of activated parameters in the understanding and generation components, shown in the format Und. Param. & Gen. Param.. Method Baseline Expert Partition Dense Finetuning Expert-frozen Tuning MoE Adaptation Expert Partition Dense Finetuning Expert-frozen Tuning MoE Adaptation Adapt. Comp. Activated Params. Single Obj. Two Obj. Counting Colors Position Color Attri. Overall N/A 7.62B + 7.62B Gen. 7.42B + 4.96B Und. & Gen. 4.96B + 4.96B 0.99 0.90 0.97 0.99 0.99 0.69 0.97 0.94 0.99 0. 0.70 0.88 0.94 0.95 0.18 0.89 0.63 0.96 0.81 0.49 0.75 0.62 0.85 0.23 0.76 0.62 0.78 0. 0.74 0.91 0.93 0.95 0.45 0.91 0.77 0.95 0.72 0.53 0.67 0.69 0.75 0.10 0.70 0.47 0.70 0. 0.34 0.71 0.54 0.79 0.05 0.64 0.34 0.72 0.86 0.62 0.82 0.78 0.88 0.28 0.81 0.63 0."
        },
        {
            "title": "Baseline",
            "content": "Zeroshot w/o SE Zeroshot w/ SE E.F. Tuning MoE Adapt. famous flower that symbolizes wealth in China. Traditional activity during Easter in Western countries. Old analog picture of parked car on side street, quiet night. lone astronaut paints swirling galaxies onto massive canvas in vast space station. Figure 8. Visual comparison across different stages of MoE Adaptation. Shown are outputs from the Baseline model (without modification), Zeroshot settings after expert partition with or without shared experts (Zeroshot w/o and Zeroshot w/ S), the model after Expert-Frozen Tuning (E.F. Tuning), and the model further optimized via MoE Adaptation (MoE Adapt.). Test prompts are sampled from WISE [36] and the 4o-Image Generator prompt set. sity in this component. In summary, we consider two configurations of MoE adaptation: (1) Gen.: Expert partitioning and adaptation are applied only to generation experts; and (2) Und. & Gen.: Both understanding and generation experts are partitioned, but only the generation experts are adapted while the understanding experts remain frozen."
        },
        {
            "title": "These additional training stages enable the experts to\nrefine their internal representations and develop stronger",
            "content": "specialization, improving both structural coherence and semantic consistency in generated outputs. As shown in Table 4 and Figure 8, the model achieves consistently higher generation quality after Expert-Frozen Tuning and further gains after full MoE Adaptation. Although generation components are more sensitive to training-free compression, sparsely activating fewer parameters still has the potential to preserve the original performance. 6. Conclusion This work investigates the slimness and sparsity of unified multimodal models, proposing training-free pruning and dynamic activation methods to enhance efficiency. Our investigation reveals that the understanding component exhibits strong compressibility across both understanding and generation tasks. In contrast, the generation component is far more sensitive to compression, where even moderate pruning can lead to catastrophic degradation in generation quality. Inspired by the dynamic activation patterns observed across different samples, we introduce Mixture-ofExperts (MoE) Adaptation that dynamically activates generation parameters, effectively restoring generation quality under sparse activation. By combining training-free analysis with training-aware adaptation, our study uncovers substantial optimization space for improving parameter efficiency in unified multimodal modeling. 7. Acknowledgment We sincerely thank Shuangye Li, Yuhong Yang, and Yi Lin for their technical discussion that contributed to the development of this work."
        },
        {
            "title": "References",
            "content": "[1] Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, and Zhengyu He. Ming-omni: unified multimodal model for perception and generation, 2025. [2] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming Yang, Kevin P. Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. ArXiv, abs/2301.00704, 2023. [4] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [5] Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [6] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-ofexperts language models, 2024. [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Instructblip: Towards general-purpose visionSteven Hoi. language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. [8] DeepSeek-AI et al. Deepseek-v3 technical report, 2025. [9] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025. [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [11] Abhimanyu Dubey et al. The llama 3 herd of models, 2024. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv: Learning, 2018. [14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. ArXiv, abs/2210.17323, 2022. [15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. [16] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Dan Roberts. The unreasonable ineffectiveness of the deeper layers. In The Thirteenth International Conference on Learning Representations, 2025. [17] Shwai He, Liang Ding, Daize Dong, Boan Liu, Fuqiang Yu, and Dacheng Tao. PAD-net: An efficient framework for dynamic networks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1435414366, Toronto, Canada, 2023. Association for Computational Linguistics. [18] Shwai He, Guoheng Sun, Zheyu Shen, and Ang Li. What matters in transformers? not all attention is needed, 2024. [19] Shwai He, Tao Ge, Guoheng Sun, Bowei Tian, Xiaoyang Wang, and Dong Yu. Router-tuning: simple and effective approach for enabling dynamic-depth in transformers, 2025. [20] Shwai He, Ang Li, and Tianlong Chen. Rethinking pruning for vision-language models: Strategies for effective sparsity. SIGMETRICS Perform. Eval. Rev., 53(2):914, 2025. [21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, 2019. [22] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. [23] Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 35773599, Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [25] Weixin Liang, LILI YU, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen tau Yih, Luke Zettlemoyer, and Xi Victoria Lin. Mixture-oftransformers: sparse and scalable architecture for multimodal foundation models. Transactions on Machine Learning Research, 2025. [26] Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, and Zhenan Sun. Mope-clip: Structured pruning for efficient vision-language models with modulewise pruning error metric. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2736027370, 2024. [27] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. GetMobile: Mobile Computing and Communications, 28:12 17, 2023. [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [29] James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, and Ben Athiwaratkun. Training-free activation sparsity in large language models, 2024. [30] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. In ICLR, 2019. [31] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world visionlanguage understanding, 2024. [32] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with In Thirty-seventh Conference on large language models. Neural Information Processing Systems, 2023. [33] Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Xiaojun Meng, Liqun Deng, Jiansheng Wei, Zhiyuan Liu, and Maosong Sun. Sparsing law: Towards large language models with greater activation sparsity. In Forty-second International Conference on Machine Learning, 2025. [34] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. In Advances in Neural Information Processing Systems, 2023. [35] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect, 2024. [36] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Chaoran Feng, Bin Zhu, and Li Yuan. Wise: world knowledge-informed arXiv semantic evaluation for text-to-image generation. preprint arXiv:2503.07265, 2025. [37] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. [38] Fuli Qiao and Mehrdad Mahdavi. Learn more, but bother less: parameter efficient continual learning. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. [39] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. In Proceedings of the Zero-shot text-to-image generation. 38th International Conference on Machine Learning, pages 88218831. PMLR, 2021. [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. [41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. [42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic textto-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. [43] Rishov Sarkar, Hanxue Liang, Zhiwen Fan, Zhangyang Wang, and Cong Hao. Edge-moe: Memory-efficient multiLiu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [55] Mengxia Yu, De Wang, Colorado Reed, and Alvin Wan. The super weight in large language models, 2025. [56] Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Unified multimodal understanding and generation models: arXiv preprint Advances, challenges, and opportunities. arXiv:2505.02567, 2025. [57] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. task vision transformer architecture with task-level sparsity via mixture-of-experts. 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), pages 0109, 2023. [44] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. [45] Yi-Lin Sung, Jaehong Yoon, and Mohit Bansal. ECoFLap: Efficient coarse-to-fine layer-wise pruning for visionlanguage models. In The Twelfth International Conference on Learning Representations, 2024. [46] Ling Team. Every flop counts: Scaling 300b mixtureof-experts ling llm without premium gpus. arXiv preprint arXiv:2503.05139, 2025. [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. [48] Hugo Touvron et al. Llama 2: Open foundation and finetuned chat models, 2023. [49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. Survey Certification. [50] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [51] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. [52] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. [53] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise child in large language model: Towards effective and generalizable fine-tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 95149528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. [54] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng A. Generation Component Compression Generation components exhibit substantially higher sensitivity to compression than understanding components. Beyond the neuron partition results, we further investigate depth reduction (Figure 9) and find that removing entire layers also causes catastrophic degradation in generated outputs. These results indicate that, for generation components, static compression along either width or depth alone struggles to preserve the performance of the original model. Figure 9. Depth reduction applied to MLP layers in the generation component. Figures are shown with decreasing numbers of removed layers: 14 (50%), 7 (25%), 4 (14%), and 0. (a) Depth reduction achieved by removing 7, 4, 2, or 0 layers. (b) Width reduction evaluated at ratios of 50%, 25%, 10%, and 0. Figure 10. Compression of generation components through pruning of attention layers and heads. On the other hand, compressing the attention layers leads to substantial degradation in both depth and width settings. As shown in Figure 10, applying more than 10% reduction results in noticeable performance drops. B. Attention Head Partition While our main focus is width reduction in MLP layers, since they account for most of the parameters, our compression metric can be seamlessly extended to attention by computing the importance scores of attention heads. Attention Head Parttion still works for understanding components as shown in Table 5. Table 5. Performance of neuron partition applied on attention heads at sparsity ratio of 50% per layer. Model Compressed Layers Single Obj. Two Obj. Counting Colors Position Color Attri. Overall BAGEL N/A 3-27 4-27 0.99 0.97 0.98 0.94 0.87 0.91 0.81 0.66 0. 0.95 0.88 0.89 0.72 0.33 0.41 0.77 0.31 0.40 0.86 0.67 0.72 Table 6. Performance of depth reduction on understanding tasks. Model Sparsity MME-P MME-C MMMU MMBench MMVP Ming-Omni BAGEL 50% 50% 1584.3 1197. 1684.8 304.5 670.4 308.2 696.7 127.1 66.7 51.7 65.0 16.7 86.7 81. 88.1 18.6 54.6 46.0 69.6 23.1 multimodal understanding performance, as shown in Table 6. Moreover, Figure 11 illustrates that the reduceddepth model fails to produce coherent answers, collapsing after only few generated tokens. We attribute this to error accumulation in autoregressive decoding: small deviations in early generated tokens compound over time, ultimately causing the entire response to collapse. D. Comparison with Compression Methods While neuron partition leverages reconstruction errors to estimate the importance of each neuron, another mainstream approach relies on gradient information to approximate the impact of neuron removal. In Table 7, we adopt gradientbased metrics, such as those used in LLM-Pruner [34], as references to provide more comprehensive comparison for our neuron partition method. Neuron Partition not only demonstrates superior performance but also eliminates the dependence on labeled data and explicit gradient computation, thereby enabling more efficient deployment of training-free sparsity methods. Table 7. Comparison between neuron partition with LLM-Pruner. Model Metrics Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Ming-Omni LLM-Pruner Neuron Partition 0.96 0.96 0.82 0.81 0.72 0.58 0.85 0.86 0.47 0. 0.55 0.56 0.70 0.71 On the other hand, neuron partition also achieve competive performance compared to quantization. Specifically, in Table 8, we apply AWQ [27] to quantize the understanding component of Qwen-Image, i.e., the Qwen-VL backbone. Notably, neuron partition achieves an overall score of 0.90 at 50% compression ratio, outperforming the 0.88 obtained by 4-bit quantization. This stands in contrast to traditional LLM compression, where pruning at similar ratios typically leads to noticeable performance degradation and remains significantly weaker than 4-bit quantization. C. Depth Reduction on Understanding Tasks E. Calibration Data for Neuron Partitioning While reducing depth in the understanding component has only limited impact on generation tasks, it severely harms"
        },
        {
            "title": "The choice of calibration data influences the measurement\nof activation scores and thus determines which neurons are",
            "content": "Question: Can someone explain whats funny about this meme? Neuron Partition: The humor in this meme comes from the exaggerated change in handwriting style as the exam progresses. At the beginning, the handwriting is clear and legible, indicating confident and well-prepared student. However, as the exam continues, the handwriting becomes increasingly difficult to read, suggesting that the student is becoming more anxious and less focused. Depth Reduction: The comedic resurgence biting meme portrays comedic portrayal portraying trope portraying idioms portraying idioms portraying idioms portraying idioms portraying idioms portraying idioms portraying exposes exposing exposing injuring injuring injuring injuring injuring injuring injuring injuring injuring injuring...... Meme: My Handwriting In Exams Figure 11. Comparison of neuron partition and depth reduction on toy example under 50% compression ratio. The model with fewer neurons per layer still answers the question reasonably well, whereas the model with reduced depth collapses and degenerates into repeating single word. Model Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Qwen-Image w/ 50% neuron partition w/ 4-bit quantization 0.99 0.99 0.99 0.98 0.94 0.97 0.91 0.94 0.93 0.94 0.93 0. 0.80 0.76 0.79 0.89 0.87 0.70 0.92 0.90 0.88 Table 8. Comparison between neuron partition and quantization, with AWQ [27] applied to quantize the understanding component. retained. To comprehensively assess this effect, we conduct an ablation study on MME for understanding and GenEval for generation. Specifically, we sample small number of instances from each dataset for calibration, compress the models using these calibration sets, and then evaluate them on MME and GenEval, as shown in Figures 12 and 13, respectively. Notably, calibration data that align with the target task consistently yield better performance in both understanding and generation. This suggests that unified models rely on distinct neuron partitions for different tasks. Moreover, our test-time few-shot compression, which directly uses few test samples for calibration, seamlessly adapts to downstream tasks and achieves competitive performance. Figure 12. Ablation on the choice of calibration datasets for understanding tasks. Figure 13. Ablation of calibration datasets on generation tasks."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "University of Maryland, College Park"
    ]
}