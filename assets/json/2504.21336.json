{
    "paper_title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation",
    "authors": [
        "Linshan Wu",
        "Yuxiang Nie",
        "Sunan He",
        "Jiaxin Zhuang",
        "Hao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis."
        },
        {
            "title": "Start",
            "content": "UniBiomed: Universal Foundation Model for Grounded Biomedical Image Interpretation Linshan Wu1, Yuxiang Nie1, Sunan He1, Jiaxin Zhuang1, Hao Chen1,2,3,4,5* 1Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China. 2Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China. 3Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China. 4State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China. 5Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China. *Corresponding author(s). E-mail(s): jhc@cse.ust.hk;"
        },
        {
            "title": "Abstract",
            "content": "Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible realworld deployment and failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware 1 5 2 0 2 0 3 ] . [ 1 6 3 3 1 2 . 4 0 5 2 : r diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis. Keywords: Biomedical Image Analysis, Foundation Model, Multi-modal Large Language Model, Universal Grounded Interpretation"
        },
        {
            "title": "1 Introduction",
            "content": "Multi-modal information plays an important role in biomedical image analysis [1, 2]. Visual information from biomedical imaging enables detailed anatomical and functional analysis from cell to organ levels [37], while the textual information from diagnostic findings provides fine-grained descriptions for interpreting imaging [810]. Although recent Multi-modal Large Language Models (MLLMs) [5, 8, 1113] have witnessed rapid advancements in interpreting biomedical images and generating text descriptions, these models are typically region-agnostic [1417] and fail to extract target regions (e.g., tiny lesions) corresponding to the generated text descriptions. This lack of spatial awareness will reduce diagnostic accuracy and hinder the feasibility of clinical applications. Biomedical image segmentation is practical solution for extracting structured visual information from biomedical images [3, 4, 1824], which enables the identification of regions of interest (ROI) across multiple scales, such as organs, lesions, tissues, and cells. Although recent segmentation foundation models [3, 4, 1820] have demonstrated remarkable performance in this task, they still lack the ability to interpret and generate clinical text descriptions (e.g., clinical reports) simultaneously. The inability to utilize holistic visual and textual biomedical information limits their further development in clinical applications. To this end, we highlight the importance of grounded interpretation in biomedical image analysis, i.e., generating text descriptions and segmenting the corresponding biomedical targets simultaneously. In this work, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed innovatively incorporates advanced MLLM [12, 25] and Segment Anything Model [26, 27] (SAM) for tackling diverse biomedical tasks, as shown in Figure 1. Specifically, the MLLM is capable of interpreting multi-modal biomedical images and generating clinical text descriptions, while the language embeddings produced by the MLLM will be leveraged to prompt the SAM model for segmenting the corresponding biomedical objects. In this way, UniBiomed can tackle wide range of biomedical tasks, including biomedical image segmentation, grounded disease recognition, region-aware diagnosis, vision question answering (VQA), and grounded report 2 generation. These advancements enable UniBiomed to provide fine-grained visual and textual information simultaneously for end-to-end biomedical image analysis. To develop UniBiomed, we curate large-scale dataset containing 27 million triplets of images, annotations, and text descriptions spanning 10 biomedical imaging modalities. The annotations include both segmentation masks and bounding boxes, providing detailed spatial localization information for region-aware diagnosis. The text descriptions are extracted from readily available clinical text accompanying public datasets, encompassing biomedical definitions, diagnostic findings, medical knowledge, and clinical reports. To the best of our knowledge, this is the largest and most comprehensive dataset for biomedical grounded interpretation. By integrating comprehensive and multi-granular visual and textual biomedical information for training, UniBiomed unleashes the untapped power of MLLM in biomedical image analysis and demonstrates unprecedented effectiveness in diverse biomedical tasks. We conduct large-scale validation on 84 internal and external datasets across 10 diverse biomedical imaging modalities. Extensive experiments demonstrate that UniBiomed surpasses previous segmentation foundation models [4, 1820] by substantial margin, e.g., surpassing BiomedParse [4] by an average of 10.25% in Dice scores across 60 internal and external segmentation datasets. Beyond segmentation, UniBiomed also achieves state-of-the-art performance in wide range of biomedical tasks, including disease recognition, VQA, ROI classification, region-aware report generation, and grounded report generation. More importantly, we further showcase that UniBiomed is more effective biomedical AI tool for optimizing clinical workflow efficiency. Specifically, prior approaches [4, 1820, 28] heavily rely on clinical experts to pre-diagnose images and manually design accurate textual or visual prompts, e.g., instruct the model that the input image contains tumors (text prompts) or provide tight bounding boxes to highlight the tumors (visual prompts). In contrast, UniBiomed eliminates these processes and enables automated and end-to-end grounded interpretation of biomedical images, significantly streamlining the analysis workflow. In summary, UniBiomed represents versatile and powerful foundation model that delivers superior performance across diverse biomedical tasks, demonstrating promising potential towards more accurate and efficient biomedical image analysis. 3 4 Fig. 1: Overview of the study. a. UniBiomed enables universal grounded interpretation across 10 different biomedical imaging modalities. CT, computed tomography; MRI, magnetic resonance imaging; OCT, optical coherence tomography; PET, positron emission tomography. b. We curate 27 million triplets of images, annotations, and text descriptions for training UniBiomed. Annotations include segmentation masks and bounding boxes. The text descriptions are processed from disease information, diagnostic findings, and clinical reports. c. The framework of UniBiomed. UniBiomed incorporates Multi-modal Large Language Models (MLLMs) [12, 25] with Segment Anything Model [26, 27] (SAM) to tackle text description and segmentation tasks jointly. MLLM contains both vision and language models to interpret visual and language information, ultimately generating clinical text descriptions. SAM includes vision encoder, prompt encoder, and mask decoder to address the segmentation task. Given user instructions with biomedical images, UniBiomed can segment target objects and generate grounded text descriptions simultaneously, enabling end-to-end analysis of multi-modal biomedical images. d. UniBiomed is designed to handle diverse biomedical tasks within universal model, including segmentation, disease recognition, region-aware diagnosis, vision question answering (VQA), and report generation."
        },
        {
            "title": "2.1 Overview of UniBiomed",
            "content": "Dataset. UniBiomed is designed to address the biomedical grounded interpretation tasks, necessitating training datasets to provide both spatial localization information and expert-annotated text descriptions. For the spatial localization information, we meticulously curate the readily available segmentation masks and bounding boxes from public biomedical datasets. Specifically, the segmentation masks are used for dense segmentation training, the bounding boxes are used for ROI classification and region-aware report generation training. To ensure high-quality textual annotations, we rigorously extract semantic labels, diagnostic findings, medical knowledge, and clinical reports from existing datasets. These descriptions capture fine-grained biomedical information across multiple granularities, including organs, lesions, tissues, and cells, enabling comprehensive interpretation of biomedical targets. critical step in our dataset pre-processing pipeline is to transform the textual descriptions into uniform Vision-Question-Answering (VQA) format [1113], aiming to facilitate universal grounded interpretation. For example, given CT image with liver tumors, we define the question as: Can you identify any abnormality within this CT image? Please respond with segmentation masks.. Then the corresponding answer is structured as: It is [SEG]. Liver tumor. For the ROI classification task without segmentation outputs, the answer will only contain the biomedical class without the [SEG] token [13, 29]. This uniform VQA format enables UniBiomed to jointly recognize abnormalities and segment biomedical targets within universal foundation model. More details are presented in Section 4.1. Through this process, we construct large-scale dataset consisting of 27 million image-text-annotation triplets across 10 modalities, as shown in Figure 1 (a) 5 and (b). Among 27 million image-text-annotation triplets, there is total number of 18 million images, where the 3D medical images (CT and MRI) are pre-processed as 2D slices following previous methods [4, 18]. To the best of our knowledge, this is the largest and most comprehensive dataset for biomedical grounded interpretation. For example, compared with the dataset used in the representative biomedical foundation model BiomedParse [4], our dataset is over 30 times larger in scale. Unleashing the power of this large-scale dataset, UniBiomed demonstrates state-of-the-art performance across diverse tasks, outperforming existing biomedical foundation models [4, 11, 14, 15, 1820] by large margin. The details of the used datasets are presented in Extended Data Tables A2 and A3. Method. For the first time, UniBiomed innovatively incorporates Multi-modal Large Language Model (MLLM) [12, 25] and the Segment Anything Model (SAM) [26, 27] for universal grounded interpretation of biomedical images, as shown in Figure 1 (c). Based on the state-of-the-art SA2VA [13] architecture, our approach combines the complementary strengths of MLLM and SAM: MLLM processes both visual and textual information to generate descriptive interpretations, while SAM performs precise image segmentation through its vision encoder, prompt encoder, and mask decoder modules. The framework operates as an end-to-end solution that, given user instructions and biomedical images, UniBiomed simultaneously: (1) segments target biomedical objects, and (2) generates grounded textual descriptions of the corresponding objects. The effectiveness of UniBiomed arises from the complementary integration of MLLM and SAM. First, UniBiomed adopts textual prompts generated by MLLM to guide the segmentation of SAM. This process effectively eliminates the manual efforts of crafting bounding boxes [18, 19] or detailed text descriptions [20, 28] as prompts for segmentation. Second, the segmentation training process of SAM can implicitly enhance the diagnostic accuracy of the MLLM. Since the segmentation of SAM relies on the language embeddings generated by MLLM, the back-propagation process of segmentation will propagate supervision signals from the segmentation masks back to the MLLM. In this way, the segmentation training process can further enhance the capabilities of MLLM. For implementation, we select InternVL2.5 [25] as our foundation MLLM due to its robust multi-modal understanding capabilities, and SAM2 [27] is adopted as our segmentation model for its improved segmentation performance. The details of the method are described in Section 4.1. Evaluation. To demonstrate the effectiveness of UniBiomed on diverse biomedical tasks, we conduct comprehensive evaluation on 84 internal and external datasets. (1) First, we conduct extensive comparisons with several foundation models [4, 1820] in biomedical image segmentation. (2) Second, we evaluate UniBiomed in challenging grounded VQA task, i.e., grounded disease recognition, which aims to simultaneously generate diagnostic descriptions and segment the corresponding targets in the biomedical images. (3) Then, we verify the effectiveness of UniBiomed in two region-aware diagnosis tasks: ROI classification and region-aware report generation. (4) Finally, we conduct extensive experiments on grounded report generation, which is designed to generate clinical reports of the given images and highlight the target regions. Specifically, for the ROI classification task, we extract bounding boxes from segmentation 6 masks to indicate the ROIs following previous methods [15, 30]. We assess the regionaware report generation task using the MedTrinity [17] dataset and verify the grounded report generation task on the RadGenome [31] dataset. The details of the datasets are presented in Extended Data Tables A2 and A3. More details of the validation are presented in Section 4.2."
        },
        {
            "title": "2.2 UniBiomed excels in universal biomedical image",
            "content": "segmentation We first compared UniBiomed with multiple biomedical segmentation foundation models, i.e., MedSAM [18], SegVol [19], SAT [20], and BiomedParse [4], as shown in Figure 2. Specifically, we evaluate our method on 46 internal and 14 external datasets across nine diverse biomedical imaging modalities. The details of the datasets are shown in Extended Data Tables A2. For segmentation, we adopt text prompts for referring segmentation as SAT [20] and BiomedParse [4]. For example, to segment liver tumor in CT, we simply provide text instruction Please segment liver tumor in the CT image for the model, which eliminates the necessity of offering bounding box prompts like MedSAM [18]. More details of the competing methods are presented in Section 4. As shown in Figure 2 (a) and (b), UniBiomed achieves superior performance compared with previous segmentation foundation models [3, 4, 1820]. Specifically, UniBiomed surpasses the best-competing method BiomedParse [4] by clear margin, i.e., 10.25% dice score improvements on 60 internal and external datasets across nine modalities. As presented in Figure 2 (d), UniBiomed outperforms BiomedParse by 9.13% and 13.95% in 46 internal and 14 external datasets, respectively. The two-sided paired t-test -values are < 1.12 106 and < 4.09 107, respectively. In addition, as described in Figure 2 (e), UniBiomed can achieve competitive performance compared with 46 nnUNet [3] models by one universal model. These findings robustly underscore UniBiomeds substantial advancement in universal biomedical image segmentation. The key to UniBiomeds breakthrough in biomedical image segmentation lies in its ability to unleash the power of MLLM and SAM in large-scale multi-modal biomedical dataset. Notably, unlike prior segmentation models [3, 4, 1820] that rely solely on segmentation datasets for training, UniBiomed effectively incorporates large-scale VQA and report generation datasets [17, 32] in universal training. This approach enables UniBiomed to develop more generalizable representations, leading to superior segmentation performance across diverse biomedical imaging modalities. More importantly, beyond segmentation, UniBiomed establishes more comprehensive interpretation capability for biomedical image analysis, unlocking novel opportunities in broader clinical applications. As illustrated in Figure 2 (f ), prior segmentation models [3, 4, 1820] are limited in segmentation tasks, which heavily hamper their clinical applications in biomedical image analysis. In contrast, UniBiomed extends segmentation to grounded interpretation with text generation, which is universal foundation model capable of handling diverse biomedical tasks, making it more adaptable for clinical applications. 8 Fig. 2: Comparison on biomedical image segmentation. a. Comparison of Dice scores between UniBiomed and the best-competing segmentation foundation model BiomedParse [4] across nine modalities. Significance levels at which UniBiomed outperforms BiomedParse [4], with two-sided paired t-test -value is ****P < 1104. The exact -value is 1.17 107 for 60 internal and external datasets. Notably, UniBiomed outperforms BiomedParse [4] by an average of 10.25% in dice scores on 60 internal and external datasets. b. Radar chart comparisons with several representative segmentation foundation models, MedSAM [18], SegVol [19], SAT [20], and BiomedParse [4]. Specifically, SAT [20] is only applicable to 3D modalities like CT and MRI. SegVol [19] is solely available for CT datasets. c. Qualitative segmentation results of UniBiomed. Specifically, the colors of segmentation masks for internal and external validations are in green and orange, respectively. d. Box plot comparisons between UniBiomed and BiomedParse [4]. Significance levels at which UniBiomed outperforms BiomedParse [4], with two-sided paired t-test -values are ****P < 1104 for both internal and external validations. The exact -values are 1.12 106 and 4.09 107 for 46 internal and 14 external datasets, respectively. e. Box plot comparisons between UniBiomed and nnUNet [3]. We train 46 nnUNet [3] models on 46 internal datasets and evaluate them on 14 external datasets. It can be seen that UniBiomed achieves competitive performance compared with 46 nnUNet [3] models by one universal model. f. Overall application comparisons with representative biomedical segmentation models [3, 4, 1820]. Referring segmentation represents using text instructions as prompts for segmentation, which eliminates the efforts of providing tight bounding boxes for the biomedical objects [18]. Notably, UniBiomed not only supports multi-modal biomedical image segmentation but also establishes remarkable capabilities in ROI classification, VQA, and report generation, where the previous segmentation models [3, 4, 1820] fail to establish."
        },
        {
            "title": "2.3 UniBiomed enables accurate grounded disease recognition",
            "content": "Visual Question Answering (VQA) is fundamental ability in MLLMs [12, 25], requiring the model to analyze an input image and answer predefined questions about its content. In medical MLLMs [8, 11, 28, 3235], disease recognition is one of the most important VQA tasks, which aims to identify the lesions, tumors, or abnormalities in the given images. However, existing medical MLLMs [8, 11, 28, 3235] are typically region-agnostic, which solely generate diagnostic descriptions but fail to localize relevant regions (e.g., tiny lesions) in the images. This lack of spatial awareness severely limits their clinical applications, as precise localization of abnormalities is essential for diagnosis and treatment planning. In this work, we introduce novel and challenging grounded VQA task, i.e., grounded disease recognition, which is designed to simultaneously generate diagnostic predictions and segment the corresponding targets. In this task, the model is asked to predict abnormality class (e.g., liver tumor or pancreas tumor) and segment the targets in the images. If there is no abnormality in the given images, the model is supposed to output No findings and produce all-zero segmentation masks. 9 10 Fig. 3: a. We introduce novel and challenging task, i.e., grounded disease recognition, and curate new dataset for training and validation. Specifically, this dataset contains 270K images with 15 types of abnormalities. The details of the dataset are presented in Extended Data Table A4. b. We compare UniBiomed with two stateof-the-art methods, LISA [29] and GLaMM [36], in segmentation dice scores and disease recognition accuracy. Notably, UniBiomed outperforms the best competing method LISA [29] by 3.86% in dice scores and 3.29% in accuracy. The two-sided paired t-test -values are 1.85 105 and 2.81 103, respectively. c. Radar chart comparisons across 15 types of abnormalities. d. Dice scores comparisons between UniBiomed and LISA [29] across 15 types of abnormalities. e. The qualitative visualization results of grounded disease recognition. Given template instructions, UniBiomed can recognize the abnormalities in the images and highlight the precise localizations. f. Comparisons with MedRegA [14] and MedPLIB [15] in ROI classification. Significance levels at which UniBiomed outperforms MedPLIB [15], with two-sided paired t-test -values are ****P < 1 104, ***P < 1 103, and ****P < 1 104 for CT, MRI, and pathology, respectively. g. Radar chart comparison with MedRegA [14] and MedPLIB [15] across ten diverse biomedical imaging modalities. h. UniBiomed surpasses MedPLIB [15] by 8.32% in ROI classification accuracy, two-sided paired t-test -value is 3.12 1014. i. Qualitative visualization results of ROI classification. Specifically, given bounding box prompt, UniBiomed can effectively predict the class of biomedical targets within ROIs. To this end, we first curate comprehensive dataset encompassing 15 distinct abnormality types for model training and validation, as shown in Figure 3 (a). Specifically, these 15 abnormality types contain liver tumor, pancreas tumor, kidney tumor, colon cancer, lung tumor, lung nodule, COVID-19 infection, fibrotic lung disease, brain tumor, breast lesion, colon polyp, pneumothorax, prostate cancer, skin lesion, and retinal lesion. The details of the datasets are shown in Extended Data Tables A2 and A4. Existing medical models lack the capability to tackle this task, which requires outputting segmentation masks and text descriptions simultaneously. Although BiomedParse [4] can conduct segmentation and employ meta-object classifier for classification, this classifier requires users to pre-diagnose the images and provide diagnostic findings as textual prompts, thus failing to perform the grounded disease recognition task introduced in this work. To this end, we re-implement two stateof-the-art approaches from the general domain, LISA [29] and GLaMM [36], on our curated datasets for fair comparisons. The comparison results are shown in Figure 3 (b-d). Specifically, UniBiomed outperforms the best competing method LISA [29] by 3.86% and 3.29% in segmentation dice scores and disease recognition accuracy, respectively. Consistent improvements across different types of abnormalities are observed in Figure 3 (d), which robustly validates the effectiveness of UniBiomed in this challenging VQA task. More importantly, the strong capability of UniBiomed in grounded disease recognition potentially leads to significant paradigm shift in the biomedical image analysis 11 workflow. Specifically, previous biomedical foundation models heavily rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts. For example, for the task of lung tumor recognition, one of the representative foundation models, BiomedParse [4], requires radiologists to identify lung tumors in each slice of 3D CT scan in advance, then input an instruction Please segment lung tumor in this CT image to prompt the model for lung tumor segmentation. In contrast, UniBiomed enables an end-to-end pipeline for recognizing abnormalities in images. As shown in Figure 3 (e), UniBiomed is free of pre-diagnosis by clinical experts, which simply adopts template instruction and automatically recognizes the abnormality class with segmentation predictions. These findings underscore the clinical practice of UniBiomeds grounded interpretation capabilities, which is non-trivial advancement compared to previous biomedical foundation models [4, 1820]."
        },
        {
            "title": "2.4 UniBiomed effectively improves region-aware diagnosis",
            "content": "To evaluate region-aware diagnosis, we first assess fundamental task, i.e., ROI classification, which aims to identify biomedical classes within user-defined ROIs. For fair comparisons, we compare UniBiomed with two state-of-the-art medical MLLMs, MedRegA [14] and MedPLIB [15], since both of them were trained on large-scale ROI classification datasets. The results are shown in Figure 3 (f-i). Specifically, UniBiomed surpasses the best competing method MedPLIB [15] by an average of 8.32% in ROI classification accuracy across ten diverse imaging modalities, with two-sided paired t-test -value = 3.12 1014. We further evaluate more challenging region-centric task, i.e., region-aware report generation, which aims to generate detailed reports within corresponding bounding boxes in the biomedical images. Our experiments are conducted on the MedTrinity [17] dataset, large-scale benchmark featuring region-centric reports across diverse biomedical imaging modalities. Specifically, MedTrinity is aggregated from 23 public VQA and report generation datasets across 10 biomedical imaging modalities, as described in Extended Data Table A3. We compare our approach with several state-of-the-art MLLMs, including InternVL2.5 [25], LLaVA-Med [11], MedRegA [14], and MedPLIB [15]. Among them, InternVL2.5 is the state-of-the-art MLLM in the general domain. LLaVA-Med, MedRegA, and MedPLIB are generalist MLLMs in the medical domain, while MedRegA and MedPLIB were trained on large-scale region-centric report generation datasets. We report the results of Bilingual Evaluation Understudy (BLEU) [37], Metric for Evaluation of Translation with Explicit ORdering (METEOR) [38], and Recall-Oriented Understudy for Gisting Evaluation (ROUGE L) [39]. The details of the comparison methods are described in Section 4.2. The details of the evaluation metrics are described in Section 4.3. 12 13 Fig. 4: a. Comparisons on region-aware report generation in the MedTrinity [17] dataset. We compare UniBiomed with multiple representative MLLMs, including InternVL2.5 [25], LLaVA-Med [11], MedRegA [14], and MedPLIB [15]. We report the results of BLEU [37], METEOR [38], and ROUGE [39]. b. Radar chart comparisons across different metrics in region-aware report generation. c. An example of region-aware report generation on prostate pathology images. The text in green indicates the correct contents. Reference denotes the ground truth of the image from the corresponding dataset [17]. d. Comparisons on grounded report generation in the RadGenome [31] dataset. We re-implement GLaMM [36] and LISA [29] on this dataset for comparison. We report dice scores results for segmentation evaluation, BLEU [37], METEOR [38], and ROUGE [39] results for report generation evaluation. e. Radar chart comparisons across different metrics in grounded report generation. f. An example of grounded report generation on abdomen CT images. The text in green indicates the correct contents. Reference denotes the ground truth of the image from the corresponding dataset [31]. The results are shown in Figure 4 (a) and (b). It can be seen that UniBiomed achieves superior performance compared with previous MLLMs [11, 14, 15, 25]. Specifically, UniBiomed achieves 52.4%, 30.4%, and 47.9% in BLEU 1, METEOR, and ROUGE L, respectively, surpassing the best-competing method MedPLIB [15] by 8.8%, 2.3%, and 15.8%. We further present an example of region-aware report generation on prostate pathology images, as shown in Figure 4 (c). It can be seen that UniBiomed not only accurately identifies the biomedical class (i.e., prostate cancer) within the target regions, but also generates detailed description to illustrate the observed features. More examples are presented in Extended Data Figure A2."
        },
        {
            "title": "2.5 UniBiomed enables end-to-end grounded report generation",
            "content": "Grounded report generation is challenging task that combines biomedical image segmentation with report generation, enabling end-to-end biomedical image analysis in unified framework. Unlike prior methods, which rely on separate segmentation models and standalone language models for report generation, UniBiomed is the first medical MLLM capable of performing both tasks within one universal model. This integration allows UniBiomed to leverage holistic biomedical knowledge more effectively, improving both segmentation and report quality. To evaluate its effectiveness, we conduct extensive comparisons on the RadGenome [31] dataset. Specifically, the RadGenome dataset is processed from the CT-RATE [32] dataset, which contains detailed CT reports with segmentation masks for training and validation. Since existing medical MLLMs [8, 11, 14, 15, 28, 3335] cannot tackle this task, we also re-implement GLaMM [36] and LISA [29] on this dataset [31] for comparisons. The results are shown in Figure 4 (d) and (e). We report the dice scores results for segmentation evaluation, BLEU [37], METEOR [38], and ROUGE [39] results for report generation evaluation. It can be observed that UniBiomed not only achieves better segmentation performance (53.8% dice scores) but also demonstrates superior report generation capabilities 14 Fig. 5: Workflow comparisons. The upper part presents the workflow of the representative biomedical segmentation foundation models, e.g., MedSAM [18] and BiomedParse [4]. The lower part shows the workflow of our introduced UniBiomed. We present an example of grounded report generation on CT images. (45.7%, 24.8%, and 51.6% in BLEU 1, METEOR, and ROUGE L, respectively). We further showcase an example of grounded report generation on abdomen CT images, as presented in Figure 4 (f ). Specifically, within an end-to-end process, UniBiomed can observe the sliding type hiatal hernia abnormality in the esophagus with precise segmentation predictions and detailed descriptions. While previous methods [3, 4, 8, 11, 14, 15, 1820] require multiple models and stages to establish this diagnosis process. More examples are presented in Extended Data Figure A3."
        },
        {
            "title": "2.6 UniBiomed optimizes biomedical image analysis workflow",
            "content": "In this section, we demonstrate that UniBiomed is more effective biomedical AI tool for enhancing clinical workflow efficiency compared to existing solutions. As shown in the upper part of Figure 5, previous biomedical segmentation foundation models such as MedSAM [18] and BiomedParse [4] follow cumbersome process. Given an input CT scan, these models require radiologists to first pre-diagnose each slice to locate target abnormalities, such as liver disease in the example. Once identified, additional manual inputs are necessary, e.g., BiomedParse [4] demands text description, while MedSAM [18] relies on tightly drawn bounding box prompt. More critically, these models are limited to segmentation tasks and fail to generate detailed diagnostic descriptions of the targets. These inefficiencies significantly hinder their clinical applications. In contrast, UniBiomed overcomes these limitations through novel integration of MLLM and SAM. Given an input CT scan, UniBiomed automatically detects abnormalities, generates precise segmentation masks, and provides detailed diagnostic descriptions. There are two significant advantages in this workflow: (1) First, this process is free of pre-diagnosis by radiologists and requires no manual 15 prompts, which eliminates the time-consuming process of slice-by-slice analysis and manual prompt engineering. (2) Moreover, unlike prior foundation models [4, 18 20], which are limited to segmentation outputs, UniBiomed is capable of combining visual analysis with diagnostic text generation. For example, UniBiomed not only segments targets but also delivers clinically relevant interpretations, such as identifying minimal decrease in liver parenchyma density compatible with adiposity as shown in the example. These advancements lead to significant paradigm shift in biomedical image analysis, which effectively optimizes the workflow efficiency and enables an end-to-end diagnosis workflow."
        },
        {
            "title": "3 Discussion",
            "content": "In this work, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed unifies the segmentation of biomedical targets with the text description generation of corresponding objects, enabling end-to-end analysis across ten diverse biomedical imaging modalities. Specifically, UniBiomed is based on novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM) [26, 27], which can effectively leverage multi-modal biomedical information for tackling diverse biomedical tasks. To develop UniBiomed, we curate large-scale dataset comprising 27 million triplets of images, annotations, and text descriptions spanning 10 biomedical imaging modalities, which is the largest and most comprehensive dataset for biomedical grounded interpretation. To evaluate the effectiveness of UniBiomed, we conduct large-scale validation on 84 internal and external datasets with comprehensive comparisons of previous methods [3, 4, 11, 14, 15, 1820, 25, 29, 36]. Extensive experiments demonstrate that UniBiomed achieves state-of-the-art performance on wide range of biomedical tasks, including segmentation, disease recognition, ROI classification, and report generation. We further showcase that UniBiomed represents novel paradigm shift in biomedical image analysis workflows, which will significantly improve diagnostic efficiency. These findings demonstrate the promising prospects of UniBiomed in clinical applications. Multi-modal information for biomedical image analysis has attracted increasing attention in recent advances [8, 11, 28, 3335]. Concretely, visual features extracted from biomedical images provide anatomical and functional information from cell to organ levels [37], while textual descriptions from clinical experts offer fine-grained information for interpreting biomedical images [810]. The prior challenge is how to effectively integrate holistic vision and language information to develop robust biomedical foundation model. To this end, for the first time, we underscore the importance of grounded interpretation in biomedical image analysis, i.e., segment the target biomedical objects and generate corresponding text descriptions simultaneously. This task enables models to extract important biomedical targets (e.g., lesions, tumors, cancer cells, or abnormal organs) and generate detailed descriptions (e.g., clinical reports) of corresponding objects for assisting clinical experts in diagnosis, which unlocks novel opportunities for end-to-end biomedical image analysis. 16 However, existing biomedical models [3, 4, 11, 14, 15, 1820, 28, 29, 33, 34, 36, 40] fail to tackle this challenging but critical task. Specifically, grounded interpretation requires models to perform segmentation and text generation simultaneously, while these models can only conduct these two tasks separately. Concretely, the biomedical segmentation foundation models [4, 1820, 40] can deliver segmentation masks while failing to establish text generation tasks such as VQA and report generation. Although recent medical MLLMs [11, 14, 15, 28, 3335] have demonstrated promising results in generating text descriptions for biomedical images, these models are not capable of segmenting the corresponding biomedical objects (e.g., tiny lesions) simultaneously, making it difficult for users to identify the important regions described in the reports. To illustrate the drawbacks of these methods more clearly, we present detailed comparison in Extended Data Table A1. Compared with previous methods, UniBiomed is the first foundation model to support multi-modal image interpretation, flexible user prompts, segmentation prediction, text description generation, region-aware diagnosis, pixel-level grounding, and end-to-end training in one universal model. This breakthrough paves promising path towards more accurate and efficient biomedical image analysis. One of the major bottlenecks is the availability of data. To train UniBiomed, the training datasets necessitate both spatial localization and corresponding text descriptions for the biomedical objects. Inspired by the recent advances of Visual Instruction Tuning in MLLMs [12, 13, 29], we construct uniform VQA format to facilitate universal grounded interpretation of biomedical images. In this way, we curate large-scale dataset comprising 27 million image-text-annotation triplets across ten biomedical imaging modalities for training and validation, which is the largest and most comprehensive dataset in this field. Our curated dataset will fortify the foundation of future research in the field of biomedical grounded interpretation. Moreover, we further showcase that UniBiomed is more practical biomedical AI tool for optimizing workflow efficiency. Prior segmentation foundation models [4, 18 20] heavily rely on clinical experts to pre-diagnose images and manually design accurate textual or visual prompts. Take the representative models, MedSAM [18] and BiomedParse [4] as examples. Both of these models require the users to identify the targets of the input images in advance. Then, visual or textual prompts should be provided, e.g., MedSAM [18] requires tightly drawn bounding boxes as visual prompts while BiomedParse [4] necessitates text instructions from pre-diagnostic findings as textual prompts. This process is tedious for the users and significantly hampers workflow efficiency. For example, for 3D medical images like CT and MRI, there are probably few hundred slices within scan, while these models [4, 1820] require users to pre-diagnose the scans slice-by-slice and provide accurate prompts. In addition, these models [4, 1820] still fail to generate text descriptions for interpreting the images, which significantly hinders their clinical applications. In contrast, UniBiomed provides automated and end-to-end grounded interpretation of biomedical images, with both precise segmentation predictions and detailed text descriptions, which significantly optimizes the workflow efficiency of biomedical image analysis. Although promising results have been demonstrated, there are still numerous areas for growth and improvement. Moving forward, we will extend the application 17 of UniBiomed to encompass more downstream biomedical tasks. In addition, while UniBiomed has achieved satisfactory performance on large-scale public datasets, further exploration of its application in clinical practice is necessary to substantiate the effectiveness of our method."
        },
        {
            "title": "4.1 UniBiomed Framework",
            "content": "The framework of UniBiomed is shown in Figure 1 (c), which is based on an integration of Multi-modal Large Language Model (MLLM) [12, 25] and Segment Anything Model (SAM) [27]. Concretely, MLLM is responsible for interpreting multi-modal images and generating text descriptions, while SAM is employed to segment the target biomedical objects based on the given text instructions. Typically, the MLLM consists of vision encoder and Large Language Model (LLM) [41]. Given an image and text instruction, the vision encoder encodes the image into visual tokens, while pre-trained tokenizer [42] will process the text into language tokens. These visual and language tokens are then fed into the LLM to generate textual outputs, which enables UniBiomed to tackle diverse text generation tasks. SAM [26, 27] has demonstrated astonishing segmentation abilities in many fields [4351]. Thus, SAM [26, 27] is further leveraged for tackling the segmentation task, which comprises vision encoder, prompt encoder, and mask decoder. The image features extracted by SAMs vision encoder are passed to the mask decoder. Unlike conventional SAM [26, 27], which relies on visual prompts (e.g., points or bounding boxes), the SAM in UniBiomed is based on the textual prompts generated by MLLM. Specifically, UniBiomed combines the hidden states of MLLM with the tokenized user instructions as the language embeddings and injects them into SAMs prompt encoder, enabling text-guided segmentation. This effectively bridges the MLLM and SAM for grounded interpretation [13, 29]. Notably, following the previous methods [13, 29], we adopt special token [SEG] to instruct the mask decoder to produce segmentation masks. While for the tasks that do not require segmentation predictions, e.g., ROI classification and region-aware report generation, this special token is discarded, and the model will not produce segmentation predictions. Notably, the success of UniBiomed stems from the complementary integration of MLLM and SAM. (1) First, unlike the original SAM [26, 27], which relies on visual prompts, UniBiomed employs language embeddings as textual prompts to guide SAMs mask decoder. This innovation eliminates the need for manually crafting precise bounding boxesa major bottleneck in segmenting biomedical images with dense, irregularly shaped objects [4]. (2) Second, segmentation training in UniBiomed implicitly enhances the diagnostic accuracy of the MLLM. Since segmentation predictions depend on the MLLMs textual prompts, the back-propagation process propagates supervision signals from the segmentation masks back to the MLLM, refining its ability to generate precise prompts. This synergistic integration not only improves segmentation performance but also strengthens the MLLMs diagnostic capabilities, creating mutually reinforcing loop between the two components. 18 Following the state-of-the-art architecture Sa2VA [13], we adopt InternVL2.5 [25] as the foundation MLLM and SAM2-hiera-large [27] as the segmentation model. To preserve the learned knowledge of the strong pre-trained MLLM, we leverage LoRA [52] to perform efficient fine-tuning, and completely freeze the vision encoder of MLLM. For SAM2 [27], following previous settings [13, 29], we freeze the vision encoder and fine-tune the prompt encoder and mask decoder only."
        },
        {
            "title": "4.2 Implementation",
            "content": "We aggregate different sources of datasets into universal training process. The overall training loss consists of text generation loss Ltext and segmentation loss Lseg. The text generation loss Ltext is an auto-regressive cross-entropy loss as standard LLM [12, 13, 29], while the segmentation loss Lseg is combination of per-pixel binary cross-entropy (BCE) loss LBCE and typical dice loss Ldice. We simply balance the weights, and the overall loss is defined as: = Ltext + Lseg, Lseg = 2 LBCE + 0.5 Ldice. The per-pixel binary cross-entropy (BCE) loss LBCE is defined as: LBCE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 [yi log(pi) + (1 yi) log(1 pi)] (1) (2) where yi {0, 1} is the ground truth label, pi [0, 1] is the predicted probability, and is the total number of pixels. The dice loss Ldice is given by: Ldice = 1 i=1 piyi + ϵ 2 (cid:80)N i=1 pi + (cid:80)N i=1 yi + ϵ (cid:80)N (3) where ϵ is small constant for numerical stability. Specifically, for the ROI classification and region-aware report generation tasks, the Lseg will be discarded during training as described in Section 4.1. In this case, the loss function is formulated as: = Ltext. (4) For evaluation, in the internal datasets, we strictly split the datasets into 80% training and 20% test sets as previous method [4] for fair comparisons. Notably, we did not conduct validation during training for selecting hyperparameters. To avoid information leakage, all the external datasets are unseen in training, and we conduct direct inference on these datasets. 14 external datasets for segmentation are listed in Extended Data Table ??. Comparison methods. We first compare nnUNet [3], MedSAM [18], BiomedParse [4], SegVol [19], and SAT [20] in the biomedical image segmentation task. Specifically, nnUNet is specialist model that is designed for specific datasets. Thus, 19 we train 46 nnUNet models on 46 internal datasets for comparisons. MedSAM, BiomedParse, SegVol, and SAT are generalist models that can be applied to multiple datasets. MedSAM and SegVol adopt visual prompts while BiomedParse and SAT use textual prompts for segmentation. SegVol and SAT are only applicable to 3D medical images like CT and MRI, which are not available for multi-modal biomedical images. In addition, the methods based on visual prompts, MedSAM and SegVol, are not applicable to the biomedical objects with irregular shapes [4]. Most of the segmentation datasets used in our method have already been trained in these models. For fair comparisons, we fine-tune these models on the segmentation datasets used in our method. We conduct five experimental runs and report the confidence ranges to ensure reliable results. The details of datasets are shown in Extended Data Table A2. For the text generation task, we first compare GLaMM [36] and LISA [29] in grounded disease recognition and grounded report generation. Specifically, we reimplement these methods on our curated datasets for fair comparisons. The details of datasets for grounded disease recognition are shown in Extended Data Table A4. We evaluate the performance of grounded report generation on the RadGenome [31, 32] dataset. Then, we compare MedRegA [14] and MedPLIB [15] in ROI classification. Both of these two medical MLLMs [14, 15] have been trained on large-scale ROI classification datasets. For fair comparisons, we further fine-tune them on our dataset to make sure the training datasets are consistent. For ROI classification, following MedPLIB [15], we degrade the segmentation masks of segmentation datasets to bounding boxes as visual prompts for representing the ROIs, then predict the biomedical classes within the ROIs. The details of datasets are shown in Extended Data Table A2. The results are shown in Extended Data Table ??. in region-aware report generation. Notably, Furthermore, we compare InternVL2.5-8B [25], LLaVA-Med [11], MedRegA [14], and MedPLIB [15] the medical MLLMs [11, 14, 15] are trained on the VQA and report generation datasets used in this method, enabling us to conduct fair comparisons. We report the results on the MedTrinity [17] dataset. Specifically, MedTrinity is aggregated from 23 report generation datasets, as shown in Extended Data Table A3. In this work, we report the overall results on the whole MedTrinity instead of the separate datasets. For training and validation, we use Pytorch [53] to conduct all the experiments. The training of UniBiomed is conducted on 8 NVIDIA H800 (80G) GPUs for 10 epochs, which takes about 5 days to finish the training. All the inference tasks can be established within one NVIDIA 3090 GPU (24GB)."
        },
        {
            "title": "4.3 Evaluation metrics",
            "content": "For the evaluation of segmentation, the standard dice score (%) is employed to evaluate the performance. Dice score is calculated as: Dice(P re, Gro) = 2P re Gro re + Gro , (5) where re denotes the segmentation predictions, Gro is the ground truth of segmentation labels. For the task of grounded disease recognition, we evaluate the performance of both dice scores and accuracy. The accuracy is calculated as: Accuracy ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1(yi = ˆyi), (6) where is the total number of samples. and ˆy are the predictions and labels, respectively. 1(yi = ˆyi) denotes the prediction of ith sample is correct. For the task of ROI classification, we simply use the accuracy metric to measure the performance. As for the region-aware report generation task, we further employ Bilingual Evaluation Understudy (BLEU) [37], Metric for Evaluation of Translation with Explicit ORdering (METEOR) [38], and Recall-Oriented Understudy for Gisting Evaluation with Longest Common Subsequence (ROUGE L) [39] for evaluation. Specifically, the formulation of BLEU is as follows: BLEU = BP exp (cid:33) wn log pn , (cid:32) (cid:88) n=1 (7) where BP is the brevity penalty, pn are modified n-gram precisions, and wn are weights. The BLEU variants, i,e BLEU1, BLEU2, BLEU3, and BLEU4 are differ in their n-gram scope: BLEU-1 = BP exp(log p1), BLEU-2 = BP exp BLEU-3 = BP exp BLEU-4 = BP exp (cid:32) (cid:32) (cid:32) 1 2 1 1 4 2 (cid:88) n=1 3 (cid:88) n=1 4 (cid:88) n=1 (cid:33) (cid:33) (cid:33) , , . log pn log pn log pn (8) (9) (10) (11) The METEOR [38] is an automatic evaluation metric for evaluating the quality of machine translation, which not only considers vocabulary matching but also combines word order similarity and alignment information. It is calculated as: METEOR = 1 (cid:88) ggold max hhyp Precision(g, h), (12) where is the number of gold standard (reference) sentences, and recision(g, h) refers to the precision score between specific gold standard sentence (g) and hypothesis sentence (h) from the set of all gold standard sentences (gold) and the set of all hypothesis sentences (hyp). 21 ROUGE-L is an automatic evaluation metric for assessing machine-generated text by measuring the longest common subsequence (LCS) between the hypothesis and reference. It computes recall, precision, and their harmonic mean (F-score) as follows: RLCS = LCS(X, ) , PLCS = LCS(X, ) , FLCS = (1 + β2)RLCSPLCS RLCS + β2PLCS , (13) where is the hypothesis, is the reference, and β controls recall emphasis. For multiple references, it averages the best FLCS across pairs, similar to METEOR in Equation 12. Unlike METEOR, it ignores synonyms and alignment, focusing on word-order-agnostic overlap. For the grounded report generation evaluation, we also use (BLEU) [37], METEOR [38], and ROUGE [39] as the metrics. In addition, we further use dice scores to measure the performance of segmentation."
        },
        {
            "title": "5 Data availability",
            "content": "This study incorporates total of 84 public datasets across 10 diverse biomedical imaging modalities. For detailed information about the data used in this project, please refer to Extended Data Tables A2 and A3."
        },
        {
            "title": "6 Code availability",
            "content": "The codes, datasets, and models of UniBiomed will be available at GitHub (https://github.com/Luffy03/UniBiomed)."
        },
        {
            "title": "7 Author contributions",
            "content": "L.W. designed the framework and conducted the experiments. Y.N., S.H., and J.Z. provided suggestions on the framework and experiments. H.C. conceived and supervised the work."
        },
        {
            "title": "Declaration",
            "content": "The authors have no conflicts of interest to declare."
        },
        {
            "title": "Ethics declaration",
            "content": "This project has been reviewed and approved by the Human and Artefacts Research Ethics Committee (HAREC). The protocol number is HREP-2025-0188."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the Hong Kong Innovation and Technology Commission (Project No. MHP/002/22, GHP/006/22GD and ITCPD/17-9), HKUST (Project No. 22 FS111), and the Research Grants Council of the Hong Kong Special Administrative Region, China (Project Reference Number: T45-401/22-N). We also thank the support of HKUST SuperPod for providing the GPU platform for model training."
        },
        {
            "title": "References",
            "content": "[1] Royer, L.A.: The future of bioimage analysis: dialog between mind and machine. Nature Methods 20(7), 951952 (2023) [2] Li, X., Zhang, Y., Wu, J., Dai, Q.: Challenges and opportunities in bioimage analysis. Nature Methods 20(7), 958961 (2023) [3] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18(2), 203211 (2021) [4] Zhao, T., Gu, Y., Yang, J., Usuyama, N., Lee, H.H., Kiblawi, S., Naumann, T., Gao, J., Crabtree, A., Abel, J., Moung-Wen, C., Piening, B., Bifulco, C., Wei, M., Poon, H., Wang, S.: foundation model for joint segmentation, detection, and recognition of biomedical objects across nine modalities. Nature Methods (2024) https://doi.org/10.1038/s41592-024-02499-w [5] Xu, H., Usuyama, N., Bagga, J., Zhang, S., Rao, R., Naumann, T., Wong, C., Gero, Z., Gonzalez, J., Gu, Y., et al.: whole-slide foundation model for digital pathology from real-world data. Nature 630(8015), 181188 (2024) [6] Sun, Y., Wang, L., Li, G., Lin, W., Wang, L.: foundation model for enhancing magnetic resonance images and downstream segmentation, registration and diagnostic tasks. Nature Biomedical Engineering, 118 (2024) [7] Antonelli, M., Reinke, A., Bakas, S., Farahani, K., Kopp-Schneider, A., Landman, B.A., Litjens, G., Menze, B., Ronneberger, O., Summers, R.M., et al.: The medical segmentation decathlon. Nature Communications 13(1), 4128 (2022) [8] Zhang, K., Zhou, R., Adhikarla, E., Yan, Z., Liu, Y., Yu, J., Liu, Z., Chen, X., Davison, B.D., Ren, H., et al.: generalist visionlanguage foundation model for diverse biomedical tasks. Nature Medicine, 113 (2024) [9] Peng, Y., Rousseau, J.F., Shortliffe, E.H., Weng, C.: Ai-generated text may have role in evidence-based medicine. Nature Medicine 29(7), 15931594 (2023) [10] Lu, M.Y., Chen, B., Williamson, D.F., Chen, R.J., Zhao, M., Chow, A.K., Ikemura, K., Kim, A., Pouli, D., Patel, A., et al.: multimodal generative ai copilot for human pathology. Nature, 13 (2024) [11] Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H., Gao, J.: Llava-med: Training large language-and-vision assistant for 23 biomedicine in one day. Advances in Neural Information Processing Systems 36, 2854128564 (2023) [12] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural information processing systems 36, 3489234916 (2023) [13] Yuan, H., Li, X., Zhang, T., Huang, Z., Xu, S., Ji, S., Tong, Y., Qi, L., Feng, J., Yang, M.-H.: Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001 (2025) [14] Wang, L., Wang, H., Yang, H., Mao, J., Yang, Z., Shen, J., Li, X.: Interpretable bilingual multimodal large language model for diverse biomedical tasks. In: International Conference on Learning Representations (2025) [15] Huang, X., Shen, L., Liu, J., Shang, F., Li, H., Huang, H., Yang, Y.: Towards multimodal large language model with pixel-level insight for biomedicine. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, pp. 3779 3787 (2025) [16] Ni, X., Wu, L., Zhuang, J., Wang, Q., Wu, M., Vardhanabhuti, V., Zhang, L., Gao, H., Chen, H.: Mg-3d: Multi-grained knowledge-enhanced 3d medical vision-language pre-training. arXiv preprint arXiv:2412.05876 (2024) [17] Xie, Y., Zhou, C., Gao, L., Wu, J., Li, X., Zhou, H.-Y., Liu, S., Xing, L., Zou, J., Xie, C., et al.: Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine. In: International Conference on Learning Representations (2025) [18] Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medical images. Nature Communications 15(1), 654 (2024) [19] Du, Y., Bai, F., Huang, T., Zhao, B.: Segvol: Universal and interactive volumetric medical image segmentation. Advances in Neural Information Processing Systems 37, 110746110783 (2024) [20] Zhao, Z., Zhang, Y., Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: One model to rule them all: Towards universal segmentation for medical images with text prompts. arXiv preprint arXiv:2312.17183 (2023) [21] Zhuang, J., Wu, L., Wang, Q., Vardhanabhuti, V., Luo, L., Chen, H.: Mim: Mask in mask self-supervised pre-training for 3d medical image analysis. arXiv preprint arXiv:2404.15580 (2024) [22] Wu, L., Zhuang, J., Ni, X., Chen, H.: Freetumor: Advance tumor segmentation via large-scale tumor synthesis. arXiv preprint arXiv:2406.01264 (2024) [23] Wu, L., Zhuang, J., Zhou, Y., He, S., Ma, J., Luo, L., Wang, X., Ni, X., 24 Zhong, X., Wu, M., et al.: Freetumor: Large-scale generative tumor synthesis in computed tomography images for improving tumor recognition. arXiv preprint arXiv:2502.18519 (2025) [24] Wu, L., Zhuang, J., Chen, H.: Large-scale 3d medical image pre-training with geometric context priors. arXiv preprint arXiv:2410.09890 (2024) [25] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198 (2024) [26] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.-Y., et al.: Segment anything. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026 (2023) [27] Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Radle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. International Conference on Learning Representations (2024) [28] Bai, F., Du, Y., Huang, T., Meng, M.Q.-H., Zhao, B.: M3d: Advancing 3d medical image analysis with multi-modal large language models. arXiv preprint arXiv:2404.00578 (2024) [29] Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning segmentation via large language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95799589 (2024) [30] Yuan, Y., Li, W., Liu, J., Tang, D., Luo, X., Qin, C., Zhang, L., Zhu, J.: Osprey: Pixel understanding with visual instruction tuning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820228211 (2024) [31] Zhang, X., Wu, C., Zhao, Z., Lei, J., Zhang, Y., Wang, Y., Xie, W.: Radgenomechest ct: grounded vision-language dataset for chest ct analysis. arXiv preprint arXiv:2404.16754 (2024) [32] Hamamci, I.E., Er, S., Almas, F., Simsek, A.G., Esirgun, S.N., Dogan, I., Dasdelen, M.F., Wittmann, B., Simsar, E., Simsar, M., et al.: foundation model utilizing chest ct volumes and radiology reports for supervised-level zero-shot detection of abnormalities. arXiv preprint arXiv:2403.17834 (2024) [33] Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. arXiv preprint arXiv:2308.02463 (2023) 25 [34] Shui, Z., Zhang, J., Cao, W., Wang, S., Guo, R., Lu, L., Yang, L., Ye, X., Liang, T., Zhang, Q., et al.: Large-scale and fine-grained vision-language pre-training for enhanced ct image understanding. In: International Conference on Learning Representations (2025) [35] He, S., Nie, Y., Wang, H., Yang, S., Wang, Y., Cai, Z., Chen, Z., Xu, Y., Luo, L., Xiang, H., et al.: Gsco: Towards generalizable ai in medicine via generalistspecialist collaboration. arXiv preprint arXiv:2404.15127 (2024) [36] Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer, R.M., Xing, E., Yang, M.-H., Khan, F.S.: Glamm: Pixel grounding large multimodal model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1300913018 (2024) [37] Papineni, K., Roukos, S., Ward, T., Zhu, W.-J.: Bleu: method for automatic evaluation of machine translation. In: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311318 (2002) [38] Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In: Proceedings of the Acl Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation And/or Summarization, pp. 6572 (2005) [39] Lin, C.-Y.: Rouge: package for automatic evaluation of summaries. In: Text Summarization Branches Out, pp. 7481 (2004) [40] Zhu, J., Qi, Y., Wu, J.: Medical sam 2: Segment medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874 (2024) [41] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023) [42] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (long and Short Papers), pp. 41714186 (2019) [43] Ke, L., Ye, M., Danelljan, M., Tai, Y.-W., Tang, C.-K., Yu, F., et al.: Segment anything in high quality. Advances in Neural Information Processing Systems 36, 2991429934 (2023) [44] Wu, L., Fang, L., He, X., He, M., Ma, J., Zhong, Z.: Querying labeled for unlabeled: Cross-image semantic consistency guided semi-supervised semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(7), 88278844 (2023) 26 [45] Wu, L., Zhong, Z., Fang, L., He, X., Liu, Q., Ma, J., Chen, H.: Sparsely annotated semantic segmentation with adaptive gaussian mixtures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1545415464 (2023) [46] Wu, L., Zhong, Z., Ma, J., Wei, Y., Chen, H., Fang, L., Li, S.: Modeling the label distributions for weakly-supervised semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2025) [47] Wu, L., Lu, M., Fang, L.: Deep covariance alignment for domain adaptive remote sensing image segmentation. IEEE Transactions on Geoscience and Remote Sensing 60, 111 (2022) [48] Wu, L., Fang, L., Yue, J., Zhang, B., Ghamisi, P., He, M.: Deep bilateral filtering network for point-supervised semantic segmentation in remote sensing images. IEEE Transactions on Image Processing 31, 74197434 (2022) [49] Liu, Q., He, M., Kuang, Y., Wu, L., Yue, J., Fang, L.: multi-level labelaware semi-supervised framework for remote sensing scene classification. IEEE Transactions on Geoscience and Remote Sensing 61, 112 (2023) [50] Wu, L., Zhuang, J., Chen, H.: Voco: simple-yet-effective volume contrastive image analysis. In: Proceedings of the learning framework for 3d medical IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2287322882 (2024) [51] Wang, X., Zhang, X., Cao, Y., Wang, W., Shen, C., Huang, T.: Seggpt: Towards segmenting everything in context. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11301140 (2023) [52] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. International Conference on Learning Representations 1(2), 3 (2022) [53] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, highperformance deep learning library. Advances in Neural Information Processing Systems 32 (2019) [54] Gong, S., Zhong, Y., Ma, W., Li, J., Wang, Z., Zhang, J., Heng, P.-A., Dou, Q.: 3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable tumor segmentation. Medical Image Analysis 98, 103324 (2024) [55] Jaeger, S., Candemir, S., Antani, S., Wang, Y.-X.J., Lu, P.-X., Thoma, G.: Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery 4(6), 475 (2014) 27 [56] Chowdhury, M.E., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M.A., Mahbub, Z.B., Islam, K.R., Khan, M.S., Iqbal, A., Al Emadi, N., et al.: Can ai help in screening viral and covid-19 pneumonia? IEEE Access 8, 132665132676 (2020) [57] Tahir, A.M., Chowdhury, M.E., Khandakar, A., Rahman, T., Qiblawey, Y., Khurshid, U., Kiranyaz, S., Ibtehaz, N., Rahman, M.S., Al-Maadeed, S., et al.: Covid-19 infection localization and severity grading from chest x-ray images. Computers in biology and medicine 139, 105002 (2021) [58] Khaled, R., et al.: Categorized digital database for low energy and subtracted contrast enhanced spectral mammography images. The Cancer Imaging Archive (2021) [59] Sae-Lim, W., Wettayaprasit, W., Suwannanon, R., Cheewatanakornkul, S., Aiyarak, P.: Automated pneumothorax segmentation and quantification algorithm based on deep learning. Intelligent Systems with Applications 22, 200383 (2024) [60] Gamper, J., Koohbanani, N.A., Benes, K., Graham, S., Jahanifar, M., Khurram, S.A., Azam, A., Hewitt, K., Rajpoot, N.: Pannuke dataset extension, insights and baselines. arXiv preprint arXiv:2003.10778 (2020) [61] Sirinukunwattana, K., Pluim, J.P., Chen, H., Qi, X., Heng, P.-A., Guo, Y.B., Wang, L.Y., Matuszewski, B.J., Bruni, E., Sanchez, U., et al.: Gland segmentation in colon histology images: The glas challenge contest. Medical Image Analysis 35, 489502 (2017) [62] Sitnik, D., Aralica, G., Hadˇzija, M., Hadˇzija, M.P., Paˇcic, A., Periˇsa, M.M., Manojlovic, L., Krstanac, K., Plavetic, A., Kopriva, I.: dataset and methodology for intraoperative computer-aided diagnosis of metastatic colon cancer in liver. Biomedical Signal Processing and Control 66, 102402 (2021) [63] Mahbod, A., Schaefer, G., Bancher, B., Low, C., Dorffner, G., Ecker, R., Ellinger, I.: Cryonuseg: dataset for nuclei instance segmentation of cryosectioned h&estained histological images. Computers in biology and medicine 132, 104349 (2021) [64] Da, Q., Huang, X., Li, Z., Zuo, Y., Zhang, C., Liu, J., Chen, W., Li, J., Xu, D., Hu, Z., et al.: Digestpath: benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system. Medical Image Analysis 80, 102485 (2022) [65] Silva-Rodrıguez, J., Colomer, A., Sales, M.A., Molina, R., Naranjo, V.: Going deeper through the gleason scoring scale: An automatic end-to-end system for histology prostate grading and cribriform pattern detection. Computer Methods and Programs in Biomedicine 195, 105637 (2020) 28 [66] Han, C., Lin, J., Mai, J., Wang, Y., Zhang, Q., Zhao, B., Chen, X., Pan, X., Shi, Z., Xu, Z., et al.: Multi-layer pseudo-supervision for histopathology tissue semantic segmentation using patch-level classification labels. Medical Image Analysis 80, 102487 (2022) [67] Graham, S., Chen, H., Gamper, J., Dou, Q., Heng, P.-A., Snead, D., Tsang, Y.W., Rajpoot, N.: Mild-net: Minimal information loss dilated network for gland instance segmentation in colon histology images. Medical Image Analysis 52, 199211 (2019) [68] Kumar, N., Verma, R., Anand, D., Zhou, Y., Onder, O.F., Tsougenis, E., Chen, H., Heng, P.-A., Li, J., Hu, Z., et al.: multi-organ nucleus segmentation challenge. IEEE Transactions on Medical Imaging 39(5), 13801391 (2019) [69] Vitale, S., Orlando, J.I., Iarussi, E., Larrabide, I.: Improving realism in patientspecific abdominal ultrasound simulation using cyclegans. International Journal of Computer Assisted Radiology and Surgery 15(2), 183192 (2020) [70] Leclerc, S., Smistad, E., Pedrosa, J., Østvik, A., Cervenansky, F., Espinosa, F., Espeland, T., Berg, E.A.R., Jodoin, P.-M., Grenier, T., et al.: Deep learning for segmentation using an open large-scale dataset in 2d echocardiography. IEEE Transactions on Medical Imaging 38(9), 21982210 (2019) [71] Jieyun, B., ZhanHong, O.: Pubic symphysis-fetal head segmentation and angle of progression (2024) [72] Orlando, J.I., Fu, H., Breda, J.B., Van Keer, K., Bathula, D.R., Diaz-Pinto, A., Fang, R., Heng, P.-A., Kim, J., Lee, J., et al.: Refuge challenge: unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. Medical Image Analysis 59, 101570 (2020) [73] Staal, J., Abr`amoff, M.D., Niemeijer, M., Viergever, M.A., Van Ginneken, B.: Ridge-based vessel segmentation in color images of the retina. IEEE Transactions on Medical Imaging 23(4), 501509 (2004) [74] Amelard, R., Glaister, J., Wong, A., Clausi, D.A.: High-level intuitive features (hlifs) for intuitive skin lesion description. IEEE Transactions on Biomedical Engineering 62(3), 820831 (2014) [75] Ngoc Lan, P., An, N.S., Hang, D.V., Long, D.V., Trung, T.Q., Thuy, N.T., Sang, D.V.: Neounet: Towards accurate colon polyp segmentation and neoplasm detection. In: Advances in Visual Computing: 16th International Symposium, ISVC 2021, Virtual Event, October 4-6, 2021, Proceedings, Part II, pp. 1528 (2021). Springer [76] Ahmed, Z., Panhwar, S.Q., Baqai, A., Umrani, F.A., Ahmed, M., Khan, A.: Deep learning based automated detection of intraretinal cystoid fluid. International 29 Journal of Imaging Systems and Technology 32(3), 902917 (2022) [77] Ji, Y., Bai, H., Ge, C., Yang, J., Zhu, Y., Zhang, R., Li, Z., Zhanng, L., Ma, W., Wan, X., et al.: Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. Advances in neural information processing systems 35, 3672236732 (2022) [78] Landman, B., Xu, Z., Igelsias, J., Styner, M., Langerak, T., Klein, A.: Miccai multi-atlas labeling beyond the cranial vaultworkshop and challenge. In: Proc. MICCAI Multi-Atlas Labeling Beyond Cranial VaultWorkshop Challenge, vol. 5, p. 12 (2015) [79] Luo, X., Liao, W., Xiao, J., Chen, J., Song, T., Zhang, X., Li, K., Metaxas, D.N., Wang, G., Zhang, S.: Word: large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image. Medical Image Analysis 82, 102642 (2022) [80] Ma, J., Zhang, Y., Gu, S., An, X., Wang, Z., Ge, C., Wang, C., Zhang, F., Wang, Y., Xu, Y., et al.: Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge. Medical Image Analysis 82, 102616 (2022) [81] Heller, N., Isensee, F., Trofimova, D., Tejpaul, R., Zhao, Z., Chen, H., Wang, L., Golts, A., Khapun, D., Shats, D., et al.: The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct. arXiv preprint arXiv:2307.01984 (2023) [82] Ma, J., Zhang, Y., Gu, S., Zhu, C., Ge, C., Zhang, Y., An, X., Wang, C., Wang, Q., Liu, X., Cao, S., Zhang, Q., Liu, S., Wang, Y., Li, Y., He, J., Yang, X.: Abdomenct-1k: Is abdominal organ segmentation solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence 44(10), 66956714 (2022) https://doi.org/10.1109/TPAMI.2021.3100536 [83] Armato III, S.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer, C.R., Reeves, A.P., Zhao, B., Aberle, D.R., Henschke, C.I., Hoffman, E.A., et al.: The lung image database consortium (lidc) and image database resource initiative (idri): completed reference database of lung nodules on ct scans. Medical physics 38(2), 915931 (2011) [84] Nan, Y., Del Ser, J., Tang, Z., Tang, P., Xing, X., Fang, Y., Herrera, F., Pedrycz, W., Walsh, S., Yang, G.: Fuzzy attention neural network to tackle discontinuity in airway segmentation. IEEE Transactions on Neural Networks and Learning Systems (2023) [85] Roth, H.R., Xu, Z., Tor-Dıez, C., Jacob, R.S., Zember, J., Molto, J., Li, W., Xu, S., Turkbey, B., Turkbey, E., et al.: Rapid artificial intelligence solutions in pandemicthe covid-19-20 lung ct lesion segmentation challenge. Medical Image Analysis 82, 102605 (2022) 30 [86] Radl, L., Jin, Y., Pepe, A., Li, J., Gsaxner, C., Zhao, F.-h., Egger, J.: Avt: Multicenter aortic vessel tree cta dataset collection with ground truth segmentation masks. Data in brief 40, 107801 (2022) [87] Kavur, A.E., Gezer, N.S., Barıs, M., Aslan, S., Conze, P.-H., Groza, V., Pham, D.D., Chatterjee, S., Ernst, P., Ozkan, S., Baydar, B., Lachinov, D., Han, S., Pauli, J., Isensee, F., Perkonigg, M., Sathish, R., Rajan, R., Sheet, D., Dovletov, G., Speck, O., Nurnberger, A., Maier-Hein, K.H., Bozdagı Akar, G., Unal, G., Dicle, O., Selver, M.A.: CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation. Medical Image Analysis 69, 101950 (2021) https://doi.org/10.1016/j.media.2020.101950 [88] Soler, L., Hostettler, A., Agnus, V., Charnoz, A., Fasquel, J.-B., Moreau, J., Osswald, A.-B., Bouhadjar, M., Marescaux, J.: 3d image reconstruction for comparison of algorithm database. URL: https://www. ircad. fr/research/datasets/liver-segmentation-3d-ircadb-01 (2010) [89] Ginneken, B.: SLIVER07 [Data set]. Zenodo (2019). https://doi.org/10.5281/ zenodo.2597908 . https://doi.org/10.5281/zenodo.2597908 [90] Morshid, A., Elsayes, K.M., Khalaf, A.M., Elmohr, M.M., Yu, J., Kaseb, A.O., Hassan, M., Mahvash, A., Wang, Z., Hazle, J.D., et al.: machine learning model to predict hepatocellular carcinoma response to transcatheter arterial chemoembolization. Radiology: Artificial Intelligence 1(5), 180021 (2019) [91] Roth, H.R., Farag, A., Turkbey, E.B., Lu, L., Liu, J., Summers, R.M.: Data From Pancreas-CT. https://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU. The Cancer Imaging Archive (2016) [92] ˇZukovec, M., Dular, L., ˇSpiclin, ˇZ.: Modeling multi-annotator uncertainty as multi-class segmentation problem. In: International MICCAI Brainlesion Workshop, pp. 112123 (2021). Springer [93] Aerts, H.J., Velazquez, E.R., Leijenaar, R.T., Parmar, C., Grossmann, P., Carvalho, S., Bussink, J., Monshouwer, R., Haibe-Kains, B., Rietveld, D., et al.: Decoding tumour phenotype by noninvasive imaging using quantitative radiomics approach. Nature Communications 5(1), 4006 (2014) [94] Alves, N., et al.: The PANORAMA Study Protocol: Pancreatic Cancer Diagnosis-Radiologists Meet AI. Zenodo (2024). https://doi.org/10.5281/ zenodo.10599559 . https://doi.org/10.5281/zenodo.10599559 [95] Qu, C., Zhang, T., Qiao, H., Tang, Y., Yuille, A.L., Zhou, Z., et al.: Abdomenatlas-8k: Annotating 8,000 ct volumes for multi-organ segmentation in three weeks. Advances in Neural Information Processing Systems 36 (2023) [96] Bassi, P.R., Li, W., Tang, Y., Isensee, F., Wang, Z., Chen, J., Chou, Y.-C., 31 Kirchhoff, Y., Rokuss, M.R., Huang, Z., et al.: Touchstone benchmark: Are we on the right way for evaluating ai algorithms for medical segmentation? Advances in Neural Information Processing Systems 37, 1518415201 (2024) [97] Buda, M., Saha, A., Mazurowski, M.A.: Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by deep learning algorithm. Computers in Biology and Medicine 109, 218225 (2019) [98] Bernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X., Heng, P.-A., Cetin, I., Lekadir, K., Camara, O., Ballester, M.A.G., et al.: Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE Transactions on Medical Imaging 37(11), 25142525 (2018) [99] Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor image segmentation benchmark (brats). IEEE Transactions on Medical Imaging 34(10), 19932024 (2014) [100] Gatidis, S., Hepp, T., Fruh, M., La Foug`ere, C., Nikolaou, K., Pfannenberg, C., Scholkopf, B., Kustner, T., Cyran, C., Rubin, D.: whole-body fdg-pet/ct dataset with manually annotated tumor lesions. Scientific Data 9(1), 601 (2022) [101] Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 20972106 (2017) [102] Akbarnejad, A., Ray, N., Barnes, P.J., Bigras, G.: Predicting ki67, er, pr, and her2 statuses from h&e-stained breast cancer images. arXiv preprint arXiv:2308.01982 (2023) [103] Kather, J.N., Halama, N., Marx, A.: 100000 Histological Images of Human Colorectal Cancer and Healthy Tissue. https://doi.org/10.5281/zenodo.1214456 . https://doi.org/10.5281/zenodo. [104] Tsuneki, M., Kanavati, F.: Inference of captions from histopathological patches. In: International Conference on Medical Imaging with Deep Learning, pp. 1235 1250 (2022). PMLR [105] He, X., Zhang, Y., Mou, L., Xing, E., Xie, P.: Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286 (2020) [106] Kawai, M., Ota, N., Yamaoka, S.: Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks. In: Workshop on Medical Image Learning with Limited and Noisy Data, pp. 257267 (2023). Springer 32 [107] Yan, K., Wang, X., Lu, L., Summers, R.M.: Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. Journal of medical imaging 5(3), 036501036501 (2018) [108] Lou, M., Ying, H., Liu, X., Zhou, H.-Y., Zhang, Y., Yu, Y.: Sdr-former: siamese dual-resolution transformer for liver lesion classification using 3d multi-phase imaging. Neural Networks, 107228 (2025) [109] Garrucho, L., Reidel, C.-A., Kushibar, K., Joshi, S., Osuala, R., Tsirikoglou, A., Bobowicz, M., Riego, J., Catanese, A., Gwozdziewicz, K., et al.: Mama-mia: large-scale multi-center breast cancer dce-mri benchmark dataset with expert segmentations. arXiv preprint arXiv:2406.13844 (2024) [110] Lau, J.J., Gayen, S., Ben Abacha, A., Demner-Fushman, D.: dataset of clinically generated visual questions and answers about radiology images. Scientific data 5(1), 110 (2018) [111] Liu, B., Zhan, L.-M., Xu, L., Ma, L., Yang, Y., Wu, X.-M.: Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pp. 16501654 (2021). IEEE [112] Lin, W., Zhao, Z., Zhang, X., Wu, C., Zhang, Y., Wang, Y., Xie, W.: Pmcclip: Contrastive language-image pre-training using biomedical documents. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 525536 (2023). Springer [113] Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., Xie, W.: Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415 (2023) [114] Ye, J., Cheng, J., Chen, J., Deng, Z., Li, T., Wang, H., Su, Y., Huang, Z., Chen, J., Jiang, L., et al.: Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. arXiv preprint arXiv:2311.11969 (2023)"
        },
        {
            "title": "Appendix A Extended Data",
            "content": "Table A1: Comparison of different representative biomedical models. Compared with previous methods, UniBiomed supports various modalities, prompts, and tasks. LLM denotes Large-Language Models. Region-Aware Diagnosis denotes models that can interpret regions of interest defined by users via visual prompts. Pixel-level Grounding indicates models that can generate text descriptions with corresponding segmentation masks. End-to-End Training indicates the models are trained in an endto-end process. Method MedSAM [18] BiomedParse [4] 3DSAM [54] SegVol [19] SAT [20] MedSAM2 [40] M3D [28] RadFM [33] LLaVA-Med [11] BiomedGPT [8] fVLM [34] MedRegA [14] MedPLIB [15] UniBiomed Multi-Modal Support Prompts Support Response Region-Aware Pixel-level End-End LLM Images Visual Text Mask Text Mask+Text Diagnosis Grounding Training (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) 34 Fig. A1: Overall comparisons on various biomedical tasks, including segmentation, disease recognition, ROI classification, region-aware report generation, and grounded report generation. We compare UniBiomed with nnUNet [3], MedSAM [18], BiomedParse [4], SegVol [19], SAT [20], InternVL [25], LLaVA-Med [11], MedRegA [14], MedPLIB [15], GLaMM [36], and LISA [29]. Notably, while previous methods can address only limited number of these tasks, UniBiomed excels by delivering stateof-the-art performance across all of them. 35 Table A2: 61 datasets across 10 modalities for segmentation, ROI classification, and grounded disease recognition. 47 and 14 of them are used for internal and external validation, respectively. The external datasets are annotated with *."
        },
        {
            "title": "Link",
            "content": "CXRMask [55] Radiography [56] COVID-QU-Ex [57] CDD-CESM [58] SIIM [59] PanNuke [60] GlaS [61] CoCaHis [62] CryoNuSeg [63] DigestPath [64] SICAPv2 [65] X-Ray X-Ray X-Ray X-Ray X-Ray https://datasetninja.com/chest-xray https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database https://www.kaggle.com/datasets/anasmohammedtahir/covidqu https://www.cancerimagingarchive.net/collection/cdd-cesm/ https://www.kaggle.com/datasets/vbookshelf/pneumothorax-chest-xray-images-and-masks"
        },
        {
            "title": "Pathology",
            "content": "https://jgamper.github.io/PanNukeDataset/"
        },
        {
            "title": "Pathology",
            "content": "https://warwick.ac.uk/fac/cross fac/tia/data/glascontest/"
        },
        {
            "title": "Pathology",
            "content": "https://cocahis.irb.hr/"
        },
        {
            "title": "Pathology",
            "content": "https://github.com/masih4/CryoNuSeg"
        },
        {
            "title": "Pathology",
            "content": "https://digestpath2019.grand-challenge.org/"
        },
        {
            "title": "Pathology",
            "content": "https://data.mendeley.com/datasets/9xxm58dvs3/1 WSSS4LUAD [66]"
        },
        {
            "title": "Pathology",
            "content": "https://wsss4luad.grand-challenge.org/ CRAG* [67]"
        },
        {
            "title": "Pathology",
            "content": "https://warwick.ac.uk/fac/cross fac/tia/data/mildnet/ MoNuSeg* [68]"
        },
        {
            "title": "Pathology",
            "content": "https://monuseg.grand-challenge.org/ US [6971] REFUGE [72] DRIVE [73] UWater [74]"
        },
        {
            "title": "Ultrasound",
            "content": "https://www.kaggle.com/datasets/ignaciorlando/ussimandsegm"
        },
        {
            "title": "Fundus",
            "content": "https://bitbucket.org/woalsdnd/refuge/src https://drive.grand-challenge.org/"
        },
        {
            "title": "Dermoscopy",
            "content": "https://uwaterloo.ca/ NeoPolyp [75]"
        },
        {
            "title": "Endoscope",
            "content": "https://www.kaggle.com/c/bkai-igh-neopolyp/data OCT-CME [76]"
        },
        {
            "title": "OCT",
            "content": "https://www.kaggle.com/datasets/zeeshanahmed13/intraretinal-cystoid-fluid MSD [7] AMOS [77] BTCV [78] WORD [79] FLARE22 [80] FLARE23* [80] KiTS23 [81] AbdomenCT1K* [82] LIDC-IDRI [83] AIIB [84] COVID-CT [85] AVT [86] CHAOS* [87] IRCADb* [88] SLIVER07* [89] HCCTACE* [90] TCIAPancreas* [91] QUBIQ* [92] Rider* [93] PANORAMA* [94] CT&MRI http://medicaldecathlon.com/ CT&MRI https://amos22.grand-challenge.org/ CT CT CT CT CT CT CT CT CT CT CT CT CT CT CT CT CT CT https://www.synapse.org/#!Synapse:syn3193805/wiki https://github.com/HiLab-git/WORD https://flare22.grand-challenge.org/ https://codalab.lisn.upsaclay.fr/competitions/12239 https://kits-challenge.org/kits23/ https://github.com/JunMa11/AbdomenCT-1K https://www.cancerimagingarchive.net/collection/lidc-idri/ https://codalab.lisn.upsaclay.fr/competitions/13238 https://covid-segmentation.grand-challenge.org/ https://figshare.com/articles/dataset/Aortic Vessel Tree AVT CTA Datasets and Segmentations/ https://chaos.grand-challenge.org/ https://www.ircad.fr/research/data-sets/liver-segmentation-3d-ircadb-01 https://sliver07.grand-challenge.org/ https://www.cancerimagingarchive.net/collection/hcc-tace-seg https://www.cancerimagingarchive.net/collection/pancreas-ct/ https://qubiq21.grand-challenge.org/QUBIQ/ https://www.cancerimagingarchive.net/analysis-result/rider-lungct-seg/ https://panorama.grand-challenge.org/ AbdomenAtlas* [95, 96] CT https://github.com/MrGiovanni/AbdomenAtlas RadGenome [31] LGG [97] ACDC [98] BraTS21* [99] AutoPET [100] CT"
        },
        {
            "title": "PET",
            "content": "https://huggingface.co/datasets/RadGenome/RadGenome-ChestCT https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html https://www.synapse.org/Synapse:syn51156910/wiki/621282 https://autopet.grand-challenge.org/ Table A3: 23 Datasets across 10 modalities for training and validation of report generation. Multi-modal contains: X-ray, Pathology, Endoscopy, Ultrasound, Dermoscopy, Fundus, PET, CT, OCT, and MRI. The bounding box annotations are generated by MedTrinity [17]."
        },
        {
            "title": "Link",
            "content": "NIH-CXR [101] X-Ray https://www.kaggle.com/datasets/nih-chest-xrays/data"
        },
        {
            "title": "Pathology",
            "content": "https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images"
        },
        {
            "title": "CPD",
            "content": "IHC4BC [102]"
        },
        {
            "title": "Pathology",
            "content": "https://zenodo.org/records/6633721 https://academictorrents.com/details/99f2c7b57b95500711e33f2ee4d14c9fd7c7366c https://zenodo.org/records/7282326 https://www.kaggle.com/datasets/akbarnejad1991/ihc4bc-compressed/data NCT-CRC-HE-100K [103] Pathology https://www.kaggle.com/datasets/imrankhan77/nct-crc-he-100k PatchGastricADC22 [104] Pathology https://zenodo.org/records/6550925 Path-VQA [105] TCGA-UT PTCGA [106]"
        },
        {
            "title": "BHX",
            "content": "DeepLesion [107] CT-RATE [32] RadGenome [31] BRATS24-MICCAI LLD-MMRI [108] MAMA-MIA [109]"
        },
        {
            "title": "Pathology",
            "content": "CT CT CT CT"
        },
        {
            "title": "MRI",
            "content": "https://huggingface.co/datasets/flaviagiammarino/path-vqa https://zenodo.org/records/5889558 https://drive.google.com/drive/folders/18CmL-WLyppK1Rk29CgV7ib5MACFzg5ei https://physionet.org/content/bhx-brain-bounding-box/1.1/ https://huggingface.co/datasets/farrell236/DeepLesion https://huggingface.co/datasets/ibrahimhamamci/CT-RATE https://huggingface.co/datasets/RadGenome/RadGenome-ChestCT https://www.synapse.org/Synapse:syn53708126 https://github.com/LMMMEng/LLD-MMRI-Dataset https://www.synapse.org/Synapse:syn60868042/wiki/628716 VQA-RAD [110] X-Ray&MRI https://osf.io/89kps/ SLAKE [111] X-Ray&CT&MRI https://www.med-vqa.com/slake/ PMC-OA [112] Multi-modal https://huggingface.co/datasets/axiong/pmc oa PMC-VQA [113] Multi-modal https://huggingface.co/datasets/xmcmic/PMC-VQA SA-Med2D-20M [114] Multi-modal https://openxlab.org.cn/datasets/GMAI/SA-Med2D-20M 37 Table A4: 270K images with 15 types of abnormalities for grounded disease recognition experiments. 66,226 normal images as negative samples are also included in training and validation, constructing dataset of 341,284 images. The links of the datasets are provided in Table A2."
        },
        {
            "title": "Liver tumor",
            "content": "10,338 MSD03-Liver [7]"
        },
        {
            "title": "Pancreas tumor",
            "content": "2,504 MSD07-Pancreas [7]"
        },
        {
            "title": "Kidney tumor",
            "content": "12,182 KiTS23 [81]"
        },
        {
            "title": "Colon cancer",
            "content": "14,718 MSD10-Colon [7], CoCaHis [62], DigestPath [64]"
        },
        {
            "title": "Lung nodule",
            "content": "1,483 9,122 MSD06-Lung [7] LIDC [83] COVID19 infection 21, COVID-QU-Ex [57], COVID-CT [85]"
        },
        {
            "title": "Fibrotic lung disease",
            "content": "51,715 AIIB [84]"
        },
        {
            "title": "Brain tumor",
            "content": "119,523 LGG [97], MSD01-Brain [7]"
        },
        {
            "title": "Pneumothorax",
            "content": "1,294 2,050 2,669 BreastUS [69] NeoPolyp [75] Pneumonia [56]"
        },
        {
            "title": "Prostate cancer",
            "content": "23,924 SICAPv2 [65]"
        },
        {
            "title": "Retinal lesion",
            "content": "119 1,460 UWater [74] OCT-CME [76]"
        },
        {
            "title": "No findings",
            "content": "66,226 CXRMask [55], CAMUS [70], FH-PS-AOP [71] MSD [7], CHAOS [87], KiTS23 [81]"
        },
        {
            "title": "Total",
            "content": "341,284 38 Fig. A2: Region-aware report generation results on the MedTrinity [17] dataset. The text in green indicates the correct contents in generated reports. 39 Fig. A3: Grounded report generation results on the RadGenome [31] dataset. The generated segmentation masks are in orange. The text in green indicates the correct contents in generated reports."
        }
    ],
    "affiliations": [
        "Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",
        "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",
        "Division of Life Science, The Hong Kong University of Science and Technology, Hong Kong, China",
        "Shenzhen-Hong Kong Collaborative Innovation Research Institute, The Hong Kong University of Science and Technology, Shenzhen, China",
        "State Key Laboratory of Molecular Neuroscience, The Hong Kong University of Science and Technology, Hong Kong, China"
    ]
}