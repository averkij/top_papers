{
    "paper_title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
    "authors": [
        "Sangwoon Kwak",
        "Joonsoo Kim",
        "Jun Young Jeong",
        "Won-Sik Cheong",
        "Jihyong Oh",
        "Munchurl Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 1 7 3 0 . 1 0 5 2 : r MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting Sangwoon Kwak1,2, Joonsoo Kim1, Jun Young Jeong1, Won-Sik Cheong1, Jihyong Oh3, Munchurl Kim2 1Electronics and Telecommunications Research Institute, 2Korea Advanced Institute of Science and Technology, 3Chung-Ang University {s.kwak, joonsookim, jyj0120, wscheong}@etri.re.kr jihyongoh@cau.ac.kr {sw.kwak, mkimee}@kaist.ac.kr https://kaist-viclab.github.io/MoDecGS-site/ Figure 1. Novel view synthesis results on [45]. We introduce MoDec-GS, novel framework for learning compact dynamic 3D Gaussians from real-world videos with complex motion. While existing SOTA methods [20, 54, 57] have difficulty modeling such complex combination of global and local motions, our approach effectively handles them thanks to GLMD (Sec. 4.1), and outperforms the prior methods in rendering quality even with compact model size. The metrics under each framework are, PSNR (dB) / LPIPS [59] / Storage (MB)."
        },
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDec-GS, memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce Co-corresponding authors. Global-to-Local Motion Decomposition (GLMD) to effectively capture dynamic motions in coarse-to-fine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the tempo1 ral coverage of each Local CS during training, allowing MoDec-GS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDec-GS achieves an average 70% reduction in model size over state-of-the-art methods for dynamic 3D Gaussians from real-world dynamic videos while maintaining or even improving rendering quality. 1. Introduction Novel view synthesis (NVS) generates new perspectives of scene from limited set of images, closely approximating real footage. NVS has been key research area for many years, with advancements in techniques such as depth-image-based rendering [12, 40, 61]. This ongoing interest is largely driven by the broad applicability of NVS in areas such as virtual reality, augmented reality, and immersive media, where natural viewpoint transitions that mimic real-life experiences are essential for enhancing user realism [1, 2]. Approaching NVS as the task of modeling the radiance field has taken the computer vision community by storm. This paradigm shift, led by Neural Radiance Field (NeRF) [38], has set new photorealism standard that surpasses conventional methods. The original NeRF represents the radiance field as an implicit function linked to volume rendering, achieving remarkable visual fidelity. However, it faces challenges with its slow training and, more critically, rendering speed, which is far from real-time. Despite various optimization efforts [14, 32, 41], achieving real-time rendering on consumer-level devices remains difficult, largely due to NeRFs reliance on pixel-wise volumetric rendering. [22] has emerged as compelling alternative, offering exceptional rendering speed without compromising visual quality. By representing the radiance field as collection of 3D Gaussian ellipsoids, 3DGS enables efficient patch-wise rasterization through Gaussian projection and alpha compositing. This patch-level rasterization pipeline, fully leveraging GPU parallel computation, allows 3DGS to achieve unrivaled rendering speed. Subsequently, 3DGS has inspired diverse research trajectories, with extension to video inputs and compression emerging as key areas of focus [24, 28, 36, 39, 54, 56]. Recently, 3D Gaussian Splatting (3DGS) Current approaches to dynamic adaptation often pair static canonical 3DGS with implicit [9, 20, 23, 29, 48, 54, 57] or explicit [21, 30] components to manage the temporal deformation of the attributes within the canonical 3DGS. Another approach extends 3D Gaussian primitives into 4D by adding temporal dimension [8, 56]. While both approaches preserve solid rendering quality for dynamic scenes, they face storage issues due to the multidimensional attributes assigned to numerous 3D Gaussians in canonical 3DGS or 4D Gaussians [25]. Handling longduration content with complex motion also poses difficulties, as representing all frames with unified model causes blurring due to its limited capacity as noted by Shaw et al. [48]. To address this, they segment sequences based on scene motion and train separate model for each segment. However, this approach requires an extra step to compute motion vectors, which diminishes the usability of 3DGS. Early methods to address the storage demands of 3DGS had focused on compressing the original 3DGS representation, employing techniques like vector quantization [10, 24, 42, 43], Gaussian pruning [10, 24], and implicit encoding of Gaussians attributes [17, 24, 55]. Some recent studies have successfully introduced more memory-efficient representations based on the 3DGS framework, with notable example being the anchor-based representation [4, 34, 36, 52], which assigns implicit features at sparse anchor points to predict attributes for broader set of neighboring 3D Gaussians. However, extending these methods, originally designed for static scenes, to dynamic videos may be challenging, as dynamic scene modeling typically requires substantial additional components. few recent methods have aimed to unify dynamic extension and memory efficiency, but one [50] is constrained to multi-view sequences, while another [25] still struggles with long-duration contents. To address the limitations of existing methods, we propose novel dynamic 3D Gaussian splatting framework, enhancing model compactness and rendering quality while preserving real-time capability. Our framework employs deformation-based approach with an anchor-based representation [36] for the canonical 3DGS due to its compactness. Building on this, we introduce two-stage deformation process inspired by the observation that natural motion involves both global and local components. In the first stage, called Global Anchor Deformation (GAD), the canonical representation is deformed to specific time interval using an anchor deformation encoder that captures global motion across the entire sequence. In the second stage, called Local Gaussian Deformation (LGD), the deformed representation is refined via local deformation encoder capturing finer motion within the specific time interval around the chosen time point. final feature of our framework is Temporal Interval Adjustment (TIA) which assigns the temporal interval to each deformation encoder is automatically determined during training. This does not require any precomputed external information such as optical flow, and effectively localizes the dynamic motion of the scene. In short, our contributions are as follows: We propose novel framework MoDec-GS based on Global-to-Local Motion Decomposition (GLMD), which effectively handles real worlds complex motions composed of global and local movements. We introduce the TIA to adaptively control the temporal intervals of each local canonical anchor during training, 2 enabling our MoDec-GS to achieve optimal visual quality even with compactly limited model size. Extensive experiments on three widely used monocular datasets show that our method significantly reduces storage while maintaining or even improving visual quality, specifically, on iPhone dataset [16], it shows PSNR gain of + 0.7dB and storage reduction of -94% compared to the second best method in terms of quality, SC-GS [20]. 2. Related Works 2.1. 3D Gaussian Splatting for Dynamic Scenes natural evolution of 3DGS [22] for static scenes is its extension to dynamic scenes, with recent research in this area generally split into two main categories: deformationbased and 4D Gaussian-based methods. Deformation-based methods rely on static canonical 3DGS, paired with component that captures the temporal deformation of the attributes within this canonical representation. This deformation can be modeled implicitly using structures like MLP [20, 29, 57] or feature grids [9, 54], or explicitly via polynomial [28, 30], Fourier [21, 30] or learned basis functions [23]. In contrast, 4D Gaussian-based methods [8, 56] introduce time as an extra dimension in the 3D Gaussian formulation. Both works aim to integrate the 4D Gaussian paradigm within the established 3DGS training and rendering framework, with particular focus on efficiently representing rotations in 4D space. Although both categories achieve decent rendering quality for dynamic scenes, they still demand substantial storage, to handle the multi-dimensional attributes of millions of 3D Gaussians in canonical 3DGS or 4D Gaussians [25, 60]. Additionally, representing long-duration or largemotion contents with compact model is challenging [48]. This difficulty arises from the need to maintain fast rendering speed while ensuring reliable training on limited set of sampled videos. straightforward approach is to train separate model for each empirically determined interval; however, determining the optimal interval empirically is inefficient, as the flow of motion information can vary significantly depending on the content. Shaw et al. [48] proposed method for temporal segmentation to address this issue, but this considers only the magnitude of motion. In contrast, we integrates temporal segmentation method directly into the training process, enabling MoDec-GS to explore optimal intervals by itself to achieve the best rendering quality. 2.2. Compact 3D Gaussian Splatting To address the substantial memory demands of 3DGS, various strategies have been proposed. The first category focuses on compressing the original 3DGS representation, with key approaches including vector quantization [10, 24, 42, 43], pruning redundant Gaussians [10, 24, 51], implicit encoding of high-dimensional attributes [17, 24, 55], using standardized compression pipelines [10, 39, 55] and applying entropy constraint [51]. The second category explores more efficient Gaussian representations to mitigate storage challenges [19, 36]. prominent example is ScaffoldGS, which introduces unique method by assigning learnable features to sparse set of anchor points that predict attributes for broader set of neighboring 3D Gaussians. Recent advancements in the Scaffold-GS framework have further enhanced memory efficiency by organizing anchor points hierarchically across multiple levels [52] or using binary hash grid to model context for unstructured anchor attributes [4]. However, adapting methods from both categories to 4DGS may not be straightforward, as most 4D extensions require substantial architectural modifications to extend 3DGS for dynamic scene modeling. few recent approaches have covered both dynamic extension and memory efficiency; for instance, Sun et al. [50] propose framework for on-the-fly training, where adaptive control over the quantity of 3D Gaussians is employed, allowing the model size to remain moderate for streaming. However, this method assumes multi-view inputs. Lee et al. [25] implement combination of compression techniques, including residual vector quantization and hash grid-based encoding, on top of the Spacetime Gaussian proposed by Li et al. [28]. However, this approach doesnt provide an efficient solution for handling long-duration, large-motion content. Very recently, concurrent work [5] proposes method to extend 3D scaffolds [36] to 4D space for compactly representing dynamic scenes. However, this approach focuses solely on multi-views scenes and is not be able to handle casual monocular data, which is more common real-world setting. Additionally, it assumes object motion to be piecewise linear within uniformly divided time intervals, lacking solution for handling complex motions, which generally contains non-linear motions. In contrast, our MoDec-GS can accommodate both multi-view and monocular content, providing an effective solution for dynamic scenes with complex motion where global and local motions are combined over non-uniform time intervals. 3. Preliminary 3.1. Splatting of Gaussian primitives Gaussian primitives, or Gaussians, are characterized using their respective rotation matrices and scaling matrices = diag([si]). To render the primitives for viewport that corresponds to viewing transformation matrix , one can calculate the following covariance matrix: Σ = T ΣW J, (1) where Σ = RT ST SR and is an affine approximation of the projective transformation [22]. The covariance matrix 3 Figure 2. Overview of our MoDec-GS framework. To effectively train dynamic 3D Gaussians with complex motion, we introduce Global-to-Local Motion Decomposition (GLMD) (Sec 4.1). We first train Global Canonical Scaffold-GS (Global CS) with entire frames, and apply Global Anchor Deformation (GAD) to Local Canonical Scaffold-GS (Local CS) dedicated to represent its corresponding temporal segment (Sec 4.2). Next, to finely adjust the remaining local motion, we apply Local Gaussian Deformation (LGD) which explicitly deforms the reconstructed 3D Gaussians with shared hexplane (Sec 4.3). During the training, Temporal Interval Adjustment (TIA) is performed, optimizing the temporal interval into non-uniform interval that adopts to the scenes level of motion (Sec 4.4). represents the approximate shape of the projected Gaussian. Once the 2D covariance matrices and the projected central positions for each Gaussians are calculated, the primitives are sorted in the order of the depth values. Lastly, the colors for each pixel in the viewport can be calculated by the following alpha-blending procedure: = (cid:88) i=1 i1 (cid:89) ciαi (1 αj), j=1 (2) where ci is the color of the i-th primitive that is calculated from spherical harmonic coefficients. The opacity αi is obtained by evaluating the 2D Gaussian distribution function at the pixel position. 3.2. Scaffold-GS Scaffold-GS representation consists of set of anchor points which are the central points of voxels of predefined size, set of neural Gaussians associated with the anchor points, and set of neural networks that predict the attributes of the neural Gaussians. There are k-number of neural Gaussians that are associated to each anchor point, and such group of Gaussians that are softly bound to spatial point work as local representation. The centers of the neural Gaussians are given as follows: mi = xv + oi, (3) where {0, ..., 1} and xv is the position of an anchor point v. mi and oi are the center position and the learnable offset for the i-th neural Gaussian. The opacities of the neural Gaussians are predicted as follows: {attrv,0, , attrv,k1} = Fattr( ˆfv, δv,cam, v,cam) (4) where attrv,i is the attribute of the i-th neural Gaussian associated with the anchor point v. The attributes include opacity, color, quarternion, and scales. Separate neural networks Fattr are used to predict the attributes where the networks take inputs including learnable anchor feature ˆfv and the v,cam) from the viewing position to displacement (δv,cam, the anchor v. Similar to the densification in 3DGS, the anchor points are added or removed based on the gradient accumulation and the opacity. 4. Proposed Method 4.1. Overview of MoDec-GS We adopt deformation-based real-time dynamic scene rendering method [54], but use anchor-based representation [36] as canonical 3DGS due to its compactness. To effectively capture real-world videos with complex combination of dynamic global and local motions, we introduce Global-to-Local Motion Decomposition (GLMD) as shown in Fig. 2. GLMD consists of first stage primarily focused on modeling global motion, followed by second stage that models local motion. In the first stage, we apply Global Anchor Deformation (GAD), which deforms the position and attributes of the anchors with tiny global hexplane, transforming the Global Canonical Scaffold-GS (Global CS) into the Local Canonical Scaffold-GS (Local CS) (Sec. 4.2). As shown in Fig. 3, this anchor-based transformation helps effectively represent global motion. Additionally, we embed learnable parameters on anchors re4 flecting its motion characteristics, enabling effective control over both the anchor-wise global motion and the local motion within the anchor. In the second stage, the Local CS is reconstructed into 3DGs through neural Gaussian derivation and then explicitly deformed to each target timestamp with shared local hexplane (Sec. 4.3). To optimize the temporal interval assigned to each Local CS according to the motion characteristics of the scene, we propose Temporal Interval Adjustment (TIA) (Sec. 4.4). This method efficiently re-balances the temporal intervals during training, efficiently utilizing limited representation capability, without requiring any precomputed external information such as optical flow or tracking [26, 31, 33]. 4.2. Global Anchor Deformation (GAD) Anchor Deformation. One approach to representing dynamic motion based on deformation is to learn hexplane that deforms the attributes of 3DGs after reconstruction [54]. While this method is general and effective, it may struggle to handle complex motions, such as combinations of global and local motion, as limited capability In contrast, another method to achieve this of hexplane. involves directly deforming the anchors position and attributes in anchor-based representation [36]. As shown in Fig 3, the method of deforming the anchor itself is more efficient for representing global motion of relatively large objects, rather than learning deformation fields for each reconstructed individual 3DGs. Therefore, as shown in Fig. 2-(a), we deform the anchors position and its attributes in GAD stage. For given anchor point v, the anchor position xv, yv, zv is queried in tiny global hexplane HG along with timestamp. Here, the timestamp corresponds to the canonical time tc, which is time representing each divided temporal segment, determining temporal interval of the Local CS. The queried feature is decoded by tiny MLP and multi-head anchor decoder, producing the deformations for the position and attributes associated with the anchor: (x, y, z), fv, Ov, sv. For example, deformation of local context feature can be obtained by fv = φf [FG(HG(xv, yv, zv, tc)], (5) where FG is light MLP and φf is local context feature head among the multi-head anchor decoders. The deformation values are added to the anchor attributes, producing deformed anchor, at which point the proposed novel term, learnable anchor dynamics, are incorporated. Anchor Dynamics. Even for long-range video, considerable portion of the scene is still static. Instead of disentangled generation of static and dynamic parts [29] or exploiting precomputed external dynamic mask [37], we aim to learn dynamics by assigning additional learnable attributes to the anchor, allowing them to be optimized during the training process. To separately model the global movement characteristics of the anchor and the local movements Figure 3. Concept and effect of 2-stage deformation. For representing complex motion of 3D Gaussians, global movement over time intervals can be more efficiently handled through deformation of anchor itself. In contrast, subtle motions of individual 3D Gaussians within time interval can be effectively addressed by explicit deformation of each Gaussian. within the anchor, we applied and trained dG and dL independently. Global dynamics dG learns whether the entire anchor moves globally and applies binary masking based on threshold. This learnable masking inspired by [24] is derived as follows: (dG) = sg(I[σ(dG) > ϵ] σ(dG)) + σ(dG), (6) where sg() is the stop gradient operator, is an indicator, σ() is sigmoid function, and ϵ is the masking threshold. Local dynamics dL is simply activated and then multiplied to the attributes of the corresponding anchor. Finally, the attributes of the deformed anchor are given by xv, yv, zv = (xv, yv, zv) + (dG) (x, y, z) (7) fv = fv + σ(dL), (8) ov = ov + σ(dL), sv = sv + σ(dL). (10) (9) 4.3. Local Gaussian Deformation (LGD) After global movements of each anchor over time intervals are well-captured in the first stage, the remaining movements of individual 3D Gaussians are expected to become relatively small and simplified, as shown in Fig. 3. Representing such movements can be effectively handled by the explicit deformation [54] of the reconstructed Gaussians, rather than anchor deformation, since it would require learning feature changes capable of generating the attributes of the displaced Gaussians (See Sec. 3 and Tab. 3). Based on this reasoning, deformed Local CS is first reconstructed into 3D Gaussians through neural Gaussian derivation. Neural Gaussian Derivation. Within given view frustum, neural Gaussians are spawned from the deformed anchor, and each Gaussians attributes are reconstructed using the deformed feature along with the viewing direction and distance. For instance, an opacity set of Gaussians is spawned as follows: {α0, , αk1} = Fα( ˆf v, δv,cam, v,cam), (11) where Fα is MLP decoder, ˆf is feature bank constructed from the deformed feature on anchor v, δv,cam and v,cam are relative distance and viewing direction from viewpoint to the anchor, respectively [36]. Gaussian Deformation. The spawned neural Gaussians are then explicitly deformed to the target timestamp [54]. For example, positional deformations of k-th neural Gaussian in Local CS is given by: xk, yk, zk = φp[FL(HL(xk, yk, zk, tL)] (12) where HL is local multi-resolution hexplane, FL is MLP decoder, φp is position head, and tL is target timestamp. Note that the local hexplane and corresponding MLP decoder are shared across each Local CS for compactness. 4.4. Temporal Interval Adjustment (TIA) To effectively localize the degree of motion of the entire scene and guide the temporal scope of each Local CS, we divided the whole frames into segments, where 1 < < . The initial temporal segments are evenly divided to have uniform temporal intervals. However, depending on the motion characteristics of given scene, the optimal time interval that each Local CS can effectively represent may vary. To account for this, we propose Temporal Interval Adjustment (TIA) to adjust the temporal intervals to fit the scene during the training process. The TIA rebalances the complexity of deformation required for each local canonical Gaussians, enabling effective scene representation even with compactly limited size of hexplane. Canonical time shift. For the temporal interval adjustment, we employ the canonical time-based shift method, as illustrated in Fig. 2-(c). Basically, temporal intervals are managed by canonical time list Tc = [t1, t2, , tl1], which represents the lowest timestamp of each interval and serves as the boundary between temporal segments. During the training process, this list is fixed at equal intervals from the normalized entire time range [0, 1], until preset iteration for starting temporal adjustment, TIA from. After the starting iteration, the TIA process, as described in Algo. 1, is repeated until preset iteration for ending the process, TIA until. During each adjustment period TIA period, positional gradients are accumulated in the temporal interval to where the timestamp of each training view belongs. The accumulated gradient list Gacc = [gacc 1 , , gacc l1] and the accumulation count list νacc = [νacc 0 , νacc l1], are initially set to zero, and updated in each iteration as follows: = gacc gacc = νacc νacc + gpos + 1, 1 , , νacc 0 , gacc (13) (14) , where gpos is the Frobenius norm of positional gradient for certain iteration where the training view has timestamp within [tc, tc+1). Please note that the left boundary of the first temporal interval should not be represented as an adjustable canonical time but always be fixed at 0. Therefore, Tc is one element shorter in length than Gacc. Based on the statistics of the accumulated gradients, we identify Local CS with insufficient expressiveness, and shrink the corresponding temporal intervals. The idea behind this approach is that temporal segments with significantly high accumulated positional gradient indicate regions that the Local CS experiences difficulty in representing efficiently, therefore, we reduce the time interval they are responsible for. Each temporal interval with an accumulated gradient greater than the preset threshold τTIA is shrunk by step size sTIA on both sides. if TIA Algorithm 1 Temporal Interval Adjustment (Fig. 2-(c)) 1: procedure TIA(Tc, Gacc, νacc, gpos from iter TIA until then 2: Update Gacc with gpos 3: if iter % TIA 4: µ = (cid:80)l1 period = 0 then /νacc c=0(gacc , τTIA, sTIA) (Eq. 13) 5: acc. grad. mean ) µ]2/l std. (cid:113)(cid:80)l1 σ = for = 0 to 1 do c=0[(gacc )/l /νacc if gacc µ + τTIA σ then if = 0 and tj tj+1 sTIA then shrink tj tj + sTIA if = 1 and tj tj+1 sTIA then tj+1 tj+1 sTIA Init Gacc, νacc, µ, σ 6: 7: 8: 9: 10: 11: 12: 13: 5. Experiments 5.1. Experimental Setup Implementation Details. Our framework is built upon 4DGS [54] and Scaffold-GS [36], and almost follows the original hyperparameters. The parameters related to newly designed modules are empirically derived, and we refer readers to the Sec. for the details. Datasets and Metrics. There are various and wellvalidated multi-view videos datasets [27, 46], but achieving"
        },
        {
            "title": "Block",
            "content": "Paper-windmill Space-out SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 14.96 / 0.692 / 0.508 15.61 / 0.696 / 0.367 15.41 / 0.691 / 0.524 16.48 / 0.699 / 0.402 173.3 13.98 / 0.548 / 0.483 115.7 14.87 / 0.221 / 0.432 446.3 14.87 / 0.559 / 0.390 118.9 14.89 / 0.213 / 0.341 87.71 13.89 / 0.550 / 0.539 63.52 61.52 15.57 / 0.590 / 0.478 23.78 114.2 42.01 14.44 / 0.201 / 0.445 123.9 14.29 / 0.515 / 0.473 52.02 18. 14.79 / 0.511 / 0.440 160.2 14.59 / 0.510 / 0.450 13.65 14.92 / 0.220 / 0.377 14.65 / 0.522 / 0.467 17."
        },
        {
            "title": "Average",
            "content": "SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 14.32 / 0.407 / 0.445 13.10 / 0.392 / 0.490 14.89 / 0.413 / 0.441 15.53 / 0.433 / 0.366 219.1 12.51 / 0.516 / 0.562 133.9 11.20 / 0.508 / 0.573 71.80 26.84 12.56 / 0.521 / 0.598 318.7 11.90 / 0.354 / 0.484 239.2 117.1 11.79 / 0.345 / 0.394 106.1 12.31 / 0.509 / 0.605 80.44 10.83 / 0.339 / 0.538 96.50 16.68 12.28 12.44 / 0.374 / 0.413 232.4 13.90 / 0.464 / 0.479 13.72 / 0.461 / 0.430 109.4 13.72 / 0.460 / 0.509 78.54 18.37 14.60 / 0.480 / 0. Table 1. Quantitative results comparison on iPhone datasets [16]. Red and blue denote the best and second best performances, respectively. Each block element of 5-performance denotes (mPSNR(dB) / mSSIM / mLPIPS Storage(MB)). Figure 4. Qualitative results comparison on three datasets [16, 45, 58]. The yellow boxes highlight areas where the proposed method achieves notable visual quality improvements, and the storage for the corresponding sequence is displayed below each rendered patch. high rendering quality especially in monocular reconstruction remains challenging, which is the most accessible reallife application due to the prevalence of camera-equipped mobile devices. To focus on the complex motion present in such real-world videos, we evaluate our method on recent monocular video benchmark, Dycheck-iPhone [16], which closely reflects the real-life video characteristics without teleporting. We also used HyperNeRF [45] and Nvidiamonocular dataset [58] which are widely used for monocular evaluation, employed to assess the generalization performance of out method. For all three datasets, initial point cloud data was manually generated by COLMAP [47] using the script provided in [54]. The image quality of our approach is evaluated using three metrics: Peak Signal-toNoise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [59]. Each metric is calculated on per-frame basis and then averaged over all test frames. Storage efficiency is measured in megabytes (MB) by summing the size of the trained model. For the iPhone dataset, we used the masked metrics (mPSNR, mSSIM, mLPIPS) based on the official co-visible mask provided by the authors [16]. Comparison Methods. We compare our approach with recent dynamic 3D Gaussian representation methods that can reconstruct dynamic scenes from monocular video footage: Deformable-3DGS [57], SC-GS [20], and 4DGS [54]. The official codes are used for the methods, and we adjusted several parameters to achieve reasonable rendering quality, with details provided in the Sec. A. 5.2. Results Quantitative Comparison. As detailed in Tab. 1, our method significantly reduces storage while achieving the best or second-best visual quality performance across almost all sequences of iPhone dataset [16]. On average, it maintains or even improves visual quality with only about 6% of the storage compared to the second-best method in terms of quality, SC-GS [20]. On HyperNeRF dataset [45], we present only the average performance shown as Tab. 2- (a); please refer to Tab. 5 for sequence-wise detailed performance. For this dataset as well, our method attains the best performance in PSNR, SSIM and second-best in LPIPS, while having approximately 18% of the storage compared to SC-GS [20] and around 57% compared to 4DGS [54]. 7 Methods PSNR SSIM LPIPS Storage (a) HyperNeRF SC-GS [CVPR24] [20] Deformable 3DGS [CVPR24] [57] 4DGS [CVPR24] [54] Ours 26.95 25.96 27.44 27. 0.815 0.766 0.797 0.827 0.213 0. 0.302 0.219 226.0 87.13 72.65 40. (b) Nvidia Methods PSNR SSIM LPIPS Storage 4DGS [CVPR24] [54] Ours 25. 26.65 0.844 0.876 0.219 0.171 67. 39.64 Table 2. Quantitative results comparison on (a) HyperNeRF [45] and (b) Nvidia monocular [58] dataset. Variant mPSNR mSSIM mLPIPS Storage (a) 1stage, Gaussian deform. ([54]) (b) 1stage, anchor deform. (c) 2stage, all anchor deform. (d) 2stage, GAD + LGD (GLMD) (e) (d) with smaller hexplane (f) (e) with dG and dL (anchor dynamics) (g) (f) with TIA (our final MoDec-GS) 13. 13.56 13.93 14.48 14.46 14.51 14. 0.460 0.449 0.453 0.475 0.475 0. 0.480 0.509 0.510 0.492 0.455 0. 0.447 0.443 78.54 36.92 55.29 49. 22.67 22.72 18.37 Table 3. Ablation studies of the proposed methods. Yellowgreen backgrounds highlight cases where the applying of the proposed method resulted in noticeable reduction in storage. For the Nvidia dataset [58], we compare our method only with the second-best method in terms of visual quality relative to storage, 4DGS [54]. Tab. 2-(b) shows that our method reduces storage while simultaneously improving visual quality in all metrics. Qualitative Comparison. To qualitatively evaluate the visual quality of the proposed method, we conducted quality assessments on three datasets [16, 45, 58] shown in Fig. 4. We focused particularly on the visual quality in regions where dynamic objects are in motion. As we already introduced in Fig. 1, while the comparison methods struggle to handle complex motion, our method demonstrates better quality in regions with such motion, thanks to GLMD. Furthermore, as presented in the results on NVIDIA dataset, our method shows fine visual quality even in cases where static objects exhibit only local changes (e.g., facial expressions). This is due to TIA effectively localizing the coverage of the Local CS. Not only does our method achieve these quality improvements, but it also maintains storage requirements at about half the average size of the compared methods in all cases. 5.3. Analysis Ablation studies. We analyze the effectiveness of the components in our MoDec-GS through intensive ablation In the table, all results are studies as shown in Tab 3. the average outcomes across 7 sequences in iPhone dataset [16]. Note that our baseline is the case of single-stage de8 Figure 5. TIA effectiveness. formation that explicitly deforms Gaussians, as in [54]. We first examine the effectiveness of our anchor deformation - (b). The results show that leveraging anchor-based representation [36] for the canonical Gaussians results in slight performance decrease but significantly reduces storage size (52% reduction). Configuring the Local CS by adding global hexplane - (c), we observe that the performance improvement outweighs the increase in storage due to the additional grid, compared to (a). We then show the effectiveness of LGD - (d). Instead of applying consistent anchor deformation for both global and local deformation, performing LGD after reconstructing the neural Gaussian noticeably improves performance. Additionally, it allows for slight storage reduction, because the size of Global CS can be decreased in an anchor adjustment processing, thanks to the regions representable through the deformation of adjacent Gaussians no longer require anchors. The efficient design of GLMD allows for reduction in the size of the global and local hexplanes with minimal impact on visual quality - (e) (55% reduction). By adding the proposed learnable parameters that control anchors motion dynamics, the increase is almost marginal while improving visual quality to some extent. Finally, applying TIA to this variant - our final MoDec-GS - enables both quality improvement and storage reduction. When temporal intervals are appropriately adapted to the degree of motion in the scene through TIA, the representational capacity of limited-size hexplane can be more efficiently utilized, and also the Gaussian movements represented by each LGD become simpler (See Fig. 3), thus enhancing the efficiency of anchor deformation as well. We also conducted additional experiments to verify the precise effects of TIA. We precomputed optical flow by [49] to measure the degree of motion in the scene, and evaluated how adaptively the proposed TIA responds to it. As shown in Fig. 5, initially uniform temporal intervals (black dotted line) are adjusted into non-uniform temporal intervals (blue solid line) during the training process, shrinking and shifting toward regions with relatively higher normalized optical flow magnitude. Additionally, by examining the accumulated magnitudes in each region, we observe that TIA effectively re-balances the degree of motion covered by each interval. 6. Conclusion We propose MoDec-GS, novel compact framework for high-quality dynamic 3D Gaussian splatting, addressing storage demands and complex motion challenges in dynamic scene reconstruction. By leveraging Global-toLocal Motion Decomposition (GLMD), which incorporates Global Anchor Deformation (GAD) for global motion and Local Gaussian Deformation (LGD) for fine-grained local adjustments, MoDec-GS effectively captures complex motions with minimal storage use. Additionally, our Temporal Interval Adjustment (TIA) allows adaptive temporal segmentation, across dynamic intervals without requiring external motion data. Extensive evaluations confirm that MoDec-GS significantly reduces model sizeup to average 70%while either preserving or enhancing rendering quality across challenging datasets, offering compact yet powerful solution for real-world dynamic 3D reconstruction."
        },
        {
            "title": "References",
            "content": "[1] Jill Boyce, Renaud Dore, Adrian Dziembowski, Julien Fleureau, Joel Jung, Bart Kroon, Basel Salahieh, Vinod Kumar Malamal Vadakital, and Lu Yu. Mpeg immersive video coding standard. Proceedings of the IEEE, 109(9):1521 1536, 2021. 2 [2] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, and Paul Debevec. Immersive light field video with layered mesh representation. ACM Transactions on Graphics (TOG), 39(4):861, 2020. 2 [3] Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, et al. Textured gaussians for enhanced 3d scene appearance modeling. arXiv preprint arXiv:2411.18625, 2024. 3 [4] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. Hac: Hash-grid assisted context for 3d gaussian splatting compression. In European Conference on Computer Vision, pages 422438. Springer, 2025. 2, 3 [5] Woong Oh Cho, In Cho, Seoha Kim, Jeongmin Bae, Youngjung Uh, and Seon Joo Kim. 4d scaffold gaussian splatting for memory efficient dynamic scene reconstruction. arXiv preprint arXiv:2411.17044, 2024. [6] Mengyu Chu, You Xie, Laura Leal-Taixe, and Nils Thuerey. Temporally coherent gans for video super-resolution (tecogan). arXiv preprint arXiv:1811.09393, 1(2):3, 2018. 2 [7] Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixe, and Nils Thuerey. Learning temporal coherence via selfsupervision for gan-based video generation. ACM Transactions on Graphics (TOG), 39(4):751, 2020. 2, 4 [8] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 3 [9] Bardienus Duisterhof, Zhao Mandi, Yunchao Yao, JiaWei Liu, Mike Zheng Shou, Shuran Song, and Jeffrey Ichnowski. Md-splatting: Learning metric deformation from 4d gaussians in highly deformable scenes. arXiv preprint arXiv:2312.00583, 2023. 2, 3 [10] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. arXiv preprint arXiv:2311.17245, 2023. 2, 3 [11] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia 2022 Conference Papers, pages 19, 2022. 1, 2 [12] Christoph Fehn. Depth-image-based rendering (dibr), compression, and transmission for new approach on 3d-tv. In Proceedings of SPIE 5291, Stereoscopic Displays and Virtual Reality Systems XI, pages 93104, San Jose, California, United States, 2004. International Society for Optics and Photonics. [13] Franke, Ruckert, Fink, and Stamminger. Trips: Trilinear point splatting for real-time radiance field rendering. arxiv abs/2401.06003 (2024). 3 [14] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 2, 1 [15] Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, and Naoto Yokoya. V4d: Voxel for 4d novel view synthesis. IEEE Transactions on Visualization and Computer Graphics, 2023. 2 [16] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. pages 3376833780, 2022. 3, 7, 8, 1, 2, 4 [17] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023. 2, 3 [18] Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiaoqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, and Jingdong Wang. Forward flow for novel view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1602216033, 2023. 1, 2 [19] Abdullah Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, and Andrea Vedaldi. Ges: Generalized exponential splatting In Proceedings of for efficient radiance field rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1981219822, 2024. [20] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42204230, 2024. 1, 2, 3, 7, 8, 4 [21] Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. An efficient 3d gaussian representation for monocular/multi-view 9 dynamic scenes. arXiv preprint arXiv:2311.12897, 2023. 2, 3 32nd ACM International Conference on Multimedia, pages 29362944, 2024. 2, 1 [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 1 [23] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis. Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting. In European Conference on Computer Vision, pages 252269. Springer, 2025. 2, 3, [24] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21719 21728, 2024. 2, 3, 5, 1 [25] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian splatting for static and dynamic radiance fields. arXiv preprint arXiv:2408.03822, 2024. 2, 3 [26] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. 5 [27] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55215531, 2022. 6 [28] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. 2, 3 [29] Yiqing Liang, Numair Khan, Zhengqin Li, Thu NguyenPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis. In ArXiv, 2024. 2, 3, 5 [30] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21136 21145, 2024. 2, [31] Bangya Liu and Suman Banerjee. Swings: Sliding window gaussian splatting for volumetric video streaming with arbitrary length. arXiv preprint arXiv:2409.07759, 2024. 5 [32] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:1565115663, 2020. 2, 1 [33] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from causually-captured monocular videos. arXiv preprint arXiv:2406.00434, 2024. 5 [34] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: Efficient 3d scene representation via compressed gaussian splatting. In Proceedings of the [35] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 1 [36] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2, 3, 4, 5, 6, 8, 1 [37] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Tracking arXiv preprint Deva Ramanan. by persistent dynamic view synthesis. arXiv:2308.09713, 2023. Dynamic 3d gaussians: [38] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Communications of the ACM, pages 99106, 2021. 2 [39] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. Compact 3d scene representation via selforganizing gaussian grids. arXiv preprint arXiv:2312.13299, 2023. 2, 3 [40] Yuji Mori, Norishige Fukushima, Tomohiro Yendo, Toshiaki Fujii, and Masayuki Tanimoto. View generation with 3d warping using depth information for ftv. Signal Processing: Image Communication, 24(1-2):6572, 2009. 2 [41] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresIn ACM Transactions on Graphics olution hash encoding. (TOG), pages 102:1102:15, 2022. 2, 1 [42] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159, 2023. 2, [43] Simon Niedermayr, Josef Stumpfegger, and Rudiger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1034910358, 2024. 2, 3 [44] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58655874, 2021. 1, 2 [45] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven Seitz. Hypernerf: higherdimensional representation for topologically varying neural radiance fields. In SIGGRAPH Aisa, 2021. 1, 7, 8, 2, 4, 5 [46] Neus Sabater, Guillaume Boisson, Benoit Vandame, Paul Kerbiriou, Frederic Babon, Matthieu Hog, Remy Gendrot, Tristan Langlois, Olivier Bureller, Arno Schubert, et al. Dataset and pipeline for multi-view light-field video. In Proceedings of the IEEE conference on computer vision and pattern recognition Workshops, pages 3040, 2017. 6 10 deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 1, 7, 2, [60] Xinjie Zhang, Zhening Liu, Yifan Zhang, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Zehong Lin, Shuicheng Mega: Memory-efficient 4d Yan, and Jun Zhang. arXiv preprint gaussian splatting for dynamic scenes. arXiv:2410.13613, 2024. 3 [61] Sveta Zinger, Luat Do, and Phn De With. Free-viewpoint depth image based rendering. Journal of Visual Communication and Image Representation, 21(56):533541, 2010. 2 [47] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 7, 1 [48] Richard Shaw, Michal Nazarczuk, Jifei Song, Arthur Moreau, Sibi Catley-Chandar, Helisa Dhamo, and Eduardo Perez-Pellitero. Swings: sliding windows for dynamic 3d In European Conference on Computer gaussian splatting. Vision. Springer, 2024. 2, 3 [49] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15991610, 2023. 8, [50] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic freeIn Proceedings of the IEEE/CVF Conviewpoint videos. ference on Computer Vision and Pattern Recognition, pages 2067520685, 2024. 2, 3, 1 [51] Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, and Zhibo Chen. End-to-end ratearXiv distortion optimized 3d gaussian representation. preprint arXiv:2406.01597, 2024. 3 [52] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex Kot, and Bihan Wen. Contextgs: Compact 3d gaussian splatting with anchor level context model. arXiv preprint arXiv:2405.20721, 2024. 2, 3 [53] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 2, 4 [54] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1, 2, 3, 4, 5, 6, 7, 8 [55] Minye Wu and Tinne Tuytelaars. Implicit gaussian splatting with efficient multi-level tri-plane representation. arXiv preprint arXiv:2408.10041, 2024. 2, 3 [56] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and arXiv preprint rendering with 4d gaussian splatting. arXiv:2310.10642, 2023. 2, 3 [57] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highIn Profidelity monocular dynamic scene reconstruction. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2033120341, 2024. 1, 2, 3, 7, 8, 4 [58] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53365345, 2020. 7, 8, 1, 4 [59] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of 11 MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details Our MoDec-GS is implemented using PyTorch and built upon 4DGS [54] and Scaffold-Gs [36] codebase. Similar to 4DGS [54] we adopt hexplane-based deformation method to represent video content, while using anchorbased representation [36] for the canonical 3D Gaussians. The key hyperparameters for the anchor repesentation include offset=20, voxel size=0.01, feat dim=32, and appearance dim=16, with no feature bank utilized. Iterations are set as follows: 3,000 for the Global stage, and between 20,000 and 60,000 for the Local stage depending on the sequence length. The global and local hexplanes are set to [32, 32, 32, 10] and [64, 64, 64, 100] with two-level multiresolution, respectively, for all test cases. The parameters for TIA are as follows: TIA until is set to 10,000 or 20,000 depending on the total number of iterations, and TIA period is set to 1,000, also depending on the iterations. τTIA is set to either 1.0 or 1.5, and the sTIA is chosen within the range of 0.01 to 0.1. For the comparison methods, Deformable-3DGS and SC-GS were compared in the same local experimental environment and retrained on all datasets. For the Deformable-3DGS, the option for 6-degrees of freedom deformation is turned on for better rendering quality. We set the number of node and the dimension of hyper coordinates for the SC-GS at 2048 and 8, respectively. No mask images for background separation were used. from is set to 500, TIA B. Demo Videos Please refer to the demo videos in our project page: https://kaist-viclab.github.io/MoDecGSsite/. To reflect various motion characteristics, we demonstrated subjective quality comparisons for HyperNeRFs interp-cut-lemon, interp-torchocolate, misc-espresso, misc-tampling, vrig-peel-banana. We compared four framworks which are SC-GS [20], Deformable 3DGS [57], 4DGS [54], and MoDec-GS (Ours). The videos are concatenated in 22 or 41 format depending on the shape fo the video. Please note that due to the supplementary materials size limit (200MB), the entire video has been resized from the original resolution to 720p. C. Comparison with NeRF-extension frameworks The latest trends in NVS are driven by 3DGS [22] and its extensions [20, 24, 34, 50, 54, 57]. However, NeRFbased methods that utilize differentiable volume rendering are still being actively researched and demonstrate strong performance in terms of visual quality [14, 23, 32, 35, 41]. Although our primary target application focuses on being compact without losing real-time rendering capabilities, we also provide comparison with NeRF-based approaches for reference information using results taken from [54]. Tab. 4 presents the comparison results. In the table, the numbers for [11, 18, 22, 44, 45] are sourced from [54] and those of the other are generated in our local environment. We confirmed that the performances of 4DGS reported in [54] is nearly reproduced in our side, and note that there are differences of GPU environment ([54]: RTX 3090, Ours: RTX A6000 - due to the time limitation, the speed comparison measured on the same machine has not prepared, but it is generally known that RTX A6000 is slower than RTX 3090. We plan to fairly measure the training time and rendering speed on the same GPU in the future). Through the comparison, we confirmed that our method achieved the lowest storage requirement at only 52% of the second-best [11], while maintaining the highest visual quality scores and high-speed rendering performance exceeding 20 fps. Additionally, to visualize the comparison results with other frameworks, we present performance comparison graph, as shown in Fig. 6 where the x-axis represents rendering speed (FPS), the y-axis denotes PSNR, and the bubble size (MB) indicates the models storage size. Our method achieves an exceptionally small storage size while maintaining the highest level of visual quality performance. D. Detailed Experimental Results D.1. Datasets and metrics In this paper, the following three datasets are used: iPhone [16], HyperNeRF [45], and Nvidia [58]. All datasets were downloaded from their official repositories, and the COLMAP [47] data for iPhone and HyperNeRF datasets were directly generated using the script provided in [54] by ourselves. Note that the script is designed to subsample frames to ensure that the number of frames does not exceed 200 when obtaining the initial point cloud. For COLMAP inputs, we used the 2 version of the sequences for each dataset. Specifically, 360480 for the iPhone dataset and 536960 for the HyperNeRF dataset. For the Nvidia dataset, multi-view video frames were sampled sequentially at one frame per timestamp, resulting in total of 192 monocular frames. To define the test frames, every 1 Methods PSNR(dB) MS-SSIM Training times Run times(FPS) Storage(MB) Nerfies [44] HyperNeRF [45] TiNeuVox-B [11] FFDNeRF [18] V4D [15] 3DGS [22] 4DGS [54] MoDec-GS (Ours) 22.2 22.4 24.3 24.2 24.8 19. 25.0 25.0 0.803 0.814 0.836 0. 0.832 0.680 0.838 0.836 hours 32 hours 30 mins - 5.5 hours 40 mins 1.2 hour 1.2 hour < 1 < 1 1 0.05 0.29 24.9 23.8 - - 48 377 52 61 28 Table 4. Performance comparison with NeRF-extension framework, including training and rendering speed. Averaged over 536960 HyperNeRFs vrig datasets [45]. The performance numbers of [11, 18, 22, 44, 45] are sourced from [54]. The training times and run times reported in [54] were measured on an NVIDIA RTX 3090 GPU, while our framework was tested on an RTX A6000 GPU. Please note that the A6000 GPU has approximately 20 % lower memory bandwidth compared to that of the RTX 3090. Figure 6. Performance comparison visualization graph. The x-axis represents rendering speed (FPS), and the y-axis indicates PSNR. Each framework is depicted as bubble, with the size of the bubble representing the model storage size (MB). 8th frame was excluded from the training views. This is one of the settings provided in [54], resulting in total of 168 training views and 24 test views meaning the temporal interpolation, which is more challenging setting. Through validation on this setting of NVIDIA dataset, which features long-range time duration and high resolution (around FHD to 2K), we aimed to effectively verify the storage reduction capabilities of our model. Regarding the metrics, PSNR, SSIM [53], and LPIPS [59] metrics are calculated using the functions in [54], while masked metrics for the iPhone dataset are obtained by the functions and covisible masks provided by DyCheck [16]. For tOF [7], module form TecoGAN [6] is utilized. D.2. Detailed results The full quantitative results on three datasets are detailed in Tab 5. Our method achieves the best or second-best visual quality performance in almost all sequences while using significantly less storage. Regarding the average performance of HyperNeRF, not only in the interp results which are reported in the main paper, but also in misc and vrig, our method shows the highest PSNR/tOF and second-best SSIM performance, while using about 40% less storage compared to the second-best model from storage perspective [54]. 2 E. Ablation Studies E.1. Visualization of GLMD Our MoDec-GS is characterized by its ability to decompose global and local motion through 2-stage deformation process. This technique, called GLMD, enables effective representation of complex motions even with limitedsize hexplane. To verify whether GLMD operates as our design intention, we visualize the individual rendering results of Global CS, Local CS, and the final deformed frame, which is shown in Fig. 7. For the cut-lemon scene in HyperNeRF, we rendered the Global CS directly, as shown in the topmost image. After the Global CS is deformed into each Local CS through GAD, we rendered each Local CS as shown in the central image and then measured the optical flow [49] between the two. As we can see in the rendered Local CS and the optical flow, it can be observed that global motion with an overall similar direction is represented according to the movement of the knife cutting the lemon. Based on the optical flow color map, we visualized this by overlaying arrows on the rendered patch. The Local CS is then deformed into individual frames through LGD. We also rendered the frames at fixed camera position during this process and observed the optical flow. As result, various directional components of local motion were observed, which were also overlaid as arrows on the rendered patch. Through this detailed and easy-to-understand visualization, we confirmed that the proposed GLMD effectively captures both global and local motions. Thanks to this capability, it achieves high scene representation for complex motions even with smaller model size. F. Limitations and Future Works Fig. 8 illustrates failure case on the HyperNeRF-broom dataset. Within the challenging setting of monocular video, there are limitations in representing thin and highly detailed textured objects using only finite numbers of 3D Gaussians. Consequently, neither the comparison methods [20, 54, 57] nor ours are able to effectively learn the scene. To address this issue, previous studies have proposed integrating traditional graphics techniques such as texture and alpha mapping into 3DGS [3], utilizing generalized exponential functions instead of 3D Gaussians [19], or incorporating hierarchical pyramid features to capture finer details [13]. As part of future work, we aim to enhance the Gaussian primitives used in MoDec-GS by building upon these prior studies, enabling robust expressivity even in scenes with intricate and highly detailed textures. 3 Method Apple Block Paper-windmill Space-out (a) iPhone dataset SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 14.96 / 0.692 / 0.508 / 0.704 15.61 / 0.696 / 0.367 / 0.523 15.41 / 0.691 / 0.524 / 0.591 16.48 / 0.699 / 0.402 / 0.459 173.3 87.71 61.52 23.78 13.98 / 0.548 / 0.483 / 0.931 115.7 14.87 / 0.221 / 0.432 / 0.473 446.3 14.87 / 0.559 / 0.390 / 0.924 14.89 / 0.213 / 0.341 /0.519 118.9 13.89 / 0.550 / 0.539 / 1.095 63.52 14.44 / 0.201 / 0.445 / 0.375 15.57 / 0.590 / 0.478 / 0.852 13.65 14.92 / 0.220 / 0.377 / 0.357 17.08 14.79 / 0.511 / 0.440 / 0.411 160.2 14.59 / 0.510 / 0.450 / 0.562 123.9 14.29 / 0.515 / 0.473 / 0.331 114.2 42.01 52.02 14.65 / 0.522 / 0.467 / 0.310 18.24 Spin Teddy Wheel Average SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 14.32 / 0.407 / 0.445 / 1.191 219.1 133.9 13.10 / 0.392 / 0.490 / 1.482 71.80 14.89 / 0.413 / 0.441 / 1.362 26.84 15.53 / 0.433 / 0.366 / 1.265 12.51 / 0.516 / 0.562 / 1.095 318.7 11.90 / 0.354 / 0.484 / 1.623 239.2 11.20 / 0.508 / 0.573 / 1.460 117.1 11.79 / 0.345 / 0.394 / 1.732 106.1 12.31 / 0.509 / 0.605 / 1.156 80.44 10.83 / 0.339 / 0.538 / 2.007 96.50 12.56 / 0.521 / 0.598 / 1.056 12.28 12.44 / 0.374 / 0.413 / 1.561 16. 13.90 / 0.464 / 0.479 / 0.923 232.4 13.72 / 0.461 / 0.430 / 1.029 109.4 78.54 13.72 / 0.460 / 0.509 / 0.988 14.60 / 0.480 / 0.443 / 0.837 18.37 interp - Aleks-teapot interp - Chickchiken interp - Cut-lemon interp - Hand (b) Hypernerf dataset SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 426.0 24.86 / 0.854 / 0.186 / 5.406 108.0 20.13 / 0.625 / 0.479 / 11.00 26.99 / 0.853 / 0.193 / 3.309 105.6 55.69 26.72 / 0.871 / 0.162 / 3.074 26.05 / 0.781 / 0.239 / 4.176 25.89 / 0.782 / 0.272 / 4.539 26.88 / 0.797 / 0.336 / 7.036 50.34 26.65 / 0.793 / 0.271 / 4.884 130.8 28.97 / 0.859 / 0.192 / 4.206 101.2 29.63 / 0.862 / 0.182 / 2.469 50.77 28.61 / 0.792 / 0.269 / 3.936 82.65 28.91 / 0.855 / 0.191 / 4.574 29.87 / 0.847 / 0.223 / 4.928 29.65 / 0.867 / 0.187 / 4.355 30.17 / 0.776 / 0.325 / 5.598 56.05 31.17 31.08 / 0.878 / 0.161 / 2.462 25.40 404.3 144.6 85.26 73. interp - Slice-banana interp - Torchocolate misc - Americano misc - Cross-hands SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 24.57 / 0.641 / 0.323 / 7.697 76.15 52.10 24.74 / 0.647 / 0.380 / 8.594 25.27 / 0.676 / 0.428 / 11.10 47.45 24.70 / 0.653 / 0.428 / 8.729 31. 27.62 / 0.893 / 0.155 / 2.640 217.0 27.47 / 0.890 / 0.171 / 2.924 84.52 25.44 / 0.829 / 0.301 / 6.784 91.10 27.34 27.86 / 0.896 / 0.136 / 2.657 30.84 / 0.928 / 0.101 / 3.055 271.4 28.78 / 0.844 / 0.198 / 2.209 222.1 30.87 / 0.929 / 0.094 / 2.896 142.8 141.6 27.70 / 0.813 / 0.246 / 2.683 31.30 / 0.917 / 0.137 / 3.706 85.72 62.10 28.06 / 0.763 / 0.350 / 6.644 43.99 28.39 / 0.821 / 0.253 / 4.545 23.97 30.55 / 0.932 / 0.100 / 2.934 misc - Espresso misc - Keyboard misc - Oven-mitts misc - Split-cookie SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 26.52 / 0.910 / 0.167 / 5.162 160.4 60.93 25.47 / 0.899 / 0.179 / 5.513 72.93 25.82 / 0.899 / 0.191 / 5.732 25.06 26.16 / 0.905 / 0.170 / 5.808 28.47 / 0.904 / 0.129 / 3.980 28.15 / 0.900 / 0.137 / 4.190 28.64 / 0.895 / 0.177 / 4.762 62.57 28.68 / 0.906 / 0.136 / 4.230 33.01 / 0.940 / 0.087 / 2.529 229.4 27.54 / 0.830 / 0.182 / 3.483 97.77 27.51 / 0.832 / 0.175 / 3.396 39.83 32.63 / 0.937 / 0.087 / 2.417 88.63 255.1 107.9 32.64 / 0.919 / 0.147 / 3.362 67.00 32.84 / 0.935 / 0.093 / 2.400 45. 27.99 / 0.801 / 0.316 / 6.241 45.73 25.63 27.78 / 0.820 / 0.220 / 4.630 20.03 misc - Tamping vrig - 3dprinter vrig - Broom vrig - Chicken SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 23.10 / 0.781 / 0.326 / 6.352 259.4 17.92 23.95 / 0.804 / 0.331 / 6.409 24.15 / 0.801 / 0.342 / 6.656 78.26 24.33 / 0.809 / 0.339 / 6.329 24.77 18.79 / 0.613 / 0.269 / 15.17 101.7 18.66 / 0.269 / 0.505 / 14.12 122.6 21.85 / 0.616 / 0.257 / 11.83 21.00 / 0.306 / 0.646 / 13.12 40.33 20.33 / 0.666 / 0.306 / 14.11 21.85 / 0.365 / 0.559 / 9.279 21.97 / 0.704 / 0.328 / 14.92 55.82 22.00 / 0.706 / 0.265 / 13.06 26.60 21.04 / 0.303 / 0.666 / 13.50 111.2 181.8 22.66 / 0.642 / 0.276 / 11.12 63.25 51.13 28.53 / 0.807 / 0.295 / 8.137 46.11 30.83 28.77 / 0.834 / 0.197 / 4.936 23.22 vrig - Peel-banana Average - interp Average - misc Average - vrig SC-GS [20] Deformable 3DGS [57] 4DGS [54] MoDec-GS (Ours) 519.9 25.49 / 0.806 / 0.215 / 4.568 268.0 26.93 / 0.851 / 0.193 / 4.386 27.66 / 0.847 / 0.206 / 4.179 93.02 29.80 28.25 / 0.873 / 0.171 / 3.801 26.95 / 0.815 / 0.213 / 4.432 231.9 86.97 22.72 / 0.616 / 0.355 / 10.68 138.3 25.96 / 0.766 / 0.294 / 5.929 87.13 28.04 / 0.873 / 0.178 / 3.929 25.00 / 0.680 / 0.347 / 9.131 61.52 27.44 / 0.797 / 0.302 / 6.459 72.65 28.37 / 0.857 / 0.237 / 5.301 67.76 27.78 / 0.827 / 0.219 / 4.360 40.82 28.39 / 0.875 / 0.187 / 4.411 29.90 25.01 / 0.679 / 0.324 / 8.827 27.61 226.0 28.32 / 0.876 / 0.170 / 3.824 212.3 21.19 / 0.575 / 0.311 / 11.42 Method Balloon1 Balloon2 Jumping dynamicFace 4DGS [54] MoDec-GS (Ours) 67.43 25.46 / 0.856 / 0.198 / 0 - 0 26.35 / 0.884 / 0.173 / 0 - 0 38. 27.12 / 0.842 / 0.151 / 0 - 0 58.36 27.18 / 0.875 / 0.101 / 0 - 0 41.37 22.43 / 0.842 / 0.264 / 0 - 0 46.19 23.14 / 0.858 / 0.226 / 0 - 0 29.09 123.8 27.32 / 0.935 / 0.121/ 0 - 0 29.65 / 0.955 / 0.094 / 0 - 0 46.57 4DGS [54] MoDec-GS (Ours) 22.17 / 0.743 / 0.215/ 0 - 0 81.94 23.35 / 0.817 / 0.149 / 0 - 0 49.41 Playground Skating 28.94 / 0.932 / 0.195 / 0 - 0 42.08 25.27 29.31 / 0.942 / 0.155 / 0 - 0 Truck 28.28 / 0.889 / 0.234 / 0 - 0 53.69 37.68 29.21 / 0.911 / 0.184 / 0 - 0 Umbrella 24.80 / 0.714 / 0.297 / 0 - 0 25.04 / 0.762 / 0.223 / 0 - 0 65.96 49.08 (c) Nvidia monocular Table 5. Quantitative results comparison on (a) iPhone [16], (b) HyperNeRF [45], (c) Nvidia [58] datasets. Red and blue denote the best and second best performances, respectively. Each block element of 5-performance denotes (PSNR(dB) / SSIM [53] / LPIPS [59] / tOF [7] Storage(MB)). For iPhone dataset, the masked metrics are used. For Nvidia monocular dataset, the tOF values are not calculated due to the teleporting artifacts present in the test views. 4 Figure 7. Visualization of GLMD. For cut-lemon scene in HyperNeRF [45] dataset, the rendered patch of Global CS, Local CS, and each time stamp are presented for fixed camera viewpoint. We also illustrate the optical flow color map between those patches to observe the captured motion at each deformation stage. At GAD stage, deformation in mainly found near objects with dominant motion (e.g., the lemon and knife), and the overall color trends are similar, indicating similar global motion direction. In contrast, at the LGD stage, motion is observed across the entire scene, with relatively more diverse range of motion directions. 5 Figure 8. Failure case: HyperNeRF-broom. In the face of challenges in reconstructing dynamic scenes from monocular video, there are limitations in adequately representing thin and highly intricate textured objects."
        }
    ],
    "affiliations": [
        "Chung-Ang University",
        "Electronics and Telecommunications Research Institute",
        "Korea Advanced Institute of Science and Technology"
    ]
}