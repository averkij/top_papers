{
    "paper_title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "authors": [
        "Yang Zhou",
        "Mingyu Zhao",
        "Zhenting Wang",
        "Difei Gu",
        "Bangwei Guo",
        "Ruosong Ye",
        "Ligong Han",
        "Can Jin",
        "Dimitris N. Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 2 9 2 7 7 1 . 1 1 5 2 : r M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark Yang Zhou* Mingyu Zhao* Zhenting Wang Difei Gu Bangwei Guo Ruosong Ye Ligong Han Can Jin Dimitris N. Metaxas Rutgers University {eta.yang, zhenting.wang, bangwei.guo}@rutgers.edu {ligong.han, can.jin, dnm}@rutgers.edu {mz751, dg1188, ry233}@scarletmail.rutgers.edu"
        },
        {
            "title": "Abstract",
            "content": "We present M3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce similaritydriven alignment that serializes each tool call, embeds signatures with sentence encoder, and performs similaritybucketed Hungarian matching to obtain auditable one-toone correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-theart Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmarks anonymous repository is at https:// github.com/EtaYang10th/Open-M3-Bench 1. Introduction We introduce M3-Bench, the Multi-Modal, Multiplex, Matching-aware MCP Benchmark, as principled evaluation suite for multimodal tool use under the Model Context Protocol (MCP). Multimodal Large Language Models (MLLMs) have recently shown that, once they are allowed to perform function calling (tool use), they can query ex- *Equal contribution. Figure 1. MLLMs on M3-Bench (higher is better). Summary leaderboard of state-of-the-art (SOTA) ternal services and reason over information that is not contained in their parameters [3, 7, 8, 18, 28, 40, 41, 46, 47, 51]. MCP specifies how models communicate with heterogeneous tools through standardized servers, which makes execution more reproducible across systems [27]. However, existing MCP benchmarks are mostly text-only and focus on linear API planning or database queries [10, 11, 22, 23, o M s c i r I d r k y F t c & - u d h a n ToolBench [34] BFCL v3 [33] τ -Bench [48] MCP-RADER [11] MCPEval [22] MCPToolBench++ [10] MCP-Atlas [39] MCP-Universe [23] MCP-Bench [43] M3-Bench (Ours) a 49 8 2 9 5 6 3451 24 28 42 19 1500+ 40+ 300+ 113 250 6 28 28 l m u t l S t Table 1. Comparisons to existing tool-using benchmarks (compact single-column). 33, 34, 39, 48]. systematic evaluation of multimodal MCP workflows, where images and text jointly condition tool calls and results, is still missing [2, 23]. The core challenge in real-world MCP trajectories is visual grounding: multimodal tool invocation hinges on correctly interpreting the image before any tool can be parameterized. In Figure 2a, the agent receives photograph and must first resolve the landmark/city from visual cues, only then can it condition subsequent MCP calls; The agent must fully recognize the fully stocked and well-displayed products on the shelf in photograph before proceeding to the next MCP tools call. Second, real-world MCP trajectories are not single-shot calls. They are multi-hop, they contain causally dependent operations, and they frequently execute several tool calls in parallel within one step [20, 25, 50, 52]. The task in Figure 2b illustrates this setting. The agent receives an image of hazardous construction scene and reporting-style instruction. Inside one step, actions that do not depend on each other, such as adding an image and updating bullets, can be executed concurrently. Across steps, operations that do depend on earlier results, such as annotating before inserting or creating the deck before saving, must follow the ground-truth order. These properties make simple string matching or linear-sequence scoring inadequate [25, 52], and it is precisely what our benchmark aims to stress. We define two important concepts in what follows: Multi-Hop refers to workflows with more than one causally dependent step, where later actions consume artifacts produced earlier [12, 49]; Multi-Threaded refers to order-independent tool calls executed within single step under shared state, allowing safe parallelism while preserving cross-step causality (Mathematical definition in 1 & 2). Table 1 contrasts M3-Bench with prior tool-use evaluations along 8 axes. Domains and Tools enumerate functional breadth and unique tool count. MCP ecosystem marks whether the benchmark connect the agent directly to set of production-grade, live MCP servers, and Information grounding indicates that answers must be supported by evidence returned from tools. Fuzzy task description denotes underspecified, natural instructions without clear trajectory. Critically, Multi-Hop & Threaded captures cross-step causal dependencies together with safe within-step parallelism; Multimodality requires joint imagetext conditioning; and Similarity Metric denotes large language models (LLM) free, similarity-aware alignment of predicted to reference calls (see Section 3). As discussed above, Multimodality and Multi-Hop/Threaded causality are central to realistic MCP workflows, and Similarity-Based scorer is necessary to credit semantically correct calls. Most existing benchmarks are text-only and/or lack explicit multithreaded causality, and none pair these with similarityaware alignment; in contrast, M3-Bench satisfies all three while retaining the key advantages of prior benchmarks. We align predicted and reference tool calls with similarity-bucketed Hungarian alignment: each call is serialized, embedded with fixed sentence encoder, and one-toone matched within tool-name buckets under weak/strong cosine thresholds. This gives deterministic, auditable correspondences without relying on an LLM judge for call-level scoring. On top of this alignment, we report compact, recall-aware suite that separates Section 3. small fourmodel judge ensemble is used for evaluating the overall quality of the trajectory to retain some of the advantages of the LLM judge feature. Besides, to standardize references, our experiments also provide best trajectory obtained via an ExecutorJudge loop. Contributions. 1. We present, to our knowledge, the first benchmark explicitly targeting multimodal MCP workflows. 2. Our repository provides an end-to-end pipeline for generating MCP best trajectories, with an optional lightweight human verification pass to enhance stability. 3. We introduce structure-aware metric suite that aligns tool calls via bucketed Hungarian matching on highdimensional feature similarities, yielding robust and auditable correspondences for effectively diagnosing distinct error modes. 4. We evaluate wide range of mainstream MLLMs and conduct comprehensive assessment across tasks, budgets, error types, and other factors. 2. Related Work 2.1. LLM Tool Use and Protocols Early studies showed that large language models (LLMs) can autonomously decide when/what/how to call external tools via simple APIs [1, 1517, 25, 26, 34, 40]. Reliability improves with API-aware finetuning and retrieval [32]; instruction/data scaling expands tool repertoires and evaluations (ToolBench) [34]; and hierarchical retrieval with self-reflection addresses large tool spaces and brittle pro- (a) (b) Figure 2. Overview of tasks and realistic multimodal MCP workflow. (a) Representative task samples in M3-Bench. (b) realistic multimodal MCP workflow: the agent (i) calls vision MCP server to detect and mark hazards in the input image, (ii) calls Imagesorcery MCP to process the image, and (iii) calls an office/productivity MCP server to create and populate slide deck. tocols (AnyTool/AnyToolBench) [9]. Benchmarks targeting the decision to use tool and tool selection (MetaTool Benchmark) complement these directions [14]. To reduce integration friction across heterogeneous tools, the Model Context Protocol (MCP) standardizes interfaces [2, 27]. 2.2. Benchmarks for Tool Use and MCP Beyond general agent evaluations such as AgentBench [21] and web-agent suites like WebArena and BrowserGym [19, 52], recent MCP-native efforts emphasize realistic, multi-hop use over live servers: MCP-Bench links LLMs to dozens of MCP servers and hundreds of tools [43]; MCP-Universe provides an end-to-end framework grounded in real-world servers and long-horizon tasks [36, 37]; MCPToolBench++ scales tool categories and tasks via marketplace-sourced servers [10]; and MCP-Atlas curates human-authored dataset and leaderboard for multitool workflows [38, 39]. Relative to these, we target multimodal tasks and introduce similarity-aware alignment with structure-sensitive diagnostics. 3. M3-Bench 3.1. Benchmark Construction Figure 3. Benchmark trajectory generation and judging pipeline. The Executor plans tools, executes calls with controlled multi-threaded execution, and summarizes; the Judge reconstructs an optimal trajectory and verifies correctness. where cℓ,nℓ denotes the nℓ-th tool call in step ℓ. Appendix Section 6.2. Via system prompts, the Executor receives the full tool list (with docstrings) and outputs the planned tool set for the current stepselection only to stabilize reasoning and keep the chain controllable. (See Appendix Prompt: Plan.) We generate benchmark trajectories with an ExecutorJudge loop that decouples tool execution from adjudication (Figure 3). For each imagetask pair, human instruction fixes intent and permissible tools. Given the prompt, image, and registry, the Executor produces stepwise trajectory; the Judge audits and consolidates it into standardized optimal trajectory for alignment. All prompts are in returns Process The Executor strictly valid JSON tool calls (each with name and arguments). multi-hop workflow allows multi-threaded interaction with the MCP Ecosystem within single step. Structured outputs are appended to the dialogue, and any file-image paths from MCP tools persist in temporary workspace for later steps. (See Appendix Prompt: Process.) 4. Metrics In Section 4.1, we first establish the mathematical definitions and foundations for our metrics. Building on these theoretical bases, we then construct the Detection Metrics Section 4.2, Trajectory Alignment Metrics Section 4.3, and LLM Judges Metrics Section 4.4. 4.1. Mathematical foundations trajectory is DAG ordered by turns (steps). Denote as the number of steps in trajectory . The ground-truth and predicted trajectories (cid:98)T are: = {S }, (cid:98)L}, where each step enumerates its calls (cid:98)T = { (cid:98)S1, . . . , (cid:98)S ℓ,1, . . . , ℓ = { c, }, (cid:98)Sˆℓ = { ˆcˆℓ,1, . . . , ˆcˆℓ,nˆℓ }. where ℓ is the step of the trajectory, so ℓ and nˆℓ are per-step call counts. call is = (tool(c), args(c)), where tool() is the categorical tool identifier and args() serializes all deterministic inputs. 1 , . . . , ℓ,n ℓ Definition 1 (Multi-Hop). Let = {S1, . . . , SL} be tooluse trajectory with steps Sℓ = {cℓ,1, . . . , cℓ,nℓ} and intercall dependency edges across steps. We call multi-hop if 2 and there exists dependency (ci,a cj,b) with < j, where a, and [1, ni], [1, nj]. Definition 2 (Multi-Threaded). step Sℓ is multi-threaded if it contains at least two calls cℓ,a = cℓ,b that are orderindependent given the shared state at depth ℓ (no dependency edge between them). These definitions make explicit that trajectory is structured object with cross-step causal edges and withinstep parallelism. general option is to delegate this alignment to an LLM judge, but it introduces (i) circularity/lineage bias, where judges trained like the systems under test tend to over-credit them; (ii) susceptibility to prompt gaming, as long rationales or CoT responses can inflate scores even when the underlying tool use is wrong; (iii) weak argument verification, since judges rarely check whether argument objects are semantically correct or whether order- (iv) knowledge substitution, invariant sets are handled; where missing tool evidence is replaced by the models own world knowledge; and (v) opacity, as scores lack auditable, one-to-one correspondences between predicted and reference calls. Real MCP trajectories exacerbate these issues: steps often contain unordered sets of calls, later calls can self-correct earlier errors, and partial success is common. Consequently, the alignment primitive must be deterministic, auditable, and robust to surface-form variation. Similarity-Bucketed Hungarian Alignment To address this problem, we introduce similarity computation method based on text encoder. Each call is flattened to canonical text string by serialization function ϕ() that preserves Figure 4. Overview of MCP server ecosystem used in the M3-Bench. Category distribution of MCP servers; and tool distribution across servers. The inventory follows the MCP design and public specification. Decision and Final Reply After each step, binary prompt checks whether evidence suffices to conclude; if so, the Executor emits only the final natural-language answer. (See Appendix Prompt: Decision, Prompt: Final.) Judge and Summarize The Judge consumes the initial prompt, image, and full Executor dialogue (including the tool-call chain). Trajectories with causal misordering or factual errors are rejected; otherwise, the Judge reconstructs the optimal trajectory, removing failed/redundant calls and merging operations that could run concurrently. Human verifiers audit scope and logical consistency; the audited result becomes the benchmark reference. (See Appendix Prompt: Judge, Prompt: Summarize.) 3.2. Task Distribution We construct curated suite of 28 multimodal MCP tasks; detailed per-task goals and inputoutput constraints are provided in Appendix Section 6.1, and representative full trajectories appear in Section 7. In total, the evaluation set contains 208 trajectories, whose reference trajectories comprise 644 steps and 1337 MCP tool calls. 3.3. MCP Ecosystem To keep trajectories controllable and reproducible, we curate compact set of MCP servers and tools with minimal functional overlap. We favor orthogonal, well-documented, and stable utilities, discarding redundant or unstable options to reduce confounds during evaluation. Our benchmark spans 231 representative tools across 28 MCP servers (Figure 4(a)). This diverse ecosystem covers Academic & Knowledge, Science & Space, Geography & Travel, Ecommerce & Finance, Office Automation, Weather & Air Quality, and Health, enabling broad capability coverage in M3-Bench (see Figure 4(b)). ,(cid:98)xj 2 (cid:98)xj 2 tool name, required arguments and order-invariant sets. We embed serialized calls with sentence encoder and compute cosine similarity: Sij = [1, 1], where )(cid:1) and (cid:98)xj = E(cid:0)ϕ(ˆcj)(cid:1). = E(cid:0)ϕ(c We align with tool-specific, step-agnostic assignment that yields auditable one-to-one correspondences. Concretely, we serialize every call into canonical text signature and embed it with the sentence encoder SentenceTransformers [35] from Textual Encoding and Similarity, giving similarity matrix S. For each tool k, we form the bucketed index sets Ik = { tool(c ) = }, (cid:98)Jk = { tool(ˆcj) = }, so that only same-tool pairs are considered. We convert similarity to cost with weak screening threshold τweak to forbid low-similarity edges: (cid:40) (k) rc = 1 S(k) λpen, rc , S(k) rc τweak, S(k) rc < τweak, (1) and solve linear assignment in each bucket via the Hungarian (KuhnMunkres) algorithm minΠ(k)P (k), Π(k) retaining only feasible pairs above the weak threshold to form the global match set: = {(i, j, Sij) Sij τweak}. Properties and rationale. Tool bucketing prevents crosslatertool credit; step agnosticism allows early-wrong, correct fixes to match without forcing same-step alignment; and similarity gating recognizes paraphrase and orderinvariant arguments without inflating coverage. Together, these choices deliver: (i) deterministic, auditable correspondences in place of LLM judge; (ii) invariance to withinstep call permutations (sets are unordered); and (iii) faithful scoring under partial success, misaligned or missing calls simply remain unmatched, while correctly recovered calls still receive credit. This alignment thus targets exactly the failure modes we observe in MCP trajectories while satisfying the three requirements in Section 4.1. 4.2. Detection Metrics Based on the above mathematical foundations, we introduce detection-based metrics: 1. Systems may call incorrect tools or omit required ones, motivating Recall; 2. MLLM may make unnecessary calls, resulting in additional waste of resources Precision. Figure 5. Overview of our structure-sensitive metrics. where ℓ and (cid:98)Sˆℓ denote the sets of calls in the ground-truth step ℓ and the predicted step ˆℓ, respectively, and ℓ -th ground-truth call in ℓ,n ℓ step ℓ and the nˆℓ-th predicted call in step ˆℓ. denote the and ˆcˆℓ,nˆℓ and Npred = (cid:80) (cid:98)T , respectively. ˆℓ nˆℓ be the total numbers of calls in and 4.3. Trajectory Alignment Metrics Next, we introduce metric to quantify the degree of Trajectory Alignment. 1. MLLM may select the correct tool but supply semantically incorrect arguments, requiring an Average Similarity measure over serialized calls; 2. MLLM may unnecessarily split operations that could run in single step in multi-threaded scenario, captured by Step Coherence; 3. MLLM may collapse causally distinct multi-hop subgoals into one step in multi-hop scenario, quantified by Merge Purity; 4. MLLM may invert the execution order of causally dependent steps in multi-hop scenario, detected by Order Consistency. Recall and Precision. (Figure 5 (a)-(b)) These metrics quantify, respectively, how many ground-truth calls are successfully matched (coverage) and how many predicted calls are valid (false-positive control). Given per-sample match sets Mm with counts Ngt,m and Npred,m, we define Recall = , where Mm counts matched pairs for sample m, Ngt = (cid:80) ℓ ℓ (cid:80) Mm Npred,m , Precision = Mm Ngt,m (cid:80) (cid:80) (cid:80) Argument Similarity. We summarize the semantic faithfulness of matched arguments as (cid:80) ArgSim = (i,j)M, Sij τstrong {(i, j) : Sij τstrong} , (2)"
        },
        {
            "title": "Sij",
            "content": "where τstrong controls alignment confidence. Step Coherence. (Figure 5 (c)) This metric penalizes scattering matches from one ground-truth step across mulModel Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Qwen2.5-VL-72B [6] InternVL 3.5 [42] Gemini 2.5 Flash Lite [8] Claude 4.5 Haiku [4] GPT-5 Nano [30] Llama-4-Scout-17B16E [24] Grok-4 Fast [45] Claude 4.5 Sonnet [5] Gemini 2.5 Flash [8] GPT-5 Mini [29] Grok 4 (0709) [44] Gemini 2.5 Pro [8] GPT-5 [31] 0.013 0.143 0.224 0.200 0.193 0.287 0.367 0.343 0.395 0.454 0.481 0.465 0.501 0.627 0.023 0.073 0.143 0.173 0.480 0.103 0.176 0.359 0.361 0.397 0.238 0.387 0.449 0.264 0.011 0.125 0.193 0.168 0.166 0.276 0.325 0.309 0.354 0.409 0.462 0.408 0.433 0. 0.012 0.125 0.191 0.179 0.190 0.228 0.286 0.284 0.389 0.357 0.392 0.415 0.464 0.502 0.000 0.022 0.048 0.030 0.053 0.045 0.068 0.122 0.127 0.211 0.199 0.177 0.249 0.290 0.013 0.112 0.161 0.152 0.108 0.258 0.290 0.289 0.257 0.412 0.360 0.390 0.424 0.453 0.285 0.303 0.325 0.299 0.412 0.551 0.367 0.501 0.535 0.530 0.652 0.664 0.520 0.671 0.130 0.388 0.298 0.241 0.246 0.529 0.335 0.378 0.451 0.474 0.633 0.638 0.438 0.657 Average Score 0.029 0.141 0.179 0.180 0.205 0.247 0.264 0.298 0.333 0.388 0.395 0.411 0.423 0.482 Table 2. Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories: Detection, Trajectory Alignment, and LLM-Judge Metrics, The average score results correspond to Figure 1. tiple predicted steps, reflecting unnecessary splits of operations that could have been executed within single multithreaded step. For any call c, let gtst(c) and predst(c) denote its step indices in the ground-truth and predicted trajectories. For each ground-truth step ℓ, collect the predicted step indices touched by its matches: P(ℓ) = { predst(ˆcj) (i, j) M, gtst(c ) = ℓ }.Then we define the per-step coherence and similarity-weighted global average as P(ℓ) 1, , otherwise, (3) sc(ℓ) = 1, 1 P(ℓ) (cid:80) StepCoh = ℓ wℓ sc(ℓ) (cid:80) ℓ wℓ , where wℓ = (cid:80) (i,j)M [gtst(c nating from ground-truth step ℓ. ) = ℓ] counts matches origiMerge Purity. (Figure 5 (d)) This metric is used to evaluate whether, in multi-hop scenario, different causally related steps are incorrectly merged. For total alignment weight between ground-truth step and predicted step b: Wab = (cid:88) (i,j)M [gtst(c ) = a] [predst(ˆcj) = b] Sij, (4) where [] denotes the Iverson bracket (indicator function), equal to (1) when its condition is true and (0) otherwise. Define column sums Sb = (cid:80) Sb, and column prior (b) = Sb/S. The conditional distribution of ground-truth steps given predicted step is qab = Wab/Sb for active columns with Sb > 0. The normalized purity is: Wab, total mass = (cid:80) MergePur = 1 H(G ) = (cid:88) H(G ) log Gact (cid:16) (b) (cid:88) qab log qab (5) (cid:17) , b:Sb>0 a:Wab>0 where Gact = {a : (cid:80) Wab > 0} counts active groundtruth steps, H() uses natural logarithm, and larger values indicate predicted steps concentrating mass on single ground-truth steps (i.e., less over-merging); denotes the random variable of ground-truth step indices and denotes the random variable of predicted step indices; Gact = {a : (cid:80) Wab > 0} is the number of active ground-truth steps. The normalization by log Gact maps [0, log Gact] to [0, 1]. Order Consistency. (Figure 5 (e)) This metric is used to evaluate whether, in multi-hop scenario, the order of causally related calls is incorrectly swapped, capturing order consistency across steps. Let each match (i, j) be represented by its step pair (a, b) = (gtst(c ), predst(ˆcj)). Over all unordered pairs of matches that are comparable in both axes, we compute: OrdCons ="
        },
        {
            "title": "1\nQ",
            "content": "(cid:88) (u,v) (cid:2)(au av)(bu bv) < 0(cid:3), (6) where the sum runs over pairs with au = av and bu = bv, is the number of such pairs, and the indicator records inversions between the ground-truth and predicted step orders; higher values indicate fewer inversions. To penalize degenerate cases with very few matches (weak systems that match only few easy MCP tools from inflating structure-aware metrics, leading to very small denominator in the metric calculation, and thus an inflated outcome), we weight per-sample metrics by their recall. Let Model Input Tokens Output Tokens Model Avg. Steps Avg. Tool Calls Qwen2.5-VL-72B InternVL 3.5 Claude 4.5 Haiku Claude 4.5 Sonnet GPT-5 Nano Llama-4-Scout-17B16E Grok-4 Fast Gemini 2.5 Flash Lite Gemini 2.5 Flash GPT-5 Mini Grok 4 (0709) Gemini 2.5 Pro GPT-5 0.362 0.069 0.118 0.408 0.002 0.345 0.210 0.477 0.621 0.125 0.132 0.606 0.252 0.008 0.002 0.007 0.029 0.001 0.008 0.028 0.010 0.032 0.014 0.027 0.031 0.037 Table 3. Average token cost per task for each MLLM in millions. GLM 4.5v Qwen2.5-VL-72B InternVL 3.5 Claude 4.5 Haiku Claude 4.5 Sonnet GPT-5 Nano Llama-4-Scout-17B16E Grok-4 Fast Gemini 2.5 Flash Lite Gemini 2.5 Flash GPT-5 Mini Grok 4 (0709) Gemini 2.5 Pro GPT-5 1.91 4.29 3.65 0.80 2.74 5.13 4.72 2.91 2.73 4.04 3.72 3.21 3.04 3. 3.84 12.30 9.57 2.73 6.99 17.97 13.26 5.93 7.06 7.12 12.56 7.64 6.95 14.77 the per-sample recall be rm = Mm . For any per-sample Ngt,m metric Fm {StepCoh, MergePur, OrdCons}, we report recall-covered score: cov = (cid:80) . Ngt,m rm Fm (cid:80) Ngt,m 4.4. LLMs Judges Metrics To leverage the advantages of large language models (LLMs) in complex logical reasoning, we introduce metric based on LLMs Judges to assess: 1. Whether the MLLM achieves Task Completion; 2. Whether the answer provided by the MLLM possesses MCP Information Grounding. Task Completion. For each instance m, fixed set of four LLMs Judges produces scalar scores {s(j) }jJ [0, 1]. We compute trimmed mean that discards the highest and lowest scores and averages the remaining two (Prompt: Appendix Section 6.2): TaskCompm = 1 2 (cid:88) s(j) , jJ {jmin, jmax} (7) jmin = arg min jJ s(j) , jmax = arg max jJ s(j) . We report TaskComp (Task Completion) as the mean of per-instance scores over the evaluation set. The judge pool includes strong large language models (e.g., GPT, Gemini, DeepSeek, Grok). Information Grounding. Judges are additionally shown both the ground-truth and predicted trajectories and emit For instance with step-level groundedness scores. ground-truth steps indexed by ℓ and ˆℓ, each judge outputs g(j) [0, 1]. The calculation of InfoGrndm,ℓ,ˆℓ follows Equation 7 (Prompt: Appendix Section 6.2). m,ℓ,ˆℓ Table 4. Average interaction steps and tool calls per task across models on our benchmark. 5. Experiment 5.1. Evaluation Results As shown in Table 2, we evaluate SOTA MLLMs on M3Bench. The three flagship families, GPT-5 [31], Gemini, and Grok, form the leading cluster under our coverageweighted alignment and judge signals. GPT-5 [31] attains the highest Average Score (0.482) with strong Recall (0.627) and high Argument Similarity (0.583), and it also tops all structure-aware metrics: its multi-threaded Step Coherence (0.502) and multi-hop sensitive Order Consistency / Merge Purity (0.290 / 0.453) indicate that it preserves both within-step parallel branches and cross-step merges in long tool graphs. Gemini 2.5 Pro [8] (0.423) and Grok 4 (0709) [44] (0.411) exhibit similarly balanced profiles. In contrast, compact/latency-oriented variants tend to trade coverage and structural consistency for speed: for example, Claude 4.5 Haiku [4] achieves high Precision (0.480) but much lower Recall, and its structure metrics remain modest, pointing to short, locally correct trajectories that under-utilize multi-threaded branches and often break multi-hop dependencies. Finally, GLM 4.5v [13], Qwen2.5-VL-72B [6], and InternVL 3.5 [42] lag notably on both coverage and structure, suggesting that MLLMs SFT objectives favor pure visionlanguage understanding over instruction following and trajectory planning. 5.2. Per-Task Analysis Figure 7 shows that model capabilities are far from uniform across selected tasks (details in Section 6.1): curves cross frequently, indicating complementary strengths rather than single, uniformly dominant system. Grok-4 is weak on Fruit Cal (numerical calibration/unit handling) yet strong on QR-code/Attraction (optical character recognition (OCR) and points-of-interest (POI) chaining). GPT-5 trails on Vision Research (fine-grained figure grounding) but dominates Biology Cut (precise localization/cropping). Figure 6. Per-call outcome distributions across models when replaying predicted MCP trajectories through the MCP interpreter. Gemini 2.5 Flash [8] (4.04 / 7.12), and Grok-4 Fast [45] (2.91 / 5.93) reach goals with fewer, more precise calls, consistent with their higher detection precision. Very short dialogues (e.g., Claude 4.5 Haiku [4], GLM 4.5v [13]) reflect under-coverage rather than efficiency. 5.4. Analysis of Error Distributions We assess per-call reliability by replaying each models trajectory through the MCP interpreter and categorizing every tool call into one of five mutually exclusive outcomes shown in Figure 6: (i) Illegal calling format: the MCP request cannot be parsed into valid schema (e.g., missing name or arguments); (ii) Unknown Tool Invocation: the request targets non-existent or non-exposed tool; (iii) Invalid Invocation Arguments: the tool is found but arguments are syntactically/semantically invalid (typically surface as HTTP 400 or [Tool error] Invalid arguments); (iv) Successful (Resource Not Found): the call is structurally correct yet the backend cannot route/locate the requested resource (e.g., HTTP 404); and (v) Successful call: the tool executes and returns normal result. These categories allow us to decouple schema conformance, tool selection, argument fidelity, and downstream retrieval robustness. Overall, higher-performing systems allocate larger share to Successful call in Figure 6, with the GPT-5 Grok and Gemini families exhibiting the strongest success rates in our setting. For Qwen2.5-VL-72B [6] and GLM-4.5V [13], the shortfall appears driven less by multi-hop reasoning capacity and more by tool-use hygiene: hallucinationtriggered invocations of non-existent tools and weaker adherence to instruction/schema constraints (the trajectory error analysis in Appendix Error: GLM-4.5v, Error: Qwen2.5-VL, Error: Internvl-3.5). This pattern suggests that improvements in schema guidance and instruction following would likely yield larger gains than refining highlevel reasoning alone. Although the Claude family is generally regarded as possessing strong capabilities in coding, mathematics, and reasoning, its cross-modal prior knowlFigure 7. Average score of each selected task (details in Section 6.1) in the representative model. Gemini 2.5 Pro excels on Travel Comparison (web aggregation and itinerary synthesis). Claude 4.5 lags on vision-dependent tasks (e.g., Items Price), suggesting comparatively weaker multimodal grounding; disparities align with differences in OCR robustness. In Section 9 (See Appendix), we present the performance of different models on each task, based on the evaluation metrics of M3-Bench. 5.3. Budgets across models. Table 3 complements these interaction statistics by reporting average per-task input and output token budgets. Table 4 summarizes the average interaction depth per task. Larger models often pursue longer, multi-hop plans: GPT-5 [31] and GPT-5 Nano [30] average (3.55 / 14.77) and (5.13 / 17.97) (steps / tool calls), indicating frequent retries and non-trivial orchestration; Llama-4-Scout-17B16E [24] and Qwen2.5-VL-72B [6] show similarly heavy tool use. By contrast, Gemini 2.5 Pro [8] (3.04 / 6.95), edge is not particularly prominent Section 8. 6. Conclusion We introduced M3-Bench, benchmark tailored to multimodal tool use under the Model Context Protocol (MCP), with trajectories that support multi-hop and multi-threaded execution. Our evaluation pipeline aligns predicted and reference calls by serializing each call, embedding it with text encoder, and performing tool-namebucketed Hungarian matching to obtain auditable one-to-one correspondences. On this alignment, we report structure-aware suite of interpretable metrics. Across representative MLLMs, results reveal persistent gaps in argument fidelity and structural consistency, highlighting the need for agents that jointly reason over images, text, and tool graphs."
        },
        {
            "title": "References",
            "content": "[1] Pierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja, Ricardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, JeanBaptiste Gaya, et al. Are: Scaling up agent environments and evaluations. arXiv preprint arXiv:2509.17158, 2025. 2 [2] Anthropic. Introducing the model context protocol. https: //www.anthropic.com/news/modelcontextprotocol, 2024. Accessed: 2025-10-30. 2, 3 [3] Anthropic. Introducing claude 4. https : / / www . anthropic.com/news/claude-4, 2025. Product announcement; system card available; no peer-reviewed venue acceptance announced as of 2025-11-10. 1 [4] Anthropic. Claude haiku 4.5 system card. https://www. anthropic.com/claudehaiku45- systemcard, 2025. Oct. 2025, official system card; accessed 202511-10. 6, 7, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [5] Anthropic. Claude sonnet 4.5 system card. https : //www.anthropic.com/claudesonnet45system-card, 2025. Oct. 2025, official system card; accessed 2025-11-10. 6, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [6] Shuai Bai, Keqin Chen, Xuejing Liu, and et al. Qwen2.5-VL technical report, 2025. Covers Qwen2.5-VL-72B flagship variant. 6, 7, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [7] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint, 2025. Status: arXiv preprint; no peer-reviewed venue acceptance found as of 2025-11-10. 1 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next-generation agentic capabilities. arXiv preprint, 2025. Status: arXiv preprint; no peer-reviewed venue acceptance found as of 2025-11-10. 1, 6, 7, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [9] Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: SelfIn reflective, hierarchical agents for large-scale API calls. Proceedings of the 41st International Conference on Machine Learning, pages 1181211829. PMLR, 2024. 3 [10] Shiqing Fan, Xichen Ding, Liang Zhang, and Linjian Mo. MCPToolBench++: large scale ai agent MCP tool use benchmark. arXiv preprint arXiv:2508.07575, 2025. 1, 2, [11] Xuanqi Gao, Siyi Xie, Juan Zhai, Shiqing Ma, and Chao Shen. MCP-RADAR: multi-dimensional benchmark for evaluating tool-use capabilities in large language models. arXiv preprint arXiv:2505.16700, 2025. 1, 2 [12] Jie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Z. Pan. Mintqa: multi-hop question answering benchmark In arXiv for evaluating llms on new and tail knowledge. preprint arXiv:2412.17032, 2024. 2 [13] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, and et al. Glm4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. 6, 7, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [14] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Z. Gong, and Lichao Sun. Metatool benchmark for large language models. 2024. 3 [15] Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris N. Metaxas, and Tong Che. Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning. arXiv preprint arXiv:2504.09772, 2025. 2 [16] Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, and Dimitris N. Metaxas. Apeer: Automatic prompt engineering enhances large language model reranking. In WWW Companion 2025 - Companion Proceedings of the ACM Web Conference 2025, pages 24942502, Sydney, Australia, 2025. Association for Computing Machinery. [17] Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, and Dimitris N. Metaxas. Your reward function for rl is your best prm for search: Unifying rl and search-based tts. arXiv preprint arXiv:2508.14313, 2025. [18] Team Kimi, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint, 2025. Status: arXiv preprint; no peer-reviewed venue acceptance found as of 2025-11-10. 1 [19] Thibault Le Sellier De Chezelles et al. gym ecosystem for web agent research. arXiv:2412.05467, 2024. 3 The browserarXiv preprint [20] Bin Li, Wenhai Wang, Jiayi Yang, Shilong Zhang, Jian Sun, and Hongyang Li. Seed-bench: Benchmarking multimodal In Proceedings of the IEEE/CVF large language models. Conference on Computer Vision and Pattern Recognition, pages 111, 2024. 2 [21] Xiao Liu et al. Agentbench: Evaluating llms as agents. 2024. [22] Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, et al. MCPEval: Automatic MCPbased deep evaluation for AI agent models. arXiv preprint arXiv:2507.12806, 2025. 1, 2 [23] Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. MCP-Universe: Benchmarking large language models with real-world model context protocol servers, 2025. 1, 2 [24] Meta AI. Llama 4 scout 17b-16e model card. https: / / huggingface . co / meta - llama / Llama - 4 - Scout-17B-16E, 2025. HF official model card; release Apr. 5, 2025; accessed 2025-11-10. 6, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [25] Gregoire Mialon, Clementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In International Conference on Learning Representations, 2023. 2 [26] Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, and Le Sun. Livemcpbench: Can agents navigate an ocean of mcp tools? arXiv preprint arXiv:2508.01780, 2025. 2 [27] Model Context Protocol Working Group. Model https : context //modelcontextprotocol.io/specification/ 2025-06-18, 2025. Accessed 2025-10-22. 1, 3 specification. protocol (mcp) [28] OpenAI. Introducing o3 and o4-mini. https://openai. com / index / introducing - o3 - and - o4 - mini/, 2025. Product blog; not peer-reviewed publication; no venue acceptance announced as of 2025-11-10. 1 [29] OpenAI. GPT-5 mini. https://platform.openai. com/docs/models/gpt5mini, 2025. API docs; accessed 2025-11-10. 6, 14, 15, 16, 17, 18, 19, 20, 21, 22, [30] OpenAI. GPT-5 nano. https://platform.openai. com/docs/models/gpt5nano, 2025. API docs; accessed 2025-11-10. 6, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [31] OpenAI. GPT-5 system card. https://cdn.openai. com/gpt5systemcard.pdf, 2025. Aug. 2025, official system card; accessed 2025-11-10. 6, 7, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [32] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. 37, 2024. 2 [33] Shishir G. Patil, Huanzhi Mao, Fanjia Yan, Charlie ChengJie Ji, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Proceedings of the 42nd International Conference on Machine Learning, 2025. 2 [34] Yujia Qin et al. Toolllm: Facilitating large language modarXiv preprint els to master 16,000+ real-world apis. arXiv:2307.16789, 2023. [35] Nils Reimers and Iryna Gurevych. Sentence-BERT: SenIn Protence embeddings using siamese BERT-networks. ceedings of EMNLP-IJCNLP, pages 39823992, 2019. 5 [36] Salesforce AI Research. MCP-Universe: Benchmarking large language models with real-world MCP servers. https://github.com/SalesforceAIResearch/ MCP-Universe, 2025. Accessed 2025-10-22. 3 [37] Salesforce AI Research. MCP-Universe: comprehensive framework for ai agent development and benchmarking. https://mcp-universe.github.io/, 2025. Accessed 2025-10-22. 3 [38] Scale AI. MCP-Atlas leaderboard/dataset overview. https://scale.com/leaderboard/mcp_atlas, 2025. Accessed 2025-10-22. [39] Scale AI Research. MCP-Atlas raises the bar for agentic evaluation. https://scale.com/blog/mcp-atlas, 2025. Accessed 2025-10-22. 2, 3 [40] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In International Conference on Learning Representations, 2023. 1, 2 [41] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving AI tasks with ChatGPT and its friends in huggingface. In Advances in Neural Information Processing Systems, 2023. 1 [42] Weiyun Wang, Zhangwei Gao, Lixin Gu, and et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency, 2025. 6, 7, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [43] Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, and Eugene Siow. MCPBench: Benchmarking tool-using llm agents with comarXiv preprint plex real-world tasks via MCP servers. arXiv:2508.20453, 2025. 2, [44] xAI. Grok 4 model card. https://x.ai/docs/ model-cards/grok-4, 2025. Initial public release July 9, 2025 (0709); accessed 2025-11-10. 6, 7, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [45] xAI. Grok 4 fast model card. https://x.ai/docs/ modelcards/grok4fast, 2025. Official model card; accessed 2025-11-10. 6, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 [46] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, et al. Qwen3 technical report. arXiv preprint, 2025. Status: arXiv preprint; no peer-reviewed venue acceptance found as of 2025-11-10. 1 [47] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023. 1 [48] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik R. Narasimhan. τ -bench: benchmark for tool-agent-user inIn The Thirteenth Interteraction in real-world domains. national Conference on Learning Representations (ICLR), 2025. 2 [49] Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiecao Chen. Toolhop: query-driven benchmark for evaluating large language models in multi-hop tool use. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025. 2 [50] Wenxuan Yu, Zeyu Yang, et al. Mm-vet v2: challenging benchmark to evaluate large vision-language models. arXiv preprint arXiv:2408.00765, 2024. [51] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint, 2025. Status: arXiv preprint; no peer-reviewed venue acceptance found as of 2025-11-10. 1 [52] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In International Conference on Learning Representations, 2024. 2, 3 M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark"
        },
        {
            "title": "Supplementary Material",
            "content": "6.1. Task Descriptions 1. Use wiki/summary to summarize the detected fish (0) Items Price Estimate prices for all visible retail items. 1. Identify product names from the image (OCR/visual grounding as needed). 2. Use Amazon MCP (search products) to retrieve current prices. 3. Aggregate results into concise table. (1) Attraction Resolve the attraction, surface context, and plan around-visit options. 1. Use Wikipedia MCP to obtain the city/background of the attraction. 2. Call weather/get weather for current conditions. 3. Use google-maps/places nearby search to find nearby restaurants."
        },
        {
            "title": "Identify the movie and retrieve showtimes",
            "content": "(2) Movies contextually. 1. Use OCR to read the movie title in the image. 2. Query TMDB to disambiguate the title. 3. Search cinemas near New York showing the resolved film via Google Maps MCP. (3) Vision Research Bridge detected fauna to literature snippets. 1. Detect animals and identify two distinct categories from detections. species. 2. Use wiki/search to search for the top spot for fishing in NJ recently. 3. Call weather/get weather for local conditions at the spot. 4. Use google-maps/compute route to plan route from default start. 5. Use google-maps/places nearby search to search for restaurants and bait shops. (6) Drug Compare medicines for given condition safely and conveniently. 1. Look up drug facts via fda drug lookup MCP. 2. Search similar products via Amazon (search products). 3. Create comparison list using Excel MCP. (7) Hazard Document construction-safety hazards with visual annotations and slides. 1. Run detector to find hardhats, vests, ladders, and unguarded edges. 2. Use imagesorcery-mcp/draw circles to annotate detections. 3. Create presentation with ppt/create presentation, ppt/add slide and ppt/manage image. (8) Food Reddit Wiki Excel Quantify food popularity and descriptions, then export to Excel. 1. For each food, query Reddit MCP for the hottest post 2. For each, call paper search/search arxiv with targeted and extract comment count. query. 3. Download and read the paper; extract the shortest Abstract sentence containing the animal name. 2. Use Wiki MCP to get brief description. 3. Create workbook and write two columns (Food, Wikipedia Description) via Excel MCP; save the file. (4) Camping Plan 3-day national-park camping itinerary with verified amenities and logistics. 1. Use nationalparks/getCampgrounds and nationalparks/getAlerts to select an open campground with 3 amenities and no active alerts. 2. Call nationalparks/getVisitorCenters and pick the closest visitor center; record hours. 3. Use google-maps/compute route, distance matrix, and elevation to compute routes and elevation; compile the itinerary. (5) Fish Plan New Jersey fishing trip for the detected species. (9) MedExcel Summarize PubMed signal into key terms in Excel. 1. Use pubmed search to retrieve the first three titles for the drug/disease. 2. Double-check whether each title is semantically relevant to the queried drug/disease. 3. Write the drug/disease name and the relevant paper count to an Excel file under ./media and save. (10) Barcode Lookup Identify the book via barcode and fetch bibliographic data. 1. Decode the barcode using pyzbar. 2. Query openlibrary mcp to return the book record. (11) Biology Cut Produce one image per detected animal instance. 1. Detect all animals in the image. 2. Crop each detection to an individual image. (12) Fruit Cal Assess daily calories and potential longterm risks from fruit-only meals. 1. Use Nutrition MCP to estimate total daily calories from the shown fruits. 2. Use Healthcare MCP to assess long-term health concerns of such diet. (13) Medical Recommend safe medicines with purchase links conditioned on ICD-10. 1. Use lookupICDCode MCP to validate the ICD-10 code. 2. Use fda drug lookup and clinical trials search to check safety/compatibility. 3. Suggest appropriate options via Amazon MCP with links. (18) Travel Comparison Choose the cleaner destination between two landmarks and suggest nearby venues. 1. Recognize both landmarks and geocode each via Google Maps MCP. 2. Use google-air/current conditions to compare AQI and pollutant levels. 3. Recommend the cleaner destination; list nearby cinemas and hotels. (19) Heatmap Generate an air-pollution heatmap visualization for the depicted city. 1. Geocode the city via Google Maps MCP. 2. Render static base map (static map) with zoom=10. 3. Overlay heatmap PM2.5 tiles airvia quality/heatmap tile (indexType=US AQI). (20) Museum Retrieve Met object and attach concise Wikipedia summary. 1. Call metmuseum-mcp/list-departments and select the best-aligned departmentId. (14) QR Attraction Infer location from QR-sourced page and report air quality and guidance. 1. Use QR scanner tool to extract the URL; fetch the pri2. Call metmuseum-mcp/search-museum-objects (hasImages=true, departmentId) and select an objectId. 3. Call metmuseum-mcp/get-museum-object to get title, mary image. 2. Infer the location and geocode it via Google Maps MCP. 3. Query google-air/current conditions for AQI and health recommendations. (15) Routine Plan multi-stop route with supermarket stop and choose the fastest option. 1. Geocode the start, the image landmark, and the destination via Google Maps MCP. 2. Search for nearby supermarkets as waypoints. 3. Compare 3 candidate routes (compute route or directions) and select the fastest. (16) Biology Chain Evaluate whether detected organisms can form plausible food chain. 1. Detect all organisms; blur any human instances. 2. Use Wikipedia to retrieve ecological roles (diet, trophic level). classification, and image URL. 4. Use wiki/search then wiki/summary to obtain short cultural summary. (21) Research Station Relate solar flare events to nearby research stations for the inferred location. 1. Use nasa-mcp/get notifications to filter M5+ flares within the specified window. 2. For each event date, get coronal mass ejection; CME IDs/types. photo location; 3. Infer call and get solar flare store flux/peak time and mcp/places text search and place details to list research stations and compile table. use google-mapstop (22) Hugeicons Map salient objects to Wikipedia titles and validated icons. 1. Use dinox-mcp/detect-all-objects and keep detections 3. Check whether roles form valid chain. with area > 10,000. (17) Generate PowerPoint Recreate title slide with style and optional Q&A enhancement. 1. OCR the slide and segment into blocks; estimate font color/size/style. 2. For each object, call wiki/search wikipedia to get an article title. 3. Call hugeicons/search icons and hugeicons/list icons; cross-validate up to two icon names per object. 4. Output JSON table with Object, Wikipedia Title, and 2. Call create presentation to get presentation id; use creIcon Names. ate slide from template to build the slide. 3. If question is present, fetch and summarize Reddit opinions for Q&A slide. 4. save presentation to output PPTX. (23) Crypto Purchase Estimation Estimate how many main products 100 units of the most volatile currency can buy. 1. Use dinox-mcp/detect-all-objects and select the object with the largest bounding box as the main product. 2. Query amazon/search products for price/title; compute 24h volatilities from okx-mcp/get candlesticks across candidate instruments and pick the most volatile. 3. Get current price and 24h volume via okx-mcp/get price. 4. Compute purchasing power and estimated items purchasable; return structured JSON. (24) Stock Growth Estimation Estimate the short-term growth potential of the companies and annotate the image with the final result. 1. Use yahoo-finance/get stock info to retrieve real-time prices for group of candidate company tickers and keep the two highest-priced companies for further analysis. yahoocompanies, finance/get historical stock prices 2024-10-01 to 2024-10-03, compute the average of their daily Close prices, and keep the company with higher average for further analysis. selected 2. For over call 3. Call yahoo-finance/get recommendations for the final selected company and compute the upgrade ratio. Then compute the short-term growth percentage using the formula: Growth Percentage = Real-time PriceAverage Close Price 100 + (Upgrade Ratio Average Close Price 10). 4. Use imagesorcery-mcp/draw texts to write the computed growth estimate onto the input image, and return the resulting ticker, its price metrics, the upgrade ratio, the estimated growth, and the annotated image. (25) FIPE Car Variant Lookup Identify the vehicle, determine its 20xx FIPE variant, and summarize its fuel type. 1. Identify the vehicle and extract both the brand name and model keywords. Query car-price/get car brands for Call okx/get exchange rate to obtain the current BRLUSD and BRLBTC conversion rates. the complete list of supported brands. 2. If the brand exists in the supported list, then use its brand name and model keywords, call carprice/search brand model price to retrieve all matching FIPE entries, filter these results to keep only the 20xx variant. 3. Use wiki/summary to look up the fuel type of the 20xx variant, and provide short description based on the search result. (26) Class Demand Estimation Estimate class demand, compare it with the detected object count, and determine whether the class budget is sufficient. 1. Use Math-MCP tools math/mean and math/median to compute median and mode from the collected survey statistic {6, 8, ...}. Use dinox-mcp/detect-all-objects to detect all objects and identify the most frequently appearing object category. Compare the survey median, the survey mode, and the detected top object count, then take the largest among them as the estimated demand. 2. Use amazon/search products to query the detected top object category, obtain the first returned product, and compute the total cost as the ceiling of the estimated demand multiplied by the item price. 3. If the total cost is within the class budget of xxx dollars budget, call Reddit-MCP-Server/search hot posts to fetch the first recent hot post and wiki/summary to obtain short description for the detected object category. (27) NixOS Diagnostic Perform staged NixOS diagnostic workflow using OCR, NixOS tools, and Wiki tools. 1. Use ocr/perform ocr exactly once to extract the package names appearing in the code on the image. These package names will be used in the following steps. 2. Take the first five package names in their original order and call nixos/nixos search once for each of them to check whether they exist in NixOS. If fewer than three of these packages exist, stop the workflow and report. 3. Otherwise, collect the first three package names that do exist. Call nixos/nixos info on services.openssh.enable. Then call nixos/home manager search with the query zsh and keep the first three returned options; choose the first one and call nixos/home manager info. Finally, call wiki/summary once for each of the three existing packages to provide brief background information. 6.2. LLM Prompts Used in Executor and Judge Prepare (Tool Planning) Following the planning protocol described in Experiment Section 3.1, the Executor first selects (but does not execute) the tool set for the current step. The exact prompt template used for this stage is given below. Prepare (Tool Planning) You are an assistant with MCP tool invocation capability. You must call one or more MCP tools, possibly multi-hop, to complete the task. Consider necessary steps as much as possible, consider completed steps in conjunction with conversation history, and skip unnecessary steps. Extra steps may reduce your final score This is step {i} of {self.max_step}. Now decide which tools to use for this step."
        },
        {
            "title": "Output EXACTLY in the following format",
            "content": "(no extra text): \"server_name/tool_name: <description Stop/Continu > Args: <arguments (type): description> Returns: <returns(type): description>\" <plan for how to use> Here are the MCP tools for you. All tools list (name: description): [the full tool catalog is appended programmatically] Process (Controlled Execution) Given the selected tools, execution proceeds under controlled, JSON-only invocation with bounded parallelism as specified in Experiment Section 3.1. The prompt below instantiates that protocol. Process (Controlled Execution) Now execute the selected tools for this step. Return STRICT JSON ONLY with single top-level key tool_calls containing 1 to {self.max_concurrent} items. Consider necessary steps as much as possible, consider completed steps in conjunction with conversation history, and skip unnecessary steps. Extra tool calls may reduce your final score Each item must be an object: {\"name\": \"server_name/tool_name\", \"arguments\": { ... }}."
        },
        {
            "title": "Do NOT include markdown fences or any",
            "content": "extra text or special delimiters and tokens. If tool requires images/files, include the appropriate arguments per the tools schema. You may use the image pool paths listed below if applicable. Selected tools (name: description): [The selected tools list is appended programmatically] [if available, image pool paths are appended]"
        },
        {
            "title": "If the task can be considered complete",
            "content": "based on current tool results and question, here is the original question: {last_user}, answer yes. Otherwise no. Answer strictly with yes or no, no extra words. Final Reply When the stopping criterion is met, the agent emits only the final natural-language answer (Experiment Section 3.1). The corresponding prompt template is provided below."
        },
        {
            "title": "Final Reply",
            "content": "You are finalizing the conversation. Produce ONLY the final answer in natural language. Do NOT include any <tool_call> tags or mention tools explicitly. Be concise and accurate, relying on the prior tool results contained in the conversation. Judge The Judge independently audits causal order and The factual correctness, per Experiment Section 3.1. prompt below operationalizes this decision."
        },
        {
            "title": "Judge",
            "content": "You are strict task finisher. Read the provided interaction history and decide if the task is completed. Very important! If there is an incorrect causal order in multi-step MCP tools calls, or if the information returned by the MCP tools call is clearly wrong, but the call chain proceeds smoothly due to LLM hallucinations, it should be judged as an error. Output EXACTLY one of: success or fail . No other words. Stop/Continu binary stopping criterion gates the transition to summarization, as defined in Experiment Section 3.1. The following prompt enforces strict yes/no decision. Summarize Upon success, the Judge reconstructs an optimal trajectory by merging/removing calls and grouping parallel operations (Experiment Section 3.1). We provide the standard summarization prompt below."
        },
        {
            "title": "You will summarize the optimal",
            "content": "trajectory of useful MCP tool calls to solve the task. Rules: - Remove failed or unnecessary tool calls. - Group calls by step; within step, multiple calls are in parallel (array order arbitrary). - Operations that can invoke the MCP tools call in parallel were mistakenly executed across multi-hop --- they should be merged into single step. - Preserve only name and arguments for each call. - Output STRICT JSON only. No markdown , no comments, no extra text. - If you cannot determine valid trajectory, output exactly the string fail (no JSON). Task Completion For end-to-end task completion, we use four-judge trimmed-mean score defined in Experiment Section 4.4. The prompt below elicits the required scalar."
        },
        {
            "title": "Task Completion",
            "content": "You are strict and professional academic reviewer tasked with evaluating task completion when model uses MCP tools. Judge solely based on the visible prediction content; do not use or allude to any ground truth. Scoring rule: output single scalar in the form boxed{S}, and nothing else. is float in [0,10]. Rules: - 1) Planning (0-3): Clearly presents the MCP call process and plan; complete and organized. - 2) Process (0-3): MCP calls obtain desirable tool feedback and intermediate results; tool use is effective. - Final result (0-4): Degree of task completion (partial credit allowed); final output addresses the task. - Please provide an overall score from 0 to 10 combining the three parts, and output only boxed{S}. Information Grounding Step-level groundedness is evaluated against the reference steps as defined in Experiment Section 4.4. The prompt below collects the coverageonly score."
        },
        {
            "title": "You are a strict and professional",
            "content": "academic reviewer responsible for evaluating information grounding. Task: compare the reference steps ( ground truth steps) with the predicted steps and determine whether the prediction includes all key steps from the reference. Rules: - Coverage-only criterion: equivalence /paraphrase/minor order changes count as covered; - Extra steps are not penalized; - Missing or clearly deviating key steps are penalized linearly by the proportion of missing steps relative to the total reference steps; Output format: output only scalar in the form boxed{G}, where is float in [0,1]. 6.3. Bucketed Hungarian sample pairs based on similarity To make the alignment intuition concrete, Figure 8 shows three representative call pairs after canonicalization and sentence-encoder cosine scoring. Within each tool bucket, only pairs clearing weak screen τweak are eligible for Hungarian assignment; those above stronger bar τstrong are treated as high-confidence matches. The high/medium/low examples illustrate how semantically close arguments receive credit despite surface variation, while dissimilar pairs are withheldclarifying how our Argument Similarity feeds the bucketed matching used in Section 4.1 and, downstream, the structure-aware metrics. 6.4. Human Study We conducted small-scale user study with 30 Ph.D. and researchers working with Model Context Protocol (MCP) tools to validate two design choices: (i) the use of human prompts and human judges (see Section 4.4) for obtaining an optimal trajectory; and (ii) the similarity thresholding used together with the bucketed Hungarian (Kuhn Munkres) matching (see Section 4.1). Preference for Optimal Trajectory. Each participant received 20 randomly sampled pairs of trajectories (the (a) High similarity (b) Medium similarity step, dashed separators denote parallel (multi-threaded) tool calls. Each call reports Tool (invoked tool), ARG (arguments), and OUT (returned output). The final answer is summarized under Final Reply. Object detection with per-instance cropping; parallel crop operations produce one image per detected animal. See (11) Biology Cut. Example: Object detection and cropping IMG Could you spot every creature in the picture and give me individual cutouts for each? Step Tool dinox-mcp/detect-all-objects ARG imageFileUri=./media/png; includeDescription=false (c) Low similarity Figure 8. Illustrative call-level similarity under our sentenceencoder + cosine metric with bucketed Hungarian matching: three representative pairs exhibiting high, medium, and low similarity. This visualization contextualizes Argument Similarity and the weak/strong thresholding used for alignment (see Section 4.1; cf. related agent evaluation protocols. original MLLM-produced trajectory versus the humanoptimized Best Trajectory), with order randomized and content blinded. Participants chose the better one per pair based on faithfulness and procedural soundness. Overall, the Best Trajectory was preferred in 98.16% of cases; under our pairwise setup this corresponds to 589/600 selections (98.17%). Calibration of similarity thresholds. From the evaluation pool, we selected 40 ground-truth/predicted call pairs and asked participants to categorize them as strong, medium, or weak correlation (majority vote). We then swept the similarity thresholds and found that (τstrong, τweak) = (0.8, 0.6) produced the closest agreement with human labels. 7. Trajectory Case Study Format. We present each trajectory using the two-column keyvalue layout defined in trajectoryformat.tex. Labels in the left column are typeset in monospace. IMG denotes the input image and the task query. Step indicates the k-th multi-hop step in the execution. Within OUT Objects detected: elephant (1). ymax}; coordinates describe exact positions and spatial relations. Bboxes use {xmin, ymin, xmax, horse (1), dog (2), Step Tool imagesorcery-mcp/crop ARG input path=./media/png; x1=473; y1=153; x2=1079; y2=952; output path=./media/elephant.png OUT Image crop completed; saved as ./media/elephant.png. Tool imagesorcery-mcp/crop ARG input path=./media/png; x1=1105; y1=364; x2=1428; y2=1015; output path=./media/horse.png OUT Image crop completed; saved as ./media/horse.png. Tool imagesorcery-mcp/crop ARG input path=./media/png; x1=61; y1=671; x2=359; y2=1024; output path=./media/dog1.png OUT Image crop completed; saved as ./media/dog1.png. Tool imagesorcery-mcp/crop ARG input path=./media/png; x1=338; y1=682; x2=531; y2=1024; output path=./media/dog2.png OUT Image crop completed; saved as ./media/dog2.png. Final Reply Found 4 creatures: Each has precise bbox ({xmin, ymin, xmax, ymax}, origin at top-left) and cropped cutout 1 elephant, 1 horse, 2 dogs. under ./media/*.png. Fish Species identification with geospatial planning: Wiki species summary, suitable local spot, weather, routing, and nearby POIs for an executable trip plan. See (5) Fish. Example: Saltwater fish ID and NJ trip planning IMG From the NJ fish photo, can you ID the species with brief overview, pick recent good spot to try, check the current weather there, map route from the usual starting point, and list nearby restaurants and bait shop within about 5 km? Step 1 Tool wiki/summary ARG title=\"American eel\" OUT Species identified as American eel (Anguilla rostrata); catadromous, slender body, continuous dorsal/anal fin, nocturnal feeder common along Atlantic coast and NJ estuaries. Tool wiki/search ARG query=\"Sandy Hook fishing New Jersey\"; n=5 OUT Sandy Hook (Gateway NRA, near Highlands, NJ) highlighted as productive surf/shore area for eels and other inshore species. Step Tool weather/get weather ARG location=\"Highlands, NJ\"; units=\"us\" OUT Current conditions around Highlands, NJ show mild temperatures ( 52.2F) and light winds ( 4.1 mph), suitable for an evening shore session. Step 3 Tool google-maps/compute route ARG origin={latLng:{40.7357,-74.1724}}; destination={latLng:{40.4333,-73.9885}}; travelMode = DRIVE; fields = routes.distanceMeters, ... OUT Route from Newark, NJ to Sandy Hook, NJ computed; distance 44 miles, duration 1 hour via Garden State Parkway and NJ-36 S. Step 4 ARG location={40.4333,-73.9885}; radiusMeters=5000; includedTypes=[\"restaurant\",\"fishing store\"]; languageCode=\"en\"; regionCode=\"US\" OUT Nearby options include several waterfront restaurants (e.g., Bahrs Landing, The Proving Ground, Off The Hook, One Willow) and bait/tackle shops within 5 km. Final Reply Spot: Route: Weather: Sandy Hook, NJ (Gateway American eel (Anguilla rostrata); Species: catadromous, active at night in tidal creeks/marshes. NRA near Highlands). fishable. mi / restaurants and local bait & tackle within km. along bayside creek mouths and rock structures; use bloodworms or cut bait; register with NJ Saltwater Recreational Registry and follow current regulations. fish evening high/outgoing tides NewarkSandy Hook = 44 multiple waterfront calm, cool, Amenities: Plan: 1 h. 5 Generate Powerpoint Text-based slide reconstruction and restyling; recreated via PPT tools with correct presentation id; optional Q&A from Reddit if question is present. See (17) Generate PowerPoint. Example: Recreating Slide through Screenshot IMG From the screenshot of the first slide, pull out all visible text grouped into blocks (title, subtitle, footer if any) with rough font size/color/style guesses, recreate it as new presentation with matching title page, and if any line is question, add short Answer made from few relevant community takes, then save it. Step 1 Tool ocr/perform ocr ARG input data=./media/png; language=\"eng\"; config=\"--oem 3 --psm 6\" OUT Extracted multi-line title and subtitle text; detected embedded question about Reddit cartoons. Tool Reddit-MCP-Server/search hot posts ARG subreddit=\"cartoons\"; limit=3 OUT Fetched top hot threads: fav purple character?\", Ruh Roh Raggy\", Choo choo guess...\". Tool google-maps/places nearby search Step 2 Tool ppt/create presentation ARG id=\"pres\" OUT Initialized new PowerPoint deck with modern IMG blue theme. Step 3 Tool ppt/create slide from template ARG template id=\"title slide\"; presentation id=\"pres\"; content mapping={title, subtitle, question, answer} OUT Title + subtitle mapped; question block appended for Q&A enhancement. Tool ppt/manage text ARG operation=\"add\"; slide index=0; text=\"Question: in reddit/cartoon?\"; font size=20; bold=True; color=(0,120,215) what are latest hot thread OUT Inserted question line below subtitle with bright blue accent. Tool ppt/manage text ARG operation=\"add\"; slide index=0; text=\"- fav purple character?...\"; font size=20; bold=True; color=(0,102,204) OUT Added Reddit answer list under question; auto-fit layout maintained. Step 4 Tool ppt/save presentation ARG file path=./media/Recreated Slide.pptx; presentation id=\"pres\" OUT File saved successfully; final slide index 0 complete. Final Reply Recreated title slide for presentation on multi-modal vision{language models. Title (large, bold, blue #0078D7); Subtitle (medium, italic, gray #444444). Detected question: latest hot thread in reddit/cartoon?\". answers inserted as Q&A section: character?... Final file: guess... Recreated Slide.pptx. Slide successfully rebuilt with question and answers in matching blue theme. what are Reddit 1) fav purple 3) Choo choo 2) Ruh Roh Raggy... Presentation ID: pres. From the back-cover barcode, can you identify the book and provide its title, author, publisher, year, and page count? Step 1 Tool pyzbar-mcp/scan barcode ARG image paths=[./media/png] OUT Barcode scan completed successfully. Found 9780670856688. ISBN(s): scanning is needed proceeding directly to book lookup. No further barcode Step 2 Tool openlibrary mcp/get book info ARG isbn=\"9780670856688\" OUT Retrieved metadata from OpenLibrary: The Discovery of Heaven by Harry Mulisch; published by Viking in 1996, 730 pages. Query type confirmed as ISBN lookup. Final Reply The Discovery of Heaven. ISBN-13: Title: Mulisch. 0670856681). Pages: 730. from the scanned barcode using pyzbar-mcp and validated via OpenLibrary metadata. Harry 9780670856688 (ISBN-10: 1996; Viking; Year: Book successfully recognized Publisher: Author: QR Attraction QR decoding to retrieve image, location inference with geocoding, and air-quality query for current AQI and guidance. See (14) QR Attraction. Example: QR-linked Attraction and Air Quality Retrieval IMG Barcodelookup Barcode decoding followed by OpenLibrary lookup for bibliographic metadata. See (10) Barcode Lookup. Example: Book Identification via Barcode Lookup Pull the webpage link hidden in the image, grab the main photo from that page, infer the location shown, turn that location into coordinates, check current air quality there, and report AQI, pollutant levels, and health advice. IMG Step 1 Tool pyzbar-mcp/scan barcode ARG image paths=./media/png OUT Barcode scan completed successfully. Decoded one QRCode linking to https : //unsplash.com/photos/peoplenearbuilding_K2uemedTGo. No further scanning required proceeding to image retrieval. Step Tool linkimage-mcp/fetch unsplash image ARG url=\"https://unsplash.com/photos /people-near-buildingK2uemedTGo\" OUT Downloaded Unsplash photo by Nick Fewings successfully. Image saved locally and accessible at ./images/ K2uemedTGo.jpg. Step 3 Can you check this picture, note every organism, blur any people for privacy, figure out what each one eats and where it sits in the web, and tell me if they could make valid food chain? Step 1 Tool dinox-mcp/detect-all-objects ARG imageFileUri=./media/png; includeDescription=false OUT Objects detected: wolf (1), person (1), fox (1), rabbit (1). xmax, ymax}; coordinates describe exact positions and spatial relations. Bboxes use {xmin, ymin, Tool google-maps/geocode ARG address=\"Natural History Museum, London\"; Step 2 language=\"en\" OUT Geocoding completed location resolved to Natural History Museum, London. Coordinates: 51.496715, -0.1763672 (Cromwell Rd, South Kensington, London SW7 5BD, UK). Step Tool imagesorcery-mcp/blur ARG input path=./media/png; areas=[{x1=120,y1=93,x2=528,y2=1330, ...}]; invert areas=false; output path=./media/blurred.png OUT Privacy blur applied to the person region. ./media/blurred.png. Output: Tool google-air/current conditions ARG lat=51.496715; lng=-0.1763672 OUT Fetched current air-quality data for London Tool wiki/summary ARG title=\"Rabbit\" OUT Rabbits are small herbivorous mammals (Natural History Museum area). Excellent air quality. NO2. AQI: 82 Dominant pollutant: Final Reply Main image Natural History Museum, QR decoded to Unsplash link. downloaded successfully (Nick Fewings). Detected location: London (51.496715, -0.1763672). Quality Index (UAQI): 82 Excellent. pollutant: may continue outdoor activity safely; those with asthma or NO2 sensitivity should still avoid prolonged exposure to traffic-heavy roads. Ventilation and light exercise remain safe. NO2. Health guidance: Current Air Dominant All individuals Biology Chain Organism detection with privacy blur, Wikipedia-based ecological roles, and validation of plausible food chain. See (16) Biology Chain. Example: BiologyChain Organism Listing, Privacy Blur, and FoodWeb Check (family Leporidae) feeding on grasses and herbs; primary consumers and common prey for foxes and wolves. Tool wiki/summary ARG title=\"Red fox\" OUT Red fox (Vulpes vulpes) an omnivorous mesopredator eating rabbits, rodents, birds, insects, and fruits; trophic level 3. Tool wiki/summary ARG titles=\"Gray wolf\" OUT Wolf (Canis lupus) apex carnivore hunting large and small mammals (including hares and occasionally foxes); trophic level 4. Tool wiki/summary ARG title=\"Human\" OUT Humans (Homo sapiens) omnivorous primates with advanced cognition; top-level consumers but not part of the natural food chain here. Tool wiki/summary ARG title=\"Grass\" OUT Grasses (Poaceae) photosynthetic primary producers forming the base of terrestrial food webs; trophic level 1. long-term health concerns might that raise? Tool wiki/summary (Tree) ARG title=\"Tree\" OUT Trees perennial woody producers performing photosynthesis; provide structure and habitat within ecosystems. Final Reply Final Reply \"<begin of box>\"\"name\":\"food mcp/get nutrition\" \"arguments\":{\"query\":\"1 watermelon\"}\"<end of box> ...\" Unconventional invocation format using \"<begin of box> ... <end of box>\" ./media/blurred.png Privacy-safe image: Organisms noted -- Grass/green plants producers (photosynthesis). -- Trees producers. -- Rabbit herbivore; primary consumer. -- Fox omnivore/mesopredator; secondary consumer. -- Wolf apex carnivore; tertiary consumer. -- Person human omnivore (not part of wild chain). Valid food chain? Yes. 1) Grass Rabbit Wolf (common). 2) Grass Rabbit Fox Wolf (ecologically possible; wolves often kill foxes as competitors and may occasionally consume them). Plants act as producers; rabbit as primary consumer; fox as secondary; wolf as tertiary/apex. valid trophic sequence. The organisms present can form Examples: In the Movies repeatedly Qwen2.5-VL-72B-Instruct task, Qwen2.5-VL-72B-Instruct issues Unknown Tool Invocation to ocr/perform and tmdb/* endpoints absent from the registry, and later misuses generic server/tool wrapper. Without tool discovery or correction, the trajectory culminates in an unsupported final claim. Unknown Tool Invocation Task: Movies Model: Qwen2.5-VL-72B-Instruct IMG 8. Error Trajectory Distribution Analysis We illustrate representative failure trajectories to analyze the characteristic error patterns of different multimodal large language models (MLLMs). Each case is presented following the same format. Can you pull the movie title from the image, look it up, and tell me where its playing near New York? In the Fruit Cal task, glm-4.5v exhibits GLM-4.5v an Illegal calling format: the reply is wrapped with non-JSON delimiters, violating the MCP schema for name/arguments. This malformed structure prevents any valid tool execution and halts progress. Step 1 (Model attempted to call an MCP tool with non-existent tool name) Tool ocr/perform ARG {\"input data\": [{\"image\": \"./media/00020004.png\", \"language\": \"config\": \"--oem 3 --psm 6\"}]} \"eng\", OUT [Tool error] Unknown tool: ocr/perform Illegal calling format Task: Fruit Cal Model: glm-4.5v IMG Step 2 (Parallel invalid tool calls with non-existent tool name) Tool ocr/perform ARG images=[{\"path\":\"./media/png\"}] OUT [Tool error] Unknown tool: ocr/perform Tool tmdb/search/movie ARG query=\"Friday the 13th\" OUT [Tool error] Unknown tool: tmdb/search/movie If woman ate only the fruits shown here for all three meals, would that cover minimum daily calories, and what brief Tool tmdb/get movie details ARG movie id=\"REPLACE WITH ACTUAL MOVIE ID\" OUT [Tool error] Unknown tool: tmdb/get movie details IMG Tool tmdb/get movie showtimes ARG movie id=\"REPLACE WITH ACTUAL MOVIE ID\"; region=\"US\"; locale=\"en US\"; latitude=40.7128; longitude=-74.006 OUT [Tool error] Unknown tool: tmdb/get movie showtimes Step 3 (Repeated the same invalid MCP call structure with no tool correction) Tool ocr/perform ARG {\"input data\": OUT [Tool error] Unknown tool: {\"image\": \"./media/png\"}} ocr/perform Tool tmdb/search/movie ARG query=\"Friday the 13th\" OUT [Tool error] Unknown tool: tmdb/search/movie Tool tmdb/get movie details ARG movie id=\"12345\" OUT [Tool error] Unknown tool: tmdb/get movie details Tool tmdb/get movie showtimes ARG movie id=\"12345\"; region=\"NY\" OUT [Tool error] Unknown tool: tmdb/get movie showtimes Step 4 (Used server/tool wrapper incorrectly for TMDB MCP call) Tool server/tool ARG {\"tool\": {\"query\": \"2009\"}} \"tmdb/search movies\", \"query\": \"Friday the 13th\", \"year\": OUT [Tool error] Unknown tool: server/tool Final Reply The movie \"Friday the 13th\" is not currently showing near New York. You may want to check back later for updated showtimes. InternVL-3.5 In the Food Reddit, Wiki Excel task, InternVL-3.5 exhibits Unknown Tool Invocation with repeated calls to non-existent OCR/Reddit/Excel tools and no corrective recovery. The failure mode mirrors the prior case, ending with fabricated conclusion unsupported by executed tools. Unknown Tool Invocation Task: Food Reddit, Wiki Excel Model: InternVL-3.5 From the foods in the image, can you list each name, find the hottest post about each, and note just its comment count (no repeated searches), then pull short encyclopedia blurb and save everything to spreadsheet with columns for Food Name and wiki Description? Step 1 (Mixed errors: invalid invocation arguments and non-existent tools) Tool food nutrition mcp/get food nutrition ARG food=\"green beans\" OUT Error executing tool food nutrition mcp/get food nutrition Tool wiki/search ARG query=\"green beans\"; n=1 OUT Top results: Green bean Tool excel/create workbook ARG {} OUT Error executing tool create workbook: Field required [filepath] Tool excel/create worksheet ARG sheet name=\"Food Info\" OUT Error executing tool create worksheet: Missing filepath argument Tool excel/write data to excel ARG filepath=\"output.xlsx\"; sheet name=\"Food Info\"; data=[[\"Food Name\",\"Comment Count\",\"Description\"]]; start cell=\"A1\" OUT Error executing tool write data to excel: Invalid filename must be absolute path when not in SSE mode Step 2 (Parallel invalid tool calls with non-existent tool name) Tool food nutrition mcp/get food nutrition ARG food=\"green beans\" OUT Error executing tool food nutrition mcp/get food nutrition Tool server/tool ARG tool=\"Reddit-MCP-Server/search hot posts\"; subreddit=\"food\"; limit=1 OUT [Tool error] Unknown tool: server/tool Tool create worksheet ARG filepath=\"/path/to/food info.xlsx\"; sheet name=\"Food Info\" OUT [Tool error] Unknown tool: create worksheet Tool write data to excel ARG filename=\"/path/to/food info.xlsx\"; sheet name=\"Food Info\"; data=[[\"Food Name\",\"Comment Count\",\"Description\"]] OUT [Tool error] Unknown tool: write data to excel also known as French beans, string beans, or snap beans. and versatility. Popular worldwide for their mild flavor Step 3 (Mixed errors: invalid invocation arguments and non-existent tools) Tool food nutrition mcp/get nutrition ARG query=\"green beans\" OUT Error executing tool get nutrition: measure Tool Reddit-MCP-Server/search hot posts ARG subreddit=\"r/food\"; limit=5 OUT Error: Reddit API returned 400 Tool excel/create workbook ARG filepath=\"/path/to/output.xlsx\" OUT Error: Permission denied at /path. Tool excel/create worksheet ARG sheet name=\"Food Info\"; filepath=\"/path/to/output.xlsx\" File not found OUT Error: /path/to/output.xlsx. Tool excel/write data to excel ARG filepath=\"/path/to/output.xlsx\"; sheet name=\"Food Info\"; data=[[\"Food Name\",\"Comment Count\",\"Description\"]]; start cell=\"A1\" OUT Error: No such file or directory /path/to/output.xlsx. Step 4 (Invalid invocation arguments) Tool Reddit-MCP-Server/search hot posts ARG subreddit=\"food\"; limit=1; query=\"Green bean\" OUT Top Reddit post found: 86 comments (r/food). Tool excel/create worksheet ARG filepath=\"/path/to/output.xlsx\"; sheet name=\"Food Info\" OUT Error: File not found /path/to/output.xlsx. Tool excel/write data to excel ARG filepath=\"/path/to/output.xlsx\"; sheet name=\"Food Info\"; data=[[\"Food Name\",\"Comment Count\",\"Description\"],[\"Green bean\",\"comment count\",\"description\"]]; start cell=\"A1\" OUT Error: No such file or directory /path/to/output.xlsx. Final Reply Heres the final compiled information for the food item: Food Name: Comment Count: Description: Green beans are the young, unripe fruits of various cultivars of the common bean, Green bean 86 In the Travel Comparison task, Claude-Sonnet-4-5 Claude-Sonnet-4-5 reveals characteristic failure mode driven primarily by weak image understanding. The model misidentifies the right-hand landmark in the image, triggering cascade of downstream geolocation errors. Finally, the trajectory ultimately produces superficially coherent but unsupported final summary. Incorrect image understanding Task: Travel Comparison Model: Claude-Sonnet-4-5 IMG Look at the side-by-side photo of two tourist spots, identify each landmark, get their coordinates, check current AQI and main pollutant levels for both, pick the healthier destination (if tied pick the first), find 2--3 nearby cinemas and 2--3 hotels for the chosen spot with approximate distances, and summarize. Step 1 (Irrelevant tool invocation. PowerPoint was not required for this task.) Tool ppt/create presentation ARG {\"id\": OUT Created new presentation with ID: \"tourist comparison presentation\"} tourist comparison presentation (0 slides). Step 2 (The image understanding of the model was incorrect. The righthand landmark was not in Xining, China, but actually in Tokyo, Japan. This misidentification caused the subsequent geolocation and air-quality retrieval steps to fail.) Tool google-maps/places text search ARG {\"query\": \"White Pagoda Beijing Beihai Park\", \"maxResultCount\": 3} OUT Top match Beihai Park (39.9254 N, 116.3893 E). Tool google-maps/places text search ARG {\"query\": \"Yakult building Xining China\", \"maxResultCount\": 3} OUT Top match Downtown Xining (36.6133 N, 101.8035 E). Step 3 (Model repeatedly attempted non-existent tools for air-quality retrieval.) Tool weather aqi/get air quality ARG {\"lat\": OUT 50--150 (moderate to unhealthy) 39.9254, \"lon\": 116.3893} Tool weather aqi/get air quality ARG {\"lat\": OUT 30--80 (generally better) 36.6133, \"lon\": 101.8035} Final Reply Location Identification A: White Pagoda (Beihai Park, Beijing) (39.9254 N, 116.3892 E) B: Yakult Building area (Xining, Qinghai Province) 101.8035 E) (36.6133 N, Air Quality Comparison & Decision Live AQI queries failed because the model called unsupported tools. Using historical patterns: Beijing = 50150 (moderate to unhealthy), Xining = 3080 (generally better). Nevertheless, the model chose Location (Beihai Park) for its tourist infrastructure. Recommended Cinemas near Beihai Park (23 km): 1. Wangfujing CGV Cinema 2 km SE 2. Capital Cinema (Xidan) 2.5 km 3. Broadway Cinematheque (MOMA) 3 km Recommended Hotels near Beihai Park (13 km): 1. Shichahai Shadow Art Performance Hotel 1 km 2. Beijing Jianguo Hotel Qianmen 3 km 3. Grand Hyatt Beijing 3.5 km SE Overall verdict: Model achieved accurate landmark resolution but failed in air-quality retrieval due to non-existent MCP tools. Final summarization remained coherent and informative. 9. Per-tasks M3-Bench Evaluation Corresponding to Table 2, we present the evaluation results of each task described in Section 6.1 across all models, ordered by average score from lowest to highest. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (0) Task: Items Price GLM 4.5v [13] Claude 4.5 Haiku [4] Qwen2.5-VL-72B [6] Grok-4 Fast [45] InternVL 3.5 [42] Gemini 2.5 Flash Lite [8] Llama-4-Scout-17B16E [24] Gemini 2.5 Pro [8] Gemini 2.5 Flash [8] Claude 4.5 Sonnet [5] GPT-5 Mini [29] GPT-5 [31] GPT-5 Nano [30] Grok 4 (0709) [44] 0.000 0.000 0.000 0.200 0.264 0.217 0.327 0.360 0.400 0.400 0.400 0.400 0.550 0.700 0.000 0.000 0.000 0.100 0.248 0.400 0.114 0.189 0.264 0.260 0.193 0.167 0.492 0.207 0.000 0.000 0.000 0.190 0.239 0.177 0.268 0.250 0.286 0.304 0.390 0.387 0.535 0.496 0.000 0.000 0.000 0.067 0.236 0.217 0.275 0.360 0.400 0.400 0.300 0.400 0.550 0. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.200 0.264 0.217 0.329 0.360 0.400 0.400 0.400 0.400 0.550 0.700 0.285 0.386 0.303 0.501 0.325 0.359 0.356 0.487 0.498 0.535 0.571 0.640 0.502 0.606 0.130 0.247 0.388 0.378 0.298 0.292 0.322 0.427 0.443 0.451 0.566 0.603 0.478 0.530 Average Score 0.052 0.079 0.086 0.204 0.234 0.235 0.249 0.304 0.337 0.344 0.353 0.375 0.457 0. Table 5. (0) Task: Items Price, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (1)Task: Attraction Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Qwen2.5-VL-72B [6] Llama-4-Scout-17B16E [24] Grok-4 Fast [45] InternVL 3.5 [42] GPT-5 Mini [29] GPT-5 Nano [30] Gemini 2.5 Flash [8] Gemini 2.5 Flash Lite [8] Claude 4.5 Sonnet [5] GPT-5 [31] Claude 4.5 Haiku [4] Grok 4 (0709) [44] Gemini 2.5 Pro [8] 0.167 0.133 0.333 0.278 0.444 0.333 0.389 0.500 0.500 0.489 0.444 0.667 0.611 0.778 0.067 0.031 0.049 0.403 0.232 0.177 0.390 0.200 0.500 0.443 0.272 0.833 0.526 0.611 0.135 0.118 0.249 0.257 0.400 0.329 0.385 0.462 0.402 0.430 0.417 0.573 0.565 0.609 0.111 0.067 0.333 0.278 0.389 0.333 0.389 0.361 0.500 0.506 0.444 0.667 0.611 0. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.167 0.133 0.333 0.278 0.444 0.333 0.389 0.500 0.500 0.399 0.444 0.667 0.611 0.778 0.285 0.303 0.367 0.501 0.325 0.571 0.502 0.498 0.359 0.518 0.640 0.386 0.606 0.487 0.130 0.388 0.335 0.378 0.298 0.566 0.478 0.443 0.292 0.433 0.603 0.247 0.530 0.427 Average Score 0.133 0.147 0.250 0.296 0.317 0.330 0.365 0.371 0.382 0.402 0.408 0.505 0.508 0. Table 6. (1) Task: Attraction, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (2) Task: Movies Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] InternVL 3.5 [42] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] Claude 4.5 Sonnet [5] Llama-4-Scout-17B16E [24] GPT-5 Nano [30] GPT-5 Mini [29] Gemini 2.5 Flash [8] Gemini 2.5 Flash Lite [8] Gemini 2.5 Pro [8] Grok-4 Fast [45] Grok 4 (0709) [44] GPT-5 [31] 0.000 0.111 0.200 0.333 0.250 0.333 0.278 0.389 0.444 0.611 0.556 0.611 0.722 0.889 0.000 0.200 0.267 0.500 0.417 0.398 0.306 0.344 0.667 0.369 0.522 0.792 0.607 0.519 0.000 0.082 0.169 0.244 0.196 0.258 0.263 0.374 0.351 0.470 0.401 0.513 0.579 0.849 0.000 0.111 0.133 0.333 0.250 0.333 0.278 0.389 0.444 0.500 0.556 0.556 0.667 0. 0.000 0.000 0.000 0.250 0.125 0.000 0.000 0.000 0.250 0.500 0.333 0.500 0.583 0.667 0.000 0.111 0.200 0.006 0.083 0.333 0.278 0.389 0.444 0.611 0.556 0.611 0.722 0.249 0.285 0.325 0.303 0.386 0.535 0.367 0.502 0.571 0.498 0.359 0.487 0.501 0.606 0.640 0.130 0.298 0.388 0.247 0.451 0.335 0.478 0.566 0.443 0.292 0.427 0.378 0.530 0.603 Average Score 0.052 0.155 0.207 0.287 0.288 0.295 0.298 0.378 0.443 0.464 0.480 0.558 0.627 0. Table 7. (2) Task: Movies, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (3) Task: Vision Research GLM 4.5v [13] Claude 4.5 Haiku [4] InternVL 3.5 [42] Llama-4-Scout-17B16E [24] GPT-5 Nano [30] GPT-5 Mini [29] GPT-5 [31] Gemini 2.5 Flash Lite [8] Grok-4 Fast [45] Claude 4.5 Sonnet [5] Gemini 2.5 Flash [8] Grok 4 (0709) [44] Gemini 2.5 Pro [8] Qwen2.5-VL-72B [6] 0.000 0.029 0.204 0.211 0.167 0.167 0.167 0.167 0.290 0.291 0.265 0.363 0.486 0.600 0.000 0.067 0.137 0.093 0.053 0.082 0.115 0.793 0.258 0.258 0.510 0.538 0.675 0.500 0.000 0.027 0.182 0.198 0.153 0.159 0.159 0.155 0.267 0.270 0.246 0.318 0.414 0. 0.000 0.029 0.176 0.211 0.139 0.167 0.167 0.167 0.269 0.291 0.245 0.363 0.437 0.600 0.000 0.000 0.018 0.029 0.000 0.000 0.000 0.000 0.061 0.079 0.056 0.112 0.214 0.250 0.000 0.029 0.204 0.211 0.167 0.167 0.167 0.167 0.290 0.291 0.265 0.363 0.486 0.600 0.285 0.386 0.325 0.367 0.502 0.471 0.640 0.359 0.501 0.535 0.498 0.606 0.487 0.303 0.130 0.247 0.298 0.335 0.478 0.466 0.603 0.292 0.378 0.451 0.443 0.530 0.427 0.388 Average Score 0.052 0.102 0.193 0.207 0.208 0.210 0.252 0.263 0.289 0.308 0.316 0.399 0.453 0.474 Table 8. (3) Task: Vision Research, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (4) Task: Camping Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Qwen2.5-VL-72B [6] InternVL 3.5 [42] Claude 4.5 Haiku [4] Llama-4-Scout-17B16E [24] Gemini 2.5 Flash [8] Grok-4 Fast [45] GPT-5 Nano [30] Gemini 2.5 Pro [8] Claude 4.5 Sonnet [5] Grok 4 (0709) [44] GPT-5 [31] GPT-5 Mini [29] 0.015 0.042 0.162 0.254 0.219 0.417 0.341 0.390 0.433 0.446 0.449 0.520 0.562 0.583 0.029 0.083 0.093 0.207 0.608 0.223 0.362 0.455 0.181 0.490 0.458 0.391 0.382 0.374 0.013 0.040 0.147 0.223 0.195 0.382 0.315 0.363 0.427 0.419 0.416 0.486 0.547 0. 0.015 0.042 0.152 0.254 0.231 0.347 0.258 0.345 0.403 0.431 0.472 0.499 0.488 0.562 0.000 0.013 0.041 0.131 0.099 0.128 0.178 0.229 0.238 0.243 0.238 0.369 0.356 0.402 0.010 0.003 0.090 0.099 0.087 0.233 0.217 0.232 0.269 0.142 0.206 0.347 0.326 0.342 0.262 0.316 0.279 0.319 0.379 0.337 0.496 0.461 0.502 0.487 0.492 0.606 0.616 0.571 0.118 0.259 0.353 0.292 0.224 0.305 0.449 0.344 0.478 0.427 0.410 0.530 0.594 0.566 Average Score 0.058 0.100 0.165 0.222 0.255 0.296 0.327 0.352 0.366 0.386 0.393 0.469 0.484 0.497 Table 9. (4) Task: Camping, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (5) Task: Fish Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] InternVL 3.5 [42] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] Gemini 2.5 Flash Lite [8] GPT-5 Nano [30] Claude 4.5 Sonnet [5] Llama-4-Scout-17B16E [24] Gemini 2.5 Flash [8] Grok-4 Fast [45] GPT-5 Mini [29] Gemini 2.5 Pro [8] GPT-5 [31] Grok 4 (0709) [44] 0.018 0.061 0.050 0.160 0.179 0.164 0.317 0.400 0.327 0.376 0.315 0.533 0.552 0.630 0.091 0.028 0.125 0.137 0.094 0.035 0.220 0.135 0.213 0.218 0.078 0.299 0.134 0.346 0.013 0.052 0.032 0.139 0.154 0.152 0.276 0.339 0.286 0.318 0.302 0.418 0.508 0. 0.018 0.045 0.050 0.160 0.161 0.164 0.317 0.367 0.312 0.345 0.315 0.418 0.506 0.512 0.000 0.035 0.000 0.100 0.051 0.010 0.097 0.210 0.111 0.132 0.091 0.260 0.162 0.259 0.018 0.041 0.050 0.021 0.123 0.127 0.043 0.198 0.216 0.174 0.279 0.464 0.327 0.272 0.285 0.325 0.303 0.386 0.359 0.502 0.535 0.367 0.498 0.501 0.571 0.487 0.640 0.606 0.130 0.298 0.388 0.247 0.292 0.478 0.451 0.335 0.443 0.378 0.566 0.427 0.603 0.530 Average Score 0.072 0.111 0.125 0.169 0.176 0.204 0.282 0.294 0.301 0.305 0.315 0.413 0.429 0.463 Table 10. (5) Task: Fish, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (6) Task: Drug GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Qwen2.5-VL-72B [6] Gemini 2.5 Flash [8] Claude 4.5 Haiku [4] Grok-4 Fast [45] InternVL 3.5 [42] Gemini 2.5 Pro [8] GPT-5 Nano [30] Grok 4 (0709) [44] GPT-5 Mini [29] Claude 4.5 Sonnet [5] Llama-4-Scout-17B16E [24] GPT-5 [31] 0.011 0.055 0.111 0.106 0.171 0.143 0.208 0.116 0.144 0.155 0.204 0.276 0.439 0.382 0.125 0.333 0.204 0.189 0.338 0.247 0.479 0.343 0.357 0.258 0.213 0.422 0.594 0. 0.007 0.048 0.087 0.095 0.149 0.126 0.171 0.109 0.136 0.147 0.196 0.250 0.367 0.361 0.011 0.055 0.082 0.055 0.154 0.143 0.157 0.116 0.130 0.135 0.137 0.276 0.199 0.351 0.000 0.003 0.009 0.003 0.016 0.046 0.028 0.001 0.003 0.003 0.026 0.045 0.082 0.101 0.011 0.030 0.051 0.106 0.110 0.053 0.010 0.103 0.121 0.133 0.177 0.234 0.207 0.289 0.285 0.359 0.303 0.498 0.386 0.501 0.325 0.487 0.502 0.606 0.571 0.535 0.367 0.640 0.130 0.292 0.388 0.443 0.247 0.378 0.298 0.427 0.478 0.530 0.566 0.451 0.335 0. Table 11. (6) Task: Drug, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (7) Task: Hazard Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Claude 4.5 Haiku [4] Gemini 2.5 Flash Lite [8] Qwen2.5-VL-72B [6] GPT-5 Nano [30] Llama-4-Scout-17B16E [24] InternVL 3.5 [42] GPT-5 Mini [29] Grok-4 Fast [45] Gemini 2.5 Flash [8] Grok 4 (0709) [44] Claude 4.5 Sonnet [5] Gemini 2.5 Pro [8] GPT-5 [31] 0.000 0.000 0.143 0.163 0.124 0.205 0.278 0.148 0.421 0.519 0.608 0.656 0.714 0.802 0.000 0.000 0.160 0.045 0.021 0.123 0.239 0.077 0.410 0.333 0.280 0.321 0.442 0.251 0.000 0.000 0.110 0.138 0.118 0.170 0.219 0.139 0.356 0.436 0.499 0.556 0.565 0.695 0.000 0.000 0.127 0.139 0.124 0.185 0.259 0.130 0.402 0.500 0.571 0.638 0.698 0. 0.000 0.000 0.008 0.020 0.016 0.018 0.091 0.040 0.141 0.310 0.245 0.386 0.395 0.317 0.000 0.000 0.143 0.163 0.124 0.164 0.049 0.101 0.263 0.259 0.364 0.193 0.192 0.328 0.285 0.386 0.359 0.303 0.502 0.367 0.325 0.571 0.501 0.498 0.606 0.535 0.487 0.640 0.130 0.247 0.292 0.388 0.478 0.335 0.298 0.566 0.378 0.443 0.530 0.451 0.427 0.603 Table 12. (7) Task: Hazard, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (8) Food Reddit Wiki Excel Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Claude 4.5 Haiku [4] Qwen2.5-VL-72B [6] Gemini 2.5 Flash Lite [8] GPT-5 Nano [30] Llama-4-Scout-17B16E [24] InternVL 3.5 [42] GPT-5 Mini [29] Gemini 2.5 Flash [8] Grok-4 Fast [45] Claude 4.5 Sonnet [5] Grok 4 (0709) [44] GPT-5 [31] Gemini 2.5 Pro [8] 0.000 0.006 0.171 0.163 0.378 0.476 0.525 0.562 0.599 0.636 0.634 0.685 0.765 0.752 0.000 0.017 0.032 0.168 0.104 0.488 0.556 0.322 0.496 0.460 0.674 0.639 0.416 0.622 0.000 0.005 0.156 0.143 0.373 0.446 0.476 0.553 0.544 0.571 0.574 0.635 0.708 0.645 0.000 0.006 0.135 0.128 0.206 0.311 0.351 0.290 0.298 0.378 0.634 0.389 0.379 0.656 0.000 0.000 0.057 0.029 0.000 0.000 0.000 0.054 0.229 0.290 0.229 0.113 0.175 0.386 0.000 0.006 0.103 0.163 0.378 0.476 0.525 0.539 0.589 0.549 0.448 0.685 0.765 0. 0.285 0.386 0.303 0.359 0.502 0.367 0.325 0.571 0.498 0.501 0.535 0.606 0.640 0.487 0.130 0.247 0.388 0.292 0.478 0.335 0.298 0.566 0.443 0.378 0.451 0.530 0.603 0.427 Average Score 0.073 0.147 0.155 0.187 0.196 0.204 0.210 0.213 0.234 0.246 0.261 0.311 0.324 0.396 Average Score 0.052 0.079 0.168 0.170 0.189 0.196 0.220 0.221 0.359 0.412 0.463 0.467 0.490 0. Average Score 0.052 0.084 0.168 0.181 0.302 0.362 0.382 0.432 0.462 0.470 0.522 0.535 0.556 0.589 Table 13. (8) Food Reddit Wiki Excel, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (10) Task: Barcode Lookup GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Grok-4 Fast [45] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] GPT-5 Mini [29] InternVL 3.5 [42] GPT-5 Nano [30] Llama-4-Scout-17B16E [24] Claude 4.5 Sonnet [5] GPT-5 [31] Gemini 2.5 Flash [8] Grok 4 (0709) [44] Gemini 2.5 Pro [8] 0.000 0.000 0.083 0.200 0.250 0.333 0.417 0.500 0.667 0.667 0.917 0.917 0.917 1. 0.000 0.000 0.033 0.225 0.417 0.417 0.750 0.667 0.756 0.625 0.806 1.000 0.861 1.000 0.000 0.000 0.083 0.187 0.230 0.333 0.384 0.484 0.593 0.625 0.880 0.878 0.826 0.962 0.000 0.000 0.083 0.200 0.250 0.333 0.417 0.500 0.667 0.583 0.833 0.833 0.833 0.917 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.333 0.167 0.667 0.667 0.667 0.833 0.000 0.000 0.083 0.200 0.250 0.333 0.417 0.500 0.503 0.667 0.752 0.917 0.917 1.000 0.285 0.359 0.501 0.303 0.386 0.571 0.325 0.502 0.367 0.535 0.640 0.498 0.606 0. 0.130 0.292 0.378 0.388 0.247 0.566 0.298 0.478 0.335 0.451 0.603 0.443 0.530 0.427 Average Score 0.052 0.081 0.156 0.213 0.254 0.361 0.376 0.454 0.528 0.540 0.762 0.769 0.770 0.828 Table 14. (10) Task: Barcode Lookup, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (11) Biology Cut Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding Gemini 2.5 Flash Lite [8] InternVL 3.5 [42] GLM 4.5v [13] GPT-5 Nano [30] GPT-5 Mini [29] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] Llama-4-Scout-17B16E [24] Gemini 2.5 Pro [8] Grok-4 Fast [45] Grok 4 (0709) [44] Claude 4.5 Sonnet [5] Gemini 2.5 Flash [8] GPT-5 [31] 0.000 0.024 0.125 0.000 0.000 0.400 0.318 0.389 0.425 0.743 0.476 0.693 0.575 0. 0.000 0.056 0.028 0.000 0.000 0.118 0.443 0.400 0.345 0.262 0.735 0.538 0.739 0.939 0.000 0.024 0.117 0.000 0.000 0.375 0.304 0.362 0.390 0.698 0.429 0.665 0.549 0.919 0.000 0.024 0.125 0.000 0.000 0.200 0.318 0.389 0.363 0.391 0.476 0.693 0.575 0.958 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.167 0.000 0.000 0.167 0.500 0.833 0.000 0.024 0.125 0.000 0.000 0.400 0.318 0.389 0.425 0.743 0.476 0.693 0.575 0.958 0.359 0.325 0.285 0.502 0.571 0.303 0.386 0.367 0.487 0.501 0.606 0.535 0.498 0. 0.292 0.298 0.130 0.478 0.566 0.388 0.247 0.335 0.427 0.378 0.530 0.451 0.443 0.603 Table 15. (11) Biology Cut, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (12) Fruit Cal Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] InternVL 3.5 [42] Qwen2.5-VL-72B [6] Grok 4 (0709) [44] Claude 4.5 Haiku [4] Llama-4-Scout-17B16E [24] Grok-4 Fast [45] GPT-5 Mini [29] GPT-5 Nano [30] Claude 4.5 Sonnet [5] Gemini 2.5 Pro [8] Gemini 2.5 Flash Lite [8] Gemini 2.5 Flash [8] GPT-5 [31] 0.000 0.071 0.113 0.095 0.207 0.225 0.164 0.164 0.307 0.293 0.360 0.488 0.460 0.481 0.000 0.034 0.059 0.038 0.268 0.127 0.271 0.160 0.082 0.476 0.462 0.256 0.952 0.782 0.000 0.045 0.089 0.084 0.167 0.194 0.148 0.157 0.290 0.272 0.307 0.430 0.416 0. 0.000 0.069 0.110 0.093 0.202 0.219 0.160 0.160 0.172 0.286 0.309 0.441 0.448 0.400 0.005 0.007 0.011 0.009 0.020 0.022 0.016 0.016 0.017 0.029 0.031 0.045 0.046 0.041 0.000 0.069 0.110 0.093 0.202 0.220 0.160 0.160 0.303 0.286 0.352 0.477 0.449 0.471 0.282 0.324 0.301 0.604 0.381 0.362 0.497 0.567 0.498 0.528 0.480 0.348 0.487 0.630 0.128 0.298 0.388 0.530 0.247 0.335 0.378 0.566 0.478 0.451 0.427 0.292 0.443 0.603 Average Score 0.081 0.097 0.117 0.123 0.142 0.273 0.292 0.329 0.379 0.464 0.466 0.554 0.557 0.851 Average Score 0.052 0.115 0.147 0.193 0.212 0.213 0.224 0.244 0.268 0.328 0.341 0.347 0.463 0.479 Table 16. (12) Fruit Cal, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (13) Medical GLM 4.5v [13] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] InternVL 3.5 [42] Gemini 2.5 Flash Lite [8] Gemini 2.5 Flash [8] Llama-4-Scout-17B16E [24] Grok-4 Fast [45] Gemini 2.5 Pro [8] Claude 4.5 Sonnet [5] GPT-5 [31] GPT-5 Nano [30] GPT-5 Mini [29] Grok 4 (0709) [44] 0.000 0.036 0.152 0.199 0.265 0.215 0.396 0.223 0.332 0.350 0.462 0.528 0.636 0.568 0.000 0.031 0.472 0.361 0.242 0.444 0.318 0.647 0.506 0.534 0.382 0.317 0.506 0.466 0.000 0.035 0.119 0.147 0.227 0.198 0.332 0.201 0.296 0.314 0.402 0.515 0.620 0.506 0.000 0.036 0.152 0.158 0.223 0.215 0.233 0.223 0.332 0.350 0.414 0.463 0.550 0.544 0.000 0.000 0.026 0.023 0.053 0.053 0.108 0.093 0.095 0.133 0.275 0.153 0.407 0.418 0.000 0.036 0.091 0.161 0.189 0.215 0.279 0.223 0.275 0.181 0.170 0.447 0.254 0. 0.285 0.303 0.386 0.325 0.359 0.498 0.367 0.501 0.487 0.535 0.640 0.502 0.571 0.606 0.130 0.388 0.247 0.298 0.292 0.443 0.335 0.378 0.427 0.451 0.603 0.478 0.566 0.530 Table 17. (13) Medical, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (14) QR Attraction Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] GPT-5 Nano [30] InternVL 3.5 [42] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] GPT-5 Mini [29] Claude 4.5 Sonnet [5] Llama-4-Scout-17B16E [24] Gemini 2.5 Flash Lite [8] GPT-5 [31] Gemini 2.5 Flash [8] Grok-4 Fast [45] Gemini 2.5 Pro [8] Grok 4 (0709) [44] 0.019 0.136 0.214 0.280 0.285 0.364 0.421 0.571 0.493 0.571 0.757 0.757 0.829 0.829 0.034 0.050 0.236 0.302 0.717 0.202 0.322 0.204 0.612 0.405 0.491 0.738 0.485 0. 0.016 0.136 0.168 0.220 0.237 0.363 0.353 0.479 0.420 0.516 0.672 0.671 0.745 0.748 0.020 0.136 0.214 0.280 0.322 0.364 0.421 0.543 0.493 0.543 0.729 0.729 0.800 0.800 0.000 0.000 0.000 0.022 0.118 0.103 0.190 0.221 0.262 0.268 0.556 0.516 0.651 0.651 0.023 0.136 0.214 0.280 0.190 0.364 0.336 0.442 0.411 0.542 0.757 0.757 0.794 0.829 0.270 0.502 0.319 0.303 0.390 0.571 0.535 0.385 0.316 0.616 0.496 0.501 0.487 0.606 0.122 0.478 0.292 0.388 0.231 0.566 0.451 0.354 0.259 0.594 0.449 0.378 0.427 0. Average Score 0.052 0.108 0.206 0.209 0.231 0.285 0.296 0.311 0.344 0.356 0.418 0.425 0.514 0.516 Average Score 0.063 0.197 0.207 0.259 0.311 0.362 0.379 0.400 0.408 0.507 0.613 0.631 0.652 0.710 Table 18. (14) QR Attraction, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (9) Task: MedExcel Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Gemini 2.5 Flash Lite [8] InternVL 3.5 [42] GPT-5 Nano [30] Claude 4.5 Haiku [4] GPT-5 Mini [29] Grok-4 Fast [45] Gemini 2.5 Flash [8] Grok 4 (0709) [44] Claude 4.5 Sonnet [5] Gemini 2.5 Pro [8] GPT-5 [31] Qwen2.5-VL-72B [6] Llama-4-Scout-17B16E [24] 0.004 0.036 0.071 0.050 0.067 0.071 0.118 0.107 0.071 0.136 0.143 0.179 0.250 0.333 0.009 0.013 0.029 0.021 0.189 0.041 0.141 0.086 0.057 0.142 0.114 0.095 0.351 0.104 0.003 0.027 0.049 0.043 0.052 0.059 0.097 0.085 0.060 0.111 0.115 0.150 0.204 0.283 0.005 0.036 0.071 0.050 0.079 0.071 0.118 0.107 0.071 0.161 0.143 0.179 0.250 0.333 0.000 0.000 0.000 0.000 0.006 0.000 0.013 0.000 0.000 0.013 0.024 0.024 0.033 0.056 0.005 0.036 0.071 0.020 0.044 0.071 0.117 0.107 0.071 0.104 0.143 0.107 0.250 0. 0.266 0.316 0.319 0.402 0.385 0.471 0.468 0.496 0.606 0.500 0.487 0.616 0.303 0.385 0.121 0.259 0.292 0.378 0.229 0.466 0.351 0.449 0.530 0.419 0.427 0.594 0.388 0.354 Average Score 0.052 0.090 0.113 0.121 0.131 0.156 0.178 0.180 0.183 0.198 0.200 0.243 0.254 0.273 Table 19. (9) Task: MedExcel, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (15) Task: Routine GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Llama-4-Scout-17B16E [24] Qwen2.5-VL-72B [6] InternVL 3.5 [42] Grok-4 Fast [45] Claude 4.5 Sonnet [5] Claude 4.5 Haiku [4] GPT-5 Nano [30] Gemini 2.5 Flash [8] Grok 4 (0709) [44] Gemini 2.5 Pro [8] GPT-5 Mini [29] GPT-5 [31] 0.000 0.081 0.115 0.162 0.239 0.105 0.190 0.258 0.452 0.475 0.341 0.447 0.635 0.613 0.000 0.327 0.238 0.162 0.417 0.714 0.311 0.732 0.202 0.566 0.761 0.929 0.344 0.422 0.000 0.066 0.098 0.132 0.195 0.086 0.168 0.226 0.432 0.419 0.297 0.396 0.614 0.565 0.000 0.081 0.115 0.162 0.194 0.105 0.190 0.258 0.343 0.261 0.341 0.447 0.537 0.359 0.000 0.000 0.000 0.031 0.029 0.000 0.029 0.143 0.201 0.171 0.143 0.221 0.370 0. 0.000 0.081 0.115 0.106 0.191 0.105 0.142 0.131 0.416 0.475 0.341 0.369 0.446 0.538 0.285 0.316 0.385 0.303 0.319 0.501 0.535 0.386 0.502 0.496 0.606 0.487 0.571 0.616 0.130 0.259 0.354 0.388 0.292 0.378 0.451 0.247 0.478 0.449 0.530 0.427 0.566 0.594 Average Score 0.052 0.152 0.177 0.181 0.234 0.249 0.252 0.297 0.378 0.414 0.420 0.465 0.510 0.521 Table 20. (15) Task: Routine, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (16) Task: Biology Chain Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Qwen2.5-VL-72B [6] GPT-5 Nano [30] Gemini 2.5 Flash Lite [8] InternVL 3.5 [42] GPT-5 Mini [29] Llama-4-Scout-17B16E [24] Claude 4.5 Haiku [4] Grok-4 Fast [45] Gemini 2.5 Pro [8] Gemini 2.5 Flash [8] Claude 4.5 Sonnet [5] Grok 4 (0709) [44] GPT-5 [31] 0.011 0.117 0.049 0.150 0.090 0.071 0.318 0.375 0.329 0.360 0.457 0.454 0.411 0.810 0.050 0.051 0.063 0.202 0.375 0.031 0.367 0.683 0.681 0.651 0.486 0.547 0.663 0.284 0.011 0.091 0.048 0.130 0.068 0.071 0.268 0.294 0.289 0.287 0.363 0.381 0.334 0.677 0.011 0.117 0.049 0.150 0.090 0.036 0.293 0.375 0.294 0.335 0.363 0.417 0.380 0.633 0.000 0.015 0.000 0.035 0.009 0.000 0.098 0.156 0.051 0.104 0.171 0.189 0.155 0. 0.011 0.051 0.049 0.026 0.058 0.071 0.183 0.150 0.214 0.290 0.321 0.158 0.342 0.430 0.285 0.303 0.502 0.316 0.319 0.571 0.385 0.386 0.501 0.487 0.496 0.535 0.606 0.616 0.130 0.388 0.478 0.259 0.292 0.566 0.354 0.247 0.378 0.427 0.449 0.451 0.530 0.594 Average Score 0.064 0.142 0.155 0.158 0.163 0.177 0.283 0.333 0.342 0.368 0.388 0.391 0.428 0.564 Table 21. (16) Task: Biology Chain, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (17) Task: Generate PowerPoint Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Qwen2.5-VL-72B [6] Gemini 2.5 Flash Lite [8] GPT-5 Nano [30] GPT-5 Mini [29] Claude 4.5 Haiku [4] Claude 4.5 Sonnet [5] InternVL 3.5 [42] Grok-4 Fast [45] Llama-4-Scout-17B16E [24] Grok 4 (0709) [44] Gemini 2.5 Flash [8] GPT-5 [31] Gemini 2.5 Pro [8] 0.024 0.200 0.210 0.136 0.200 0.350 0.333 0.552 0.467 0.867 0.771 0.857 0.971 0.924 0.026 0.057 0.083 0.054 0.094 0.536 0.365 0.326 0.234 0.191 0.422 0.261 0.255 0.567 0.020 0.164 0.176 0.134 0.196 0.306 0.310 0.478 0.420 0.788 0.706 0.787 0.938 0.840 0.024 0.200 0.210 0.136 0.200 0.384 0.333 0.524 0.438 0.814 0.748 0.833 0.890 0.924 0.000 0.083 0.048 0.009 0.026 0.186 0.071 0.328 0.369 0.423 0.655 0.857 0.607 0. 0.023 0.103 0.210 0.086 0.159 0.193 0.333 0.365 0.467 0.830 0.490 0.857 0.765 0.812 0.270 0.303 0.316 0.502 0.571 0.390 0.535 0.319 0.501 0.385 0.606 0.496 0.616 0.487 0.122 0.388 0.259 0.478 0.566 0.231 0.451 0.292 0.378 0.354 0.530 0.449 0.594 0.427 Average Score 0.064 0.187 0.189 0.192 0.252 0.322 0.342 0.398 0.409 0.582 0.616 0.675 0.705 0.727 Table 22. (17) Task: Generate PowerPoint, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (18) Task: Travel Comparison GLM 4.5v [13] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] InternVL 3.5 [42] Gemini 2.5 Pro [8] Llama-4-Scout-17B16E [24] GPT-5 Nano [30] Claude 4.5 Sonnet [5] Grok 4 (0709) [44] Gemini 2.5 Flash Lite [8] Grok-4 Fast [45] GPT-5 Mini [29] GPT-5 [31] Gemini 2.5 Flash [8] 0.023 0.111 0.348 0.476 0.571 0.714 0.643 0.619 0.619 0.714 0.667 0.714 1.000 1.000 0.017 0.026 0.359 0.224 0.088 0.171 0.265 0.417 0.099 0.388 0.446 0.203 0.119 0.239 0.020 0.108 0.297 0.400 0.509 0.650 0.619 0.532 0.543 0.601 0.601 0.691 0.938 0.897 0.020 0.111 0.317 0.381 0.429 0.476 0.562 0.619 0.524 0.619 0.571 0.714 0.810 0. 0.000 0.000 0.199 0.429 0.571 0.071 0.149 0.143 0.571 0.643 0.429 0.438 0.857 1.000 0.024 0.111 0.200 0.476 0.356 0.714 0.421 0.485 0.416 0.386 0.667 0.481 0.573 1.000 0.270 0.303 0.390 0.319 0.487 0.385 0.502 0.535 0.606 0.316 0.501 0.571 0.616 0.496 0.122 0.388 0.231 0.292 0.427 0.354 0.478 0.451 0.530 0.259 0.378 0.566 0.594 0.449 Average Score 0.062 0.145 0.293 0.375 0.430 0.442 0.455 0.475 0.488 0.491 0.532 0.547 0.688 0. Table 23. (18) Task: Travel Comparison, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (19) Task: Heatmap Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Claude 4.5 Haiku [4] Qwen2.5-VL-72B [6] Grok-4 Fast [45] InternVL 3.5 [42] Gemini 2.5 Flash Lite [8] Claude 4.5 Sonnet [5] Llama-4-Scout-17B16E [24] Gemini 2.5 Flash [8] GPT-5 Mini [29] GPT-5 Nano [30] Gemini 2.5 Pro [8] Grok 4 (0709) [44] GPT-5 [31] 0.016 0.040 0.172 0.100 0.225 0.325 0.360 0.650 0.500 0.619 0.714 0.625 0.700 0.750 0.028 0.200 0.090 0.219 0.280 0.848 0.269 0.253 0.488 0.127 0.123 0.674 0.520 0.653 0.014 0.037 0.158 0.088 0.214 0.311 0.346 0.605 0.479 0.586 0.688 0.594 0.687 0.710 0.017 0.040 0.177 0.100 0.225 0.325 0.360 0.650 0.500 0.524 0.667 0.625 0.700 0. 0.000 0.000 0.038 0.038 0.038 0.088 0.140 0.292 0.188 0.429 0.571 0.338 0.525 0.538 0.017 0.040 0.146 0.072 0.194 0.170 0.268 0.383 0.500 0.524 0.524 0.568 0.657 0.683 0.269 0.386 0.286 0.501 0.319 0.316 0.535 0.385 0.496 0.571 0.502 0.487 0.606 0.616 0.122 0.247 0.364 0.378 0.292 0.259 0.451 0.354 0.449 0.566 0.478 0.427 0.530 0.594 Average Score 0.060 0.124 0.179 0.187 0.223 0.330 0.341 0.447 0.450 0.493 0.533 0.542 0.616 0. Table 24. (19) Task: Heatmap, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (20) Task: Museum Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Qwen2.5-VL-72B [6] InternVL 3.5 [42] Claude 4.5 Haiku [4] Llama-4-Scout-17B16E [24] Gemini 2.5 Flash Lite [8] Grok-4 Fast [45] Gemini 2.5 Pro [8] Claude 4.5 Sonnet [5] Gemini 2.5 Flash [8] GPT-5 Nano [30] Grok 4 (0709) [44] GPT-5 Mini [29] GPT-5 [31] 0.014 0.150 0.200 0.203 0.385 0.438 0.360 0.325 0.414 0.375 0.475 0.438 0.550 0.638 0.034 0.109 0.217 0.714 0.262 0.334 0.534 0.692 0.537 0.604 0.229 0.467 0.249 0.296 0.012 0.138 0.198 0.183 0.359 0.415 0.341 0.306 0.391 0.359 0.465 0.417 0.543 0.607 0.010 0.101 0.163 0.153 0.231 0.275 0.229 0.238 0.314 0.267 0.475 0.363 0.550 0. 0.000 0.012 0.000 0.028 0.036 0.064 0.064 0.048 0.067 0.076 0.150 0.105 0.272 0.176 0.014 0.124 0.200 0.120 0.321 0.295 0.320 0.193 0.284 0.350 0.425 0.438 0.466 0.548 0.262 0.279 0.319 0.379 0.337 0.316 0.461 0.487 0.492 0.496 0.502 0.606 0.571 0.616 0.118 0.353 0.292 0.224 0.305 0.259 0.344 0.427 0.410 0.449 0.478 0.530 0.566 0.594 Average Score 0.058 0.158 0.199 0.250 0.279 0.299 0.332 0.340 0.364 0.372 0.400 0.420 0.471 0. Table 25. (20) Task: Museum, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (21) Task: Research Station GLM 4.5v [13] Qwen2.5-VL-72B [6] InternVL 3.5 [42] Claude 4.5 Haiku [4] Llama-4-Scout-17B16E [24] Gemini 2.5 Flash Lite [8] Grok-4 Fast [45] Gemini 2.5 Pro [8] Claude 4.5 Sonnet [5] Gemini 2.5 Flash [8] GPT-5 Nano [30] Grok 4 (0709) [44] GPT-5 Mini [29] GPT-5 [31] 0.014 0.150 0.200 0.203 0.385 0.438 0.360 0.325 0.414 0.375 0.537 0.438 0.550 0.638 0.034 0.109 0.217 0.714 0.262 0.334 0.534 0.692 0.537 0.604 0.324 0.467 0.271 0.296 0.012 0.138 0.198 0.183 0.359 0.415 0.341 0.306 0.391 0.359 0.531 0.417 0.542 0. 0.010 0.101 0.163 0.153 0.231 0.275 0.229 0.238 0.314 0.267 0.237 0.363 0.275 0.388 0.000 0.012 0.000 0.028 0.036 0.064 0.064 0.048 0.067 0.076 0.016 0.105 0.076 0.176 0.014 0.124 0.200 0.120 0.321 0.295 0.320 0.193 0.284 0.350 0.479 0.438 0.516 0.548 0.262 0.279 0.319 0.379 0.337 0.316 0.461 0.487 0.492 0.496 0.502 0.606 0.571 0.616 0.118 0.353 0.292 0.224 0.305 0.259 0.344 0.427 0.410 0.449 0.478 0.530 0.566 0.594 Average Score 0.058 0.158 0.199 0.250 0.279 0.299 0.332 0.340 0.364 0.372 0.388 0.420 0.421 0.483 Table 26. (21) Task: Research Station, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (22) Task: Hugeicons Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Qwen2.5-VL-72B [6] InternVL 3.5 [42] GPT-5 Nano [30] Claude 4.5 Haiku [4] GPT-5 Mini [29] Llama-4-Scout-17B16E [24] Grok-4 Fast [45] Claude 4.5 Sonnet [5] Grok 4 (0709) [44] Gemini 2.5 Flash [8] Gemini 2.5 Pro [8] GPT-5 [31] 0.014 0.113 0.149 0.208 0.143 0.201 0.190 0.382 0.357 0.412 0.351 0.482 0.542 0.542 0.036 0.256 0.115 0.190 0.140 0.754 0.250 0.277 0.564 0.567 0.729 0.440 0.369 0.534 0.010 0.096 0.114 0.164 0.137 0.151 0.187 0.296 0.281 0.322 0.296 0.365 0.396 0. 0.013 0.113 0.137 0.208 0.143 0.209 0.190 0.314 0.312 0.427 0.351 0.399 0.488 0.419 0.000 0.000 0.040 0.000 0.000 0.096 0.089 0.124 0.222 0.231 0.213 0.377 0.352 0.457 0.016 0.113 0.135 0.208 0.143 0.131 0.190 0.351 0.350 0.311 0.304 0.482 0.443 0.542 0.262 0.316 0.279 0.319 0.502 0.379 0.571 0.337 0.461 0.492 0.606 0.496 0.487 0.616 0.118 0.259 0.353 0.292 0.478 0.224 0.566 0.305 0.344 0.410 0.530 0.449 0.427 0.594 Average Score 0.059 0.158 0.165 0.199 0.211 0.268 0.279 0.298 0.361 0.397 0.423 0.436 0.438 0.518 Table 27. (22) Task: Hugeicons, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (23) Task: Crypto Purchase Estimation Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Qwen2.5-VL-72B [6] Gemini 2.5 Flash Lite [8] Claude 4.5 Haiku [4] Llama-4-Scout-17B16E [24] InternVL 3.5 [42] GPT-5 Nano [30] Grok-4 Fast [45] Gemini 2.5 Pro [8] Claude 4.5 Sonnet [5] Gemini 2.5 Flash [8] GPT-5 Mini [29] Grok 4 (0709) [44] GPT-5 [31] 0.021 0.232 0.360 0.314 0.596 0.524 0.571 0.557 0.569 0.642 0.635 0.706 0.754 0.770 0.051 0.163 0.415 1.000 0.394 0.654 0.223 0.803 0.649 0.808 0.696 0.449 0.811 0.716 0.018 0.209 0.348 0.278 0.544 0.480 0.566 0.517 0.542 0.593 0.590 0.699 0.695 0. 0.019 0.203 0.360 0.309 0.465 0.524 0.402 0.461 0.529 0.632 0.479 0.706 0.558 0.690 0.000 0.046 0.000 0.110 0.142 0.251 0.090 0.254 0.123 0.265 0.245 0.288 0.440 0.337 0.016 0.137 0.360 0.132 0.355 0.185 0.478 0.353 0.477 0.314 0.577 0.292 0.350 0.486 0.262 0.279 0.316 0.379 0.337 0.319 0.502 0.461 0.487 0.492 0.496 0.571 0.606 0.616 0.118 0.353 0.259 0.224 0.305 0.292 0.478 0.344 0.427 0.410 0.449 0.566 0.530 0.594 Average Score 0.063 0.203 0.302 0.352 0.392 0.404 0.414 0.469 0.476 0.519 0.521 0.535 0.593 0.617 Table 28. (23) Task: Crypto Purchase Estimation, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (24) Task: Stock Growth Estimation GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Llama-4-Scout-17B16E [24] Qwen2.5-VL-72B [6] Claude 4.5 Haiku [4] Gemini 2.5 Flash [8] InternVL 3.5 [42] Grok-4 Fast [45] GPT-5 Mini [29] GPT-5 Nano [30] Claude 4.5 Sonnet [5] Gemini 2.5 Pro [8] GPT-5 [31] Grok 4 (0709) [44] 0.050 0.388 0.575 0.531 0.563 0.613 0.713 0.688 0.925 0.950 0.950 0.838 0.888 0.875 0.041 0.275 0.156 0.444 0.361 0.436 0.479 0.359 0.474 0.361 0.748 0.729 0.375 0. 0.048 0.384 0.562 0.514 0.552 0.604 0.704 0.680 0.911 0.937 0.934 0.811 0.885 0.871 0.050 0.388 0.525 0.531 0.563 0.600 0.688 0.688 0.838 0.900 0.925 0.813 0.875 0.863 0.000 0.071 0.198 0.306 0.243 0.329 0.391 0.519 0.682 0.813 0.859 0.662 0.695 0.671 0.050 0.263 0.187 0.206 0.365 0.613 0.604 0.658 0.603 0.752 0.446 0.807 0.793 0.832 0.128 0.184 0.287 0.234 0.355 0.390 0.355 0.380 0.350 0.358 0.372 0.507 0.512 0.505 0.022 0.165 0.412 0.312 0.331 0.525 0.419 0.479 0.717 0.580 0.502 0.576 0.725 0. Average Score 0.049 0.265 0.363 0.385 0.417 0.514 0.544 0.556 0.687 0.706 0.717 0.718 0.718 0.735 Table 29. (24) Task: Stock Growth Estimation, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (25) Task: FIPE Car Variant Lookup Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Gemini 2.5 Flash Lite [8] Claude 4.5 Haiku [4] InternVL 3.5 [42] Llama-4-Scout-17B16E [24] Grok-4 Fast [45] Gemini 2.5 Flash [8] Qwen2.5-VL-72B [6] Claude 4.5 Sonnet [5] GPT-5 Nano [30] GPT-5 Mini [29] Grok 4 (0709) [44] Gemini 2.5 Pro [8] GPT-5 [31] 0.033 0.033 0.200 0.200 0.333 0.333 0.367 0.500 0.500 0.567 0.467 0.533 0.500 0.700 0.028 0.167 0.184 0.299 0.086 0.506 0.357 0.349 0.397 0.229 0.305 0.295 0.570 0. 0.030 0.033 0.191 0.188 0.316 0.312 0.357 0.474 0.472 0.535 0.445 0.516 0.483 0.640 0.033 0.033 0.200 0.200 0.333 0.283 0.283 0.367 0.450 0.467 0.467 0.433 0.450 0.450 0.000 0.000 0.024 0.048 0.143 0.048 0.148 0.191 0.167 0.191 0.167 0.238 0.119 0.071 0.033 0.033 0.133 0.134 0.062 0.239 0.244 0.435 0.162 0.338 0.307 0.249 0.248 0.398 0.204 0.258 0.355 0.367 0.267 0.442 0.496 0.346 0.492 0.471 0.513 0.508 0.646 0.600 0.126 0.089 0.244 0.255 0.320 0.386 0.372 0.347 0.467 0.327 0.550 0.486 0.534 0. Average Score 0.061 0.081 0.191 0.211 0.233 0.319 0.328 0.376 0.388 0.391 0.403 0.407 0.444 0.447 Table 30. (25) Task: FIPE Car Variant Lookup, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. (26) Task: Class Demand Estimation Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding GLM 4.5v [13] Claude 4.5 Haiku [4] InternVL 3.5 [42] Gemini 2.5 Flash Lite [8] Claude 4.5 Sonnet [5] Qwen2.5-VL-72B [6] Llama-4-Scout-17B16E [24] GPT-5 Nano [30] Gemini 2.5 Flash [8] Gemini 2.5 Pro [8] Grok-4 Fast [45] GPT-5 Mini [29] Grok 4 (0709) [44] GPT-5 [31] 0.117 0.167 0.150 0.222 0.278 0.442 0.442 0.397 0.464 0.558 0.544 0.540 0.614 0.856 0.052 0.166 0.233 0.111 0.384 0.191 0.226 0.622 0.544 0.418 0.722 0.325 0.528 0. 0.117 0.160 0.150 0.217 0.274 0.439 0.439 0.392 0.460 0.547 0.544 0.526 0.610 0.799 0.117 0.167 0.150 0.160 0.215 0.442 0.442 0.397 0.436 0.376 0.482 0.518 0.586 0.681 0.000 0.030 0.000 0.000 0.000 0.015 0.141 0.181 0.000 0.061 0.000 0.366 0.167 0.606 0.117 0.167 0.150 0.222 0.278 0.386 0.442 0.206 0.464 0.558 0.544 0.447 0.614 0.387 0.354 0.321 0.350 0.279 0.429 0.396 0.371 0.438 0.467 0.521 0.517 0.567 0.625 0.653 0.294 0.197 0.349 0.333 0.297 0.436 0.400 0.501 0.416 0.502 0.475 0.568 0.707 0. Average Score 0.146 0.172 0.192 0.193 0.269 0.343 0.363 0.392 0.406 0.443 0.478 0.482 0.556 0.630 Table 31. (26) Task: Class Demand Estimation, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench. Model Detection Trajectory Alignment LLM Judge Recall Precision Argument Similarity Step Coherence Order Consistency Merge Purity Task Completion Information Grounding (27) Task: NixOS Diagnostic GLM 4.5v [13] Llama-4-Scout-17B16E [24] InternVL 3.5 [42] Gemini 2.5 Flash Lite [8] Claude 4.5 Haiku [4] GPT-5 Nano [30] Grok-4 Fast [45] Qwen2.5-VL-72B [6] Claude 4.5 Sonnet [5] Grok 4 (0709) [44] Gemini 2.5 Flash [8] GPT-5 Mini [29] Gemini 2.5 Pro [8] GPT-5 [31] 0.000 0.222 0.167 0.153 0.306 0.306 0.319 0.389 0.403 0.292 0.333 0.597 0.569 0. 0.000 0.121 0.413 0.536 0.329 0.821 0.571 0.647 0.585 0.964 0.804 0.538 0.538 0.392 0.000 0.213 0.157 0.132 0.294 0.301 0.316 0.378 0.354 0.274 0.295 0.590 0.512 0.747 0.000 0.188 0.132 0.118 0.306 0.292 0.271 0.354 0.375 0.292 0.333 0.542 0.399 0.572 0.000 0.045 0.000 0.024 0.053 0.049 0.069 0.041 0.150 0.163 0.155 0.191 0.268 0.499 0.000 0.173 0.167 0.112 0.252 0.215 0.319 0.389 0.219 0.292 0.333 0.356 0.472 0.556 0.225 0.221 0.279 0.254 0.383 0.312 0.354 0.271 0.362 0.444 0.400 0.446 0.533 0. 0.042 0.160 0.083 0.301 0.354 0.354 0.496 0.361 0.409 0.416 0.548 0.549 0.524 0.583 Average Score 0.033 0.168 0.175 0.204 0.285 0.331 0.339 0.354 0.357 0.392 0.400 0.476 0.477 0.567 Table 32. (27) Task: NixOS Diagnostic, Cross-Model Evaluation of Multimodal MCP Tool-Calling Trajectories on M3-Bench."
        }
    ],
    "affiliations": [
        "Rutgers University"
    ]
}