{
    "paper_title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
    "authors": [
        "Zhiyuan Peng",
        "Ting-ruen Wei",
        "Tingyu Song",
        "Yilun Zhao",
        "Yi Fang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 3 2 2 6 0 . 7 0 5 2 : r Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers Zhiyuan Peng Santa Clara University Santa Clara, CA zpeng@scu.edu Ting-ruen Wei Santa Clara University Santa Clara, CA twei2@scu.edu Tingyu Song Independent Researcher Beijing, China songtingyu220@gmail.com Yilun Zhao Yale University New Haven, CT yilun.zhao@yale.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (e.g., parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E2R-FLOPS, for LLMbased rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community."
        },
        {
            "title": "Introduction",
            "content": "A typical search system balances efficiency and quality with two-stage pipeline: lightweight retriever retrieves hundreds of documents from vast corpus, prioritizing efficiency, and then more powerful but computationally expensive reranker refines their order. Thanks to the rapid progress of LLMs (Brown et al., 2020; Grattafiori et al., 2024; Team et al., 2023), LLM-based rerankers have achieved impressive gains in reranking metrics, such as NDCG; however, these gains often come at the cost of substantial computational expense, making them difficult to deploy at scale in Yi Fang Santa Clara University Santa Clara, CA yfang@scu.edu production. This underscores the need for evaluation metrics that consider not only reranking quality but also computational efficiency. Existing studies evaluate the efficiency of LLMbased rerankers using proxies such as latency (Jin et al., 2025), the number of LLM calls (i.e., forward passes) (Zhuang et al., 2024), and input and output token usage (Chen et al., 2025). However, these metrics lack computational granularity needed to distinguish differences in internal compute per token or per model call. Specifically, latency is heavily dependent on hardware and runtime choices (GPU vs. CPU, batch size, parallelism), making it an inconsistent basis for comparing algorithms across studies. The number of LLM calls ignores the model size: single call to 70B LLM costs orders of magnitude more compute than call to 3B model, yet both appear identical under this metric. Similarly, token usage overlooks the model size and is difficult to interpret as the cost of the input token and the output token can be different. Inspired by the scaling law in LLMs that studies the connection between total compute and performance (Kaplan et al., 2020), we employ floating-point operations (FLOPs) as fundamental measure of cost for each forward pass or LLM call. The total number of FLOPs required by model to rerank documents is hardwareagnostic, intrinsic metric of computational work (Sukthanker et al., 2024). Building on this insight, we introduce E2R-FLOPS, EfficiencyEffectiveness Reranking FLOPS for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. The proposed metrics thus enable fair comparisons between methods that might utilize different LLMs, reranking algorithms, and running-time choices. Companied with the proposed metrics, an interpretable FLOPs estimator was built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate wide range of LLM-based rerankers, examining the efficiencyeffectiveness trade-off and drawing attention to this issue within the research community. Our key contributions include: We derive closed-form, interpretable formula for the FLOPs of LLM-based rerankers and provide an open-source calculator covering upto-date models and decoding settings. We propose two efficiency-effectiveness metrics: RPP for relevance per compute and QPP for hardware-agnostic throughput. We conduct the first large-scale study of the efficiencyeffectiveness trade-off in LLM-based rerankers, bring this issue to the attention of the research community. All code, data, and the FLOPs estimator are publicly released for reproducible research on computationally efficient rerankingEfficiencyEffectiveness Reranking FLOPs1."
        },
        {
            "title": "2.1 LLM-based Rerankers",
            "content": "Based on how the documents are compared with each other, LLM-based rerankers can be categorized as pointwise, pairwise and listwise. Pointwise methods primarily compute the query-document relevance score by either the likelihood of generating the query conditioned on the document (Ponte and Croft, 2017; Zhuang and Zuccon, 2021; Zhuang et al., 2021; Peng et al., 2024) or the normalized possibility of generating the Yes when prompting the LLM whether the query-document pair is relevant or not (Liang et al., 2023; Nogueira et al., 2020). The ranking can be easily accomplished by sorting the relevance score of each document. Pairwise methods compare the relevance of pair of documents to given query and output the document ID of the more relevant one. To rank list of documents, sorting (Qin et al., 2024) and sampling (Gienapp et al., 2022; Mikhailiuk et al., 2020) methods are proposed. Sorting uses the pairwise comparison to replace the comparison operation in sorting algorithms like bubble sorting and heap sorting, while the 1https://github.com/zhiyuanpeng/EER-FLOPs. sampling method. Soring method is more efficient to get the top-K document as it does not need to compare all the pairs. Setwise (Zhuang et al., 2024) extends pairwise by comparing more than two documents in one LLM call. Listwise methods directly output ranked list of document IDs. Most of the existing listwise methods are zero-shot (Ma et al., 2023) or few-shot (Sun et al., 2023; Ma et al., 2023; Pradeep et al., 2023) prompting methods. Recently, researchers (Zhang et al., 2025) have resorted to adopting reinforcement learning to fine-tune LLMs to generate reasoning followed by ranked document IDs to tackle reasoning-intensive tasks like BRIGHT (Su et al., 2025). To get full rank list, listwise methods usually adopt strategies like sliding-window (Sun et al., 2023) and tournament ranking (Chen et al., 2025) to get full rank list with limited window size."
        },
        {
            "title": "2.2 FLOPs Calculation",
            "content": "Several FLOPs profilers exist for deep learning models, but most are limited to standard forward passes and do not support token-level generation with KV-cache, which is essential for accurate LLM inference estimation. PyPAPI (LOISON, 2023) measures CPU-level FLOPs for general Python code but is not designed for PyTorch or GPU workloads. ptflops (Sovrasov, 2018) and fvcore (facebookresearch, 2019) compute FLOPs by running models forward function, but do not support autoregressive decoding. DeepSpeeds FLOPs profiler (Rasley et al., 2020) and calflops (MrYxJ, 2023) both support the FLOPs of the decoding process, but they also require full forward execution. All existing tools require model execution and lack closed-form support for generation-aware FLOPs estimation. For reranking, prior studies utilize coarse FLOP estimates, e.g., double the total parameter count (Shao et al., 2025) or open-source tooling (Abdallah et al., 2025), which lack interpretability regarding the specific facts that impact the FLOPs count. Our work differs from theirs in that our FLOPs estimator is well-interpretable, and we propose new metrics to comprehensively evaluate the efficiency and effectiveness of LLM-based rerankers. Multi-head Attention Grouped-query Attention Ops Parameters FLOPs per Token Parameters FLOPs per Token Atten: QKV Atten: Atten: Mask Feedforward nlayerdmodel3dattn nlayerdattndmodel - nlayer2dmodelddff 2nlayerdmodel3dattn 2nlayerdattndmodel 4nlayerdattndmodel 2nlayer2dmodelddff nlayerdmodel (1 + 2nkv/nq) dattn nlayerdattndmodel nlayer 2dmodelddff 2nlayerdmodel (1 + 2nKV/nQ) dattn 2nlayerdattndmodel 4nlayernctx(nKV/nQ)dattn 2nlayer 2dmodelddff Table 1: FLOP count for the attention mechanism for multi-head attention and grouped-query attention"
        },
        {
            "title": "3 Method",
            "content": "3.1 FLOPs Estimator the number of layers nlayer, We parameterize Transformer with four hyperparameters: the residual-stream width dmodel, the hidden size of the feed-forward block dff, and the dimension of attention output dattn which is the dimension of Q, K, projections before splitting into multiple heads and by default dattn = dmodel. Because decoderonly and encoderdecoder designs dominate LLMbased rerankers, we derive estimates for both. To keep the analysis general yet concrete, we adopt the baseline decoder-only configuration of Kaplan et al. (2020) and the T5 encoderdecoder architecture (Raffel et al., 2020). In typical reranking call, the model receives prompt (the context, denoted ctx) and produces an output sequence (opt). The prompt concatenates task-specific prefix p, the query q, and list of documents, giving length nctx. The generated sequence has length nopt."
        },
        {
            "title": "3.1.1 Decoder-only\nFollowing Kaplan et al. (2020), we ignore sub-\nleading costs such as nonlinearities, biases, and\nlayer-normalization. The number of attention and\nfeedforward relevant parameters Ndec is:",
            "content": "Ndec 2dmodel nlayer (2dattn + dff) (1) Given an LLM with KV cache enabled, we now compute the FLOPs of generating sequence named opt (short for output), consisting of nopt tokens, conditioned on prompt ctx (short for context) of length nctx. The context includes task-specific prompt p, query q, and list of documents. Each token within ctx requires 2Ndec + 4nlayer nctxdattn FLOPs, where 2Ndec comes from the fact that each parameter in Ndec has one addition and one multiplication operation and 4nlayer nctxdattn is taken by the basic multi-head attention operation (Kaplan et al., 2020). The total FLOPs for nctx tokens C(ctx) is: (ctx) = 2Ndecnctx + 4nlayer n2 ctxdattn (2) When generating token opti, the total sequence length seen by LLM is nctx+(i1) and the FLOPs for token is: (opti) = 2Ndec + 4nlayer [nctx + (i 1)] dattn (3) The FLOPs of generating nopt tokens is computed by summing over all the nopt tokens: C(opt) = 2Ndecnopt + 2nlayer dattn (cid:2)2noptnctx + nopt(nopt 1)(cid:3) (4) The total FLOPs of taking prompt ctx and generating opt is: C(ctx + opt) = C(ctx) + C(opt) (5) For LLM-based reranker, nctx consists of taskspecific prompt p, query q, and list of documents. By approximating the length of each document as the average document length Ldoc, nctx can be estimated as: nctx = np + nq + wldoc (6) Suppose nQ represents the number of heads for and nKV denotes the number of heads for and V. Compare to multi-head attention, the number of parameters and the FLOPs per token changed accordingly as shown in Table 1. The equations are rewritten as: Ndec 2dmodel nlayer (1 + (cid:18) nKV nQ )dattn + dff (cid:19) (7) (ctx) = 2Ndecnctx + 4nlayer n2 ctx nKV nQ dattn (8) C(opt) = 2Ndecnopt + 2nlayer + 2nlayer nKV nQ nKV nQ dattn 2noptnctx dattn nopt(nopt 1) (9) 3.1.2 Encoder-Decoder 3.2 Metrics Decoder-only LLMs, such as GPT (Radford et al., 2019), do not include encoder-decoder attention and therefore share similar structure with the encoder component of encoder-decoder models. The main difference lies in the attention masking strategy, which, however, does not affect the FLOPs required to process the prompt. Although decoderonly models are designed to compute attention only over previous tokens, in practice, they compute full self-attention (e.g., Qprompt Kprompt) and apply causal mask to prevent attending to future tokens. So, for encoder-decoder LLMs, the FLOPs of consuming prompt ctx is the same as that of encoder-only LLMs: (ctx) = 2Nencnctx + 4nlayer n2 ctxdattn (10) Where Nenc is same as Ndec in Equation 1. The decoder has different attention mechanism from that of the encoder, as it employs an encoder-decoder attention mechanism followed by self-attention. In an encoderdecoder model each decoder layer must, once per prompt, project the encoder outputs to cross-attention keys and values. This setup cost is calculated as: Ccross-KV = 4 nlayer nctx dmodel dattn (11) Even, it has two attentions, when generating token opti, it only goes through two projections, two one projection, and one projections, as the left and projections are for prompt ntx, so Ndec 2dmodel nlayer (3dattn + dff) (12) The total sequence length seen by self-attention is (i 1) and the attention operation takes 4nlayer (i 1)dattn FLOPs. The sequence length seen by the following encoder-decoder attention is fixed as nctx, which requires 4nlayer nctxdattn for computing attention. The total attention FLOPs is 4nlayer (nctx + 1)dattn which is the same as that of decoder-only models as shown in the right part of equation 3 and thus the total FLOPs at generating token opti is also same as equation 3 and the only difference is that the value of Ndec is different. Similary, the C(opt) is same as equation 4 but with different value of Ndec. The cost of encoder-decoder model is: C(ctx+opt) = C(ctx)+Ccross-KV+C(opt) (13) We report two FLOPs-normalized metrics to compare different LLM-based rerankers, thereby effectively evaluating the effectiveness-efficiency tradeoff without being tied to specific hardware. 3.2.1 Ranking metrics per PetaFLOP (RPP) RPP = m(q) Cq/1015 (14) where m(q) can be any ranking metric for query (e.g., NDCG, MRR, MAP). RPP therefore expresses ranking metrics per peta-FLOP, the larger the value, the higher the ranking quality is obtained for fixed compute budget. 3.2.2 Queries per PetaFLOP (QPP) QPP = 1 AV GCq/1015 (15) QPP measures throughput: how many queries can be processed with one peta-FLOP. Together, RPP and QPP trace methods efficiencyeffectiveness frontier: RPP weights quality per compute, while QPP captures raw FLOPs-normalized throughput."
        },
        {
            "title": "4 Experiment Setup",
            "content": "Following the setwise setup in Zhuang et al. (2024), we utilize the Flan-T5 (Chung et al., 2024) as the backbone for most of the LLM-based rerankers except for IRL and Tourrank as these two methods require longer context than Flan-T5s input limitation allows. For IRL and Tourrank, we employ the Llama-3.1-8B-Instruct model. To compare our estimated FLOPs with those reported by open-source packages, we also include Qwen2.5 (Yang et al., 2025) (3B, 7B, 14B), which implements group query attention instead of multi-head attention. For all LLM-based rerankers, we report their performance using the new metrics on the TREC-DL19 and DL20 datasets (Craswell et al., 2020). The top 100 documents are retrieved using Pyserinis BM25 (Lin et al., 2021). For FLOPs measurement, we utilize DeepSpeeds FLOPs profiler (Rasley et al., 2020) and calflops (MrYxJ, 2023) profilers and find identical results from both sources."
        },
        {
            "title": "5 Experimental Results and Analysis",
            "content": "We conduct extensive experiments to study four key research questions. Our results and analysis are as follows: TREC DL 2019 TREC DL 2020 NDCG #LLM Out #FLOPs RPP QPP NDCG #LLM Methods BM25 pointwise.qlm pointwise.yes_no listwise.generation listwise.likelihood pairwise.allpair pairwise.heapsort pairwise.bubblesort setwise.heapsort setwise.bubblesort pointwise.qlm pointwise.yes_no listwise.generation listwise.likelihood pairwise.allpair pairwise.heapsort pairwise.bubblesort setwise.heapsort setwise.bubblesort pointwise.qlm pointwise.yes_no listwise.generation listwise.likelihood pairwise.allpair pairwise.heapsort pairwise.bubblesort setwise.heapsort setwise.bubblesort a - 5 - F - 5 - F - 5 - F 1 IRL . 3 Tourrank In - 152.12 161.12 486.21 384.49 304.48 455.72 451.77 322.65 320.9 152.12 161.12 486.38 385.49 298.33 455.26 451.42 321.74 335.53 152.12 161.12 487.08 385.87 282.32 456.98 453.06 323.43 321. - 100 100 245 245 9900 230.3 844.2 125.4 460.5 100 100 245 245 9900 241.9 886.9 129.5 446.9 100 100 245 245 9900 239.4 870.5 130.1 468.3 .506 .557 .654 .561 .669 .666 .657 .636 .670 . .542 .650 .569 .689 .713 .705 .683 .693 .705 .506 .644 .662 .701 .699 .708 .679 .706 .711 .649 .757 - 0.0 0.0 10.54 0.0 5.0 10.0 10.0 5.0 5.0 0.0 0.0 11.87 0.0 5.0 10.0 10.0 5.0 5. 0.0 0.0 11.53 0.0 5.0 10.0 10.0 5.0 5.0 0.0 27.91 - 0.009 0.009 0.076 0.058 1.865 0.066 0.242 0.025 0.091 0.034 0.036 0.282 0.216 6.826 0.259 0.942 0.096 0.346 0.135 0.143 1.105 0.851 25.51 1.010 3.642 0.383 1. 0.096 2.274 - 61.89 72.67* 7.38 11.53 0.36 9.95 2.63 26.8 7.45 15.94 18.06 2.02 3.19 0.1 2.72 0.73 7.22 2.04 3.75 4.5 0.6 0.82 0.03 0.7 0.19 1.84 0.52 6.76 0. - 111.1* 111.1* 13.16 17.24 0.536 15.15 4.132 40.0 10.99 29.41 27.78 3.546 4.629 0.146 3.861 1.061 10.42 2.890 7.407 6.993 0.904 1.175 0.039 0.990 0.275 2.610 0.727 10.42 0.440 .480 .567 .615 .547 .626 .622 .619 .589 .618 .624 .542 .636 .547 .672 .682 .692 .662 .678 .676 .492 .632 .637 .690 .688 .699 .681 .688 . .639 .777 - 100 100 245 245 9900 226.8 778.5 124.2 457.4 100 100 245 245 9900 244.3 863.9 127.8 463.5 100 100 245 245 9900 240.5 842.9 128.1 467.9 In - 152.85 161.85 488.28 388.61 304.47 459.62 459.03 325.5 325.63 152.85 161.85 489.04 388.97 297.93 455.76 457.18 325.27 326.32 152.85 161.85 489.60 389.73 282.32 458.26 459.56 325.01 326.37 Out #FLOPs RPP QPP - 0.0 0.0 10.04 0.0 5.0 10.00 10.0 5.0 5.0 0.0 0.0 11.49 0.0 5.0 10.0 10.0 5.0 5.0 0.0 0.0 11.05 0.0 5.0 10.0 10.0 5.0 5. 0.0 26.79 - 0.009 0.009 0.076 0.058 1.865 0.066 0.227 0.025 0.092 0.034 0.036 0.283 0.218 6.817 0.262 0.930 0.096 0.349 0.136 0.144 1.110 0.860 25.51 1.017 3.577 0.379 1.393 0.098 2. - 63.0 68.33* 7.2 10.79 0.33 9.38 2.59 24.72 6.78 15.94 17.67 1.93 3.08 0.1 2.64 0.71 7.06 1.94 3.62 4.39 0.57 0.8 0.03 0.69 0.19 1.82 0.49 6.52 0.34 - 111.1* 111.1* 13.16 17.24 0.536 15.15 4.405 40.0 10.87 29.41 27.78 3.534 4.587 0.147 3.817 1.075 10.42 2.865 7.352 6.944 0.901 1.162 0.039 0.983 0.279 2.638 0. 10.20 0.438 2 130 4469.12 1651.62 2 130 4556.31 1659.93 Table 2: Results on TREC DL. All the methods re-rank BM25 top 100 documents. #LLM represents average number of LLM call per query for reranking 100 documents. In and Out denote the average input tokens and output tokens per LLM call. #FLOPs is the estimated FLOPs per query for reranking 100 documents. Bold value is the best within each LLM and stared value is the best across different LLMs. L3.1 represent Llama-3.1-8B-Instruct model. We report NDCG@10 for the NDCG metric. Q1: What are the LLM-based reranker performances under the new metrics? Table 2 reports the efficiencyeffectiveness trade-offs of broad set of LLM-based rerankers under the proposed RPP and QPP metrics. Overall, these systems perform poorly once computation is taken into account. Across all LLMs and methods, TourRank with Llama-3.1-8B-Instruct secures the highest NDCG on both DL19 and DL20 at the cost of almost the lowest RPP and QPP. Beyond this result, several additional insights emerge: of Flan-T5-large Pointwise methods dominate the RPP and QPP metrics across different LLMs and datasets. yields pointwise.yes_no the highest RPP of 72.67 (DL19) and 68.3 (DL20) and achieves the maximum QPP of 111 queries/PetoFLOPs. The variant pointwise.qlm obtains the similar QPP metric but 5 10 worse RPP points. These methods obtain 10% to 30% relative NDCG gains over the baseline BM25 with negligible FLOPs consumption compared with other LLM-based rerankers. Scaling up hurts efficiency far more than it helps effectiveness. Most LLM-based rerankers gain NDCG when moving from Flan-T5-large to FlanT5-xl, yet see only marginal improvement from xl to xxl. For example, setwise. Heapsort rises from 0.670 to 0.693 and then to 0.706 in NDCG. Meanwhile, efficiency collapses: RPP plunges from 26.8 to 7.22 and then to 1.84, while QPP falls from 40.0 to 10.42 and finally to 2.61. In short, scaling boosts quality slowly but sacrifices RPP and QPP on much larger scale. Pairwise and listwise methods are intensely FLOP-hungry. Allpair sorting, though it delivers the highest NDCG on Flan-T5-xl (0.713), issues 9,900 LLM calls per query; its RPP collapses to around 0.10, and it processes barely 0.15 queries per peta FLOPs, making large-scale deployment impractical. Heapsort and bubblesort based variants cut the number of calls by roughly 90%, yet remain about orders of magnitude less efficient than pointwise methods on both RPP and QPP. Q2: Do the estimated FLOPs reflect the measured FLOPs? Figure 1 shows the relationship between the estimated and real FLOPs on DL19 for models of various sizes. The comparison contains both decoder-only and encoder-decoder architectures, providing comprehensive view of scaling trends. In both cases, the estimated and real FLOP counts scale with the model size, reflecting the expected rise in computational cost with increasing model parameters. The linear pattern across models illustrates that the estimated FLOPs correlate linearly with the real FLOPs and is consistent across model families and architectural types, suggesting that our FLOPs estimator is accurate and reliable. The close alignment between the two quantities provides empirical justification for the FLOPs estimator described in Section 3, affirming its reliability as proxy when real measurements are unavailable. Q3: How does latency relate to the FLOP counts? Figure 2 shows the relationship between latency and FLOP counts for two representative models: Qwen-7B (a decoder-only architecture) and Flan-T5-XXL (an encoder-decoder architecture), evaluated on the DL19 dataset. For both models, we observe that latency increases in accordance with the number of FLOPs. Importantly, the estimated FLOP counts exhibit correlation with latency that closely mirrors the relationship between real FLOP counts and latency. This alignment indicates that our FLOPs estimator approximates computational cost accurately and can serve as reliable predictor of real-world latency trends. This means that the estimator can be used to anticipate inference time without requiring direct hardware profiling, which is particularly useful when comparing models in platform-agnostic setting or during early-stage architecture design. Q4: What is the relationship between prompt length and FLOPs? Figure 3 shows the relationship between prompt length and FLOPs. As expected, both the estimated and actual FLOPs increase with longer prompts, reflecting the greater computational cost required to process more input tokens. Notably, the estimated FLOPs show strong correlation with prompt length that closely mirrors the pattern observed in the actual FLOPs. the estimator As the prompt becomes longer, reliably tracks the resulting increase in computation, consistent with what is observed empirically. This result provides additional validation for the robustness of our FLOPs estimator, demonstrating its ability to respond to changes in input length in the same way real FLOPs do. (a) Decoder (Qwen) (b) Encoder-Decoder (T5) Figure 1: Linear trends between estimated and real FLOPs for decoder (left) and encoder-decoder (right) models of various sizes on DL19. The same is observed for the DL20 dataset. (a) Decoder (Qwen) (b) Encoder-Decoder (T5) Figure 2: Latency in milliseconds increases with FLOPs on Qwen-7B (left) and Flan-T5-XXL (right) (a) Decoder (Qwen) (b) Encoder-Decoder (T5) Figure 3: FLOPs increases with prompt length for Qwen-7B (left) and Flan-T5-XL (right) on the DL19 dataset."
        },
        {
            "title": "6 Conclusion and Future Works",
            "content": "Due to limitation of existing metrics in evaluating the efficiency-effectiveness tradeoff of large language models as rerankers, we propose two metrics RPP and QPP to quantify the model performance. In addition, we provide calculator through closed-form and interpretable formula to compute the FLOPs and validate this estimation through experiments on existing decoder-only and encoder-decoder model architectures. The estimated FLOP count exhibits strong linear correlation with the actual measured values, allowing it to approximate real-world computational cost even without model execution. Future work includes conducting linear regression between the real and estimated FLOP counts to refine our estimation and adapting to more advanced architectures."
        },
        {
            "title": "Limitations",
            "content": "facebookresearch. 2019. fvcore. While our proposed FLOP-normalized metrics and estimator provide hardware-agnostic and interpretable approach to evaluating the efficiencyeffectiveness tradeoff of LLM-based rerankers, there are some limitations. The FLOP estimation relies on model architecture specifications and assumes consistent implementation across different frameworks, which may not hold in practice due to library-level optimizations or kernel differences. Although the estimator shows strong linear correlation with actual FLOP measurements, the approximation may be less accurate for models with more advanced architectures such as mixtureof-experts. While FLOPs offer more stable proxy than latency or token counts, they do not capture other real-world constraints such as memory bandwidth, energy consumption, or inference-time variability under dynamic system loads."
        },
        {
            "title": "References",
            "content": "Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, and Adam Jatowt. 2025. ASRank: Zeroshot re-ranking with answer scent for document retrieval. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 29502970, Albuquerque, New Mexico. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Xinyu Ma, Wei Yang, Daiting Shi, Jiaxin Mao, and Dawei Yin. 2025. Tourrank: Utilizing large language models for documents ranking with tournament-inspired strategy. In Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 20252 May 2025, pages 16381652. ACM. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, and 16 others. 2024. Scaling instruction-finetuned language models. J. Mach. Learn. Res., 25:70:170:53. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen Voorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820. Lukas Gienapp, Maik Fröbe, Matthias Hagen, and Martin Potthast. 2022. Sparse pairwise re-ranking with pre-trained transformers. In ICTIR 22: The 2022 ACM SIGIR International Conference on the Theory of Information Retrieval, Madrid, Spain, July 11 - 12, 2022, pages 7280. ACM. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Can Jin, Hongwu Peng, Anxiang Zhang, Nuo Chen, Jiahui Zhao, Xi Xie, Kuangzheng Li, Shuya Feng, Kai Zhong, Caiwen Ding, and Dimitris N. Metaxas. 2025. Rankflow: multi-role collaborative reranking workflow utilizing large language models. In Companion Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May 2025, pages 24842493. ACM. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, and 31 others. 2023. Holistic evaluation of language models. Trans. Mach. Learn. Res., 2023. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: python toolkit for reproducible information retrieval research with sparse and dense In Proceedings of the 44th Interrepresentations. national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2356 2362. Fabien LOISON. 2023. pypapi. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with large language model. CoRR, abs/2305.02156. Aliaksei Mikhailiuk, Clifford Wilmot, María PérezOrtiz, Dingcheng Yue, and Rafal K. Mantiuk. 2020. Active sampling for pairwise comparisons via approximate message passing and information gain maximization. In 25th International Conference on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15, 2021, pages 25592566. IEEE. MrYxJ. 2023. calflops: flops and params calculate tool for neural networks. Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document ranking with preIn Findings trained sequence-to-sequence model. of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 708718. Association for Computational Linguistics. Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, and Yi Fang. 2024. Q-PEFT: query-dependent parameter efficient fine-tuning for text reranking with large language models. CoRR, abs/2404.04522. Jay M. Ponte and W. Bruce Croft. 2017. language modeling approach to information retrieval. SIGIR Forum, 51(2):202208. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. Rankvicuna: Zero-shot listwise document reranking with open-source large language models. CoRR, abs/2309.15088. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2024. Large language models are effective text rankers with pairwise ranking prompting. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 15041518. Association for Computational Linguistics. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Ö. Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Rhea Sukthanker, Arber Zela, Benedikt Staffler, Aaron Klein, Lennart Purucker, Jörg K. H. Franke, and Frank Hutter. 2024. Hw-gpt-bench: Hardware-aware architecture benchmark for language models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1491814937. Association for Computational Linguistics. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD 20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 35053506. ACM. Stephen Robertson, Hugo Zaragoza, and 1 others. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, and Luke Zettlemoyer. 2025. Reasonir: Training retrievers for reasoning tasks. CoRR, abs/2504.20595. Vladislav Sovrasov. 2018. ptflops: flops counting tool for neural networks in pytorch framework. Le Zhang, Bo Wang, Xipeng Qiu, Siva Reddy, and Aishwarya Agrawal. 2025. Rearank: Reasoning re-ranking agent via reinforcement learning. arXiv preprint arXiv:2505.20046. Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. Deep query likelihood model for information retrieval. In Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II, volume 12657 of Lecture Notes in Computer Science, pages 463470. Springer. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024. setwise approach for effective and highly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 3847. ACM. Shengyao Zhuang and Guido Zuccon. 2021. TILDE: term independent likelihood model for passage reranking. In SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pages 14831492. ACM."
        },
        {
            "title": "A Appendix",
            "content": "A.1 BM25 (cid:88) i=1 IDF(qi) (qi, D)(k + 1) (cid:16) (qi, D) + 1 + avgdl (cid:17) (16) BM25 (Robertson et al., 2009) computes relevance score between query and document with Equation 16 where IDF denotes the inverse document frequency, (qi, D) represents the frequency of the ith query token in the document, is the length of the document, avgdl is the average document length in the corpus, and and are hyperparameters. Assuming that the term frequencies and the inverse document frequencies are precomputed and do not contribute to the runtime FLOP count, the upper bound on the FLOPs required for BM25 scoring can be estimated as: C(BM25) = 11 LQ ND (17) where LQ is the length of the query and ND is the number of documents, which is 100 for reranking top-100 documents in our experiments. This represents an upper bound because it assumes that every query token appears in every document. In cases where query token does not appear in document, the corresponding numerator becomes zero, resulting in zero FLOPs for that term-document pair instead of 11."
        }
    ],
    "affiliations": [
        "Independent Researcher, Beijing, China",
        "Santa Clara University, Santa Clara, CA",
        "Yale University, New Haven, CT"
    ]
}