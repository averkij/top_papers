{
    "paper_title": "Agency Is Frame-Dependent",
    "authors": [
        "David Abel",
        "Andr√© Barreto",
        "Michael Bowling",
        "Will Dabney",
        "Shi Dong",
        "Steven Hansen",
        "Anna Harutyunyan",
        "Khimya Khetarpal",
        "Clare Lyle",
        "Razvan Pascanu",
        "Georgios Piliouras",
        "Doina Precup",
        "Jonathan Richens",
        "Mark Rowland",
        "Tom Schaul",
        "Satinder Singh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning."
        },
        {
            "title": "Start",
            "content": "Agency Is Frame-Dependent David Abel Google DeepMind Andre Barreto Google DeepMind Michael Bowling Amii, University of Alberta Will Dabney Google DeepMind Shi Dong Google DeepMind Steven Hansen Google DeepMind Anna Harutyunyan Google DeepMind Khimya Khetarpal Google DeepMind Clare Lyle Google DeepMind Razvan Pascanu Google DeepMind Georgios Piliouras Google DeepMind Doina Precup Google DeepMind Jonathan Richens Google DeepMind Mark Rowland Google DeepMind Tom Schaul Google DeepMind Satinder Singh Google DeepMind"
        },
        {
            "title": "Abstract",
            "content": "Agency is systems capacity to steer outcomes toward goal, and is central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if system exhibits agency is notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether rock, thermostat, or robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of systems agency must be made relative to reference frame. We support this claim by presenting philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning. Keywords: Agency, Philosophy of Reinforcement Learning Acknowledgements The authors would like to thank Kim Stachenfeld and Vlad Mnih for their thoughtful comments on draft of the paper. 5 2 0 F 6 ] . [ 1 3 0 4 4 0 . 2 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement learning (RL) involves learning or decision making over time to achieve goal. Agents are often taken as the primary vehicles that carry out this learning and decision making, and as such have long been an essential element of RL. Moreover, agency is the lifeblood of an agentagency is the capacity that endows given system with the status of agent-hood. Thus, agency also stands as one of the elemental concepts of RL. In The Evolution of Agency, Tomasello (2022) makes an even stronger case for the role of agency in psychology: Every scientific discipline begins with proper domain, first principle ... In psychology, depending on ones theoretical predilections, that proper domain or first principle might be either behavior or mentality. But my preferred candidate would be agency, precisely because agency is the organizational framework within which both behavioral and mental processes operate. (p. 134, Tomasello, 2022). Following similar reasoning to Tomasello, we take it as essential that the science of RL is borne from an understanding not just of intelligence, learning, and decision making, but also of agency. To this end, we here investigate fundamental question about agency through the lens of RL: is agency an invariant, measurable property of an input-output system, or does it vary depending on other independent commitments? We draw from several distinct results within the RL literature to arrive at the conclusion that agency is fundamentally frame-dependent. Agency. Typical views of agency across biology (Ball, 2023), complex systems (Moreno, 2018; Moreno and Etxeberria, 2005), and philosophy (Barandiaran et al., 2009; Dretske, 1999) roughly define the concept as an input-output systems capacity to steer outcomes toward goal. We build around one canonical definition of agency developed by Barandiaran et al. (2009) that we present in four parts. First, to have agency, the system must be individual; it has boundary that separates it as an independent entity from its surroundings. Second, once we have chosen boundary, the system must be the source of its own action. Third, the system has some goals or norms that regulate its interactions with the environment. Fourth, the system steers its experience in light of these goals. We take these four conditions to be reasonable starting point for any account of agency, summarized in Figure 1(a). For the purposes of our main argument, we do not take debates about the precise definition of agency to have significant impact on our conclusionwe anticipate that regardless of how the semantics of agent or agency are worked out, the need for frame dependence will remain. For instance, Barandiaran et al. combine normativity and adaptivity into one property: this definition is perfectly valid, but will still admit frame-dependence. For further debates on this topic, see work by Moreno (2018), Ball (2023), or Dretske (1999). The Puzzle of Agency. fundamental puzzle then arises: which systems can be said to have agency? Dennett (1989) considers the cases of rock rolling down hill and thermostat modulating the temperature of roomin what sense do these two systems possess agency, and to what extent? And, the more critical scientific question: what principles can we turn to in order to determine whether each of these systems possess agency? Barandiaran et al. stipulate that system possesses agency if the four conditions are present within the system. If any one of them is missing, the system lacks agency. In this way, agency is taken to be binary, though naturally there is room for developing non-binary account. (a) Four-Part Account of Agency (b) Frame-Dependence Figure 1: (Left) four-part account of agency due largely to Barandiaran et al. (2009): system such as thermostat has agency if it has (1) boundary, (2) is the source of its own actions, (3) has goal, and (4) adaptively selects outputs based on inputs to pursue this goal. (Right) Our main claim: determination of the agency of system, such as thermostat, is dependent on choice of reference frame. The two reference frames depicted make different commitments about how we measure the four essential conditions of agency. For example, we could draw the boundary around our thermostat in several different ways, or understand the goal of the thermostat in different ways."
        },
        {
            "title": "2 Agency is Frame-Dependent",
            "content": "Main Claim. We here address this puzzle by arguing that the attribution of agency to system is fundamentally dependent on choice of reference frame. That is, the agency of any system is relative in the sense that it depends on arbitrary commitments that we collectively call reference frame. For example, one such commitment is whether the system is meaningfully pursuing goal; depending on how we codify what counts as meaningful goal-pursuit, the system will either be understood as having agency or not. We support this main claim by illustrating that each of the four properties of agency are themselves relative to choice of some reference object or commitmentthat is, reaching conclusion about whether given system possesses each property requires an independent commitment whose choice is arbitrary. At present, our definitions, claims, and arguments are purely philosophical, though we note that rigorous presentation of this perspective is natural and fruitful direction for future work. What is Reference Frame? An agent reference frame is collection of these four commitments that allow us to determine whether system has each of the four properties. That is, frame must include (1) boundary that decides what is internal to the agent and what is external; (2) reference object such as set of causal variables that allow for determination of whether the system is the cause of its action; (3) principle for isolating whether the system is meaningfully pursuing goal; and (4) choice of what changes in behavior count as meaningful adaptation. There are many valid ways to formalise these components; boundary could be cut in graph (Jiang, 2019) or Markov blanket (Friston et al., 2009). precise mathematical construction of reference frames is natural next step for further research. (1) Individuality. In order to attribute agency to an entity, an observer first must determine which entity they are referring to. Establishing boundary that separates this entity from its surroundings is critical step in determining whether system has agency. However, clearly identifying such boundary is non-trivial. Neils Bohr gives the example of person wielding stickdepending on the activity, the stick could be taken as part of the persons propensity for both action and observation (p. 93, Klein, 1967). As Harutyunyan (2020) notes, we might plausibly draw the boundary around the person and exclude or include the stick. In fact, as argued by Clark and Chalmers (1998), Jiang (2019), and Harutyunyan (2020), there are often many ways we can establish boundary that separates an environment and an agent in meaningful way. Jiang considers the example of model-free learning algorithm that is implemented using neural network to process its observations. As Jiang points out, the boundary we choose to draw could include the pseudo-random number generator and all layers of this network, or only include the last few layers of the network. Proposition 10 by Jiang (2019) (further discussed in Section 6.1), illustrates that many important quantities of an RL agent, such as the optimal policy or Bellman error, are boundary-dependent. We summarize these points in the following claim. Claim 1 (Adapted from Jiang, 2019 and Harutyunyan, 2020). Individuality is frame-dependent: Nearly all agents admit many plausible boundaries one could draw that separates them from their environment. Moreover, key quantities of an agent can change depending on which boundary is chosen. Individuality might be seen as qualitatively different from the other properties, since it involves selecting which system we would like to attribute agency to and what its inputs and outputs are. As such, it is natural to conclude that we recover different agent depending on how we draw the boundary. However, our argument does not stand only on the frame-dependency of individualitysince we take all four key properties of agency to be frame-dependent, our argument remains agnostic to any perceived qualitative difference between them. (2) Source of Action. Second, system must be the source of its own action. In the terms of Ball (2023), this property reflects whether the system is pushed around by its environment, or does the pushing itself. For instance, wall being knocked over by wrecking ball could be understood as taking the action of being knocked over. However, the source of this action (and the corresponding potential energy) did not ultimately originate in the wall, but rather in the wrecking ball and its operator. It might therefore be better to view the wall as lacking in the source of action. In contrast, bird flapping its wings intuitively satisfies the condition since this action is best thought of as originating from within the bird. Kenton et al. (2023) recently develop causal account that determines which entities in causal model might be said to satisfy roughly this property. The difficulty, as Kenton et al. note, is that reaching conclusion about the source of action in causal model rests entirely on the choice of causal variables. In this way, identifying whether given subsystem originates its own action depends on an independent, unrelated choice: the causal variables. Kenton et al. state directly: Note [discovering an agent in causal model] is relative to frame choice of variables that appear in our causal model (p. 2, Kenton et al., 2023). Following this reasoning, we claim that the source of action is also frame-dependent in that it depends on an arbitrary upstream commitment. Claim 2 (Adapted from Kenton et al., 2023). Source of action is frame-dependent: There exist cases with at least two plausible choices of causal variables where the former choice identifies an agent in the causal model, and the latter choice refutes the presence of an agent in the causal model. 2 (3) Normativity. Third, and perhaps most crucially, agency is about goal-directedness. The trouble is that every system with outputs can be understood as if (Friedman, 1953) it is goal-directed. More concretely, every input-output system can be well-explained in terms of goal-directedness. For example, in the case of our thermostat, even broken thermostat whose output always sets the temperature of room to 20 can be understood as having the goal set the temperature to 20. More trivially, rock can be viewed as having the goal of rolling down hill, or as having the goal of convincing all observers that it is rock. What makes these meaningless, as opposed to meaningful goals? This challenge is reflected in one of the classical results of inverse RL first discussed by Russell (1998) and Ng and Russell (2000): the zero reward function is always consistent with every system that outputs signals that are understood as decisions. In other words, reward is under-determined by behavior (Cao et al., 2021) without intervention (Amin et al., 2017). To overcome this challenge, variety of approaches have explored the use of biases or other principles that constrain the space of viable reward functions, such as Occams razor (Armstrong and Mindermann, 2018), or the now standard approach invoking maximum entropy proposed by Ziebart et al. (2008). These upstream principles can in some cases rule out certain kinds of goal-directedness as uninteresting, or elevate others as meaningful. Hence, we must again invoke an additional principle to determine whether given system is meaningfully goal-directed. One common approach is to ask how useful it is to explain or understand the system in terms of goal-directedness, as first argued by Dennett (1989). In other words, to determine if system meaningfully has goal, we require some extraneous commitments that again amount to reference frame. Claim 3. Normativity is frame-dependent: For nearly all cases of input-output systems, whether that system is meaningfully goal-directed depends on reference point whose choice is arbitrary. (4) Adaptivity. Fourth, agency is about adaptivity, which captures whether systems outputs are influenced by its inputs, and to what extent. In On the Definition of Adaptivity, Zadeh (1963) suggests form of frame-dependence: ...every system is adaptive with respect to [something] ... what matters is not whether [the system] is adaptive or not, but what ... it is adaptive [to] (p. 470). Following this reasoning, adaptivity can be understood as frame-dependent in the sense that it depends on what is chosen as the relevant reference class used to reach determinations about systems adaptivity. For example, in RL, we might ask whether policy that maps each input state to an action is adaptive. On one reference frame, we might treat any change in the output as adaptivity, while on another reference frame, we could view this policy as fixed and non-adaptive function since it always chooses the same action every time it receives the same input. Abel et al. (2023) make this argument in more general case and show that all policies can either be understood as adaptive, or not, depending on reference class of meaningful changes in behavior. In other words, if we want to determine whether system is adaptive, we need to first agree on the class of experience-influenced changes of behavior that count as adaptivity. Depending on this choice, system will either be adaptive, or not. This choice then acts as reference frame. Claim 4 (Adapted from Zadeh, 1963 and Theorem 3.1 of Abel et al., 2023). Adaptivity is frame-dependent: For many inputoutput systems, there will exist at least two reference frames (and in most cases, many more) such that according to the first reference frame the system is adaptive, while according to the second, the system is not. In summary, reaching determination about each of the four properties of agency requires reference to other fixed commitments that collectively comprise reference frame. Since agency is simply the logical conjunction of these latter three properties conditioned on the choice of boundary, then reaching conclusion about agency itself must be made in reference to these commitments. In other words: agency is frame-dependent."
        },
        {
            "title": "3 Discussion",
            "content": "We have here argued that agency is frame-dependent by illustrating the sense in which each of the four essential conditions of agency are themselves frame-dependent. We take this to have far reaching implications for disciplines that study agents and agency. Intelligence and Agency. The relationship between intelligence and agency is not yet well understood. For instance, does intelligence require agency, and vice versa? Depending on how this question is addressed, frame-dependence may have significant implications for our understanding of intelligence, in addition to other emergent properties of information processing systems. We believe that exploring this relationship through the lens of frame-dependence offers new frontier for understanding central concepts of RL. Reference Frames. We stop short of presenting rigorous mathematical definition of reference frames, as well as formal proof of the frame-dependence of agency. natural next step to further this line of work is to develop precise definition of agent reference frames along with formal proof supporting our main claim. We speculate that the building blocks to do so are already in place in the field, but have not gone through the careful work of developing the definitions and arguments more formally. 3 Choosing Reference Frame. How do we choose an appropriate reference frame? It is unclear which frame-selection principles are defensible, and what implications these principles carry for our study of agents. The Intentional Stance (Dennett, 1989) asserts roughly that it is most useful to understand certain systems as agents; taken to its natural conclusion, this could be formalised as principle for selecting frames in terms of predictive or explanatory power. An important direction for further research will investigate, formalise, and compare different principles for selecting reference frames. Marrs Levels, Dennetts Stances. The proposal to adopt reference frames coheres with the perspectives of Dennett (1989) as well as Marr (2010), who each argue for understanding certain phenomena at different levels of abstraction. Marr argues that our attempts to understand cognitive process such as vision can be cast through three distinct levels: the hardware, the algorithmic, and the computational. Dennett argues that there are distinct levels of abstraction we should adopt depending on the content of our study. Like Marr, the lowest-level of abstraction is the physical, according to which we examine physical properties of system. The second level is the design stance, according to which we examine how the system is designed to make predictions about its operation or purpose. The third is the intentional stance, according to which we study the content of minds themselves: of beliefs and desires. Both Dennetts intentional stance and Marrs levels suggest that our study of concepts like agency needs to carefully calibrate in order to examine the right kinds of content, and reach the right kinds of conclusions. We suggest that reference frames may offer path to connect physical substrates with more abstract propositions such as those related to agency."
        },
        {
            "title": "References",
            "content": "D. Abel, A. Barreto, B. V. Roy, D. Precup, H. van Hasselt, and S. Singh. definition of continual reinforcement learning. In Advances in Neural Information Processing Systems, 2023. K. Amin, N. Jiang, and S. Singh. Repeated inverse reinforcement learning. In Advances in Neural Information Processing Systems, 2017. S. Armstrong and S. Mindermann. Occams razor is insufficient to infer the preferences of irrational agents. In Advances in Neural Information Processing Systems, 2018. P. Ball. Organisms as agents of evolution. John Templeton Foundation: West Conshohocken, PA, USA, 2023. X. E. Barandiaran, E. Di Paolo, and M. Rohde. Defining agency: Individuality, normativity, asymmetry, and spatiotemporality in action. Adaptive Behavior, 17(5):367386, 2009. H. Cao, S. Cohen, and L. Szpruch. Identifiability in inverse reinforcement learning. In Advances in Neural Information Processing Systems, 2021. A. Clark and D. Chalmers. The extended mind. Analysis, 58(1):719, 1998. D. C. Dennett. The intentional stance. MIT press, 1989. F. I. Dretske. Machines, plants and animals: the origins of agency. Erkenntnis (1975-), 51(1):1931, 1999. M. Friedman. Essays in positive economics. University of Chicago press, 1953. K. J. Friston, J. Daunizeau, and S. J. Kiebel. Reinforcement learning or active inference? PloS One, 4(7):e6421, 2009. A. Harutyunyan. What is an agent? http://anna.harutyunyan.net/wp-content/uploads/2020/09/What_ is_an_agent.pdf, 2020. N. Jiang. On value functions and the agent-environment boundary. arXiv preprint arXiv:1905.13341, 2019. Z. Kenton, R. Kumar, S. Farquhar, J. Richens, M. MacDermott, and T. Everitt. Discovering agents. Artificial Intelligence, page 103963, 2023. O. Klein. Glimpses of Niels Bohr as scientist and thinker. Niels Bohr. His life and work as seen by his friends and colleagues. London: Interscience Publishers, pages 7493, 1967. D. Marr. Vision: computational investigation into the human representation and processing of visual information. MIT press, 2010. A. Moreno. On minimal autonomous agency: natural and artificial. Complex Systems, 27(3), 2018. A. Moreno and A. Etxeberria. Agency in natural and artificial systems. Artificial Life, 11(1-2):161175, 2005. A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In Proceedings of the International Conference on Machine Learning, 2000. S. Russell. Learning agents for uncertain environments. In Proceedings of the Conference on Computational Learning Theory, 1998. M. Tomasello. The evolution of agency: Behavioral organization from lizards to humans. MIT Press, 2022. L. A. Zadeh. On the definition of adaptivity. Proceedings of the IEEE, 51(3):469470, 1963. B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforcement learning. In Proceedings of the AAAI Conference on Artificiall Intelligence, 2008."
        }
    ],
    "affiliations": [
        "Amii, University of Alberta",
        "Google DeepMind"
    ]
}