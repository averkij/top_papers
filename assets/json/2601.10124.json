{
    "paper_title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "authors": [
        "Sicheng Yang",
        "Zhaohu Xing",
        "Lei Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 4 2 1 0 1 . 1 0 6 2 : r VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation Sicheng Yang1 Zhaohu Xing1 Lei Zhu1,2 1The Hong Kong University of Science and Technology (Guangzhou) 2The Hong Kong University of Science and Technology"
        },
        {
            "title": "Abstract",
            "content": "Consistency learning with feature perturbation is widely used strategy in semisupervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require careful manual tuning of the dropout rate, which is sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design dualbranch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce PostVQ Feature Adapter (PFA) to incorporate guidance from foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg."
        },
        {
            "title": "Introduction",
            "content": "Medical image segmentation serves as fundamental step in numerous clinical applications, such as disease diagnosis [1, 2], anatomical structure delineation [35], lesion localization [68], and surgical planning [911]. In recent years, supervised deep learning methods have demonstrated outstanding performance in segmentation tasks, significantly surpassing traditional approaches in both accuracy and robustness [1214]. However, these methods typically require large amounts of finely annotated medical data, the collection of which is not only expensive and labor-intensive but also demands substantial domain expertise. To address this limitation, semi-supervised learning, which leverages small set of labeled data alongside larger pool of unlabeled samples, has emerged as promising direction and is receiving growing attention in the medical imaging community. Consistency learning represents widely adopted paradigm in semi-supervised medical image segmentation, designed to enforce prediction invariance under various perturbations [15]. Featurelevel dropout [16, 15] is commonly employed technique within this framework, introducing perturbation into intermediate representations to enhance model robustness. However, its effectiveness is critically dependent on the selection of the dropout rate (DR), hyperparameter. Equal contribution. Lei Zhu (leizhu@ust.hk) is the corresponding author. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). As depicted in Fig. 1, our experiments reveal two primary challenges associated with this methodology. Firstly, the adoption of lower dropout rates (e.g., DR=0.3, DR=0.5) yields negligible impact on segmentation performance. This suggests that the induced perturbation is insufficient to provide meaningful regularization. Secondly, elevating the dropout rate (e.g., DR 0.7) leads to rapid decline in performance metrics. Specifically, Dice and Jaccard scores exhibit sharp decrease, while HD95 and ASD values significantly increase, indicating substantial degradation in both structural accuracy and boundary delineation. Qualitative analyses further corroborate these findings, demonstrating that segmentation outputs under high dropout rates frequently fail to yield meaningful segmentation results, rendering them practically unusable. These observations underscore the inherent difficulty in identifying an optimal dropout rate that consistently enhances performance while mitigating the risk of model collapse. Consequently, there is pressing need for more stable perturbation strategy, thereby achieving the desired regularization effect in predictable and robust manner. In this paper, we introduce VQ-Seg, novel semisupervised framework for medical image segmentation. The core innovations of our approach include: first, the Quantized Perturbation Module (QPM) designed to replace traditional dropout by enabling controlled and structured perturbations of encoded features within discrete VQ space. Unlike dropout, which relies on hyperparameters such as the dropout rate, QPM leverages distances between codebook codewords to define perturbation strategies, thereby offering enhanced interpretability and stability. To address potential visual information loss arising from vector quantization, we tackle this issue from two perspectives. Initially, we construct dual-branch architecture that shares post-quantized space, unifying image reconstruction and semantic segmentation tasks within joint optimization framework in discrete representation space. This design facilitates the preservation of critical structural information from images while also utilizing the reconstruction task as self-supervisory signal to encourage the VQ encoder to learn improved representations. Furthermore, to enhance semantic consistency and mitigate the loss of high-level semantic information during quantization, we incorporate foundation model-guided alignment strategy. Specifically, we develop Post-VQ Feature Adapter (PFA) that employs contrastive learning to align quantized features with semantic features derived from pre-trained visual foundation model. This approach effectively enriches the semantic content and spatial consistency of discrete representations. Extensive experiments conducted on our collected Lung Cancer (LC) dataset (comprising 828 annotated cases) and the open-source ACDC dataset demonstrate that VQ-Seg significantly outperforms state-of-theart semi-supervised segmentation methods across key evaluation metrics, including Dice, Jaccard, HD95, and ASD. Detailed ablation studies further validate the efficacy and synergistic effects of the individual components of VQ-Seg. Figure 1: Effect of dropout rate on segmentation performance in fully supervised setting on the LC dataset. rates show negligible Low dropout impact, whereas high dropout rate (DR 0.7) severely degrades both quantitative metrics and visual outputs. Notably, DR = 0.9 leads to unusable predictions, highlighting the challenge of selecting an optimal dropout rate. In summary, our contributions are five-fold: We collected new large-scale dataset, the Lung Cancer (LC) dataset, comprising 828 chest CT scans with annotations of central-type carcinoma of the lung. We propose novel Quantized Perturbation Module (QPM) to perturb features within discrete vector quantization (VQ) space. QPM enables more structured and controllable mechanism for representation perturbation by shuffling the spatial locations of codebook indices, offering enhanced interpretability and stability compared to traditional dropout. We introduce dual-branch architecture where the post-quantized space is concurrently utilized for both image reconstruction and the downstream segmentation task. This design 2 uses reconstruction as self-supervisory signal, encouraging the VQ encoder to learn better representations and preserving essential visual information. We develop foundation model-guided alignment strategy, where frozen foundation model (FM) serves as an external semantic prior to guide and regularize the Post-VQ representation. Post-VQ Feature Adapter (PFA) is introduced to transform the quantized codebook embeddings into semantically aligned space using contrastive learning, mitigating the loss of high-level semantic features. We compare our method with multiple cutting-edge methods on the LC dataset and opensource datasets, demonstrating that our approach achieves state-of-the-art performance, which verifies its effectiveness."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Semi-supervised Medical Image Segmentation Obtaining large-scale, high-quality manual annotations for medical images is both time-consuming and labor-intensive [17, 18]. Semi-supervised learning (SSL) methods have thus gained attention as promising solution for medical image segmentation under limited annotation settings [19]. Among various SSL strategies, pseudo-labeling-based approaches are widely adopted due to their simplicity and ease of implementation [20]. These methods [2124] typically involve training an initial model on the labeled dataset and then generating pseudo-labels. However, the use of pseudo-labels can introduce noise and lead to training instability, especially when incorrect labels are propagated [25]. To address this, subsequent research [2628] has incorporated consistency regularization to enforce prior constraints on the learned representations. Consistency regularization is grounded in the smoothness assumption, which posits that small perturbations to the input data should not significantly alter the models predictions [29]. In this context, the design and implementation of perturbations play critical role in determining the effectiveness of the model. 2.2 Feature-level Dropout for Consistency Regularization We focus on feature-level Dropout strategies. Leveraging Dropout-based regularization for consistency learning has become widespread approach in semi-supervised medical image segmentation [30]. Specifically, techniques such as Monte Carlo Dropout (MC-Dropout) [15] have been employed to model uncertainty and enforce prediction consistency under random feature perturbations. These perturbations help improve the generalization ability of the model by encouraging it to make stable and robust predictions despite the uncertainty inherent in the data. Several studies [15, 31, 28] have shown the effectiveness of such methods, demonstrating improved performance in semi-supervised medical image Segmentation tasks. However, while these approaches have shown promise, they typically require the careful tuning of hyperparameterthe Dropout rate. The appropriate setting of the Dropout rate is critical. Both peer observations [28] and our own empirical studies (see Fig. 1) suggest that this process is often challenging in practice, as the optimal Dropout rate may vary depending on the specific dataset, task, and network architecture. In particular, inappropriate Dropout configurations can lead to unstable training dynamics, such as excessive regularization that prevents the model from learning effectively [32, 30]. This highlights significant limitation of traditional Dropout-based methods. This situation motivates the need for alternative strategies that can provide controlled and effective feature-level perturbations."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Overview Fig. 2 provides an overview of our Vector Quantization-based Semi-supervised Segmentation framework (VQ-Seg). The input image is first encoded into continuous latent features, which are then quantized into discrete codebook space via the Vector Quantization (VQ) module. To introduce structured perturbations for consistency learning, we propose the Quantized Perturbation Module (QPM), which shuffles codebook indices based on their learned distances, offering stable and interpretable alternative to dropout. To compensate for potential visual information loss, VQ-Seg 3 Figure 2: Overview of the VQ-Seg framework. The input image is encoded into continuous features z, which are then quantized into discrete codebook space via vector quantization (VQ). Quantized Perturbation Module (QPM) introduces controllable perturbations for consistency learning. The dual-branch architecture jointly optimizes image reconstruction and segmentation using the shared Post-VQ features. Additionally, Post-VQ Feature Adapter (PFA) aligns the quantized features with semantic embeddings from foundation model (FM). adopts dual-branch architecture that jointly optimizes reconstruction and segmentation tasks using the shared Post-VQ feature space. Furthermore, the Post-VQ Feature Adapter (PFA) aligns quantized features with semantic embeddings from pre-trained foundation model through patch-wise contrastive learning, enriching representation semantics and reducing drift. 3.2 Theoretical Motivation To analyze the sensitivity of the model to perturbations in the feature space, we interpret the KL divergence KL(P Q) as measure of the perturbation radius from the original distribution to the perturbed one Q. This is inspired by distributionally robust optimization (DRO) [33, 34], where the worst-case risk is evaluated over an uncertainty set defined by bounded KL divergence. Thus, the divergence reflects the extent to which the input perturbation has structurally shifted the feature representation. Supported by recent studies [35], for Dropout, the KL divergence between the posterior distribution (induced by dropout) and prior distribution can be approximated as: DKL(P Q) (cid:18) 1 1 2 (cid:19) + log(1 p) , (1) where (0, 1) denotes the dropout rate. As increases, the approximation indicates growing perturbation radius, with the KL divergence rising sharply (see Appendix for full derivation). Such behavior reveals the inherent instability of dropout from theoretical standpoint: large dropout rate causes the posterior distribution to deviate significantly from the prior, potentially leading to over-regularization and degraded learning performance, as supported by our empirical results (see Fig. 1). To mitigate this issue, we propose the Quantized Perturbation Module (QPM), which perturbs features within discrete vector quantization (VQ) space, enabling more structured and controllable mechanism for representation perturbation. 4 Figure 3: concrete example of the Quantized Perturbation Mechanism (QPM) with codebook size of = 4 and perturbation strength ϵ = 0.7. It illustrates the probabilistic transitions from the original codeword c1 (index 1) to itself and other codewords (c2, c3, c4) with their respective probabilities π(j1), where the transition to c2 (49%) exhibits the highest probability of replacement. Figure 4: Architecture of the Post-Quantization Feature Adapter (PFA) designed for aligning postquantization features with frozen Foundation Model (FM) via patch-wise contrastive loss, Lalign. The PFA initially employs resizing operation followed by 1 1 convolution to match the spatial resolution and channel dimensionality of the FM features, thereby facilitating subsequent semantic alignment. 3.3 Quantized Perturbation Module (QPM) In our method, the encoder output = fenc(x) represents continuous feature embedding of the input medical image x. This embedding is then projected into discrete latent space by selecting the nearest codeword from learnable codebook = {c1, c2, ..., cK}, where denotes the total number of codewords, i.e., the size of the codebook. The quantized codeword index is obtained by = arg min j{1,...,K} cj. (2) After the encoding step, we apply perturbation strategy π(j i). For each original codeword ci (with index i), new codeword cj (with index j) is sampled based on the conditional probability π(j i), and cj replaces the original codeword as the input to the decoder. This procedure introduces structured perturbation within the discrete latent space, resulting in more controlled and interpretable form of regularization. To implement this perturbation, we first define prior distribution (ci) over the codebook Z. We assume uniform distribution over all codewords: (ci) = 1 , {1, 2, ..., K}. (3) The perturbation strategy defines the conditional probability of transitioning from the current codeword ci to another codeword cj: π(j i) = (cid:40) 1 ϵ, ϵ exp(d(ci,cj )) Zi if = if = , (4) where ϵ [0, 1] is control term controlling the perturbation strength, d(ci, cj) is distance metric between codewords ci and cj, and Zi = (cid:80) k=i exp(d(ci, ck)) is normalization factor. The resulting perturbed distribution Q(cjϵ) over the codewords is then given by: Q(cjϵ) = (cid:88) i= (ci)π(j i) = 1 ϵ δij + ϵ (cid:88) i=j exp(d(ci, cj)) k=i exp(d(ci, ck)) (cid:80) . (5) The KL divergence between the prior distribution and the perturbed distribution is: DKL(P Q) = (cid:88) j=1 (cj) log (cid:19) (cid:18) (cj) Q(cjϵ) = 1 (cid:88) j=1 log (KQ(cjϵ)) . (6) Compared to Dropout, our QPM offers several advantages. The perturbed distribution Q(cjϵ) is always well-defined and bounded, ensuring numerical stability (Detailed proof can be found 5 in Appendix B). QPM is directly controlled via single control term ϵ and is influenced by the learned codebook structure. Moreover, QPM introduces structured perturbations by probabilistically transitioning between learned codebook entries based on their distances, yielding potentially more interpretable and controllable form of regularization compared to the stochastic perturbations inherent in Dropout (refer to an example presented in Fig. 3). The perturbation remains within the learned discrete latent space, offering distinct approach to regularization. 3.4 Dual-Branch Architecture with Shared Post-VQ Space Visual information inherently exists in continuous space. The process of quantization, while enabling discrete representations, can lead to loss of fine-grained details and potentially reduce the representational capacity for modeling intricate visual structures [36, 37]. To address this, we introduce dual-branch architecture where the post-quantization feature space (Post-VQ Space), is concurrently utilized for both image reconstruction and the downstream segmentation task. By jointly optimizing the quantized feature space with respect to these two objectives, we encourage it to encode fundamental structural information, as well as segmentation-relevant representations. This dual utilization facilitates the preservation of essential visual information within the Post-VQ Space, without compromising the performance of the segmentation task. ) for xa, q(zS As depicted in Fig. 2, both the teacher (T ) and student (S) decoder networks adopt this design. Following the encoding stages and vector quantization, we obtain the discrete representations q(zS ) for xl, q(zS ) for xu processed by the student, and q(zT ) for xu processed by the teacher. The QPM-perturbed quantized representation of the unlabeled data is denoted as q(zS ) = QPM (q(zS )), which is used by the student network. These discrete representations, where q(z) {q(zS ), q(zT ), q(zS )}, serve as the input to the image decoder (Di) and the segmentation decoder (Ds), as described by: ), q(zS The loss function for the labeled data xl with its associated ground truth segmentation yl is defined as: ˆx = Di(q(z)), ˆy = Ds(q(z)). (7) Ll = Lrec(xl, ˆxS ) + Lseg(yl, ˆyS ). (8) = DT = DT )). Subsequently, pseudo-label yu is derived from yT For the unlabeled data xu, we employ pseudo-labeling strategy. Initially, predicted segmentation map is generated by the teacher network Tdec as yT )), with corresponding reconstructed image ˆxT (q(zT by selecting the class with the maximum probability (argmax operation). This pseudo-label yu is then used as the target segmentation for the QPM-perturbed quantized representation q(zu) of the unlabeled data in the student network Sdec. The loss for the unlabeled data in the student network Sdec integrates both the reconstruction loss of the original unlabeled data and segmentation loss based on the teacher-generated pseudo-label applied to the perturbed representation: (q(zT Lu = Lrec(xu, ˆxS ) + Lseg(yu, ˆyS ) + Lseg(yu, ˆyS ). The overall dual-branch loss is defined as: Ldb = Ll + λuLu (9) (10) where λu is hyperparameter that balances the contribution of the unlabeled loss Lu relative to the labeled loss Ll. Lrec denotes the L1 loss, and Lseg represents the Cross-Entropy loss. By jointly minimizing Ldb, we encourage the shared quantized feature space to encode information beneficial for both reconstructing the input image and segmenting relevant structures. This synergistic optimization leverages supervision from labeled data and self-supervisory signals from unlabeled data via pseudo-labeling, where the teacher network generates pseudo-labels to guide the student networks learning on the QPM-perturbed quantized representations of the unlabeled data. 3.5 Foundation Model-Guided Alignment for Post-VQ Space Although vector quantization (VQ) compacts the feature space, its discretization process introduce semantic bias and loss of fine details [38], which is particularly detrimental in high-precision tasks such as medical image segmentation. To mitigate these issues, we propose foundation model-guided alignment strategy, where frozen foundation model (FM) serves as an external semantic prior to guide and regularize the Post-VQ representation. Specifically, we introduce Post-VQ Feature Adapter (PFA) that transforms the quantized codebook embeddings into semantically aligned space, as illustrated in Fig. 4. The PFA first resizes the VQ features and then applies 1 1 convolution to match the spatial resolution and channel dimension of the FM features. Denoting the output of the PFA as pfa RH C and the corresponding FM features as fm. We then apply patch-wise contrastive learning objective [39, 40] to minimize the semantic discrepancy between the adapted VQ features and the FM representations: Lalign ="
        },
        {
            "title": "1\nHW",
            "content": "HW (cid:88) i=1 log (cid:16) exp (cid:17) , fm )/τ sim(f pfa (cid:16) sim(f pfa , fm )/τ (cid:80)HW j=1 exp (cid:17) , (11) ab , and τ is the temperature parameter. pfa where sim(a, b) = ab represent the feature vectors at spatial location (flattened from the 2D grid) extracted from the adapted VQ features and the FM features, respectively, both with dimensionality . The indices {1, . . . , } correspond to all patch locations on the spatial grid. The patch-wise formulation enables localized semantic supervision, allowing the model to align not just global representations but also fine-grained spatial semantics. By minimizing Lalign, the discretized features are encouraged to retain rich and spatially semantic information that are consistent with the external FM prior, effectively mitigating the loss of detail and semantic drift introduced during quantization. In practice, we adopt DINOv2 [41] as the foundation model to provide semantic supervision. and fm 3.6 Total Optimization Objective Drawing upon the loss formulations in equations 10 and 11, the overall optimization objective of our framework is defined as follows: = Ldb + λaLalign, (12) where λa is hyperparameter that balances the two loss terms. decrease in Ldb indicates that the model is learning more effective representations for reconstructing images and segmenting relevant lesion regions. Furthermore, the optimization of Lalign suggests that stronger consistency is achieved between quantized features and external foundation model priors, thereby mitigating detail loss and semantic shift introduced during the quantization process. Detailed discussions on the hyperparameter can be found in the ablation study part 4.5."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets. Lung Cancer (LC) Dataset. We collect multi-center dataset including 828 chest CT scans of central-type lung carcinoma, which reveals the inherent challenges associated with detecting and analyzing such cases, presenting subtle anomalies within the imaging data. There exists one segmentation target per volume, and the dominant lesion is annotated precisely for each case. ACDC dataset. This dataset [42] is cardiac MRI collection comprising 100 short-axis cine-MRI scans, acquired using both 3T and 1.5T scanners. Following previous studies, we applied the 701020 split ratio for training, validation, and testing on the LC dataset. To ensure fair comparison, all experiments were conducted on 2D slices. 4.2 Implementation Details. Our model is implemented in PyTorch 2.4.1. All 2D slices are resized to 224 224 before being used as input. We adopt mini-batch consisting of 8 labeled samples and 4 unlabeled samples per iteration. The training process runs for 100,000 iterations, optimizing the standard pixel-wise cross-entropy loss using the AdamW optimizer with polynomial learning rate schedule. The initial learning rate is set to 5 106 for the backbone and scaled by factor of 40 for the task-specific heads, followed by 7 Table 1: Quantitative comparison on the LC dataset with two labeled ratio settings (5%, 10%) using four metrics: Dice and Jaccard (), HD95 and ASD (). Best results are in bold, second best are underlined. Method UNet-F [47] UNet-S [47] nnUNet-F [54] nnUNet-S [54] UA-MT [15] MCNet [48] SSNet [49] BCP [50] ARCO [51] ABD [52] Unimatch [53] Ours 5% Labeled 10% Labeled Dice Jaccard HD95 ASD Dice Jaccard HD95 ASD 0.8345 0.4343 0.8259 0.4590 0.6029 0.6378 0.6328 0.6243 0.6162 0.6414 0.6493 0. 0.7386 0.3118 0.7236 0.3438 0.4647 0.4970 0.4886 0.4854 0.4778 0.5024 0.5071 0.5257 6.9634 26.0498 4.2533 13.2746 48.6681 15.2759 25.1005 26.9303 36.2256 12.5608 17.8700 12.2525 2.2913 12.6188 1.4216 8.8636 24.6020 4.9231 9.3180 10.4789 14.6243 5.9661 5.4526 4. 0.8345 0.6490 0.8259 0.6538 0.7222 0.7555 0.7480 0.7252 0.7246 0.7468 0.7511 0.7852 0.7386 0.5175 0.7236 0.5194 0.5989 0.6414 0.6278 0.5994 0.5945 0.6244 0.6333 0.6731 6.9634 21.4063 4.2533 25.2100 11.6724 16.1903 14.9581 18.9768 14.4803 12.6570 17.0178 11. 2.2913 7.3382 1.4216 8.9332 5.4939 9.9647 7.3399 6.5105 4.3660 6.7437 5.7388 4.2094 Figure 5: Visual results on LC with 5% and 10% labeled data show that VQ-Seg consistently yields more accurate predictions of anatomical structures and boundaries than all other compared methods. decay of (1 t/T )0.9, where and denote the current and total training iterations, respectively. As illustrated in Fig. 2, several data augmentation strategies are applied during training, including random rotation, color jittering, and CutMix-based strong perturbations [43] for unlabeled data. We employ the Straight-Through Estimator (STE) [44] in VQ training. In our main experiments, the VQ codebook size is set to = 16,384, and we further conduct ablation studies in Sec. 4.5 to evaluate its impact. Moreover, the codebook mapping mechanism together with the entropy-based regularization loss [45, 46] is incorporated to accelerate the learning dynamics of the codebook. The teacher network is updated using an exponential moving average (EMA) of the student network, i.e., θt α θt + (1 α) θs, where θt and θs denote the parameters of the teacher and student networks, respectively. We set the EMA decay rate to α = 0.996. All experiments are conducted on cloud-computing platform equipped with four NVIDIA GeForce RTX 4090 GPUs. 4.3 Evaluation and Metrics. We compare our method with other state-of-the-art (SOTA) approaches, including UNet [47], UAMT [15], MCNet [48], SSNet [49], BCP [50], ARCO [51], ABD [52], and Unimatch [53]. Note that methods labeled with denote fully supervised models trained using all labeled data, while those labeled with indicate semi-supervised models trained with limited annotations. To ensure fair comparison, our VQ-Seg model adopts the same encoder and decoder architecture as Unimatch [53]. The main difference lies in the introduction of quantization and alignment process after the encoder output. Segmentation performance is evaluated using four common metrics: Dice and Jaccard for region overlap, and HD95 and ASD for boundary accuracy. 8 Table 2: comprehensive ablation study evaluating the contributions of the Quantized Perturbation Module (QPM), the dual-branch architecture with shared Post-VQ space (DB), and the Post-VQ Feature Adapter (PFA) on segmentation performance using the LC dataset (10% labeled). Base QPM DB PFA Dice 0.7443 0.7701 0.7784 0.7761 0.7852 Jaccard HD95 14.2153 0.6238 13.0246 0.6559 12.4728 0.6620 12.7381 0.6597 11.6179 0.6731 ASD 5.2301 4.9378 4.6013 4.7005 4.2094 ϵ Table 3: Impact of hyperparameters on metrics. Param. Value Dice 0.7741 0.3 0.7803 0.5 0.7852 0.7 0.7418 0.9 0.7720 1 0.7852 5 0.7765 10 0.7852 1 0.7670 5 0.7843 Jaccard HD95 12.3821 0.6612 11.9042 0.6685 11.6179 0.6731 14.8054 0.6142 11.5080 0.6591 0.6731 11.6179 11.9235 0.6640 11.6179 0.6731 11.7421 0.6553 11.8852 0.6724 ASD 4.2983 4.2150 4.2094 5.1328 4.2672 4.2094 4.1926 4.2094 4.1652 4.1013 λu λa 4.4 Comparison Results Quantitative Comparisons. Tables 1 and 7 summarize the quantitative comparisons for each method on the LC and ACDC datasets across two labeled ratio settings (5% and 10%). On the LC dataset, our method demonstrates superior performance. At 5% labeled data, VQ-Seg achieves the highest Dice (0.6643) and Jaccard (0.5257), outperforming the second-best Unimatch (Dice: 0.6493, Jaccard: 0.5071) by 1.5% and 1.86%, respectively. It also yields the best HD95 (12.2525) and ASD (4.2276), improving upon the second-best ABD (HD95: 12.5608) and MCNet (ASD: 4.9231) by 0.3083 and 0.6955. With 10% labeled data, our method maintains the lead, achieving the highest Dice (0.7852) and Jaccard (0.6731), surpassing the second-best MCNet (Dice: 0.7555, Jaccard: 0.6414) by 2.97% and 3.17%. It also yields the best HD95 (11.6179) and ASD (4.2094), improving upon the second-best UA-MT (HD95: 11.6724) and ARCO (ASD: 4.3660) by 0.0545 and 0.1566. Results on the ACDC dataset (see Appendix and Table 7) exhibit similar trend, confirming the robustness and generalizability of our approach across diverse datasets. Visual Comparisons. As depicted in Fig. 5, our VQ-Seg effectively identifies the cancerous areas with high precision. Segmentation outcomes exhibit improved consistency and clearer boundary delineation when compared to other state-of-the-art techniques. Notably, our method better preserves the structural integrity of cancer regions. These visual advantages further demonstrate the superior performance of our model across various cases. For more comprehensive understanding, please refer to Appendix for the statistical visualization analysis and Appendix for the t-SNE visualization of the codebook update dynamics. 4.5 Ablation Studies Module Analysis. As shown in Table 2, we conduct step-by-step ablation to evaluate the contribution of each proposed component. The baseline model is VQ-embedded Unimatch framework. Based on this, incorporating the Quantized Perturbation Module (QPM) leads to notable improvement in Dice from 0.7443 to 0.7701. Adding the dual-branch (DB) architecture further enhances performance to 0.7784, while using the Post-VQ Feature Adapter (PFA) alone yields Dice of 0.7761. When all three modules are combined, the full model achieves the best performance across all metrics. Hyper-parameter Experiment. Table 3 reports the impact of key hyperparameters. For the perturbation strength ϵ, performance improves as ϵ increases to 0.7, achieving the best Dice score of 0.7852. Regarding the loss weights, setting λa = 5 and λu = 1 leads to the best overall results. Effect of Foundation Models. We choose DINOv2 as our backbone because it has demonstrated remarkable generalization across diverse downstream tasks [55, 56]; despite being trained on natural images, it transfers strongly to medical domains [5]. To further validate this design choice, we conducted an ablation replacing DINOv2 with alternative foundation models, including those pretrained on medical data. CLIP [57] and BiomedCLIP [58] are visionlanguage models trained on large-scale imagetext pairs (natural and medical, respectively), MAE [59] is trained via masked autoencoding, and Rad-DINO [60] adopts DINO-style architecture on radiology images. As summarized in Table 4, DINOv2 consistently outperforms all alternatives under both 5% and 10% labeled regimes, including models specialized for medical domains, empirically supporting DINOv2 as robust and effective semantic prior for semi-supervised medical image segmentation. Table 4: Ablation on foundation models. Foundation Model Dice (5%) Dice (10%) Table 5: Ablation on the codebook size. Codebook Size Dice (5%) Dice (10%) Uti. (%) CLIP [57] BiomedCLIP [58] MAE [59] Rad-DINO [60] DINOv2 [41] 0.6421 0.6507 0.6386 0.6535 0.6643 0.7483 0.7629 0.7541 0.7793 0.7852 1024 2048 4096 16384 32768 0.6531 0.6582 0.6627 0.6643 0.6595 0.6415 0.7608 0.7748 0.7775 0.7852 0.7764 0.7638 100 100 99 98 95 92 Table 6: Comparison of performance under different labeled data ratios. 10% 50% 100% Method 20% 5% UNet-S [47] MCNet [48] ABD [52] Unimatch [53] VQ-Seg (Ours) 0.4343 0.6378 0.6414 0.6493 0.6643 0.6490 0.7555 0.7468 0.7511 0.7852 0.7205 0.7812 0.7780 0.7855 0. 0.7880 0.8203 0.8235 0.8279 0.8507 0.8345 0.8751 0.8824 0.8871 0.9102 Effect of Codebook Size. We further investigate the impact of codebook size on model performance, as shown in Table 5. The codebook size determines the granularity of the discrete latent space: smaller codebook provides limited representational capacity, while an excessively large one may lead to code redundancy and unstable optimization. Our results show that the Dice score steadily increases as the codebook size grows from 1,024 to 16,384, indicating that moderately large codebook enables richer and more discriminative representations. However, further enlargement (e.g., 32,768 or 65,536) slightly degrades performance due to decreased code utilization and overfitting. Here, Uti. denotes codebook utilization, which measures the proportion of code vectors actively used during training. Ablation on Labeled Ratio Settings. As shown in Table 6, we evaluate VQ-Seg under different labeled ratios to examine its scalability in semi-supervised learning. The results show consistent performance gain with more labeled data, demonstrating the models strong ability to exploit supervision. Remarkably, VQ-Seg achieves significant improvements in low-label regimes, indicating that the discrete representation learned via vector quantization provides effective regularization and semantic consistency. Even with increasing supervision, the performance gain remains stable, highlighting VQ-Segs robustness and scalability across varying annotation levels."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We present VQ-Seg, novel semi-supervised medical image segmentation framework. VQ-Seg introduces Quantized Perturbation Module (QPM) that performs controlled perturbations in the vector-quantized (VQ) feature space, enhancing the robustness of representation learning. In addition, dual-branch architecture with Post-VQ Feature Adapter (PFA) is designed to refine the quantized features and integrate high-level semantic information. Extensive experiments on the Lung Cancer (LC) and ACDC datasets demonstrate that VQ-Seg achieves state-of-the-art performance, substantially improving segmentation accuracy under limited supervision. However, the current perturbation operates solely in the discrete VQ space, making it difficult to extend to continuous feature representations commonly used in existing semi-supervised frameworks. Moreover, while the adoption of foundation model introduces richer semantic priors, it also brings additional computational overhead. Future work will focus on developing controllable perturbation mechanisms in continuous spaces and exploring more efficient foundation model integration."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work is supported by the Guangdong Science and Technology Department (2024ZDZX2004) and the Guangzhou Industrial Information and Intelligent Key Laboratory Project (No. 2024A03J0628)."
        },
        {
            "title": "References",
            "content": "[1] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas De Bel, Moira SN Berens, Cas Van Den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis, 42:113, 2017. [2] Mandong Hu, Yi Zhong, Shuxuan Xie, Haibin Lv, and Zhihan Lv. Fuzzy system based medical image processing for brain disease prediction. Frontiers in Neuroscience, 15:714318, 2021. [3] Dinesh Patil and Sonal Deore. Medical image segmentation: review. International Journal of Computer Science and Mobile Computing, 2(1):2227, 2013. [4] Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, and Lei Zhu. Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 578588. Springer, 2024. [5] Sicheng Yang, Hongqiu Wang, Zhaohu Xing, Sixiang Chen, and Lei Zhu. Segdino: An efficient design for medical and natural image segmentation with dino-v3. arXiv preprint arXiv:2509.00833, 2025. [6] Huiyan Jiang, Zhaoshuo Diao, Tianyu Shi, Yang Zhou, Feiyu Wang, Wenrui Hu, Xiaolin Zhu, Shijie Luo, Guoyu Tong, and Yu-Dong Yao. review of deep learning-based multiple-lesion recognition from medical images: classification, detection and segmentation. Computers in Biology and Medicine, 157:106726, 2023. [7] Hongqiu Wang, Xiangde Luo, Wu Chen, Qingqing Tang, Mei Xin, Qiong Wang, and Lei Zhu. Advancing uwf-slo vessel segmentation with source-free active domain adaptation and novel multi-center dataset. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 7585. Springer, 2024. [8] Hongqiu Wang, Yixian Chen, Wu Chen, Huihui Xu, Haoyu Zhao, Bin Sheng, Huazhu Fu, Guang Yang, and Lei Zhu. Serp-mamba: Advancing high-resolution retinal vessel segmentation with selective state-space model. IEEE Transactions on Medical Imaging, 2025. [9] Dzung Pham, Chenyang Xu, and Jerry Prince. Current methods in medical image segmentation. Annual review of biomedical engineering, 2(1):315337, 2000. [10] Hongqiu Wang, Guang Yang, Shichen Zhang, Jing Qin, Yike Guo, Bo Xu, Yueming Jin, and Lei Zhu. Video-instrument synergistic network for referring video instrument segmentation in robotic surgery. IEEE Transactions on Medical Imaging, 2024. [11] Yijun Yang, Zhaohu Xing, Lequan Yu, Chunwang Huang, Huazhu Fu, and Lei Zhu. Vivim: video vision mamba for medical video segmentation. arXiv preprint arXiv:2401.14168, 2024. [12] Muhammad Imran Razzak, Saeeda Naz, and Ahmad Zaib. Deep learning for medical image processing: Overview, challenges and the future. Classification in BioApps: Automation of decision making, pages 323350, 2017. [13] Mohammad Hesam Hesamian, Wenjing Jia, Xiangjian He, and Paul Kennedy. Deep learning techniques for medical image segmentation: achievements and challenges. Journal of digital imaging, 32:582596, 2019. [14] Risheng Wang, Tao Lei, Ruixia Cui, Bingtao Zhang, Hongying Meng, and Asoke Nandi. Medical image segmentation using deep learning: survey. IET image processing, 16(5): 12431267, 2022. [15] Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, and Pheng-Ann Heng. Uncertaintyaware self-ensembling model for semi-supervised 3d left atrium segmentation. In Medical image computing and computer assisted interventionMICCAI 2019: 22nd international conference, Shenzhen, China, October 1317, 2019, proceedings, part II 22, pages 605613. Springer, 2019. [16] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. [17] Rushi Jiao, Yichi Zhang, Le Ding, Bingsen Xue, Jicong Zhang, Rong Cai, and Cheng Jin. Learning with limited annotations: survey on deep semi-supervised learning for medical image segmentation. Computers in Biology and Medicine, 169:107840, 2024. [18] Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Mae, Adamo Young, Cheng Zhu, Xin Yang, Kangkang Meng, Ziyan Huang, et al. Unleashing the strengths of unlabelled data in deep learning-assisted pan-cancer abdominal organ quantification: the flare22 challenge. The Lancet Digital Health, 6(11):e815e826, 2024. [19] Veronika Cheplygina, Marleen De Bruijne, and Josien PW Pluim. Not-so-supervised: survey of semi-supervised, multi-instance, and transfer learning in medical image analysis. Medical image analysis, 54:280296, 2019. [20] Isaac Triguero, Salvador García, and Francisco Herrera. Self-labeled techniques for semisupervised learning: taxonomy, software and empirical study. Knowledge and Information systems, 42:245284, 2015. [21] Bethany Thompson, Gaetano Di Caterina, and Jeremy Voisey. Pseudo-label refinement using superpixels for semi-supervised brain tumour segmentation. In 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2022. [22] Liang Qiu, Jierong Cheng, Huxin Gao, Wei Xiong, and Hongliang Ren. Federated semisupervised learning for medical image segmentation via pseudo-label denoising. IEEE journal of biomedical and health informatics, 27(10):46724683, 2023. [23] Moucheng Xu, Yukun Zhou, Chen Jin, Marius de Groot, Daniel Alexander, Neil Oxtoby, Yipeng Hu, and Joseph Jacob. Expectation maximisation pseudo labels. Medical Image Analysis, 94:103125, 2024. [24] Wenji Wang, Qing Xia, Zhiqiang Hu, Zhennan Yan, Zhuowei Li, Yang Wu, Ning Huang, Yue Gao, Dimitris Metaxas, and Shaoting Zhang. Few-shot learning by cascaded framework with shape-constrained pseudo label assessment for whole heart segmentation. IEEE Transactions on Medical Imaging, 40(10):26292641, 2021. [25] Kai Han, Victor Sheng, Yuqing Song, Yi Liu, Chengjian Qiu, Siqi Ma, and Zhe Liu. Deep semi-supervised learning for medical image segmentation: review. Expert Systems with Applications, 245:123052, 2024. [26] Cheng Chen, Kangneng Zhou, Zhiliang Wang, and Ruoxiu Xiao. Generative consistency for semi-supervised cerebrovascular segmentation from tof-mra. IEEE Transactions on Medical Imaging, 42(2):346353, 2022. [27] Ling-Li Zeng, Kai Gao, Dewen Hu, Zhichao Feng, Chenping Hou, Pengfei Rong, and Wei Wang. Ss-tbn: semi-supervised tri-branch network for covid-19 screening and lesion segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(8):1042710442, 2023. [28] Liyun Lu, Mengxiao Yin, Liyao Fu, and Feng Yang. Uncertainty-aware pseudo-label and consistency for semi-supervised medical image segmentation. Biomedical Signal Processing and Control, 79:104203, 2023. [29] Jianfeng Wang and Thomas Lukasiewicz. Rethinking bayesian deep learning methods for semi-supervised volumetric medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 182190, 2022. [30] Vikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala, Arno Solin, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. Neural Networks, 145:90106, 2022. 12 [31] Xinyue Huo, Lingxi Xie, Jianzhong He, Zijie Yang, Wengang Zhou, Houqiang Li, and Qi Tian. Atso: Asynchronous teacher-student optimization for semi-supervised image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12351244, 2021. [32] Yixin Wang, Yao Zhang, Jiang Tian, Cheng Zhong, Zhongchao Shi, Yang Zhang, and Zhiqiang He. Double-uncertainty weighted method for semi-supervised learning. In Medical Image Computing and Computer Assisted InterventionMICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part 23, pages 542551. Springer, 2020. [33] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distributionally robust optimization. Open Journal of Mathematical Optimization, 3:185, 2022. [34] Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. Mathematics of Operations Research, 48(2):603655, 2023. [35] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems, 30, 2017. [36] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [37] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [38] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [39] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [40] Taesung Park, Alexei Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IX 16, pages 319345. Springer, 2020. [41] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [42] Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging, 37(11):25142525, 2018. [43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 60236032, 2019. [44] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [45] Yongxin Zhu, Bocheng Li, Yifei Xin, Zhihua Xia, and Linli Xu. Addressing representation collapse in vector quantized models with one linear layer. arXiv preprint arXiv:2411.02038, 2024. [46] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. Advances in Neural Information Processing Systems, 37: 1261212635, 2024. 13 [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks In Medical image computing and computer-assisted for biomedical image segmentation. interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [48] Yicheng Wu, Zongyuan Ge, Donghao Zhang, Minfeng Xu, Lei Zhang, Yong Xia, and Jianfei Cai. Mutual consistency learning for semi-supervised medical image segmentation. Medical Image Analysis, 81:102530, 2022. [49] Yicheng Wu, Zhonghua Wu, Qianyi Wu, Zongyuan Ge, and Jianfei Cai. Exploring smoothness and class-separation for semi-supervised medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 3443. Springer, 2022. [50] Yunhao Bai, Duowen Chen, Qingli Li, Wei Shen, and Yan Wang. Bidirectional copy-paste for semi-supervised medical image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1151411524, 2023. [51] Chenyu You, Weicheng Dai, Yifei Min, Fenglin Liu, David Clifton, Kevin Zhou, Lawrence Staib, and James Duncan. Rethinking semi-supervised medical image segmentation: variancereduction perspective. Advances in neural information processing systems, 36:998410021, 2023. [52] Hanyang Chi, Jian Pang, Bingfeng Zhang, and Weifeng Liu. Adaptive bidirectional displacement for semi-supervised medical image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40704080, 2024. [53] Lihe Yang, Zhen Zhao, and Hengshuang Zhao. Unimatch v2: Pushing the limit of semiIEEE Transactions on Pattern Analysis and Machine supervised semantic segmentation. Intelligence, 2025. [54] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. [55] Mohammed Baharoon, Waseem Qureshi, Jiahong Ouyang, Yanwu Xu, Abdulrhman Aljouie, and Wei Peng. Evaluating general purpose vision foundation models for medical image analysis: An experimental study of dinov2 on radiology benchmarks. arXiv preprint arXiv:2312.02366, 2023. [56] Xinrui Song, Xuanang Xu, and Pingkun Yan. Dino-reg: General purpose image encoder for training-free multi-modal deformable medical image registration. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 608617. Springer, 2024. [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [58] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. [59] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [60] Fernando Perez-Garcia, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Matthew Lungren, et al. Exploring scalable medical image encoders beyond text supervision. Nature Machine Intelligence, 7(1):119130, 2025."
        },
        {
            "title": "A KL Divergence Approximation under Dropout Perturbation",
            "content": "To support the theoretical analysis in Section 3.2, we derive an approximation of the KL divergence between prior distribution (h) and perturbed distribution Q(h) induced by applying dropout to intermediate feature. This divergence is used to quantify the perturbation radius caused by dropout in the feature space. A.1 Dropout-Induced Feature Distribution as Mixture Consider simplified setting where feature activation follows Gaussian prior: (h) = (0, σ2 0). (13) Under dropout with rate p, the feature is zeroed out with probability p, or retained with probability 1 p. This results in the perturbed distribution: Q(h) = (1 p) (0, σ2 0) + δ(h), (14) where δ(h) is the Dirac delta function centered at zero. This mixture captures the effect of randomly dropping activations. A. Intractability of Exact KL Divergence The KL divergence between (h) and Q(h) is: (cid:90) DKL(P Q) = (h) log (cid:19) (cid:18) (h) Q(h) dh. Substituting the forms of (h) and Q(h), we obtain: (cid:18) (cid:90) DKL(P Q) = (h; 0, σ 0) log (h; 0, σ2 0) (1 p)N (h; 0, σ2 0) + pδ(h) (15) (16) (cid:19) dh. Due to the singular nature of δ(h) at = 0, this expression is intractable in closed form. To make progress, we adopt moment-matching approximation. A.3 Moment-Matching Approximation We approximate Q(h) with Gaussian distribution Qapprox(h) that matches the first and second moments of the dropout-perturbed activations. Let Q(h) denote the post-dropout feature. Then: E[h] = 0, Var(h) = σ2 0(1 p). Therefore, we define: Qapprox(h) = (0, σ2 The KL divergence between the original and approximated feature distributions becomes: 0(1 p)). DKL(P Qapprox) = (cid:18) 1 σ2 0 σ2 0(1 p) 1 + log (cid:18) σ2 0(1 p) σ2 0 (cid:19)(cid:19) . Simplifying gives: (17) (18) (19) DKL(P Qapprox) = (cid:18) 1 1 2 1 + log(1 p) = (cid:19) (cid:18) 1 1 2 (cid:19) + log(1 p) . (20) A.4 Interpretation and Connection to Perturbation Radius This result provides clean, analytical expression for the perturbation radius induced by dropout, interpreted as KL divergence. Notably, as 1, the divergence grows rapidly and even diverges (.e., becomes unbounded), indicating severe deviation from the original distribution. This supports our theoretical claim: Dropout induces increasingly unstable perturbations when the dropout rate is high, as observed empirically (see Fig. 1), potentially leading to over-regularization, distorted representations, and degraded model performance. This motivates our Quantized Perturbation Module (QPM), which constrains perturbations to structured, discrete space and avoids such instability."
        },
        {
            "title": "B Proof of Numerical Stability of QPM",
            "content": "In this appendix, we prove that the perturbed distribution Q(cj ϵ) used in the Quantized Perturbation Module (QPM) is always well-defined and bounded, thereby ensuring the numerical stability of the associated KL divergence, even in the extreme case of ϵ = 1. B.1 Definition of Q(cj ϵ) Let the prior distribution over codewords be uniform: (ci) ="
        },
        {
            "title": "1\nK",
            "content": ", {1, . . . , K}. (21) The perturbation mechanism π(j i) is defined as: (cid:40) π(j i) = 1 ϵ, ϵ exp(d(ci,cj )) Zi , if = if = , where Zi = (cid:88) k=i exp(d(ci, ck)). (22) Then, the overall perturbed distribution is: Q(cj ϵ) = (cid:88) i= (ci)π(j i) = 1 (cid:88) i=1 π(j i). (23) B.2 Basic Properties of Q(cj ϵ) (1) Non-negativity and Positivity: Since π(j i) 0 for all i, j, it follows that Q(cj ϵ) 0. Furthermore, under mild assumptions (e.g., finite distances and ϵ > 0), we have π(j i) > 0 for some i, so Q(cj ϵ) > 0 for all j. (2) Normalization: We verify that is valid probability distribution: (cid:88) j=1 Q(cj ϵ) = (cid:88) (cid:88) j=1 i=1 1 π(j i) = 1 (cid:88) (cid:88) i=1 j=1 π(j i) = 1 (cid:88) i=1 1 = 1. (24) B.3 KL Divergence Between and The KL divergence between and is defined as: DKL(P Q) = (cid:88) j=1 (cj) log (cid:19) (cid:18) (cj) Q(cj ϵ) = 1 (cid:88) j=1 log (cid:18) 1/K (cid:19) Q(cj ϵ) = 1 (cid:88) j=1 log (KQ(cj ϵ)) . (25) (26) This expression is numerically stable as long as Q(cj ϵ) > 0 and bounded away from 0, which we prove next for the extreme case ϵ = 1. B.4 QPM Perturbation Distribution at ϵ = Recall that when ϵ = 1, the transition distribution becomes: π(j i) = (cid:40) 0, exp(d(ci,cj )) Zi if = if = , , where Zi = (cid:88) k=i exp(d(ci, ck)). (27) Thus, the resulting perturbed distribution is given by: Q(cj ϵ = 1) = (cid:88) i=1 (ci)π(j i) = 1 (cid:88) i=j exp(d(ci, cj)) k=i exp(d(ci, ck)) (cid:80) . (28) B.5 Lower Bound of Q(cj ϵ = 1) Assume the codebook = {c1, ..., cK} is fixed and that the pairwise distance is bounded: 0 < Dmin d(ci, cj) Dmax < for all = j. Then for any = j, the transition term is lower bounded: exp(d(ci, cj)) k=i exp(d(ci, ck)) (cid:80) exp(Dmax) (K 1) exp(Dmin) = exp(Dmin Dmax) 1 . Hence, Q(cj ϵ = 1) ="
        },
        {
            "title": "1\nK",
            "content": "(cid:88) i=j exp(d(ci, cj)) k=i exp(d(ci, ck)) (cid:80) (K 1) exp(Dmin Dmax) = exp(Dmin Dmax) > 0. (31) (29) (30) B.6 Upper Bound of Q(cj ϵ = 1) Likewise, for any = j: exp(d(ci, cj)) k=i exp(d(ci, ck)) (cid:80) exp(Dmin) (K 1) exp(Dmax) = exp(Dmax Dmin) 1 . Thus, Q(cj ϵ = 1) 1 (K 1) exp(Dmax Dmin) 1 = exp(Dmax Dmin) . (32) (33) B.7 Implication for KL Divergence Since Q(cj ϵ = 1) is strictly positive and bounded above by 1, the term log(KQ(cj)) in the KL divergence remains finite: DKL(P Q) = 1 (cid:88) j=1 log(KQ(cj)) < . (34) Therefore, the QPM perturbation strategy ensures numerical stability for all valid ϵ [0, 1]."
        },
        {
            "title": "C Performance Analysis on the ACDC Dataset",
            "content": "Table 7: Quantitative comparison on the ACDC dataset with two labeled ratio settings (5%, 10%) using Dice and Jaccard (). Best results are in bold. Method 5% Labeled 10% Labeled Dice Jaccard Dice Jaccard UNet-F [47] UNet-S [47] nnUNet-F [54] nnUNet-S [54] UA-MT [15] MCNet [48] SSNet [49] BCP [50] ARCO [51] ABD [52] Unimatch [53] Ours 0.9130 0.4674 0.9185 0.4892 0.6123 0.6485 0.6542 0.8621 0.8879 0.8874 0.8915 0. 0.8427 0.3698 0.8491 0.3876 0.5324 0.5338 0.5568 0.7846 0.8021 0.7924 0.7983 0.8173 0.9130 0.7952 0.9185 0.8113 0.8423 0.8621 0.8689 0.8827 0.9026 0.8992 0.8978 0.9103 0.8427 0.6882 0.8491 0.7040 0.7382 0.7701 0.7753 0.8032 0.8252 0.8213 0.8290 0. Table 7 compares our method with leading baselines on the ACDC dataset under 5% and 10% labeled data settings. Our model consistently outperforms others in both Dice and Jaccard metrics. 17 With 5% labeled data, our method achieves Dice of 0.9057, exceeding Unimatch (0.8915), ABD (0.8874), and ARCO (0.8879), and approaching the fully supervised nnUNet-F (0.9185). This highlights its strong representation ability under limited supervision. At 10% labeled data, the advantage becomes more evident, reaching the best Dice (0.9103) and Jaccard (0.8327), surpassing ABD (0.8992) and ARCO (0.9026). These consistent gains demonstrate the robustness and scalability of our approach as more labeled data are available."
        },
        {
            "title": "D Statistical Analysis",
            "content": "Figure 6: Comparison under 5% labeled LC dataset. To evaluate the statistical reliability of our method, we ran the competing models MCNet [48], Unimatch [53], and our VQ-Seg ten times under the 5% labeled LC dataset using different random seeds. The averaged Dice scores together with their standard deviations are visualized in Fig. 6, where the error bars denote the standard deviation across repeated runs, reflecting the robustness and stability of each method. The noticeably smaller error bar of VQ-Seg indicates lower performance variance, demonstrating its stronger training stability across random seeds. To confirm the statistical significance of the observed improvements, we performed paired two-tailed t-tests between VQ-Seg and each baseline method over the ten independent trials. The results show that the differences are statistically significant with < 0.05."
        },
        {
            "title": "E Codebook Evolution",
            "content": "Figure 7: T-SNE visualization of codebook evolution. We visualize in Fig. 7 the t-SNE projections of all codebook vectors across three training stages. Each point represents codeword, where orange and blue indicate activated and inactive entries, respectively. Initially (left), only few codewords (3.4%) are activated, showing compact cluster and limited diversity. As training proceeds (middle), activation increases to 67%, and the distribution becomes more uniform. By convergence (right), nearly all codewords (98%) are active and evenly dispersed, forming stable and well-structured embedding space. These results demonstrate that our quantization strategy progressively enhances codebook utilization and representation diversity."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}