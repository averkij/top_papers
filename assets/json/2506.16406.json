{
    "paper_title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
    "authors": [
        "Zhiyuan Liang",
        "Dongwen Tang",
        "Yuhao Zhou",
        "Xuanlei Zhao",
        "Mingjia Shi",
        "Wangbo Zhao",
        "Zekai Li",
        "Peihao Wang",
        "Konstantin Schürholt",
        "Damian Borth",
        "Michael M. Bronstein",
        "Yang You",
        "Zhangyang Wang",
        "Kai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \\textbf{Drag-and-Drop LLMs (\\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \\textbf{12,000$\\times$} lower overhead than full fine-tuning, ii) average gains up to \\textbf{30\\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 6 0 4 6 1 . 6 0 5 2 : r Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights Zhiyuan Liang1 Dongwen Tang1 Yuhao Zhou1 Xuanlei Zhao1 Mingjia Shi1 Wangbo Zhao1 Zekai Li1 Peihao Wang2 Konstantin Schürholt3 Damian Borth3 Michael M. Bronstein4 Yang You1 Zhangyang Wang2 Kai Wang1 1National University of Singapore, 2UT Austin, 3University of St. Gallen, 4Oxford University"
        },
        {
            "title": "Abstract",
            "content": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require separate optimization run for every downstream dataset. We introduce Drag-and-Drop LLMs (DnD), prompt-conditioned parameter generator that eliminates per-task training by mapping handful of unlabeled task prompts directly to LoRA weight updates. lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by cascaded hyperconvolutional decoder into the full set of LoRA matrices. Once trained in diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to 12,000 lower overhead than full fine-tuning, ii) average gains up to 30% in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at https://jerryliang24.github.io/DnD."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) such as GPT-4, Llama 2/3, Qwen2.5, and DeepSeek have rapidly become the backbone of contemporary natural-language processing and artificial intelligence more broadly, thanks to their Internet-scale pre-training and transformer architectures [1, 18, 62, 35]. This pre-training endows single model with broad zero-shot competence across mathematics, coding, reasoning, and even multimodal understanding [23, 11, 61, 28]. Yet, real-world deployments rarely stop at zero-shot use; they demand task-specific behavior that reflects internal data, domain jargon, or bespoke response styles. Parameter-Efficient Fine-Tuning (PEFT) aims to satisfy this demand by inserting small set of trainable weights, most prominently the low-rank adapters of LoRA [24]. While LoRA allows to maintain the number of trainable parameters and storage overhead small by keeping the model frozen, the wall-clock cost remains very high: for example, adapting the lightest 0.5 B-parameter Qwen2.5 with LoRA still occupies four A100 GPUs for half day [62]. Moreover, each downstream user or dataset requires its own optimization run, which quickly becomes the computational bottleneck for practitioners when deploying PEFT at massive scales. We observe that LoRA adapter is nothing more than function of its training data: gradient descent drags the base weights towards task-specific optimum (Figure 1). If that mapping from prompts to weights can be learned directly, we could bypass gradient descent altogether. Early work on parameter generation has shown that hyper-networks can synthesize billions of weights in minutes [50, 42, 57, 48, 52]. Yet they either ignore task conditioning or use simple binary embeddings. *Zhiyuan, Zhangyang, and Kai are core contributors. Preprint. Under review. (a) Parameter-Efficient-Fine-Tuning (b) Drag-and-Drop Figure 1: Left: Parameter-efficient methods such as LoRA need hours to optimize LLMs in order to adapt them to novel datasets. Right: Our method adapts LLMs by directly generating LoRA matrices for novel datasets in seconds without any tuning. Recent progress makes this goal attainable. RPG [58] is one of the first approaches to condition on task information and generate an entire classifier in single pass, matching from-scratch training on previously unseen image classes in zero-shot. Translating that success to language, however, raises new obstacles. First, linguistic prompts carry orders of magnitude more semantic variation than the binary embeddings used by RPG. practical generator must therefore ingest rich task descriptors and preserve their nuances. Second, an LLM in production may face hundreds of heterogeneous workloads, so the conditioning mechanism must scale gracefully while injecting task-specific cues with high fidelity. These challenges bring the need for compact yet expressive representation that both captures salient features of the input texts and steers the hyper-network towards the corresponding region of LoRA weight space. Designing such representation is the central challenge that our method, introduced next, is built to address. We introduce Drag-and-Drop LLMs (DnD), prompt-conditioned hyper-generator that converts handful of unlabeled task prompts into complete set of LoRA adapters in seconds, eliminating any per-task optimization. DnD employs an off-the-shelf, lightweight text encoder to compress given batch of prompts into conditional embeddings, which cascaded hyper-convolutional decoder then expands into LoRA updates for every transformer layer. On common-sense reasoning, mathematics, code-generation, and multimodal benchmarks, DnD cuts adaptation overhead by up to 12,000 while yielding up to 30% increased performance on unseen datasets compared with the strongest training LoRAs, and transfers seamlessly from 0.5B to 7B parameter backbones. By collapsing the classical data gradients weights loop into single forward step, DnD challenges the notion that gradient descent is indispensable for model specialization and opens new path where weights themselves become new data modality and generative target conditioned on concise task descriptors. Our main contributions are outlined as follows: New LLM adaptation paradigm. We cast LoRA adaptation as direct generation of task-specific weights from raw prompts in novel dataset and realize this mapping with scalable hyper-generator, which is way more efficient than traditional tuning. Practical Architecture. frozen text encoder coupled with hyper-convolutional decoder is able to generate large scale parameters while reducing adaptation overhead by four orders of magnitude. Comprehensive evaluation. Experiments across reasoning, math, coding, and multimodal show up to 30% zero-shot gains on unseen datasets and smooth transfer across model sizes, highlighting DnDs impressive efficiency and versatility."
        },
        {
            "title": "2 Drag-and-Drop Your LLMs",
            "content": "2.1 Preliminary Parameter-Efficient Fine-Tuning. Parameter-Efficient Fine-Tuning (PEFT) saves training costs via introducing and tuning only small number of additional parameters while keeping the original model weights frozen. This approach has been applied to LLMs and other foundation models, particularly in Low-Rank Adaptation (LoRA) manner, such as LLaMA [18] and Stable Diffusion [47]. The optimization process can be formulated as: 2 Figure 2: Our approach obtains dragging ability via two processes: prepare training data (upper left) and training the parameter generator (upper right). When preparing training data, we explicitly pair parameters with dataset-specific conditions. During training, DnD takes condition as input and generate parameters, using original parameters as supervision. L(W0 + BA, D), (1) min A,B where W0 is the frozen original weight matrix, low-rank matrices Rdr and Rrk with min(d, k) are the only trainable parameters and represents the fine-tuning dataset. Based on Equation 1, we can conclude that LoRA uses data as raw material and optimization as the driving force to obtain weight space shift = BA, thus making and strongly associated. Parameter generation. This approach treats models or trained weights as data, aiming to synthesize high-performing neural network parameters without conventional training. Recent advances, such as COND P-DIFF [26], RPG [58], SANE [52], and ORAL [27] have achieved controllable generation by incorporating conditioning mechanisms, allowing primitive personalized generation of parameters for simple datasets. The parameter generation process shares fundamental commonalities with PEFT, where conditions serve as raw materials and the parameter generator provides the driving force to produce target weights with specific properties. One questions remains: Can we utilize parameter generation to effectively drag-and-drop LLMs weights towards configurations better suited for given novel task? By drag-and-drop, we draw an analogy to our simple, tuning-free process that directly generates task-specific weights, associating it to the intuitive action of dragging file and dropping it into place without further configuration. Key challenges. Before addressing the above question, we analyze the potential challenges. Challenge 1: How to equip the parameter generator with effective drag-and-drop ability? The generator should produce parameters that can effectively adapt LLMs towards specific tasks. Challenge 2: How to enable adaptation without task-specific training? Traditional PEFT methods typically require training on new tasks, but can we achieve comparable performance by directly generating parameters without any fine-tuning on the target task? Challenge 3: How to make the drag-and-drop function user-friendly and accessible? The generation mechanism should be simple and intuitive, enabling broader adoption and practical deployment. 2.2 Overview of DnD To address the identified challenges, we present DnD as illustrated in Figure 2. As preparation, we first train and save LoRA adapters on various datasets. To develop the drag-and-drop capability, our approach should be aware of parameters correlations with datasets. Consequently, we randomly pair collected checkpoints with prompt batches from their training data. pre-trained text encoder then extracts embeddings from the prompts and feed them to our parameter generator. The generator features simple architecture of cascaded pure convolutional decoder blocks (details in Section 2.5 and Appendix A.3). We optimize this generator using mean squared error (MSE) loss between the generated and original tokenized model weights. During inference, we evaluate our approach in both 3 in-domain and cross-domain scenarios: simply feeding prompts from novel datasets (not seen during training) to DnD to obtain tailored model parameters with one single forward pass. 2.3 Data Preparation of DnD. Checkpoint collection. We collect the checkpoints across various datasets, i.e., serving as diverse supervisions, to equip the capability of DnD. The collection process follows previous parameter generation works [57, 58]: training for specified epochs, then performing iterative fine-tuning while preserving checkpoints at each iteration (more details in Appendix A.1). The role of prompts. Recent studies [37, 65] demonstrate that samples from different datasets exhibit distinct features, i.e., samples could be considered as fingerprints of specified datasets (tasks). Based on this observation, we utilize data samples (prompts) as representative proxies for their respective datasets (tasks). To establish data-parameter mapping, we incorporate prompts from the datasets used to train these checkpoints. These prompts contain dataset-specific features, enabling the generator to infer appropriate dragging directions for models across various tasks. Prompt-checkpoint pairing. Based on the above analysis, the next important question is: how to utilize these elements to equip DnD with drag-and-drop ability in training? Given dataset , we first divide it into non-overlapping prompt batches [p1, , pi, , pI ]. We note the trained LLM checkpoints of this dataset as = [m1, , mj, , mJ ]. We randomly pick batch of prompts and corresponding checkpoint. The process can be formulated as, [p1, , pi, , pI ] randomly pick {pi, mj} randomly pick [m1, , mj, , mJ ], (2) where {pi, mj} serves as pair for parameter generator training. Prompt pi and checkpoint mj serve as the input and supervision, respectively. 2.4 Prompt Embedding. For each batch of prompts, we employ open-sourced text encoder to extract embeddings that serve as inputs for the parameter generator. This extraction process can be formally represented as: ci = Encoder(pi, θ), (3) where Encoder(, ) denotes the embedding extraction function parameterized by θ, and ci represents the extracted embedding corresponding to prompt pi. By default, we leverage an encoder-based language model architecture [13] for prompt embedding. In the experimental section, we further explore and quantitatively evaluate alternative embedding approaches, including word2vec representations [43], encoder-decoder architecture [45], and decoder-only language models [62]. 2.5 Training and Inference of DnD. Figure 3: Block details of parameter generator. Each block of hyper-convolution contains three hyper-convolution modules, extracting and fusing features in different dimensions. More details are in Appendix A.3. Structure of parameter generator. Different from diffusion-based parameter generation [57, 26, 58], we use hyper-convolutional decoder to learn the mapping between input prompts and parameters. That design mainly considers efficiency, as the decoder-only structure has shown its superiority in LLMs [1, 18, 62, 21]. We show the block details of the decoder in the right part (Figure 3). We assume the dimension of input prompt embeddings is [B, N, L, C], where B, , L, and denote batch size, length of prompt batch (i.e., number of prompts), sequence length, and hidden dimension, respectively. The cascaded convolutional blocks transform prompt embeddings to match the dimensions of tokenized weights. Here, we refer the output of the last block as [B, Nw, Lw, Cw]. Learning objective. The learning objective is simple: we calculate the mean squared error (MSE) loss between the output of the last block of the parameter generator and the corresponding tokenized checkpoints. Similar to RPG [58], we tokenize each layers parameters into non-overlapping segments and apply padding, ensuring checkpoints have consistent shape of [B, Nw, Lw, Cw]. Formally, we write the MSE loss below, 4 prompt embeddings parameter generator LM SE tokenization [58] corresponding checkpoints. (4) Inference. We expect the parameter generator to develop effective drag-and-drop ability, particularly for novel datasets or tasks not encountered during training. Therefore, our evaluation primarily focuses on performance across novel datasets. The inference process consists of four steps: 1) sampling prompts from novel datasets, 2) extracting embeddings from these prompts, 3) feeding the embeddings into the parameter generator, and 4) evaluating the generated parameters on the novel datasets. To comprehensively demonstrate the drag-and-drop ability of our approach, we examine performance in both in-domain scenarios (e.g., common sense-to-common sense) and cross-domain scenarios (e.g., common sense-to-science)."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Implementation Details We choose Qwen2.5 [62] series as foundation model and conduct experiments on common sense reasoning, coding, math, and multimodal tasks. The details of involved model sizes and datasets for each task are listed in the table below. The default text encoder is Sentence-BERT [46], and length of prompt batch is set to 128, 64, 64 and 32 for common sense reasoning, math, coding, and multimodal task, respectively. For other hyper-parameter settings, please refer to Appendix A.1. task common sense #model size (B) 0.5 coding 1.5, 7 math multimodal 1.5 3 3.2 Common Sense Reasoning datasets ARC-e [10], ARC-c [10], BoolQ [9], OBQA [40], HelaSwag [64], PIQA [5], WinoGrande [49] Evol-Instruct-68K-V1 [39], Glaive-Assistant-V2 [17], Python-Codes-25K [15], Code-74k-ShareGPT [2], Rosetta-Code [12], LLaMA-Python-Codes-30K [14], CodeAlpaca-20K [7] Competition-Math[23], Math-QA[3], Math-IIO-68K-Mini [44] Math-Plus [55], Mu-Math [63], ToT-Math-V1 [41] MathV360K [54] Evaluating setting. We employ LoRA [24] to fine-tune Qwen2.5-0.5B on seven common sense reasoning datasets and save the checkpoints as training data. In every column of Table 1, we use the specified dataset as test set (i.e., not used in training) and train DnD on other datasets LoRAs. method test set training LoRAs DnD ARC-e 37.5 68.6 OBQA 30.2 40.8 ARC-c 39.5 51. PIQA 40.5 87.9 HellaSwag 22.4 25.9 BoolQ WinoGrande 13.5 44.9 38.8 50.0 average accuracy improvement: 21.0 on training LoRAs Table 1: Generalization on novel (test) datasets. Our approach significantly outperforms LoRAs used in training in terms of accuracy (%) across all unseen datasets. Bold entries are the best results. Analysis. We report the average accuracy of training LoRAs and our generated ones in Table 1. Several observations can be made from these results: i) Our method consistently outperforms LoRAs used for training on unseen datasets, indicating it manages to drag-and-drop LLM parameters to task-specific distribution specified via condition. ii) This drag-and-drop ability holds across different datasets, showing strong robustness towards various data inspirations. Cross-domain Drag-and-Drop. To further explore DnDs zero-shot ability, we not only use in-domain novel datasets in inference, but also test it on cross-domain task. In this part, we use the checkpoint trained on common sense reasoning tasks and feed it with prompts from science-dataset [33]. The generated parameters are compared with training LoRAs on the science dataset. From Table 2, we can observe that DnD surpasses its training LoRAs average accuracy, Table 2: Our approach also succeeds on crossdomain scenario (i.e., both novel data and task). training LoRAs DnD improvement () 45.3 testset science 35.6 8. 5 indicating our method manages to drag-and-drop LLMs for cross-domain tasks (i.e., from common sense reasoning to science). method task training LoRAs DnD improvement () Coding (HumanEval) pass@5 28.6 55.3 26.7 pass@10 33.2 64.1 30. pass@1 17.6 32.7 15.1 Math Multimodal gsm8K MATH Math-Vision Math-Vista 42.9 66.3 23.4 14.8 23.9 9. 23.0 24.3 1.3 61.5 62.3 0.8 Table 3: DnD can generate parameters for more complex tasks like math, code and multimodal question answering. Our method continues to show strong zero-shot ability on these tasks. 3.3 Generalization to coding, math, and multimodal tasks To further validate our methods applicability in more complex scenarios, we also employ DnD for coding, math, and multimodal tasks. The empirical results and findings are as belows. Coding. Similar to common sense reasoning task, we use LoRA to fine-tune Qwen2.5-1.5B on seven coding datasets and save the checkpoints as training data. Evaluation is carried out on HumanEval [8] benchmark using pass@k [32] score (k = 1, 5, 10). Note that neither LoRA fine-tuned models nor DnD has seen any samples of the benchmark in their training. Therefore, we directly test training LoRAs and our synthesized ones on HumanEval. From Table 3, we can draw some findings: i) Our method yields promising results, with improvement over average pass@1 = 15.1, pass@5 = 26.7, pass@10 = 30.9. ii) Despite training LoRAs perform poorly on the test set, DnD still obtains good performance. This means instead of memorizing parameters seen in training, it learns to fit novel datasets given condition as inspirations. Math. We fine-tune Qwen2.5-1.5B on six math datasets and save the checkpoints. We adopt gsm8K [11] and MATH [23] as our benchmarks and accuracy as evaluation metrics. Results listed in Table 3 show commonalities with those regarding common sense reasoning and coding tasks, underscoring the superiority of our method across wide range of scenarios. Multimodal. The above results confirm our methods effectiveness in text modality. In the following, we explore its greater potential by pacing towards other modalities. We fine-tune Qwen2.5-VL-3B [4] on MathV360K [54], save the checkpoints, and evaluate using Math-Vision [59] and Math-Vista [38]. Results in Table 3 show that DnD perform well in multimodal tasks, revealing that our method can be adapted to modalities other than texts and has promising application potential. Takeaway: Based on the above results and comparisons, DnD is high-performing zero-shot learner with strong robustness and wide applicability, as reflected by the significant improvements compared to its training data and its promising performance across various scenarios. In the following, we continue to explore more interesting features of our proposed approach. 3.4 Ablation Studies This section mainly aims at exploring series of interesting features about our approach. For those exploring various settings in our experiment, we report them in Appendix B.3 for thoroughness. If not stated, we use ARC-c as the test set and other common sense reasoning datasets for training. What types of data will help Drag-and-Drop LLMs better? As introduced in Section 2.3, we use prompts as conditions to inspire DnD. Can this drag-and-drop ability holds when condition types changes (e.g., answers)? We carry out ablation studies by changing condition types as prompt, prompt + answer and their mixture (prompt : answer=4 : 1) and report the results in Table 4a. It can be observed that the prompt + answer group surprisingly lead to poor performance. We conclude that it is because answers in common sense reasoning datasets lack diversity (i.e., A/B/C/D) and combining them with prompts may detriment dataset-specific representations. This shall hinder generator to distinguish different datasets and generate specific parameters. Consequently, we advise not to use answer alone as inspirations. However, conclusions may be different for some tasks where answers are more complicated and diverse and we show the results on these tasks in Appendix B.3. How does the choice of condition extractor affect DnDs performance? Our default condition extractor is Sentence-BERT [46], yet it is interesting to explore other models potentials. To ensure 6 condition type prompt prompt + answer mix training LoRAs accuracy 51.6 27.0 49.7 39. condition extractor Glove Sentence-BERT T5-base Qwen2.5-7B accuracy 50.8 51.6 50.2 fail dataset arrangement 6 train, 1 test 4 train, 3 test 3 train, 4 test 2 train, 5 test improves. 12.1 11.4 0.8 -1.4 (a) Condition types. Pure prompts used as inspirations yield the best results compared to other formats. Table 4: Ablation studies about condition types, condition extractors type, and train-test set arrangement. This series of explorations validate several designs of DnD. (c) Train-test set arrangements. More diverse training data introduces higher improvements. (b) Extractor structure. Several encoder-based extractors perform better than decoder-only ones. thorough comparisons, we include classical word2vector method Glove [43], the default encoder-only Sentence-BERT [46], encoder-decoder model T5 [45] and decoder-only Qwen2.5-7B [62]. Results in Table 4b reveal several insights: i) Even traditional methods such as Glove can help DnD to obtain promising result, indicating our method can fit plenty of text encoders. ii) Qwen2.5-7B performs not as good as expect, which has two possible causes: First, its heaviness limits the number of conditions paired with parameters per iteration, leading to poor awareness of novel datasets. Similar conclusions can be drawn from our experiments in Appendix B.3. Second, Qwen2.5-7Bs decoder-only architecture may constraint conditions diversity, since it encodes prompts to answers. What property of training data equips our method with drag-and-drop ability? By default, we train on several datasets and test on 1 novel dataset. In this part, we explore DnDs robustness by shrinking the number of train sets and test on more datasets. Train-test set arrangements are: 6-1, 4-3, 3-4 and 2-5. Generated parameters are compared with training LoRAs average accuracy on the unseen datasets and their average improvements in accuracy are reported in Table 4c. It can be observed that: i) Generally, more training datasets lead to better performance improvement. This is expected since more data ensures better coverage of condition-parameter correlations and lead to better robustness for novel data. ii) DnD fails to drag-and-drop LLMs to novel datasets when training samples are few. As datasets used for training lessen, the average improvement of DnD drops accordingly. It hardly improves over training LoRAs for the 2-5 case. We can conclude that basic amount of training samples are needed for DnD to learn condition-parameter correlations. How does DnDs performance compared with foundation LLMs? Given massive pretraining LLMs often take, fine-tuning on small downstream dataset may detriment their zero-shot performance on novel test sets. Aware of this phenomenon, we compare DnD generated weights performance with foundation LLMs across all tasks involved in our experiment. Specifically, for foundation LLMs, we adopt Qwen2.5-0.5B for common sense reasoning, 1.5B for math and coding, and Qwen2.5-VL-3B for multimodal task. Results in Table 5 again show our approachs superiority: DnD outperforms foundation LLMs across all tasks. Its drag-and-drop ability can generate task-specific parameters, with performance better than foundation LLMs that go through abundant pretraining. 3.5 Open Explorations and Analysis testset method foundation LLM DnD improves. () ARC-c HumanEval gsm8K Math-Vision Table 5: DnD surpasses foundation LLMs across various tasks, showing the drag-and-drop effect. 13.3 31.8 1.9 1.6 38.3 32.3 64.4 22.7 51.6 64.1 66.3 24.3 Condition-Parameter Pairing. performance than that introduced in Section 2.3. We test 2 condition pairing strategies: In this part, we explore other pairing strategies influence on Strategy 1: We fix the total number of prompts to be 128, 256, 512, 1024, 2048, 5000 and use all those prompts to pair with parameters every iteration (x x). Strategy 2: We fix the length of prompt batch to be 128 in every iteration and randomly picks these prompts from 128, 256, 512, 1024, 2048, 5000 candidate prompts (128 x). Based on the results in Figure 4a, we can draw several conclusions: i) With limited number of conditions, DnD fails to generalize over novel data, since it can hardly learn comprehensive knowledge about condition-parameter mappings landscape. ii) As number of condition increases, Strategy 2s performance skyrockets since DnD is exposed to sufficient condition-parameter pairs. This indicates Strategy 2 may help DnD to converge efficiently. iii) Strategy 1 needs more conditions to reach 7 (a) Random selection and pairing is better than using the same conditions for all parameters. (b) DnD can reach comparable or even better performance than fullshot while being 2.512K faster. (c) DnD outperforms popular fewshot tuning and ICL before 256 shots while avoiding using answers. Figure 4: Explorations about DnDs condition-parameter pairing strategy, compare DnD with stateof-the-art methods, and its superiority over few-shot tuning and in-context learning. comparable performance as Strategy 2. With large number of conditions, Strategy 1 suffers from out of memory issues. The same-condition-per-parameter strategy may hinder DnD to associate conditions with specific datasets. Conclusively, Strategy 2 is superior than Strategy 1 both in model generality, convergence speed, and memory consumption. These conclusions are consistent with findings in Table 4c: Diversity of training data equips our method with drag-and-drop ability. DnD vs full-shot tuning. In this part, we compare DnD with full-shot tuning in accuracy and overhead. Specifically, we test ARC-c tuned LoRAs (75 iterations for ARC-c, detailed in Appendix A.4) full-shot performance using ARC-cs test set. Since LoRAs performance might improve with more iterations, we fine-tune LoRA on ARC-c for 300 iterations and test its performance as well. These results are compared with zero-shot DnD in Figure 4b: i) using mild tuned checkpoints for training, DnD already yields better results than full-shot tuning. This showcases DnDs impressive zero-shot ability even surpasses full-shot tuning. ii) DnD is incredibly efficient, with better performance than full-shot tuning while being 2500 faster. Moreover, as training continues, though full-shot prevails, our method shows minimal performance gap with it while being 12,000 more efficient. Efficiency analysis of DnD. In additional to full-shot tuning, in-context learning (ICL) and few-shot tuning (FS) are also popular methods in LLM fine-tuning. In Figure 4c, we conduct experiments investigating the performance-efficiency trade-off of them. Several observations can be drawn: i) Both ICL and FS results are poor when shots are few, but their overhead rises as shots increase. ii) DnD can reach better performance than FS and ICL before 256 shots with negligible overhead. iii) It is noteworthy that both few-shot and ICL use answers for instructing the LLM to obtain better performance. On the contrary, DnD relies on only 128 unlabeled prompts. Based on the above results, we anticipate DnD is powerful and efficient zero-shot learner. Scalability of DnD. In this part, we explore DnDs scalability. Since common sense reasoning is simple and 0.5B model suffices, we focus on math and coding tasks while increasing foundation model size from 1.5B to 7B. We use original math and coding datasets in Section 3.3, evaluate using gsm8K for math and more difficult benchmark LiveCodeBench [25] for coding. We report accuracy on math task and pass@1 for coding in Table 6. It can be observed that: i) DnD consistently surpasses training LoRAs in both tasks under 7B model setting, underscoring its eminent scalability for larger foundation LLMs. ii) With more difficult coding benchmark, DnD maintains superior performance with training LoRAs, i.e., improvement over average pass@1 = 20.3. It reveals DnDs capacity for generalizing to more complex benchmarks, showing promising application potential and robustness. Comparisons with previous methods. We compare our method with most recent parameter generation method RPG [58]. We explore both methods performance in two scenarios: Close Figure 5: DnD and RPG perform well in most close-set tests. set generation: generate paramHowever, RPG can hardly generate parameters for novel dataset eters seen in training. Open set while DnD still presents strong zero-shot ability on open-set test. generation: generate parameters for unseen datasets. Figure 5 shows both methods work well on close set generation, but RPG fails on testset method training LoRAs DnD improves () LiveCodeBench gsm8K Table 6: DnD scales well with larger 7B foundation models, and maintains strong performance in more complex benchmark LiveCodeBench. 13.0 65.9 20.3 17. 33.3 83.1 8 open set generation, showing that our designs (prompts as conditions, condition-parameter pairing) have certain robustness and generality towards novel datasets. Visualization for the effect of drag-and-drop. In this part, we visualize original and generated It can parameters in Figure 6. be seen that original parameters exhibit diverse patterns in the weight space, forming different clusters. Moreover, even parameters close to those tuned on the target dataset (i.e., ARC-c) can have large performance gap (i.e., 19.1% compared with 40.7%). After training on these models that have distinct features, DnD can generate parameters for the target dataset in zero-shot manner. The generated parameters are close to the original ones in the weight space and with even superior performance than full-shot tuning (i.e., 51.6% compared with 40.7%). This brings the drag-and-drop effect to life. Figure 6: DnD generates parameters with close distribution to original ones in the weight space and promising performance."
        },
        {
            "title": "4 Related Works",
            "content": "Parameter-Efficient-Fine-Tuning (PEFT). LLMs size rapidly scales up in the 2020s, making full parameter fine-tuning infeasible. To address this, Low-Rank Adaptation (LoRA) [24] has been proposed, leveraging LLMs inherent sparsity and substantially reduces fine-tuning costs by optimizing two low rank matrices instead of the original weights. Multiple variants of LoRA emerge afterwards, such as DoRA [36], LoRA+ [22], VeRA [30], and DyLoRA [56]. However, all of them have one potential flaw: they requires tuning model parameters for every novel dataset, therefore lacks generality. This can still induce extra costs as model size enlarges and training data increases. Parameter generation. Parameter generation trains on model checkpoints and aims at generate high-performing parameters, both seen and unseen in training. Previous works [6, 19, 29, 16] focuses on learning distributions over the parameters, yet struggle to reconstruct original models performance. With the development of diffusion models, Hyper-Representations [50, 51, 53] and p-diff [57], use the latent diffusion architecture to generate high-performing parameters. Armed with Mamba [20] and appropriate tokenization strategy, RPG [58] can generate 200M of parameters in minutes. Regarding conditional generation, COND P-DIFF [26], Tina [34] and ORAL [27] explores text-controlled parameter generation method. RPG even generate parameters for novel datasets on binary embedding classification task for CIFAR-10 [31]. However, these methods can hardly keep promising zero-shot ability on more complex tasks, hindering parameter generations greater potential. On the other hand, our method use prompts in novel datasets as conditions, captures parameters relations with datasets better, and is able to generate competent parameters for novel datasets."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "PEFT offers an efficient solution to reduce costs when customizing LLMs/VLLMs for downstream tasks, yet it is still expensive when models are large and tasks are diverse. In our study, we train parameter generator to map prompt-weight pairs, which can produce customized weights for novel tasks by processing unlabeled task prompts. Notably, our approach can transform unlabeled task prompts directly to LoRA weights update in seconds. This prompt-to-weight paradigm, which can generate task-specific weights without further tuning, sheds lights on promising new direction for efficient LLM and VLLM customization. Previous research [50, 57, 58] and our approach demonstrate that neural network weights can be effectively synthesized. It appears that network parameters can be viewed as new form of data modality. To excavate this emerging fields potential, several challenges remain to be tackled. First, scaling parameter generation to larger models (7B-70B parameters) requires novel architectural and algorithmic advances. Second, leveraging existing pre-trained checkpoints from the Internet could enhance the practicality of parameter generators. Third, generating structurally diverse models adaptable to various hardware configurations would improve deployment flexibility. 9 Ackonwledgement. We sincerely appreciate Yuxiang Li, Jiaxin Wu, Bohan Zhuang, Ziheng Qin, Zangwei Zheng, Zihan Qiu, Zexi Li, Gongfan Fang, Xinyin Ma, and Qinglin Lu for valuable discussions and feedbacks during this work."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] ajibawa 2023. ajibawa-2023/code-74k-sharegpt. Code-74k-ShareGPT, 2023. [3] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In NAACL, 2019. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In AAAI, 2020. [6] Léon Bottou et al. Stochastic gradient learning in neural networks. NeuroNîmes, 1991. [7] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/codealpaca, 2023. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [12] Rosetta Code. Rosetta code rosetta code,, 2022. [Online; accessed 8-December-2022]. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. [14] flytech. llama-python-codes-30k. llama-python-codes-30k, 2023. [15] flytech. flytech/python-codes-25k. python-codes-25k, 2024. [16] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In ICML, 2016. [17] glaiveai. glaiveai/glaive-code-assistant-v2. glaive-code-assistant-v2, 2024. [18] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [19] Alex Graves. Practical variational inference for neural networks. In NeurIPS, 2011. [20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In COLM, 2024. [21] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [22] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. In ICML. PMLR, 2024. [23] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, 2021. [24] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [25] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In ICLR, 2024. [26] Xiaolong Jin, Kai Wang, Dongwen Tang, Wangbo Zhao, Yukun Zhou, Junshu Tang, and Yang You. Conditional lora parameter generation. arXiv preprint arXiv:2408.01415, 2024. [27] Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, and Tianlong Chen. Oral: Prompting your large-scale loras via conditional recurrent diffusion. arXiv preprint arXiv:2503.24354, 2025. [28] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In ICML, 2021. [29] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [30] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Asano. Vera: Vector-based random matrix adaptation. In ICLR, 2024. [31] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. [32] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang. Spoc: Search-based pseudocode to code. In NeurIPS, 2019. [33] LawalAfeez. Lawalafeez/science-dataset. , 2023. [34] Zexi Li, Lingzhi Gao, and Chao Wu. Text-to-model: Text-conditioned neural network diffusion for train-once-for-all personalization. arXiv preprint arXiv:2405.14132, 2024. [35] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [36] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In ICML, 2024. [37] Zhuang Liu and Kaiming He. decades battle on dataset bias: Are we there yet? In ICLR, 2025. [38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. [39] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In ICLR, 2024. 11 [40] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. [41] moremilk. Tot-math-v1. ToT-Math-V1, 2023. [42] William Peebles, Ilija Radosavovic, Tim Brooks, Alexei Efros, and Jitendra Malik. Learning to learn with generative models of neural network checkpoints. arXiv preprint arXiv:2209.12892, 2022. [43] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014. [44] prithivMLmods. Math-iio-68k-mini. Math-IIO-68K-Mini, 2023. [45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. [46] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In EMNLP, 2019. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In CVPR, 2024. [49] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In AAAI, 2020. [50] Konstantin Schürholt, Boris Knyazev, Xavier Giró-i Nieto, and Damian Borth. Hyperrepresentations for pre-training and transfer learning. In NeurIPS, 2022. [51] Konstantin Schürholt, Boris Knyazev, Xavier Giró-i Nieto, and Damian Borth. Hyperrepresentations for pre-training and transfer learning. arXiv preprint arXiv:2207.10951, 2022. [52] Konstantin Schürholt, Michael Mahoney, and Damian Borth. Towards scalable and versatile weight space learning. ICML, 2024. [53] Konstantin Schürholt, Michael Mahoney, and Damian Borth. Towards scalable and versatile weight space learning. arXiv preprint arXiv:2406.09997, 2024. [54] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See Kiong Ng, Lidong Bing, and Roy Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. In EMNLP, 2024. [55] TIGER-Lab. Math-plus. MATH-plus, 2023. [56] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameterefficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In ACL, 2023. [57] Kai Wang, Dongwen Tang, Boya Zeng, Yida Yin, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, and Yang You. Neural network diffusion. arXiv preprint arXiv:2402.13144, 2024. [58] Kai Wang, Dongwen Tang, Wangbo Zhao, Konstantin Schürholt, Zhangyang Wang, and Yang You. Recurrent diffusion for large-scale parameter generation. arXiv preprint arXiv:2501.11587, 2025. [59] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In NeurIPS, 2024. [60] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In ACL, pages 1348413508, 2023. [61] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [62] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [63] Shuo Yin, Weihao You, Zhilong Ji, Guoqiang Zhong, and Jinfeng Bai. Mumath-code: Combining tool-use large language models with multi-perspective data augmentation for mathematical reasoning. arXiv preprint arXiv:2405.07551, 2024. [64] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In ACL, 2019. [65] Boya Zeng, Yida Yin, and Zhuang Liu. Understanding bias in large-scale visual datasets. In NeurIPS, 2024. 13 We organize our appendix as follows. Hyper-parameter Settings Section A.1: Training Recipe. Section A.2: Description of Datasets. Section A.3: Detailed Structure of Hyper-Convolutional Decoder. Additional Experiment Results Section B.1: More Results of Common Sense Reasoning, Math, and Coding. Section B.2: Inference Efficiency Analysis. Section B.3: More Ablation Studies. Hyper-parameter Settings A.1 Training Recipe In this section, we provide details of our training recipe and various hyper-parameter settings. We incorporate multiple tasks in language models, each involves different foundation model sizes, different generator architecture, and training schedules. We report settings for every task in Table 7. training setting batch size optimizer learning rate length of prompt batch training step weight decay max grad norm noise aug. amplitude common sense 128 (0.5B) AdamW 3e-5 128 5,000 0.1 1.0 1e-4 coding 128 (1.5B), 48 (7B) AdamW 3e-5 16 5,000 0.1 1.0 1emath 128 (1.5B), 48 (7B) AdamW 3e-5 32 5,000 0.1 1.0 1e-4 multimodal 64 (3B) AdamW 3e-5 16 5,000 0.1 1.0 1e-4 Table 7: Training recipe for different tasks in Section 3.2 and Section 3.3. Length of prompt batch. It has been introduced in Section 2.3 that every parameter is grouped with certain amount of texts each iteration. Due to the length of prompt in different datasets varies and induces variable training costs, the length of prompt batch also varies. A.2 Description of Datasets In this section, we introduce the datasets used in the paper, including those for common sense reasoning, math, coding, and multimodal tasks. Common sense reasoning. ARC dataset [10] contains grade-school level, multiple-choice science questions and is splited into easy and challenge sets. OBQA [40] aims to promote research in advanced question-answering with salient facts summarized as an open book. PIQA [5] focuses on everyday situations with preference for atypical solutions. HellaSwag [64] instructs models to select from choices that best finish the sentence among ground truth and an adversarial set of machine-generated wrong answers. WinoGrande [49] features fill-in-a-blank task with binary options for commonsense reasoning questions. BoolQ [9] is question answering dataset for yes/no questions containing various factual problems. Coding. Evol-Instruct [39] contains evolutionary prompts tailored for code-related tasks and incorporates code debugging and time-space complexity constraints. Glaive-Assistant [17] is dataset of code problems and solutions generated using Glaives synthetic data generation platform. Python-Codes [15] is cleaned Python dataset covering instructional tasks. Code-ShareGPT [2] consists of conversations along with detailed Python code explanations. It is generated using GPT-3.5, GPT-4 etc. Rosetta-Code [12] presents solutions to the same task in as many different languages as possible, to aid person with grounding in one approach to problem in learning another. LLaMAPython-Codes [14] primarily focuses on instructional tasks in Python, tokenized specifically for the 14 Llama architecture. It is blend of GPT-4 generated content, custom codes, behavioral approaches and tasks. Code-Alpaca [7] dataset is generated by the techniques in [60], with some modifications. Math. Competition-Math [23] consists of problems from math competitions, including the AMC 10, AMC 12, AIME, and more. Math-IIO-68K-Mini [44] mathematical questions and with corresponding step-by-step solutions. MathQA [3] is large-scale dataset of math word problems that are densely annotated with operation programs. Math-Plus [55] is augmented dataset with GPT-4. Mu-Math [63] is meta-evaluation dataset derived from the U-MATH [63] benchmark, intended to assess the ability of LLMs to judge free-form mathematical solutions. ToT-Math-V1 [41] prioritizes reasoning and explanatory problem-solving over provided answers. Multimodal. MathV360K [54] consists 40K images from 24 datasets and 360K question-answer pairs. It is used to enhance the multimodal mathematical reasoning capabilities of MLLMs. A.3 Detailed Structure of Hyper-Convolutional Decoder Details of condition extractor. As introduced in Section 3, we use Sentence-BERT [46] as our condition extractor in default (all-MiniLM-L6-v2 specifically). Since BERTs supported sequence length is only 512, for longer sequences, we need to preprocess the input sequences. Specifically, we first pad the sequence to the length that can be divided by 512, slice it into multiple sub-strings, and encode the sub-strings respectively. The input length for different tasks is in the table below. length common sense coding 26624 math 4608 multimodal 1536 Details of hyper-convolutional decoder. In this part, we delve into hyper-convolutional decoders inner architecture introduced in Section 2.5. The decoder consists of multiple cascading hyper-convolutional blocks, each containing 5 2D convolution modules. Specifically, we divide convolutions into three categories: i) width convolution that operates on (C, L) dimension, ii) height convolution that operates on (L, ) dimension) iii) layer-wise convolution that on (N, L) dimension) , with notations ConvW , ConvH , and ConvL. In the above convolution modules, the input tensor is transposed to the shape that specified dimensions act as feature maps, the remaining dimension act as channel dimension like conventional convolution. Each hyper-convolutional block consists of two ConvW , two ConvH and one ConvL. Given this, the forward operation of hyper-convolutional block can be formulated as: = Conv1 cl = Conv2 cl cl = ConvL( (cl1)); (cl1)); + (Conv1 (Conv2 + cl cl 3 ), (5) where cl is hidden state output by the th layer, c0 is prompt embedding encoded by the condition extractor, and is learnable bias. Through this process, it transforms the input with shape of [B, N, L, C] to [B, , L, ]. Model architecture used in different tasks. In this part, we show the architecture of hyperconvolutional decoders. We use the three element tuple (N, L, C) to represent decoder structure since it reflects input conditions changes in the network. Also, dimension is omitted since it doesnt affect the model architecture. Note that for math and coding tasks, we first project the dimension to 1000 to reduce overwhelming memory costs induced by giant convolution kernels. A.4 Details of trained checkpoints collection In this section, we discuss how we collect checkpoints in detail. It is noteworthy that all checkpoint collection process go through two phases: i) Pretraining on the target dataset for specified steps. ii) Fine-tuning on the target dataset for certain additional steps, while saving checkpoint at each step. Therefor, the essence of checkpoint collection is the pretrain and fine-tune phases learning rate, training steps, number of samples, and batch size. Note that except for learning rate and training steps, all other hyper-parameters are kept the same. Detailed settings are in Table 9. For those datasets contain less samples than the number specified below, we use the entire dataset for training. 15 task common sense foundation model 0.5B coding math multimodal 1.5B 7B 1.5B 7B 3B channel (128, 384, 384)(128, 200, 300)(128, 100, 256) (256, 50, 200)(512, 50, 200)(1024, 25, 200) (1024, 10, 200)(2048, 10, 200)(4296, 8, 128) (32, 1000, 384)(64, 500, 300)(256, 500, 300)(512, 125, 300) (1024, 64, 256)(2048, 32, 256)(4508, 16, 256) (16, 1000, 384)(64, 500, 384)(256, 125, 400)(512, 64, 400) (1024, 64, 400)(2048, 32, 400), (4928, 16, 512) (16, 1000, 384)(64, 500, 300)(256, 125, 300)(512, 64, 300) (1024, 64, 256)(2048, 32, 256)(4508, 16, 256), (16, 1000, 384)(64, 500, 384)(256, 125, 400)(512, 64, 400) (1024, 64, 400)(2048, 32, 400), (4928, 16, 512) (16, 1536, 384)(64, 500, 300)(256, 125, 300) (1024, 64, 300)(2048, 16, 256)(7308, 16, 256) Table 8: Model architectures for different tasks. task setting common sense coding math multimodal lr. 1e-4 1e-4 1e-4 1e-4 training step 75 4000 4000 8000 pretrain batch size 32 64 64 64 #num. samples 5000 10000 10000 100000 finetune training step 50 100 100 200 lr. 1e-5 1e-6 1e-6 1e-6 Table 9: Details for checkpoint collection."
        },
        {
            "title": "B Additional Experiment Results",
            "content": "B.1 More Results of Common Sense Reasoning, Math, and Coding In this section, we delve into details about each group of training LoRAs performance on test sets, report their performance and discuss further findings. ARC-e OBQA ARC-c 46.2 38.4 38.2 27.0 43.2 fail 26.8 16.6 30.2 40.8 train set test set training LoRA of ARC-e of OBQA of ARC-c of PIQA of HellaSwag of BoolQ of WinoGrande Qwen2.5-0.5B average of training LoRAs DnD BoolQ WinoGrande 44.6 1.3 23.6 9.2 0.5 22.11 1.6 37.0 13.5 44.9 PIQA HellaSwag 80.4 56.1 65.9 66.2 40.4 fail 0.1 16.6 40.5 87.9 52.2 - - 50.3 52.8 fail 50.5 50.2 38.8 50. 30.6 29.0 46.7 24.4 23.4 fail 3.8 26.5 22.4 25.9 46.7 53.4 40.7 24.7 41.0 52.0 19.1 38.3 39.5 51.6 59.4 64.3 57.2 27.9 57.2 fail 18.6 54.8 37.5 68.6 Table 10: More results for common sense reasoning task. Red marks full-shot results, gray shows the average of training LoRAs, and green marks the results of DnD. DnD not only consistently surpasses average of training LoRAs, but also outperform their full-shot performance in most cases. Common sense reasoning. In this part, we show training LoRAs performance on each test set in Table 10. Each row specifies the dataset LoRA is trained on, and each column denotes the dataset used for testing. Consequently, the diagonal elements are full-shot results of training LoRAs, which are marked in red . We also incorporate their average accuracy (exclude diagonal elements, marked in gray ), Qwen2.5-0.5Bs performance, and DnDs zero-shot results (marked in green ). It can be observed that: i) original LoRAs generally perform poorly on novel datasets, e.g., LoRAs tuned on BoolQ fail (accuracy=0.0) on half of zero-shot datasets, this may because training on one specific dataset will fit the parameters for this dataset (binary True/False in BoolQs case) and limit 16 their generality (multiple choices question of other datasets). ii) DnD even outperforms training LoRAs full-shot performances in most cases, showing it has incredible zero-shot ability with great efficiency. These findings further illustrates that: fitting parameters for certain data may lack generality, learning the correlations between data and parameters may be better. Coding. In this part, we elaborate on training LoRAs performance on coding datasets, along with their average (marked in gray ), Qwen2.5-1.5B/7B, and DnDs performance (marked in green ). test set train set pass@1 pass@5 pass@10 Glaive Share Evol GPT Instruct Assistant 28.8 46.2 52. 40.0 53.4 56.4 14.0 27.8 35.0 Python Rosetta 9.8 19.1 23.8 17.6 28.6 33.2 LLaMA Python 13.7 25.5 29. Alpaca 23.2 31.0 34.1 Qwen2.5 train set avg. DnD 1.5B 14.7 26.5 32.3 17.6 28.6 33.2 32.7 55.3 64. testset: HumanEval, average improvement: pass@1 = 15.1, pass@5 = 26.7, pass@10 = 30.9 (a) pass@k (k = 1, 5, 10) scores of foundation LLM, original and generated LoRA for Qwen-1.5B on HumanEval. DnD shows improves largely over its training data and base Qwen, validating its effectiveness. test set train set pass@1 pass@5 pass@10 Glaive Share Evol GPT Instruct Assistant 22.4 28.4 30. 23.2 28.2 30.3 24.6 32.7 35.8 Python Rosetta fail fail fail fail fail fail LLaMA Python fail fail fail Alpaca fail fail fail Qwen2.5 train set avg. DnD 7B 34.1 41.6 43.8 13.0 16.4 17.6 33.4 42.1 46. testset: LiveCodeBench, average improvement: pass@1 = 20.3, pass@5 = 25.7, pass@10 = 28.4 (b) DnD constantly present promising results at more complex benchmarks (i.e., LiveCodeBench), even when over half of its training LoRAs fail (pass@k = 0.0) on this dataset. Table 11: DnD surpasses the average pass@k score of training data on zero-shot coding benchmarks. Empirical results in Table 11 indicates that: i) our approach is able to generalize to complex realworld problems, generating parameters for zero-shot benchmarks with promising results (reflected in improvement over training data). ii) DnDs good performance on the 7B group, despite poor performance of training LoRAs (pass@k=0.0), stressing that our method is not simply memorizing seen parameters, but learn to establish data-parameter mappings and adapt LLMs for novel datasets. test set train set IIO-Mini ToT-Mini Math-Plus Mu-Math Competition Math-QA Qwen train set avg. DnD 66.3 22.8 gsm8K 23.9 7.2 MATH average accuracy improvement: 23.4 on gsm8K, 9.1 on MATH. 31.4 3.5 42.9 14. 31.0 16.5 64.4 29.3 68.3 30.2 68.6 30.0 35.4 1.5 (a) Even with some low-performing training LoRAs (i.e., accuracy less than 50% of base Qwen), DnD still maintains good zero-shot performance, showing the drag-and-drop ability to fit LLMs for novel datasets. test set train set IIO-Mini ToT-Mini Math-Plus Mu-Math Competition Math-QA Qwen train set avg. DnD 50.7 83.1 61.1 gsm8K average accuracy improvement: 17.2 on gsm8K. 81.2 88.3 68.3 60.7 65. 88.3 (b) Using larger Qwen2.5-7B, DnD continues to drag-and-drop it, indicating our method has good scalability. Table 12: DnD continues to work well on math tasks, showing our method has broad applicability. Math. In this part, we report more results for math experiments in Table 12. Similar to common sense reasoning and coding tasks, DnD continues to work well on math datasets, improving largely over the average accuracy of training data on zero-shot benchmarks. This showcases its drag-and-drop ability has wide application scenarios. Also, results on Qwen2.5-7B prove its promising scalability. B.2 Inference Efficiency Analysis In this part,we further analyze DnDs efficiency by presenting its memory usage and inference time for generating various LLMs on respective tasks. We show the cost of generating one single model on NVIDIA A100 80G GPU in Table 13. metrics common sense time (second) memory cost (GB) 0.11 9. math 0.53 (1.5B) 0.55 (7B) 15.43 (1.5B) 16.22 (7B) coding 0.70 (1.5B) 0.73 (7B) 19.17 (1.5B) 20.48 (7B) multimodal 0.61 20.31 Table 13: Inference time and memory cost for different LLMs generation. All metrics are measured on single NVIDIA A100 80G GPU. The time and memory is the cost to generate single model. B.3 More Ablation Studies Can answer serve as condition in more complex scenarios? In Section 3.4, weve explored different condition types and come up with the conclusion: simple, identical answers will limit the diversity of conditions. In this part, we explore using more complex answers from math datasets in Section 3.3 as conditions for building condition-parameter pairs and training DnD. From results in Table 14, we can observe that more complex, diverse answers can lead to better performance. This may because their diversity is able to provide DnD with comprehensive view of conditionparameter mapping. However, as answers are typically much longer than prompts in math and coding datasets due to problem complexity, we still recommend to use prompts as conditions for DnD. condition type answer prompt accuracy on gsm8K(%) 64.0 66.3 Table 14: Answer in math task can serve as conditions, but prompts still work better."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Oxford University",
        "UT Austin",
        "University of St. Gallen"
    ]
}