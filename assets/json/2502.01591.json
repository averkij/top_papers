{
    "paper_title": "Improving Transformer World Models for Data-Efficient RL",
    "authors": [
        "Antoine Dedieu",
        "Joseph Ortiz",
        "Xinghua Lou",
        "Carter Wendelken",
        "Wolfgang Lehrach",
        "J Swaroop Guntupalli",
        "Miguel Lazaro-Gredilla",
        "Kevin Patrick Murphy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) \"Dyna with warmup\", which trains the policy on real and imaginary data, (b) \"nearest neighbor tokenizer\" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) \"block teacher forcing\", which allows the TWM to reason jointly about the future tokens of the next timestep."
        },
        {
            "title": "Start",
            "content": "2025-02-01 Improving Transformer World Models for Data-Efficient RL Antoine Dedieu*,1, Joseph Ortiz*,1, Xinghua Lou1, Carter Wendelken1, Wolfgang Lehrach1, J. Swaroop Guntupalli1, Miguel Lazaro-Gredilla1 and Kevin Murphy1 *Equal contributions, 1Google DeepMind 5 2 0 2 3 ] . [ 1 1 9 5 1 0 . 2 0 5 2 : r We present an approach to model-based RL that achieves new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit wide range of general abilitiessuch as strong generalization, deep exploration, and long-term reasoning. With series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves reward of 67.42% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing SOTA model-free baseline, using novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) Dyna with warmup, which trains the policy on real and imaginary data, (b) nearest neighbor tokenizer on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) block teacher forcing, which allows the TWM to reason jointly about the future tokens of the next timestep. 1. Introduction Reinforcement learning (RL) (Sutton and Barto, 2018) provides framework for training agents to act in environments so as to maximize their rewards. Online RL algorithms interleave taking actions in the environmentcollecting observations and rewardsand updating the policy using the collected experience. Online RL algorithms often employ model-free approach (MFRL), where the agent learns direct mapping from observations to actions, but this can require lot of data to be collected from the environment. Model-based RL (MBRL) aims to reduce the amount of data needed to train the policy by also learning world model (WM), and using this WM to plan in imagination\". To evaluate sample-efficient RL algorithms, it is common to use the Atari-100k benchmark (Kaiser et al., 2019). However, the near-deterministic nature of Atari games allows agents to memorize action sequences without demonstrating true generalization (Machado et al., 2018). In addition, although the benchmark encompasses variety of skills (memory, planning, etc), each individual game typically only emphasizes one or two such skills. To promote the development of agents with broader capabilities, we focus on the Crafter domain (Hafner, 2021), 2D version of Minecraft that challenges single agent to master diverse skill set. Specifically, we use the Craftax-classic environment (Matthews et al., 2024), fast, near-replica of Crafter, implemented in JAX (Bradbury et al., 2018). Key features of Craftax-classic include: (a) procedurally generated stochastic environments (at each episode the agent encounters new environment sampled from common distribution); (b) partial observability, as the agent only sees 63 63 pixel image representing local view of the agents environment, plus visualization of its inventory (see Figure 1[middle]); and (c) an achievement hierarchy that defines sparse reward signal, requiring deep and broad exploration. In this paper, we study improvements to MBRL methods, based on transformer world models (TWM), in the context of the Craftax-classic environment. We make contributions across the following three axes: (a) how the TWM is used (Section 3.4); (b) the tokenization scheme used to create TWM inputs (Section 3.5); (c) and how the TWM is trained (Section 3.6). Collectively, our improvements result in Corresponding author(s): adedieu@google.com, joeortiz@google.com 2025 Google DeepMind. All rights reserved Improving Transformer World Models for Data-Efficient RL Figure 1 [Left] Reward on Craftax-classic. Our best MBRL and MFRL agents outperform all the previously published MFRL and MBRL results, and for the first time, surpass the reward achieved by human expert. We display published methods which report the reward at 1M steps with horizontal line from 900k to 1M steps. [Middle] The Craftax-classic observation is 63 63 pixel image, composed of 9 9 patches of 7 7 pixels. The observation shows the map around the agent and the agents health and inventory. Here we have rendered the image at 144 144 pixels for visibility. [Right] 64 different patches. an agent that, with only 1M environment steps, achieves Craftax-classic reward of 67.42% and score of 27.91%, significantly improving over the previous state of the art (SOTA) reward of 53.20% (Hafner et al., 2023) and the previous SOTA score of 19.4% (Kauvar et al., 2023)1. Our first contribution relates to the way the world model is used: in contrast to recent MBRL methods like IRIS (Micheli et al., 2022) and DreamerV3 (Hafner et al., 2023), which train the policy solely on imagined trajectories (generated by the world model), we train our policy using both imagined rollouts from the world model and real experiences collected in the environment. This is similar to the original Dyna method (Sutton, 1990), although this technique has been abandoned in recent work. In this hybrid regime, we can view the WM as form of generative data augmentation (Van Hasselt et al., 2019). Our second contribution addresses the tokenizer which converts between images and tokens that the TWM ingests and outputs. Most prior work uses vector quantized variational autoencoder (VQ-VAE, Van Den Oord et al. 2017), e.g. IRIS (Micheli et al., 2022), DART (Agarwal et al., 2024). These methods train CNN to process images into feature map, whose elements are then quantized into discrete tokens, using codebook. The sequence of observation tokens across timesteps is used, along with the actions and rewards, to train the WM. We propose two improvements to the tokenizer. First, instead of jointly quantizing the image, we split the image into patches and independently tokenize each patch. Second, we replace the VQ-VAE with simpler nearest-neighbor tokenizer (NNT) for patches. Unlike VQ-VAE, NNT ensures that the meaning\" of each code in the codebook is constant through training, which simplifies the task of learning reliable WM. Our third contribution addresses the way the world model is trained. TWMs are trained by maximizing the log likelihood of the sequence of tokens, which is typically generated autoregressively both over (cid:205)ğ‘ 1The score ğ‘† is given by the geometric mean of the success rate ğ‘ ğ‘– for each of the ğ‘ = 22 achievements; this puts more weight on occasionally solving many achievements than on consistently solving subset. More precisely, the score is given by ğ‘† = exp (cid:16) 1 1, where ğ‘ ğ‘– [0, 100] is the success percentage for achievement ğ‘– (i.e., fraction of episodes in which the achievement was obtained at least once). By contrast, the rewards are just the expected sum of rewards, or in percentage, the arithmetic mean ğ‘… = 1 ğ‘–=1 ğ‘ ğ‘– (ignoring minor contributions to the reward based on the health of the ğ‘ agent). The score and reward are correlated, but are not the same. Unlike some prior work, we report both metrics to make comparisons easier. ğ‘–=1 ln(1 + ğ‘ ğ‘–) (cid:205)ğ‘ ğ‘ (cid:17) 2 Improving Transformer World Models for Data-Efficient RL time and within timeslice. We propose an alternative, which we call block teacher forcing (BTF), that allows TWM to reason jointly about the possible future states of all tokens within timestep, before sampling them in parallel and independently given the history. With BTF, imagined rollouts for training the policy are both faster to sample and more accurate. Our final contributions are some minor architectural changes to the MFRL baseline upon which our MBRL approach is based. These changes are still significant, resulting in simple MFRL method that is much faster than Dreamer V3 and yet obtains much better average reward and score. Our improvements are complementary to each other, and can be combined into ladder of improvements\"similar to the Rainbow\" papers (Hessel et al., 2018) series of improvements on top of model-free DQN agents. 2. Related Work In this section, we discuss related work in MBRL see e.g. Moerland et al. (2023); Murphy (2024); OpenDILab for more comprehensive reviews. We can broadly divide MBRL along two axes. The first axis is whether the world model (WM) is used for background planning (where it helps train the policy by generating imagined trajectories), or decision-time planning (where it is used for lookahead search at inference time). The second axis is whether the WM is generative model of the observation space (potentially via latent bottleneck) or whether is latent-only model trained using self-prediction loss (which is not sufficient to generate full observations). Regarding the first axis, prominent examples of decision-time planning methods that leverage WM include MuZero (Schrittwieser et al., 2020) and EfficientZero (Ye et al., 2021), which use MonteCarlo tree search over discrete action space, as well as TD-MPC2 (Hansen et al., 2024), which uses the cross-entropy method over continuous action space. Although some studies have shown that decision-time planning can sometimes be better than background planning (Alver and Precup, 2024), it is much slower, especially with large WMs such as transformers, since it requires rolling out future hypothetical trajectories at each decision-making step. Therefore in this paper, we focus on background planning (BP). Background planning originates from Dyna (Sutton, 1990), which focused on tabular Q-learning. Since then, many papers have combined the idea with deep RL methods: World Models (Ha and Schmidhuber, 2018b), Dreamer agents (Hafner et al., 2020a,b, 2023), SimPLe (Kaiser et al., 2019), IRIS (Micheli et al., 2022), Î”-IRIS (Micheli et al., 2024), Diamond (Alonso et al., 2024), DART (Agarwal et al., 2024), etc. Regarding the second axis, many methods fit generative WMs of the observations (images) using model with low-dimensional latent variables, either continuous (as in VAE) or discrete (as in VQ-VAE). This includes our method and most background planning methods above 2. In contrast, other methods fit non-generative WMs, which are trained using self-prediction losssee Ni et al. (2024) for detailed discussion. Non-generative WMs are more lightweight and therefore well-suited to decision-time planning with its large number of WM calls at every decision-making step. However, generative WMs are generally preferred for background planning, since it is easy to combine real and imaginary data for policy learning, as we show below. In terms of the WM architecture, many state-of-the-art models use transformers, e.g. IRIS (Micheli et al., 2022), Î”-IRIS (Micheli et al., 2024), DART (Agarwal et al., 2024). Notable exceptions are DreamerV2/3 (Hafner et al., 2020b, 2023), which use recurrent state space models, although improved transformer variants have been proposed (Chen et al., 2022; Robine et al., 2023; Zhang et al., 2024). 2A notable exception is Diamond (Alonso et al., 2024), which fits diffusion world model directly in pixel space, rather than learning latent WM. 3 Improving Transformer World Models for Data-Efficient RL 3. Methods 3.1. MFRL Baseline Our starting point is the previous SOTA MFRL approach which was proposed as baseline in Moon et al. (2024)3. This method achieves reward of 46.91% and score of 15.60% after 1M environment steps. This approach trains stateless CNN policy without frame stacking using the PPO method (Schulman et al., 2017), and adds an entropy penalty to ensure sufficient exploration. The CNN used is modification of the Impala ResNet (Espeholt et al., 2018a). 3.2. MFRL Improvements We improve on this MFRL baseline by both increasing the model size and adding RNN (specifically GRU) to give the policy memory. Interestingly, we find that naively increasing the model size harms performance, while combining larger model with carefully designed RNN helps (see Section 4.3). For the RNN, we find it crucial to ensure the hidden state is low-dimensional, so that the memory is forced to focus on the relevant bits of the past that cannot be extracted from the current image. We concatenate the GRU output to the image embedding, and then pass this to the actor and critic networks, rather than directly passing the GRU output. Algorithm 2, Appendix A.1, presents pseudocode for our MFRL agent. With these architectural changes, we increase the reward to 55.49% and the score to 16.77%. This result is notable since our MFRL agent beats the considerably more complex (and much slower) DreamerV3 agent, which obtains reward of 53.20% and score of 14.5. It also beats other MBRL methods, such as IRIS (Micheli et al., 2022) (reward of 25.0%) and Î”-IRIS (Micheli et al., 2024) 4 (reward of 35.0%). In addition, our MFRL agent only takes 15 minutes to train for 1M environment steps on one A100 GPU. 3.3. MBRL baseline We now describe our MBRL baseline, which combines our MFRL baseline above with transformer world model (TWM)as in IRIS (Micheli et al., 2022). Following IRIS, our MBRL baseline uses VQ-VAE, which quantizes the 8 8 feature map ğ‘ğ‘¡ of CNN to create set of latent codes, (ğ‘1 ğ‘¡ {1, . . . , ğ¾} is discrete code, and ğ¾ = 512 is the size of the codebook. These codes are then passed to TWM, which is trained using teacher forcingsee Equation (2) below. Our MBRL baseline achieves reward of 31.93%, and improves over the reported results of IRIS, which reaches 25.0%. ğ‘¡ ) = enc(ğ‘‚ğ‘¡), where ğ¿ = 64, ğ‘ğ‘– ğ‘¡ , . . . , ğ‘ğ¿ Although these MBRL baselines leverage recent advances in generative world modeling, they are largely outperformed by our best MFRL agent. This motivates us to enhance our MBRL agent, which we explore in the following sections. 3.4. MBRL using Dyna with warmup As discussed in Section 1, we propose to train our MBRL agent on mix of real trajectories (from the environment) and imaginary trajectories (from the TWM), similar to Dyna (Sutton, 1990). Algorithm 1 presents the pseudocode for our MBRL approach. Specifically, unlike many other recent 3The authors main method uses external knowledge about the achievement hierarchy of Crafter, so cannot be compared with other general methods. We use their baseline instead. 4This is consistent with results on Atari-100k, which show that well-tuned model-free methods, such as BBF (Schwarzer et al., 2023), can beat more sophisticated model-based methods. 4 Improving Transformer World Models for Data-Efficient RL MBRL methods (Ha and Schmidhuber, 2018a; Hafner et al., 2020b, 2023; Micheli et al., 2022, 2024) which train their policies exclusively using world model rollouts (Step 4), we include Step 2 which updates the policy with real trajectories. Note that, if we remove Steps 3 and 4 in Algorithm 1, the approach reduces to MFRL. The function rollout(ğ‘‚1, ğœ‹Î¦, ğ‘‡, M) returns trajectory of length ğ‘‡ generated by rolling out the policy ğœ‹Î¦ from the initial state ğ‘‚1 in either the true environment Menv or the world model MÎ˜. trajectory contains collected observations, actions and rewards during the rollout ğœ = (ğ‘‚1:ğ‘‡+1, ğ‘1:ğ‘‡ , ğ‘Ÿ1:ğ‘‡ ). Algorithm 4 in Appendix A.3 details the rollout procedure. We discuss other design choices below. Algorithm 1 MBRL agent. See Appendix A.3 for details. Input: number of environments ğ‘env, environment dynamics Menv, rollout horizon for environment ğ‘‡env and for TWM ğ‘‡WM, background planning starting step ğ‘‡BP, total number of environment steps ğ‘‡total, number of TWM updates ğ‘ iters WM and policy updates ğ‘ iters AC 1 Menv for ğ‘› = 1 : ğ‘, Initialize: observations ğ‘‚ğ‘› data buffer = , TWM model and parameters Î˜, AC model ğœ‹ and parameters Î¦, number of environment steps ğ‘¡ = 0. repeat // 1. Collect data from environment ğœğ‘› env = rollout(ğ‘‚ğ‘› = ğœ1:ğ‘ env ; ğ‘‚1:ğ‘ 1 = ğœ1:ğ‘ 1, ğœ‹Î¦, ğ‘‡env, Menv), ğ‘› = 1 : ğ‘env env [1] ; ğ‘¡+ = ğ‘envğ‘‡ // 2. Update policy on environment data Î¦ = PPO-update-policy(Î¦, ğœ1:ğ‘ env ) // 3. Update world model for it = 1 to ğ‘ iters WM do ğœğ‘› replay = sample-trajectory(D, ğ‘‡WM), ğ‘› = 1 : ğ‘env Î˜ = update-world-model(Î˜, ğœ1:ğ‘env replay) end for // 4. Update policy on imagined data if ğ‘¡ ğ‘‡BP then for it = 1 to ğ‘ iters AC do ğ‘‚ğ‘› 1 = sample-obs(D), ğ‘› = 1 : ğ‘env WM = rollout(ğ‘‚ğ‘› ğœğ‘› 1, ğœ‹Î¦, ğ‘‡WM, MÎ˜), ğ‘› = 1 : ğ‘env Î¦ = PPO-update-policy(Î¦, ğœ1:ğ‘env WM ) end for end if until ğ‘¡ ğ‘‡total PPO. Since PPO (Schulman et al., 2017) is an on-policy algorithm, trajectories should be used for policy updates immediately after they are collected or generated. For this reason, policy updates with real trajectories take place in Step 2 immediately after the data is collected. An alternative approach is to use an off-policy algorithm and mix real and imaginary data into the policy updates in Step 4, hence removing Step 2. We leave this direction as future work. 5 Improving Transformer World Models for Data-Efficient RL Rollout horizon. We set ğ‘‡WM ğ‘‡env, to avoid the problem of compounding errors due to model imperfections (Lambert et al., 2022). However, we find it beneficial to use ğ‘‡WM 1, consistent with Holland et al. (2018); Van Hasselt et al. (2019), who observed that the Dyna approach with ğ‘‡WM = 1 is no better than MFRL with experience replay. Multiple updates. Following IRIS, we update TWM ğ‘ iters ğ‘ iters AC times. WM times and the policy on imagined trajectories Warmup. When mixing imaginary trajectories with real ones, we need to ensure the WM is sufficiently accurate so that it does not pollute\" the replay buffer, thus harming policy learning. Consequently, we only begin training the policy on imaginary trajectories after the agent has interacted with the environment for ğ‘‡BP steps, which ensures it has seen enough data to learn reliable WM. We call this technique Dyna with warmup. In Section 4.3, we show that removing this warmup, and using ğ‘‡BP = 0, drops the reward dramatically, from 67.42% to 33.54%. We additionally show that removing the Dyna method (and only training the policy in imagination) drops the reward to 55.02%. 3.5. Patch nearest-neighbor tokenizer Many MBRL methods based on TWMs use VQ-VAE to map between images and tokens. In this section, we describe our alternative which leverages property of Craftax-classic: each observation is composed of 9 9 patches of size 7 7 each (see Figure 1[middle]). Hence we propose to (a) factorize the tokenizer by patches and (b) use simpler nearest-neighbor style approach to tokenize the patches. Patch factorization. Unlike prior methods which process the full image ğ‘‚ into tokens (ğ‘1, . . . , ğ‘ğ¿) = enc(ğ‘‚), we first divide ğ‘‚ into ğ¿ non-overlapping patches ( ğ‘1, . . . , ğ‘ğ¿) which are independently encoded into ğ¿ tokens: (ğ‘ğ‘–, . . . , ğ‘ğ¿) = (enc( ğ‘1), . . . , enc( ğ‘ğ¿)) . To convert the discrete tokens back to pixel space, we just decode each token independently into patches, and rearrange to form full image: ( Ë†ğ‘1, . . . , Ë†ğ‘ğ¿) = (dec(ğ‘1), . . . , dec(ğ‘ğ¿)) . Factorizing the VQ-VAE on the ğ¿ = 81 patches of each observation boosts performance from 43.36% to 58.92%. Nearest-neighbor tokenizer. On top of patch factorization, we propose simpler nearest-neighbor tokenizer (NNT) to replace the VQ-VAE. The encoding operation for each patch ğ‘ [0, 1] â„ğ‘¤3 is similar to nearest neighbor classifier w.r.t the codebook. The difference is that, if the nearest neighbor is too far away, we add new code equal to ğ‘ to the codebook. More precisely, let us denote CNN = {ğ‘’1, . . . , ğ‘’ğ¾ } the current codebook, consisting of ğ¾ codes ğ‘’ğ‘– [0, 1] â„ğ‘¤3, and ğœ threshold on the Euclidean distance. The NNT encoder is defined as: ğ‘ = enc( ğ‘) = ğ‘ ğ‘’ğ‘– 2 2 argmin 1ğ‘– ğ¾ ğ¾ + 1 ğ‘ ğ‘’ğ‘– 2 2 ğœ if min 1ğ‘– ğ¾ otherwise. (1) The codebook can be thought of as greedy approximation to the coreset of the patches seen so far (Mirzasoleiman et al., 2020). To decode patches, we simply return the code associated with the codebook index, i.e. dec(ğ‘ğ‘–) = ğ‘’ğ‘ğ‘–. 6 Improving Transformer World Models for Data-Efficient RL key benefit of NNT is that once codebook entries are added, they are never updated. static yet growing codebook makes the target distribution for the TWM stationary, greatly simplifying online learning for the TWM. In contrast, the VQ-VAE codebook is continually updated, meaning the TWM must learn from non-stationary distribution, which results in worse WM. Indeed, we show in Section 4.1 that with patch factorization, and when â„ = ğ‘¤ = 7meaning that the patches are aligned with the observationreplacing the VQ-VAE with NNT boosts the agents reward from 58.92% to 64.96%. Figure 1[right] shows an example of the first 64 code patches extracted by our NNT. The main disadvantages of our approach are that (a) patch tokenization can be sensitive to the patch size (see Figure 5[left]), and (b) NNT may create large codebook if there is lot of appearance variation within patches. In Craftax-classic, these problems are not very severe due to the grid structure of the game and limited sprite vocabulary (although continuous variations exist due to lighting and texture randomness). 3.6. Block teacher forcing Figure 2 Approaches for TWM training with ğ¿ = 2, ğ‘‡ = 2. ğ‘â„“ ğ‘¡ denotes token â„“ of timestep ğ‘¡. Tokens in the same timestep have the same color. We exclude action tokens for simplicity. [Left] Usual autoregressive model training with teacher forcing. [Right] Block teacher forcing predicts token ğ‘â„“ from input token ğ‘â„“ ğ‘¡ with block causal attention. ğ‘¡+1 Transformer WMs are typically trained by teacher forcing which maximizes the log likelihood of the token sequence generated autoregressively over time and within timeslice: ğ‘‡ (cid:214) ğ¿ (cid:214) LTF = log Lğ‘– ğ‘¡ , Lğ‘– ğ‘¡ = ğ‘(ğ‘ğ‘– ğ‘¡+1ğ‘1:ğ¿ 1:ğ‘¡ , ğ‘1:ğ‘– ğ‘¡+1 , ğ‘1:ğ‘¡) (2) ğ‘¡=1 ğ‘–=1 We propose more effective alternative, which we call block teacher forcing (BTF). BTF modifies both the supervision and the attention of the TWM. Given the tokens from the previous timesteps, BTF independently predicts all the latent tokens at the next timestep, removing the conditioning on previously generated tokens from the current step: LBTF = log ğ‘‡ (cid:214) ğ¿ (cid:214) ğ‘¡=1 ğ‘–=1 Lğ‘– ğ‘¡ , Lğ‘– ğ‘¡ = ğ‘(ğ‘ğ‘– ğ‘¡+1ğ‘1:ğ¿ 1:ğ‘¡ , ğ‘1:ğ‘¡) (3) Importantly BTF uses block causal attention pattern (see Figure 2), in which tokens within the same timeslice are decoded in-parallel in single forward pass. This attention structure allows the model to reason jointly about the possible future states of all tokens within timestep, before the tokens are ultimately sampled with independent readouts. This property mitigates autoregressive drift. As result, we find that BTF returns more accurate TWMs than fully AR approaches. Overall, adding BTF increases the reward from 64.96% to 67.42%, leading to our best MBRL agent. In addition, we find that BTF is twice as fast, even though in theory, when using key-value caching, BTF and AR both have complexity (ğ¿2ğ‘‡) for generating all the ğ¿ tokens at one timestep, and (ğ¿2ğ‘‡ 2) for generating the entire rollout. Improving Transformer World Models for Data-Efficient RL Table 1 Results on Craftax-classic after 1M environment interactions. * denotes results on Crafter, which may not exactly match Craftax-classic. means unknown. denotes the reported timings on single A100 GPU. Our DreamerV3 results are based on the code from the author, but differ slightly from the reported number, perhaps due to hyperparameter discrepancies. IRIS and Î”-IRIS do not report standard errors for the score. Method Human Expert M1: Baseline M2: M1 + Dyna M3: M2 + patches M4: M3 + NNT M5: M4 + BTF. Our best MBRL Previous best MFRL (Moon et al., 2024) Previous best MFRL (our implementation) Our best MFRL DreamerV3 (Hafner et al., 2023) Our DreamerV3 IRIS (Micheli et al., 2022) Î”-IRIS (Micheli et al., 2024) Curious Replay (Kauvar et al., 2023) Parameters Reward (%) Score (%) Time (min) NA 60.0M 60.0M 56.6M 58.5M 58.5M 4.0M 4.0M 55.6M 201M 201M 48M 25M 65.0 10.5 50.5 6.8 31.93 2.22 43.36 1.84 58.92 1.03 64.96 1.13 67.42 0.55 46.91 2.41 47.40 0.58 55.49 1. 53.2 8. 47.18 3.88 25.0 3.2 35.0 3.2 4.98 0.50 8.85 0.63 19.36 1.42 25.55 0.86 27.91 0.63 15.60 1.66 10.71 0.29 16.77 1.11 14.5 1.6 6.66 9.30 19.4 1.6 NA 560 563 746 1328 759 26 2100 8330 833 - 4. Results In this section, we report our experimental results on the Craftax-classic benchmark. Each experiment is run on 8 H100 GPUs. All methods are compared after interacting with the environment for ğ‘‡total = 1M steps. All the methods collect trajectories of length ğ‘‡env = 96 in ğ‘env = 48 environment (in parallel). For MBRL methods, the imaginary rollouts are of length ğ‘‡WM = 20, and we start generating these (for policy training) after ğ‘‡BP = 200k environment steps. We update the TWM ğ‘ iters WM = 500 times and the policy ğ‘ iters AC = 150 times. For all metrics, we report the mean and standard error over 10 seeds as ğ‘¥ (ğ‘¦). 4.1. Climbing up the MBRL ladder First, we report the normalized reward (the reward divided by the maximum reward of 22) for series of agents that progressively climb our MBRL ladder\" of improvements in Section 3. Figure 3 show the reward vs. the number of environment steps for the following methods, which we detail in Appendix A.2: M1: Baseline. Our baseline MBRL agent, described in Section 3.3, reaches reward of 31.93%, and improves over IRIS, which gets 25.0%. M2: M1 + Dyna. Training the policy on both (real) environment and (imagined) TWM trajectories, as described in Section 3.4, increases the reward to 43.36%. M3: M2 + patches. Factorizing the VQ-VAE over the ğ¿ = 81 observation patches, as presented in Section 3.5, increases the reward to 58.92%. M4: M3 + NNT. With patch factorization, replacing the VQ-VAE with NNT, as presented in Section 3.5, further boosts the reward to 64.96%. M5: M4 + BTF. Our best MBRL: Finally, incorporating BTF, as described in Section 3.6, leads to our best agent. It achieves reward of 67.42%(0.55), while BTF reduces the training time by factor of two. Improving Transformer World Models for Data-Efficient RL As in IRIS (Micheli et al., 2022), methods M1-3 use codebook size of 512. For M4 and our best MBRL, which use NNT, we found it critical to use larger codebook size of ğ¾ = 4096 and threshold of ğœ = 0.75. Interestingly, when training in imagination begins (at step ğ‘‡BP = 200k), there is temporary drop in performance as the TWM rollouts do not initially match the true environment dynamics, resulting in distribution shift for the policy. Figure 4 Ablations results on Craftax-classic after 1M environment interactions. Method Best MBRL 5 5 quantized 9 9 quantized 7 7 continuous Remove Dyna Remove NNT Remove NNT & patches Remove BTF Reward (%) 67.42 0.55 57.28 1.14 45.55 0.88 21.20 0.55 55.02 5.34 60.66 1.38 45.86 1.42 64.96 1.13 Score (%) 27.91 0.63 18.26 1.18 10.12 0.40 2.43 0.09 18.79 2.14 21.79 1.33 10.36 0.69 25.55 0.86 Use ğ‘‡BP = 33.54 10.09 12.86 4.05 Best MFRL Remove RNN Smaller model 55.49 1.33 41.82 0.97 51.35 0.80 16.77 1.11 8.33 0.44 12.93 0.56 Figure 3 The ladder of improvements presented in Section 3 progressively transforms our baseline MBRL agent into state-of-the-art method on Craftax-classic. Training in imagination starts at step 200k, indicated by the dotted vertical line. 4.2. Comparison to existing methods Figure 1[left] compares the performance of our best MBRL and MFRL agents against various previous methods. See also Figure 7 in Appendix for plot of the score, and Table 1 for detailed numerical comparison of the final performance. First, we observe that our best MFRL agent outperforms almost all of the previously published MFRL and MBRL results, reaching reward of 55.49% and score of 16.77%5. Second, our best MBRL agent achieves new SOTA reward of 67.42% and score of 27.91%. This marks the first agent to surpass human-level reward, derived from 100 episodes played by 5 human expert players (Hafner, 2021). Note that although we achieve superhuman reward, our score is significantly below that of human expert. 4.3. Ablation studies We conduct ablation studies to assess the importance of several components of our proposed MBRL agent. Results are presented in Figure 5 and Table 4. Impact of patch size. We investigate the sensitivity of our approach to the patch size used by NNT. While our best results are achieved when the tokenizer uses the oracle-provided ground truth patch size of 7 7, Figure 5[left] shows that performance remains competitive when using smaller (5 5) or larger (9 9) patches. The necessity of quantizing. Figure 5[left] shows that, when the 77 patches are not quantized, but instead the TWM is trained to reconstruct the continuous 7 7 patches, MBRL performance collapses. 5The only exception is Curious Replay (Kauvar et al., 2023), which builds on DreamerV3 with prioritized experience replay (PER) to train the WM. However, PER is only better on few achievements; this improves the score but not the reward. 9 Improving Transformer World Models for Data-Efficient RL Figure 5 [Left] MBRL performance decreases when NNT uses patches of smaller or larger size than the ground truth, but it remains competitive. However, performance collapses if the patches are not quantized. [Middle] Removing any rung of the ladder of improvements leads to drop in performance. [Right] Warming up the world model before using it to train the policy on imaginary rollouts is required for good performance. BP denotes background planning. For each method, training in imagination starts at the color-coded vertical line, and leads to an initial drop in performance. This is consistent with findings in DreamerV2 (Hafner, 2021), which highlight that quantization is critical for learning an effective world model. Each rung matters. To isolate the impact of each individual improvement, we remove each individual rung of our ladder from our best MBRL agent. As shown in Figure 5[middle], each removal leads to performance drop. This underscores the importance of combining all our proposed enhancements to achieve SOTA performance. When to start training in imagination? Training the policy on imaginary TWM rollouts requires reasonably accurate world model. This is why background planning (Step 4 in Algorithm 1) only begins after ğ‘‡BP environment steps. Figure 5[right] explores the effect of varying ğ‘‡BP. Initiating imagination training too early (ğ‘‡BP = 0) leads to performance collapse due to the inaccurate TWM dynamics. MFRL ablation. The final 3 rows in Table 4 show that either removing the RNN or using smaller model as in Moon et al. (2024) leads to drop in performance. 4.4. Comparing TWM rollouts In this section, we compare the TWM rollouts learned by three world models in our ladder, namely M1, M3 and our best model M5. To do so, we first create an evaluation dataset of ğ‘eval = 160 trajectories, each of length ğ‘‡eval = ğ‘‡WM = 20, collected during the training of our best MFRL agent: (cid:111). We evaluate the quality of imagined trajectories generated by each Deval = TWM. Given TWM checkpoint at 1M steps and the ğ‘›th trajectory in Deval, we execute the sequence of actions ğ‘ğ‘› , to obtain rollout trajectory Ë†ğ‘‚TWM, ğ‘› 1:ğ‘‡eval+1 , starting from ğ‘‚ğ‘› , ğ‘1:ğ‘eval 1:ğ‘‡eval , ğ‘Ÿ1:ğ‘eval 1:ğ‘‡eval ğ‘‚1:ğ‘eval 1:ğ‘‡eval+1 1:ğ‘‡eval (cid:110) . Quantitative evaluations. For evaluation, we leverage an appealing property of Craftax-classic: each observation ğ‘‚ğ‘¡ comes with an array of ground truth symbols ğ‘†ğ‘¡ = (ğ‘†1:ğ‘… ), with ğ‘… = 145. Given 100k pairs (ğ‘‚ğ‘¡, ğ‘†ğ‘¡), we train CNN ğ‘“ğœ‡, to predict the symbols from the observation; ğ‘“ğœ‡ achieves 99% validation accuracy. Next, we use ğ‘“ğœ‡ to predict the symbols from the generated rollouts. Figure 6[left] ğ‘¡ 10 Improving Transformer World Models for Data-Efficient RL Figure 6 Rollout comparison for world models M1, M3 and M5. [Left] Symbol accuracies decrease with the TWM rollout step. The stationary NNT codebook used by M5 makes it easier to learn reliable TWM. [Right] Best viewed zoomed in. Map. All three models accurately capture the agents motion. All models can struggle to use the history to generate consistent map when revisiting locations, however only M1 makes simple map errors in successive timesteps. Feasible hallucinations. M3 and M5 generate realistic hallucinations that respect the game dynamics, such as spawning mobs and losing health. Infeasible hallucinations. M1 often does not respect game dynamics; M1 incorrectly adds wood inventory, and incorrectly places plant at the wrong timestep without the required sapling inventory. M3 exhibits some infeasible hallucinations in which the monster suddenly disappears or the spawned cow has an incorrect appearance. M5 rarely exhibits infeasible hallucinations. Figure 9 in Appendix C.4 shows more rollouts with similar behavior. displays the average symbol accuracy at each timestep ğ‘¡: Ağ‘¡ = 1 ğ‘evalğ‘… ğ‘eval ğ‘… ğ‘›= ğ‘Ÿ=1 1( ğ‘“ ğ‘Ÿ ğœ‡ ( Ë†ğ‘‚TWM, ğ‘› ğ‘¡ ), ğ‘†ğ‘Ÿ,ğ‘› ğ‘¡ ), ğ‘¡, where 1(ğ‘¥, ğ‘¦) = 1 iff. ğ‘¥ = ğ‘¦ (and 0 o.w.), ğ‘†ğ‘Ÿ,ğ‘› ğ‘¡ denotes the ground truth ğ‘Ÿth symbol in the array ğ‘†ğ‘› ğ‘¡ associated with ğ‘‚ğ‘› ğ‘¡ , and ğ‘“ ğ‘Ÿ ) its prediction for the rollout observation. As expected, symbol accuracies decrease with ğ‘¡ as mistakes compound over the rollouts. Our best method, which uses NNT, achieves the highest accuracies for all timesteps, as it best captures the game dynamics. This highlights that stationary codebook makes TWM learning simpler. ğœ‡ ( Ë†ğ‘‚TWM, ğ‘› ğ‘¡ We include two additional quantitative evaluations in Appendix C, showing that M5 achieves the lowest tokenizer reconstruction errors and rollout reconstruction errors. Qualitative evaluations. Due to environment stochasticity, TWM rollouts can differ from the environment rollout but still be useful for learning in imaginationas long as they respect the game dynamics. Visual inspection of rollouts in Figure 6[right] reveals (a) map inconsistencies, (b) feasible hallucinations that respect the game dynamics and (c) infeasible hallucinations. M1 can make simple mistakes in both the map and the game dynamics. M3 and M5 both generate feasible hallucinations of mobs, however M3 more often hallucinates infeasible rollouts. 4.5. Craftax Full Table 2 compares the performance of various agents on the full version of Craftax (Matthews et al., 2024), significantly harder extension of Craftax-classic, with more levels and achievements. While the previous SOTA agent reached 2.3% reward (on symbolic inputs), our MFRL agent reaches 4.63% reward and our MBRL agent reaches new SOTA reward of 5.44%. See Appendix for implementation details. These results show that our techniques can generalize to harder environments. 11 Improving Transformer World Models for Data-Efficient RL Table 2 Results on Craftax after 1M environment interactions. The previous SOTA reward uses symbolic input (score is unknown), whereas our results use image input. Method Reward (%) Score (%) Prev. SOTA MFRL Our best MFRL Our best MBRL 2.3 (symbolic) 4.63 0.20 5.44 0.25 1.22 0.07 1.53 0.10 5. Conclusion and future work In this paper, we present three improvements to vision-based MBRL agents which use transformer world models for background planning: Dyna with warmup, patch nearest-neighbor tokenization and block teacher forcing. We also present improvements to the MFRL baseline, which may be of independent interest. Collectively, these improvements result in MBRL agent that achieves significantly higher reward and score than previous SOTA agents on the challenging Craftax-classic benchmark. Notably, our MBRL agent surpasses expert human reward for the first time. In the future, we plan to examine how well our techniques generalize beyond Craftax. However, we believe our current results will already be of interest to the community. We see several paths to build upon our method. Prioritized experience replay is promising approach to accelerate TWM training, and an off-policy RL algorithm could improve policy updates by mixing imagined and real data. In the longer term, we would like to generalize our tokenizer to extract patches and tokens from large pre-trained models, such as SAM (Ravi et al., 2024) and Dino-V2 (Oquab et al., 2024). This inherits the stable codebook of our approach, but reduces sensitivity to patch size and superficial\" appearance variations. To explore this direction, and other non-reconstructive world models which cannot generate future pixels, we plan to modify the policy to directly accept latent tokens generated by the TWM. 12 Improving Transformer World Models for Data-Efficient RL"
        },
        {
            "title": "References",
            "content": "P. Agarwal, S. Andrews, and S. E. Kahou. Learning to play atari in world of tokens. ICML, 2024. E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. Storkey, T. Pearce, and F. Fleuret. Diffusion for world modeling: Visual details matter in atari. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= NadTwTODgC. S. Alver and D. Precup. look at value-based decision-time vs. background planning methods across different settings. In Seventeenth European Workshop on Reinforcement Learning, Oct. 2024. URL https://openreview.net/pdf?id=Vx2ETvHId8. J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. C. Chen, Y.-F. Wu, J. Yoon, and S. Ahn. Transdreamer: Reinforcement learning with transformer world models. URL http://arxiv. org/abs/2202, 9481, 2022. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In ICML, pages 14071416. PMLR, July 2018a. URL https: //proceedings.mlr.press/v80/espeholt18a.html. L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages 14071416. PMLR, 2018b. J. Farebrother, J. Orbay, Q. Vuong, A. A. Taiga, Y. Chebotar, T. Xiao, A. Irpan, S. Levine, P. S. Castro, A. Faust, A. Kumar, and R. Agarwal. Stop regressing: Training value functions via classification for scalable deep RL. In Forty-first International Conference on Machine Learning, June 2024. URL https://openreview.net/pdf?id=dVpFKfqF3R. D. Ha and J. Schmidhuber. World models. In NIPS, 2018a. URL http://arxiv.org/abs/1803. 10122. D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018b. D. Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021. D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In ICLR, 2020a. URL https://openreview.net/forum?id=S1lOTC4tDS. D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020b. D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. N. Hansen, H. Su, and X. Wang. TD-MPC2: Scalable, robust world models for continuous control. 2024. URL http://arxiv.org/abs/2310.16828. 13 Improving Transformer World Models for Data-Efficient RL K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, 2018. URL http://arxiv.org/abs/1710.02298. G. Z. Holland, E. J. Talvitie, and M. Bowling. The effect of planning shape on dyna-style planning in high-dimensional state spaces. arXiv [cs.AI], June 2018. URL http://arxiv.org/abs/1806. 01825. L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine, A. Mohiuddin, R. Sepassi, G. Tucker, and H. Michalewski. Model-based reinforcement learning for atari. arXiv [cs.LG], Mar. 2019. URL http://arxiv.org/abs/1903. 00374. S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018. I. Kauvar, C. Doyle, L. Zhou, and N. Haber. Curious replay for model-based adaptation. In ICML, June 2023. URL https://arxiv.org/abs/2306.15934. D. P. Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. N. Lambert, K. Pister, and R. Calandra. Investigating compounding prediction errors in learned dynamics models. arXiv [cs.LG], Mar. 2022. URL http://arxiv.org/abs/2203.09637. J. Lei Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. ArXiv e-prints, pages arXiv1607, 2016. C. Lu, J. Kuba, A. Letcher, L. Metz, C. Schroeder de Witt, and J. Foerster. Discovered policy optimisation. Advances in Neural Information Processing Systems, 35:1645516468, 2022. M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. J. Artif. Intell. Res., 2018. URL http://arxiv.org/abs/1709.06009. M. Matthews, M. Beukman, B. Ellis, M. Samvelyan, M. Jackson, S. Coward, and J. Foerster. Craftax: lightning-fast benchmark for open-ended reinforcement learning. arXiv preprint arXiv:2402.16801, 2024. V. Micheli, E. Alonso, and F. Fleuret. Transformers are sample-efficient world models. arXiv preprint arXiv:2209.00588, 2022. V. Micheli, E. Alonso, and F. Fleuret. Efficient world models with context-aware tokenization. arXiv preprint arXiv:2406.19320, 2024. B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efficient training of maIn ICML, 2020. URL http://proceedings.mlr.press/v119/ chine learning models. mirzasoleiman20a/mirzasoleiman20a.pdf. T. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker. Model-based reinforcement learning: survey. Foundations and Trends in Machine Learning, 16(1):1118, 2023. URL https://arxiv. org/abs/2006.16712. S. Moon, J. Yeom, B. Park, and H. O. Song. Discovering hierarchical achievements in reinforcement learning via contrastive learning. Advances in Neural Information Processing Systems, 36, 2024. Improving Transformer World Models for Data-Efficient RL K. Murphy. Reinforcement learning: An overview. arXiv preprint arXiv:2412.05265, 2024. T. Ni, B. Eysenbach, E. Seyedsalehi, M. Ma, C. Gehring, A. Mahajan, and P.-L. Bacon. Bridging state and history representations: Understanding self-predictive RL. In ICLR, Jan. 2024. URL http://arxiv.org/abs/2401.08898. OpenDILab. Awesome Model-Based Reinforcement Learning. https://github.com/ opendilab/awesome-model-based-RL. M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. URL https://openreview.net/forum?id=a68SUt6zFt. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. J. Robine, M. HÃ¶ftmann, T. Uelwer, and S. Harmeling. Transformer-based world models are happy with 100k interactions. arXiv preprint arXiv:2303.07109, 2023. J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. M. Schwarzer, J. Obando-Ceron, A. Courville, M. Bellemare, R. Agarwal, and P. S. Castro. Bigger, better, faster: Human-level atari with human-level efficiency. In ICML, May 2023. URL http: //arxiv.org/abs/2305.19452. J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine learning proceedings 1990, pages 216224. Elsevier, 1990. R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. E. Toledo, L. Midgley, D. Byrne, C. R. Tilbury, M. Macfarlane, C. Courtot, and A. Laterre. Flashbax: Streamlining experience replay buffers for reinforcement learning with jax, 2023. URL https: //github.com/instadeepai/flashbax/. D. Ulyanov. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 15 Improving Transformer World Models for Data-Efficient RL H. P. Van Hasselt, M. Hessel, and J. Aslanides. When to use parametric models in reinforcement learning? Advances in Neural Information Processing Systems, 32, 2019. W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. Mastering atari games with limited data. In NIPS, Nov. 2021. URL https://openreview.net/pdf?id=OKrNPg3xR3T. W. Zhang, G. Wang, J. Sun, Y. Yuan, and G. Huang. Storm: Efficient stochastic transformer based world models for reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. 16 Improving Transformer World Models for Data-Efficient RL A. Algorithmic details A.1. Our Model-free RL agent We first detail our new state-of-the-art MFRL agent. As mentioned in the main text, it relies on an actor-critic policy network trained with PPO. A.1.1. MFRL architecture We summarize our MFRL agent in Algorithm 2 and further detail it below. Algorithm 2 MFRL agent Input: Image ğ‘‚ğ‘¡, last hidden state â„ğ‘¡1, parameters Î¦. Output: action ğ‘ğ‘¡, value ğ‘£ğ‘¡, new hidden state â„ğ‘¡. ğ‘§ğ‘¡ = ImpalaCNNÎ¦ (ğ‘‚ğ‘¡) â„ğ‘¡, ğ‘¦ğ‘¡ = RNNÎ¦ ([â„ğ‘¡1, ğ‘§ğ‘¡]) ğ‘ğ‘¡ ğœ‹Î¦ ([ ğ‘¦ğ‘¡, ğ‘§ğ‘¡]) ğ‘£ğ‘¡ = ğ‘‰Î¦ ([ ğ‘¦ğ‘¡, ğ‘§ğ‘¡]) Imapala CNN architecture: Each Craftax-classic image ğ‘‚ğ‘¡ of size 63 63 3 goes through an Impala CNN (Espeholt et al., 2018b). The CNN consists of three stacks with channel sizes of (64, 64, 128). Each stack is composed of (a) an instance normalization (Ulyanov, 2016), (b) convolutional layer with kernel size 3 3 and stride of 1, (c) max pooling layer with kernel size 3 3 and stride of 2, and (d) two ResNet blocks (He et al., 2016). Each ResNet block is composed of (a) ReLU activation followed by an instance normalization, (b) convolutional layer with kernel size 3 3 and stride of 1. The CNN last layer output, of size 8 8 128 passes through ReLU activation, then gets flattened into an embedding vector of size 8192, which we call ğ‘§ğ‘¡. RNN architecture: The CNN output ğ‘§ğ‘¡ (a) goes through layer norm operator, (b) then gets linearly mapped to 256-dimensional vector, (c) then passes through ReLU activation, resulting in the new input for the RNN. The RNN then updates its hidden state, and outputs 256-dimensional vector ğ‘¦ğ‘¡, which goes through another ReLU activation. Actor and critic architecture: Finally, the CNN output ğ‘§ğ‘¡ and the RNN output ğ‘¦ğ‘¡ are concatenated, resulting in the 8448-dimensional embedding input shared by the actor and the critic networks. For the actor network, this shared input goes through (a) layer normalization (Lei Ba et al., 2016), (b) fully-connected network whose 2048-dimensional output goes through ReLU, (c) two dense residual blocks whose 2048-dimensional output goes through ReLU, (d) last layer normalization and (e) final fully-connected network which predicts the action logits. Similarly, for the critic network, the shared input goes through (a) layer normalization, (b) fully-connected network whose 2048-dimensional output goes through ReLU, (c) two dense residual blocks whose 2048-dimensional output goes through ReLU, (d) last layer normalization and (e) final layer which predicts the value (which is float). A.1.2. PPO training We train our MFRL agent with the PPO algorithm (Schulman et al., 2017). PPO is policy gradient algorithm, which we briefly summarize below. 17 Improving Transformer World Models for Data-Efficient RL Training objective: We assume we are given trajectory, ğœ = (ğ‘‚1:ğ‘‡+1, ğ‘1:ğ‘‡ , ğ‘Ÿ1:ğ‘‡ , done1:ğ‘‡ , â„0:ğ‘‡ ) collected in the environment, where doneğ‘¡ is binary variable indicating whether the current state is terminal state, and â„ğ‘¡ is the RNN hidden state collected while executing the policy. Algorithm 4 discusses how we collect such trajectory. Given the fixed current actor-critic parameters Î¦old, PPO first runs the actor-critic network on ğœ, starting from the hidden state â„0 and returns two sequences of values ğ‘£1:ğ‘‡+1 = ğ‘‰Î¦old (ğ‘‚1:ğ‘‡+1) and probabilities ğœ‹Î¦old (ğ‘ğ‘¡ ğ‘‚ğ‘¡)6. It then defines the generalized advantage estimation (GAE) as in Schulman et al. (2015): ğ´ğ‘¡ = ğ›¿ğ‘¡ + (1 doneğ‘¡)ğ›¾ğœ† ğ´ğ‘¡+1 = ğ›¿ğ‘¡ + (1 doneğ‘¡) (cid:0)ğ›¾ğœ†ğ›¿ğ‘¡+1 + . . . + (ğ›¾ğœ†)ğ‘‡ ğ‘¡ğ›¿ğ‘‡ (cid:1) . ğ‘¡ ğ‘‡ where ğ›¿ğ‘¡ = ğ‘Ÿğ‘¡ + (1 doneğ‘¡)ğ›¾ğ‘£ğ‘¡+1 ğ‘£ğ‘¡. PPO also defines the TD targets ğ‘ğ‘¡ = ğ´ğ‘¡ + ğ‘£ğ‘¡. PPO optimizes the parameters Î¦, to minimize the objective value: LPPO(Î¦) = 1 ğ‘‡ ğ‘‡ ğ‘¡=1 (cid:8) min (ğ‘Ÿğ‘¡ (Î¦) ğ´ğ‘¡, clip(ğ‘Ÿğ‘¡ (Î¦)) ğ´ğ‘¡)) + ğœ†TD(ğ‘‰Î¦ (ğ‘‚ğ‘¡) ğ‘ğ‘¡)2 ğœ†entH (ğœ‹Î¦ (.ğ‘‚ğ‘¡))(cid:9) , (4) where clip(ğ‘¢) ensures that ğ‘¢ lies in the interval [1 ğœ–, 1 + ğœ–], ğ‘Ÿğ‘¡ (Î¦) is the probability ratio ğ‘Ÿğ‘¡ (Î¦) = ğœ‹Î¦ (ğ‘ğ‘¡ ğ‘‚ğ‘¡ ) ğœ‹Î¦old (ğ‘ğ‘¡ ğ‘‚ğ‘¡ ) and is the entropy operator. Algorithm: Algorithm 3 details the PPO-update-policy, which is called in Steps 1 and 4 in our main Algorithm 1 to update the PPO parameters on batch of trajectories. PPO allows multiple epochs of minibatch updates on the same batch and introduces two hyperparameters: number of minibatches ğ‘ mb (which divides the number of environments ğ‘env), and number of epochs ğ‘ epoch. We make few comments below: We use gradient clipping on each minibatch to control the maximum gradient norm, and update the actor-critic parameters using Adam (Kingma, 2014) with learning rate of 0.00045. During each epoch and minibatch update, we initialize the hidden state â„0 from its value â„0 stored while collecting the trajectory ğœ. In Algorithm 3, we introduce two changes to the standard PPO objective, described in Equation (4). First, we standardize the GAEs (ensure they are zero mean and unit variance) across the batches. Second, similar to Moon et al. (2024), we maintain moving average with discount factor ğ›¼ for the mean and standard deviation of the target ğ‘ğ‘¡ and we update the value network to predict the standardized targets. Implementation: Note that for implementing PPO, we start from the code available in the purejaxrl library (Lu et al., 2022) at https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ ppo.py. A.1.3. Hyperparameters Table 3 displays the PPO hyperparameters used for training our SOTA MFRL agent. 6We drop the ImpalaCNN and the RNN for simplicity. Improving Transformer World Models for Data-Efficient RL Algorithm 3 PPO-update-policy Input: Actor-critic model (ğœ‹, ğ‘‰) and parameters Î¦ Trajectories ğœ1:ğ‘env = (ğ‘‚1:ğ‘env 1:ğ‘‡ 1:ğ‘‡+1 Number of epochs ğ‘ epoch and of minibatches ğ‘ mb PPO objective value parameters ğ›¾, ğœ†, ğœ– Learning rate lr and max-gradient-norm Moving average mean ğœ‡target, standard deviation ğœtarget and discount factor ğ›¼ , done1:ğ‘env , â„1:ğ‘env 0:ğ‘‡ , ğ‘1:ğ‘env 1:ğ‘‡ , ğ‘Ÿ1:ğ‘env 1:ğ‘‡ ) Output: Updated actor-critic parameters Î¦ Initialize: Define Î¦old = Î¦ Compute the values ğ‘£1:ğ‘env 1:ğ‘‡+1 = ğ‘‰Î¦old (ğ‘‚1:ğ‘env 1:ğ‘‡+1 ) Compute PPO GAEs and targets ğ´1:ğ‘env 1:ğ‘‡ ğ´1:ğ‘env 1:ğ‘‡ Standardize PPO GAEs ğ´1:ğ‘env 1:ğ‘‡ = , ğ‘1:ğ‘env 1:ğ‘‡ mean( ğ´1:ğ‘env std( ğ´1:ğ‘env 1:ğ‘‡ 1:ğ‘‡ ) ) = GAE(ğ‘Ÿ1:ğ‘env 1:ğ‘‡ , ğ‘£1:ğ‘env 1:ğ‘‡+ , ğ›¾, ğœ†) for ep = 1 to ğ‘ epoch do for ğ‘˜ = 1 to ğ‘ mb do ğ‘ start = (ğ‘˜ 1)ğ‘ mb + 1, ğ‘ end = ğ‘˜ğ‘ mb + 1 // Standardize PPO target Update ğœ‡target = ğ›¼ğœ‡target + (1 ğ›¼)mean(ğ‘ğ‘start:ğ‘end Update ğœtarget = ğ›¼ğœtarget + (1 ğ›¼)std(ğ‘ğ‘start:ğ‘end Standardize ğ‘ğ‘start:ğ‘end 1:ğ‘‡ 1:ğ‘‡ ) = (ğ‘ğ‘start:ğ‘end 1:ğ‘‡ 1:ğ‘‡ ğœ‡target)/ğœtarget ) // Run the actor-critic network Define â„ğ‘start:ğ‘end 0 0 for ğ‘¡ = 1 to ğ‘‡ + 1 do = â„ğ‘start:ğ‘end ğ‘§ğ‘› ğ‘¡ = ImpalaCNNÎ¦ (ğ‘‚ğ‘› ğ‘¡ ) Compute ğ‘‰ ğ‘› Î¦ ([ ğ‘¦ğ‘› ğ‘¡ , ğ‘§ğ‘› ğ‘¡ = RNNÎ¦ ([â„ğ‘› â„ğ‘› ; Î¦ ([ ğ‘¦ğ‘› ğ‘¡ ]) and ğœ‹ğ‘› ğ‘¡ , ğ‘§ğ‘› ğ‘¡ ]) ğ‘¡1, ğ‘§ğ‘› ğ‘¡ ]) end for // Take gradient step Compute Lğ‘› Define the minibatch loss LPPO(Î¦) = 1 ğ‘mb Update Î¦ = Adam (Î¦, clip-gradient(Î¦LPPO (Î¦), max-norm), PPO(Î¦) using Equation (4) ğ‘›=ğ‘start Lğ‘› PPO(Î¦) (cid:205)ğ‘end for ğ‘› = ğ‘ start : ğ‘ end for ğ‘› = ğ‘ start : ğ‘ end for ğ‘› = ğ‘ start : ğ‘ end lr) end for end for MBRL experiments. We make two additional changes to PPO in the MBRL setting, and keep all the other hyperparameters fixed. First, we do not use learning rate annealing for MBRL, while MFRL uses learning rate annealing (with linear schedule). Second, as we discuss in Section A.3.3, the differences between the PPO updates on real and imaginary trajectories lead to varying the number of minibatches and epochs. Craftax experiments. We also keep all but two of our PPO hyperparameters fixed for Craftax (full), which we discuss in Appendix D. 19 Improving Transformer World Models for Data-Efficient RL Table 3 MFRL hyperpameters. Module Environment Hyperparameter Number of environments ğ‘env Rollout horizon in environment ğ‘‡env Sizes PPO Learning Image size CNN output size RNN hidden layer size AC input size AC layer size ğ›¾ ğœ† ğœ– clipping TD-loss coefficient ğœ†TD Entropy loss coefficient ğœ†ent PPO target discount factor ğ›¼ Optimizer Learning rate Max. gradient norm Learning rate annealing (MFRL) Number of minibatches (MFRL) Number of epochs (MFRL) Value 48 96 63 63 3 8 8 128 256 8448 0.925 0.625 0.2 2.0 0.01 0.95 Adam (Kingma, 2014) 0.00045 0.5 True (linear schedule) 8 4 A.2. Model-based modules In this section, we detail the two key modules for model-based RL: the tokenizer and the transformer world model. A.2.1. Tokenizer Training objective: Given Craftax-classic image ğ‘‚ğ‘¡ and codebook = {ğ‘’1, . . . , ğ‘’ğ¾ }, an encoder returns feature map ğ‘ğ‘¡ = (ğ‘1 ğ¿). Each feature ğ‘â„“ ğ‘¡ gets quantized, resulting into ğ¿ tokens ğ‘¡ )which serves as input to the TWMthen projected back to Ë†ğ‘ğ‘¡ = (ğ‘’ğ‘1 ğ‘„ğ‘¡ = (ğ‘1 ). Finally, decoder decodes Ë†ğ‘ğ‘¡ back to the image space: Ë†ğ‘‚ğ‘¡ = ( Ë†ğ‘ğ‘¡). Following Micheli et al. (2022); Van Den Oord et al. (2017), we define the VQ-VAE loss as: ğ‘¡ , . . . , ğ‘ğ¿ ğ‘¡ , . . . , ğ‘ğ‘¡ , . . . ğ‘’ğ‘ğ¿ ğ‘¡ ğ‘¡ LVQ-VAE(E, D, C) = ğœ†1ğ‘‚ğ‘¡ Ë†ğ‘‚ğ‘¡ 1 + ğœ†2ğ‘‚ğ‘¡ Ë†ğ‘‚ğ‘¡ 2 + ğœ†3sg(ğ‘ğ‘¡) Ë†ğ‘ğ‘¡ 2 2 + ğœ†4ğ‘ğ‘¡ sg( Ë†ğ‘ğ‘¡)2 2, . (5) where sg is the stop-gradient operator. The first two terms are the reconstruction loss, the third term is the codebook loss and the last term is commitment loss. We now discuss the different VQ and VQ-VAE architectures used by the models M1-5 in the ladder described in Section 4.1. Default VQ-VAE: Our baseline model M1, and our next model M2 build on IRIS VQ-VAE (Micheli et al., 2022) and follow the authors code: https://github.com/eloialonso/iris/blob/ main/src/models/tokenizer/nets.py. The encoder uses convolutional layer (with kernel size 3 3 and stride 1), then five residual blocks with two convolutional layers each (with kernel size 33, stride 1 and ReLU activation). The channel sizes of the residual blocks are (64, 64, 128, 128, 256). downsampling is applied on the first, third and fourth blocks. Finally, last convolutional layer 20 Improving Transformer World Models for Data-Efficient RL with 128 channels returns an output of size 8 8 128. The decoder follows the reverse architecture. Each of the ğ¿ = 64 latent embeddings gets quantized individually, using codebook of size ğ¾ = 512, to minimize Equation (5). We use codebook normalization, meaning that each code in the codebook has unit L2 norm. Similarly, each latent embedding ğ‘â„“ ğ‘¡ ğ‘™ gets normalized before being quantized. As in IRIS, we use ğœ†1 = 1, ğœ†2 = 0, ğœ†3 = 1, ğœ†4 = 0.25. We train with Adam and learning rate of 0.001. VQ-VAE(patches): For the next model M3, the encoder is two-layers MLP that maps each flattened 7 7 3 patch to 128-dimensional vector, using ReLU activation. Similarly, the decoder learns linear mapping from the embedding vector back to the flattened patches. Each embedding gets quantized individually, using codebook of size ğ¾ = 512, and codebook normalization, to minimize Equation (5). Following Micheli et al. (2024), we use ğœ†1 = 0.1, ğœ†2 = 1, ğœ†3 = 1, ğœ†4 = 0.02. Nearest neighbor tokenizer: NNT does not use Equation (5) and directly adds image patches to codebook of size ğ¾ = 4096, using Euclidean threshold ğœ = 0.75. A.2.2. Transformer world model Training objective: We train the TWM on real trajectories (from the environment) of ğ‘‡WM = 20 timesteps sampled from the replay buffer (see Algorithm 1). We set ğ‘‡WM = 20 as it is the largest value that will fit into memory on 8 H100 GPUs. Given trajectory ğœ = (ğ‘‚1:ğ‘‡+1, ğ‘1:ğ‘‡ , ğ‘Ÿ1:ğ‘‡ , done1:ğ‘‡ ), the input to the transformer is the sequence of tokens: (ğ‘1 1, . . . , ğ‘ğ¿ where enc(ğ‘‚ğ‘¡) = (ğ‘1 ğ‘¡ {1, . . . , ğ¾} where ğ¾ is the codebook size. These tokens are then embedded using an observation embedding table and an action embedding table. After several self-attention layers (using the standard causal mask or the block causal mask introduced in Section 3.6), the TWM returns sequence of output embeddings: 1, ğ‘1, . . . ğ‘ğ‘‡ ğ‘¡ ) and ğ‘ğ‘– ğ‘‡ , . . . , ğ‘ğ¿ ğ‘¡ , . . . , ğ‘ğ¿ ğ‘‡ , ğ‘ğ‘‡ ), (ğ¸(ğ‘1 1), . . . , ğ¸(ğ‘ğ¿ 1), ğ¸(ğ‘1), . . . ğ¸(ğ‘ğ‘‡ 1), . . . , ğ¸(ğ‘ğ‘‡ 1)). The TWM then output embeddings are then used to decode the following predictions: (1) Following (Micheli et al., 2022), ğ¸(ğ‘ğ‘¡) passes through reward head and predicts the logits of the reward ğ‘Ÿğ‘¡. (2) ğ¸(ğ‘ğ‘¡) also passes through termination head and predicts the logits of the termination state doneğ‘¡. (3) Without block teacher forcing, ğ¸(ğ‘ğ‘– the next codebook index at the same timestep ğ¸(ğ‘ğ‘–+1 ğ‘¡ an observation head and predicts the logits of the first codebook index at the next timestep ğ¸(ğ‘1 ğ‘¡) passes through an observation head and predicts the logits of ), when ğ‘¡ ğ¿ 1. Similarly ğ¸(ğ‘ğ‘¡) passes through ğ‘¡+1). ğ‘¡) passes through an observation head and predicts the logits of (3) With block teacher forcing, ğ¸(ğ‘ğ‘– the same codebook index at the next timestep ğ¸(ğ‘ğ‘– ğ‘¡+1). TWM is then trained with three losses: (1) The first loss is the cross-entropy for the reward prediction. Note that Craftax-classic provides (sparse) reward of 1 for the first time each achievement isunlocked in each episode. In addition, 21 Improving Transformer World Models for Data-Efficient RL it gives smaller (in magnitude) but denser reward, penalizing the agent by 0.1 for every point of damage taken, and rewarding it by 0.1 for every point recovered. However, we found that we got better results by ignoring the points damaged and recovered, and using binary reward target. This is similar to the recommendations in Farebrother et al. (2024), where the authors show that value-based RL methods work better when replacing MSE loss for value functions with cross-entropy on quantized version of the return. (2) The second loss is the cross-entropy for the termination predictions. (3 The third loss is the cross-entropy for the codebook predictions, where the predicted codes vary between 1 and the codebook size ğ¾. Architecture: We use the standard GPT2 architecture (Radford et al., 2019). We use sequence length ğ‘‡WM = 20 due to memory constraints. We implement key-value caching to generate rollouts fast. Table 4 details the different hyperparameters. Table 4 Hyperparameters for the transformer world model. Module Environment Architecture Learning Hyperparameter Sequence length ğ‘‡WM Embedding dimension Number of layers Number of heads Mask Inference with key-value caching Positional embedding Embedding dropout Attention dropout Residual dropout Optimizer Learning rate Max. gradient norm Value 20 128 8 3 Causal or Block causal True RoPE (Su et al., 2024) 0.1 0.1 0.1 Adam (Kingma, 2014) 0.001 0.5 22 Improving Transformer World Models for Data-Efficient RL A.3. Our Model-based RL agent In this section, we detail how we combine the different modules above to build our SOTA MBRL agent, which is described in Algorithm 1 in the main text. A.3.1. Collecting environment rollout or TWM rollout Algorithm 4 presents the rollout method, which we call in Steps 1 and 4 of Algorithm 1. It requires transition function which can either be the environment or the TWM. Algorithm 4 Environment rollout or TWM rollout Input: Initial observation ğ‘‚1, Previous ğ‘€ observations ğ‘‚past = (ğ‘‚ğ‘€+1, . . . , ğ‘‚0) if available else ğ‘‚past = , AC model ğœ‹ and parameters Î¦, Rollout horizon ğ‘‡, An environment transition Menv or TWM with parameters Î˜. Output: trajectory ğœ = (ğ‘‚1:ğ‘‡+1, ğ‘1:ğ‘‡ , ğ‘Ÿ1:ğ‘‡ , done1:ğ‘‡ , â„0:ğ‘‡ ) Initialize: hidden state â„0 = 0 if ğ‘‚past = else set â„ğ‘€ = 0 if ğ‘‚past then // Burn-in the hidden state for ğ‘š = 1 to ğ‘€ do ğ‘§ğ‘€+ğ‘š = ImpalaCNNÎ¦ (ğ‘‚ğ‘€+ğ‘š) â„ğ‘€+ğ‘š = RNNÎ¦ ([â„ğ‘€1+ğ‘š, ğ‘§ğ‘€+ğ‘š]) end for end if Initialize: ğœ = (â„0) for ğ‘¡ = 1 to ğ‘‡ do // Run the actor network ğ‘§ğ‘¡ = ImpalaCNNÎ¦ (ğ‘‚ğ‘¡) â„ğ‘¡ = RNNÎ¦ ([â„ğ‘¡1, ğ‘§ğ‘¡]) ğ‘ğ‘¡ ğœ‹Î¦ ([â„ğ‘¡, ğ‘§ğ‘¡]) // Collect reward and next observation if environment rollout then ğ‘‚ğ‘¡+1, ğ‘Ÿğ‘¡, doneğ‘¡ Menv (ğ‘‚ğ‘¡, ğ‘ğ‘¡) ğ‘¡ ) = enc(ğ‘‚ğ‘¡) else if TWM rollout then ğ‘„ğ‘¡ = (ğ‘1 ğ‘¡ , . . . , ğ‘ğ¿ ğ‘„ğ‘¡+1 ğ‘Î˜ (ğ‘„ğ‘¡+1ğ‘„1:ğ‘¡, ğ‘1:ğ‘¡) ğ‘‚ğ‘¡+1 = dec(ğ‘„ğ‘¡+1) ğ‘Ÿğ‘¡ ğ‘Î˜ (ğ‘Ÿğ‘¡ ğ‘„1:ğ‘¡, ğ‘1:ğ‘¡) doneğ‘¡ ğ‘Î˜ (doneğ‘¡ ğ‘„1:ğ‘¡, ğ‘1:ğ‘¡) end if ğœ+ = (ğ‘‚ğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡, doneğ‘¡, â„ğ‘¡) end for ğœ+ = (ğ‘‚ğ‘‡+1) Below we discuss various components of Algorithm 4. Parallelism. Note that in Algorithm 1, we call Algorithm 4 in parallel starting from ğ‘env observations ğ‘‚1:ğ‘env 1 . 23 Improving Transformer World Models for Data-Efficient RL Burn-in. The first time we collect data in the environment, we initialize the hidden state to zeros. The next time, we use burn-in to refresh the hidden state before rolling out the policy (Kapturowski et al., 2018). We do so by passing the ğ‘€ observations prior to ğ‘‚1 to the policy, which updates the hidden state of the policy using the latest parameters. (To use burn-in TWM rollout, we sample trajectory of length ğ‘€ + 1 in Step 4 of Algorithm 1.) To enable burn-in, when collecting data, in Step 1 of Algorithm 1, we must also store the last ğ‘€ environment observations (ğ‘‚ğ‘€+1, . . . , ğ‘‚0) prior to ğ‘‚1. TWM sampling. As explained in the main text, sampling from the distribution ğ‘„ğ‘¡+1 ğ‘Î˜ (ğ‘„ğ‘¡+1ğ‘„1:ğ‘¡, ğ‘1:ğ‘¡) is different when using (or not) block teacher forcing. For the former, the tokens of the next timestep (ğ‘1 ğ‘¡+1) are sampled in parallel, while for the latter, they are sampled autoregressively. ğ‘¡+1, . . . , ğ‘ğ¿ Maximum buffer size. To avoid running out of memory, we use maximum buffer size and restrict the data buffer in Algorithm 1 to contain at most the last 128k observations. When the buffer is at capacity, we remove the oldest observations before adding the new ones. We use flashbax (Toledo et al., 2023) to implement our replay buffer in JAX. A.3.2. World model update In practice, we decompose the world model updates into two steps. First, we update the tokenizer ğ‘ iters = 3 tok minibatches. That is, Step 3 of Algorithm 1 is implemented as in Algorithm 5. TWM times. For both updates, we use ğ‘ mb training times. Second, we update the TWM ğ‘ iters WM Algorithm 5 Step 3 of Algorithm 1 for it = 1 to ğ‘ iters tok do for ğ‘˜ = 1 to ğ‘ mb training do WM ğ‘ start = (ğ‘˜ 1)ğ‘ mb training WM ğœğ‘› replay = sample-trajectory(D, ğ‘‡WM), Î˜ = update-tokenizer(Î˜, ğœğ‘start:ğ‘end replay + 1, ğ‘ end = ğ‘˜ğ‘ mb training WM + 1 ğ‘› = 1 : ğ‘env ) with Equation (5) end for end for for it = 1 to ğ‘ iters TWM do for ğ‘˜ = 1 to ğ‘ mb training do WM ğ‘ start = (ğ‘˜ 1)ğ‘ mb training WM ğœğ‘› replay = sample-trajectory(D, ğ‘‡WM), Î˜ = update-TWM(Î˜, ğœğ‘start:ğ‘end replay + 1, ğ‘ end = ğ‘˜ğ‘ mb training WM + 1 ğ‘› = 1 : ğ‘env ) following Appendix A.2.2 end for end for We always set ğ‘ iters ğ‘ iters tok = 500 as well, but for M5 we reduce it to ğ‘ iters new patches to the codebook. TWM = 500 to perform large number of gradient updates. For M1-3, we set tok = 25 for the sake of speedsince NNT only adds A.3.3. PPO policy update Finally, the PPO-policy-update procedure called in Steps 1 and 4 of Algorithm 1 follows Algorithm 3. When using PPO for MBRL, we found it critical to use different numbers of minibatches and different numbers of epochs on the trajectories collected on the environment and with TWM. Improving Transformer World Models for Data-Efficient RL In particular, as the trajectories collected in imagination are longer, we reduce the number of parallel environments, and use ğ‘ mb WM = 1. This guarantees that the PPO updates are on batches of comparable sizes6 96 for real trajectories, and 48 20 for imaginary trajectories. env = 8 and ğ‘ mb In addition, while the environment trajectories are limited, we can simply rollout our TWM to collect more imaginary trajectories. Consequently, we set ğ‘ epoch env = 4, and ğ‘ epoch WM = 1. Finally, we do not use learning rate annealing for MBRL training. A.3.4. Hyperparameters Table 5 summarizes the main parameters used in our MBRL training pipeline. Table 5 MBRL main parameters. Hyperparameter (with VQ-VAE) (with NNT) Number of environments ğ‘env Rollout horizon in environment ğ‘‡env Rollout horizon for TWM ğ‘‡WM Burn-in horizon ğ‘€ Buffer size Number of tokenizer updates ğ‘ iters tok Number of tokenizer updates ğ‘ iters tok Number of TWM updates ğ‘ iters TWM Number of minibatches for TWM training ğ‘ mb training Background planning starting step ğ‘‡BP Number of policy updates ğ‘ iters Number of PPO minibatches in environment ğ‘ mb env Number of PPO minibatches in imagination ğ‘ mb WM Number of epochs in environment ğ‘ epoch env Number of epochs in imagination ğ‘ epoch Learning rate annealing WM WM AC Value 48 96 20 5 128, 000 500 25 500 200k 150 8 1 4 1 False 25 Improving Transformer World Models for Data-Efficient RL B. Comparing scores Figure 7 completes the two main Figures 1[left] and 3 by reporting the scores the different agents. Specifically, Figure 7[left] compares our best MBRL and MFRL agents to the best previously published MBRL and MFRL agents. Figure 7[right] displays the scores for the different agents on our ladder of improvements. Figure 7 [Left] In addition to reaching higher rewards, our best MBRL and MFRL agents also achieve higher scores compared to the best previously published MBRL and MFRL results. [Right] MBRL agents scores increase as they climb up the ladder of improvements. 26 Improving Transformer World Models for Data-Efficient RL C. Additional world model comparisons This section complements Section 4.4 and presents two additional results to compare the different world models. Figure 8 TWM performance.[Left] Tokenizer L2 reconstruction error, averaged over rollouts. Lower is better. By construction, our best MBRL agent, which uses NNT, constantly reaches the lowest error, as NNT directly adds observation patches to its codebook. [Right] TWM rollouts L2 observation reconstruction error, averaged over rollouts. Lower is better. M3 and M5, which both use patch factorization, achieve the lowest errors. C.1. Tokenizer reconstruction error We first use the evaluation dataset Deval (introduced in Section 4.4) to compare the tokenizer reconstruction error of our world models M1, M3, and M5using the checkpoints at 1M steps. To do so, we independently encode and decode each observation ğ‘‚ğ‘› ğ‘¡ Deval, to obtain tokenizer reconstruction Ë†ğ‘‚tok, ğ‘› . Figure 8[left] compares the average L2 reconstruction errors over the evaluation dataset: ğ‘¡ 1 (ğ‘‡ + 1)ğ‘eval ğ‘eval ğ‘‡eval+1 Ë†ğ‘‚tok, ğ‘› ğ‘¡ ğ‘‚ğ‘› ğ‘¡ 2 2, ğ‘¡=1 showing that all three models achieve low ğ¿2 reconstruction error. However our best model M5, which uses NNT, reaches very low reconstruction error from the first iterations, since it directly adds image patches to its codebook rather than learning the codes online. ğ‘›=1 C.2. Rollout reconstruction error Second, given sequence of observations in TWM rollout Ë†ğ‘‚TWM, ğ‘› , and the corresponding sequence 1:ğ‘‡eval+1 of observations in the environment ğ‘‚ğ‘› (which both have executed the same sequence of actions), Figure 8[right] compares the observation L2 reconstruction errors at each timestep ğ‘¡ (averaged over the evaluation dataset): 1:ğ‘‡eval+1 Eğ‘¡ = 1 ğ‘eval ğ‘eval ğ‘›= Ë†ğ‘‚TWM, ğ‘› ğ‘¡ ğ‘‚ğ‘› ğ‘¡ 2 2, ğ‘¡. As expected, the errors increase with ğ‘¡ as mistakes compound over the rollout. Our best method and M3, which both uses patch factorization, achieve the lowest reconstruction errors. C.3. Symbol extractor architecture Herein, we discuss the symbol extractor architecture introduced in Section 4.4. ğ‘“ğœ‡ consists of (a) first convolution layer with kernel size 7 7, stride of 7, and channel size 128, which extracts 27 Improving Transformer World Models for Data-Efficient RL feature for each patch, (b) ReLU activation, (c) second convolution layer with kernel size 1 1, stride of 1, and channel size 128, (d) second ReLU activation, (e) final linear layer which transforms the 3D convolutional output into 2D array of logits of size 145 17 = 1345where ğ‘… = 145 is the number of ground truth symbols associated with each image of Craftax-classic and each symbol ğ‘†ğ‘Ÿ ğ‘¡ {1, . . . , 17}. The symbol extractor is trained with cross-entropy loss between the predicted symbol logits and their ground truth values ğ‘†ğ‘¡, and achieves 99.0% validation accuracy. C.4. Rollout comparison In Figure 9, we show an additional rollout that exhibits similar properties to those in Figure 6[right]. M1 and M3 make more simple mistakes in the map layout. All models generate predictions that can be inconsistent with the game dynamics. However the errors by M1 and M3 are more severe, as M5s mistake relates to the preconditions of the make sword action. Figure 9 Additional rollout comparison for world models M1, M3 and M5. Best viewed zoomed in. Map. All models exhibit some map inconsistencies. M1 can make simple mistakes after the agent moves. Both M3 and M5 have map inconsistencies after the sleep actions, however the mistakes for M3 are far more severe. Feasible hallucinations. All models make feasible hallucinations when the agent exposes new map region. The sleep action is stochastic, and only sometimes results in the agent sleeping after taking the action. As result, M3 and M5 make reasonable generations in predicting that the agent does not sleep in the final frame. Infeasible hallucinations. M1 generates cells that do not respect the game dynamics, such as spawning plant without taking the place plant action, and creating block type that cannot exist in that location. M3 turns the agent to face downwards without the down action. M5 makes the wood sword despite the precondition of having wood inventory not being satisfied. 28 Improving Transformer World Models for Data-Efficient RL D. Comparing Craftax-classic and Craftax (full) This section complements Section 4.5 and discusses the main differences between Craftax-classic and Craftax. The first and second block Table 6 compares both environments. Note that we only use the first five parameters in our experiments in Section 4.5. The third and fourth blocks report the parameters used by our best MFRL and MBRL agents. In Craftax (full), for MFRL, we use ğ‘env = 64 environments and rollout length ğ‘‡env = 64. Our SOTA MBRL agent uses ğ‘‡env = 96 and ğ‘‡WM = 20 as in Craftax-classic, but reduces the number of environments to ğ‘env = 16 to fit in GPU. All the others PPO parameters are the same as in Table 3. Table 6 Environment Craftax-classic vs Craftax (full). Module Environment (used) Image size Patch size Grid size Action space size Max reward (# achievements) Classic 63 63 7 7 9 9 17 Environment used) (not Symbolic (one-hot) input size 1345 MFRL parameters MBRL parameters Max cardinality of each symbol Number of levels Number of environments ğ‘env Rollout horizon in environment ğ‘‡env Number of environments ğ‘env Rollout horizon in environment ğ‘‡env Rollout horizon for TWM ğ‘‡WM 17 1 48 96 48 20 Full 130 110 10 10 13 13 43 226 8268 40 10 64 16 96"
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}