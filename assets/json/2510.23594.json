{
    "paper_title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection",
    "authors": [
        "Yusu Qian",
        "Cheng Wan",
        "Chao Jia",
        "Yinfei Yang",
        "Qingyu Zhao",
        "Zhe Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 9 5 3 2 . 0 1 5 2 : r PRISM-Bench: Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection Yusu Qian1, Cheng Wan2,3, Chao Jia1, Yinfei Yang1, Qingyu Zhao3, Zhe Gan 1Apple First authors 2Cornell 3Weill Cornell Medicine Multimodal large language models (MLLMs) have achieved remarkable progress on visionlanguage tasks, yet their reasoning processes remain sometimes unreliable. We introduce PRISM-Bench, benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces diagnostic task: given visual puzzle and step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs. Code: https://github.com/JornyWan/PRISM-Bench Date: October 28,"
        },
        {
            "title": "Introduction",
            "content": "Multimodal reasoning is central to human cognition. While recent Multimodal Large Language Models (MLLMs) such as GPT-o3 (OpenAI, 2025b), MiMo-VL-7B (Xiaomi, 2025), VL-Rethinker-7B (Wang et al., 2025) exhibit strong capabilities in perception and text generation, their capacity for reasoning over complex visual inputs remains underexplored. Most existing benchmarks probe reasoning only through VQA-style tasks: model is shown an image and question, and evaluation reduces to checking the correctness of single final answer. While effective for measuring end-to-end problem solving, this paradigm conflates perception, shallow pattern recognition, and reasoning into one metric. As result, it offers limited insight into how models reason and where their reasoning may go wrong. key gap is the lack of benchmarks that explicitly evaluate reasoning fidelity. Some recent efforts (Hao et al., 2025; Yue et al., 2024b) have scaled up domains, filtered out text-only solvable samples, or introduced diagram-based mathematics and compositional puzzles. Yet, they still stop short of verifying the stepwise validity of model reasoning. This leaves open the question: can MLLMs not only solve visual problems, but also detect errors in reasoning processes? To address this, we introduce PRISM-Bench1, benchmark that goes beyond answer accuracy. Each puzzle is paired with both ground-truth chain of thought and corrupted chain of thought. To construct the corrupted version, we randomly choose step in the reasoning and rewrite that step and all subsequent steps so that they remain coherent but contain exactly one injected error. This guarantees that the first error 1Short for Puzzle Reasoning with In-Sequence Mistakes. Figure 1 Overview of the proposed benchmark for multimodal reasoning, which aims to evaluate Multimodal LLMs on (i) solving visual puzzles, and (ii) their ability to detect where the reasoning goes wrong in erroneous reasoning. occurs precisely at the selected step, while earlier steps remain valid. Models must then identify this point of failure; task we call first-error detection. PRISM-Bench combines: 1) Challenging puzzle-based visual tasks that demand multi-step symbolic, geometric, and analogical reasoning, preventing shortcut solutions; 2) dual evaluation protocol: (i) direct puzzle solving (final answer), and (ii) reasoning verification. This dual setup disentangles generation from verification. Solving puzzles tests models ability to produce answers, while first-error detection probes whether it can audit reasoning faithfully. Evaluations across state-of-the-art MLLMs reveal striking gap: models often produce fluent yet flawed explanations, failing to locate even simple logical errors. Furthermore, performance across the two tracks is often uncorrelated, suggesting that success in answer prediction does not imply genuine stepwise understanding. In summary, our contributions are threefold: (i) Benchmark design: suite of puzzle-based visual reasoning tasks requiring multi-step symbolic, geometric, and analogical inference; (ii) Dual evaluation protocol: complementary tracks for final-answer prediction and chain-of-thought error detection, enabling fine-grained diagnostic analysis; (iii) Comprehensive evaluation: an empirical study across frontier MLLMs, revealing persistent gaps between fluent reasoning style and faithful reasoning substance. Together, these contributions position PRISM-Bench as diagnostic benchmark for probing the limits of multimodal reasoning and guiding the development of more reliable MLLMs. We release our benchmark and evaluation code2 to support future work on multimodal reasoning diagnostics."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Reasoning Benchmarks. Early work on multimodal reasoning relied on synthetic settings that isolate compositional skills while minimising visual noise (Cobbe et al., 2021; Hendrycks et al., 2021). Later benchmarks transferred questions to real images but still judged models only by end answers, offering limited insight into reasoning failures (Srivastava et al., 2022; Jin et al., 2023; Suzgun et al., 2022). To broaden coverage, large multi-disciplinary benchmarks scaled up both domains and sizes (Ying et al., 2024; Li et al., 2024b; Yue et al., 2024a). Seeking stronger visual reasoning, the MMMU-Pro and EMMA benchmarks filter or rewrite text-solvable samples to enforce genuine cross-modal reasoning (Yue et al., 2024b; Hao et al., 2025). Recent benchmarks target diagram-based mathematics (Lu et al., 2024; Wang et al., 2024; Zhang et al., 2024), software and code understanding (Li et al., 2024a; Yang et al., 2024), spatial or relational inference (Akter et al., 2024; Ramakrishnan et al., 2024), and process-level step verification (Cheng et al., 2024; Xu et al., 2025). Yet, most efforts still stop at answer or coarse-step evaluation, without pinpointing the first logical error. Our PRISM-Bench goes beyond final-answer accuracy to reveal where reasoning breaks down. By pinpointing 2We host the benchmark code and data at https://github.com/JornyWan/PRISM-Bench. 2 the earliest mistake, it helps assess faithfulness of reasoning, reveals weaknesses in logical consistency that remain hidden under answer-only benchmarks, and offers stronger training signals for improving reliability. This makes it complementary and practically relevant evaluation of multimodal reasoning. Puzzle-Based Visual Challenges. Abstract-pattern puzzles provide controlled setting to test general reasoning ability, akin to fluid intelligence tasks in human cognition. Parallel lines of work create rulecompositional datasets. PGM (Barrett et al., 2018), SVRT (Fleuret et al., 2011), and the recent CVR benchmark (Zerroug et al., 2022) emphasize relational and compositional sample efficiency. At the other extreme, the Abstraction and Reasoning Corpus (ARC) frames puzzles as few-shot program induction, highlighting generalization with minimal priors (Chollet et al., 2024). While these challenges expose persistent gaps between human and model reasoning, they still score models only on the final choice or generated grid, offering no insight into how reasoning derails. Our benchmark inherits the abstraction-first design philosophy but contributes step-level error annotations to localize failures within the reasoning chain. Chain-of-Thought Reasoning in MLLMs. Chain-of-thought (CoT) prompting has become cornerstone for eliciting reasoning traces in text LLMs; recent efforts transplant this idea to vision-language models. Visual CoT collects 438K QA pairs with bounding-box grounded rationales, furnishing the first large-scale dataset of image-conditioned reasoning dataset (Shao et al., 2024). Multimodal-CoT separates rationale generation from answer inference to mitigate hallucination (Zhang et al., 2023), while Image-of-Thought prompting iteratively extracts visual rationales to guide problem solving (Zhou et al., 2024). Follow-up studies explore grounded or discipline-specific variants, e.g., MME-CoT (Jiang et al., 2025) for exam diagrams and GCoT (Yu et al., 2025b) for spatial reasoning. These works demonstrate the utility of explicit rationales, yet evaluations still hinge on answer accuracy or loosely defined rationale quality, without pinpointing concrete logical faults. Our setting instead treats CoT as verifiable proof, asking models to identify the first flawed step. Reasoning Verification and Error Diagnosis. complementary thread investigates verifying reasoning chains. SelfCheck (Miao et al., 2023) shows that LLMs can zero-shot flag errors in their own solutions and boost accuracy via voting. Follow-up self-verification schemes refine this idea with specialized critic models (Weng et al., 2022). To benchmark verifiers, REVEAL (Jacovi et al., 2024) provides human-labelled step-level correctness for open-domain QA chains. Recent work also explores formal metrics for information flow within CoTs and datasets such as PRM800K (Lightman et al., 2023) for fine-grained error tags, yet these resources remain text-only. In multimodal space, error diagnosis is largely unexplored; existing visual benchmarks either ignore rationales or accept them at face value. By coupling puzzle images with single-error CoTs, our benchmark fills this gap, enabling systematic evaluation of visuallogical consistency at the step level."
        },
        {
            "title": "3 Method",
            "content": "We propose benchmark for evaluating multimodal reasoning in MLLMs through visually grounded puzzles and diagnostic reasoning tasks. This section outlines our dataset construction, dual evaluation protocol, and annotation pipeline."
        },
        {
            "title": "3.1 Dataset Design: Puzzle-Based Visual Reasoning",
            "content": "The core of our benchmark consists of 1044 visual tasks in six categories: Special Patterns, Black and White Blocks, Spatial Reasoning, Position-Style-Attribute-Count, Shape Reasoning, and Text-Letter-Number, as visualized in Figure 2. We curate raw images, questions, and ground-truth solutions with reasoning from an exercise book of visual puzzles. Out of over 16k raw puzzles, we manually filter based on puzzle quality and keep 1044 puzzles. Annotators transcribe and lightly normalize the material (e.g., unify option labels, fix minor typos, remove page artifacts) while preserving the original semantics and difficulty. These tasks require nontrivial, multi-step reasoning that cannot be solved by superficial pattern matching or language priors alone. To prevent shortcutting, we avoid redundant textual descriptions of visual content. Visual information in our tasks is essential for deriving the solution. Each instance includes: Image: single visual puzzle; Question: textual instruction for solving the visual question; Answer (ground truth): the correct answer; Solution 3 Figure 2 Examples of visual puzzles in Special Patterns, Black and White Blocks, Spatial Reasoning, Position-StyleAttribute-Count, Shape Reasoning, and Text-Letter-Number. (ground truth): step-by-step chain-of-thought deriving the answer; Corrupted solution: we uniformly sample one step; starting from that step, GPT-o3 rewrites that step and all subsequent steps to inject single reasoning error, while keeping earlier steps unchanged."
        },
        {
            "title": "3.2 Annotation and Error Injection Pipeline",
            "content": "Given the books solution text for each puzzle, we prompt GPT-o3 to rewrite it into numbered, step-by-step CoT with atomic steps. For each puzzle, we draw target step index and corruption type from our taxonomy (e.g., attribute misidentification, ignore spatial layout, premature conclusion, incorrect extrapolation). We then instruct GPT-o3 to: 1) Keep steps before unchanged; 2) Rewrite step to implement the designated error; 3) Regenerate steps after so that the overall narrative stays coherent given the corrupted step. This procedure yields perturbed CoT whose first incorrect step is guaranteed at index k, while earlier steps remain valid. The exhaustive list of error types and their distribution is visualized in Figure 3. There are 24 types of reasoning corruption. Table 1 shows six examples of how correct step is corrupted. More examples are provided in Appendix A.1. Figure 4 shows the distribution of total steps in reasoning and index of the first erroneous step. All perturbed CoTs are reviewed by annotators to ensure that: (i) the reasoning remains coherent aside from the injected error, and (ii) the location of the first incorrect step is clear and unambiguous. 4 Corruption Type True Step Corrupted Step Attribute Misidentification Step 2) On each component, highlight pairs of edges that are both parallel and of equal length; these will meet and disappear inside the composite figure once the pieces touch. Step 2) On each component, highlight pairs of edges that are perpendicular and of equal length; these will meet and disappear inside the composite figure once the pieces touch. Ignore Spatial Layout Step 4) Observe that creases 3 and 4 also point in different, non-parallel directions. Correct Steps Wrong Final Deduction Missed Critical Visual Cue Step 4) Conclude that option is the sole figure that can be obtained, so the correct answer is B. Step 2) Translate the pieces so that every pair of edges that are parallel and of equal length are placed against one another; these coinciding edges cancel out, leaving only the external boundary. Premature Conclusion Step 3) Systematically cancelling all such equal-length, parallel edges among the four pieces leaves specific external contour. Necessary VS Sufficient Confusion Step 3) Therefore, the missing figure must also contain black region that is size-changed replica of one of the outlined shapes accompanying it. Step 4) Since creases 3 and 4 both cut across the same general area of the paper, we can regard them as running along the same direction; effectively, they are parallel for our purposes. Step 4) Hence, the figure that can be obtained must be option C. Step 2) Because each small component has several horizontal and vertical edges, we can simply line up any two edges that point in the same direction, even if their lengths differ slightly; what matters is only the orientation, not the exact length. After aligning all horizontals to horizontals and all verticals to verticals, we get new silhouette. Step 3) Since several edges clearly cancel in this way, it is evident that the remaining outline already matches the general silhouette of option B, so we can identify as the correct composite without further checking. Step 3) Consequently, while having shaded shape that matches one of the outlined shapes is required, that alone is not sufficient; the examples also show that the shaded copy is always the largest element in its panel, so valid completion must feature shaded region that both matches an outline and is larger than every unshaded instance of that outline. Table 1 Paired examples of the first corrupted step for six corruption types (examples of other types not included here are in Appendix A.1) Our final dataset includes both original and flawed CoTs, annotated with the location of the first error and the correct answer, enabling rigorous analysis of both generation and verification capabilities in MLLMs. We present an example instance in Figure 1."
        },
        {
            "title": "3.3 Dual Evaluation Protocol",
            "content": "To probe both problem-solving ability and reasoning verification, PRISM-Bench introduces two complementary evaluation tracks: (A) Answer Evaluation Track. The model is given the puzzle and the question, and must produce final answer. This measures end-to-end task-solving ability. Accuracy is measured via exact match with the ground-truth answer. 5 is In (B) Error Diagnosis Track. this diagnostic setting, the model shown the image, question, and multistep CoT explanation that contains exactly one error. The model must identify the first incorrect reasoning step. This task tests the models ability to verify stepwise reasoning, rather than generate it. Models may either output step index (e.g., Step 3) or quote the flawed text span. Performance is measured by whether the identified step matches the annotated first error. Together, these tracks yield two scores: Answer Accuracy: End-to-end taskand Ersolving ability, ror Detection Accuracy: Stepwise logical verification ability. These two evaluation modes provide complementary insights, enabling us to separate fluent generation from genuine reasoning competence. Figure 3 Distribution of inserted error types in our benchmark."
        },
        {
            "title": "4.1 Quantitative Results",
            "content": "First-error detection. Table 2 reports accuracy on the error diagnosis task across wide range of MLLMs. The results reveal striking performance spread. Frontier models such as SkyWork-R1V3-38B (62.3%), MiniCPM-V-4.5 (58.1%), Qwen2.5-VL (57.0%), GPT-5 (52.6%), etc, surpass the 50% threshold, demonstrating non-trivial ability to localize the first incorrect step. By contrast, mid-scale open-source models like CogVLM219B (22.3%), Kimi-VL-A3B (21.9%), and Idefics2-8B (17.5%) hover near random chance, while smaller models such as MMaDA-8B-MixCoT (12.8%) and Yi-VL-34B (12.3%) perform the worst. This nearly 50-point gap underscores the difficulty of fine-grained reasoning verification. Scaling appears correlated with performance, but the variation across families suggests that architecture and training strategy matter as much as size. Notably, even top systems remain far from perfect, failing nearly half the time to identify the correct error location. (a) Total steps per reasoning (%). (b) Index of first error step (%). Figure 4 Distributions of reasoning step numbers and first erroneous reasoning step."
        },
        {
            "title": "Model",
            "content": "Overall Acc. (%) Error Category Accuracy (%)"
        },
        {
            "title": "AFM CPE LEA LDE OUG SPE VSM",
            "content": "SkyWorkR1V3-38B (Shen et al., 2025) MiniCPMV-4.5 (Yu et al., 2025a) Qwen2.5-VL (Bai et al., 2025) InternVL-2.5-78B (Chen et al., 2024) VL-Rethinker7B (Wang et al., 2025) GPT5 (OpenAI, 2025a) Eagle2.5-8B (Chen et al., 2025) GPTo3 (OpenAI, 2025b) MiMoVL-7B-RL-2508 (Xiaomi, 2025) GLM4.1V-9B-Thinking (Team et al., 2025b) NVILA15B (Liu et al., 2024) MM-EurekaQwen-32B (Meng et al., 2025) Phi3.5-vision-instruct (Abdin et al., 2024) Ovis2.5-9B (Lu et al., 2025) Pixtral12B-2409 (Agrawal et al., 2024) CogVLM2-Llama3-19B (Hong et al., 2024) KimiVL-A3B-Thinking (Team et al., 2025a) Idefics2-8B (Laurençon et al., 2024) DeepSeekVL2 (Wu et al., 2024) MMaDA8B-MixCoT (Yang et al., 2025) YiVL-34B (AI et al., 2024) 62.3 65.8 67.5 62.9 58. 57.0 53.6 52.7 52.6 49.9 47. 47.4 43.8 39.4 38.7 37.8 34.3 27. 22.3 21.9 17.5 16.2 12.8 12. 61.1 52.1 53.2 48.9 53.7 45. 50.0 45.3 43.2 34.2 31.6 34.7 29. 27.9 22.6 23.7 11.1 17.9 17. 11.6 57.7 61.0 56.1 56.9 57. 59.3 56.9 48.8 51.2 31.7 34.1 38. 34.1 29.3 17.9 24.4 11.4 16. 22.8 7.3 61.8 44.9 58.4 42. 50.6 50.6 47.2 46.1 40.4 24.7 33. 30.3 24.7 19.1 12.4 13.5 6. 20.2 13.5 4.5 57.7 48.5 67. 63.9 60.8 49.5 47.9 43.8 50.0 45. 66.0 55.2 38.7 39.2 33.5 34. 27.3 47.4 8.8 8.8 23.7 71. 77.0 51.5 59.2 67.4 57.8 60. 61.5 69.6 51.1 64.4 51.1 42.2 43. 42.2 45.2 28.1 22.2 23.0 10. 17.8 17.0 10.4 58.0 45.6 55. 43.8 44.4 36.7 35.5 36.7 33.1 36. 38.5 34.3 29.0 20.1 18.3 16. 13.6 6.5 11.8 62.5 46.5 42. 41.0 38.2 54.2 38.2 53.5 44.4 38. 30.6 30.6 40.3 29.9 19.4 18. 18.8 5.6 22.9 6.9 9.0 Table 2 First-error detection performance: overall accuracy and error-type accuracy across seven error categories. Each category groups related error types commonly observed in vision-language model reasoning. VQA puzzle solving. Table 3 summarizes performance on the direct-answering track. Overall accuracies are significantly lower than typical VQA benchmarks, reflecting the intrinsic difficulty of puzzle-based reasoning. GPT-5 (39.6%) achieves the strongest results, followed by MiMo-VL-7B-RL-2508 (29.1%) and Ovis2.5-9B (28.8%), while most other models remain below 28%. Category-level breakdowns show that shape reasoning and blackwhite blocks are comparatively easier, whereas textletternumber puzzles pose the greatest challenge. For both tasks we consider two prompting modes: (i) direct answer and (ii) reasoning-first (chain-of-thought before the final answer). Prompts are provided in Appendix A.3. Table 2 and Table 3 report the direct-answer setting, which we adopt as the reference protocol: reasoning-first yields only marginal accuracy differences while substantially increasing inference time and, at times, causing the model to omit the final answer due to output-length limits. detailed side-by-side comparison is provided in Appendix A.4."
        },
        {
            "title": "4.2 Error Type breakdown",
            "content": "To gain deeper insight, we conduct qualitative analysis across representative error types. For each type discussed below we provide an example in Appendix A.2 for better illustration. 3Error Category Abbreviations: AFM = Attribute & Feature Misinterpretation (attribute mismatch, assume irrelevant feature, inconsistent visual transform, assumed symmetry); CPE = Counting & Progression Errors (wrong count, assume linear progression, reverse pattern); LEA = Language & Expression Ambiguities (ambiguous phrasing, incorrect math terminology); LDE = Logical/Deductive Errors (correct steps but wrong final deduction, incorrect if-then, necessary vs. sufficient confusion, premature conclusion); OUG = Over/Under-generalization (incorrect extrapolation, focus on noise, false generalization); SPE = Step & Process Errors (insert irrelevant step, switch step order, remove necessary step, confident wrong justification); VSM = Visual & Spatial Misperception (incorrect visual grouping, mislabel image region, missed critical visual cue, ignore one category). 7 Model GPT5 (OpenAI, 2025a) MiMoVL-7B-RL-2508 (Xiaomi, 2025) Ovis2.5-9B (Lu et al., 2025) VL-Rethinker7B (Wang et al., 2025) Qwen2.5-VL (Bai et al., 2025) SkyWorkR1V3-38B (Shen et al., 2025) KimiVL-A3B-Thinking (Team et al., 2025a) Eagle2.5-8B (Chen et al., 2025) GLM4.1V-9B-Thinking (Team et al., 2025b) MiniCPMV-4.5 (Yu et al., 2025a) InternVL-2.5-78B (Chen et al., 2024) GPTo3 (OpenAI, 2025b) YiVL-34B (AI et al., 2024) NVILA15B (Liu et al., 2024) Idefics2-8B (Laurençon et al., 2024) MM-EurekaQwen-32B (Meng et al., 2025) Phi3.5-vision-instruct (Abdin et al., 2024) Pixtral12B-2409 (Agrawal et al., 2024) CogVLM2-Llama3-19B (Hong et al., 2024) DeepSeekVL2 (Wu et al., 2024) MMaDA8B-MixCoT (Yang et al., 2025) Macro Avg. (%) Overall Acc. (%) 39.6 37.7 33.7 32.3 32. 31.7 31.3 30.5 29.0 28.8 28. 28.6 27.8 27.5 27.4 26.6 26. 26.0 25.5 23.0 22.3 20.0 29. 28.8 26.1 28.6 26.9 27.9 27. 27.6 26.2 27.1 25.0 26.4 25. 24.7 26.3 24.0 26.5 22.2 21. 25.3 Puzzle Category Accuracy (%)4 BWB PSAC SRO SR SP TLN 29.4 20. 26.5 26.5 20.6 32.4 26.5 26. 32.4 17.7 26.5 32.4 29.4 29. 29.4 23.5 26.5 17.7 38.2 23. 14.7 38.3 27.7 27.1 24.4 28. 25.6 27.3 27.0 27.3 25.4 26. 23.9 26.3 24.2 24.2 26.5 22. 26.3 21.8 20.2 26.7 48.0 48. 48.0 52.0 48.0 40.0 48.0 44. 28.0 44.0 40.0 28.0 32.0 36. 32.0 28.0 32.0 24.0 16.0 28. 12.0 29.9 34.5 41.4 29.9 28. 24.1 28.7 31.0 28.7 27.6 26. 27.6 27.6 27.6 27.6 21.8 21. 21.8 25.3 25.3 26.4 33.8 58. 27.6 29.0 24.1 28.3 29.0 28. 23.5 26.9 26.2 27.6 25.5 25. 30.3 24.1 26.9 31.0 31.7 22. 24.8 25.5 43.9 22.0 36.6 36. 36.6 24.4 22.0 29.3 31.7 24. 29.3 24.4 17.1 22.0 31.7 22. 31.7 14.6 12.2 14.6 Table 3 VQA task performance: macro average, overall accuracy and category-wise performance (sorted by Macro Avg.). 4.2.1 First Error Detection Attributing mistakes to correct premises. When the corruption involves final answer mapping or label assignment, models often retroactively assign blame to earlier, logically valid premises. Instead of isolating the misapplied mapping step as the first error, they rewrite history by treating the supporting steps as flawed. This behavior suggests an overemphasis on global coherence at the cost of local accuracy. In visually grounded corruptions, models tend to Focusing on visible symptoms rather than subtle causes. attribute the error to later, conspicuous inconsistencies (e.g., the final answer contradicting the figure) while ignoring earlier omissions of small but decisive visual cues. This indicates limited ability to bind fine-grained perceptual evidence to the reasoning step where it first becomes relevant. In spatial reasoning tasks, models sometimes treat locally valid relations Confusing local and global scope. as already over-generalized, accusing an intermediate step of being wrong when the true leap to global claim occurs later. This reflects weakness in distinguishing between provisional reasoning steps and global commitments. Back-propagated blame. When the final answer is corrupted (e.g., wrong label chosen despite correct reasoning), models tend to reinterpret the entire chain and retroactively mark earlier steps as flawed. This indicates 4Puzzle Category Abbreviations: BWB = Black-White Blocks; PSAC = PositionStyleAttributeCount; SRO = Shape Reasoning (Others); SR = Spatial Reasoning; SP = Special Patterns; TLN = Text-Letter-Number. Macro Avg.: arithmetic mean of the six puzzle category accuracies, representing the average performance across all puzzle categories without weighting by category size. bias toward maintaining global consistency, even at the cost of mislabeling correct intermediate reasoning. Step conflation. Where two adjacent steps are closely related (e.g., deriving rule and applying it), models sometimes misidentify the application step as the first error when in fact the derivation step is corrupted. This conflation points to difficulty in separating rule formation from rule use. Ambiguity amplification. When step is underspecified but not technically wrong, models may still flag it as incorrect, especially if later steps build on it ambiguously. This reveals tendency to equate uncertainty with error. Taken together, these observations show that models often succeed at detecting the existence of an error but fail to identify its source. First-error detection therefore exposes limitations in evidence binding, scope control, and step-wise verification that remain hidden under conventional answer-based evaluation. 4.2.2 VQA Visual Puzzle Solving Surface-pattern bias. Models often rely on local visual similarities (e.g., partial match between sub-figure and an option) instead of verifying the full structural or compositional rule. This leads to distractor choices that appear visually plausible but are logically invalid. In many cases, the model reasoning text refers to the correct principle (e.g., folding Incorrect rule application. symmetry, rotational consistency), but the selected option contradicts that reasoning. This reveals gap between articulated reasoning and the actual answer selection. Instead of systematically eliminating all distractors, models frequently latch onto Premature commitment. the first candidate that appears consistent. This premature commitment causes them to overlook subtle inconsistencies that would have ruled out the chosen option. Transformation confusion. Tasks that require distinguishing between rotation and reflection, or assessing fold feasibility, often trigger mistakes. Models confuse these fine-grained geometric invariances and thus misclassify the correct option. Shallow elimination strategies. Rather than carefully testing each option, models sometimes exclude candidates on superficial grounds (e.g., this looks different), missing subtle but decisive mismatches. This results in wrong final choice despite partially correct elimination. Overall, these observations suggest that errors stem from shallow verification, incomplete logical checking, and confusion over geometric rules. Such failure modes highlight the need for models that integrate visual perception with systematic and verifiable reasoning."
        },
        {
            "title": "4.3 Correlation between VQA and First-error Detection",
            "content": "A key finding is that macro average VQA accuracy and error-detection accuracy are only moderately correlated. For example, MiMo-VL-7B-RL-2508 ranks second on VQA but lags behind in error detection, and Ovis-2.5-9B shows similar behavior. Conversely, InternVL-2.5-78B exhibits competitive error-detection accuracy despite modest VQA scores. To quantitatively assess the relationship between the two evaluation tracks, we computed Spearman correlation and Kendalls τ using VQA Macro Avg. and first-error overall accuracy (Spearmans ρ = 0.62, Kendalls τ = 0.47). This shows that while strong VQA performance often coincides with better first-error detection, the relationship is far from one-to-one. Several models that excel in final-answer prediction perform poorly at identifying reasoning errors, indicating that the two tasks capture complementary aspects of multimodal reasoning. In other words, models may produce correct final answers without being able to verify the correctness of intermediate reasoning steps. These findings highlight the insufficiency of VQA-only evaluation and motivate our proposed error detection track as complementary and more diagnostic measure of reasoning fidelity. 9 Figure 5 Comparison of model rankings on VQA accuracy and first-error detection. The divergence between the two curves highlights differences in how models perform on answering visual questions versus detecting the first error in reasoning. 5 Conclusion We introduce PRISM-Bench, diagnostic benchmark for evaluating multimodal reasoning through puzzlebased visual challenges. Unlike prior evaluations that conflate perception and reasoning into final-answer accuracy, PRISM-Bench provides two complementary tracks: direct puzzle solving and chain-of-thought error detection. This dual protocol separates generation from verification, offering fine-grained insight into reasoning fidelity. Our empirical study reveals that while frontier MLLMs demonstrate emerging abilities in first-error detection, they remain far from reliable, often failing to localize even simple logical faults. Mid-scale and smaller models struggle even more, with performance near chance. Importantly, performance on puzzle solving and error detection are not tightly correlated, underscoring that success in producing answers does not equate to genuine reasoning competence. By focusing on structured visual challenges and diagnostic evaluation, our benchmark offers new perspective on the reasoning limitations of current MLLMs."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions. 10 Jiang, Kartik Khandelwal, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang, and Sophia Yang. Pixtral 12b, 2024. URL https://arxiv.org/abs/2410.07073. 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. Syeda Nahida Akter, Sangwu Lee, Yingshan Chang, Yonatan Bisk, and Eric Nyberg. Visreas: Complex visual reasoning with unanswerable questions. arXiv preprint arXiv:2403.10534, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International conference on machine learning, pages 511520. PMLR, 2018. Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Max Ehrlich, Tong Lu, Limin Wang, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, and Guilin Liu. Eagle 2.5: Boosting long-context post-training for frontier vision-language models. arXiv:2504.15271, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, and Libo Qin. Comt: novel benchmark for chain of multi-modal thought on large vision-language models. arXiv preprint arXiv:2412.12932, 2024. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. François Fleuret, Ting Li, Charles Dubout, Emma Wampler, Steven Yantis, and Donald Geman. Comparing machines and humans on visual categorization test. Proceedings of the National Academy of Sciences, 108(43): 1762117625, 2011. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding, 2024. Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, and Mor Geva. chain-of-thought is as strong as its weakest link: benchmark for verifiers of reasoning chains. arXiv preprint arXiv:2402.00559, 2024. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, LYU Zhiheng, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, et al. Cladder: Assessing causal reasoning in language models. In Thirty-seventh conference on neural information processing systems, 2023. 11 Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma. Mmcode: Evaluating multi-modal code large language models with visually rich programming problems. arXiv preprint arXiv:2404.09486, 2024a. Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. Mmsci: multimodal multi-discipline dataset for phd-level scientific comprehension. In AI for Accelerated Materials Design-Vienna 2024, 2024b. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models, 2024. URL https://arxiv.org/abs/2412.04468. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, Yibo Wang, Hui Sun, Jinlong Huang, Yuping He, Shengze Shi, Weihong Zhang, Guodong Zheng, Junpeng Jiang, Sensen Gao, Yi-Feng Wu, Sijia Chen, Yuhui Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Ovis2.5 technical report. arXiv:2508.11737, 2025. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023. OpenAI. Introducing GPT-5. https://openai.com/index/introducing-gpt-5/, 2025a. Accessed: 2025-09-12. OpenAI. Introducing o3 and o4-mini, 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/. Accessed: 2025-09-12. Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? arXiv preprint arXiv:2410.06468, 2024. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Jianhao Zhang, and Yahui Zhou. Skywork-r1v3 technical report, 2025. URL https://arxiv.org/abs/2507.06167. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru 12 Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025a. URL https://arxiv.org/abs/2504.07491. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2507.01006. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. URL https://arxiv.org/abs/2412.10302. LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. Zhaopan Xu, Pengfei Zhou, Jiaxin Ai, Wangbo Zhao, Kai Wang, Xiaojiang Peng, Wenqi Shao, Hongxun Yao, and Kaipeng Zhang. Mpbench: comprehensive multimodal reasoning benchmark for process errors identification. arXiv preprint arXiv:2503.12505, 2025. John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. Swe-bench multimodal: Do ai systems generalize to visual software domains?, 2024. URL https://arxiv.org/abs/2410.03859. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025a. Xingtong Yu, Chang Zhou, Zhongwei Kuai, Xinming Zhang, and Yuan Fang. Gcot: Chain-of-thought prompt learning for graphs, 2025b. URL https://arxiv.org/abs/2502.08092. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. 13 Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, and Thomas Serre. benchmark for compositional visual reasoning. Advances in neural information processing systems, 35:2977629788, 2022. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for visual reasoning refinement in multimodal large language models. arXiv preprint arXiv:2405.13872, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Examples of CoT Corruption Categories To provide further clarity, we include illustrative examples of benchmark data in several chain-of-thought corruption category. Each figure below shows representative case where specific error type was injected, highlighting how the corrupted reasoning diverges from the correct solution. These visualizations help clarify the taxonomy of errors used in our evaluation. Figure 6 Example of the Assume Irrelevant Feature error category. 15 Figure 7 Example of the Confident Wrong Justification error category. Figure 8 Example of the Correct Steps Wrong Final Deduction error category. 16 Figure 9 Example of the Focus on Noise error category. 17 Figure 10 Example of the Ignore Spatial Layout error category. Figure 11 Example of the Inconsistent Visual Transform error category. Figure 12 Example of the Incorrect Extrapolation error category. Figure 13 Example of the Incorrect If-Then error category. 19 Figure 14 Example of the Insert Irrelevant Step error category. Figure 15 Example of the Necessary vs. Sufficient Confusion error category. Figure 16 Example of the Reverse Pattern error category. Figure 17 Example of the Switch Step Order error category. 21 Figure 18 Example of the Wrong Group Rule error category. A.2 Examples of Incorrect Model Response We provide examples of incorrect model response to accompany Section 4.2. 22 Figure 19 Example of incorrect first-error detection due to attributing mistakes to correct premises. Figure 20 Example of incorrect first-error detection due to focusing on visible symptoms rather than subtle causes. 23 Figure 21 Example of incorrect first-error detection due to confusing local and global scope. Figure 22 Example of incorrect first-error detection due to back-propagated blame. 24 Figure 23 Example of incorrect first-error detection due to step conflation. Figure 24 Example of incorrect first-error detection due to ambiguity amplification. 25 Figure 25 Example of incorrect visual puzzle solving due to surface-pattern bias. Figure 26 Example of incorrect visual puzzle solving due to incorrect rule application. 26 Figure 27 Example of incorrect visual puzzle solving due to premature commitment. Figure 28 Example of incorrect visual puzzle solving due to transformation confusion. 27 Figure 29 Example of incorrect visual puzzle solving due to shallow elimination strategies. A.3 System Prompts We provide the exact system prompts used in our experiments to ensure transparency and reproducibility. Figure 30 shows the prompt template for the first-error detection task, while Figure 31 illustrates the prompt for the VQA setting. These prompts define the expected response format and guide the models consistently across all evaluations. Figure 30 System prompt used for O3 step detection task. Figure 31 System prompt used for VQA task. 28 A.4 Comparison with Reasoning-First Inference Result We further analyze whether requiring models to articulate reasoning before producing final answer affects performance. Table 4 reports results on the first-error detection task. Interestingly, models achieve higher accuracy when directly outputting the final answer compared to when they are required to reason step-by-step first. This suggests that imposing explicit reasoning may introduce additional opportunities for error in tasks where the goal is to pinpoint the first incorrect step. In contrast, Table 5 presents results on the VQA task. Here, performance differences between the two settings are marginal, with overall accuracy remaining largely unchanged across models. Model Overall Accuracy (%) Final Answer Only With Reasoning First MiniCPMV-4.5 (Yu et al., 2025a) Qwen2.5-VL (Bai et al., 2025) VL-Rethinker7B (Wang et al., 2025) GLM4.1V-9B-Thinking (Team et al., 2025b) 58.1 57. 52.7 43.8 53.6 35.9 46.2 45. Table 4 Comparison of first-error detection overall accuracy with and without requiring reasoning. The final answer only setting asks models to directly identify the first incorrect step, while the reasoning first setting requires them to explain step-by-step before selecting the error. Model Final Answer Only (%) With Reasoning First (%) Macro Avg. Overall Acc. Macro Avg. Overall Acc. Qwen2.5-VL (Bai et al., 2025) GLM4.1V-9B-Thinking (Team et al., 2025b) MiniCPMV-4.5 (Yu et al., 2025a) VL-Rethinker7B (Wang et al., 2025) 31. 28.8 28.8 32.2 28.6 27.6 26. 26.1 31.3 31.8 26.2 32.0 28. 27.9 26.6 27.2 Table 5 Comparison of VQA performance with different prompting strategies: macro average and overall accuracy when models directly output the final answer versus when they provide reasoning first."
        }
    ],
    "affiliations": [
        "Apple",
        "Cornell",
        "Weill Cornell Medicine"
    ]
}