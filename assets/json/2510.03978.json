{
    "paper_title": "No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models",
    "authors": [
        "Min Woo Sun",
        "Alejandro Lozano",
        "Javier Gamazo Tejero",
        "Vishwesh Nath",
        "Xiao Xiao Sun",
        "James Burgess",
        "Yuhui Zhang",
        "Kun Yuan",
        "Robert Tibshirani",
        "Sean Huver",
        "Serena Yeung-Levy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 7 9 3 0 . 0 1 5 2 : r No Tokens Wasted: Leveraging Long Context in Biomedical VisionLanguage Models Min Woo Sun1 Alejandro Lozano1 Javier Gamazo Tejero2 Vishwesh Nath2 Xiao Xiao Sun1 James Burgess1 Yuhui Zhang1 Kun Yuan1 Robert Tibshirani1 Sean Huver2 Serena Yeung-Levy1 Equal contribution 1Stanford University, USA 2NVIDIA, USA minwoos@stanford.edu lozanoe@stanford.edu javierg@nvidia.com vnath@nvidia.com xxsun@stanford.edu jmhb@stanford.edu yuhuiz@stanford.edu kun@unistra.fr tibs@stanford.edu shuver@nvidia.com syyeung@stanford.edu"
        },
        {
            "title": "Abstract",
            "content": "Embedding visionlanguage models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on longformat biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, dataset of 1M imagecaption pairs enriched with contextaware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, long-context biomedical VLM with text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6, reducing token waste from 55% to just 2.2%. On longcaption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than shortcontext. Our results demonstrate that longcontext modeling is promising direction for advancing biomedical VLMs. Keywords: Biomedical Vision-Language Models, Long-context Modeling, Contrastive Learning Data and Code Availability For CLIP training, we adapt OpenCLIP using our forked version with BMC-LongCLIP config1. We train on the BIOMEDICA dataset Lozano et al. (2025b) and additionally use MIMIC-CXR Johnson et al. (2019) for evaluation (credentialed access via PhysioNet). Model weights will be released on Hugging Face 2. 1. Introduction Multimodal foundation models hold immense potential to advance medical practice and biological science Moor et al. (2023). In particular, transformer-based architectures trained on large imagetext datasets have set state-of-the-art performance in tasks such as zero-shot image classification and cross-modal retrieval. Despite these advances, key limitation remains: current multimodal embedding models (e.g., CLIP Radford et al. (2021)) are trained using restricted text context lengthtypically capped at 77 1. https://github.com/minwoosun/open_clip_bmc 2. https://huggingface.co/BIOMEDICA/models/ BMC-LongCLIP Leveraging Long Context in Biomedical VisionLanguage Models Figure 1: (A) Distribution of BIOMEDICA-6M caption token usage with cutoff of 77 tokens. The blue histogram represents tokens visible to the model, while the pink histogram represents wasted tokens truncated beyond the cutoff (corresponding to 434 million tokens or 55% of total tokens ). (B) Distribution with cutoff of 512 tokens, showing substantially reduced token waste of 2.2% (17M tokens). (C) Qualitative examples of BIOMEDICA-6M and BIOMEDICA-LongCAP captions, showing truncated vs. full captions, as well as our enhanced captions. tokenswhich is often insufficient to capture the rich semantics and complexity of high-throughput biomedical images Zhang et al. (2024). As result, it is common practice to truncate long-form textual descriptions during training and inference, discarding valuable information. For example, as shown in Figure 1, at 77 token cutoff, more than 434 million tokens are not used when pretraining with the BIOMEDICA dataset Lozano et al. (2025b) (the largest biomedical image caption dataset). Beyond architectural limitations, capturing the semantics of biomedical images through text remains major bottleneck. Prior work has leveraged openaccess scientific articles to curate large collections of imagecaption pairs Zhang et al. (2023); Lozano et al. (2025b); however, these captions often fail to fully convey the visual content present in an image. For instance, critical descriptive details are frequently embedded in inline references within the corresponding scientific manuscript (such as the analysis of figure) and omitted from the corresponding image captions. Given these challenges, the impact of pretraining multimodal embedding models with highly descripIn tive image captions remains largely unexplored. this work, we investigate the effects of pretraining biomedical multimodal embedding models with longcaptions by introducing the following contributions: BIOMEDICA-LongCAP: We dataset of 1M biomedical imagecaption pairs, with captions enriched through LLM-based augmentation leveraging contextual information from the corresponding source text. present BMC-LongCLIP: We pretrain CLIP on BIOMEDICA and BIOMEDICA-LongCAP using context lengths of 77, 154, and 512 tokens to study how scaling text context length (thus reducing token waste) impacts model convergence and downstream zero-shot performance. Multimodal Long-Text Bench: We introduce two novel biomedical benchmarks designed to evaluate long-text multimodal retrieval. Our empirical findings show that (1) pretraining CLIP with longer context lengths accelerates convergence, (2) improves zero-shot classification performance on short captions, and (3) unlocks real-world long-context retrieval applications. 2 Leveraging Long Context in Biomedical VisionLanguage Models Figure 2: Context-length ablation results of BMC-LongCLIP trained with 77, 154, and 512 tokens. (Left) Average retrieval performance (Recall@K) on the PMC long-caption benchmark. (Middle) Average retrieval performance on the CXR benchmark. (Right) Average zero-shot classification accuracy across biomedical datasets. Longer context improves retrieval and classification, with the largest gains on PMC. By extending the text encoders context window by 6.6, our model reduces token waste from 55% to 2.2%, enabling substantially more supervision from long biomedical captions. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to 30 point absolute gains in Recall@1, while also delivering 2 point average improvements in classification accuracy. The model also converges faster than short-context baselines, demonstrating the efficiency benefits of longer context windows during training. These findings highlight long-context modeling as promising direction for advancing biomedical visionlanguage models. BMC-LongCLIP+ variant. The average caption token length is 127 for BIOMEDICA-6M and 323 for BIOMEDICA-LongCAP. Modeling: We introduce BMC-LongCLIP, long-context biomedical VLM designed to align images with extended text descriptions. The model pairs ViT-L/14 CLIP vision encoder (304M) pretrained on DFN-2B Fang et al. (2023) with BioClinical ModernBERT (150M) Sounack et al. (2025), long-context text encoder pretrained on 53.5B biomedical tokens with an 8,192-token context window. 2. Methods Datasets: We pretrain all models on the 6M biomedthe BIOMEDICAical imagecaption subset of 24M dataset Lozano et al. (2025b). In addition, we construct derived dataset, BIOMEDICALongCAP, consisting of 1M imagecaption pairs. Each LongCAP caption is created by enriching the original figure caption with contextual information from the corresponding article (e.g., in-line mentions, abstract text, and acronym expansions). VLMbased augmentation pipeline then refines these captions to retain only features that are visually supported by the image (see Appendix for details). We use BIOMEDICA-6M for all baseline pretraining, and BIOMEDICA-LongCAP specifically for the 2.1. Benchmarks We build and evaluate on two complementary benchmarks that stress different aspects of long-context retrieval. MIMIC-CXR radiology report (CXR) We constructed long-text benchmark from the MIMICCXR dataset Johnson et al. (2019) by pairing chest X-ray images with their full radiology reports. We sampled 1,000 unique imagereport pairs, where the reports provide free-text descriptions. PubMed Long-Caption (PMC) From 1,000 PMC-OA articles (restricted to recent 2025 publications), we construct long captions by concatenating inline references with figure captions, testing retrieval in scientific literature where extended technical context is essential. 3 Leveraging Long Context in Biomedical VisionLanguage Models Table 1: TextImage (T2I) and ImageText (I2T) retrieval on long-text CXR and PMC benchmarks, reported as Recall@K (higher is better; bold = best, underline = second-best). Panel shows the context-length ablation for BMC-LongCLIP; Panel benchmarks against prior models. Benchmark Model T2I I2T Name Context Batch R@1 R@5 R@10 R@1 R@5 R@10 Panel A: Context-length ablation Panel B: Baseline comparison CXR PMC CXR PMC BMC-LongCLIP BMC-LongCLIP BMC-LongCLIP BMC-LongCLIP BMC-LongCLIP BMC-LongCLIP 77 154 77 154 512 77 PMC-CLIP 256 BiomedCLIP 77 BMC-CLIP 512 BMC-LongCLIP BMC-LongCLIP 512 BMC-LongCLIP+ 512 77 PMC-CLIP 77 MedSigLIP 256 BiomedCLIP 77 BMC-CLIP 512 BMC-LongCLIP BMC-LongCLIP 512 BMC-LongCLIP+ 512 8K 8K 8K 8K 8K 8K 128 4K 8K 8K 16K 16K 128 N/A 4K 8K 8K 16K 16K 1.3 1.7 1.8 4.7 5.9 5.6 59.8 37.2 44.2 64.9 68.9 84.3 0.0 0.5 0.1 1.8 2.1 1.9 0.2 20.1 68.8 49.0 68.9 80.0 80. 0.5 2.6 1.1 5.6 9.5 7.1 0.7 37.0 86.2 67.6 84.3 92.3 91.2 9.4 10.7 10.3 66.4 72.5 89.3 0.7 5.7 2.9 10.3 12.1 12.2 1.2 46.0 91.1 74.0 89.3 95.1 94. 0.7 1.4 1.4 4.9 5.5 5.5 63.8 42.5 48.8 69.8 71.2 85.9 0.2 0.6 0.3 1.4 2.5 3.0 1.0 3.3 1.9 5.5 9.1 9.5 0.7 0.1 49.0 30.9 89.3 73.3 60.4 40.8 71.2 85.9 80.8 91.2 90.6 79. 7.3 9.3 9.8 70.8 76.8 89.9 1.6 5.5 3.4 9.8 14.2 14.5 1.2 60.1 93.7 68.4 89.9 93.5 93.8 Zero-shot Classification. For zero-shot image classification, we evaluate on 39 benchmarks spanning biology, radiology, dermatology, and pathology, as collected and described in Lozano et al. (2025a) (see Appendix section for more details). ing 8K and 16K global batches. In addition, we trained BMC-LongCLIP on both BIOMEDICA-6M and BIOMEDICA-LongCAP, 1M imagecaption dataset with captions enriched from full-text context, using 16K batch size and 512-token context window. We denote this model as BMC-LongCLIP+. 2.2. Baselines To contextualize our results, we benchmark our models against several baselines, including: PMC-CLIP Eslami et al. (2023), BiomedCLIP Zhang et al. (2023), MedSigLIP Sellergren et al. (2025), and BMC-CLIP Lozano et al. (2025b), 3. Experiments 3.1. Context-length ablation We assess the impact of extending text context length, thus reducing token waste on downstream zero-shot performance. To this end, models were trained with context windows of 77, 154, and 512 tokens under identical settings (batch size, learning rate, optimizer, epochs; as described in appendix section F). 4. Results 4.1. Context-length ablation Extending the text encoder context length consistently improves retrieval performance and training efficiency. Table 1 shows the zero-shot image-totext and text-to-image recall at in the long context benchmarks. Panel shows that longer context improves retrieval across recall levels for both CXR and PMC. On CXR, gains are steady but relatively modest. PMC benefits most, especially at stricter thresholds, highlighting the value of long contexts for textheavy tasks. In addition, we observe that pretraining CLIP with longer context lengths accelerates convergence (appendix section G), indicating that context extension improves not only downstream retrieval but also training efficiency. 3.2. Batch size and BIOMEDICA-LongCAP 4.2. Benchmarking against baselines We investigate the effect of scaling training batch size while holding other settings fixed, comparBMC-LongCLIP outperforms prior biomedical VLMs on both long-text benchmarks. Table 1 Panel 4 Leveraging Long Context in Biomedical VisionLanguage Models Table 2: Zero-shot classification results of different visionlanguage models across six biomedical domains. Numbers report average accuracy per domain (higher is better; bold = best, underline = secondbest). Panel shows ablations of BMC-LongCLIP; Panel benchmarks against prior models. Model Biology Dermatology Microscopy Ophthalmology Pathology Radiology Avg Name Context Batch Panel A: Context-length ablation BMC-LongCLIP BMC-LongCLIP BMC-LongCLIP 77 154 512 8K 8K 8K Panel B: Baseline comparison PMC-CLIP MedSigLIP BiomedCLIP BMC-CLIP BMC-LongCLIP BMC-LongCLIP BMC-LongCLIP+ 77 77 256 77 512 512 128 N/A 4K 8K 8K 16K 16K 40.82 37.21 34.95 7.75 33.98 34.07 34.08 34.95 34.98 34.34 40.69 51.34 55.16 12.59 20.13 36.01 65.81 55.16 38.80 55.54 46.04 55.76 53. 10.91 34.56 49.71 50.09 53.37 23.16 37.30 59.80 49.88 55.41 23.26 38.23 37.36 36.74 55.41 48.79 53.05 42.28 47.32 42.87 19.11 39.74 38.40 41.21 42.87 46.25 47.65 59.42 59.47 63. 38.64 53.03 56.05 59.15 63.20 52.79 66.99 48.18 50.16 50.16 18.71 36.61 41.93 47.85 50.16 40.79 49.48 and Table 2 compare BMC-LongCLIP with existing biomedical VLMs, including PMC-CLIP, BiomedCLIP, MedSigLIP, and BMC-CLIP. We exclude MedSigLIP from the CXR benchmark comparison, as it was trained on the same MIMIC-CXR imagereport pairs used to construct our benchmark. On the CXR benchmark, baselines achieve <6% Recall@10, while BMC-LongCLIP variants reach 1014%, more than two-fold improvement. On PMC, BMC-LongCLIP achieves 8995% R@10, performing on par with or slightly better than BiomedCLIP (9194%) and outperforming MedSigLIP (4660%). At the stricter R@1 threshsurpassing old, BMC-LongCLIP attains 6981%, BiomedCLIP (6973%) and outperforming MedSigLIP (2031%). Beyond retrieval, BMC-LongCLIP also improves zero-shot classification accuracy across six biomedical domains  (Table 2)  . While BiomedCLIP and MedSigLIP achieve average accuracies of 41.9% and 36.6%, respectively, BMC-LongCLIP (8K) attains 50.2%, the best overall performance. These results highlight that extending context length not only benefits long-text retrieval but also provides improvements in classification tasks. (16K) with BIOMEDICA-LongCAP data recovers this drop and matches or exceeds the 8K model. These results indicate that long-context modeling is most effective when paired with sufficient longthough performance in micaption supervision, croscopy remains comparatively weak and warrants further investigation. Overall. Across all experiments, BMC-LongCLIP outperforms prior biomedical VLMs in long-text retrieval and provides competitive advantages in zero-shot classification. 5. Conclusion Our results show that extending text context length in biomedical VLMs delivers clear gains. On longtext retrieval tasks, BMC-LongCLIP outperforms prior baselines on both CXR and PMC benchmarks, with the largest gains observed for PMC benchmark. key limitation is the scarcity of long-text benchmarks across biomedical domains; expanding such resources will be essential for fuller evaluation of long-context models. Taken together, these results establish long-context modeling as promising direction for advancing biomedical VLMs. 4.3. Effect of batch size and long-caption"
        },
        {
            "title": "Acknowledgments",
            "content": "training Long-context models benefit from enriched captions, while larger batch sizes yield mixed results. Doubling batch size with BMC-LongCLIP (16K) underperforms in microscopy and dermatology. This suggests that long context windows combined with large batch sizes may not uniformly translate into performance gains across domains. In contrast, BMC-LongCLIP+ This research was supported by grants from NVIDIA and utilized NVIDIA A100 GPUs. We gratefully acknowledge additional support from the AIMIAWS Cloud Credit Program, which provided AWS cloud computing. We further acknowledge support from the Stanford Data Science Scholars fellowship and ARPA-H to M.S., and the Arc Institute Graduate Fellowship to 5 Leveraging Long Context in Biomedical VisionLanguage Models for generalist medical artificial intelligence. Nature, 616(7956):259265, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748 8763. PmLR, 2021. Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, CÄ±an Hughes, Charles Lau, et al. Medgemma technical report. arXiv preprint arXiv:2507.05201, 2025. Thomas Sounack, Joshua Davis, Brigitte Durieux, Antoine Chaffin, Tom J. Pollard, Eric Lehman, Alistair E. W. Johnson, Matthew McDermott, Tristan Naumann, and Charlotta Lindvall. Bioclinical modernbert: state-of-the-art long-context encoder for biomedical and clinical nlp, 2025. URL https://arxiv.org/abs/2506.10896. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.12191. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European conference on computer vision, pages 310325. Springer, 2024. Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. A.L. This work was also supported by NIH grant R01 GM134483 to R.T., the Hoffman-Yee Research Grant to S.Y.L., and NSF grant 19DMS1208164. S.Y.L. is Chan Zuckerberg Biohub San Francisco Investigator."
        },
        {
            "title": "References",
            "content": "Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual question answering in the medical domain? In Findings of the Association for Computational Linguistics: EACL 2023, pages 11811193, 2023. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks, 2023. URL https://arxiv.org/abs/2309.17425. Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih ying Deng, Roger G. Mark, Steven Horng, and et al. MIMIC-CXR, deidentified publicly available database of chest Scientific radiographs with free-text reports. Data, 6(1):317, December 2019. doi: 10.1038/ s41597-019-0322-0. URL https://doi.org/10. 1038/s41597-019-0322-0. Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, and Serena Yeung-Levy. Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature, 2025a. URL https://arxiv.org/abs/ 2501.07171. Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Anita Rau, Austin Wolfgang Katzer, et al. Biomedica: An open biomedical image-caption archive, dataset, and visionlanguage models derived from scientific literature. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1972419735, 2025b. Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan Krumholz, Jure Leskovec, Eric Topol, and Pranav Rajpurkar. Foundation models 6 Leveraging Long Context in Biomedical VisionLanguage Models Appendix A. BIOMEDICA-LongCAP details are preprocessed and embedded via the vision encoder Eimg. We obtain BIOMEDICA-LongCAP data generation pipeline using Qwen2-VL-72B-Instruct Wang et al. (2024): ztext = Etext(ti) Etext(ti) , zimg = Eimg(xi) Eimg(xi) , 1. Context-Aware Caption Augmentation We enhance the original figure caption by contextualizing the image description with additional information from the full-text article. Specifically, we collect the original caption, inline mentions from the main text, the abstract, and acronyms used throughout the aforementioned data. Then VLM is prompted to augment the original caption, by only leveraging the provided information. 2. Feasibility Assessment. Given an image and its augmented caption, we extract all atomic features from the generated caption and prompt the VLM to evaluate whether it is feasible to discern each feature from the image alonewithout relying on external sources or any information not visually present, unless explicitly overlaid with feasibility text. The output is an XML file in which each atomic feature is labeled as either FEASIBLE or NOT FEASIBLE, along with rationale explaining the label. 3. Caption Refinement via Feasibility Filtering. Based on the feasibility assessment, we generate refined caption that preserves only atomic features labeled as FEASIBLE. Features labeled as NOT FEASIBLE are removed or reworded to ensure that the final image description reflects only information that can be visually supported. 4. Acronym Expansion. While all previous steps had access to acronym definitions, we explicitly expand all acronyms based on curated acronym list derived from the full-text article. This ensures that the captions are readable and unambiguous. Appendix B. CXR Benchmark We evaluate cross-modal retrieval between chest radiographs and their paired full-text reports. Let = {(xi, ti)}N i=1 denote the dataset, where xi is an image and ti its paired report. Each pair is treated as one-to-one ground-truth match. Reports are tokenized with the CLIP tokenizer and embedded via the text encoder Etext, while images where all embeddings are L2-normalized. For textimage retrieval, we rank all image embeddings {zimg } by cosine similarity with query ztext . Imagetext retrieval is defined analogously. Performance is reported as Recall@{1, 5, 10, 100} for both directions. ; the ground-truth match is zimg We analyzed the token length distribution of the 1,000 reports in our evaluation set using the BioClinical-ModernBERT tokenizer. The reports contained on average 168.3 tokens, with median of 158 tokens. The shortest report had 49 tokens, while the longest contained 427 tokens. Appendix C. PMC Benchmark We adopt the same retrieval formulation as CXR Benchmark on biomedical articles from PubMed Central. To construct the benchmark, we used the PubMed Central FTP service to download media bundles containing both .nxml full-text files and associated image files for recently published 2025 articles. From each article, we sampled exactly one unique imagecaption pair and did not reuse articles across the benchmark, ensuring that each pair represents distinct source document. For the 1,000 CXR reports, tokenization with BioClinical-ModernBERT yielded an average length of 510 tokens, with median of 460 tokens. Report lengths ranged from 251 tokens at the lower end to 1,022 tokens at the upper bound. Appendix D. Zero-shot classification benchmark the detailed dataset provenance, including For dataset names, citations, modalities, and class counts, please refer to BIOMEDICA Lozano et al. (2025a), Table S8. Each datasets classification task is reformulated into closed-form VQA task. Labels are mapped to short human-readable text descriptions, and each image is paired with multiple-choice list of candidate answers (including distractors). The correct label is randomly permuted among the options, and evaluation is performed by computing the similarity between image and answer embeddings. 7 Leveraging Long Context in Biomedical VisionLanguage Models Appendix E. Compute details Appendix G. Training loss curves by training context length"
        },
        {
            "title": "GPU Model GPU Memory Quantity",
            "content": "NVIDIA H200 NVIDIA A"
        },
        {
            "title": "141 GB\n80 GB",
            "content": "8 16 Table 3: Compute resources used for training. Appendix F. Training parameters Model Hyperparameters BMC-LongCLIP BMC-LongCLIP+ context length: 77/154/512 batch size (per GPU): 1024 GPUs: 8H200 effective batch size: 8192 learning rate: 5e4 beta1: 0.9, beta2: 0.95 warmup: 1000 max epochs: 20 precision: FP32 grad. clip norm: 1.0 dataset type: WebDataset dataset: Biomedica-6M context length: 512 batch size (per GPU): 1024 GPUs: 16A100 effective batch size: 16384 learning rate: 5e4 beta1: 0.9, beta2: 0.95 warmup: 1000 max epochs: 20 precision: FP32 grad. clip norm: 1.0 dataset type: WebDataset dataset: Biomedica-6M + LongCAP Table 4: Hyperparameters used for pretraining. 8 Figure 3: Training loss curves across context lengths, illustrating that longer text windows accelerate convergence."
        }
    ],
    "affiliations": [
        "NVIDIA, USA",
        "Stanford University, USA"
    ]
}