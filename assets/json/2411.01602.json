{
    "paper_title": "DreamPolish: Domain Score Distillation With Progressive Geometry Generation",
    "authors": [
        "Yean Cheng",
        "Ziqi Cai",
        "Ming Ding",
        "Wendi Zheng",
        "Shiyu Huang",
        "Yuxiao Dong",
        "Jie Tang",
        "Boxin Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 2 0 6 1 0 . 1 1 4 2 : r DreamPolish: Domain Score Distillation With Progressive Geometry Generation Yean Cheng Zhipu AI yean.cheng@zhipuai.cn Ziqi Cai Peking University zqtsai@gmail.com Ming Ding Zhipu AI ming.ding@zhipuai.cn Wendi Zheng Tsinghua University zhengwd23@mails.tsinghua.edu.cn Shiyu Huang Zhipu AI shiyu.huang@zhipuai.cn Yuxiao Dong Tsinghua University yuxiaod@tsinghua.edu.cn Jie Tang Tsinghua University jietang@tsinghua.edu.cn Boxin Shi Peking University shiboxin@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "We introduce DreamPolish, text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add surface polishing stage with only few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods."
        },
        {
            "title": "Introduction",
            "content": "Generating 3D assets benefits series of downstream applications like virtual reality, movies, video game, 3D printing, etc. Existing 3D generation methods can be divided into 3D native method and 2D/3D hybrid methods. 3D native generation methods [22, 34, 23] directly model 3D data, and demonstrate considerable success in producing basic 3D objects. However, these methods often Equal contribution. Corresponding author. Preprint. Under review. Figure 1: DreamPolish excels in producing 3D models with polished geometry and photorealistic texture. Please refer to the supplemental material for more results and videos. 2 struggle to produce complex objects with intricate geometry details and photorealistic textures, primarily due to shortage of high-quality training data. Recent advances in text-to-image diffusion models [12, 31, 44] demonstrate the effectiveness of large-scale pretraining on high-quality imagetext pairs. The 2D/3D hybrid models, which combine such 2D diffusion models with 3D neural representations, have shown promising outcomes. The Score Distillation Sampling (SDS) objective introduced in DreamFusion [29] is widely used in the 2D/3D hybrid models. It serves as bridge to bring photorealistic generation capability embedded in 2D text-to-image models to 3D content generation. During training, renderings with random viewpoints are sampled and sent to pretrained 2D text-to-image diffusion model (e.g.,[31, 1, 28]). The pretrained 2D diffusion model is incorporated to leverage the difference in distribution between the renderings and images posteriorly generated by 2D text-to-image models. However, using SDS as the sole optimization criterion can lead to inconsistent geometry among viewpoints. Furthermore, to maintain training stability, the SDS loss demands an excessively high classifier-free guidance (CFG) weight, leading to overly saturated outcomes. Recent trials [21, 30, 38] propose to decompose the text-to-3D process into multiple phases (e.g., geometry construction and texture generation). Under such strategy, researchers could leverage varying priors on decomposed sub-tasks, hence boosting the generation performance. Despite these advancements, the quality of generated 3D content has not reached the level of handcrafted and professionally produced 3D assets. In the geometric construction phase, the utilization of view-conditioned diffusion prior leads to artifacts on the generated surfaces, mainly because such model cannot to offer adequate supervision on geometric intricacies such as surface details. In the texture generation phase, the quality of the resulting texture continues to be inferior compared to those produced by text-to-image diffusion models. Moreover, obtaining higher-quality textures often requires compromising on stability, presenting the challenge of balancing texture photorealism with training stability. We introduce DreamPolish, text-to-3D generation method that produces 3D objects with polished geometry and photorealistic textures. We first focus on the geometric surface refinement. We employ variety of neural implicit and explicit representations to progressively retrieve geometry details, and we utilize an additional polishing stage that boosts the surface smoothness with the help of pretrained normal estimation prior. We further propose score distillation objective, dubbed domain score distillation (DSD), designed to identify the target domain that can ensure stability throughout the distillation process and enhance the photorealism of textures. Our pipeline consists of two phases: progressive geometry polishing and domain-guided texture enhancing. In the progressive geometry polishing phase, we start with text prompt and its corresponding image, and construct neural representation with image-level supervision in the reference view and employ vanilla SDS from view-conditioned diffusion prior [22] in novel views. We use NeRF [26] in the early iterations and progressively change the neural representation to advanced surface representations (i.e., NeuS [40] and DMTet [35]). However, the sampled field-of-view remains constrained by the pretrained distribution of the view-conditioned diffusion prior, and there is no surface supervision in novel views. To further polish the geometry surface, we incorporate diffusion prior that produces 3D-consistent normal estimation [9] to refine and polish the surface in the novel views. The initial limitation to the field-of-view, dictated by the pretrained distribution of 3D diffusion priors, is eventually relaxed, enabling the normal estimator to address and rectify any preceding artifacts. In the domain-guided texture enhancing phase, we first demonstrate through empirical analysis why the vanilla SDS tends to be unstable, requiring disproportionately high weight for CFG. Subsequently, we illustrate why recent works [41, 38] using guidance from variational domains improve the texture quality and effectively reduce the CFG weight to its normal ranges. In the conditional text-to-image generation, the use of classifier-free guidance leveraging unconditional image domains is shown to mitigate the decline in generated performance observed when text conditioning is introduced. This suggests that the unconditional image domain is beneficial for generation diversity and stability. Building upon these analyses, we propose DSD, aiming to balance the generation quality and stability. DSD mimics the classifier-free guidance generation process and proposes to use the unconditional gradient direction to guarantee the stability of score distillation. As illustrated in Figure 1, DreamPolish demonstrates its capability in generating 3D content featuring polished geometry and photorealistic textures. We have conducted extensive experiments to showcase the superior performance of DreamPolish compared to previous methods. Additionally, we have 3 performed ablation studies to validate the effectiveness of our proposed modules. The complete implementation will be made publicly available upon acceptance of our work."
        },
        {
            "title": "2 Related work",
            "content": "3D representations are the foundation for accurate and detailed text-to-3D generation. These representations can be categorized into explicit, implicit, and hybrid forms. Explicit representations, such as point clouds, voxel grids, and meshes, provide precise control by directly describing 3D geometry. However, integrating these forms into deep learning frameworks is challenging due to their lack of differentiability [19, 43]. Implicit representations, including NeRF [26], NeuS [40], Gaussian splatting [17] and Instant-NGP [27], leverage neural networks to generate continuous volumetric fields, making them more suitable for deep learning applications. Implicit representations produce fine-grained 3D models, however, they pose challenges in terms of editability and controllability [10], and require additional tools like MarchingCubes [24] for rasterization. hybrid approach such as DMTet [35], Plenoxels [7] and TensoRF [3] combines the advantages of both explicit and implicit methods. DMTet [35] utilizes deformable tetrahedral grid to optimize surface geometry and topology, converting signed distance functions into explicit meshes for high-resolution 3D synthesis. We propose using series of representations to progressively generate 3D assets, leveraging the advantages of different forms of representations. 3D diffusion priors extend the capabilities of diffusion models to 3D data, improving the quality of generation and reconstruction. Notable techniques such as the Zero123 series (e.g., Zero123 [22] and Zero123++ [36]) and GeoWizard [9] incorporate geometric priors to ensure the consistency and coherence in 3D structures. GeoWizard [9] combines depth and normal information with U-Net [32], and excels in providing high-quality surface normal estimations from single images, leveraging large-scale 3D datasets like OmniData [6] to mitigate data limitations. These methods show significant potential in enhancing 3D generation quality. We use combination of 3D diffusion priors to eliminate the ambiguity in novel views due to lack of information, thereby improving the robustness and photorealism of 3D asset generation. Text-to-3D generative models have been extensively studied to automate the creation of 3D assets, reducing the need for manual intervention. These models can be classified into native 3D models and 2D/3D hybrid models. Native 3D models such as ZeroNVS [34], Text2Shape [4], ShapeCrafter [8], SyncDreamer [23] and DreamGaussian [39], generate 3D data directly using specialized neural networks. These models employ complex neural architectures to create detailed 3D shapes conditioned on text and viewpoints. However, these 3D native models fall short when generating intricate objects attributing to limited representation ability. In contrast, 2D/3D hybrid models, such as DreamFusion [29], Magic3D [21], Fantasia3D [5], DreamControl [15], and HIFA [46] combine pretrained 2D diffusion models with 3D consistency constraints. Hybrid models harness the diverse generation capabilities of 2D models while ensuring geometric coherence in 3D space, addressing the challenges of data scarcity and high computational demands inherent to native 3D models. Despite these advances, the quality of generated 3D content is still inferior to that of the handcrafted 3D content in terms of surface quality and texture photorealism. Our method addresses these limitations, producing both high-quality surfaces and realistic textures."
        },
        {
            "title": "3 Preliminaries",
            "content": "Diffusion models [37, 12] are collection of likelihood-based generative models used to learn data distributions. Forward process is Gaussian process that progressively adds random noise ϵ to the data sampled from q(x). The reverse process reconstructs the original data from xT . The generative process is parameterized as Markov process: pϕ(x0:T ) := p(xT ) (cid:81)T t=1 pϕ(xt1xt). The diffusion model is trained to maximize the variational lower bound of the data log-likelihood [12]. The objective has been optimized into the mean squared error form by the researchers through series of mathematical derivations and the re-parameterization trick for better convergence: LDiffusion (ϕ) := Exq(x),tU (0,1),ϵN (0,I) (cid:104) ω(t) ϵϕ (αtx + σtϵ; t) ϵ2 2 (cid:105) . (1) Later research attempt to add text condition on to the generation process [31, 2], which can be expressed as ϵϕ (xt; y, t). The diversity of the generated images and the alignment with the 4 Figure 2: Overview of DreamPolish. Given text prompt and its corresponding generated image shown in the top left as input, DreamPolish first progressively constructs fine-grained 3D geometry with coherent and smoothed surface. Then, DreamPolish leverages DSD as the score distillation objective to guide the representation towards domain with both consistent and photorealistic texture. text conditions are two variables that need to be traded off. The CFG weight [13] is employed to resolve the balance between these two aspects. CFG employs the implicit approach to train diffusion models under conditional and unconditional modes, enabling the estimation of xt log (xty) and xt log (xt). During the sampling process, the strength of the diversity and alignment can be parameterized as weighted form of conditioned and un-conditioned objectives, ϵϕ(xt; y, t) ϵϕ(xt; y, t) + ω [ϵϕ(xt; y, t) ϵϕ(xt; t)] , (2) where ω is the guiding scale controlling the trade-off between fidelity and diversity. In typical image generation task, generally setting ω [7.5, 12.5] can yield the best generation results. Dreamfusion [29] proposes SDS, effectively integrating the diffusion models into the text-to-3D generation. SDS uses pretrained diffusion model ϵϕ to emulate the conditioned posterior distribution of the 3D model generation, i.e., for each image rendered from the differentiable 3D representation θ, SDS uses the diffusion model to verify if the generated image aligns with the distribution of images generated by the diffusion model pϕ(xty). This optimization is interpreted as minimizing the following KL divergences for all t, and can be simplified as, θLSDS = Et,ϵ,c (cid:20) w(t)(ϵϕ(xt; y, t) ϵ) (cid:21) , θ (3) where θ refers to the learnable parameters of certain differentiable 3D representation (e.g., NeRF [26], DMTet [35]). However, balancing consistency and photorealism is challenging. For example, DreamFusion [29] requires significantly high ω = 100 to achieve consistent 3D model, which results in unrealistic artifacts such as over-smoothed texture and abnormally high saturation [41, 38]. Recently, DreamCraft3D [38] proposes bootstrapped score distillation (BSD) paradigm to improve the texture generation quality. BSD builds on VSD [41] and proposes to utilize low-rank adaptation (LoRA) of the pretrained diffusion prior to modeling the variational distribution of each 3D object, and use generated images at different views to finetune the original model (i.e., Dreambooth [33]) to further bootstrap the performance: θLBSD = Et,ϵ,c (cid:20) w(t)(ϵDreamBooth(xt; y, t) ϵLoRA(xt; y, t)) (cid:21) . θ (4)"
        },
        {
            "title": "4 Method",
            "content": "The overall pipeline is illustrated in Figure 2. We decompose the intricate text-to-3D task into two phases: progressive geometry polishing and domain-guided texture enhancing. In this section, we first describe the geometry construction process detailed in Section 4.1. Next, we introduce the texture generation method in Section 4.2. 5 4.1 Progressive geometry polishing Progressive construction. Explicit and implicit neural representations exhibit both strengths and limitations. We propose to leverage the strengths of different representations by progressively generating 3D content with combination of these 3D representations. The generation starts with NeRF [26] due to its stable training process and ability to quickly generate rough 3D structure. We then transition to NeuS [40], surface-based representation that provides more accurate and detailed surface information. Inspired by [20], we incorporate progressive hash band to further enhance the quality of NeuS [40]. Subsequently, we switch to DMTet [35], which allows for the use of NVDiffrast [18] and integration of the graphics pipeline, enabling fast rendering. The sampled views can be divided into reference view (the view with input image as reference) and novel views (novel synthesized views without any reference). We employ different training objectives on these views, dubbed as reference-based loss Lref and guidance-based loss Lguide. The reference-based loss Lref primarily measures the error between the rendered image under the reference frame and the reference image itself. Lref can be considered reconstruction loss: Lref = λ1Lref normal + λ2Lref depth + λ3Lref mask + λ4Lref rgb. (5) where λ are the respective loss weights, and {Lref normal, depth, foreground mask, and reference image, respectively, defined as follows: normal, Lref depth, Lref mask, Lref rgb} are the losses for surface Lref rgb = ˆm (ˆx g(θ, ˆc))2, Lref mask = ˆm g(θ; ˆc)2, (6) where ˆc is the camera pose corresponding to the reference image ˆx. We add normal and depth supervision based on prediction from an off-the-shelf normal and depth estimator [6]: Lref normal = nˆn n2ˆn2 , Lref depth = conv(d, ˆd) σ(d)σ(ˆd) . (7) The reference-based loss function above ensures that the 3D model generates high-quality images at pose ˆc. However, there is no corresponding reference pose under other camera coordinates, the model needs to rely on pretrained models (i.e., priors) to determine geometry information in those novel poses. Although view-conditioned native 3D models have inferior rendering performance compared to 2D/3D models, they contain 3D coherent information and can be directly used for multi-view geometric supervision. For novel views, we use the vanilla SDS Loss, as in Figure 3, for supervision: θLguide = θLSDS(ϕ1, g(θ)). (8) where ϕ1 is the Stable-Zero123 [22] model. Combining the above operations, the gradient of the geometry generation loss can be expressed as: θLgeom = λguideθLguide + λrefθLref. (9) where λref and λguide are the corresponding model weights. Surface polishing. Despite the strengths of our progressive construction pipeline, we observe that the generated geometry could still benefit from additional refinement. We introduce surface polishing stage aiming at further enhancing the surface. In this stage, we fix the texture parameters and solely update the geometry to polish the geometry. Additionally, we loosen the field-of-view camera restrictions of Stable Zero123 [22], allowing for greater diversity of camera parameters. This enables rendering novel views with more diverse viewing conditions, which in turn provides richer information for the polishing process. We utilize pretrained diffusion model [9] to predict normal maps from images rendered under novel views, eliminating the ambiguity of geometry caused by insufficient information, we leverage standard SDS loss as supervision: Lguide normal = λcompˆncomp ˆnnovel2 + λlpipsLlpips(ˆncomp, ˆnnovel), (10) where ˆncomp is rendered normal map, ˆnnovel is corresponding normal prediction generated from rendered RGB image, and Llpips denotes perceptual loss [45]. Figure 3: Illustration on different score distillation strategies. (a): Vanilla SDS [29] only has guidance direction on zero-mean noise; (b): VSD [41] and BSD [38] utilize variational domain to improve texture quality; (c): our proposed DSD provides guidance directions toward unconditional image domain and variational domain, further improving the stability and photorealism of rendered texture. 4.2 Domain-guided texture enhancing With the geometry construction stage, our pipeline yields 3D objects represented by DMTet [35] with detailed geometric intricacies and smoothed surfaces. In the following phase, we fix the geometry representation and focus on texture generation. We illustrate the score distillation process in the latent space as Fig. 3. Diffusion models are designed to predict noises, which are assumed to follow dimensional Gaussian distribution. The major around the origin, visualized population of the probability is concentrated on ring with radius as the solid circle in Fig. 3. Samples around this ring represent the photorealistic images learned by pretrained text-to-image models from large scale image datasets. The standard SDS objective (Eq. (3)) minimizes distance between the estimated noise ϵϕ(xt; y, t) from diffusion model and random noise ϵ. However, Eq. (3) is sum of the expected distance between the estimated noise and random sampled noise. Due to its zero mean nature, the ϵ term represents force toward the center (mean) of the distribution, as demonstrated in Fig. 3 (a), which eventually causes unsatisfactory over-saturated results. The recently proposed VSD [41] and BSD [38] provide guidance direction towards variational domain, represented by the dotted line in Fig. 3 (b). This domain is the learned distribution of the object being constructed, hence providing high-quality knowledge to distill from compared to the vanilla SDS term. However, this variational domain is parameterized by LoRA [14], the training process does not guarantee the learned domain is stable and contains the necessary information for high-quality texture with sufficient diversities. The unconditional image domain is located around the major distribution, and represents the diversity and stability of the text-to-image generation capability [13]. We further propose DSD, which balances quality and stability within the distillation sampling process. As visualized in Fig. 3 (c), DSD consists of two guidance from the variational domain and the unconditional image domain, we employ the variational domain guidance to maintain photorealism and utilize the unconditional image guidance to ensure the stability of the distillation process. The gradient of the proposed DSD distillation sampling method can be expressed as: θLDSD = Et,ϵ,c (cid:20) w(t)(ϵϕ(xt; y, t) λrealisticϵϕ (xt; y, t) λstableϵϕ(xt; t)) (cid:21) , θ (11) where λrealistic and λstable represents the weights of diversity and stability."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experimental settings Baselines. To evaluate the performance of our method, we compare it with several representative and state-of-the-art models. Specifically, we compare our method to DreamFusion [29], GeoDream [25], and ProlificDreamer [41], which use text inputs only, as well as Magic123 [30] and DreamCraft3D [38], which support both text and reference image inputs. 7 Figure 4: Qualitative comparisons with baseline methods. Our method produces 3D objects with high-quality geometry and photorealistic textures. Please refer to the supplementary for more results. Table 1: Quantitative Comparison Results. () indicates that higher (lower) is better. We highlight the best score in boldface. Model Magic123 [30] DreamCraft3D [38] Ours PSNR 20.30 24.40 25.13 SSIM LPIPS CLIP Score 0.803 0.933 0.933 0.720 0.754 0.759 0.148 0.093 0.087 Datasets. We use diverse dataset composed of text-image pairs for comprehensive evaluation. Additionally, we generate normal maps and depth maps using Omnidata [6], following the methodology of DreamFusion [29]. Error metrics. To assess the quality of the generated results, we calculate PSNR [16], SSIM [42], and LPIPS [45] in the reference view. Following the methodology in [38], we also employ CLIP Score [11] to measure the consistency between the generated model and the text prompt. 5.2 Qualitative comparison We compare our method with the aforementioned baselines. The results are demonstrated in Fig. 4. Given the reference image, we show the generated renderings of our model and the baselines under novel viewpoints. Benefiting from refined geometry and the DSD objective, our model could generate intricate details and photorealistic textures, such as the frogs eye in the second row and the wing of the rubber duck in the third row. Please refer to the supplement for more results. 5.3 Quantitative experiments Comparision results. We render images from novel views and compute the mean CLIP Score between each pair of rendered images and the text prompts. Table 1 summarizes the results of the quantitative evaluation. Our method outperforms the baselines in every metric. User study. We randomly select multiple examples from different viewpoints and shuffle the data from our model and the baseline methods. Users are asked to choose the model they consider to have the best performance for each example. We received total of 40 valid responses, and the Figure 5: User study results. Figure 6: Ablation study on geometry construction. Geometric quality and surface smoothness in varying representations are progressively refined along the training process. In the surface polishing stage, normal smooth loss Lnovel ablation is insufficient for surface smoothing while the proposed Lnovel normal objective can effectively polish the artifacts generated in previous stages. Figure 7: Ablation study on texture generation. With the same fixed geometry, the proposed DSD objective produces textures with the most photorealistic details. results are shown in Figure 5. It is evident that our method outperforms the other methods and is well-received by the general users. 5.4 Ablation study normal with simple normal smooth loss: Lguide 2, where hn and wn represent the gradients of the normal map. Lguide Geometry construction. Figure 6 shows the normal map of each stage in the geometry construction process. We replace the proposed Lguide ablation = hn2 2 + wn2 ablation polishes the artifacts in previous stages into refined surface. As shown in the fourth row, the smooth loss Lguide still produces artifacts and is insufficient for surface polishing. ablation Texture generation. Figure 4 proves our model generates state-of-the-art 3D objects, thanks to both the geometry construction and texture generation modules. To further prove the effectiveness of the DSD objective, we use the previous losses (SDS [29], VDS [41] and BSD [38]) on the same geometry produced by our geometry phase, and compare the texture quality with that produced by our proposed DSD. The results are shown in Fig. 7, demonstrating that the DSD can generate textures with superior photorealism (e.g., fewer artifacts in the first row and more details in the second row)."
        },
        {
            "title": "6 Conclusion",
            "content": "We present DreamPolish, text-to-3D generation model that achieves polished geometry and photorealistic textures. By progressively constructing geometry and incorporating surface polishing stage, DreamPolish could produce refined surfaces; with the proposed DSD objective, our approach addresses key challenges in texture generation and is able to stably distill photorealistic details from the latent domain. Extensive experiments demonstrate that DreamPolish can produce 3D assets with superior quality, setting new benchmark in the 3D generation field. Limitations. Despite the success of DreamPolish, several limitations remain. The geometry refinement stage, while effective, is limited by the quality of the initial geometry. Additionally, the computational cost of our approach can be further optimized."
        },
        {
            "title": "References",
            "content": "[1] DeepFloyd Lab at StabilityAI. DeepFloyd IF: novel state-of-the-art open-source text-to-image model with high degree of photorealism and language understanding. https://www.deepfloyd. ai/deepfloyd-if, 2023. [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. eDiff-I: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial radiance fields. In Proc. of European Conference on Computer Vision, 2022. [4] Kevin Chen, Christopher Choy, Manolis Savva, Angel Chang, Thomas Funkhouser, and Silvio Savarese. Text2Shape: Generating shapes from natural language by learning joint embeddings. In Proc. of Asian Conference on Computer Vision, 2019. [5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3D content creation. In Proc. of International Conference on Computer Vision, 2023. [6] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3D scans. In Proc. of International Conference on Computer Vision, 2021. [7] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proc. of Computer Vision and Pattern Recognition, 2022. [8] Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar. ShapeCrafter: recursive textconditioned 3D shape generation model. Proc. of Neural Information Processing Systems, 2022. [9] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. GeoWizard: Unleashing the diffusion priors for 3D geometry estimation from single image. arXiv preprint arXiv:2403.12013, 2024. [10] Kyle Gao, Yin Gao, Hongjie He, Denning Lu, Linlin Xu, and Jonathan Li. NeRF: Neural radiance field in 3D vision, comprehensive review. ArXiv, 2022. [11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In Empirical Methods in Natural Language Processing, 2021. [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Proc. of Neural Information Processing Systems, 2020. [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [15] Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang Xu, Songcen Xu, Rynson WH Lau, and Wangmeng Zuo. DreamControl: Control-based text-to-3D generation with 3D self-prior. arXiv preprint arXiv:2312.06439, 2023. [16] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of PSNR in image/video quality assessment. Electronics letters, 2008. [17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 2023. [18] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics, 2020. [19] Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang, Zhihao Liang, Jing Liao, Yan-Pei Cao, and Ying Shan. Advances in 3D generation: survey, 2024. [20] Zhaoshuo Li, Thomas Müller, Alex Evans, Russell Taylor, Mathias Unberath, Ming-Yu Liu, and ChenHsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In Proc. of Computer Vision and Pattern Recognition, 2023. [21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. In Proc. of Computer Vision and Pattern Recognition, 2023. [22] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object, 2023. [23] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. Proc. of International Conference on Learning Representations, 2024. [24] William Lorensen and Harvey Cline. Marching cubes: high resolution 3D surface construction algorithm. Seminal Graphics, 1998. [25] Baorui Ma, Haoge Deng, Junsheng Zhou, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. GeoDream: Disentangling 2D and geometric priors for high-fidelity and consistent 3D generation. In arXiv preprint arXiv:2311.17971, 2023. [26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proc. of European Conference on Computer Vision, 2020. [27] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics, 2022. [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [29] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In Proc. of International Conference on Learning Representations, 2023. [30] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. arXiv e-prints, 2023. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. of Computer Vision and Pattern Recognition, 2022. [32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. ArXiv, 2015. [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proc. of Computer Vision and Pattern Recognition, 2023. [34] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. ZeroNVS: Zero-shot 360-degree view synthesis from single real image. arXiv preprint arXiv:2310.17994, 2023. [35] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep Marching Tetrahedra: hybrid representation for high-resolution 3D shape synthesis. In Proc. of Neural Information Processing Systems, 2021. [36] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. ArXiv, 2023. [37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proc. of International Conference on Machine Learning, 2015. [38] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3D: Hierarchical 3D generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. [39] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. DreamGaussian: Generative gaussian splatting for efficient 3D creation. arXiv preprint, 2023. 12 [40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Proc. of Neural Information Processing Systems, 2021. [41] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. [42] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 2004. [43] Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, and Shijian Lu. Multimodal image synthesis and editing: The generative AI era. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. Proc. of International Conference on Computer Vision, 2023. [45] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proc. of Computer Vision and Pattern Recognition, 2018. [46] Joseph Zhu and Peiye Zhuang. HiFA: High-fidelity text-to-3D with advanced diffusion guidance. arXiv preprint arXiv:2305.18766, 2023."
        }
    ],
    "affiliations": [
        "Zhipu AI",
        "Peking University",
        "Tsinghua University"
    ]
}