{
    "paper_title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning",
    "authors": [
        "Hongyi James Cai",
        "Junlin Wang",
        "Xiaoyin Chen",
        "Bhuwan Dhingra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 7 2 4 2 . 5 0 5 2 : r How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning Hongyi James Cai 1, Junlin Wang 1, Xiaoyin Chen2, Bhuwan Dhingra1 1Duke University, 2Mila - Quebec AI Institute {hongyi.cai, junlin.wang2}@duke.edu, xiaoyin.chen@mila.quebec bdhingra@cs.duke.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as learned capability. However, the precise benefits of backtrackingspecifically, how significantly it contributes to reasoning improvements and the optimal extent of its useremain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent large language models such as DeepSeek-R1 [3] and OpenAIs o1 [13] have demonstrated remarkable reasoning abilities especially on various complex reasoning tasks. Their success stems from allocating substantially more inference-time compute, letting the model perform an internal search via extended chains of thought before committing to an answer[24]. Reinforcement learning (RL) has emerged as the most effective way to unlock these long reasoning traces [22, 25, 21, 33, 19]. What is new in the DeepSeek-R1 era is the shift from open-ended process rewardswhich score intermediate reasoning steps[12, 7, 36, 38]to verifiable, rule-based reward functions that Equal contribution. 2Our code is available at https://github.com/jchy20/how-much-backtrack Preprint. Under review. Figure 1: We perform controlled post-training pipeline study by curating synthetic datasets for Sudoku, Countdown and Arc 1D tasks, varying the number of backtracks. The backtrack demonstrations are generated purely through search algorithmsspecifically, depth-first search and heuristic searchrather than by sampling from the base model. Through comprehensive experiments and analysis, we discover positive correlation between problem difficulty and number of backtracks needed in the demonstrations. directly validate final answers against symbolic checkers or ground-truth programs[35, 1]. These programmatic rewards give the agent sharp learning signal, guiding exploration toward solution trajectories that yield correct outputs and thereby enabling the consistently deep chains of thought seen in todays best reasoning models. Although reinforcement-learning (RL) often strengthens an LLMs reasoning, the scope and limits of these gains remain under-explored. Gandhi et al. [6] document this uncertainty by comparing two closely matched model families. RL produces dramatic jump for Qwen, yet yields only marginal gains for Llama. One hypothesis credits Qwens advantage to its frequent use of thinking tokens such as wait and verify [30]. Gandhi et al. [6] test this by first supervised-finetuning Llama on chain-of-thought (CoT) data and then applying RL; the CoT warm-up induces richer cognitive behaviors such as backtracking or self-checking and yields larger RL gains than training vanilla Llama directly. These and related studies suggest that RL acts more as an amplifier on different behavioral patterns, such as backtracking thats present in the base model. Some evidence points to RL leveraging patterns already innate in pre-training data [37] and generalizing them to novel tasks [2]. Conversely, other work shows that simply generating longer reasoning traces does not guarantee better accuracy [16, 29, 32]. Regardless, the ability to backtrack, which allows models to rethink and revise its previous solutions, are commonly observed across the strongest reasoning models [3, 13]. We still lack principled understanding of how to construct data for SFT warm-up to prepare models for efficient RL training and have limited insights on how to best harvest the power of RL. In this work, we aim to answer several questions: is warm-up stage necessary for RL? What kinds of SFT warm-up matter to RL? How many backtracking steps are optimal, and when do they become counter-productive? We seek to shed light on the training interaction between SFT and RL and provide insights into what data we should include for SFT. To rigorously answer the questions above, we design suite of controlled experiments. We systematically compare cold-start RL and RL with short CoTs as SFT warm-up. Our results show that, while cold-start RL already yields some improvements, even simple SFT warm-upssuch as self-sampled data from the target modelcan further enhance performance. For subset of three tasks, we curate synthetic datasets using depth-first search (DFS) or heuristic search, injecting varying numbers of synthetic backtracks into the SFT warm-up. We find that backtracking demonstrably benefits RL training, with more challenging tasks requiring deeper backtracking to maximize gains. Additionally, we experiment with SFT warm-up using distillation data from QwQ-32B [26], and observe that whether trajectories are correct or incorrect has little effect on final RL performance. 2 Taken together, our controlled experiments reveal precise recipe for effective RL: initializing with synthetic SFT data containing an appropriate number of backtracksmatched to the difficulty of the taskconsistently leads to the best outcomes, while the correctness of individual trajectories is less critical. Our research provides the following key contributions: In Section 4.1, we conduct controlled investigation on eight reasoning tasks and find that short CoT data that rarely exhibit any behavioral patterns such as \"wait\" do have additional contribution to the RL training compared to cold-start RL, contrary to prior findings.[6]. In Section 4.2, we utilize both short CoTs and distilled data from QwQ-32B and conduct comparative studies to conclude that RL post-training initialized from correct and incorrect CoTs eventually converge in performance. In Section 4.3, we construct synthetic datasets and find that easier problems do not necessarily require backtrack to be present in the warm-up stage. In contrast, more difficult problems with larger search space need increasingly more backtracks to be present to allow for effective RL training."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Test time scaling Recent work has shown scaling up language model output length during test time can be more effective than scaling pretraining [24, 28, 8, 20]. As language models become increasingly larger and pre-train on more and more enormous corpus; this third axis of scaling offers promising gains in performance while remaining budget friendly [29]. There are mainly two directions for scaling up: parallel sampling and sequential search. Common parallel sampling methods such as best-of-n rely on LLMs to propose answers independently times [11]; whereas sequential search generates each response in sequence conditioned on previous attempt [17, 4, 5, 31]. There have also been attempts to combine the two and incorporate external verifiers through algorithms like Monte-Carlo Tree Search [7, 4]. Our work extends this line of research by examining the optimal training mixtures for efficiently scaling up model generation for reasoning. 2.2 Reinforcement Learning for reasoning novel dimension of scaling inference-time compute for language models has recently gained prominence through the emergence of specialized \"reasoning models\" trained via reinforcement learning (RL). Prior works predominantly explored on-policy and off-policy RL methods [34, 33, 9, 15] and saw moderate successes. However, recent approaches such as DeepSeek-R1 [3] have revitalized interest in RL training utilizing verifiable rewards (RLVR), driven by Proximal Policy Optimization (PPO) and its memory-efficient variant, Group Relative Policy Optimization (GRPO) [22]. key driver behind RLVR is the emphasis on rule-based rewards derived directly from final outputs, replacing traditional reward models. This new RL training paradigm has notably revealed critical \"aha moments,\" where models spontaneously generate distinctive tokens such as \"wait,\" indicative of active internalization of search strategies and optimized trajectory discovery. One prevailing hypothesis is that such fine-tuning allows language models to internalize search algorithms during inference-time explicitly. Methods such as Stream of Search [5], for instance, fine-tune models using linearized search trajectories, empowering continuous and coherent search capability within single-output generation [31, 10]. Alternatively, another hypothesis suggests that RL training utilizing verifiable rewards does not fundamentally induce novel reasoning skills but rather exploits and amplifies capabilities already learned during the pretraining phase [32]. This claim is supported by observations that base pretrained models can achieve comparable or superior performance in metrics such as pass@k compared to their RL-trained counterparts. Nevertheless, the precise mechanisms behind these emergent reasoning capabilities remain poorly understood. Our research addresses this gap by systematically investigating how RL leverages SFT and the associated data mixture to unlock the reasoning capacities. There are few concurrent efforts investigating how fine-grained behavioral signals influence RLtrained reasoning models. [6] analyzes four behavioral cues, demonstrating that such cues can 3 Table 1: brief explanation of each SFT warm-up setup explored in this paper. Abbr. Explanation No-SFT Self-sampled SFT Distilled SFT Synthetic backtrack SFT Shuffled SFT RL with no SFT (cold-start RL) SFT on models own generated trajectories SFT on distilled trajectories from stronger model SFT on synthetic trajectories with explicit backtracking SFT on trajectories being shuffled with other problems trajectories accelerate RL convergence. We perform controlled experiments on more tasks and study various factors of SFT mixtures. [16] examines whether explicit backtracking traces improve performance across CountDown and Sudoku. We go further by rigorously controlling trace correctness, format, frequency, and synthetic versus distilled sources. Finally, [37] explores how variations in pre-training corpus composition interact with subsequent post-training, whereas we emphasize on the post-training affects the reasoning ability."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Overview In this study, we conduct comprehensive analysis using controlled experiments to investigate the interplay between SFT and RL, focusing on how different training data mixtures influence RL outcomes. Specifically, we examine which types of SFT warm-ups benefit RL and seek to understand the underlying reasons. Table 1 presents the SFT setups we evaluate: no SFT, self-sampled SFT, distilled SFT, synthetic backtracking SFT, and shuffled SFT. Based off those different setups, we structure the remaining sections in the following ways: (1) Section 4.1 covers no SFT and self-sampled SFT, where we conclude that no SFT improves performance and simply self-sampled SFT can further boost RL performance. (2) Section 4.2 disentangles the effects of structure (e.g., reasoning patterns) and content (e.g., correctness of trajectories) in SFT and find correcness to have minimal impact. We perform ablations on self-sampled and distilled SFT, comparing RL initialized from correct versus incorrect trajectories. (3) Additionally, in Section 4.3 we vary the number of backtracks in synthetic SFT to assess its impact on RL performance. We employ total of 8 reasoning tasks, adopted from [27], and select subsets to further curate synthetic and distilled data to ensure the our experiments provide analytical insights. 3.2 Reasoning Tasks Countdown The goal is to construct an arithmetic expression using set of numbers to reach specific target value. Each number can be used at most once, and operations are limited to basic arithmetic: addition, subtraction, multiplication, and division. Sudoku The objective is to fill 99 grid so that each row, column, and 33 subgrid contains all the digits from 1 to 9 exactly once. Arc 1D The objective is to learn and apply transformation rule that maps one-dimensional input grid to corresponding output grid, given several inputoutput examples. Advanced Geometry Advanced Geometry contains three sub-tasks. In the angle measurement sub-task, the goal is to compute the internal angle at specific vertex of triangle, given the input of three vertex coordinates in the Cartesian plane. In the orthocenter sub-task, the goal is to determine the orthocenter of trianglethe point where all three altitudes intersect, given the input of the triangles three vertex coordinates. In the incircle radius sub-task, the goal is to compute the radius of the incircle of triangle, given the input of the coordinates of its three vertices. 4 Table 2: Baseline accuracy of vanilla Qwen2.5 models on 8 different reasoning tasks ARC CCR SDK AG CD LF ZP SR Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct 0.015 0.052 0.004 0.019 0.018 0.064 0.000 0.000 0.286 0. 0.254 0.388 0.199 0.314 0.134 0.138 Color Cube Rotation In the game of Color Cube Rotation, the goal is to determine the color of specific side of cube after sequence of 3D rotations, given the input of its initial face-color configuration. List Functions The goal is to generate an output list by identifying and applying an implicit rule that maps each input list to its corresponding output, given examples. Zebra Puzzles The goal is to determine specific attributesuch as the name of the person in given housebased on set of logical constraints, given the input of several people and their distinct characteristics (e.g., favorite drink, pet, or phone). Self Reference The goal is to determine how many consistent truth assignments exist for set of self-referential statements, given the input of seven logically interconnected claims. Each statement makes assertions about the truth or falsity of other statements (or the total number of true/false ones). 3.3 Model and training We use Qwen2.5 family of models [18], primarily focusing on Qwen2.5-3B-Instruct for supervisedfinetuning and reinforcement learning training [14, 23]. We also include Qwen2.5-7B-Instruct for baseline comparison. The evaluation metrics is pass@1 where we only focus on the correctness of the final answer, which should be placed inside <answer></answer> tokens. For both SFT and RL training, we adopted code from Sheng et al. [23], Pan et al. [14], the specific rollout lengths depend on whether the model is initialized from long or short CoT. Typically, for RL training initialized from long CoTs, we use rollout length from 4k to 8k tokens, depending on the tasks. For short CoTs, we use rollout length from 1k to 2k tokens, depending on the tasks (Appendix B). We adopt rule-based rewards [3], where format accuracy (successfully generate pair of <think></think> and <answer></answer> tokens with thinks tokens come before answer tokens) is rewarded 0.1 point, and the answer accuracy-exact match with the ground truth-is rewarded 0.9 point (Appendix C). Together, problem may be rewarded maximum of 1.0 point and the minimal of 0.0 point."
        },
        {
            "title": "4 Results",
            "content": "We conduct systematic experiments on various reasoning tasks under different SFT settings  (Table 1)  to investigate their influence on subsequent RL training. We summarize our takeaway as follows: (1) short CoTs as initialization for RL induce moderate performance gains; (2) incorrect SFT has comparable performance with correct counterparts post RL training; (3) number of backtracks demonstrations needed scale up with problem difficulty; (4) RL is sensitive to shuffled SFT, i.e. internal consistency of SFT is important. 4.1 Self-sampled SFT improves RL Training and evaluating setup In this series of experiments, we first perform specialized cold-start RL training starting from Qwen2.5-3B-Instruct [18] across 8 reasoning datasets [27]. After obtaining 8 RLed models, we subsequently collect their reasoning trajectories on the same tasks they are finetuned on, but with problems generated from different seed to ensure there is no data leakage across SFT and RL. We purposefully separate the trajectories that lead to correct answer and those that lead to incorrect answers, from which we further perform the complete SFT + RL post-training pipeline starting again from vanilla Qwen2.5-3B-Instruct, and obtain 16 specialized models (8 RLed 5 (a) Cold-start RL models in domain evaluation (b) Performance comparisons Figure 2: Short CoTs model evaluations. (a) shows the evaluation accuracy of 8 specialized coldstart models evaluated on their corresponding in-domain reasoning tasks. (b) shows the change in percentage on in-domain task performance for RL models initialized from correct trajectories (green) and incorrect trajectories (yellow), compared to the corresponding cold-start RL models. AG means Advanced Geometry, CD means Countdown, ARC means Arc 1D, SDK mean Sudoku, CCR means Color Cube Rotation, ZP means Zebra Puzzles, LF means List Functions, SR means Self Reference models initialized from correct SFT, and the other 8 initialized from incorrect SFT). comprehensive evaluation is subsequently performed for all 24 models, with in-domain results for cold-start RL models in Figure 2a, and the percentage of accuracy difference for the SFT + RL models in Figure 2b. No-SFT (cold-start RL) induces both internalized search and latent thinking abilities Our comprehensive experiments reveal insights into the effectiveness of cold-start RL, with concise thinking model as starting point (here we define concise models by their outputs lengths around 1k tokens). In figure 2a, we observe simple cold-start RL is able to witness improvements in models reasoning abilities, compared to their short thinking starting models benchmarked in Table 2. We manually inspect few of the generated trajectories from cold-start RLed model and identify that there is indeed naive verbalized search present in tasks such as Countdown (Appendix D.2), while latent thinking is employed in tasks like List Functions (Appendix D.1). In latent thinking outputs, the model is able to directly reason and generate answer in one go. Additionally, training the model in one environment not only sees in-domain improvements but also knowledge transfer abilities to selected tasks, as shown in Appendix F. For example, training in Advanced Geometry environment induces good generalization to Zebra Puzzles and List Functions; training in Countdown induces generalization to Advanced Geometry. It is also worthwhile to note that these tasks share little semantic similarities. Collectively, the results suggest that models that are not explicitly introduced with searching and verification behaviors, like backtracking, are able to flexibly scale up different modes of reasoning dependent on the question type. Short and self-sampled CoTs are effective warm-ups Concise and correct CoTs are valuable starting points in continuing performance gains with RL training. In Figure 2b, we observe consistent in-domain performance gains, comparing to cold-start RL, when the models first SFT on correct and concise self-generated trajectory. In fact, performance increases in all reasoning tasks except for Sudoku and Countdown, which are classic problems that can be solved efficiently by either breadthfirst search (BFS) or depth-first search (DFS). The results could suggest that short self-generated CoTs do not prompt more sophisticated search strategies like DFS during RL. It it also worthwhile noting that short CoTs enable two distinct scaling patterns with regards to response length. Similar to findings in previous paragraph 4.1, models that exhibit latent thinking patterns, when initialized on the corresponding self-generated SFT, shows downward scaling of response length as RL steps go up. In contrast, models with verbalized reasoning patterns scales up response length (Appendix G). This further reveals RL as post-training method is effectively utilizing the patterns seen during SFT. 6 (a) Countdown evaluations (b) Reward trajectories Figure 3: In domain evaluation of models and respective RL training trajectories. (a) is Countdown evaluation on models initialized from correct, incorrect, and incorrect format distilled data, (b) is RL training trajectories of models initialized from correct, incorrect, and incorrect format distilled data. 4.2 Correctness has little impact on the utility of SFT Self-generated short CoTs vs. distilled long CoTs This series of experiments summarizes common pattern found in both short and long CoTs: i) self-generated incorrect short CoTs with results in Figure 2b, and ii) distillation data generated by QwQ-32B. Specifically, we generate trajectories with the max of 4k context window from QwQ-32B on the task of countdown, and collect the responses. The responses are categorized into correct (reasoning traces are correctly encapsulated by <think></think> and correct, parse-able answer encapsulated by <answer></answer>), incorrect (has the format with correct answer, with the difference being the answer encapsulated by <answer></answer> is parse-able but incorrect), and incorrect format (the reasoning trajectories could be correct and incorrect, but fails to include parse-able answer). Models initialized from incorrect reasoning CoTs display similar behaviors with those from correct trajectories Figure 2b shows that incorrect SFT models post RL see the same trend of performance increase or decrease with correct SFT models on in-domain evaluations, with the only exception being self reference task. This conclusion not only holds true on models initialized from self-sampled SFT, but also on distilled SFT. The reward trend displayed in Figure 3 shows all three models are converging to similar trajectory on the game of Countdown. We further include comparisons of these reward trajectories with that of cold-start RL model in Figure 3b, and evaluations in Figure 3a. The comparisons show that even priming on incorrect long trajectories, the RL training does not corrupt; instead, it still marginally outperforms cold-start RL model during evaluation. These critical and surprising results are consistent with the prior findings of [6], which provide an additional layer of nuance to the claim \"SFT memorizes, RL generalizes\" made by [2]. This takeaway can have implications on the continual scaling of data and pretraining, for instance, including suboptimal reasoning trajectories 4.3 Backtracking pushes models to new heights and for hard task Building synthetic datasets In this series of experiments, we selected subset of three tasksCountdown, Arc 1D, and Sudoku-to construct three synthetic datasets with varying numbers of backtrack, as well as optimal trajectories, and study whether, and how RL amplifies such behavior. The three tasks are selected because they are representative of different difficulty levels based on vanilla models baseline performance in Table 2, and different search strategies. To construct synthetic datasets for Countdown and Sudoku, we use DFS solver and create tree structure where each node represents an intermediate step (e.g., an operation between two numbers for Countdown; fill one grid with valid number for Sudoku). Then we start from the solution node back to the root node to build optimal solutions, and include incorrect branches as detours and backtracks. For Arc 1D, we design heuristic search that applies hand-crafted transformation functions with step-wise, verbalized reasoning. detour or backtrack is created by choosing an incorrect transformation and then retrying. 7 (a) Arc 1D (b) Countdown (c) Sudoku (d) In domain evaluation Figure 4: RL training reward trajectories for (a) Arc 1D, (b) Countdown, and (c) Sudoku, as well as (d) each models in domain evaluation comparison. Optimal number of backtracks scales with problem difficulty Our controlled experiments reveal that the exposure to backtrack systematically improves RL training efficiency as shown in Figure 4. However, the number of backtracks needed depends on the difficulty of the problem: 1. Countdown Moderate. Every puzzle provides 46 numbers and target value, producing medium-sized arithmetic search tree. Although the Qwen2.5-3B-Instruct can solve 0.4% of the problems. it can generate plausible attempts. We found that using exactly one backtrack is most optimal. 2. Arc 1D Moderate/Easy. Tasks are solved by applying small pool of grid-transformation heuristics; the same baseline attains 1.8% accuracy. Using zero backtrack is optimal. 3. Sudoku Hard. Each board contains 3060 empty cells; every cell admits multiple candidates, yielding an astronomically large search space. vanilla The Qwen2.5-3BInstruct baseline solves none of the test instances. Using five backtracks is optimal. Our controlled experiments confirm consistent pattern: harder combinatorial problems require deeper back-tracking traces to seed RL, whereas easier tasks are best served by shallow or even optimal demonstrations. For Countdown, RL initialized with one backtrack achieves the highest reward curve (Figure 4b), whereas RL from zero backtrack performs substantially worse. This suggests that one backtrack initialization enables the model to internalize and execute efficient search strategies, as reflected by shorter responses. Evaluation results confirm this, with the one-backtrack model attaining 69.7% accuracy (Figure 4d), outperforming QwQ-32B [18] at 51.5% (8K context window), while the zero-backtrack model achieves only 38.9%. For Sudoku, initializing PPO with five backtracks is necessary for stable, effective training. Too few or too many backtracks (e.g., one or ten) lead to model degeneration. Notably, cold-start RL and self-generated short CoTs fail to train on Sudoku, with reward trajectories stagnating at 0.1. Backtracking consistently outperforms optimal traces, as shown by 28.9% accuracy for the fivebacktrack model versus 14.4% for the zero-backtrack model. This also surpasses the QwQ-32B baseline, which achieves 0.0% (Appendix E). 8 (a) Arc 1D (b) Countdown (c) Sudoku (d) In domain evaluation Figure 5: RL training reward trajectories for models post-trained on shuffled and not shuffled synthetic dataset of (a) Arc 1D, (b) Countdown, and (c) Sudoku, as well as (d) each models in domain evaluation comparison. Models with initialization from shuffled datasets achieve 0.0% in accuracy. For Arc 1D, the easier task among three, performance declines as the number of backtracks increases. Models trained with zero backtracks consistently outperform others, with the zero-backtrack achieving 90.8% accuracy, significantly exceeding QwQ-32Bs 24.0%. Overall, these findings validate our hypothesis: the benefits of backtrackingand the optimal number of backtracksclosely track problem difficulty. 4.4 RL is sensitive to shuffled SFT Internal consistency is important This series of experiment controls structure and varies content by randomly shuffling prompt with mismatched completion within the best performing backtrack dataset in Section 4.3. From Figure 5, we see that the training becomes extremely ineffective, suggesting that despite having the right reasoning structure, RL training is easily vulnerable to the internal inconsistency between answer and completion during SFT. In fact, we find that, for Countdown and Arc 1D in Figures 5b and 5a, only models primed on less than approximately 5k demonstration can pick up insignificant reward signals after 60 to 80 steps of exploration. This results was surprising, given the discovery of RLs immunity to CoTs and answer correctness in Section 4.2. After inspection of the generated output (Appendix D.2), we see that the models learn too strong of association between the question and the shuffled pair that they are unable to unlearn during PPO."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we thoroughly inspect the interplay between SFT and RL for reasoning tasks through controlled experiments, emphasizing on the contribution of training data mixtures towards effective RL post-training. We found that Qwen2.5-3B-Instruct models demonstrate, when performing coldstart RL, demonstrates two distinct reasoning pattern: verbalized searching and backtracking that iteratively tries to find the correct answer, and latent thinking which solves the problem in one go. Further, we have empirical evidence demonstrating that, when initializing RL training from models that have seen incorrect demonstrations, training trajectories are still stable, only slightly shy of 9 the performance of models initialized from correct CoT data. Finally, through the construction of multiple synthetic datasets, we show evidence for positive correlation between problem difficulty and the need to increasing number of backtrack demonstration during SFT stage. Leveraging the characteristics of pretrain and SFT data to continually scale model performance through RL remains an intriguing future direction."
        },
        {
            "title": "6 Author contributions",
            "content": "Hongyi James Cai led the project, designed and ran the experiments, and wrote and edited the paper. Junlin Wang mentored Hongyi James Cai in terms of ideas, experiment setups, and debugging, as well as rewriting and editing significant proportion of the final paper. Xiaoyin Chen participated in discussions, helped with experiment setups and the high level structure of the paper. Bhuwan Dhingra advised this project and provided detailed feedback on the initial draft."
        },
        {
            "title": "References",
            "content": "[1] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models, 2025. URL https://arxiv.org/abs/2503.04548. [2] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv.org/abs/2501.17161. [3] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [4] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179. [5] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. Stream of search (sos): Learning to search in language, 2024. URL https://arxiv.org/abs/2404.03683. [6] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503.01307. [7] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.04519. [8] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion, 2023. URL https://arxiv.org/abs/ 2306.02561. 11 [9] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment, 2024. URL https://arxiv.org/abs/2410.01679. [10] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping, 2024. URL https://arxiv.org/abs/2402.14083. [11] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities, 2024. URL https://arxiv.org/abs/2403.04706. [12] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. [13] OpenAI. Learning to reason with llms. https://openai.com/index/ learning-to-reason-with-llms/, 2024. Accessed: 2025-03-21. [14] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [15] Du Phan, Matthew D. Hoffman, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A. Saurous. Training chain-of-thought via latent-variable inference, 2023. URL https://arxiv.org/abs/2312.02179. [16] Tian Qin, David Alvarez-Melis, Samy Jelassi, and Eran Malach. To backtrack or not to backtrack: When sequential search limits model reasoning, 2025. URL https://arxiv.org/ abs/2504.07052. [17] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve, 2024. URL https://arxiv.org/abs/2407. 18219. [18] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [19] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. [20] Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, and Azalia Mirhoseini. Archon: An architecture search framework for inference-time techniques, 2024. URL https://arxiv.org/abs/2409.15254. [21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [22] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. [23] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 12 [24] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. [25] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. [26] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [27] Open Thought. Reasoning gym. https://github.com/open-thought/reasoning-gym, 2025. Accessed: 2025-05-12. [28] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities, 2024. URL https://arxiv.org/abs/2406. 04692. [29] Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, and James Zou. Think deep, think fast: Investigating efficiency of verifier-free inference-time-scaling methods, 2025. URL https: //arxiv.org/abs/2504.14047. [30] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768. [31] Xiao-Wen Yang, Xuan-Yi Zhu, Wen-Da Wei, Ding-Chu Zhang, Jie-Jing Shao, Zhi Zhou, LanZhe Guo, and Yu-Feng Li. Step back to leap forward: Self-backtracking for boosting reasoning of language models, 2025. URL https://arxiv.org/abs/2502.04404. [32] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. [33] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465. [34] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. Quiet-star: Language models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/2403.09629. [35] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. [36] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning, 2025. URL https://arxiv.org/abs/2501.07301. 13 [37] Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining, 2025. URL https://arxiv.org/abs/2504.07912. [38] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning, 2024. URL https://arxiv.org/abs/2412.06559."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Hongyi James Cai would like to thank Junlin Wang for his detailed and invaluable mentorship throughout this project, and the amount of time spent on editing the final draft. All authors would like to thank the infrastructure team at Duke University, and Duke NLP group for providing the compute for this project."
        },
        {
            "title": "A Limitations",
            "content": "We perform our studies primarily on Qwen2.5-3B-Instruct, while using Qwen2.5-7B-Instruct and QwQ-32B as additional baselines due to compute constraint. While we primarily focus on the intervention through training mixtures, and the fact that Qwen family of models all exhibit similar behavioral patterns like \"wait\", we recognize performing the same post-training pipeline on larger models could reinforce our takeaways. We also recognize that our experiments are conducted on toy reasoning datasets, which presents gap to more challenging math problems. However, we emphasize our finding of knowledge transfer behavioral across tasks in Section 4.1, and believe our systematic analysis has profound implications."
        },
        {
            "title": "B Model training details",
            "content": "For RL, we investigate both PPO and GRPO. We use learning rate of 1e 6 for both actor and critic, KL control coefficient of 0 (no KL penalty), gradient clipping of 1.0, entropy coefficient of 0.001, value coefficient of 1.0, gae lambda of 1.0, gamma of 1.0. These hyperparamters are held constant across all runs, with the only difference being the response length to accommodate for problems with longer solution trajectories such as Sudoku. All experiments are done on 4 A100s."
        },
        {
            "title": "C Reward Setup",
            "content": "Countdown We expect solution to not include the equal sign Advanced Geometry For angle measure task: we expect solution string rounded to two decimal places with degrees symbols attached. Failure to include degree symbol, or not rounding to two decimal places will be deemed incorrect. For orthocenter task: we expect solution to give the coordinate either in the form (1.000 2.000) or (1.000, 2.000) or (1.000,2.000). Failure to include () or not rounding to 3 decimal places will be deemed incorrect. For incircular radius task: we expect solution string in the format of 1.123. Failure to round to 3 decimal places will be deemed incorrect. Arc 1D We expect solution string in the format of 1 2 3, other formats such as 1, 2, 3 will be deemed incorrect. Sudoku We expect solution string in the format of 1 2 3n4 5 6n7 8 9, other formats such as 1, 2, 3n4, 5, 6n7, 8, 9n will be deemed incorrect and only receive format score. Parsing includes generating list of lists from the solution string. Color Cube Rotation The solution parser is case insensitive. We expects solution string in the format of cyan or Cyan. Parsing includes lower case all letter. Zebra Puzzles We expect solution string in the format of bob or Bob. Parsing includes lower case all letters. List Functions We expect solution string either in the format of [1 2 3] or [1, 2, 3] or [1,2,3]. Failure to include [] will be deemed incorrect. Will parse to numpy array. Self Reference We expect solution in the format of single digit like \"4\". Parsing includes converting the solution string to an integer."
        },
        {
            "title": "D Example Model Outputs",
            "content": "D.1 List Functions Cold-start RL model outputs <im_start>assistant Let me solve this step by step. 15 <think> The rule in the examples seems to be reversing the order of the elements in the list. Lets apply this rule to the input list [74, 72, 49, 43, 41, 3, 30, 85, 48]. Reversing [74, 72, 49, 43, 41, 3, 30, 85, 48] gives us [48, 85, 30, 3, 41, 43, 49, 72, 74]. </think> <answer> [48, 85, 30, 3, 41, 43, 49, 72, 74] </answer><im_end> D.2 Countdown Cold-start RL model outputs <im_start>assistant Let me solve this step by step. <think> First, need to create an expression that equals 194 using the numbers 14, 17, 2, 45, and 8. Lets try some combinations. If add 45 and 17, get 62. Then if add 14 to 62, get 76. Adding 8 to 76 gives me 84. If add 2 to 84, get 86. need to reach 194, so need to try different combinations. Lets try 45 * 2 = 90, then 90 + 17 = 107, then 107 + 14 = 121, then 121 + 8 = 129. This doesnt work. need to find better combination. Lets try 17 * 8 = 136, then 136 + 45 = 181, then 181 + 14 = 195, then 195 - 2 = 193. Still not 194. need to find different approach. Lets try 2 * 14 = 28, then 28 + 45 = 73, then 73 + 17 = 90, then 90 + 8 = 98. Still not 194. need to find different combination. Lets try 45 * 2 = 90, then 90 + 14 = 104, then 104 + 17 = 121, then 121 + 8 = 129. Still not 194. need to find different approach. Lets try 17 * 14 = 238, then 238 - 45 = 193, then 193 + 8 = 201, then 201 - 2 = 199. Still not 194. need to find different combination. Lets try 14 * 2 = 28, then 28 + 45 = 73, then 73 + 17 = 90, then 90 + 8 = 98. Still not 194. need to find different approach. </think> RL model from shuffled synthetic data output <im_start>assistant Let me solve this step by step. <think> Step 1: 51 - 23 = 28. Step 2: 28 * 36 = 1008. Step 3: 1008 - 57 = 951. Step 4: 951 - 48 = 885. Wait, this doesnt lead to the correct solution. 885 is not the correct answer. Let me go back to step 2 and keep thinking from there. Step 3: 1008 - 27 = 981. Step 4: 981 - 48 = 933. This matches the problem statement. This is the solution. </think> <answer>51 - 36 * 36 - 57 - 48 - 27</answer><im_end> QwQ-32B evaluation results Table 3: Baseline accuracy of vanilla QwQ-32B on 8 different reasoning tasks AG CD ARC SDK CCR LF QwQ-32B 0.344 0. 0.240 0.000 0.135 0."
        },
        {
            "title": "F Short CoTs model evaluations",
            "content": "Table 4: Evaluation of Qwen2.5-3B-Instruct baseline and their coldstart RLed models. The row names are the model names (which task they have been RLed on), and the column names represent the evaluation tasks AG CD ARC SDK CCR ZP LF SR Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct AdvGeom Countdown Arc1D Sudoku ColorCube Zebra ListFunc SelfRef 0.015 0.052 0.309 0.043 0.001 0.012 0.149 0.076 0.120 0.000 0.004 0.019 0.019 0.479 0.010 0.002 0.019 0.019 0.015 0. 0.018 0.064 0.021 0.033 0.234 0.025 0.030 0.024 0.028 0.054 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.286 0.281 0.244 0.221 0.241 0.259 0.629 0.346 0.340 0.268 0.254 0.388 0.319 0.143 0.319 0.269 0.331 0.265 0.308 0.352 0.199 0.314 0.241 0.214 0.299 0.217 0.247 0.240 0.712 0.185 0.134 0.138 0.131 0.086 0.156 0.148 0.105 0.112 0.125 0. Figure 6: Correct and incorrect short CoTs RL model evaluation"
        },
        {
            "title": "G Response Length Comparison",
            "content": "Figure 7: Response length comparison between List Functions and Countdown"
        },
        {
            "title": "H Licenses",
            "content": "All models used (qwen models) are under Apache License Version 2.0. All assets released by this work can be freely used. All datasets used (reasoning gym) are under Apache License Version 2.0. All assets released by this work can be freely used."
        }
    ],
    "affiliations": [
        "Duke University",
        "Mila - Quebec AI Institute"
    ]
}