{
    "paper_title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
    "authors": [
        "Xuanming Zhang",
        "Yuxuan Chen",
        "Yuan Yuan",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 2 9 4 9 6 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Under Review",
            "content": "SEEKER: ENHANCING EXCEPTION HANDLING IN CODE WITH LLM-BASED MULTI-AGENT APPROACH Xuanming Zhang1,2 , Yuxuan Chen1 , Yuan Yuan3 , Minlie Huang1 1The CoAI Group, Tsinghua University 2ByteDance 3Beihang University {zhangxuanming.1}@bytedance.com {chenyuxu21}@mails.tsinghua.edu.cn {yuan21}@buaa.edu.cn {aihuang}@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "In real-world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open-source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real-world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, multi-agent framework inspired by expert developer strategies for exception handling. Seeker uses agentsScanner, Detector, Predator, Ranker, and Handlerto assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability."
        },
        {
            "title": "INTRODUCTION",
            "content": "In the era of code large-scale pre-trained language models (code LLMs) such as DeepSeek-Coder (Guo et al., 2024), Code-Llama (Rozi`ere et al., 2023), and StarCoder (Li et al., 2023), the functional correctness of code generation has become the main method for evaluating the quality of these models. For example, HumanEval(Chen et al., 2021) first proposed to let LLM generating code based on human-written natural language programming problem descriptions, and measured the models code generation ability based on the Pass@k metric of the rate times of generating that pass all test cases. In addition, CoderEval (Yu et al., 2024) and DevEval (Li et al., 2024a) introduced repo-level code generation tasks by sampling real code repositories according to distribution, while also hoped to evaluate the performance of code LLM in real development scenarios based on Pass@k and Acc@k metrics. As the functional correctness of code LLM continues to gain attention and improve, more research focuses on the reliable solutions of LLM-generated code to existing defects. For example, SWEbench (Jimenez et al., 2024) evaluates the ability of LLM to generate maintenance patch code based on real software issues on GitHub, while SecurityEval (Siddiq & Santos, 2022) uses 75 vulnerability types defined by CWE as prompts to induce LLM to generate vulnerable code, aiming to evaluate the jailbreak risk of LLM code generation. In terms of methods, He & Vechev (2023b) leverages property-specific continuous vectors based on high-quality code dataset to guide code generation *Equal contribution. Equal Advising."
        },
        {
            "title": "Under Review",
            "content": "(a) Our preliminary tendency. (b) schematic diagram of human developers who wellperformed in exception handling. Figure 1: Preliminary on exception handling performance by LLM and human. Prompt1, Prompt2, Prompt3 and Prompt4 in (a) indicate General prompting, Coarse-grained Knowledge-driven prompting, Fine-grained Knowledge-driven prompting and Fine-grained Knowledge-driven with handling logic prompting respectively towards the given property, achieving the repair of CWE vulnerabilities. In order to enhance the generalization of LLM itself for code vulnerable safety, Li et al. (2024c) explored the direction of fine-tuning LLM to generate code avoiding 11 common CWE vulnerabilities. Recently, Ren et al. (2023) conducted an in-depth study on the performance of LLM-generated code in code robustness represented by exception handling mechanisms, which opened up new explorations for LLM to predict and handle potential risks of generated code itself before vulnerability occurs. Although exception detection (and handling) techniques based on static analysis or neural networks have made great progress, little attention has been paid to the standardization of the mechanism, especially the private paradigms of various exception types including custom exceptions and exception rules in the long tail area. At the same time, we believe that interpretable and generalizable fragile code detection and exception handling strategies are crucial but underestimated attributes in real code development, thus the exception mechanism requires extremely high programming literacy from developers, which significantly affects the robustness of the code in the main peak area (especially Java projects), further affects the quality of the code LLM training data and the quality of the generated code. This paper explores these neglected aspects and raises research question: Do we need to enhance the standardization, interpretability and generalizability of exception handling in real code development scenarios? To the best of our knowledge, there is currently no work studying this issue. In order to thoroughly study the role of intuitive interpretability and rule generalization of exception mechanisms on human developers or LLMs in exception handling, we optimized and expanded the preliminary experiment conducted by Ren et al. (2023), and introduced four sets of prompts for human developers and LLMs based on 100 fragile Java code slices from real projects, namely Coarse-grained Reminding prompting, Fine-grained Reminding prompting, Fine-grained Inspiring prompting, and Fine-grained Guiding prompting, which successively added intuitive interpretability and rule generalization of exceptions to change the in-context learning of code writers. Through four sets of in-context learning in parallel with two objects, we found consistent phenomenon: the code generated by the Fine-grained Guiding prompt has great performance of exception handling, while the lack of intuitive interpretability (specific exception type, current code scenario) or rule generalization (exception handling strategies) will reduce the exception handling performance, as shown in figure 1(a). Figure 1(b) explores the Chain-of-Thought used by senior human developers under the Fine-grained Guiding prompt. It is worth noting that compared with common exception types such as IOException and NullPointerException, some rare exceptions such as BrokenBarrierException, AccessControlException also cause high program risks, but are not well handled in low-level experiments. In"
        },
        {
            "title": "Under Review",
            "content": "addition, we observed that good exception handling practices pay more attention to the specificity of exceptions and tend to accurately capture exception types moving down the class hierarchy. For example, the exception SQLClientInfoException inherits the properties of its parent class SQLException. Capturing SQLClientInfoException will provide additional information about the error by obtaining detailed information about the SQL client properties, beyond what the superclass SQLException provides. This is based on the fact that each exception is an object, and exceptions thrown at lower level can also be caught by its superclass, but Exception handlers that are too general can make code more error-prone by catching and handling exceptions that were not anticipated by the programmer and for which the handler was not intended. Osman et al. (2017) further demonstrates that capturing accurate fine-grained exceptions can help developers quickly identify the source of the problem, effectively improve the readability and maintainability of the code, and avoid mishandling different types of errors. However, due to the lack of good handling paradigm experience for long-tail, domain-specific, or customized exception types, combining with the complex inheritance relationship and the multi-pattern of exception handling, it is still challenging to accurately achieve this goal. In order to improve the robustness of the code by leveraging the best exception handling practices of senior human developers, we propose method called Seeker, which disassembles the Chainof-Thought of senior human developers and divides the exception mechanism into five tasks, which are respectively handled by Scanner, Detector, Predator, Ranker, and Handler agents. We combine large amount of trusted external experience documents with exception practices to build Common Exception Enumeration (CEE) to retrieve and enhance the detection, capture, and handling tasks where the original LLM performs poorly. This method can be easily integrated into the existing code LLM in aim to generate highly robust code, among with CEE has promising community contribution and maintenance value which helps developers further understand the ideal practice of exception mechanisms. After adopting the high-concurrency interface we designed, the additional computing time overhead is constant when facing any level of code volume, also totally controllable in complexity. However, still taking Java exceptions as an example, even if only the built-in exception types are considered, the exception relationship inheritance tree contains 433 nodes, 62 branches, and 5 layers. Directly building documents and calling either LLM or human developer for retrieval may degrade performance because it may not be able to distinguish the exception specificity on the same branch for node selection, cannot adapt to multiple handling patterns of exceptions, and the cost of each query round is very high. To solve this problem, deep retrieval-augmented generation(Deep-RAG) algorithm for complex inheritance relationships is proposed as an improved alternative to traditional RAG. Specifically, development scenario label is assigned to each branch according to the inheritance relationship to identify several exception branches that may correspond to piece of fragile code. The few-sample verification step provides detection pass rate and capture accuracy feedback after the automatic generation of labels, and then fine-tunes the specific granularity and general description of the labels based on the regularization prompts of failed samples. This can identify the risk scenarios where fragile codes are located and the corresponding exception branches that are triggered, and then selectively perform node evaluation on these branches by depth, ultimately improving retrieval performance and overhead. large number of experiments show that the proposed Seeker method helps LLM optimize or generate highly robust code, further improving the performance of LLM in various code tasks."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "In this section, we study how the standardization, interpretability, and generalizability of exceptions affect the exception handling performance of code developers and determine the mitigation effect of poor exception handling. To achieve this, we conduct extensive comparative experiments by controlling the standardization of exception types, the interpretability of risk scenarios, and the generalization of handling strategies, respectively, applying the four sets of in-context learning prompt proposed in figure 4 and 5 (i.e., Coarse-grained Reminding prompting, Fine-grained Reminding prompting, Fine-grained Inspiring prompting, and Fine-grained Guiding prompting). Specifically, based on the preliminary exploration of Ren et al. (2023), we screened several wellmaintained codebases, combined manual and automatic code reviews to filter out high-quality also"
        },
        {
            "title": "Under Review",
            "content": "important exception handling therefore obtain the fragile code that is in serious situation in real development scenarios. Then we allowed code developers to familiarize with these filtered codebases and record the methods and processes they used when handling exceptions. In order to reduce the difficulty of the entire task and simulate the developers thought about exception handling during the development process, we set up four prompt links to provide developers with progressive exception handling information. The implementation results can be found in figure 1(a). The comparative experiment reveals an interesting phenomenon: prompts without effective guidance information are not helpful for both human developers and LLMs, while adding type normative information about exception mechanisms will slightly improve developers vague perception of the source of code fragility, but cannot accurately locate and handle them due to the unfamiliarity with the exception, which is easy to cause insensitive detection. Increasing the interpretability information of the development scenario will greatly improve developers understanding of the code itself and potential fragility, which is beneficial to the accuracy of exception capture. Increasing the generalization information of handling strategies further improves developers ability to analyze the source of fragility and improve the quality of handling block. The phenomenon that the above information bring significant gains in exception handling tasks is called the mitigation effect. This phenomenon answers the research questions raised in Section 1 by revealing the mitigation effect by specific prompt information, impacting the quality of code developers exception handling practices. It also inspires the proposed Seeker method to combine external document information to align the generated prompts with fine-grained guidance standards. In addition, Section 3.2 provides reasonable explanation for the occurrence of the mitigation effect, providing data and insights on the effectiveness of the proposed method. We believe that our findings can provide valuable insights for future research related with reliable code generation, laying the foundation for potential RAG code agent progress."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we introduce the proposed Seeker method. We first review the historical observations of developers on exception handling issues, and then introduce three exception handling pitfalls, Insensitive-Detection of Fragile Code, Inaccurate-Capture of Exception Type and Distorted-Solution of Handling Block. Finally, we introduce the methods dependency construction and the entire method. 3.1 REVISIT OF HUMAN EMPIRICALS Over the years, there have been numerous empirical studies and practical discussions on exception handling, but what is common is that exception handling has been repeatedly emphasized as an important mechanism directly related to code robustness. Nakshatri et al. (2016) points out that exception handling is necessary and powerful mechanism to distinguish error handling code from normal code, so that the software can do its best to run in normal state. Weimer & Necula (2004) points out that the exception mechanism ensures that unexpected errors do not damage the stability or security of the system, prevents resource leakage, ensures data integrity, and ensures that the program still runs correctly when unforeseen errors occur. In addition, Jacobs & Piessens (2009) points out that exception handling also involves solving potential errors in the program flow, which can mitigate or eliminate defects that may cause program failure or unpredictable behavior. Although the exception mechanism is an important solution to code robustness, developers have always shown difficulties in dealing with it due to its complex inheritance relationship and processing methods. de Padua & Shang (2017) points out that various programming language projects show long-tail distribution of exception types when facing exception handling, which means that developers may only have simple understanding of the frequently occurring exception types. However, according to section1, good exception practices rely on developers to perform fine-grained specific capturing. Nguyen et al. (2020b) also points out multi-pattern effect of exception handling. For example, even for peer code, capturing different exception types will play different maintenance functions, so exception handling is often not generalized or single-mapped. These complex exception mechanism practice skills have high requirements for developers programming literacy. de Sousa et al. (2020) manually reviewed and counted the exception handling of large number of open source projects, and believed that up to 62.91% of the exception handling blocks have vio-"
        },
        {
            "title": "Under Review",
            "content": "lations such as capturing general exceptions and destructive wrapping. This seriously violates the starting point of the exception mechanism. de Padua & Shang (2017) emphasizes the urgent need and importance of automated exception handling suggestion tools. The failure of human developers in the exception handling mechanism seriously affects the quality of LLMs code training data (He & Vechev (2023a)), which further leads to LLMs inability to understand the usage skills of maintenance functions (Wang et al. (2024)). To solve the above problems, we first proposed SeekerJava for the Java language. This is because the Java language has more urgent need for exception handling and is completely mapped to the robustness of Java programs. Ebert et al. (2020) pointed out that as fully object-oriented language, Javas exception handling is more complex than other languages, and it has higher degree of integration into language structures. Therefore, Java projects are more seriously troubled by exception handling bugs. In addition, Java relies heavily on exceptions as mechanism for handling exceptional events. In contrast, other languages may use different methods or have less strict exception handling mechanisms. It is worth mentioning that Seekers collaborative solution based on an inherent multi-agent framework plus an external knowledge base, they can quickly migrate multiple languages by maintaining documents for different languages. We will also maintain Seeker ython and Seeker C# in the future to provide robustness guarantees for the development of more programming languages. 3.2 RULES OF GOOD PRACTICE In this section, we introduce four prompt settings: Coarse-grained Reminding prompting, Finegrained Reminding prompting, Fine-grained Inspiring prompting and Fine-grained Guiding prompting, which can be used to demonstrate the mitigation effect of bad practices on developers when facing exception handling tasks. For Coarse-grained Reminding prompting, we use pay attention to potential exceptions to remind developers of the exception mechanism, and let developers find the fragile parts of the target code slice and handle them according to their own practical experience. As shown in figure 1(a) , figure 4 and figure 5, although developers will consciously start screening for exception handling, given the difficulties mentioned in Section 3.1, both humans and LLM developers are very insensitive to identifying fragile code. Ren et al. (2023) also found this phenomenon and summarized this series of bad practices as Incorrect exception handling. For Fine-grained Reminding prompting, we provide developers with fine-grained reminders of specific exception types based on the fragile code scenario, and let developers understand the source of code fragility and handle it in standardized manner based on the exception. Although developers will consciously learn from external documents or examples, the information in these documents is often too abstract to be interpreted, and as for the examples, most of the time there is no standardized quality assurance or generalization. Therefore, developers tend to catch exceptions inaccurately, and do not fundamentally solve the potential risks of the program. Related studies have shown that the bad practice of Abuse of try-catch often appears in this experimental benchmark. For Fine-grained Inspiring prompting, we additionally provide code-level scenario analysis of the fragile code. Although developers still rely on their own understanding of the code, the intuitive and interpretable natural language significantly improves developers insight and analysis capabilities for exceptions in this scenario. Related studies also show that for standalone function-level fragile code optimization, this experimental settings can achieve relatively stable good exception handling practices. However, in the face of real development scenarios with complex dependencies, how to generate high-quality handling blocks with generalization is still challenge. Zhang et al. (2023) pointed out that exception handling code is prone to errors in real projects. For Fine-grained Guiding prompting, we additionally give generalized handling strategy for the exception. Based on the stable exception detection performance of the above experimental benchmarks, developers finally achieve high-quality exception handling practices. de Padua & Shang (2017) also strongly recommended that developers should use generalizable exception handling strategies, because it is difficult for developers to perform higher-quality optimization before fully mastering the information of an exception type. In essence, these four prompt settings can be regarded as information progression for exception type standarization, fragile interpretability, and handling generalization, thereby changing the developers in-context learning. By changing the prompts, the robustness of the code generated by the developer will be affected, thereby affecting the quality of the final project. Note that the four sets of prompt we proposed can be applied to any code-based in-context learning, thereby promoting research on the impact of prompt specifications on LLM code generation performance."
        },
        {
            "title": "Under Review",
            "content": "Figure 2: Distribution of Exception Type. Human practice may be far from good practice, thus we conduct data and info processing to align user distribution to good practice. Figure 3: Seeker Work Flow. The workflow consists of four agents: Planner, Detector, Ranker, and Handler, collaborating to manage exception handling in code. The color circle indicates the info passing along the pipeline or used by agents. Note that for most programming languages, there are three ways to handle exceptions. Exceptions thrown using throws keyword in the method signature, Exceptions thrown using throw keyword in the method body, and Exceptions caught in try-catch block of method. Nakshatri et al. (2016) points out that the first method may not provide the real situation, because the exceptions thrown using throws in the method signature will be incorrectly added to the methods call stack, thereby propagating the exception until it is caught. In addition, the exceptions thrown using the second method will eventually be caught by the caller using try catch block. Therefore, the third method is the most efficient and common exception practice. In our method, we only take the third exception handling way as the best practice when optimize the target. 3.3 THE RAG-AGENT METHOD To enhance the standardization, interpretability, and generalizability of exception handling in real code development scenarios, we propose method called Seeker. Seeker disassembles the chainof-thought processes of senior human developers and divides the exception mechanism into five"
        },
        {
            "title": "Under Review",
            "content": "specialized tasks, each handled by dedicated agent: lanner, Detector, redator, Ranker, and Handler. By integrating large amount of trusted external experience documents with exception practices, we build the Common Exception Enumeration (CEE). CEE is comprehensive and standardized document providing structured and exhaustive repository of exception information, encompassing scenarios, properties, and recommended handling strategies for each exception type. The foundation of CEE is detailed in AppendixA.1.2. With the help of CEE, Seeker retrieves and enhances the detection, capture, and handling tasks where the original LLM performs poorly. This method can be easily integrated into existing code LLMs to generate highly robust code, and CEE has promising community contribution and maintenance value, helping developers further understand the ideal practices of exception mechanisms. Algorithm 1: Seeker Framework Input: Codebase Output: Optimized code with robust exception handling 1 Segment the codebase into manageable units = {u1, u2, . . . , uN }; 2 foreach code segment ui in do 3 if (length of ui is within predefined limit) and (function nesting level is low) and (logical flow is clear) then Add ui to ; 4 5 Initialize optimized units = {}; 6 foreach unit ui in do // Detection Phase Initialize potential exception set Ei = {}; Use the Detector agent to analyze unit ui; In parallel do { // Static Analysis Generate control flow graph CF Gi and exception propagation graph EP Gi for ui; Identify sensitive code segments Sstatic // Scenario and Property Matching Perform scenario and property matching on ui; Identify sensitive code segments Smatch = {smatch i1 } Combine sensitive code segments: Si = Sstatic foreach segment sij in Si do , . . . } in ui; ; , smatch i2 Smatch , . . . } in ui; = {sstatic , sstatic i1 Detect potential exception branches Ebij in sij; Ebi Ebi Ebij; // Retrieval Phase Use the Predator agent to retrieve fragile code and try-catch blocks; Summarize unit ui at the function level to obtain code summary Fi; Perform Deep-RAG using Fi and exception branches Ebi, get exception nodes Eni; Mapping relevant exception handling strategies Hi = {hi1, hi2, . . . } from CEE; // Ranking Phase Use the Ranker agent to assign grades to exceptions in Eni; foreach exception eik in Eni do Calculate exception likelihood score lik based on eik attribute and impact; Calculate suitability score uik of handling strategy hik; Compute overall grade gik = α lik + β uik; Rank exceptions in Eni based on grades gik in descending order to get ranked list // Handling Phase Use the Handler agent to generate optimized code i; foreach exception eik of ni if gik > γ do ni; Mapping handling strategy hik from Hi; Apply hik to code segment(s) related to eik in ui; 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 {u i}; 32 33 Combine optimized units to produce the final optimized code ; Generally, given piece of code, we first use planner agent to segment it into manageable units such as function blocks, class blocks, and file blocks. The planner employs thoughtful approach"
        },
        {
            "title": "Under Review",
            "content": "to segmentation by considering factors such as the overall code volume, dependency levels, and requirement relationships. This strategy helps mitigate the pressure on processing, particularly regarding context window limitations and complex dependency chains, ensuring that no single unit overwhelms the analysis agents. By balancing the granularity of segmentation, we can avoid overly fine divisions that may introduce high complexity, thus maintaining clarity and efficiency in handling large and intricate codebases. For the Detector agent, it simultaneously performs scenario and property matching alongside static analysis to identify fragile areas in the code that are likely to lead to errors or crashes. These two approaches run in parallel, each contributing their strengths to the detection process. Scenario and property matching offers shallow-level analysis, capturing vulnerabilities based on semantic cues and contextual scenarios that static analysis might overlook due to its challenges in achieving high coverage for exception handling issues. Conversely, static analysis excels in uncovering complex dependencies and deep-level defects, providing insights that shallow analysis may miss. By combining the results from both methodstaking their unionthe Detector agent covers both shallow and deep-level risks, effectively detecting potential exceptions with equal consideration for long-tail, domain-specific, or customized exception types. However, as discussed in section 1, detecting exceptions without considering the complex inheritance relationships between exception types may not yield optimal results, as it could lead to inaccurate exception specificity in the exception hierarchy. Therefore, it is necessary to incorporate external knowledge to guide the capture and analysis processes. To achieve this, we integrate the CEE into the redator agent. Similar to RetrievalAugmented Generation (RAG) models, the redator agent summarizes the code at the function level and queries the CEE for relevant exception attributes. It performs multi-layered deep searches to retrieve information that can be applied to the detected issues, providing valuable context for exception handling. Crucially, during few-shot testing phases, the environment supplies feedback on both the accuracy and coverage of the retrieved information. This feedback is integral to the agents learning process, enabling it to refine its search strategies and improve the relevance of the information it retrieves. We propose Deep Retrieval-Augmented Generation (Deep-RAG) algorithm to handle the complex inheritance relationships in exception types as further detailed in Appendix A.1.1. By combining the outputs from the Detector and redator agents, the Ranker assigns grades to the detected exceptions based on their likelihood and the suitability of the handling strategies retrieved from the CEE. This grading system ensures that Seeker prioritizes the most critical exceptions for immediate handling. The Ranker considers factors such as the likelihood of the exception occurring, the potential impact on the program, and the specificity of the exception type within the inheritance hierarchy. It gives feedback to Detector and redator agents along with the node selection steps through score ranking and judge, ensuring the agents learning from the actual code environment. Analyzing the ranked exceptions, the Handler agent generates optimized code that incorporates robust handling strategies. It utilizes templates and logic patterns derived from the CEE to ensure that the generated code is functionally correct. The Handler focuses on capturing accurate finegrained exceptions, moving down the class hierarchy to provide additional information about errors, beyond what the superclass exceptions provide. This approach helps developers quickly identify the source of the problem, effectively improve the readability and maintainability of the code, and avoid mishandling different types of errors. However, integrating such comprehensive exception handling mechanism introduces challenges in computational overhead, especially when dealing with large number of exception types and complex inheritance relationships. To address this, we designed high-concurrency interface that keeps the additional computing time overhead constant, regardless of the code volume level. This ensures that the method is scalable and the complexity is controllable when facing any codebase size. We discuss the time costs of Seeker in detail in Appendix A.2.3."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate the performance of our proposed method, Seeker, on the task of exception handling code generation. We compare our approach with the state-of-the-art method KPC (Ren et al., 2023), traditional Retrieval-Augmented Generation (RAG), and General Prompting methods. To comprehensively assess the effectiveness of our method, we employ six metrics: 1. Automated Code Review Score (ACRS) Based on an automated code review model, this metric evaluates the overall quality of the generated code in terms of adherence to coding standards and best practices. ACRS = CodeReviewModel(GeneratedCode) Explanation: higher ACRS indicates better code quality, reflecting well-structured and maintainable code. 2. Coverage (COV) This metric measures the coverage of sensitive code detected by the Detector agent compared to the actual sensitive code. COV = Correct Detected Sensitive Code Actual Sensitive Code Explanation: It quantifies the proportion of actual sensitive code that our method successfully detects. Over-detection (marking more code than necessary) is not penalized. 3. Coverage Pass (COV-P) This metric assesses the coverage relation between the try-blocks detected by the Predator agent and the actual code that requires try-catch blocks. COV-P = Correct Try-Blocks Actual Try-Blocks Explanation: try-block is considered correct if it exactly matches the actual code lines. Overmarking or under-marking is counted as incorrect. Over-detection is penalized in this metric by including the incorrectly detected try-catch blocks in the denominator while counting them as incorrect (zero) in the numerator, thus reducing the overall Coverage Pass score. 4. Accuracy (ACC) This metric evaluates the correctness of the exception types identified by the Predator agent compared to the actual exception types. ACC = Correct Exception Types Total Exception Types Identified Explanation: An exception type is considered correct if it matches the actual exception or is reasonable subclass of the actual exception type. 5. Edit Similarity (ES) This metric computes the text similarity between the generated try-catch blocks after processing by the Handler agent and the actual try-catch blocks. ES = Similarity(Generated Try-Catch, Actual Try-Catch) Explanation: We use the Levenshtein distance to measure similarity. higher ES indicates that the generated code closely matches the actual code."
        },
        {
            "title": "Under Review",
            "content": "6. Code Review Score (CRS) This metric involves submitting the generated try-catch blocks to GPT-4o for evaluation. The language model provides binary assessment: good or bad. CRS = Good Evaluations Total Evaluations Explanation: CRS reflects the proportion of generated exception handling implementations that are considered good according to engineering best practices. We conducted experiments using GPT-4o as the agents internal large model. Our dataset consists of 750 fragile Java code snippets extracted from real-world projects, following the rule as shown in Appendix A.2.1. We compare our method against KPC (Ren et al., 2023), traditional RAG, and General Prompting methods. The performance comparison is presented in Table 1. Table 1: Comparison of Exception Handling Code Generation Methods"
        },
        {
            "title": "Method",
            "content": "ACRS COV (%) COV-P (%) ACC (%) ES CRS (%) General Prompting Traditional RAG KPC Ren et al. (2023) Our Method 0.21 0.35 0.26 0.85 13 35 14 9 31 11 81 8 29 8 79 0.15 0.24 0.17 0.64 24 31 27 92 As shown in Table 1, our method outperforms the baselines across all metrics. Specifically, we achieve: - higher ACRS, indicating superior overall code quality. - Greater Coverage (COV) and Coverage Pass (COV-P), demonstrating our methods effectiveness in detecting and correctly wrapping sensitive code regions. - Higher Accuracy (ACC) in identifying the correct exception types, including recognizing subclass relationships. - An improved Edit Similarity (ES), showing that our generated code closely matches the actual exception handling code. - higher Code Review Score (CRS), confirming that our implementations are more frequently deemed good by the LLM reviewer. Our methods superior performance can be attributed to several factors: 1. Comprehensive Exception Knowledge: By incorporating the Common Exception Enumeration (CEE), our method benefits from extensive exception scenarios, properties, and handling logic, enabling more accurate detection and handling. 2. Specialized Agent Framework: The Seeker framework decomposes the task into specialized agents (Scanner, Detector, Predator, Ranker, Handler), each focusing on specific aspects, leading to improved overall performance. 3. Integration of Best Practices: Leveraging trusted external documents and industry best practices ensures that the generated code adheres to high standards, improving both quality and maintainability. We further evaluate our method using different open-source and closed-source model, which is detailed in AppendixA.2.4. Our experiments demonstrate that Seeker achieves state-of-the-art performance in exception handling code generation. By effectively combining comprehensive exception knowledge with specialized agent framework, our method addresses the complexities of exception handling in code generation. The superior performance across all metrics highlights the importance of integrating domain-specific knowledge and best practices into code generation models."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we extend the study of the impact of prompt specifications on the robustness of LLM generated code. We conduct extensive comparative experiments using four sets of prompt settings"
        },
        {
            "title": "Under Review",
            "content": "and further confirm the mitigating effect of developers poor exception handling practices. To exploit this phenomenon, we introduce the Seeker method, multi-agent collaboration framework that provides LLM with the prompt information required for mitigation effects with the support of CEE documents and Deep-RAG algorithms. The upper bound model achieves SOTA performance on exception handling tasks. In general, Seeker can be integrated into any base model, extended to multiple programming languages, and even generalized to knowledge analysis and reasoning of general inheritance relations, such as requirements engineering A.3. We hope that our findings and proposed methods can provide new insights and promote future research in these areas. The source code of this paper is available at https://github.com/XMZhangAI/Seeker."
        },
        {
            "title": "REFERENCES",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Clade. 2023. URL https://www.anthropic.com/index/claude-2. Guilherme B. de Padua and Weiyi Shang. Revisiting exception handling practices with exception flow analysis. In SCAM, 2017. Dˆemora Bruna Cunha de Sousa, Paulo Henrique M. Maia, Lincoln S. Rocha, and Windson Viana. Studying the evolution of exception handling anti-patterns in long-lived large-scale project. J. Braz. Comput. Soc., 2020. Felipe Ebert, Fernando Castor, and Alexander Serebrenik. reflection on an exploratory study on exception handling bugs in java programs. In SANER, 2020. GPT-3. 2022. URL https://platform.openai.com/docs/models/gpt-base. GPT-3.5. 2023. URL https://platform.openai.com/docs/models/gpt-base. GPT-4. 2023. URL https://platform.openai.com/docs/models/gpt-3-5. GPT-4o. 2024. URL https://platform.openai.com/docs/models/gpt-4o. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, et al. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Jingxuan He and Martin T. Vechev. Large language models for code: Security hardening and adversarial testing. In CCS, 2023a. Jingxuan He and Martin T. Vechev. Large language models for code: Security hardening and adversarial testing. In CCS, 2023b. Bart Jacobs and Frank Piessens. Failboxes: Provably safe exception handling. In ECOOP, 2009. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, et al. Deveval: manually-annotated code generation benchmark aligned with real-world code repositories. In ACL(Findings), 2024a. Junjie Li, Fazle Rabbi, Cheng Cheng, Aseem Sangalay, Yuan Tian, and Jinqiu Yang. An exploratory study on fine-tuning large language models for secure code generation. arXiv preprint 2408.09078, 2024b. Junjie Li, Aseem Sangalay, Cheng Cheng, Yuan Tian, and Jinqiu Yang. Fine tuning large language model for secure code generation. In FORGE, 2024c."
        },
        {
            "title": "Under Review",
            "content": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! TMLR, 2023. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In ICLR, 2024. Suman Nakshatri, Maithri Hegde, and Sahithi Thandra. Analysis of exception handling patterns in java projects: an empirical study. In MSR, 2016. Tam Nguyen, Phong Vu, and Tung Nguyen. Code recommendation for exception handling. In ESEC/FSE, 2020a. Tam Nguyen, Phong Vu, and Tung Nguyen. Code recommendation for exception handling. In ESEC/FSE, 2020b. OpenAI o1. 2024. URL https://platform.openai.com/docs/models/o1. Haidar Osman, Andrei Chis, Jakob Schaerer, Mohammad Ghafari, and Oscar Nierstrasz. On the evolution of exception usage in java projects. In SANER, 2017. Xiaoxue Ren, Xinyuan Ye, Dehai Zhao, Zhenchang Xing, and Xiaohu Yang. From misuse to mastery: Enhancing code generation with knowledge-driven AI chaining. In ASE, 2023. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Mohammed Latif Siddiq and Joanna C. S. Santos. Securityeval dataset: Mining vulnerability examples to evaluate machine learning-based code generation techniques. In MSR4P&S, 2022. Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, and Zibin Zheng. Beyond functional correctness: Investigating coding style inconsistencies in large language models. arXiv preprint 2407.00456, 2024. Westley Weimer and George C. Necula. Finding and preventing run-time error handling mistakes. In OOPSLA, 2004. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. Codereval: benchmark of pragmatic code generation with generative pre-trained models. In ICSE, 2024. Hao Zhang, Ji Luo, Mengze Hu, Jun Yan, Jian Zhang, and Zongyan Qiu. Detecting exception handling bugs in C++ programs. In ICSE, 2023. Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Yanjun Pu, and Xudong Liu. Learning to handle exceptions. In ASE, 2020. Yifan Zhang, Yang Yuan, and Andrew Chi-Chih Yao. On the diagram of thought. arXiv preprint 2409.10038, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 METHOD DETAILS A.1.1 DEEP-RAG ALGORITHM Algorithm 2: Deep Retrieval-Augmented Generation (Deep-RAG) Input: Knowledge hierarchy tree , unit summary Fi, detected queries Qi, environment context Env Output: Relevant information retrievals Ri 1 Initialize relevant knowledge branches set = {}; 2 Assign knowledge scenario labels = {l1, l2, . . . } to branches of ; 3 foreach query qik in Qi do Identify branches Bik in related to qik based on labels L; Bik; 6 foreach branch bm in do 5 // Verification Step Select few-sample document examples Xm = {xm1, xm2, . . . } associated with branch bm; foreach example xmj in Xm do Perform query matching to obtain pass rate pmj and capture accuracy amj; if pmj or amj below threshold θ then Record failure pattern pmj based on Env; Update environment context Env with pmj; Compute average pass rate pm and accuracy am for branch bm; if pm or am below threshold θ then Fine-tune labels for branch bm based on aggregated feedback from Env; 16 Initialize information retrievals set Ri = {}; 17 foreach branch bm in do 18 Select depth level for node evaluation; for = 1 to do foreach node nml at depth in branch bm do Evaluate relevance score rml to summary Fi and queries Qi; if rml > δ then Retrieve information rml from knowledge base; Ri Ri {rml}; In the Deep-RAG algorithm, we assign development scenario labels to each branch of the exception inheritance tree based on their inheritance relationships, enabling the identification of branches that may correspond to specific information of fragile code segments. Acting as an intelligent agent, the algorithm interacts dynamically with its operational environment by leveraging feedback from detection pass rates and capture accuracies obtained during the few-shot verification step. This feedback mechanism allows the system to refine the granularity and descriptions of the scenario labels through regularization prompts derived from failed samples. As result, Deep-RAG can accurately identify the risk scenarios where fragile codes are located and the corresponding knowledge branches that are activated. Subsequently, the algorithm selectively performs node evaluations on these branches by depth, thereby enhancing retrieval performance and optimizing computational overhead. Additionally, we have designed the algorithm interface to be highly general, ensuring its applicability across wide range of RAG scenarios beyond exception handling. This generality allows Deep-RAG to support diverse applications, as further detailed in Appendix A.3. By integrating environmental feedback and maintaining flexible, agent-based interaction model, Deep-RAG not only improves retrieval accuracy and efficiency but also adapts seamlessly to various domains and information retrieval tasks, demonstrating its versatility and robustness in enhancing the performance of large language models. 13 7 8 9 11 12 13 14 15 20 21 22"
        },
        {
            "title": "Under Review",
            "content": "A.1.2 COMMON EXCEPTION ENUMERATION In this section, we introduce the framework for constructing the CEE, which serves as foundational resource for enhancing the reliability of exception handling in code generation by developers. Without comprehensive and standardized document like CEE, developers may struggle to accurately detect and handle these exceptions, leading to either overly generic or improperly specific exception management. CEE addresses these challenges by providing structured and exhaustive repository of exception information, encompassing scenarios, properties, and recommended handling strategies for each exception type. The construction of CEE is guided by three essential rules, each aimed at addressing the complexities of exception management within Java development. First and foremost, we establish robust standard documentation base, drawing from the Java Development Kit (JDK) to identify and compile comprehensive set of exception nodes and their descriptions. This foundational layer comprises total of 433 nodes, organized into 62 branches and spanning five layers within the Java exception hierarchy. By utilizing the standardized documentation from the JDK, we ensure that the CEE is grounded in official, authoritative sources, providing reliable reference point for exception handling practices. Next, we enhance the CEE by integrating insights from realworld human practices. This involves gathering range of resources, including enterprise-level Java development documentation and analyzing mature open-source Java projects hosted on platforms like GitHub. By examining exemplary Java code, particularly focusing on effective exception handling practices, we can enrich each exception node in the CEE with detailed contextual information. Specifically, we define three key components for each exception node: Scenario, Property, and Handling Logic. Scenario: This component describes the specific coding situations or environments in which an exception is likely to occur. By analyzing real-world applications and common coding patterns, we can create realistic scenarios that help developers understand when to anticipate particular exceptions. This contextual understanding is critical for effective exception handling, as it allows developers to write more accurate and responsive code. Property: This aspect outlines the characteristics and attributes of each exception. Understanding the properties of an exception, such as its severity, possible causes, and the context of its occurrence, they are vital for appropriate handling. This detailed information allows developers to make informed decisions on how to respond to exceptions based on their inherent properties. Handling Logic: For each exception node, we define best practices for handling the exception. This includes recommended coding strategies, such as specific try-catch blocks, logging mechanisms, and fallback strategies. By incorporating proven handling logic derived from both successful enterprise practices and open-source contributions, we provide comprehensive guide that assists developers in implementing effective exception management. The third rule emphasizes the need for fine-grained control over the matching and handling of exceptions through the use of few-shot samples. To ensure that the CEE maintains high accuracy in matching exceptions with the appropriate handling logic, we establish testing framework comprising variety of small-scale testing libraries. These libraries are designed to cover wide range of exceptions, providing high coverage rates for various scenarios. We leverage the CEE in conjunction with these testing libraries to conduct detailed evaluations of exception matching. By analyzing the performance of the CEE in identifying and matching exceptions, we can identify instances of false positives (incorrect matches) and false negatives (missed matches). Based on this analysis, we iteratively refine the information associated with each exception node, adjusting the granularity of the descriptions until we achieve high accuracy in matching rates. This continuous feedback loop allows us to optimize the CEE for real-world application, ensuring that developers can rely on it to provide accurate and contextually relevant exception handling guidance. By adhering to these rules, the CEE is positioned as powerful resource that enhances the quality of exception handling in code generated by LLMs. The combination of authoritative documentation from the JDK, insights from real-world practices, and rigorous testing mechanisms creates comprehensive framework that not only improves the robustness of generated code but also empowers developers with the knowledge and tools they need to manage exceptions effectively. It is worth mentioning that CEE, as knowledge base, has the value of free expansion and supporting community contributions. We will"
        },
        {
            "title": "Under Review",
            "content": "continue to be responsible for the version updates and iterations of CEE. An excerpt sample of CEE can be found in Appendix A.2.2 A.2 EXPERIMENTAL DETAILS A.2.1 DATASETS To ensure the quality and representativeness of the dataset, we carefully selected projects on GitHub that are both active and large in scale. We applied stringent selection criteria, including the number of stars, forks, and exception handling repair suggestions in the project (Nguyen et al., 2020b), to ensure that the dataset comprehensively covers the exception handling practices of modern opensource projects. By automating the collection of project metadata and commit history through the GitHub API, and manually filtering commit records related to exception handling, we have constructed high-quality, representative dataset for exception handling that provides solid foundation for evaluating Seeker. Table 2: The Excerpt Data source Repo Commits Stars Forks Issue Fix Doc Under Maintenance Anki-Android AntennaPod connectbot FairEmail FBReaderJ FP2-Launcher NewsBlur Launcher3 Lawnchair-V1 MozStumbler 18410 6197 1845 30259 7159 1179 19603 2932 4400 1727 8500 6300 2480 3073 1832 25 6800 91 93 2200 1400 629 640 802 2 995 642 43 212 262 295 321 N/A 248 16 158 2 394 203 N/A Y N/A Y N/A N/A N/A We quantify the quality of datasets in the context of code generation and exception handling using multiple dimensions, encompassing project popularity, community engagement, codebase quality, security posture, documentation integrity and dynamic maintenance. To provide holistic assessment, we propose Composite Quality Metric (CQM) that aggregates these dimensions into single quantitative indicator. Open source code repositories that perform well under this metric enter our semi-automated review process to screen high-quality exception handling blocks for few-shot, CEE building, or testing. To avoid data leakage, we also performed round of variations on the test set. Considering that our method does not directly rely on data but fully utilizes the LLMs ability to understand and reason about code, the evaluation results are consistent with our predictions, and the impact of data leakage on the credibility of our method is negligible. A.2.2 PROMPT AND DOCUMENT CEE Prompt genscenario = \"\"\"Below is kind of exception in java. Please according to the sample discription of scenario of errortype, provide scenario description of the exception in java just like the sample description.Please note that the granularity of the scenario descriptions you generate should be consistent with the examples. [Sample Description] {sample_desc} [Exception] {ename}"
        },
        {
            "title": "Under Review",
            "content": "Note you should output in the json format like below, please note that the granularity of the scenario descriptions you generate should be consistent with the examples.: {{ \"scenario\": ... }} \"\"\" genproperty = \"\"\"Below is kind of exception in java and its scenario description. Please according to the sample discription of scenario and property of errortype, provide property description of the exception in java just like the sample description. You can alse adjust the given scenario description to make them consistent. Please note that the granularity of the property descriptions you generate should be consistent with the examples. [Sample Description] {sample_desc} [Exception] {ename} [Scenario Description] {scenario} Note you should output in the json format like below, please note that the granularity of the property descriptions you generate should be consistent with the examples.: {{ \"scenario\": ...; \"property\": ... }} \"\"\" Planner Prompt planner_prompt = \"\"\"You are software engineer tasked with analyzing codebase. Your task is to segment the given codebase into manageable units for further analysis. The criteria for segmentation are: - Each unit should have length within 200 lines. - The function nesting level should be low. - The logical flow should be clear and self-contained. - The segment should be complete and readable. Given the following codebase: [Codebase] {codebase} Please segment the codebase into units and list them as: Unit 1: [Code Segment] {unit1} Unit 2:"
        },
        {
            "title": "Under Review",
            "content": "[Code Segment] {unit2} ... Ensure that each unit complies with the criteria specified above. \"\"\" Detector Prompt detector_senario_match = \"\"\"You are java code auditor. You will be given doc describe different exception scenarios and java code snippet. Your task is to label each line of the code snippet with the exception scenario that it belongs to. If line does not belong to any scenario, label it with \"None\". If line belongs to one of the given scenarios, label it with all the scenarios it belongs to. [Scenario description] {scenario} [Java code] {code} Please output the labeling result in the json format like below: {{ \"code_with_label\": ... }} \"\"\" detector_prop_match = \"\"\"You are java code auditor. You will be given doc describe different exception properties and java code snippet. Your task is to label each line of the code snippet with the exception property that it belongs to. If line does not belong to any property, label it with \"None\". If line belongs to one of the given properties, label it with all the properties it belongs to. [property description] {property} [Java code] {code} Please output the labeling result in the json format like below: {{ \"code_with_label\": ... }} \"\"\" Predator Prompt predator_prompt = \"\"\"You are code analysis assistant. Your task is to process the given code unit and identify specific exception types that may be thrown."
        },
        {
            "title": "Under Review",
            "content": "[Code Unit] {code_unit} [Code Summary] {code_summary} Based on the code summary and the potential exception branches provided, identify the specific exception nodes that may be thrown. [Potential Exception Branches] {exception_branches} Please answer in the following JSON format: { \"ExceptionNodes\": [ \"ExceptionType\": \"ExceptionType1\", \"ExceptionType\": \"ExceptionType2\", { }, { }, ... ] } Ensure that your response strictly follows the specified format. \"\"\" Ranker Prompt ranker_prompt = \"\"\"You are an exception ranking assistant. Your task is to assign grades to the identified exceptions based on their likelihood and the suitability of their handling strategies. For each exception, please calculate: - Exception Likelihood Score (from 0 to 1) based on its attributes and impact. - Suitability Score (from 0 to 1) of the proposed handling strategy. [Identified Exceptions and Handling Strategies] { \"ExceptionNodes\": [ \"ExceptionType\": \"ExceptionType1\", \"HandlingStrategy\": \"{strategy1}\", \"CEE_Info\": \"{info1}\" { }, ... ] } Provide your calculations and the final grades in the following JSON format: { \"Exceptions\": ["
        },
        {
            "title": "Under Review",
            "content": "{ }, ... ] } \"ExceptionType\": \"ExceptionType1\", \"LikelihoodScore\": value, \"SuitabilityScore\": value, Please ensure your response adheres to the specified format. \"\"\" Handler Prompt handler_prompt = \"\"\"You are software engineer specializing in exception handling. Your task is to optimize the given code unit by applying appropriate exception handling strategies. [Code Unit] {code_unit} [Handling Strategy] {strategy1} Generate the optimized code with the applied exception handling strategies. Please provide the optimized code in the following format: [Optimized Code] {optimized_code} Ensure that the code is syntactically correct and adheres to best practices in exception handling. \"\"\" Sample CEE Node { \"name\": \"IOException\", \"children\": [...], \"info\": { \"definition\": \"IOException is checked exception that is thrown when an input-output operation failed or interrupted. Its general class of exceptions produced by failed or interrupted I/O operations.\", \"reasons\": \"There are several reasons that could cause an IOException to be thrown. These include: File not found error, when the file required for the operation does not exist; Accessing locked file, which another thread or process is currently using; The file system is read only and write operation is performed; Network connection closed prematurely; Lack of access rights.\", \"dangerous_operations\": \"Operations that could typically raise an IOException include: Reading from or writing to file; Opening nonexistent file; Attempting to open socket to non-existent server; Trying to read from connection after its been closed; Trying to change the position of file pointer beyond the size of the file.\", \"sample_code\": \"String fileName = nonexistentfile.txt; n"
        },
        {
            "title": "Under Review",
            "content": "FileReader fileReader = new FileReader(fileName);\", \"handle_code\": \"String fileName = nonexistentfile.txt; try { FileReader fileReader = new FileReader(fileName); } catch(IOException ex) { while processing the file + fileName); n }\", \"handle_logic\":\"Try the codes attempting to establish connection with file/stream/network, catch corresponding ioexception and report it, output openpath is suggested. \" System.out.println(An error occurred ex.printStackTrace(); }, \"scenario\": \"attempt to read from or write to file/stream/network connection\", \"property\": \"There might be an unexpected issue with accessing the file/stream/network due to reasons like the file not being found, the stream being closed, or the network connection being interrupted\" } A.2.3 COMPUTATION COST ANALYSIS Integrating comprehensive exception handling mechanism like Seeker introduces potential challenges in computational overhead, especially when dealing with large number of exception types and complex inheritance relationships. To address this, we designed high-concurrency interface that keeps the additional computing time overhead constant, regardless of the code volume level. This ensures scalability and controllable complexity when processing any size of codebase. To evaluate the efficiency of our high-concurrency interface, we conducted experiments on 100 Java code files both before and after implementing parallel processing. For each code file, we executed the exception handling process and recorded the time taken. In the parallelized version, while the processing between different code files remained sequential, the processing within each code filespecifically, the CEE retrieval involving branch and layered processingwas parallelized. The results are summarized in Table 3. After applying parallel processing, the average time per code file was reduced to approximately 19.4 seconds, which is about 1 15 of the time taken with sequential processing. This significant reduction demonstrates the effectiveness of our parallelization strategy. Table 3: Computation Time Before and After Parallelization Processing Method Average Time per Code File (s) Speedup Factor Sequential Processing Parallel Processing (Seeker) 291.0 19.4 1x 15x Notably, the size of the code files did not affect the processing time, indicating that our method efficiently handles codebases of varying sizes without compromising on speed. This stability ensures that Seeker can perform consistent and efficient exception handling across any code, making it highly suitable for practical applications. A.2.4 FURTHER RESULTS ON DIFFERENT LLMS We use different open-source (e.g. Code Llama-34B (Rozi`ere et al., 2023), WizardCoder-34B (Luo et al., 2024), Vicuna-13B (Zheng et al., 2023)) and closed-source(e.g. Claude-2 (Clade, 2023), GPT-3-davinci (GPT-3, 2022), GPT-3.5-turbo (GPT-3.5, 2023), GPT-4-turbo (GPT-4, 2023), GPT4o (GPT-4o, 2024)) LLMs as the agents internal model to further analyze models ability for exception handling. The results are summarized in Table 4. The performance variations among different models can be explained by: - Pre-training Data: Models pre-trained on larger and more diverse code datasets (e.g., GPT-4o) have better understanding of programming constructs and exception handling patterns."
        },
        {
            "title": "Under Review",
            "content": "Table 4: Performance of Different Models on Exception Handling Code Generation Model ACRS COV (%) COV-P (%) ACC (%) ES CRS (%) Code Llama-34B WizardCoder-34B Vicuna-13B Claude-2 GPT-3-davinci GPT-3.5-turbo GPT-4-turbo GPT-4o 0.31 0.37 0.23 0.42 0.56 0.63 0.84 0.85 Open-Source Models 37 35 15 35 31 Closed-Source Models 64 78 79 91 91 59 68 72 83 81 32 29 11 54 60 66 77 79 0.25 0.28 0. 0.40 0.48 0.52 0.63 0.64 34 35 26 54 58 71 89 92 - Model Architecture: Advanced architectures with higher capacities and more layers (e.g., GPT-4) capture complex patterns more effectively. - RAG Performance: Models that efficiently integrate retrieval-augmented generation, effectively utilizing external knowledge (as in our method), perform better. - Understanding Capability: Models with superior comprehension abilities can accurately detect sensitive code regions and predict appropriate exception handling strategies. Open-source models, while valuable, may lack the extensive training data and architectural sophistication of closed-source models, leading to lower performance. Closed-source models like GPT-4o and GPT-4 benefit from advanced training techniques and larger datasets, enabling them to excel in tasks requiring nuanced understanding and generation of code, such as exception handling. A.3 OTHER APPLICABLE SCENARIOS ANALYSIS Figure 6 shows the migration application of Seeker multi-agent framework in APP requirement engineering that also includes parent-child inheritance relationship. We have reason to believe that Seeker framework can try to be compatible with more complex inheritance relationship, being responsible for reasoning representation, while having high performance and interpretability. The above achievements are not easy to accomplish based on graphs or traditional algorithms. To validate the general applicability of our system in diverse scenarios, we evaluated Seeker on standard code generation benchmarks, including SWE-bench and CoderEval. We present comparative results demonstrating the incremental improvements achieved by our method. SWE-bench is an evaluation framework comprising 2,294 software engineering problems derived from real GitHub issues and corresponding pull requests across 12 popular Python repositories(Jimenez et al., 2024). It challenges language models to edit given codebase to resolve specified issues, often requiring understanding and coordinating changes across multiple functions, classes, and files simultaneously. This goes beyond traditional code generation tasks, demanding interaction with execution environments, handling extremely long contexts, and performing complex reasoning. For our experiments, we selected 50 issues related to exception handling from the SWE-bench Lite dataset. Using GPT-4o as the internal large model, the SweAgent coupled with GPT-4o achieved 19% resolve rate and 43% apply rate. In contrast, our Seeker framework attained 26% resolve rate and 61% apply rate, indicating significant improvement. Table 5: Performance on SWE-bench Lite Exception Handling Issues Method Resolve Rate (%) Apply Rate (%) SweAgent + GPT-4o Seeker + GPT-4o 43 61"
        },
        {
            "title": "Under Review",
            "content": "CoderEval is benchmark designed to assess the performance of models on pragmatic code generation tasks, moving beyond generating standalone functions to handling code that invokes or accesses custom functions and libraries Yu et al. (2024). It evaluates models ability to generate functional code in real-world settings, similar to open-source or proprietary projects. In the Java code generation tasks on CoderEval, using Codex directly yielded Pass@1 score of 27.83%. When integrating our Seeker framework with Codex, the Pass@1 score increased to 38.16%, demonstrating substantial enhancement in code generation performance. Table 6: Performance on CoderEval Java Code Generation Tasks Method Pass@1 (%) Codex Seeker + Codex 27.83 38.16 These experiments conclusively demonstrate that our Seeker framework can achieve significant incremental improvements across different scenarios and benchmarks. By effectively handling exception-related tasks and enhancing code robustness, Seeker proves to be valuable addition to existing code generation models, improving their practical applicability in real-world software engineering problems. Inspired by OpenAI o1 (o1, 2024) and DoT (Zhang et al., 2024), we found that Seeker framework has more room for development in LLM reasoning. Through pre-deduction in tree inference, LLM is expected to enter the problem-solving ideas more efficiently and optimize its reasoning actions through interaction with the external environment. In the future, we will continue to explore research in this direction."
        },
        {
            "title": "B RELATED WORK",
            "content": "At present, machine learning has been widely integrated in the field of software engineering, especially in code generation tasks. In this section, we will discuss the progress of Seeker-related work from the latest progress of automatic exception handling tools. These methods have contributed to the robustness or productivity of software engineering, but they also have limitations, which is also the focus of Seeker. B.1 AUTOMATIC EXCEPTION HANDLING TOOLS Zhang et al. (2020) introduced neural network approach for automated exception handling in Java, which predicts try block locations and generates complete catch blocks in relatively high accuracy. However, the approach is limited to Java and may not generalize well to other programming languages without retraining. Additionally, the reliance on GitHub data could introduce biases based on the types of projects and code quality present in the dataset. Li et al. (2024b) conducted an exploratory study on fine-tuning LLM for secure code generation. Their results showed that after fine-tuning issue fixing commits, the secure code generation rate was slightly improved. The best performance was achieved by fine-tuning using function-level and block-level datasets. However, the limitation of this study is that it is targeted at C/C++ and is not directly applicable to other languages. In addition, the quality of training data directly affects the performance of the fine-tuned model, and the generalization between different datasets may be inconsistent. ? also pointed out that in terms of automatic vulnerability detection, the use of traditional fine-tuning methods may not fully utilize the domain knowledge in the pre-trained language model, and may overfit to specific dataset, resulting in misclassification, excessive false positives and false negatives. Its performance is not as good as emerging methods such as prompt-based learning. Ren et al. (2023) proposed the Knowledge-driven Prompt Chaining (KPC) approach to improve code generation by chaining fine-grained knowledge-driven prompts. Their evaluation with 3,079 code generation tasks from Java API documentation showed significant improvements in exception handling. However, the approachs efficiency relies heavily on the inquiry about built-in exceptions for each built-in JDK, and its practical application may be limited if the codebase is complex."
        },
        {
            "title": "Under Review",
            "content": "Nguyen et al. (2020a) developed FuzzyCatch, tool for recommending exception handling code for Android Studio based on fuzzy logic. It achieved impressive accuracy in recommending exceptions to catch and methods to call when an exception occurs. However, the performance of FuzzyCatch depends on the quality and relevance of the training data. In addition, the tool may not perform well for less common exceptions or domains that are not well represented in the training data. common limitation of these studies is that the training data they rely on may not fully represent all possible coding scenarios. This may result in model that is effective in specific situations, but may not generalize well to other situations. In addition, the complexity of exception handling in real-world applications may exceed the capabilities of models trained on more common or simpler cases, so it is crucial to call on the understanding and reasoning capabilities of the model itself. The interpretability of exception handling also provides guarantee for the improvement of developers programming literacy. The comparison between the above methods and Seeker is shown in figure 7."
        },
        {
            "title": "Under Review",
            "content": "Figure 4: schematic diagram of Preliminary Phenomenon, highlight what information will boost LLM EH performance, and small scale preliminary data"
        },
        {
            "title": "Under Review",
            "content": "Figure 5: schematic diagram of Preliminary Phenomenon, highlight what information will boost human EH performance, and small scale preliminary data"
        },
        {
            "title": "Under Review",
            "content": "Figure 6: schematic diagram of APP requirement engineering, highlight seekers generalizability. Figure 7: Comparison of experimental metrics between our method and baselines."
        }
    ],
    "affiliations": [
        "Beihang University",
        "ByteDance",
        "The CoAI Group, Tsinghua University"
    ]
}