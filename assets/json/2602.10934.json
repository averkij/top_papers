{
    "paper_title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
    "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 2 4 3 9 0 1 . 2 0 6 2 : r MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models MOSI.AI*"
        },
        {
            "title": "Abstract",
            "content": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully endto-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSSAudio-Tokenizer consistently outperforms prior codecs over wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as unified, scalable interface for the next generation of native audio foundation models. Code: https://github.com/OpenMOSS/MOSS-Audio-Tokenizer Model: https://huggingface.co/OpenMOSS-Team/MOSS-Audio-Tokenizer"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models [18] have demonstrated the effectiveness of autoregressive modeling over discrete token sequences. By providing unified discrete interface, text tokenizers [9, 10] allow large language models to operate directly on raw text, serving as the foundation upon which compression, understanding, generation, and in-context learning capabilities emerge within single autoregressive modeling framework. Extending this paradigm to audio requires Full contributors can be found in the Contributors section. Figure 1 Audio reconstruction quality comparison. 1 Table 1 Comparison of representative audio tokenizers with respect to architectural design and functional capabilities. 4 indicates support, 8 indicates not supported, and indicates not specified. Trans. denotes Transformer, and Hybrid denotes hybrid architecture combining CNN and Transformer. End-to-end optimize indicates whether all modules are jointly optimized under unified objective. Model Frame rate Encoder Arch. Decoder Arch. Stream ing Variable Bitrate Semantic rich Reconstruction Speech Sound Music Pretrained encoder free End-to-end optimize Encodec DAC SpeechTokenizer Mimi BigCodec StableCodec XCodec2.0 XY-Tokenizer DualCodec Higgs-Audio-Tokenizer MiMo-Audio-Tokenizer Qwen3-TTS-Tokenizer MOSS-Audio-Tokenizer 75 75 50 12.5 80 25 50 12.5 12.5 25 25 12.5 12. CNN CNN CNN CNN CNN CNN Hybrid Hybrid CNN CNN Hybrid Hybrid Hybrid Hybrid Hybrid Hybrid CNN Hybrid Hybrid CNN Hybrid Hybrid Hybrid Hybrid Trans. Trans. 4 8 8 4 8 8 8 8 8 8 8 4 4 4 4 4 4 8 4 8 8 4 4 4 8 8 4 4 8 8 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8 4 8 8 8 8 8 4 4 4 4 4 8 4 8 8 8 8 8 4 4 4 4 4 8 8 4 4 8 8 8 8 4 8 4 4 8 8 4 4 8 8 8 8 8 8 4 unified discrete audio tokenizer that can serve as native interface for autoregressive modeling [1114]. Unlike text, audio contains both fine-grained acoustic details and long-range structure, making discrete tokenization more challenging [11]. unified audio tokenizer should enable high-fidelity reconstruction of diverse audio signals while remaining compatible with autoregressive sequence modeling [1416]. Existing approaches typically address these requirements through pretrained encoders [1722], multi-stage training pipelines [14, 23, 24], or architecture-specific inductive biases [2527], achieving strong performance under particular design choices. However, such designs introduce additional dependencies and architectural constraints that make it difficult to scale models, data, and quantization capacity in unified manner. From this perspective, we draw inspiration from the success of large language models, where simple and efficient architectures trained on large-scale data have proven critical for achieving strong performance [28, 29]. We posit that enabling an audio tokenizer to reach higher performance ceiling similarly requires simple and scalable architecture that can be trained end-to-end on large amounts of data. Such design emphasizes joint optimization and scale, while minimizing reliance on external priors, pretrained components, or complex architectural heuristics. In this work, we propose MOSS-Audio-Tokenizer, fully end-to-end audio tokenizer that serves as unified discrete interface for autoregressive audio language models. Our tokenizer, build on CAT (Causal Audio Tokenizer with Transformer) architecture, operates at 24 kHz sampling rate with low token frame rate of 12.5 Hz, and jointly optimizes the encoder, quantizer, decoder, and discriminator within single training pipeline, without relying on pretrained encoders, distillation, or separate optimization of individual components. Both the encoder and decoder are built entirely from causal Transformer blocks, resulting in simple and scalable architecture that is naturally aligned with autoregressive modeling [30]. All components of MOSS-Audio-Tokenizer are designed to operate in streaming manner, enabling low-latency inference and consistent traininginference behavior [15, 25, 26]. By scaling large amounts of paired audiotext data, MOSS-Audio-Tokenizer learns discrete representations that are both structurally rich and acoustically expressive, while remaining robust across wide range of bitrates. As result, the tokenizer achieves high-quality reconstruction of general audio, including speech, sound, and music, from very low to high bitrate regimes, providing strong lower bound and high upper bound for downstream audio language models. Across speech, sound, and music, MOSS-Audio-Tokenizer achieves state-of-the-art reconstruction quality at all evaluated bitrates. Leveraging its discrete tokens, we further introduce purely autoregressive text2 to-speech model with Progressive Sequence Dropout training strategy, which naturally exploits the tokenizers robustness across bitrates and, for the first time, enables fully autoregressive discrete TTS system [3134] to outperform prior non-autoregressive [35, 36] and cascaded approaches [3744]. In addition, MOSS-Audio-Tokenizer supports competitive automatic speech recognition performance without requiring an auxiliary audio encoder, matching or exceeding models that rely on dedicated audio encoders combined with large language models [4549]. Together, these results demonstrate that CAT architecture provides scalable and effective foundation for audio compression, understanding, and generation within unified autoregressive framework. Our contributions can be summarized as follows: Homogeneous and Scalable Architecture: We propose CAT (Causal Audio Tokenizer with Transformer), purely Transformer-based architecture for discrete audio tokenization. By utilizing homogeneous stack of causal Transformer blocks, CAT provides simple and highly scalable discrete interface, minimizing fixed inductive biases and facilitating effective model scaling. Large-Scale General Tokenization: Based on the CAT architecture, we develop MOSS-Audio-Tokenizer, 1.6-billion-parameter audio tokenizer pre-trained from scratch on 3 million hours of diverse audio. It achieves high-fidelity reconstruction across speech, sound, and music at low frame rate of 12.5 Hz. The model natively supports variable bitrates ranging from 0.125 kbps to 4 kbps and enables low-latency, frame-level streaming encoding and decoding for real-time applications. Breakthrough in End-to-End Autoregressive Audio Generation: Leveraging CATs discrete tokens, we develop the first purely autoregressive (AR) TTS system that outperforms prior non-autoregressive and cascaded models. Furthermore, we propose Progressive Sequence Dropout, training strategy that enables single autoregressive model to perform variable-bitrate speech generation by effectively utilizing the tokenizers hierarchical quantization structure. Consistent Scaling Performance: We investigate the scaling behavior of the CAT architecture with respect to model parameters and training computation (via batch size). Our results demonstrate that CAT exhibits consistent performance gains in reconstruction quality as the model capacity and total computational budget increase, establishing it as unified and robust foundation for future large-scale audio foundation models."
        },
        {
            "title": "2 Rethinking Discrete Audio Tokenization for Future Audio Foundation Models",
            "content": "We rethink discrete audio tokenization from the perspective of autoregressive audio language modeling. Analogous to text tokenizers in large language models, discrete audio tokenizer should serve as native interface that bridges raw audio signals and autoregressive sequence modeling [14, 15, 50]. This viewpoint places stringent requirements on the structure, representation capacity, and scalability of the tokenizer, beyond traditional objectives of audio compression or reconstruction. From this perspective, we identify several key design principles that an audio tokenizer must satisfy in order to effectively support autoregressive audio language models. Unified Audio Representation. tokenizer should provide unified discrete representation capable of modeling and reconstructing diverse audio domains, including speech, sound, and music. Crucially, the resulting tokens should preserve both fine-grained acoustic information and semantic structure, enabling them to function as meaningful sequence for autoregressive modeling rather than merely compressed signal. Simplicity and Scalability. To enable efficient scaling with model capacity, data, and computation, the tokenizer architecture should remain simple and homogeneous. Excessive architectural heterogeneity or reliance on specialized components can introduce fixed bottlenecks that hinder joint scaling and limit the effectiveness of large-scale training. 3 Figure 2 Architecture of CAT (Causal Audio Tokenizer with Transformer). Both the encoder and decoder are built upon causal Transformers. All components, including the encoder, quantizer, decoder, causal language model, and discriminator, are optimized jointly in an end-to-end manner. Causality. For compatibility with autoregressive generation and low-latency inference, tokenization should be strictly causal, ensuring that each token is computed without access to future audio context. This property aligns the tokenizer with the operational constraints of autoregressive audio language models and avoids discrepancies between training and inference. Low Frame Rate and Bitrate Robustness. An effective audio tokenizer should operate at low frame rate to reduce downstream sequence modeling complexity, while remaining robust across wide range of bitrates. Such flexibility allows single tokenizer to support diverse downstream tasks, including audio reconstruction, understanding, and generation, without requiring task-specific redesign."
        },
        {
            "title": "3.1 Homogeneous Architecture for Scalable Audio Tokenization",
            "content": "A central design goal of CAT is to enable scalable audio tokenization that can seamlessly integrate with large autoregressive and multimodal foundation models. To this end, we adopt CNN-free architecture that is built entirely upon causal Transformer blocks, as illustrated in Figure 2. Compared to prior neural audio codecs that rely heavily on convolutional inductive biases or hybrid CNNTransformer designs, our approach deliberately minimizes architectural specialization, favoring simplicity, uniformity, and scalability. Fully Transformer-based encoderdecoder. Both the encoder and decoder in CAT are implemented as stacks of causal Transformer blocks, forming CNN-free architecture and enabling streaming encoding and decoding. CAT operates directly on raw audio waveforms at both the input and output, avoiding intermediate signal representations such as mel-spectrograms. The input waveform is first patchified into sequence of fixeddimensional vectors and processed by the causal Transformer encoder. To progressively compress long audio sequences into compact representation, we insert patchify operations between Transformer blocks, which gradually reduce the temporal resolution. As result, the encoder maps 24 kHz waveforms into discrete token sequences at an average rate of 12.5 frames per second. The decoder mirrors this process in reverse, reconstructing the waveform from discrete tokens in fully causal manner. Further implementation details are provided in Appendix A. Scalable residual vector quantization. For discretization, we employ residual vector quantization (RVQ). To support robust modeling across wide range of bitrates, we adopt ùëÅùëû = 32 residual quantization layers and enable quantizer dropout during training. This variable-bitrate design directly facilitates the controllable audio generation framework introduced later."
        },
        {
            "title": "3.2 Unified Audio Modeling",
            "content": "We use multi-task learning to enable CAT to achieve both strong alignment with text and high-quality audio reconstruction. Semantic Modeling via Audio-to-Text Tasks. To encourage the token representation to be semantically rich and aligned with text-based language modeling, we incorporate an auxiliary audio-to-text objective. Specifically, we employ 0.5B-parameter decoder-only LLM [51] and condition it on the representations produced by CAT. Concretely, we feed the hidden states from the quantizer output into the LLM, which then autoregressively predicts textual tokens. We consider diverse set of audio-to-text tasks, including automatic speech recognition (ASR), multi-speaker ASR, and audio captioning. For audio samples that are paired with textual annotations, we apply the corresponding semantic modeling objective. Each task is specified by fixed task tag ùíØ , which is prepended to the LLM input. The semantic objective is optimized using standard cross-entropy loss: s(cid:213) ‚Ñísem = log ùëùùúÉLLM (sùë° ùíØ , q, s<ùë°) , ùë°=1 (1) where = (s1, . . . , ss) denotes the target text token sequence, denotes the sequence of quantized audio representations produced by CAT, ùíØ is task-specific prompt token, and ùúÉLLM are the parameters of the causal language model. Quantizer Optimization. For training simplicity and stability, each quantization layer in CAT adopts factorized vector quantization [27], where codebooks are directly optimized via gradient descent, without relying on additional codebook update mechanisms [26]. We incorporate commitment loss and codebook loss to jointly optimize the encoder and the codebook entries: ‚Ñícmt = ‚Ñícode = ùëÅùëû(cid:213) ùëê=1 ùëÅùëû(cid:213) ùëê=1 (cid:13) (cid:13) (cid:13)2 (cid:13)zùëê sg(ùëûùëê(zùëê)) 2 , (cid:13) (cid:13) (cid:13)2 (cid:13)sg(zùëê) ùëûùëê(zùëê) 2 , (2) (3) where zùëê denotes the input to the ùëê-th quantization layer, ùëûùëê(zùëê) is the corresponding quantized output, ùëÅùëû is the number of quantizers, and sg() denotes the stop-gradient operator [52]. Acoustic Modeling via Reconstruction Tasks. To ensure high-fidelity and domain-robust audio reconstruction, we adopt multi-scale mel-spectrogram loss: ‚Ñírec = 11(cid:213) ùëñ=5 ùëÜ 2ùëñ (x) ùëÜ 2ùëñ (ÀÜx) , (4) 2ùëñ () denotes the mel-spectrogram computed using normalized short-time Fourier transform (STFT) and hop size 2ùëñ2. Here, is the ground-truth waveform and ÀÜx is the reconstructed where ùëÜ with window size 2ùëñ waveform generated by the decoder. Adversarial Training. To further improve reconstruction fidelity and perceptual quality, we employ adversarial training with multiple discriminators. Specifically, we adopt the discriminator architecture and training objectives, including the adversarial loss, feature matching loss and discriminator loss, following XYTokenizer [21]. Overall Training Objective. The overall generator objective is weighted combination of all loss terms: ‚ÑíG = ùúÜsem‚Ñísem + ùúÜrec‚Ñírec + ùúÜcmt‚Ñícmt + ùúÜcode‚Ñícode + ùúÜadv‚Ñíadv + ùúÜfeat‚Ñífeat, (5) where ‚Ñíadv and ‚Ñífeat denote the adversarial and feature matching losses defined in XY-Tokenizer [21]. ùúÜsem, ùúÜrec, ùúÜcmt, ùúÜcode, ùúÜadv, ùúÜfeat are scalar hyperparameters controlling the relative contribution of each loss term. All components of CAT, including the encoder, quantizer, decoder, and discriminators, are optimized jointly in an end-to-end manner. By scaling large amounts of audio data, CAT learns to achieve both high-fidelity reconstruction of general audio and semantically rich discrete representations, without relying on pretrained encoders or external semantic teachers [17, 18, 20, 50]."
        },
        {
            "title": "3.3 Bitrate Controllable Audio Generation",
            "content": "End-to-end variable-bitrate autoregressive speech generation. Building on CAT, we construct CAT-TTS, fully end-to-end, purely autoregressive speech generation model that supports variable-bitrate synthesis. The model directly generates speech from text tokens and speaker prompt by predicting CATs RVQ tokens at controllable depth, without requiring semantic disentanglement [50, 53, 54] or cascading multiple generative models [39, 40, 44]. By leveraging CAT as unified discrete interface, both linguistic content and acoustic information are modeled within single autoregressive framework. Autoregressive modeling over RVQ tokens. Since CAT represents audio using residual vector quantization (RVQ), we adopt the Temporal Transformer + Depth Transformer architecture [15] for multi-stream autoregressive modeling. The Temporal Transformer captures long-range dependencies along the temporal dimension, while the Depth Transformer models the coarse-to-fine residual structure across RVQ layers within each time step. Under this design, each RVQ token conditions only on tokens from previous time steps and on preceding RVQ layers at the current time step, ensuring strict causality without information leakage. Progressive Sequence Dropout. To enable robust generation across wide range of bitrates within single model, we propose Progressive Sequence Dropout, simple yet effective training strategy that requires no architectural modifications or additional parameters. During training, dropout is activated with probability ùëù. When activated, we uniformly sample prefix length ùêæ {1, . . . , ùëÅùëû 1}, where ùëÅùëû denotes the total number of RVQ layers, and discard RVQ tokens from layers ùêæ+1 to ùëÅùëû. Otherwise, all RVQ layers are retained. This strategy exposes the model to truncated RVQ prefixes during training, where prefix length is randomly sampled independently for each training sample, encouraging the model to learn conditional generation under varying bitrates. Prefix definition. We introduce Bernoulli random variable ùëß Bernoulli(ùëù), (6) where ùëß = 1 indicates that Progressive Sequence Dropout is applied and ùëß = 0 otherwise. When ùëß = 1, the prefix length ùêæ is sampled uniformly as described above; when ùëß = 0, we set ùêæ = ùëÅùëû. The effective number of active RVQ layers is then defined as ÀÜùêæ = (1 ùëß) ùëÅùëû + ùëß ùêæ. (7) Global input aggregation and training objective. Let qùë°,ùëò denote the RVQ token at time step ùë° and layer ùëò, and let Embùëò() denote the embedding lookup table for the ùëò-th RVQ codebook. For each time step ùë°, the speech ÀÜùêæ RVQ layers: input to the Temporal Transformer is constructed by aggregating the embeddings of the first eùë° = ÀÜùêæ(cid:213) ùëò=1 (cid:0) qùë°,ùëò (cid:1) . Embùëò 6 (8) The Temporal Transformer processes the resulting acoustic embedding sequence {eùë°}ùëá tention mask along the temporal dimension. ùë°=1 using causal atThe Depth Transformer predicts RVQ tokens autoregressively along the depth dimension. The training loss is computed only over the retained RVQ prefix: ùëá(cid:213) ÀÜùêæ(cid:213) (cid:16) (cid:17) ‚Ñí = log ùëùùúÉ qùë°,ùëò x, q<ùë° , qùë°,<ùëò , ùë°=1 ùëò=1 (9) where ùúÉ denotes the model parameters, represents the input text token sequence, q<ùë° denotes all RVQ tokens from previous time steps, and qùë°,<ùëò denotes RVQ tokens from preceding layers at the same time step. Inference. At inference time, we explicitly control the synthesis bitrate by selecting an inference depth ùêæinfer. The Temporal Transformer takes as input the text tokens together with the first ùêæinfer RVQ token streams at each time step. The Depth Transformer then autoregressively predicts only these ùêæinfer RVQ layers, while finer layers are omitted. Finally, the predicted RVQ tokens from the first ùêæinfer layers are decoded into waveforms using the CAT decoder. As CAT is trained with quantizer dropout, the decoder is inherently robust to varying effective bitrates, which aligns naturally with Progressive Sequence Dropout in the speech generation model. Special case. When ùëù = 0, Progressive Sequence Dropout is disabled, yielding ùëß = 0 for all training samples ÀÜùêæ = ùëÅùëû. In this case, the proposed method reduces exactly to the standard Temporal Transformer + and Depth Transformer formulation for multi-stream autoregressive speech generation."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Building upon the CAT architecture, we develop MOSS-Audio-Tokenizer, large-scale audio tokenizer featuring 1.6 billion parameters. The model utilizes causal Transformer-based encoderdecoder paired with hierarchical patching, which facilitates efficient streaming audio modeling. Discrete representations are learned using 32-layer residual vector quantizer with quantizer dropout to support variable-bitrate tokenization. To encourage semantic alignment, we attach decoder-only causal language model for audioto-text supervision. Training is performed on approximately 3M hours of diverse speech, sound, and music data, using combination of reconstruction, semantic, and adversarial objectives. All components of MOSS-Audio-Tokenizer, including the encoder, quantizer, decoder, discriminator, decoder-only LLM are optimized jointly in an end-to-end manner. All architectural details, optimization hyperparameters, and training schedules are provided in Appendix A."
        },
        {
            "title": "4.2 Reconstruction Evaluation",
            "content": "We compare MOSS-Audio-Tokenizer with open-source audio tokenizers using both objective and subjective evaluation metrics across low (7501500 bps), medium (15002500 bps), and high (25006000 bps) bitrate regimes. Table 2 summarizes the objective reconstruction results on speech, general audio, and music benchmarks. Across all evaluated bitrate regimes, MOSS-Audio-Tokenizer achieves strong performance on speech reconstruction, outperforming prior methods at low bitrates and achieving state-of-the-art results at medium and high bitrates. On audio and music benchmarks, MOSS-Audio-Tokenizer maintains competitive performance across all evaluated bitrates, with reconstruction quality improving as bitrate increases, indicating that the model effectively benefits from increased bitrate and model capacity through joint end-to-end optimization. Additional details on the compared open-source audio tokenizers, as well as subjective evaluation results are provided in Appendix B. 7 Table 2 Reconstruction quality comparison of open-source audio tokenizers on speech and audio/music data. Speech metrics are evaluated on LibriSpeech test-clean (English) and AISHELL-2 (Chinese) and reported as English/Chinese. Audio metrics are evaluated on the AudioSet evaluation subset, while music metrics are evaluated on the MUSDB dataset; values are reported as audio/music. STFT-Dist. denotes the STFT distance. Higher is better for speech metrics, whereas lower is better for audio/music metrics. NVQ denotes the number of quantizers. Model bps Frame rate NVQ Speech Audio / Music SIM STOI PESQ-NB PESQ-WB Mel-Loss STFT-Dist. StableCodec 700 XCodec2.0 800 MiMo-Audio-Tokenizer 850 Higgs-Audio-Tokenizer 1000 SpeechTokenizer 1000 XY-Tokenizer 1000 BigCodec 1040 Mimi 1100 MOSS-Audio-Tokenizer 750 MOSS-Audio-Tokenizer 1000 DAC Encodec Higgs-Audio-Tokenizer SpeechTokenizer Qwen3-TTS-Tokenizer MiMo-Audio-Tokenizer Mimi 1500 1500 2000 2000 2200 2250 2475 MOSS-Audio-Tokenizer 1500 MOSS-Audio-Tokenizer 2000 DAC MiMo-Audio-Tokenizer SpeechTokenizer Mimi Encodec DAC 3000 3650 4000 4400 4500 6000 MOSS-Audio-Tokenizer 3000 MOSS-Audio-Tokenizer 4000 25 50 25 25 50 12.5 80 12.5 12.5 12.5 75 75 25 50 12.5 25 12.5 12.5 12.5 75 25 50 12.5 75 75 12.5 12.5 2 1 4 4 2 8 1 8 6 8 2 2 8 4 16 12 18 12 4 20 8 32 6 8 24 32 0.62 / 0.45 0.82 / 0.74 0.80 / 0.74 0.77 / 0.68 0.36 / 0.25 0.85 / 0.79 0.84 / 0.69 0.74 / 0.59 0.82 / 0.75 0.88 / 0.81 0.91 / 0.86 0.92 / 0.86 0.91 / 0.87 0.83 / 0.82 0.77 / 0.68 0.92 / 0.87 0.93 / 0.88 0.91 / 0.85 0.93 / 0.89 0.94 / 0.91 0.83 / 0.79 0.48 / 0.41 0.85 / 0.81 0.60 / 0.45 0.85 / 0.85 0.90 / 0.83 0.66 / 0.50 0.88 / 0.80 0.95 / 0.88 0.96 / 0.93 0.95 / 0.92 0.89 / 0.83 0.94 / 0.91 0.89 / 0.76 0.95 / 0.93 0.92 / 0.86 0.96 / 0.94 0.95 / 0.89 0.74 / 0.67 0.91 / 0.85 0.85 / 0.69 0.94 / 0.83 0.86 / 0.75 0.89 / 0.84 0.96 / 0.92 0.97 / 0.93 0.90 / 0.88 0.95 / 0.93 0.92 / 0.85 0.96 / 0.94 0.92 / 0.91 0.95 / 0.94 0.97 / 0.96 0.97 / 0. 2.91 / 2.50 3.04 / 2.46 2.94 / 2.62 3.03 / 2.61 1.59 / 1.38 3.10 / 2.63 3.27 / 2.55 2.80 / 2.24 3.14 / 2.73 3.38 / 2.96 1.87 / 1.67 1.94 / 1.80 3.59 / 3.22 2.38 / 1.79 3.66 / 3.10 3.57 / 3.25 3.49 / 2.90 3.64 / 3.27 3.78 / 3.46 2.76 / 2.47 3.73 / 3.44 3.05 / 2.20 3.80 / 3.31 2.91 / 2.63 3.75 / 3.57 3.90 / 3.64 3.95 / 3.71 2.24 / 1.93 2.43 / 1.96 2.39 / 2.14 2.48 / 2.14 1.25 / 1.17 2.50 / 2.12 2.68 / 2.06 2.25 / 1.78 2.60 / 2.22 2.87 / 2.43 1.48 / 1.37 1.56 / 1.48 3.11 / 2.73 1.92 / 1.49 3.19 / 2.62 3.05 / 2.71 2.97 / 2.35 3.20 / 2.74 3.41 / 2.96 2.31 / 2.07 3.25 / 2.89 2.60 / 1.87 3.43 / 2.78 2.46 / 2.15 3.41 / 3.20 3.61 / 3.20 3.69 / 3. / / 0.82 / 0.81 0.83 / 0.80 / / / 1.24 / 1.19 0.86 / 0.85 0.82 / 0.80 / 1.12 / 1.04 0.74 / 0.70 / / 0.70 / 0.68 1.10 / 1.06 0.77 / 0.74 0.73 / 0.70 0.86 / 0.83 0.66 / 0.65 / 1.02 / 0.98 0.91 / 0.84 0.65 / 0.63 0.69 / 0.66 0.68 / 0.64 / / 2.33 / 2.23 2.20 / 2.05 / / / 2.62 / 2.49 2.21 / 2.10 2.16 / 2.04 / 2.60 / 2.42 2.07 / 1.92 / / 2.21 / 2.10 2.45 / 2.32 2.08 / 1.96 2.03 / 1.90 2.23 / 2.10 2.17 / 2.06 / 2.34 / 2.21 2.33 / 2.17 1.97 / 1.87 1.98 / 1.84 1.96 / 1."
        },
        {
            "title": "4.3 Speech Generation",
            "content": "Experimental Settings. We initialize the Temporal Transformer with the pretrained Qwen3-1.7B model [7]. The Depth Transformer consists of four Transformer blocks and is randomly initialized. We train the model on mixture of VoxBox, as introduced in SparkAudio [33], and an internal dataset, totaling approximately 200k hours of speech data. Evaluation is conducted on the Seed-TTS-Eval benchmark [39]. Training details are provided in Appendix C. Effectiveness of Progressive Sequence Dropout. We investigate the effect of Progressive Sequence Dropout by varying the dropout probability ùëù {0.0, 0.25, 0.5, 1.0}, with results summarized in Figure 3. At full bitrate, all models achieve comparable performance, exhibiting low word error rate and high speaker similarity. However, as the bitrate decreases, the model trained without dropout exhibits much steeper degradation in similarity and word error rate. This stems from the mismatch between training and inference, as the model is trained exclusively with full RVQ depth but evaluated using truncated representations. In contrast, models trained with Progressive Sequence Dropout are substantially more robust under reduced bitrate settings. Across different dropout probabilities (ùëù = 0.25, 0.5, and 1.0), TTS performance remains 8 Figure 3 Effect of Progressive Sequence Dropout on fully autoregressive TTS across different bitrates. highly consistent at each bitrate, indicating the exact dropout probability has limited impact on generation quality. Meanwhile, increasing ùëù significantly reduces GPU memory consumption during training. Therefore, we adopt ùëù = 1.0 in all subsequent experiments to maximize training efficiency while maintaining comparable synthesis quality. Comparison with Open-Source TTS Systems. We evaluate the performance of our CAT-based fully autoregressive (AR) TTS system against comprehensive suite of open-source models. These baselines encompass three major paradigms: (i) cascaded systems (e.g., AR+NAR), (ii) purely non-autoregressive systems, and (iii) prior purely autoregressive systems based on discrete or continuous representations. Detailed descriptions and categorizations of these baseline systems are provided in Appendix D. As shown in Table 3, CAT-TTS significantly outperforms previous discrete fully autoregressive models, particularly in speaker similarity (SIM). Moreover, our method achieves competitive performance compared to recent state-of-the-art systems such as IndexTTS2, MaskGCT, and VoxCPM, with all systems maintaining very low word error rates (WER), typically below 2%. Notably, CAT-TTS achieves the highest speaker similarity scores on Seed-TTS-Eval for both English and Chinese among the compared open-source models. This demonstrates that scaling CAT, as unified discrete interface, effectively captures fine-grained acoustic characteristics required for high-quality, zero-shot speech generation."
        },
        {
            "title": "4.4 Speech Understanding",
            "content": "In addition to speech generation, we further evaluate the speech understanding capability of CAT by applying it to downstream LLM-based ASR and comparing against representative open-source state-of-the-art speech understanding models; detailed results are provided in Appendix E."
        },
        {
            "title": "5.1 End-to-End Optimization Makes CAT a Scalable Audio Tokenizer",
            "content": "A key goal of CAT is scalabilitythe ability to continuously improve reconstruction quality with increased training budget. Although CAT consists of multiple adversarial components, its optimization strategy is critical for enabling such scalability. We compare full end-to-end optimization with the partial protocol used in prior works [14, 16, 21, 23], where the encoder and quantizer are frozen while the decoder and discriminator are optimized. As shown in Figure 4, end-to-end training yields sustained improvements across all metrics without early saturation. In contrast, partial optimization plateaus early, as freezing components restricts the 9 Table 3 Comparison with open-source TTS systems on Seed-TTS-Eval. Bitrate control indicates whether TTS system allows explicit specification of the synthesis bitrate at inference time. For FlexiCodec-TTS, bitrate is controlled by switching the frame rate of the autoregressive model. For CAT-TTS, bitrate is controlled by specifying the number of RVQ tokens generated by the Depth Transformer. TTS Systems Bitrate Control. Seed-EN Seed-ZH WER SIM CER SIM Cascade (AR+NAR / NAR+NAR) MaskGCT FireRedTTS CosyVoice2 Qwen2.5-Omni CosyVoice3-1.5B IndexTTS2 FlexiCodec-TTS GLM-TTS F5-TTS VibeVoice VoxCPM 8 8 8 8 8 8 4 8 2.62 3.84 3.09 2.72 2.22 2.23 2.63 1.91 NAR / Continuous AR 8 8 Discrete AR Llasa SparkTTS OpenAudio-s1-mini HiggsAudio-v2 FireRedTTS2 CAT-TTS (Ours) 8 8 8 8 8 4 2.00 3.04 1.85 2.97 1.98 1.94 2.44 1.95 1.89 71.7 46.0 65.9 63.2 72.0 70.6 65.7 68. 67.0 68.9 72.9 57.4 58.4 55.0 67.7 66.5 73.1 2.27 1.51 1.38 1.70 1.12 1.03 - 0.89 1.53 1.16 0.93 1.59 1.20 1.18 1.50 1.14 1.23 77.4 63.5 75.7 75.2 78.1 76.5 - 76. 76.0 74.4 77.2 68.4 67.2 68.5 74.0 73.6 78.5 models ability to refine representations. These results demonstrate that end-to-end optimization is crucial for scaling CAT effectively with increased computation and capacity. Figure 4 Comparison between full end-to-end optimization and partial (stage-wise) optimization for CAT."
        },
        {
            "title": "5.2 Co-Scaling of Model Parameters and Quantization Capicity",
            "content": "We examine how CAT scales with model size. Following Section 3, we jointly optimize all components while varying the hidden dimension (256, 384, 512, 768)totaling 319M, 505M, 710M, and 1169M combined encoderdecoder parameters, respectively. Throughout, the quantizer is held constant at 32 layers and 12.5 Hz. Figure 5 shows that increasing the parameter count improves reconstruction quality across 0.54 kbps. While the 1169M model benefits most from high bitrates, smaller versions saturate early. Notably, at low bitrates, 10 Figure 5 Scaling behavior of CAT reconstruction performance with respect to bitrate and model parameters. the 1169M model can underperform smaller models operating at higher bitrates, indicating that bitrate rather than parameter countbecomes the primary bottleneck. These findings reveal that parameter scaling and quantization depth are fundamentally co-dependent. Neither can be scaled effectively in isolation, as system performance is governed by the narrowest bottleneck. Optimal scaling thus requires synchronized expansion of both model parameters and quantization capacity within an end-to-end framework. Figure 6 Scaling behavior of CAT reconstruction performance with respect to training batch size. The color gradient represents the batch size scale from 20 to 28. Larger batch sizes consistently yield superior reconstruction fidelity across all metrics."
        },
        {
            "title": "5.3 Reconstruction Fidelity Benefits Consistently from Increased Training Scale",
            "content": "Beyond model parameters and bitrate, key indicator of tokenizers scalability is its capacity to effectively translate increased training computation into higher fidelity. We investigate this by varying the global training batch size from baseline factor of 20 up to 28, while keeping the total number of training steps and other hyperparameters constant. As illustrated in Figure 6, CAT exhibits clear and positive correlation between training scale and reconstruction quality across all evaluated speech metrics (SIM, STOI, and PESQ). At any given point in the training process, increasing the batch size yields strictly better performance. Notably, the performance curves for larger batch sizes maintain strong upward trajectory even at 250k steps, achieving substantially higher quality within the same step budget compared to smaller scales. These results suggest that CAT exhibits stable and predictable scaling behavior with respect to training batch size. Increasing data throughput leads to systematically higher-fidelity representations, highlighting the models suitability for large-scale audio tokenizer training where computational resources can be traded directly for reconstruction quality."
        },
        {
            "title": "6.1 Discrete Audio Tokenizers",
            "content": "Discrete audio tokenizers aim to encode continuous audio waveforms into sequences of discrete tokens and reconstruct audio signals from these tokens. Most existing methods adopt an RVQGAN-style framework, 11 which employs an encoderquantizerdecoder architecture combined with adversarial training to achieve high-fidelity audio reconstruction [15, 2527]. SoundStream [25] introduces quantizer dropout, enabling single tokenizer to support variable bitrate reconstruction. Encodec [26] further improves reconstruction quality by incorporating multi-scale STFT (MS-STFT) discriminator to capture audio structures at different temporal resolutions. DAC [27] simplifies the training process via factorized vector codes and employs complex STFT discriminators at multiple time scales to enhance phase modeling. Other acoustic codecs, including BigCodec [55], Stable-Codec [56] and TS3-Codec [57], focus on improving reconstruction quality under extremely low bitrates. Beyond reconstruction fidelity, recent studies have explored injecting semantic information into audio tokenizers to better support downstream generative and understanding tasks. common approach is knowledge distillation from pretrained teacher models [17, 18, 58]. SpeechTokenizer [50], Mimi [15], and Qwen3 TTS Tokenizer [59] align the encoder and quantizer representations with self-supervised speech models through distillation objectives. In contrast, XCodec2.0 [20], Higgs Audio Tokenizer [60], Dual Codec [22], and SAC [61] directly initialize the tokenizer encoder using pretrained SSL or ASR models, thereby reducing the difficulty of semantic modeling. scale-driven approach introduces semantic information into audio tokenizers through large-scale audio text supervision. Methods such as Baichuan Audio Tokenizer [16], XY-Tokenizer [21], and MiMo Audio Tokenizer [14] leverage audio-to-text tasks and massive paired datasets, enabling the tokenizer to implicitly learn rich semantic representations while maintaining high-fidelity reconstruction. Despite these advances, it remains unclear what characteristics make an audio tokenizer truly suitable for native audio language models. We argue that such tokenizer should minimize handcrafted priors and architectural constraints, and instead adopt simple and scalable design. Our goal is to obtain an audio tokenizer that is well aligned with the modeling needs of audio language models by scaling up both computation and data and training the tokenizer in an end-to-end manner."
        },
        {
            "title": "6.2 Audio Generation",
            "content": "Audio generation models have witnessed rapid progress in recent years [11, 6264], largely driven by the combination of discrete audio representations [18, 26, 65] and large-scale language modeling [3, 28]. dominant paradigm is to perform generation in compressed acoustic space, where audio is represented by sequences of discrete tokens produced by neural audio codecs [15, 26], and generation is formulated as language modeling problem. AudioLM [11] proposes hierarchical generation strategy that decomposes audio generation into three stages: semantic modeling, coarse acoustic modeling, and fine acoustic modeling. By combining representations from self-supervised speech models [18] with neural codec tokens, AudioLM achieves high-quality audio generation with strong long-term consistency. VALL-E [38] introduces hybrid autoregressive (AR) [66] and non-autoregressive (NAR) [67] architecture for speech synthesis, and demonstrates that scaling training data to tens of thousands of hours leads to the emergence of in-context learning capabilities for speech generation. Tortoise-TTS [37] further explores expressive text-to-speech by combining autoregressive sequence modeling with diffusion-based [68] refinement, enabling multi-voice and highly expressive synthesis. Along this line, an important trend is the move toward end-to-end audio generation [12, 31, 63, 69, 70], where single generative model directly produces audio tokens, rather than cascading multiple generative models (e.g., BERT-style or GPT-style language models, or diffusion-based generative models) in multi-stage pipeline. This simplification substantially reduces system complexity and error propagation across stages, while also improving training stability and inference efficiency. In the context of discrete token-based generation, MusicGen [12] systematically studies different multi-sequence modeling patterns and finds that the delay pattern enables single autoregressive model to perform both textand melody-conditioned music generation. More recent systems such as Moshi [15] adopts combina12 tion of temporal transformers and depth transformers to efficiently model long audio sequences, and further leverage streaming audio tokenizers to significantly reduce inference latency, enabling faster and more responsive audio generation. Beyond discrete tokenization, there is also growing body of work on audio generation based on continuous representations. These approaches augment auto-regressive large language models with local diffusion transformers (LocDiT) [7073], enabling the auto-regressive model to directly generate continuous latent representations and capture fine-grained acoustic details without explicit discretization. Overall, modern audio generation research is converging toward scalable, end-to-end architectures that tightly couple representation learning and generation. This trend highlights the increasing importance of well-designed audio tokenizers that are not only faithful in reconstruction quality, but also compatible with the architectural choices and scaling properties of audio language models [14, 15, 50, 74]."
        },
        {
            "title": "6.3 End-to-End Audio Language Models",
            "content": "End-to-end audio language models [1416, 7578] aim to unify speech understanding, generation, and reasoning within single large-scale model, moving beyond conventional three-stage pipelines that decompose speech processing into ASR, text-based language modeling, and TTS. By directly modeling audio representations using language modeling objectives, these systems aim to equip large language models with native audio understanding and generation capabilities. Early efforts in this direction include SpeechGPT [75], which is among the first large-scale models to support end-to-end speech interaction. SpeechGPT leverages discrete speech representations derived from selfsupervised speech encoders [17] and scales training on large amounts of cross-modal data, enabling large language models to acquire intrinsic conversational abilities across speech and text modalities. Subsequent works such as Spirit-LM [76], GLM4-Voice [77], and MOSS-Speech [78] further improve speechtext alignment by scaling up speechtext interleaved data, demonstrating that tightly coupled multimodal pretraining is critical for robust end-to-end speech understanding and generation. More recent systems push this paradigm to significantly larger scales. Models such as Kimi-Audio [79] and Qwen3-Omni [49] expand training data to hundreds of thousands or even millions of hours of audio, leading to substantially improved robustness in complex and diverse audio scenarios. These results suggest that endto-end audio language models benefit strongly from data scaling, similar to trends observed in text-only large language models [3, 28, 29]. An emerging line of work explores end-to-end audio language modeling based on information-preserving or near-lossless audio representations [15, 26]. Moshi [15] employs multi-stream speech-to-speech Transformer together with streaming audio tokenizer to enable full-duplex spoken dialogue, achieving lowlatency, highly responsive, and human-like interactions. MiMo-Audio [14] further demonstrates that scaling training data to the order of 100 million hours allows end-to-end audio language models to exhibit emergent few-shot in-context learning capabilities in audio, highlighting the strong interaction between tokenizer design, data scale, and model capacity. Overall, these studies highlight the central role of audio tokenizers in end-to-end audio language models. Similar to text tokenizers for LLMs, an audio tokenizer is expected to provide native discrete interface that scales effectively with autoregressive modeling. Accordingly, our goal is to develop unified, fully end-toend trained audio tokenizer built from homogeneous causal Transformers, supporting predictable scaling with data and model capacity while minimizing handcrafted constraints."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced CAT (Causal Audio Tokenizer with Transformer), fully end-to-end Transformerbased architecture that serves as unified discrete interface for autoregressive audio language modeling. 13 Leveraging the CAT architecture, we developed MOSS-Audio-Tokenizer, 1.6-billion-parameter audio tokenizer pre-trained from scratch on 3 million hours of diverse audio data, effectively acquiring general audio representations across various domains. Through the joint end-to-end optimization of all components including the encoder, quantizer, decoder, discriminators, and decoder-only LLM for semantic alignmentwithin purely causal framework, MOSS-Audio-Tokenizer achieves state-of-the-art reconstruction performance among open-source audio tokenizers. Furthermore, its discrete representations demonstrate exceptional performance in both downstream speech generation and speech understanding. Our findings position the CAT architecture as unified, scalable interface for the next generation of native audio foundation models."
        },
        {
            "title": "Contributors",
            "content": "Contributors Yitian Gong Chen, Ruixiao Li, Qinyuan Cheng, Shimin Li , Kuangwei Chen, Zhaoye Fei, Xiaogui Yang, Ke Chen, Yang Wang, Kexin Huang, Mingshu"
        },
        {
            "title": "Advisors\nXipeng Qiu",
            "content": "Affiliations: MOSI Intelligence Shanghai Innovation Institute Fudan University ytgong24@m.fudan.edu.cn Corresponding author: xpqiu@fudan.edu.cn"
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [4] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [6] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [7] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages 17151725, 2016. [10] Taku Kudo and John Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. [11] Zal√°n Borsos, Rapha√´l Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31:25232533, 2023. [12] Andrea Agostinelli, Timo Denk, Zal√°n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325, 2023. [13] Dongchao Yang, Haohan Guo, Yuanyuan Wang, Rongjie Huang, Xiang Li, Xu Tan, Xixin Wu, and Helen Meng. Uniaudio 1.5: Large language model-driven audio codec is few-shot audio task learner. Advances in Neural Information Processing Systems, 37:5680256827, 2024. [14] Dong Zhang, Gang Wang, Jinlong Xue, Kai Fang, Liang Zhao, Rui Ma, Shuhuai Ren, Shuo Liu, Tao Guo, Weƒ≥i Zhuang, et al. Mimo-audio: Audio language models are few-shot learners. arXiv preprint arXiv:2512.23808, 2025. [15] Alexandre D√©fossez, Laurent Mazar√©, Manu Orsini, Am√©lie Royer, Patrick P√©rez, Herv√© J√©gou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. [16] Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, et al. Baichuan-audio: unified framework for end-to-end speech interaction. arXiv preprint arXiv:2502.17239, 2025. 15 [17] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. [18] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244250. IEEE, 2021. [19] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [20] Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2569725705, 2025. [21] Yitian Gong, Luozhƒ≥ie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, and Xipeng Qiu. Xy-tokenizer: Mitigating the semantic-acoustic conflict in low-bitrate speech codecs. arXiv preprint arXiv:2506.23325, 2025. [22] Jiaqi Li, Xiaolong Lin, Zhekai Li, Shixi Huang, Yuancheng Wang, Chaoren Wang, Zhenpeng Zhan, and Zhizheng Wu. Dualcodec: low-frame-rate, semantically-enhanced neural audio codec for speech generation. arXiv preprint arXiv:2505.13000, 2025. [23] Yi-Chiao Wu, Israel Gebru, Dejan Markoviƒá, and Alexander Richard. Audiodec: An open-source streaming highfidelity neural audio codec. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [24] Simon Welker, Matthew Le, Ricky TQ Chen, Wei-Ning Hsu, Timo Gerkmann, Alexander Richard, and Yi-Chiao Wu. Flowdec: flow-based full-band general audio codec with high perceptual quality. arXiv preprint arXiv:2503.01485, 2025. [25] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-toend neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507, 2021. [26] Alexandre D√©fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. [27] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36:2798027993, 2023. [28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [29] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [31] Shƒ≥ia Liao, Yuxuan Wang, Tianyu Li, Yifan Cheng, Ruoyi Zhang, Rongzhi Zhou, and Yƒ≥in Xing. Fish-speech: Leveraging large language models for advanced multilingual text-to-speech synthesis. arXiv preprint arXiv:2411.01156, 2024. [32] Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi Dai, et al. Llasa: Scaling train-time and inference-time compute for llama-based speech synthesis. arXiv preprint arXiv:2502.04128, 2025. [33] Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, et al. Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens. arXiv preprint arXiv:2503.01710, 2025. 16 [34] Kun Xie, Feiyu Shen, Junjie Li, Fenglong Xie, Xu Tang, and Yao Hu. Fireredtts-2: Towards long conversational speech generation for podcast and chatbot. arXiv preprint arXiv:2509.02020, 2025. [35] Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, et al. E2 tts: Embarrassingly easy fully non-autoregressive zero-shot tts. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 682689. IEEE, 2024. [36] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, JianZhao JianZhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 62556271, 2025. [37] James Betker. Better speech synthesis through scaling. arXiv preprint arXiv:2305.07243, 2023. [38] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. [39] Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, et al. Seed-tts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. [40] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [41] Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Xueyao Zhang, Shunsi Zhang, and Zhizheng Wu. Maskgct: Zero-shot text-to-speech with masked generative codec transformer. arXiv preprint arXiv:2409.00750, 2024. [42] Bowen Zhang, Congchao Guo, Geng Yang, Hang Yu, Haozhe Zhang, Heidi Lei, Jialong Mai, Junjie Yan, Kaiyue Yang, Mingqi Yang, et al. Minimax-speech: Intrinsic zero-shot text-to-speech with learnable speaker encoder. arXiv preprint arXiv:2505.07916, 2025. [43] Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, and Jingchen Shu. Indextts2: breakthrough in emotionally expressive and duration-controlled auto-regressive zero-shot text-to-speech. arXiv preprint arXiv:2506.21619, 2025. [44] Jiayan Cui, Zhihan Yang, Naihan Li, Jiankun Tian, Xingyu Ma, Yi Zhang, Guangyu Chen, Runxuan Yang, Yuqing Cheng, Yizhi Zhou, et al. Glm-tts technical report. arXiv preprint arXiv:2512.14291, 2025. [45] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [46] Alexander Liu, Andy Ehrenberg, Andy Lo, Cl√©ment Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, et al. Voxtral. arXiv preprint arXiv:2507.13264, 2025. [47] Kai-Tuo Xu, Feng-Long Xie, Xu Tang, and Yao Hu. Fireredasr: Open-source industrial-grade mandarin speech recognition models from encoder-decoder to llm integration. arXiv preprint arXiv:2501.14350, 2025. [48] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [49] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report, 2025. URL https://arxiv.org/abs/2509.17765. [50] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. [51] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 17 [52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [53] Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechgpt-gen: Scaling chain-ofinformation speech generation. arXiv preprint arXiv:2401.13527, 2024. [54] Xueyao Zhang, Xiaohui Zhang, Kainan Peng, Zhenyu Tang, Vimal Manohar, Yingru Liu, Jeff Hwang, Dangna Li, Yuhao Wang, Julian Chan, et al. Vevo: Controllable zero-shot voice imitation with self-supervised disentanglement. arXiv preprint arXiv:2502.07243, 2025. [55] Detai Xin, Xu Tan, Shinnosuke Takamichi, and Hiroshi Saruwatari. Bigcodec: Pushing the limits of low-bitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024. [56] Julian Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, and Xubo Liu. Scaling transformers for low-bitrate high-quality speech coding. arXiv preprint arXiv:2411.19842, 2024. [57] Haibin Wu, Naoyuki Kanda, Sefik Emre Eskimez, and Jinyu Li. Ts3-codec: Transformer-based simple streaming single codec. arXiv preprint arXiv:2411.18803, 2024. [58] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022. [59] Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang, Xiong Wang, Zhifang Guo, Ziyue Jiang, Hongkun Hao, Zishan Guo, et al. Qwen3-tts technical report. arXiv preprint arXiv:2601.15621, 2026. [60] BosonAI. Higgs https://github.com/boson-ai/higgs-audio, 2025. audio v2: Redefining expressiveness in audio generation. [61] Wenxi Chen, Xinsheng Wang, Ruiqi Yan, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiquan Li, Yuzhe Liang, Hanlin Wen, Shunshun Yin, et al. Sac: Neural speech codec with semantic-acoustic dual-stream quantization. arXiv preprint arXiv:2510.16841, 2025. [62] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D√©fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022. [63] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. [64] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang In Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. International Conference on Machine Learning, pages 1391613932. PMLR, 2023. [65] Haina Zhu, Yizhi Zhou, Hangting Chen, Jianwei Yu, Ziyang Ma, Rongzhi Gu, Yi Luo, Wei Tan, and Xie Chen. arXiv preprint Muq: Self-supervised music representation learning with mel residual vector quantization. arXiv:2501.01108, 2025. [66] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [67] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [68] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [69] Ziqian Ning, Huakang Chen, Yuepeng Jiang, Chunbo Hao, Guobin Ma, Shuai Wang, Jixun Yao, and Lei Xie. Diffrhythm: Blazingly fast and embarrassingly simple end-to-end full-length song generation with latent diffusion. arXiv preprint arXiv:2503.01183, 2025. [70] Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weƒ≥iang Xu, Hangbo Bao, Zehua Wang, et al. Vibevoice technical report. arXiv preprint arXiv:2508.19205, 2025. 18 [71] Zhƒ≥un Liu, Shuai Wang, Sho Inoue, Qibing Bai, and Haizhou Li. Autoregressive diffusion transformer for text-tospeech synthesis. arXiv preprint arXiv:2406.05551, 2024. [72] Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, et al. Ditar: Diffusion transformer autoregressive modeling for speech generation. arXiv preprint arXiv:2502.03930, 2025. [73] Yixuan Zhou, Guoyang Zeng, Xin Liu, Xiang Li, Renjie Yu, Ziyang Wang, Runchuan Ye, Weiyue Sun, Jiancheng Gui, Kehan Li, et al. Voxcpm: Tokenizer-free tts for context-aware speech generation and true-to-life voice cloning. arXiv preprint arXiv:2509.24650, 2025. [74] Ruibin Yuan, Hanfeng Lin, Shuyue Guo, Ge Zhang, Jiahao Pan, Yongyi Zang, Haohe Liu, Yiming Liang, Wenye Ma, Xingjian Du, et al. Yue: Scaling open foundation models for long-form music generation. arXiv preprint arXiv:2503.08638, 2025. [75] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. [76] Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, et al. Spirit-lm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052, 2025. [77] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. [78] Xingjian Zhao, Zhe Xu, Qinyuan Cheng, Zhaoye Fei, Luozhƒ≥ie Jin, Yang Wang, Hanfu Chen, Yaozhou Jiang, Qinghui Gao, Ke Chen, et al. Moss-speech: Towards true speech-to-speech models without text guidance. arXiv preprint arXiv:2510.00499, 2025. [79] Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. [80] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [81] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [82] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [83] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE, 2015. [84] Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research into industrial scale. arXiv preprint arXiv:1808.10583, 2018. [85] Cees Taal, Richard Hendriks, Richard Heusdens, and Jesper Jensen. short-time objective intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE international conference on acoustics, speech and signal processing, pages 42144217. IEEE, 2010. [86] Antony Rix, John Beerends, Michael Hollier, and Andries Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In 2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221), volume 2, pages 749752. IEEE, 2001. [87] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. [88] Zafar Rafii, Antoine Liutkus, Fabian-Robert St√∂ter, Stylianos Ioannis Mimilakis, and Rachel Bittner. The musdb18 corpus for music separation. 2017. 19 [89] Series. Method for the subjective assessment of intermediate quality level of audio systems. International Telecommunication Union Radiocommunication Assembly, 2, 2014. [90] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [91] Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, et al. Cosyvoice 3: Towards in-the-wild speech generation via scaling-up and post-training. arXiv preprint arXiv:2505.17589, 2025. [92] Jiaqi Li, Yao Qian, Yuxuan Hu, Leying Zhang, Xiaofei Wang, Heng Lu, Manthan Thakker, Jinyu Li, Sheng Zhao, and Zhizheng Wu. Flexicodec: dynamic neural audio codec for low frame rates. arXiv preprint arXiv:2510.00981, 2025. [93] OpenAudio. Openaudio s1: cutting-edge text-to-speech model that performs like voice actors. https://openaudio.com/blogs/s1, 2024. [94] Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-audio: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946, 2025."
        },
        {
            "title": "Appendix Contents",
            "content": "A More Details Of MOSS-Audio-Tokenizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Architecture . . . . . . . . . . A.2 Dataset and Optimization. . . A.3 Training schedule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Details on Evaluation of Audio Tokenizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Reconstruction Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Results Of Subjective Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Baseline Audio Tokenizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Details Of Bitrate Controllable Speech Generation . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Architecture . . . . C.2 Training Details . . C.3 Inference Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 22 22 22 22 23 25 25 25 25 More Details on Baseline Text-to-Speech Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Speech Understanding On CAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 More Details Of MOSS-Audio-Tokenizer A.1 Architecture The encoder and decoder of MOSS-Audio-Tokenizer each consist of 68 causal Transformer blocks with 10 sliding-window attention, enabling efficient streaming inference. To progressively reduce the sequence length, the encoder inserts patchify operations [80] at the input stage and after layers 12, 24, and 36, with patch sizes of 240/2/2/2, respectively. Since patchify operations modify the feature dimensionality, linear projection is applied after each patchify stage to map the hidden states to the corresponding Transformer block dimension. This design maps raw 24 kHz waveforms to low frame rate of 12.5 Hz. The encoder is composed of four stages with hidden dimensions of 768, 768, 768, and 1280, respectively. These stages contain 12, 12, 12, and 32 Transformer blocks. For each stage, the feed-forward network (FFN) dimension is set to four times the corresponding hidden dimension. Multi-head self-attention uses 12, 12, 12, and 20 attention heads for the four stages, respectively. All Transformer blocks employ rotary positional embeddings (RoPE) [81]. The decoder mirrors the encoder architecture in fully causal manner. Both the encoder and decoder contain approximately 0.8B parameters and are trained from scratch. Discrete tokenization is performed using 32-layer residual vector quantizer (RVQ). Each layer uses codebook of size 1024 with factorized vector quantization (latent dimension 8) [27] and L2-normalized codes. Quantizer dropout with probability 1.0 is applied during training to enable variable-bitrate tokenization. To encourage semantically structured discrete representations, we attach 0.5B decoder-only causal language model [51] for audio-to-text supervision, which autoregressively predicts text conditioned on the quantizer outputs. The audio-to-text tasks include ASR, multi-speaker ASR, and audio captioning. For adversarial training, we employ multi-period discriminator [26] and complex STFT discriminator [27]. All componentsencoder, quantizer, decoder, semantic head, and discriminatorsare optimized jointly in an end-to-end manner. A.2 Dataset and Optimization. We train MOSS-Audio-Tokenizer on approximately 3M hours of speech, sound, and music data, covering both clean and in-the-wild recordings, and mixing audio-only and paired (audio, text) samples. For samples with available transcriptions or captions, we apply an auxiliary audio-to-text training objective, while audioonly samples are used without text supervision. We optimize both the generator and discriminators using AdamW [82] optimizer and conduct training in bfloat16 (bf16) precision. The generator is trained with learning rate of 1 104 and weight decay of 0.01, while no weight decay is applied to the discriminators. The loss weights are set to ùúÜsem=20, ùúÜrec=15, ùúÜcmt=0.25, ùúÜcode=1.0, ùúÜadv=1.0, and ùúÜfeat=2.0. A.3 Training schedule. Due to computational constraints, we adopt two-stage training schedule to improve training efficiency: nonadversarial pretraining without discriminator-related losses for 520k steps (batch size 1536, approximately 5 hours of audio per batch), followed by adversarial finetuning for 500k steps (batch size 768). All modules are optimized end-to-end without pretrained encoders or semantic teachers [15, 17, 19, 20, 50]."
        },
        {
            "title": "B More Details on Evaluation of Audio Tokenizers",
            "content": "B.1 Reconstruction Evaluation Protocol We evaluate the reconstruction quality of MOSS-Audio-Tokenizer and open-source audio tokenizers across three domains: speech, sound, and music. 22 Objective evaluation. For speech reconstruction, we conduct evaluations on LibriSpeech test-clean (English) [83] and AISHELL-2 (Chinese) [84]. We report speaker similarity (SIM), computed as the cosine similarity between speaker embeddings extracted from the original and reconstructed audio using pretrained speaker verification model2. In addition, we report short-time objective intelligibility (STOI) [85] and perceptual evaluation of speech quality (PESQ) [86]. For sound and music reconstruction, following prior work [27], we evaluate on the AudioSet evaluation subset [87] and MUSDB [88]. We report mel-spectrogram distance and short-time Fourier transform (STFT) distance as objective metrics. Subjective evaluation. In addition to objective metrics, we conduct crowd-sourced listening test based on the MUSHRA protocol [89]. In this test, each listener rates the perceptual quality of reconstructed audio samples on 1100 scale. For tokenizers that support variable bitrate decoding, we report results at multiple bitrates to characterize reconstruction quality across different bitrate regimes. B.2 Results Of Subjective Evaluation We conduct subjective evaluations on speech data to compare MOSS-Audio-Tokenizer with opensource audio tokenizers. For tokenizers that support variable bitrates, we report subjective scores at multiple bitrates. The results are shown in Figure 7. Overall, MOSS-Audio-Tokenizer achieves strong and consistent performance across wide range of bitrates, indicating high perceptual quality in reconstructed speech. For Encodec, DAC, and SpeechTokenizer, the subjective scores are competitive at higher bitrates but degrade noticeably at lower bitrates. In contrast, audio tokenizers designed for specific target bitrate (e.g., BigCodec, XCodec 2.0, XY-Tokenizer, and the Qwen3 TTS Tokenizer) perform well at their respective training bitrates, where their perceptual quality is competitive with MOSS-Audio-Tokenizer at comparable bitrates. Figure 7 MUSHRA subjective evaluation results. Overall, these results demonstrate that MOSS-Audio-Tokenizer provides scalable and robust tokenizer for general audio, enabling high-fidelity compression and reconstruction of speech, sound, and music across wide range of bitrates. B.3 Baseline Audio Tokenizers In this section, we provide additional implementation and configuration details for the baseline audio tokenizers reported in Table 1. Unless otherwise specified, for models based on vector quantization, the target bitrate is controlled during evaluation by truncating residual vector quantization (RVQ) codes to the first several layers. Encodec. We evaluate the official causal EnCodec model operating at 24 kHz for monophonic audio3 [26] and it contains approximately 14 parameters. 2https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification 3https://huggingface.co/facebook/encodec_24khz 23 DAC (Descript Audio Codec). DAC [27] is neural audio codec designed for high-fidelity waveform reconstruction using carefully engineered discriminators and improved vector quantization strategies. We use the official 24 kHz monophonic model for evaluation. The released checkpoint contains approximately 74 parameters. SpeechTokenizer. We adopt the official speechtokenizer_hubert_avg model4, which is trained on monophonic speech at 16 kHz [50]. SpeechTokenizer distills HuBERT representations using the first layer of residual vector quantization, enabling effective disentanglement of speech information, and further supports unified speech language model (USLM). The model contains approximately 103.67 parameters. Mimi. We evaluate the official Mimi codec5 [15]. Mimi operates on monophonic audio at 24 kHz and produces discrete audio tokens at frame rate of 12.5 Hz, while supporting streaming encoding and decoding. BigCodec. We use the authors released checkpoint with the default 16 kHz monophonic configuration. BigCodec [55] employs single vector quantization (VQ) codebook with size of 8,192 and produces discrete tokens at an 80 Hz frame rate. The model contains approximately 159 parameters. Stable Codec. For Stable Codec, we use the released stable-codec-speech-16k-base checkpoint6, which operates on monophonic speech at 16 kHz. Stable Codec [56] adopts residual finite scalar quantization (RFSQ) bottleneck. Following the official recommendation, we apply the 1x46656_400bps and 2x15625_700bps FSQ bottleneck preset during evaluation. The base checkpoint contains approximately 953 parameters. XCodec2.0. XCodec2.0 is semantically enhanced speech codec that incorporates pre-trained speech encoder [18]. We use the authors released checkpoint7 and follow the official inference pipeline. XCodec2.0 encodes 16 kHz monophonic audio into discrete tokens at 50 Hz frame rate using single-layer vector quantizer. The released checkpoint contains approximately 822 parameters. XY-Tokenizer. XY-Tokenizer [21] is designed to mitigate the semanticacoustic conflict at ultra-low bitrates by jointly modeling semantic and acoustic information using two encoders. We evaluate the officially released checkpoint8. XY-Tokenizer encodes 16 kHz monophonic audio into discrete tokens at 12.5 Hz frame rate using an 8-layer RVQ (codebook size 1,024). Quantizer dropout is disabled in the released model. The tokenizer contains approximately 519 parameters. Higgs Audio Tokenizer. We evaluate the released Higgs-audio-v2-tokenizer checkpoint9 [60], which operates on monophonic audio at 24 kHz. The checkpoint used in our experiments contains approximately 201 parameters. MiMo Audio Tokenizer. MiMo-Audio-Tokenizer [14] is designed to support both waveform reconstruction and downstream language modeling. The tokenizer jointly optimizes semantic and reconstruction objectives on large-scale corpus, reportedly exceeding 11 million hours of audio. In our evaluation, we use the official released checkpoint10. The model contains approximately 1.2 parameters. 4https://huggingface.co/fnlp/SpeechTokenizer/tree/main/speechtokenizer_hubert_avg 5https://huggingface.co/kyutai/mimi 6https://huggingface.co/stabilityai/stable-codec-speech-16k-base 7https://huggingface.co/HKUSTAudio/xcodec2 8https://huggingface.co/fdugyt/XY_Tokenizer 9https://huggingface.co/bosonai/higgs-audio-v2-tokenizer 10https://huggingface.co/XiaomiMiMo/MiMo-Audio-Tokenizer 24 Figure 8 Architecture of bitrate controllable audio modeling. During training, Progressive Sequence Dropout randomly truncates the number of active RVQ layers. During inference, when decoding with fixed depth ùëò, only the first ùëò RVQ tokens are provided as input at each time step, and the Depth Transformer autoregressively predicts only these ùëò tokens, while finer RVQ layers are omitted. Qwen3 TTS Tokenizer. Qwen3-TTS-Tokenizer [59] is the discrete speech tokenizer used in Qwen3-TTS for speech generation and streaming text-to-speech. We evaluate the released tokenizer checkpoint11 on monophonic audio at 24 kHz. The tokenizer encodes waveforms into discrete tokens at frame rate of 12.5 Hz and contains approximately 170 parameters."
        },
        {
            "title": "C More Details Of Bitrate Controllable Speech Generation",
            "content": "C.1 Architecture The Temporal Transformer is initialized from Qwen3-1.7B [7]. The Depth Transformer is randomly initialized and consists of 4 Transformer blocks with hidden size of 1536 and an FFN dimension of 8960. C.2 Training Details We adopt global batch size of 1.35M tokens, including both text tokens and speech tokens, where speech tokens are counted at frame rate of 12.5 Hz. During training, text tokens and speech tokens are concatenated along the temporal dimension to form the input sequence for the TTS model. FlashAttention-2 [90] is used to accelerate training. All models are trained using the AdamW optimizer with peak learning rate of 2 104. For ablation studies, models are trained for 50k steps, while the final models are trained for 200k steps. C."
        },
        {
            "title": "Inference Details",
            "content": "At inference time, to maintain consistency with the training procedure, we synthesize speech in continuationbased manner. Given prompt audio with its transcription and target text to be synthesized, we concatenate the prompt transcription tokens xprompt, the target text tokens xsyn, and the prompt audio tokens sprompt 11https://huggingface.co/Qwen/Qwen3-TTS-Tokenizer-12Hz 25 into single input sequence. The TTS model then autoregressively predicts the speech tokens corresponding to the target text. Finally, the predicted speech tokens are decoded into waveforms using the CAT decoder. More Details on Baseline Text-to-Speech Systems We compare our CAT-TTS system with wide range of open-source text-to-speech (TTS) models. These models can be broadly categorized into three groups. The first group consists of cascaded TTS systems that employ multiple generative models, such as AR+NAR or NAR+NAR architectures. Representative examples include MaskGCT [41], FireRedTTS [34], CosyVoice2 [40], Qwen2.5-Omni [48], CosyVoice3 [91], IndexTTS2 [43], FlexiCodec-TTS [92], and GLM-TTS [44]. The second group includes purely non-autoregressive TTS systems, such as F5-TTS [36]. The third group comprises prior fully autoregressive TTS models based on either discrete or continuous representations, including Llasa [32], SparkTTS [33], OpenAudio-s1 [93], HiggsAudio-v2 [60], FireRedTTS2 [34], DiTAR [72], and VoxCPM [73]. CAT-TTS adopts purely autoregressive architecture based on discrete tokens to perform zero-shot TTS in an end-to-end manner, directly generating speech from text without relying on predefined intermediate representations, such as semantic tokens [17, 40]. Moreover, CAT-TTS supports variable-bitrate speech generation through Progressive Sequence Dropout."
        },
        {
            "title": "E Speech Understanding On CAT",
            "content": "Table 4 ASR performance comparison. Model Size EN-WER ZH-CER Whisper-Large-v3 Voxtral Small-24B FireredASR-AED Qwen2-Audio-Base Baichuan-Audio-Base Step-Audio-Chat Qwen2.5-Omni Kimi-Audio CAT-ASR (Ours) 1.5B 24B 1.1B 7B 7B 130B 7B 7B 1.7B 2.90 1.53 1.93 1.74 3.02 3.11 2.37 1.28 2.96 5.80 13.80 3.00 3.08 3.87 3.60 2.56 2.56 3.44 We explore the capability of CAT for speech understanding tasks by developing CAT-ASR. Specifically, we investigate whether CAT tokens can be directly used as inputs to large language model (LLM) for automatic speech recognition (ASR), in order to evaluate the alignment between CAT and text as well as the information preservation of the discrete speech representation. We adopt Qwen3-1.7B [7] as the backbone LLM. To enable speech understanding, we initialize set of 32 speech tokens in the vocabulary and directly feed the discretized CAT speech tokens into the LLM. For each speech frame, the tokens along the RVQ dimensions are summed and treated as single input embedding to the LLM. The model is then trained in fully autoregressive manner to predict the corresponding text sequence given the speech token inputs. The model is trained on an internal dataset consisting of approximately 2 million hours of paired (audio, text) data. We use global batch size of 1M tokens and train the model for 200k steps with warmup of 4k steps. The Adam optimizer is adopted with peak learning rate of 5105. All experiments are conducted without any additional alignment or auxiliary supervision beyond the standard ASR objective. We evaluate the trained CAT-based ASR model on both English and Chinese benchmarks. For English, we report word error rate (WER) on the LibriSpeech test-clean set [83]. For Chinese, we report character error rate (CER) on the AIShell-2 iOS subset [84]. We compare our model with range of previous open-source ASR systems and speech-language models [16, 19, 4548, 79, 94], as summarized in Table 4. As shown in Table 4, CAT-ASR achieves competitive performance across both English and Chinese benchmarks. These results suggest that CAT tokens retain sufficient linguistic content and exhibit good alignment with text, enabling effective speech understanding when directly consumed by an LLM. We believe CAT-ASR can be further improved by scaling up paired training data and model capacity."
        }
    ],
    "affiliations": [
        "MOSI.AI"
    ]
}