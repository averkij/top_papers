{
    "paper_title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale",
    "authors": [
        "Daniel Goldstein",
        "Eric Alcaide",
        "Janna Lu",
        "Eugene Cheah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement. Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 5 0 0 3 0 . 5 0 5 2 : r RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale Daniel Goldstein1,2, Eric Alcaide2,3, Janna Lu1,4, and Eugene Cheah1, 1Recursal AI, 2EleutherAI, 3Dalle Molle Institute for Artificial Intelligence USI-SUPSI, 4George Mason University"
        },
        {
            "title": "Abstract",
            "content": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than $2,000 USD at todays prices, yet quality at inference remains close to the original transformer. These models achieve state-of-theart downstream performance across set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement."
        },
        {
            "title": "1 Introduction",
            "content": "Linear attention transformer variants employing compressive state have begun to match or even exceed traditional softmax attention based transformers in quality across many metrics. (Peng et al., 2025) This is important for long sequences at inference time, as linear attention is computable in O(1) time per token instead of O(N) time for softmax transformers, and avoids the expensive memory bandwidth usage of Key Value cache. (Arora et al., 2023) However, the cost of training usable large models at scale is cost prohibitive for all but the largest organizations. This is especially true for large language models, where SoTA results have often required training on over ten trillion tokens of data. (Qwen et al., 2025) (Grattafiori et al., 2024) Name Tokens Required Stages SUPRA (Mercat et al., 2024) LOLCatsb (Zhang et al., 2024a) MOHAWK (Bick et al., 2024b) Mamba in the Llama (Wang et al., 2025) DiJiangb (Chen et al., 2024) ARWKV (Yueyu et al., 2025) Llamba (Bick et al., 2025) RADLADS (ours) 100B 40M 3-5B 20B 40B 60M/830M 8-12B 350-700M 1 2 3 2 1 2/3 3 3 acc. ratio (less random chance) Lambada MMLU Others 0.913 - 0.938 0.608 - 0.893 a0. 0.216 -0.288 -0.047 0.519 0.720 0.801 a0.837 0.869 0.635 0.924 0.757 - 0.935 a0.987 0.983 0.924 1.013 Table 1: Recent softmax attention to purely recurrent model conversions up to 8B parameters Ratios of best student to teacher accuracy scores, less random chance accuracy (0%, 25%, or 50%). All 0-shot except for 5-shot LolCATs mmlu. Lambada_openai (Paperno et al., 2016) and MMLU (Hendrycks et al., 2021) shown. Others means avg. of arc_c_norm, arc_e (Clark et al., 2018), piqa (Bisk et al., 2020), winogrande (Sakaguchi et al., 2021), hellaswag_norm (Zellers et al., 2019). Llamba 8B was distilled from 70B, so generous comparison. See Table 2 for RADLADS 7B from 72B Neither model weights nor the missing scores were published for this pure RNN model by its authors. 1Training Code: https://github.com/recursal/RADLADS-paper Models: https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 1 Figure 1: Conversion Method Steps We present distillation method that yields comparable performance to the original teacher model, while using only between 350-700 million tokens, less than 0.005% of the tokens required to pre-train SoTA transformers. This approach enables the creation of large linear attention language models without incurring the extreme costs of multi-trillion token training runs. It also opens up new avenues for researchers who work on the next generation of compressive state attention variants to test, train, and release models containing their new designs at scale. While converting models, we found that pre-existing RWKV architectural choices were sometimes imperfectly matched to the needs of conversion. As result, we developed two new architectures RAD-RWKV6 (\"RADFinch\") and RAD-RWKV7 (\"RADGoose\") based on RWKV-6 and RWKV-7 that allow conversion to proceed smoothly and the resultant model to be faster to inference than had we used the original RWKV-6 and RWKV-7 designs. We share these new architectural details, as well as some of the reasons behind the changes. We hope that other researchers may find this useful in converting transformers to their own linear attention designs. Our main contributions are: Details of the RADLADS conversion process, including stepwise instructions, specific hyperparameters, token counts, and dataset choice. Two new simplified RWKV based architectures: RAD-RWKV6 (\"RADFinch\") and RADRWKV7 (\"RADGoose\"), used to convert softmax attention efficiently to linear attention. Public release of several converted Qwen models: QRWKV6-7B (\"Qwerky-7B-Base\"), QRWKV6-7B-Instruct (\"Qwerky-7B\"), QRWKV7-7B-Instruct, QRWKV6-32B-Instruct (\"Qwerky-32B\"), QRWKV6-72B-Instruct (\"Qwerky-72B\"), and QRWKV6-QwQ-32B (\"Qwerky-QwQ-32B\"). Open source conversion code, allowing anyone to adapt the RADLADS process to transformer of their choice and emit highly performant converted model, including for other recurrent linear attention and/or compressive state architectures."
        },
        {
            "title": "2 Background",
            "content": "There have been many attempts at efficiently converting pre-trained transformer models to pure linear attention or other recurrent compressive state architectures. Early work, like Gerstenberger et al. (2020), performs full model logit distillation to freshly initialized student model, and requires very long training cycle. T2R (Kasai et al., 2021) attempts to reduce this training time by keeping most of the original model intact. It swaps softmax attention for traditional linear attention with learnable MLP feature map, and finetunes the resultant model on approximately 3.5 billion tokens of data. Other projects like DiJiang (Chen et al., 2024), XATL (Choi, 2024), and SUPRA (Mercat et al., 2024) combine these two techniques, but still require 100B tokens of training or more, and exhibit lacking performance on some popular benchmarks like MMLU. More recently, new model architectures such as GSA (Zhang et al., 2024b) have been proposed, which are designed to adapt well to SUPRA-like conversion process. Post-conversion downstream performance is improved, but these still require long training cycles. Other works such as Mamba in the Llama (Wang et al., 2025) have focused on pipeline of progressive distillation followed by supervised fine-tuning (Kim & Rush, 2016) and direct preference optimization (Rafailov et al., 2024). Mamba in the Llama uses 20B tokens of training. However, it focuses on hybrid models rather than pure RNNs, and performs poorly when the softmax attention is removed completely. recent development in the conversion process, featured in LOLCats (Zhang et al., 2024a) and MOHAWK (Bick et al., 2024a) has been to separate the conversion process into two phases: attention alignment and full model knowledge distillation. The first phase aligns attention scores and/or attention hidden states with the teacher model, and the second performs traditional logit distillation. MOHAWK still requires training on large amount of data. LOLCats requires only 40m tokens of training, but benchmark scores remain low, with MMLU accuracy lower than random guessing. To address this deficiency, they create hybrid with full softmax attention in sliding window (SWA), which improves scores but is no longer purely recurrent model. We theorize that their low scores are due to the use of LoRAs for training, as well as vanilla linear attention rather than more advanced recurrent architecture. This is most evident when examining their relative MMLU scores, which even with the inclusion of SWA remain low compared to ours. We shared our early code, techniques, and 32B model with another team, who worked with those and subsequently published ARWKV (Yueyu et al., 2025), which converts Qwen2.5-7B-Instruct to use standard RWKV-7 sequence mixer. As we discuss later, specific choices for weight transfer, hyperparameters, dataset, and architecture matter significantly. This is visible in the significantly higher downstream performance of our own Qwen2.5-7B-Instruct conversion, as shown in Tables 1 and 2."
        },
        {
            "title": "3 Design Choices",
            "content": "Our work builds upon this varied foundation of historical attempts to convert softmax attention transformers to linear attention, and we attempt to use all of the best parts in single process. We transfer attention weights, align attention hidden states, distill, finetune, develop strong and appropriate RNN attention-replacement architecture, use good dataset, and find the proper sequence of distillation hyperparameters. Combining all of these together results in lower training token counts while achieving new state-of-the-art in pure RNN downstream performance. The reduction in token count allows us to affordably convert very large models, up to 72 billion parameters so far. We find that choosing good RNN architecture is an important part of any highly successful conversion process. T2R, SUPRA, and LOLCats all use traditional linear attention or make small changes (e.g. removing the denominator in favor of normalization). Other modern RNN architectures have been shown to be more expressive than linear attention, even when including such changes. (Sun et al., 2023; Yang et al., 2024; Peng et al., 2025) But choosing strong architecture is not always enough. We initially used an unmodified RWKV-6 architecture in our experiments, and found disappointing performance. Removing the off-by-one decay and bonus by using Gated Linear Attention kernel allowed the model to fit the original softmax attention hidden states much 3 more closely during step 1. And we find that RWKV-7 fits even closer and more rapidly during this step, resulting in much lower distillation loss with even less compute. Some components of modern RNN architectures that appear important for pretraining turned out to have little to no effect during conversion. And some have useful effects in one architecture but not others. One interesting example is RWKV tokenshift, variety of 1D short convolution. It was useful in RAD-RWKV6, but conferred essentially no benefit in RAD-RWKV7. Another such case is gating, which was extremely beneficial only at full rank in RAD-RWKV6, but performed perfectly well in RAD-RWKV7 with reduced rank. Careful testing of each component can help discover the most efficient variant for given architecture for use in softmax attention conversion. Choice of dataset appears to matter quite bit. After trying several custom datasets, as well as fineweb and fineweb-edu (Lozhkov et al., 2024), we settled on DCLM (Li et al., 2024b) for all our conversions. We theorize that the best choice of dataset may depend upon the teacher models pre-training data distribution. DCLM worked exceptionally well for us when converting Qwen models. Training for the correct number of tokens and with good choices for learning rates was also very important, both for efficiency of training and for final downstream performance. More training was not always better. We arrived at our final hyperparameters after theorizing that most knowledge resides in the teacher models MLPs and embeddings. Therefore, we start at high learning rate to rapidly align the attention hidden states, annealing to approximately the final learning rate that the teacher model saw during pretraining. We tried various learning schedules, and cosine worked best. We keep the learning rate fixed during steps 2 and 3. This is an attempt to avoid catastrophic forgetting or overly significantly changing the knowledge encoded in the original teacher MLPs."
        },
        {
            "title": "4 Method",
            "content": "In RADLADS, our student model architecture is an exact copy of the teacher model architecture, with the exception of the attention blocks. These are replaced with choice of purely recurrent attention-replacement time mixing blocks. In this paper we detail two such replacements, which are based on RWKV6-C2 and RWKV7, which we name RAD-RWKV6 (\"RADFinch\") and RADRWKV7(\"RADGoose\") respectively. Our student models retain the runtime repetition of keys and values from Grouped Query Attention (GQA) (Ainslie et al., 2023) in the teacher model, if present. Depending on the attention-replacement chosen, we optionally include rotary positional embeddings (RoPE) (Su et al., 2023) from the teacher model. In cases where we include RoPE, it is applied at exactly the same point as in the teacher model. See Appendix for detailed formulas for RAD-RWKV6 and RAD-RWKV7. The protocol for converting multilayer transformer model from one sequence mixing operation to another can be described as 3-step process: Setup: Attention Weights Transfer. All attention related weights (Wq ,Wk ,Wv ,Wo) from the teacher model are transferred to the student model. Step 1: Attention Hidden State Alignment. Each student sequence mixing layer is trained to approximate the corresponding teacher model attention layers hidden state outputs. This can be done sequentially or in parallel. Step 2: Knowledge Distillation. All model layers are trained together to approximate original model de-embedding output logits via Kullback-Leibler Divergence Loss. Step 3: Fine Tuning. The new model is fine-tuned at longer contexts."
        },
        {
            "title": "4.1 Setup: Attention Weights Transfer",
            "content": "When equivalent parameters are available (e.g. query = receptance, key = key, value = value, output = output) we initialize them to the teacher weights, and otherwise to standard pretraining initialization (e.g. in RWKV6 or RWKV7 models). Some weights, such as tokenshift, are set such that they mimic the teacher model and have no immediate effect. The goal is to initially match the teacher but learn new more expressive behavior as needed throughout distillation. 4 4.2 Step 1: Attention Hidden State Alignment Following notation from MLPmixer (Tolstikhin et al., 2021), we divide the layers of traditional transformers into pointwise operations (MLPs) and sequence mixing operations (Attention layers in standard transformers). In Step 1, we hold frozen copy of the full teacher model in memory, and add second trainable attention-replacement layer in parallel alongside each teacher attention layer, created with the replacement sequence mixing operation of choice (ex. RAD-RWKV6 or RAD-RWKV7). For loss, we employ an L2 distance (or optionally, mean squared error) objective between the student and teacher sequence-mixer hidden state outputs. As noted earlier, this step can be done sequentially layer by layer, or in parallel with all layers at once. In practice, we train all layers at once using single optimizer and whole model forward/backward pass. The sequence length of this step is usually 512. We find that 100M tokens is enough to converge to low stable loss during step 1. We run this step with cosine annealed learning rate beginning at 1e-3 and ending at 1e-5. The final learning rate should be similar to the final learning rate the teacher saw during pretraining. At the end of this step we remove the teacher attention layers from the model, leaving only complete student model. 4.3 Step 2: Knowledge Distillation (Modelwise Approximation of Logits) During this step, we load separate copy of the full teacher model and train the student model to approximate the logits of the teacher model in similar fashion to traditional teacher-student knowledge distillation training. (Hinton et al., 2015) We use loss equal to the Kullback-Leibler divergence of the student and teacher logits. Empirically, we find that the model resulting from this step reproduces most of the initial model performance across variety of benchmarks. The sequence length of this step is usually 512. We run this step with flat learning rate of 1e-5. This should be similar to the final learning rate the teacher saw during pretraining. We find that 250M tokens is generally sufficient, and that training more than around 500M tokens during step 2 does not improve english language benchmarks. Its possible that less frequently encountered tokens still improve beyond this point. This number is somewhat dependent on the quality of the attention replacement RNN used. For example, RAD-RWKV7 reaches better alignment during step 1 than RAD-RWKV6, and can therefore tolerate shorter step 2 training runs."
        },
        {
            "title": "4.4 Step 3: Context Length Extension",
            "content": "During this step we train the model on longer sequences with the objective of enhancing its long-context modeling capabilities. We use regular cross-entropy loss on target labels with no teacher model. This process is done as separate step for efficiency and memory reasons, instead of simply increasing the context length during steps 1 & 2. Steps 1 & 2 require part or all of both teacher and student models to be held in memory at the same time, while this is no longer the case in step 3. Step 3 also runs faster since it has only one model to execute, making it better candidate for longer context lengths. It is possible to reduce the memory usage in step 2 by saving teacher logits to disk or having them generated by separate machine, but the amount of VRAM saved would be relatively modest, since the teacher model requires neither optimizer states nor backpropagation. We run this step with the same flat learning rate of 1e-5 as step 2. We provide an alternative method for step 3 in situations where VRAM is at premium. In this alternative step 3a, we freeze all weights except for decay and tokenshift (if present), and increase the learning rate to 1 10 4. 5 Name lmbda mmlu arc_c arc_e hella piqa winog Mistral-7B-v0.1-SUPRA Mamba2-Llama3.0-8B-Instruct MOHAWK-Phi1.5-1.3B DiJiang-Llama2.0-7Ba Llamba-8Bb LOLCats-Llama3.0-8Ba ARWKV-7B Our RADLADS models: QRWKV6-7B-Instruct QRWKV6-7B-Instruct RoPE QRWKV7-7B-Instruct QRWKV7-7B-Instruct-from72Bb QRWKV6-32B-Instruct QRWKV6-QwQ-32B QRWKV6-72B-Instruct 0.913 0.608 0.938 - 0.951 - 0.893 0.970 0.969 0.982 1.016 0.986 0.991 1.004 0.216 0.519 -0.047 0.720 0.837 -0.029 0.801 0.871 0.880 0.924 0.893 0.909 0.899 0.899 0.722 0.724 0.830 c0.973 0.983 0.534 0. 1.043 1.054 1.026 1.071 1.066 1.025 1.015 0.911 0.868 0.968 c0.732 1.014 0.864 0.969 0.998 1.003 1.004 1.033 1.037 0.993 1.008 0.930 0.901 0.936 0.906 0.969 0.750 0.934 0.974 0.971 0.979 0.959 0.964 0.982 0.974 0.937 0.903 0.959 0.975 0.994 0.886 0. 1.030 1.018 1.013 1.026 1.005 1.020 0.968 0.846 0.391 0.927 - 0.975 0.143 0.874 1.007 0.981 1.041 1.139 1.235 1.135 1.123 Table 2: Ratios of benchmark accuracy less random chance for student RNN models Ratios of converted model to teacher model accuracy scores, less random chance accuracy (0%, 25%, or 50%). Name Mistral-7B-v0.1-SUPRA teacher: Mistral-7B-v0. Mamba2-Llama3.0-8B-Instruct teacher: Llama3.0-8B-instruct MOHAWK-Phi1.5-1.3B teacher: Phi1.5-1.3B-base DiJiang-Llama2.0-7Bb teacher: Llama2.0-7B Llamba-8Ba 1st teacher: Llama3.1-8B-Instruct 2nd teacher: Llama3.1-70B-Instruct LoLCATs-Llama3.0-8Bb teacher: Llama3.0-8B ARWKV-7B QRWKV6-7B-Instruct QRWKV6-7B-Instruct-RoPE QRWKV7-7B-Instruct-RoPE teacher: Qwen2.5-7B-Instruct QRWKV7-7B-Instruct-from72Ba teacher: Qwen2.5-72B-Instruct QRWKV6-32B-Instruct teacher: Qwen2.5-32B-Instruct QRWKV6-QwQ-32B teacher: Qwen2.5-QwQ-32B lmbda mmlu arc_c arc_e hella piqa winog 0.692 0.758 0.439 0.722 0.501 0.534 - - 0.694 0.730 0.757 - 0. 0.622 0.675 0.674 0.684 0.696 0.707 0.751 0.742 0.752 0.662 0.668 0.331 0.624 0.452 0. 0.242 0.418 0.407 0.468 0.610 0.680 0.823 0.238 0.533 0.624 0.657 0.661 0.682 0.717 0.667 0. 0.766 0.818 0.743 0.799 0.458 0.538 0.480 0.567 0.441 0.480 0.759 0. 0.741 0.816 0.740 0.756 0.427 c0.463 0.626 0.764 0.546 0.551 0.624 0.401 0. 0.522 0.563 0.567 0.558 0.550 0.572 0.632 0.609 0.587 0.564 0.556 0.825 0.817 0.867 0.726 0. 0.797 0.814 0.817 0.817 0.815 0.833 0.859 0.843 0.822 0.806 0.810 0.771 0.810 0.708 0. 0.602 0.626 0.694 0.740 0.776 0.793 0.847 0.656 0.799 0.768 0.790 0.789 0.793 0.805 0.782 0. 0.830 0.852 0.830 0.841 0.801 0.821 0.758 0.786 0.755 0.766 0.775 0. 0.809 0.811 0.837 0.765 0.731 0.792 0.803 0.799 0.798 0.794 0.801 0.836 0.812 0.810 0.804 0. 0.703 0.740 0.586 0.719 0.717 0.734 - - 0.733 0.739 0.787 0.533 0. 0.687 0.711 0.706 0.718 0.714 0.739 0.763 0.782 0.729 0.732 0.705 QRWKV6-72B-Instruct teacher: Qwen2.5-72B-Instruct 0.754 0. 0.775 0.834 0.638 0.632 0.865 0.859 0.857 0.874 0.825 0.836 0.796 0. Table 3: Benchmark accuracy scores for student RNN models & transformer teacher models All 0-shot except for 5-shot LolCATs mmlu. Shown are lambada_openai (Paperno et al., 2016), mmlu (Hendrycks et al., 2021), arc_c_norm, arc_e (Clark et al., 2018), piqa (Bisk et al., 2020), winogrande (Sakaguchi et al., 2021), hellaswag_norm (Zellers et al., 2019). aNot Available: Neither the model weights nor the missing scores were published by the model authors. bLlamba-8B and QRWKV7-7B-Instruct-from72B are distilled from larger 70B and 72B models, respectively, but are compared with Llama3-8B and Qwen2.5-7B-Instruct in Table 3. cThe DiJiang paper incorrectly lists 0.403 and 0.561 for arc_c and arc_e respectively. We checked against Mercat et al. (2024)s evals of this same teacher model and re-ran them here."
        },
        {
            "title": "5 Language Modeling Performance",
            "content": "In Table 2 we compare pure RNN models output from various conversion methods across set of standard benchmarks. These converted models were trained from variety of different teacher models and vary in terms of parameter count. Therefore, we compare them via ratio of benchmark accuracy scores between each student and teacher model, first subtracting the random chance accuracy for each benchmark. Negative ratios indicate that model performed worse than random chance. Note that on nearly every benchmark, the RADLADS \"Qwerky\" models obtain higher score ratios than models converted via any other method. (\"Qwerky\" is our shorthand for \"Qwen-RWKVlike\") In Table 3 Qwerky6-72B demonstrates new state-of-the-art in downstream performance for pure RNN language model. Next, in Tables 4 and 5 we show models converted to hybrid-attention variants, each containing some amount of softmax attention. Even when compared to these hybrid models that still employ softmax attention, the pure RNN Qwerky models show the highest MMLU ratios, and achieve similar or better ratios on other benchmarks. Name Attention mmlu arc_c arc_e hella piqa winog LoLCATs-Mistralv0.1-7B LoLCATs-Llama3.1-8B LoLCATs-Llama3.1-70B Mamba2-Llama3.0-8B-Instruct Mamba2-Llama3.0-8B-Instruct Mamba2-Llama3.0-8B-Instruct SWA 100% SWA 100% SWA 100% 12.5% 25% 50% 0.706 0.727 0.794 0.663 0.738 0.789 1.038 1.037 0.997 0.797 0.867 1. 1.014 1.015 0.963 0.898 0.907 0.951 0.995 1.001 0.994 0.948 1.037 1.072 0.981 1.069 0.970 0.937 1.004 1.099 1.000 0.839 0.801 0.611 0.676 0.982 Table 4: Ratios of benchmark accuracy less random chance for select student hybrid models (still containing some softmax attention) converted from transformer teachers Ratios of converted model to teacher model accuracy scores, less random chance accuracy (0%, 25%, or 50%). Name LoLCATs-Mistralv0.1-7B teacher: Mistralv0.1-7B LoLCATs-Llama3.1-8B teacher: Llama-3.1-8B LoLCATs-Llama3.1-70B teacher: Llama-3.1-70B Mamba2-Llama3.0-8B-Instruct Mamba2-Llama3.0-8B-Instruct Mamba2-Llama3.0-8B-Instruct teacher: Llama3.0-8B-Instruct Attention mmlu arc_c arc_e hella piqa winog SWA 100% 100% SWA 100% 100% SWA 100% 100% 12.5% 25% 50% 100% 0.514 0.624 0.549 0.661 0.677 0.788 0.508 0.537 0.557 0.639 0.549 0. 0.544 0.534 0.605 0.606 0.503 0.525 0.582 0.567 0.817 0.809 0.824 0.815 0.850 0. 0.758 0.764 0.788 0.816 0.807 0.810 0.791 0.790 0.846 0.850 0.731 0.777 0.795 0.758 0.815 0. 0.810 0.790 0.821 0.831 0.768 0.787 0.815 0.786 0.740 0.740 0.697 0.735 0.737 0. 0.634 0.648 0.715 0.719 Table 5: Benchmark accuracy scores for select hybrid models (still containing some softmax attention) converted from attention, and their teacher models Shown are mmlu (Hendrycks et al., 2021), arc_c_norm, arc_e (Clark et al., 2018), piqa (Bisk et al., 2020), winogrande (Sakaguchi et al., 2021), and hellaswag_norm (Zellers et al., 2019). All 0-shot except for 5-shot mmlu on LoLCATs and its teachers. Lambada scores were not available for these models."
        },
        {
            "title": "6 Ablation Studies",
            "content": "We performed several ablation studies, adding or removing individual mechanisms to the RADRWKV6 architecture at the 7B scale. For the \"use groupnorm\" ablation we added the use of GroupNorm and removed the = (1 w) state balancing mechanism originally from RWKV6C2. Results are shown in Table 6. 7 Arch ablation lambada MMLU arc_c arc_e hella piqa winog RAD-RWKV6 none 0. 0.6572 0.5631 0.8136 0.7901 0.8025 0. RAD-RWKV6 use rope 0.6740 0.6610 0.5666 0. 0.7886 0.7992 0.7056 RAD-RWKV6 no tokenshift 0. 0.6584 0.5546 0.8043 0.7874 0.8036 0. RAD-RWKV6 no gate 0.6590 0.6417 0.5444 0. 0.7842 0.7971 0.6930 RAD-RWKV6 use groupnorm 0. 0.6340 0.5648 0.8186 0.7865 0.7905 0. Table 6: Ablation benchmark accuracy scores All results are Qwen2.5-7B-Instruct converted to RAD-RWKV6 variations after 100m tokens of step 1 training followed by 500m tokens of step 2 training. Arc_c and hellaswag are normalized scores. Teacher model was Qwen2.5B-7B-Instruct except in non-instruct ablation"
        },
        {
            "title": "7 Training Details and Hyper-parameters",
            "content": "We use the DCLM (Li et al., 2024b) dataset for all three steps in the RADLADS attention conversion process. Step Tokens 1 2 3 100M 1 10 250M-700M 1 10 100M 1 10 LR 5 5 3 1 10 5 1 10 5 1 10 LR decay seqlen batch size cosine none none 512 512 32 96 96 adam betas, eps 8 0.9, 0.95, 1 10 8 0.9, 0.95, 1 10 8 0.9, 0.95, 1 10 Table 7: Hyper-parameters for each training step Model Size GPU Step 1 Tokens Opt Hours Step 2 Tokens Opt Hours Step 3 Tokens Opt Hours 7B 32B 72B 8x Mi300X 8x Mi300X 8x Mi300X DS1 100M DS1 100M 100M FSDP 0.75 2.5 7.50 500M DS1 500M FSDP 500M FSDP 5.5 27.0 54.0 100M DS1 100M FSDP 100M FSDP 1 3 a6 Table 8: Approximate conversion timing guidance 16384 ctxlen does not fit on single node at 72B scale 7.1 General We manually choose between DeepSpeed (Li et al., 2024a) ZeRO stages 1 or 2 and FSDP (Zhao et al., 2023), depending on VRAM availability. Typically we use DeepSpeed Zero stage 1 for our step 1 whenever possible, as it results in faster training and step 1 requires less VRAM than step 2. We find that FSDP is more VRAM efficient than DeepSpeed ZeRO Stage 3 for larger models, and we generally use it for steps 2 and 3."
        },
        {
            "title": "8 What Did Not Work",
            "content": "In the spirit of scientific transparency and to facilitate reproducibility and further experimentation in the field, we share some negative results from our experiments: Initial Attention Score Alignment We originally included an initial \"step 0\", which was similar to step 1 but used loss based on attention scores differences. For linear RNNs of the form attention scores can be represented as cumulative compositions of transforms: St +1 = St Gt + vt kT Ai = qi ( Gt )cd d (cid:75) = +1 8 This generalizes Mamba2, RWKV5, RWKV6, RWKV7, DeltaNet, and Gated DeltaNet, as well as others. See Table 1 in Peng et al. (2025) for detailed list of such RNNs. We hypothesized that matching attention scores up-front would accelerate training compared to Step 1. While final downstream performance was similar, we observed neither benefits in total training time nor lower final loss. On the other hand, extended training of step 0 incurred final higher loss. Skipping step 1 Starting directly with step 2 distillation and skipping step 1 entirely resulted in much lower performance model. The same level of convergence simply did not occur, with loss plateauing at higher minimum, even with longer training. De-novo initialization of attention weights Initializing sequence mixer QKVO weights as if they were untrained, instead of copying them from the teacher model, resulted in consistently worse yet surprisingly reasonable performance. This is compatible with the previous hypothesis stating that factual knowledge is stored mainly in MLP model weights. Freezing model weights Although it might seem intuitive that the MLPs and embeddings could be frozen during step 2, we find that this results in significantly reduced model performance. We theorize that the internal embedding and hidden state representations need to adapt somewhat to the new RNN sequence mixers use of channels. Through continued training, the MLPs may learn to route this information differently to avoid conflicts with pre-existing information flow from the teacher model. Larger batchsizes The number of optimizer steps appears to be of key importance during conversion. Consequently, increasing batchsize did not help the model converge faster. Additionally, we are careful to keep the learning rate low during steps 2 and 3 in order to avoid excess disturbance to the MLPs. Therefore, adjusting the learning rate higher to compensate for larger batchsizes would be undesirable. LoRA training We tried using LoRA to perform PEFT, but rank reduction was generally quite detrimental to performance. The one place we found it could work without causing problems was on the embeddings. Switching datasets We tried using various custom datasets during the context-length extension step. These seemed to create confused model that would use more and more adjectives as generation progressed. We leave exploration of the ideal dataset for this step to future work."
        },
        {
            "title": "9 Conclusions",
            "content": "We have shown that RADLADS provides cost-effective way to convert transformers employing quadratic softmax attention into inexpensive RNN models that feature linear time complexity and constant memory usage. This has potential to save energy, reduce research costs, and enable the rapid testing and deployment of new varieties of RNN architectures. We have already used it to produce and distribute new state-of-the-art open weight linear attention models. However, RADLADS does have known limitations. Currently, each new architecture design requires meticulous testing to improve its compatibility with the RADLADS protocol. As one example, RAD-RWKV7 appears to exhibit reduced training stability at larger (32B+) parameter and layer counts, and we are actively working to design architectural and training methodology changes to remedy this. New varieties of model interactions will test these conversions in ways that we cannot predict. For example, the details of how the conversion process impacts reasoning models like our converted QwQ model is still unknown and requires further testing. We have many ideas for future work to enhance and extend the RADLADS process. Converting to and from different architectures and testing of more varieties of datasets against each of these would help discover the principles for optimal conversion dataset design. We also believe that parts of our existing RAD-RWKV7 architecture can still be improved; balancing of state to create implicit normalization and then removal of groupnorm could further increase downstream performance."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https://arxiv.org/abs/2305.13245. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efficient language models, 2023. URL https://arxiv.org/abs/2312.04927. Albert Bick, Kevin Li, Eric Xing, Zico Kolter, and Albert Gu. Transformers to SSMs: Distilling quadratic knowledge to subquadratic models. arXiv preprint arXiv:2408.10189, 2024a. doi: 10.48550/arXiv.2408.10189. Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, and Albert Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models, 2024b. URL https://arxiv.org/abs/ 2408.10189. Aviv Bick, Tobias Katsch, Nimit Sohoni, Arjun Desai, and Albert Gu. Llamba: Scaling distilled recurrent models for efficient language processing, 2025. URL https://arxiv.org/abs/ 2502.14458. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, and Yunhe Wang. Dijiang: Efficient large language models through compact kernelization. ArXiv, abs/2403.19928, 2024. URL https://arxiv.org/abs/2403.19928. Sehyun Choi. Cross-architecture transfer learning for linear-cost inference transformers, 2024. URL https://arxiv.org/abs/2404.02684. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Alexander Gerstenberger, Kazuki Irie, Pavel Golik, Eugen Beck, and Hermann Ney. Domain robust, fast, and compact neural language models. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 79547958, 2020. doi: 10.1109/ICASSP40776.2020.9054399. Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, and Eugene Cheah. Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression, 2024. URL https://arxiv.org/abs/2407.12077. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph 10 Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, 11 Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. URL https://arxiv.org/abs/1503.02531. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into rnns, 2021. URL https://arxiv.org/abs/2103.13076. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation, 2016. URL https: //arxiv.org/abs/1606.07947. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Connor Holmes, Cheng Li, and Yuxiong He. Deepspeed data efficiency: Improving deep learning model quality and training efficiency via efficient data sampling and routing. arXiv preprint arXiv:2212.03597, 2024a. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024b. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing large language models, 2024. URL https://arxiv.org/abs/2405. 06640. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind, Tianyi Wu, Daniel 12 Wuttke, and Christian Zhou-Zheng. Rwkv-7 \"goose\" with expressive dynamic state evolution, 2025. URL https://arxiv.org/abs/2503.14456. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/ abs/2104.09864. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021. URL https: //arxiv.org/abs/2105.01601. Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models, 2025. URL https://arxiv.org/abs/ 2408.15237. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2024. URL https://arxiv.org/abs/ 2312.06635. Lin Yueyu, Li Zhiyuan, Peter Yue, and Liu Xiao. Arwkv: Pretrain is not what we need, an rnnattention-based language model born from transformer, 2025. URL https://arxiv.org/ abs/2501.15570. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Ré. Lolcats: On low-rank linearizing of large language models, 2024a. URL https://arxiv.org/abs/2410.10254. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, and Guohong Fu. Gated slot attention for efficient linear-time sequence modeling, 2024b. URL https://arxiv.org/abs/2409.07146. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023."
        },
        {
            "title": "B Architecture Detail",
            "content": "Notation In this section, we use to denote the model dimension. Bold capital letters represent trainable matrices, and vectors without subscript are trainable parameters. The first subscript denotes sequence position and second subscript denotes layer index, where necessary. We use the convention that all vectors are row vectors unless explicitly transposed, so all matrices operate on the right side, therefore aT is an outer product and abT is an inner one. We use the square subscript to denote placeholder for variable names and use the (cid:81) sign for cumulative matrix multiplication. B.1 RAD-RWKV6 Time Mixing RAD-RWKV6 is customized variation of RWKV6-C2 (\"Finch-C2\") (Goldstein et al., 2024), featuring Gated Linear Attention (Yang et al., 2024) kernel (so no bonus), and sigmoid gate. The use of the state balancing technique from RWKV6-C2 enables us to remove state normalization, improving downstream performance. Removal of bonus did not harm downstream performance. We do not typically use RoPE in this formulation. The data-dependent linear interpolation (ddlerp) between xt and xt 1 used in Finch Token Shift is defined as: lora(x) = λ + tanh(x A)B (1) ddlerp(a, b) = + (b a) lora(a + (b a) µx ) (2) where µx and each λ introduce trainable vector and Dz and each zD introduce pair of low rank trainable matrices, where is chosen size for each such pair. (cid:180) rt = ddlerpr (xt , xt 1)W , vt = ddlerpv (xt , xt 1)W , (cid:179) = σ ddlerpg (xt , xt 1)W wt = ddlerp (xt , xt 1)W , wt = exp(max( exp(loraw ( wt )), 5))}, kt = ddlerp (xt , xt 1)W , kt = kt (1 wt )d , , 0.5 receptance value gate decay precursor decay key precursor key Time mixing is performed by the t attention calculation: t = (cid:88) =1 diag (cid:33) (cid:195)t 1 (cid:89) =i vi R(D/h)(D/h) kT The t attention calculation can alternatively be written in recurrent manner: v 0 = 0, t = diag(wt ) t 1 + kT vt pt = LayerNorm(rt t ) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) Finally, the heads are recombined via reshaping so that pt RD , gated, and transformed into the output as follows: ot = (cid:161)g pt (cid:162)W RD (14) 14 B.2 RAD-RWKV7 Time Mixing RAD-RWKV7 is customized variation of RWKV-7 (Peng et al., 2025), with tokenshift removed, RoPE applied, and no bonus. The removal of tokenshift speeds up training and inference, and bonus had no beneficial impact on downstream performance. lerp(a, b, x) = + (b a) x, loramlp( , x, bias) = (x A)B + (λ if bias else 0), (15) (16) (17) (18) (19) (20) (21) (22) in-context learning rate key precursor) removal key replacement key value residual gate value precursor at = sigmoid(loramlpa(Identity, xt , bias=True)), kt = RoPE(xt ), κt = kt ξ, kt = kt lerp(1, at , α), νt = sigmoid(loramlpν(Identity, xt , bias=True)), ,l vt = ,0, = RoPE(xt ), (cid:40)v ,0, lerp(v ,l , νt ), layer = 0 layer 1 dt = loramlpd (tanh, xt , bias=True), 0.5σ(dt )), wt = exp(e rt = xt , = loramlpg (σ, xt , bias=False) , value (23) decay precursor decay receptance rwkv gate (24) (25) (26) (27) After weight preparation, we reshape (r , w, k, v, κ, a)t , splitting them to heads, with each head sized D/h. We always assume that is factor of and heads are equally split. All operations in this section are shown per-head. Before mixing in the time dimension, κt is normalized per head: ˆκt = κt /κt 2 normalized removal key (28) Time mixing is performed by the t attention calculation: t = (cid:195) (cid:88) =1 ki (cid:89) =i +1 (cid:179) diag(w ) ˆκT (a ˆκ ) (cid:33) (cid:180) R(D/h)(D/h) The t attention calculation can alternatively be written in recurrent manner: v 0 = 0, t = t 1 (cid:161)diag(wt ) ˆκT (at ˆκt )(cid:162) + kt pt = LayerNorm(rt T ) attention result (29) (30) (31) (32) Finally, the heads are recombined via reshaping so that pt RD , gated, and transformed into the output as follows: ot = (g pt )W RD (33)"
        }
    ],
    "affiliations": [
        "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
        "EleutherAI",
        "George Mason University",
        "Recursal AI"
    ]
}