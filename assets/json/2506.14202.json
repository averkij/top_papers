{
    "paper_title": "DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion",
    "authors": [
        "Makoto Shing",
        "Takuya Akiba"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources."
        },
        {
            "title": "Start",
            "content": "DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion Makoto Shing 1 Takuya Akiba 1 5 2 0 2 7 1 ] . [ 1 2 0 2 4 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose DiffusionBlocks, novel training framework that interprets neural network blocks as performing denoising operations in continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides promising pathway for democratizing access to large-scale neural network training with limited computational resources. 1. Introduction As neural networks grow following established scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), they become increasingly inaccessible to much of the research community. Training models with hundreds of billions of parameters requires computational resources available only to select institutions, threatening to concentrate AI advancement within well-resourced organizations. The fundamental bottleneck lies in end-to-end backpropagation (Rumelhart et al., 1986; He et al., 2016), which requires storing intermediate activations across the entire network, resulting in prohibitive memory demands for large models. This memory bottleneck is particularly critical for generative AI applications, where large-scale models are essential 1Sakana AI. Correspondence to: Makoto Shing <mkshing@sakana.ai>, Takuya Akiba <takiba@sakana.ai>. Figure 1. Overview of DiffusionBlocks compared to end-to-end backpropagation. Traditional training (top) requires backpropagating gradients through all blocks, creating memory bottlenecks. Our approach (bottom) trains each block independently as diffusion-based denoiser for specific noise range, eliminating gradient dependencies and achieving B-fold memory reduction during training. for high-quality generation. Previous layerwise training approaches (Hinton, 2022; Bengio et al., 2006; Nøkland & Eidnes, 2019; Belilovsky et al., 2019; Siddiqui et al., 2024) have underperformed compared to end-to-end backpropagation, primarily because they lack principled mechanisms to coordinate information flow between independently trained layers and struggle to balance parameter allocation effectively. Moreover, these approaches have been predominantly evaluated on image classification tasks, with limited exploration of generative modeling applications. Meanwhile, diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021) have revolutionized generative modeling through their mathematically principled approach to distribution transformation. Recent advances in network conditioning (Karras et al., 2022) and sampling efficiency (Lu et al., 2022; 2023; Zhao et al., 2023) have established diffusion models as state-ofthe-art across multiple domains. TTODLer-FM Workshop of the 42 nd International Conference on Machine Learning. Copyright 2025 by the author(s). We propose DiffusionBlocks, framework that reconceptualizes neural network training by interpreting network blocks 1 DiffusionBlocks as implementing discretized steps of continuous-time reverse diffusion process. Our key innovation is principled mapping between network blocks and noise-level ranges based on equal cumulative probability mass, ensuring each block confronts an equally challenging learning problem. This approach enables independent block training without requiring gradient communication between blocks. Through experiments on image generation and language modeling tasks, we demonstrate that DiffusionBlocks reduces memory requirements proportionally to the number of blocks while achieving competitive or superior performance. Our primary contributions are: diffusion-inspired blockwise training framework achieving true block independence in continuous time, where each block can be trained without requiring gradients from other blocks. An equi-probability partitioning strategy that optimally allocates learning difficulty across blocks based on cumulative probability mass, ensuring balanced parameter utilization. Comprehensive empirical validation demonstrating Bfold memory reduction (with blocks) and improved performance on both image generation and language modeling tasks. Figure 1 illustrates our approach compared to traditional end-to-end backpropagation. Unlike conventional methods that require gradient flow across all blocks, DiffusionBlocks enables truly independent block training through diffusionbased denoising objectives. 2. Preliminaries 2.1. Score-Based Diffusion Models Let z0 Rd pdata denote clean data sample. Following the Variance-Exploding (VE) formulation (Song et al., 2021; Karras et al., 2022), we perturb z0 with Gaussian noise whose standard deviation σ(t) increases monotonically with the (continuous) time variable [0, 1]: zt = z0 + σ(t)ϵ, ϵ (0, I). (1) This gives zt (z0, σ(t)2I) = pt(ztz0) with marginal distribution pt(zt) = (cid:82) pdata(z0)pt(ztz0)dz0. The continuous-time formulation of this process is described by stochastic differential equation (SDE): dzt = (cid:114) dσ(t)2 dt dw, [0, 1] (2) where is standard Wiener process. For generating samples, we employ the Probability Flow ODE (PF-ODE), which shares the same marginal distributions as the SDE but follows deterministic trajectories: dzt dt = σ(t)σ(t)z log pt(zt) (3) where σ(t) = dσ(t) and log pt(zt) is the score of the dt density pt(zt). Following Karras et al. (2022), we can eliminate the abstract time variable by parameterizing directly in terms of noise levels. Setting σ(t) = t, the PF-ODE simplifies to: dzσ dσ = σz log pσ(zσ). (4) To estimate this score function, we parameterize it using neural network. We leverage the relation log pσ(zσ) z0zσ (Robbins, 1992) to approximate the score in terms of σ denoiser Dθ(zσ, σ) that predicts the clean data: log pσ(zσ) Dθ(zσ, σ) zσ σ2 (5) The denoiser is trained using weighted L2 loss: L(θ) = Epdata,pσ,N (0,I) (cid:2)w(σ)Dθ(zσ, σ) z02 2 (cid:3) (6) where w(σ) is weighting function and pσ is the distribution from which noise levels are sampled during training. 2.2. Neural Network Block Structure Consider deep neural network with layers, parameterized by θ = (θ0, θ1, . . . , θL). Traditional end-to-end training processes the input through the network to produce an output ˆy as follows: z(0) = f0(x; θ0) z(l) = fl(z(l1); θl), [L] (input embedding) ˆy = fL+1(z(L); θL+1) (output projection) (7) (8) (9) loss function L(ˆy, y) is computed between the predicted output ˆy and target y. Backpropagation calculates gradients θL by propagating error signals backward through the entire network, requiring storage of all intermediate activations {z(l)}L l=0. This memory requirement scales with network depth and batch size, creating bottleneck for large-scale models. When partitioning network into blocks, we group consecutive layers together to form blocks, where each block [B] consists of multiple layers and is parameterized by θi. In traditional blockwise approaches, defining appropriate training objectives for each block remains challenging, as these blocks must coordinate to accomplish the overall task without end-to-end supervision. 2.3. Residual Connections as Euler Steps of the Reverse through shared mathematical framework (Figure 1). DiffusionBlocks Diffusion Process The connection between residual networks and continuoustime ODEs has been established in prior work (Haber & Ruthotto, 2017; Chen et al., 2018), where residual updates z(l) = z(l1) + gθl (z(l1)) are shown to correspond to Euler discretizations of ODEs. We extend this perspective to our blockwise diffusion framework. In diffusion models, the forward process adds noise progressively, while the reverse process removes it to generate data. This reverse process can be formulated either as stochastic differential equation (SDE) or its deterministic counterpart, PF-ODE (Eq. (3)). While both formulations share the same marginal distributions, we focus on the PFODE due to its deterministic nature, which aligns naturally with the deterministic forward pass of neural networks. Applying Euler discretization to Eq. (4) with noise levels σ0 > σ1 > > σN yields: zσl = zσl1 σl σl1z log pσl1 (zσl1) = zσl1 + σl σl1 (cid:124) (cid:0)zσl1 Dθ(zσl1, σl1)(cid:1) (cid:125) (cid:123)(cid:122) =:gθl (zσl1 ) , (10) (11) where σl = σl1 σl > 0 and we used the score approximation from Eq. (5). This reveals that each denoising step naturally takes the form of residual update zσl = zσl1 + gθl (zσl1), matching the structure of modern neural architectures with skip connections. This mathematical correspondence explains why skip connections are essential for our framework: they naturally implement the Euler discretization of the reverse diffusion process. Architectures with residual connectionssuch as ResNets (He et al., 2016), U-Nets (Ronneberger et al., 2015), and transformer blocks with residual paths (Vaswani et al., 2017)are therefore ideally suited for our approach. Architectures without skip connections would require implicit ODE solvers, which are computationally more complex and less compatible with our blockwise training approach. Therefore, we restrict our framework to architectures with explicit residual connections, ensuring compatibility between the network structure and the underlying continuoustime diffusion process. 3. Method We now present DiffusionBlocks, our approach for training neural networks without end-to-end backpropagation. Our key insight is interpreting neural networks as implementing discretized steps of continuous-time score-based diffusion process. This perspective enables training individual blocks independently while maintaining network-wide coherence 3.1. Diffusion-Based Blockwise Training Framework Traditional neural networks transform input through hidden layers to output ˆy. We reconceptualize this as reverse diffusion process: the input corresponds to noise (zσmax (0, σ2 maxI)), and the output to clean data (z0 pdata). Each network block then performs partial denoising within specific noise range. Given neural network with layers, we partition it into blocks, where each block contains one or more consecutive layers. Instead of training the entire network endto-end, each block is assigned responsibility for specific range of noise levels in the diffusion process. Specifically, Block handles the noise level range [σi, σi+1], where {0, 1, ..., 1} and σ0 = σmax and σB = σmin (typically set to small positive value or zero). for block handling noise level During training, range [σi, σi+1], we train the corresponding denoiser Dθi (zσ, σ, x) to predict the clean target: L(θi) = (cid:2)w(σ)Dθi(zσ, σ, x) y2 (cid:3) pdata,p(i) σ ,N (0,I) 2 (12) where p(i) σ is the distribution of noise levels specifically for block i, defined by restricting the global noise distribution to the range [σi, σi+1]. For tasks like language modeling, we replace the L2 loss with cross-entropy after appropriate normalization. Each block-specific denoiser includes input embedding layers, neural network blocks, and output embedding components, making blocks truly independent. This block independence is the key to our memory efficiencyduring training, we only need to store activations for single block rather than the entire network. Specifically, our approach requires storage of activations for L/B layers instead of all layers needed by end-to-end backpropagation, resulting in approximately B-fold memory reduction during training. 3.2. Equi-Probability Block Partitioning critical innovation in our approach is how we partition the noise levels among blocks. Following Karras et al. (2022), we recognize that different noise levels present varying degrees of difficulty for the denoising task. The intermediate noise range tends to be most challenging and impactful for learning, while very low or high noise levels are comparatively simpler. To optimize parameter utilization, we partition the range of noise levels [σmin, σmax] into blocks such that each block handles an equal amount of cumulative 3 DiffusionBlocks Table 1. Image generation results comparing FID scores (lower is better). DiffusionBlocks achieves superior quality while training each block independently."
        },
        {
            "title": "Method",
            "content": "CIFAR-10 ImageNet-256 End-to-End BackProp Ours 41.87 41.39 16.62 15.55 inference by allowing each block to learn from samples slightly outside its primary range of responsibility. In all our experiments, we use γ = 0.1, which provides an effective balance between block independence and transition smoothness. 3.4. Implementation Details Our implementation follows the EDM framework (Karras et al., 2022) including the preconditioning strategy. Detailed training and inference algorithms are provided in Appendix C. 4. Experiments We evaluate DiffusionBlocks on image generation and language modeling tasks, demonstrating superior or comparable performance to end-to-end backpropagation while training with significantly reduced memory requirements. We also analyze key components of our framework. 4.1. Image Generation Experimental Setup. We evaluate our method on CIFAR10 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009) at 256256 resolution using Diffusion Transformer (DiT) architectures (Peebles & Xie, 2023). We use DiT-S with 12 layers and DiT-L with 24 layers, and partition them into 4 blocks. All models are trained with classifier-free guidance (Ho & Salimans, 2022), dropping labels with probability 0.1. For ImageNet, we follow Peebles & Xie (2023) compressing images using pre-trained VAE. Detailed hyperparameters and implementation specifics are provided in Appendix D.1. Figure 2. Block partitioning strategies for noise level assignment. Colored regions represent individual blocks under our equi-probability partitioning, where each block handles equal cumulative probability mass from the EDM log-normal distribution (blue curve). Orange circles show our equi-probability boundaries that concentrate in the challenging intermediate noise region, while gray squares show uniform boundaries (equal intervals in log-space) for comparison. This strategy ensures balanced learning difficulty across blocks. probability under the noise distribution: σi = exp (cid:0)Pmean + Pstd Φ1(pi)(cid:1) (13) where pi = CDFmin + (CDFmax CDFmin) represents the target cumulative probability for block i, Φ1 is the inverse CDF of the standard normal distribution, and CDFmin and CDFmax are the CDFs corresponding to σmin and σmax respectively. This partitioning ensures that each block handles an equal amount of cumulative probability mass: (cid:90) σi+ σi pσ(σ)dσ = . (14) 1 Figure 2 illustrates how our approach allocates block boundaries to ensure equal cumulative probability across the noise level distribution. This strategy ensures that each block contributes equally to the overall learning task, optimizing parameter utilization. In contrast, naive uniform partitioning (e.g., dividing [σmin, σmax] into equal intervals) would allocate too many parameters to easy regions while underserving challenging noise levels. 3.3. Controlled Block Overlap To mitigate potential discontinuities between blocks, we introduce controlled overlap between adjacent noise level ranges. For block responsible for noise range [σi, σi+1], we expand the training range to: [σi/α, σi+1 α], (15) where α := (σi+1/σi)γ and γ is the overlap coefficient. This controlled overlap ensures smoother transitions during Results. Table 1 compares our approach against endto-end backpropagation, showing that DiffusionBlocks achieves better FID scores on both datasets. By training only one block at time and optimizing each block independently, our approach reduces memory requirements during training by factor of (B = 4 in our experiments)backpropagation needs to be performed only through the active block rather than the entire network. Figure 3 shows examples of generated images from our model on the CIFAR-10 dataset. 4 DiffusionBlocks Table 2. Language modeling results comparing MAUVE scores (higher is better). Our method achieves superior performance compared to end-to-end backpropagation. Table 3. Effect of block partitioning strategy on CIFAR-10. Our equi-probability partitioning outperforms uniform partitioning by allocating blocks based on learning difficulty."
        },
        {
            "title": "Method",
            "content": "MAUVE () End-to-End BackProp Ours 0.595 0."
        },
        {
            "title": "Partitioning Strategy",
            "content": "FID () Uniform Equi-Probability 68.06 45.50 Additionally, significant advantage of our approach is faster inference: while the baseline model requires forwarding through all layers for each diffusion step, our method only needs to use the relevant block. This results in approximately 3 faster generation time. Table 4. Effect of block overlap on CIFAR-10. Controlled overlap between adjacent blocks significantly improves performance, with γ = 0.1 providing the optimal balance between block independence and transition smoothness. Overlap Coefficient γ FID () 4.2. Language Modeling Experimental Setup. For language modeling, we use The One Billion Words Benchmark (LM1B) (Chelba et al., 2014) with Llama-style architecture (Touvron et al., 2023) comprising 12 transformer layers partitioned into 4 blocks. We implement specialized attention mechanisms (Arriola et al., 2025) to handle autoregressive dependencies while maintaining diffusion-based denoising capabilities. We evaluate models using MAUVE score (Pillutla et al., 2021), following the conditional generation protocol established by SEDD (Lou et al., 2024). Detailed hyperparameters and implementation specifics are provided in Appendix D.2. Results. Table 2 shows that our method achieves superior MAUVE scores compared to end-to-end backpropagation, despite only requiring backpropagation through one block at time during training. This demonstrates that our blockwise training approach can effectively learn high-quality text generation while maintaining significant memory efficiency. 4.3. Ablation Studies We perform ablation studies on CIFAR-10 to analyze the importance of key components in our framework. All experiments use the same network architecture and hyperparameters unless otherwise specified. Block Partitioning Strategy. We compare our equiprobability partitioning strategy against uniform partitioning across noise levels. We disabled the block overlap in Section 3.3 to isolate the effectiveness of our partitioning strategy. As shown in Table 3, our approach outperforms uniform partitioning, achieving an FID of 45.50 compared to 68.06. While this improvement is meaningful, the difference highlights that both strategies can achieve reasonable performance, with our equi-probability approach providing consistent advantage. This supports our hypothesis that γ = 0.00 γ = 0.05 γ = 0.10 γ = 0.15 γ = 0.20 45.50 42.98 41.39 42.84 56.69 allocating block capacity based on the intrinsic difficulty of denoising at different noise levels (as visualized in Figure 2) contributes to more effective parameter utilization. The uniform strategy, while functional, appears to be less optimal as it allocates equal capacity across all noise regions rather than concentrating resources where learning is most challenging. Effect of Block Overlap. To evaluate the importance of controlled overlap between blocks, we varied the overlap coefficient γ from 0 (no overlap) to 0.2 (substantial overlap). Table 4 demonstrates that controlled overlap significantly improves performance compared to strict block boundaries. Without overlap (γ = 0), FID degrades to 45.50 due to discontinuities between independently trained blocks. Performance improves as we introduce modest overlap, reaching optimal results at γ = 0.1 (FID 41.39). However, excessive overlap (γ 0.15) begins to degrade performance, with γ = 0.2 producing significantly worse results (FID 56.69), likely due to conflicting learning objectives when blocks have substantial overlap in their training regions. These results confirm that γ = 0.1 provides an effective balance between maintaining block independence and ensuring smooth transitions during inference. Effect of Block Count. We investigate how performance varies with different numbers of blocks while keeping the total network depth constant (12 layers). Table 5 reveals clear trade-off between FID score and computational efficiency. Using fewer blocks yields better FID scores due to larger block capacityB = 2 achieves the best FID (38.58) but requires processing 6 layers per forward pass. As the number of blocks increases, inference becomes more effi5 DiffusionBlocks Table 5. Effect of block count on CIFAR-10. Fewer blocks achieve better FID but require more layers per diffusion step (L/S), creating trade-off between quality and efficiency. Note that L/S=L/B, where is the total number of layers (12) and is the number of blocks. Number of Blocks FID () L/S () Relative Speed = 1 (End-to-End BackProp) = 2 = 3 = 4 = 6 41.87 38.58 41.39 41.39 53.74 12 6 4 3 2 1.0 2.0 3.0 4.0 6.0 cient: = 4 processes only 3 layers per step (4 faster than end-to-end) while maintaining reasonable FID (41.39), and = 6 achieves 6 speedup at the cost of degraded performance (FID 53.74). The results suggest that = 3 or = 4 provide good balance points, offering substantial efficiency gains while preserving competitive generation quality. Beyond = 6, individual blocks become too small (2 layers each) to perform effective denoising, leading to significant quality degradation. This analysis enables practitioners to choose the appropriate block count based on their specific quality requirements and computational constraints. 5. Related Work Diffusion Models and Score-Based Generation. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) and score-based generative models (Song & Ermon, 2019; 2020; Song et al., 2021) have emerged as powerful frameworks for generative modeling. These models define processes that gradually transform simple distributions into complex ones through sequences of denoising steps. Recent advances in network conditioning (Karras et al., 2022), sampling efficiency (Lu et al., 2022; 2023; Zhao et al., 2023), and architectural improvements (Rombach et al., 2022; Peebles & Xie, 2023) have established diffusion models as state-of-the-art across various generative tasks. Our work leverages these mathematical foundations for neural network training, interpreting layer transformations through the lens of continuous-time diffusion processes. Layer/Block-wise Training Methods. Various approaches have been proposed to train neural networks Synthetic Gradiwithout end-to-end backpropagation. ents (Jaderberg et al., 2017) enables decoupled neural interfaces by predicting gradients locally, while biologicallymotivated methods include Feedback Alignment (Lillicrap et al., 2016), the Forward-Forward algorithm (Hinton, 2022), and Target Propagation (Lee et al., 2015). Additional approaches include local learning methods (Nøkland & Eidnes, 2019; Belilovsky et al., 2019), greedy layer-wise pretraining (Bengio et al., 2006), and Blockwise SelfSupervised Learning (Siddiqui et al., 2024). However, these methods face two fundamental limitations: they lack principled theoretical foundations for coordinating information flow between independently trained components, and have demonstrated limited effectiveness on generative modeling tasks where maintaining coherent probabilistic modeling across components remains challenging. DiffusionBlocks addresses both limitations through the mathematical rigor of continuous-time diffusion theory, where each blocks denoising objective naturally aligns with the global generative goal. Memory-Efficient Implicit Depth Models. Neural ODEs (Chen et al., 2018) parameterize network dynamics as continuous-time differential equations, using the adjoint sensitivity method to achieve constant memory backpropagation through time. Deep Equilibrium Models (DEQs) (Bai et al., 2019) represent another memory-efficient paradigm, directly solving for fixed points of implicit layers using root-finding and implicit differentiation, effectively creating infinite-depth networks with constant memory. While both approaches achieve memory efficiency through implicit computation, they fundamentally differ from our method: Neural ODEs still require end-to-end backpropagation through single monolithic network, and DEQs focus on equilibrium computation rather than generative modeling. In contrast, DiffusionBlocks achieves true block independence by partitioning the continuous-time diffusion process into disjoint noise-level ranges, enabling genuinely parallel block training without any inter-block gradient flow. Connection to Concurrent Work. Most closely related to our work is the concurrent NoProp framework (Li et al., 2025), which also interprets neural network training through diffusion principles. NoProps discrete-time formulation (NoProp-DT) treats each network layer as discrete denoising step, achieving memory-efficient training for classification tasks. However, their continuous-time variant (NoProp-CT) fundamentally differs from true blockwise training: it employs single network ˆuθ(zt, x, t) that must handle all noise levels [0, 1], requiring end-to-end backpropagation through the entire architecture. This approach more closely resembles Neural ODEs (Chen et al., 2018) than blockwise methods. Our framework achieves genuine blockwise independence in continuous time by partitioning the noise range [σmin, σmax] into intervals, with each block Dθi independently responsible for its assigned range [σi, σi+1]. This enables B-fold memory reduction during training while maintaining the mathematical rigor of continuous-time diffusion. Furthermore, our equi-probability partitioning based on cumulative distribution mass ensures optimal parameter utilization across blocksa principled approach absent in 6 DiffusionBlocks NoProps fixed layer-to-timestep mapping. Notably, while NoProp focuses primarily on classification tasks and evaluates against diffusion-inspired baselines, we demonstrate superior performance on generative modeling tasksimage generation and language modelingwhere our framework naturally excels, directly comparing against conventional end-to-end backpropagation on established architectures. 6. Conclusion We introduced DiffusionBlocks, novel framework that enables independent neural network block training by interpreting blocks as denoising operations at specific noise levels in continuous-time diffusion process. Our approach achieves blockwise independence, optimal equi-probability partitioning, and B-fold memory reduction with competitive or superior performance on generative modeling tasks. Experiments on image generation and language modeling demonstrate that DiffusionBlocks outperforms end-to-end backpropagation while requiring only 1/B the memory during training. In our experiments with = 4 blocks, this translates to 4 memory reduction with superior performance, offering principled pathway for democratizing large-scale neural network training with limited computational resources."
        },
        {
            "title": "References",
            "content": "Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2023. Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Arriola, M., Sahoo, S. S., Gokaslan, A., Yang, Z., Qi, Z., Han, J., Chiu, J. T., and Kuleshov, V. Block diffusion: Interpolating between autoregressive and diffusion language models. In International Conference on Learning Representations, 2025. Bai, S., Kolter, J. Z., and Koltun, V. Deep equilibrium models. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Belilovsky, E., Eickenberg, M., and Oyallon, E. Greedy layerwise learning can scale to ImageNet. In Chaudhuri, K. and Salakhutdinov, R. (eds.), International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 583593. PMLR, 0915 Jun 2019. Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, In H. Greedy layer-wise training of deep networks. Scholkopf, B., Platt, J., and Hoffman, T. (eds.), Advances in Neural Information Processing Systems, volume 19. MIT Press, 2006. Bengio, Y., Louradour, J., Collobert, R., and Weston, J. In International Conference on Curriculum learning. Machine Learning, pp. 4148. Association for Computing Machinery, 2009. Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2014. Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, 2009. Haber, E. and Ruthotto, L. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004, dec 2017. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778, 2016. Hinton, G. The Forward-Forward algorithm: Some preliminary investigations. arXiv preprint arXiv:2212.13345, 2022. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6840 6851. Curran Associates, Inc., 2020. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., et al. An empirical analysis of compute-optimal large language model training. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3001630030. Curran Associates, Inc., 2022. DiffusionBlocks Jaderberg, M., Czarnecki, W., Osindero, S., Vinyals, O., Graves, A., Silver, D., and Kavukcuoglu, K. Decoupled neural interfaces using synthetic gradients. In Precup, D. and Teh, Y. W. (eds.), International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 16271635. PMLR, 0611 Aug 2017. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2656526577. Curran Associates, Inc., 2022. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009. Lee, D.-H., Zhang, S., Fischer, A., and Bengio, Y. Difference target propagation. In Appice, A., Rodrigues, P. P., Santos Costa, V., Soares, C., Gama, J., and Jorge, A. (eds.), Machine Learning and Knowledge Discovery in Databases, pp. 498515, Cham, 2015. Springer International Publishing. Li, Q., Teh, Y. W., and Pascanu, R. NoProp: Training neural networks without back-propagation or forwardpropagation. arXiv preprint arXiv:2503.24322, 2025. Lillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. Nature Communications, 7(1):13276, Nov 2016. Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.1683, 2024. Lu, C., Zhou, Y., Bao, F., Chen, J., LI, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 57755787. Curran Associates, Inc., 2022. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2023. Neyshabur, B. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. In Chaudhuri, K. and Salakhutdinov, R. (eds.), International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 48394850. PMLR, 0915 Jun 2019. Parmar, G., Zhang, R., and Zhu, J.-Y. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1141011420, 2022. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41954205, 2023. Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S., Choi, Y., and Harchaoui, Z. Mauve: Measuring the gap between neural text and human text using divergence frontiers. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 48164828. Curran Associates, Inc., 2021. Robbins, H. E. An Empirical Bayes Approach to Statistics, pp. 388394. Springer New York, 1992. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Navab, N., Hornegger, J., Wells, W. M., and Frangi, A. F. (eds.), Medical Image Computing and Computer-Assisted Intervention MICCAI 2015, pp. 234241. Springer International Publishing, 2015. Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning representations by back-propagating errors. nature, 323:533536, 1986. Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. Diffusion schrodinger bridge matching. In Oh, A., Naumann, 8 DiffusionBlocks T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 6218362223. Curran Associates, Inc., 2023. Siddiqui, S., Krueger, D., LeCun, Y., and Deny, S. Blockwise self-supervised learning at scale. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In Bach, F. and Blei, D. (eds.), International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2256 2265. PMLR, 2015. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Song, Y. and Ermon, S. Improved techniques for training score-based generative models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1243812448. Curran Associates, Inc., 2020. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4984249869. Curran Associates, Inc., 2023. 9 A. Limitations and Future Directions DiffusionBlocks Limitations. Our approach has two fundamental limitations. First, our framework requires architectures with explicit residual connections, as these naturally implement the Euler discretization of the reverse diffusion process (Section 2.3). This restricts applicability to ResNet-style, U-Net, and Transformer architectures, excluding feedforward networks and other non-residual designs. While most modern architectures incorporate residual connections, this constraint prevents exploration of alternative architectural paradigms. Second, for autoregressive tasks like language modeling, our approach requires multiple diffusion steps per token, resulting in O(KM ) forward passes for generating tokens with diffusion steps. This multiplicative overhead compared to standard O(K) autoregressive generation may be prohibitive for real-time applications, despite quality improvements through test-time scaling. Future Directions. Several promising directions emerge from this work. First, while we focus on the VE formulation, our DiffusionBlocks framework is fundamentally applicable to other diffusion formulations, including Variance Preserving (VP) (Song et al., 2021), flow matching (Lipman et al., 2023; Liu et al., 2023), stochastic interpolants (Albergo & VandenEijnden, 2023; Albergo et al., 2023), and bridge matching (Shi et al., 2023). Second, theoretical analysis of why DiffusionBlocks outperforms end-to-end backpropagation warrants investigation. We hypothesize this improvement stems from two mechanisms. First, our framework may induce structured constraints on the optimization process through the diffusion formulation, potentially providing form of implicit regularization (Neyshabur, 2017) that guides parameter updates toward more principled solutions. Second, our equi-probability partitioning explicitly allocates computational resources based on the empirical difficulty distribution of denoising tasks (Bengio et al., 2009), which may lead to more efficient parameter utilization compared to the implicit allocation mechanisms in end-to-end training. Third, adapting recent advances in fast diffusion sampling (Lu et al., 2023; Zhao et al., 2023) could significantly reduce inference costs for language modeling while preserving test-time scaling benefits. Fourth, exploring block-parallel generation techniques (Arriola et al., 2025) could enable simultaneous token generation, addressing sequential bottlenecks in autoregressive tasks. Finally, investigating optimal architecture designs for different noise level ranges and extending our framework to multimodal tasks with efficient architectures like Mixture-of-Experts could broaden its applicability and impact. B. Mathematical Background B.1. Variance Exploding Diffusion Models In the Variance Exploding (VE) formulation (Ho et al., 2020; Song et al., 2021), perturbed sample at noise level σ is defined as: zσ = z0 + σϵ, where zσ (z0, σ2I) =: pσ(zσz0) with marginal distribution pσ(zσ) = (cid:82) pdata(z0)pσ(zσ z0)dz0. Following Karras et al. (2022), we parameterize the continuous diffusion process directly in terms of noise levels σ, eliminating the abstract time variable. The forward SDE becomes: ϵ (0, I), (16) where is standard Wiener process. The corresponding reverse SDE is: dzσ = 2σz log pσ(zσ)dσ + 2σd w, dzσ = 2σdw, (17) (18) where is standard Wiener process in reverse direction (from high to low noise levels). B.2. Probability Flow ODE The Probability Flow ODE (PF-ODE) is deterministic process that shares the same marginal distributions as the stochastic diffusion process. In our noise-level parameterization: dzσ dσ = σz log pσ(zσ). 10 (19) DiffusionBlocks This formulation directly connects noise levels to the dynamics of the diffusion process, making it natural for our blockwise approach where each block handles specific noise level range. B.3. Score Estimation and Denoising Score Matching Our goal is to sample z0 pdata from zσmax (0, σ2 using neural network. We leverage the relation log pσ(zσ z0) = z0zσ Dθ(zσ, σ) that predicts the clean data: σ2 maxI) by the reverse process. The score log pσ(zσ) is approximated (Robbins, 1992) to parameterize the score in terms of denoiser log pσ(zσ) Dθ(zσ, σ) zσ σ . The denoiser is trained using weighted L2 loss: L(θ) = Ez0pdata,σpσ,ϵN (0,I) (cid:2)w(σ)Dθ(zσ, σ) z02 2 (cid:3) , (20) (21) where zσ = z0 + σϵ and w(σ) = σ2+σ2 (σσdata)2 following Karras et al. (2022). data B.4. Noise Level Scheduling The distribution of noise levels pσ significantly impacts training efficiency and generation quality. Following Karras et al. (2022), we use log-normal distribution: log(σ) (Pmean, 2 This distribution concentrates probability mass in intermediate noise regions, which empirically contribute most to learning quality. Very low noise levels result in trivial denoising tasks, while very high noise levels destroy all meaningful information. std). (22) C. Algorithmic Details Algorithm 1 DiffusionBlocks Training Require: Dataset = {(x(n), y(n))}N n=1, Number of blocks B, Noise level range [σmin, σmax], Log-normal parameters Pmean, Pstd 1: Compute block boundaries {σ0, σ1, . . . , σB} using Equation (13) 2: Initialize parameters {θ0, . . . , θB1} 3: while not converged do 4: 5: Sample Uniform(0, 1) {Select block randomly} Sample (x, y) {Sample data point} Sample σ from [σi, σi+1] according to p(i) Sample ϵ (0, I) {Sample noise} zσ + σϵ {Create noisy target} if image generation task then w(σ)Dθi(zσ, σ, x) y2 else if language modeling task then 2 {Compute L2 loss} σ {Sample noise level for block i} w(σ) CrossEntropy(Normalize(Dθi (zσ, σ, x)), y) {Compute CE loss} end if Update θi to minimize {Update only block parameters} 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while D. Experimental Details D.1. Image Generation Details Model Architecture For our image generation experiments, we employ Diffusion Transformers (DiT) (Peebles & Xie, 2023). We use DiT/S-2 and DiT/L-2 for CIFAR-10 and ImageNet, respectively. We partition them into 4 blocks for DiffusionBlock. 11 Algorithm 2 DiffusionBlocks Inference Require: Input x, Trained block parameters {θ0, θ1, . . . , θB1}, Block boundaries {σ0, σ1, . . . , σB}, Number of inference DiffusionBlocks steps , ODE solver (e.g., Euler or Heun) 1: Generate discretized noise levels {σ(0), σ(1), . . . , σ(N )} {EDM discretization} 2: Sample ϵ (0, I) 3: z(0) σ(0)ϵ {Initialize with noise} 4: for = 0 to 1 do 5: 6: 7: 8: end for 9: return z(N ) σ σ(j) {Current noise level} Determine block index such that σ [σi, σi+1) {Find responsible block} z(j+1) ODESolverStep(z(j), σ(j), σ(j+1), Dθi, x) {Apply ODE solver with block i} Training Settings For both CIFAR-10 and ImageNet, we train class-conditional models with classifier-free guidance (Ho & Salimans, 2022), randomly dropping labels with 10% probability during training. All models are trained with batch size of 512 for 100 epochs using the AdamW optimizer with learning rate of 1e-4. For ImageNet, following the latent diffusion approach (Rombach et al., 2022), we first resize images to 256256 and compress them using pre-trained VAE from Stability AIs SDXL 1. The batch size is increased to 1024, with all other settings remaining the same as CIFAR-10. Inference Settings For inference, we use classifier-free guidance with scale of 2.0 and Euler sampling. We generate 50,000 samples and evaluate them using FID (specifically clean-FID (Parmar et al., 2022)) against the test sets. To reduce the impact of random variation, we compute FID three times in each experiment and report the minimum following Karras et al. (2022). Fair Comparison To ensure fair comparison, we match the total number of layer forwards between our method and the end-to-end backpropagation baseline. For the baseline with layers trained for epochs, each layer is updated times. For our method with blocks, we train each block for epochs, resulting in (L/B) = layer updates. D.2. Language Modeling Details Model Architecture For language modeling, we use Llama-style architecture with Llama 2 tokenizer (Touvron et al., 2023), choosing model size comparable to DiT/B with 12 transformer layers, 768 hidden dimensions, and 12 attention heads. The model is partitioned into 4 blocks of 3 layers each. Training Settings We train on The One Billion Words Dataset (LM1B) (Chelba et al., 2014) with batch size of 256 for 10 epochs using the AdamW optimizer with learning rate of 3e-4 and weight decay of 0.0. We use context length of 256 tokens. Specialized Attention Masks key challenge in autoregressive language modeling with diffusion models is maintaining proper conditioning on clean previous tokens while denoising current tokens. To address this, we implement the Vectorized Training approach from Block Diffusion (Siddiqui et al., 2024), which processes both clean and noisy sequences simultaneously by concatenating them and using specialized attention masks. This approach eliminates the need for multiple forward passes and KV cache storage, making training significantly more efficient. Evaluation Following SEDD (Lou et al., 2024), we evaluate using MAUVE score (Pillutla et al., 2021) on conditional generation. We sample 1000 sequences from the LM1B test set, filtering for sequences with at least 100 tokens. For each sequence, we use the first 50 tokens as prompts and generate 5 samples of 50 tokens each using 50 diffusion steps with Euler sampling. We then compute MAUVE scores between the 5000 generated samples and 1000 reference samples from the test set. 1https://huggingface.co/stabilityai/sdxl-vae 12 E. Generated Samples E.1. CIFAR-10 Class-Conditional Samples DiffusionBlocks Figure 3. Class-conditional samples generated by DiffusionBlocks on CIFAR-10. Each row shows samples from different class. E.2. Language Generation Samples Table 6 presents an example of text generated by our language model trained on LM1B. Table 6. Text samples generated by our language model trained on LM1B. the United States are fighting for the English team, resuming training on Sunday and that his players will be home from the Champions League final in Moscow on Wednesday. The evidence shows that the manufacturing sector is not quite"
        }
    ],
    "affiliations": [
        "Sakana AI"
    ]
}