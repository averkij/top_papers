{
    "paper_title": "Video models are zero-shot learners and reasoners",
    "authors": [
        "Thadd√§us Wiedemer",
        "Yuxuan Li",
        "Paul Vicol",
        "Shixiang Shane Gu",
        "Nick Matarese",
        "Kevin Swersky",
        "Been Kim",
        "Priyank Jaini",
        "Robert Geirhos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 2 3 0 2 . 9 0 5 2 : r 2025-9Video models are zero-shot learners and reasoners Thadd√§us Wiedemer*1, Yuxuan Li1, Paul Vicol1, Shixiang Shane Gu1, Nick Matarese1, Kevin Swersky1, Been Kim1, Priyank Jaini*1 and Robert Geirhos*1 1Google DeepMind The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to todays generative video models. Could video models be on trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve broad variety of tasks it wasnt explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veos emergent zero-shot capabilities indicate that video models are on path to becoming unified, generalist vision foundation models. Project page: https://video-zero-shot.github.io/ 1. Introduction We believe that video models will become unifying, general-purpose foundation models for machine vision just like large language models (LLMs) have become foundation models for natural language processing (NLP). Within the last few years, NLP underwent radical transformation: from taskspecific, bespoke models (e.g., one model for translation, another one for question-answering, yet another one for summarization) to LLMs as unified foundation models. Todays LLMs are capable of general-purpose language understanding, which enables single model to tackle wide variety of tasks including coding [38], math [12], creative writing [74], summarization, translation [89], and deep research [55, 67]. These abilities started to emerge from simple primitives: training large, generative models on web-scale datasets [e.g 6, 77]. As result, LLMs are increasingly able to solve novel tasks through few-shot in-context learning [6, 17] and zero-shot learning [43]. Zero-shot learning here means that prompting model with task instruction replaces the need for fine-tuning or adding task-specific inference heads. Machine vision today in many ways resembles the state of NLP few years ago: There are excellent task-specific models like Segment Anything [42, 62] for segmentation or YOLO variants for object detection [41, 64]. While attempts to unify some vision tasks exist [1, 47, 4951, 58, 63, 76, 80, 81, 86], no existing model can solve any problem just by prompting. However, the exact same primitives that enabled zero-shot learning in NLP also apply to todays generative video modelslarge-scale training with generative objective (text/video continuation) on web-scale data [83]. In this article, we therefore ask: Do video models develop general-purpose vision understanding, similar to how LLMs developed general-purpose language understanding? We answer this question in the affirmative: 1. Analyzing 18,384 generated videos across 62 qualitative and 7 quantitative tasks, we report that Veo 3 can solve wide range of tasks that it was neither trained nor adapted for. 2. Based on its ability to perceive, model, and manipulate the visual world, Veo 3 shows early forms of chain-of-frames (CoF) visual reasoning like maze and symmetry solving. 3. While task-specific bespoke models still outperform zero-shot video model, we observe substantial and consistent performance improvement from Veo 2 to Veo 3, indicating rapid advancement in the capabilities of video models. 2. Methods Approach and motivation Our method is simple: We prompt Veo. This minimalist strategy is intentional, as it mirrors the transformation of NLP from task-specific fine-tuning or training to * Joint leads. Video models are zero-shot learners and reasoners Perception Modeling Manipulation Reasoning Figure 1 qualitative overview of Veo 3s zero-shot abilities. The plot shows Veo 3s success rate across 12 samples as rough estimate of model performance on 62 tasks across the vision stack. Tasks are described in Sec. 3 and shown in Sec. A. Find videos of all tasks on our project page. prompting capable foundation model [53, 78, 88]. Here, we adopt the same philosophy to explore the capabilities of Veo 3 as general-purpose vision model. Takeaway 1 In NLP, prompting replaced task-specific training or adaptation. similar paradigm shift is on the horizon in machine vision, facilitated by video models. Video generation For each task, we query the publicly available Veo 2 or Veo 3 models via Google Clouds Vertex AI API. We prompt the model with an initial input image (which the model uses as the first frame) and text instruction. The models then generate 16:9 video at 720p resolution, 24 FPS, for duration of 8s. Veo 3 has model ID veo-3.0-generate-preview and Veo 2 model ID veo-2.0-generate-001. According to the Vertex documentation [29], the API uses an LLM-based prompt rewriter. This means that for some tasks, the solution is likely to come from the LLM instead of the video (e.g., Fig. 55: Sudoku). We treat the system (rewriter and video generator) as single black-box entity. However, to isolate the video models reasoning abilities, we verified that standalone LLM (Gemini 2.5 Pro [12]) could not reliably solve key tasks (Fig. 58: Robot navigation, Sec. 4.5: Maze solving, Sec. 4.6: Visual symmetry) from the input image alone. Why Veo? The core argument of this paperthat video models are zero-shot learners and reasoners can be supported by demonstrating success on any sufficiently capable model. We choose Veo because it has consistently ranked high on text2video and image2video leaderboards [54]. Unless noted otherwise, our figures are generated with Veo 3. To provide sense of how rapidly performance is improving, our quantitative analyses compare Veo 3 with its predecessor, Veo 2, released roughly within half year of each other: Veo 2 was announced in December 2024 and released in April 2025 [24, 26], while Veo 3 was announced in May 2025 and released in July 2025 [27, 28]. 3. Qualitative results: Sparks of visual intelligence? We begin with comprehensive, qualitative investigation across visual tasks to assess the potential of video models as visual foundation models. We organize our findings into four hierarchical capabilities, each building on the last (c.f. Fig. 1 and Fig. 2): 1. Perception as foundational ability to understand visual information. 2. Modeling, which builds on the perception of objects to form model of the visual world. 3. Manipulation, which meaningfully alters the perceived and modeled world. 4. Reasoning across space and time over sequence of manipulation steps. 2 Video models are zero-shot learners and reasoners Perception superresolution Perception conjunctive search / binding problem Modeling buoyancy Modeling memory of world states Manipulation 3D-aware reposing Manipulation jar opening Reasoning robot navigation Reasoning rule extrapolation Figure 2 Veo 3 zero-shot learning and reasoning examples. From classic perceptual tasks (superresolution, visual search) to modeling (buoyancy, memory of world states after zooming in), manipulation (pose editing, simulating dexterous manipulation) and visual reasoning (navigation, rule extrapolation): Veo 3 can zero-shot solve host of visual tasks that are specified as an input image and text prompt. Examples are shown in Sec. A; videos of all tasks are on our project page. While capability boundaries often overlap, this hierarchy provides framework for understanding the emergent abilities of video models. For example, solving maze (see Fig. 57 and Sec. 4.5) requires perceiving the maze, modeling its state (walls vs. floor), and finally manipulating an object (a mouse, circle) to move from start to finish. For each task in this section, we prompt Veo 3 twelve times and report the success rate in the caption. The success rate is defined as the fraction of generated videos that solved the task (as determined by the authors). success rate greater than 0 suggests that the model possesses the ability to solve the task, while success rate closer to 1 indicates that the task is solved reliably irrespective of the random seed. While not substitute for the systematic quantification we perform in Sec. 4, this provides ballpark estimate of the models capabilities. Perception Computer vision has historically relied on suite of specialized models for tasks like segmentation [42, 62], object detection [41, 64], and edge detection [82]. While some backbones can be adapted or fine-tuned for other tasks, training-free transfer to novel tasks is rare, limiting generalization. As we show here, this is changing with large video models. Without any task-specific training, Veo 3 can perform range of classic computer vision tasks, including edge detection  (Fig. 10)  , segmentation  (Fig. 11)  , keypoint localization  (Fig. 12)  , superresolution  (Fig. 13)  , blind deblurring  (Fig. 14)  , denoising  (Fig. 15)  and low-light enhancing  (Fig. 16)  . These emergent abilities extend to more complex perceptual tasks like conjunctive search  (Fig. 17)  and interpreting ambiguous images such as the the classic dalmatian illusion  (Fig. 18)  , the cat shape in texture-shape cue conflict image  (Fig. 19)  , and colored blots from the Rorschach test  (Fig. 20)  . Apart from denoisingthe classic diffusion objectivenone of these tasks are explicitly trained for in video models. Takeaway 2 Veo 3 shows emergent zero-shot perceptual abilities well beyond the training task. Just like LLMs replaced task-specific NLP models, video models will likely replace most bespoke models in computer visiononce they become sufficiently cheap and reliable. 3 Video models are zero-shot learners and reasoners Modeling: intuitive physics & world models Based on their perception of the visual world, video models are starting to model it, too. Modeling the world and the principles that govern it (e.g., laws of physics) is critical step toward successful prediction and action. Several works have investigated and quantified intuitive physics in deep models [e.g., 3, 4, 9, 19, 36, 39, 52, 57, 59, 65, 68, 73, 79, 87]. Here, we investigate an exemplary subset of tasks from these works. Veos grasp of physics is demonstrated by its ability to model rigid and soft body dynamics and their surface interactions  (Fig. 22)  . It also displays an understanding of various physical properties, like flammability  (Fig. 21)  , air resistance affecting falling objects  (Fig. 23)  , buoyancy  (Fig. 24)  , and optical phenomenon like refraction and reflection  (Fig. 27)  and additive/subtractive color mixing  (Fig. 28)  . As illustrated by the Visual Jenga task [5], Veo can remove objects from scene in physically plausible order  (Fig. 25)  and understands which objects fit into backpack  (Fig. 26)  . Beyond physical characteristics, Veo understands abstract relationships which is an important aspect of modeling the world. As trivial example, Veo can distinguish categories like toys from other objects like laptop  (Fig. 29)  . On samples inspired by the Omniglot dataset [45], we demonstrate Veos ability to recognize patterns, generate variations thereof, and parse larger wholes into parts  (Fig. 30)  . Lastly, Veo maintains memory of the world state across time and camera movements within the video context  (Fig. 31)  . Manipulation: editing & imagination Based on its ability to perceive objects and model their relation to each other and the world, Veo can meaningfully manipulate the visual world, too. This enables Veo 3 to perform variety of zero-shot image editing tasks like backgorund removal  (Fig. 32)  , style transfer  (Fig. 33)  , colorization  (Fig. 34)  , inpainting  (Fig. 35)  , and outpainting  (Fig. 36)  . Furthermore, it can manipulate text elements  (Fig. 37)  , and edit images based on doodle instructions  (Fig. 38)  . Veos understanding of 3D world enables it to compose scenes from individual components  (Fig. 39)  , generate novel views of objects and characters (Figs. 40 and 41), smoothly transform one object into another  (Fig. 42)  , or change the perspective, lighting, and appearance to turn selfie into professional photograph  (Fig. 43)  . This ability to plausibly modify scene allows it to imagine complex interactions, simulate dexterous object manipulation (Fig. 44; note that we do not test actual robots as e.g. [2] do), interpreting object affordances  (Fig. 45)  , demonstrating how to draw shape  (Fig. 46)  and roll burrito  (Fig. 47)  . Overall, video models can meaningfully manipulate and simulate the (digital) visual world. Visual reasoning across time and space Perception, modeling, and manipulation all integrate to tackle visual reasoning. While language models manipulate human-invented symbols, video models can apply changes across the dimensions of the real world: time and space. Since these changes are applied frame-by-frame in generated video, this parallels chain-of-thought in LLMs and could therefore be called chain-of-frames, or CoF for short. In the language domain, chain-of-thought enabled models to tackle reasoning problems [78]. Similarly, chain-of-frames (a.k.a. video generation) might enable video models to solve challenging visual problems that require step-by-step reasoning across time and space. We see early signs of this ability in tasks such as generating valid graph traversal  (Fig. 48)  , performing visual breadth-first search on tree  (Fig. 49)  , completing visual sequences  (Fig. 50)  , connecting matching colors  (Fig. 51)  , fitting shapes into holes  (Fig. 52)  , and sorting numbers  (Fig. 53)  . Furthermore, Veo can use tools to accomplish visual task  (Fig. 54)  and solve simple Sudokus  (Fig. 55)  or visual puzzles  (Fig. 56)  . Finally, it can solve mazes and navigation tasks (Figs. 57 and 58 and Sec. 4.5) and extrapolate rules from visual examples  (Fig. 59)  . While not always perfect, the models ability to solve such problems in zero-shot manner points to exciting possibilities for more advanced visual reasoning and planning in future, with more capable video models. Takeaway 3 Frame-by-frame video generation parallels chain-of-thought in language models. Just like chain-of-thought (CoT) enables language models to reason with symbols, chain-offrames (CoF) enables video models to reason across time and space. 4 Video models are zero-shot learners and reasoners Original image Generated edge map (Veo 3) Ground-truth edge map Figure 3 Edge detection on all 50 test images from BIPEDv2 [71, 72]. We generate 10 videos per sample and report best performance over ùëò attempts as function of ùëò. Prompt: All edges in this image become more salient by transforming into black outlines. Then, all objects fade away [...] Details & full prompt: Sec. B.1. Summary Taken together, the qualitative examples from this section indicate that capable video model like Veo 3 possesses strong and generalizable zero-shot learning ability. While the results are not always perfect, the model consistently demonstrates the capacity to solve wide variety of tasks for which it was not explicitly trained. 4. Quantitative results The previous section offered qualitative exploration of video model capabilities. In this section, we add quantitative assessment for seven select tasks. As in Sec. 3, we consider different facets of visual understanding: For perception, we assess Veo on edge detection, segmentation, and object extraction. For manipulation, we examine image editing performance. Finally, we evaluate reasoning abilities through maze solving, visual symmetry, and visual analogies. We do not include specific evaluations for modeling, since this area is well addressed by recent benchmarks, see Sec. 3. We evaluate performance separately for the best frame and the last frame (where applicable). Best frame reports the best performance across any frame in the generated videos. This indicates the performance ceiling, but the optimal frame is not known priori. Therefore, we also report performance on the last frame of each video, which may underestimate models ability but has the practical advantage that the frame is predetermined. This distinction is important because Veo tends to continue animating scene even after task completion, potentially reducing last frame performance. Where applicable, we use the state-of-the-art image editing model Nano Banana [25] as reference. As general trend, we observe large performance increase from Veo 2 to Veo 3, often matching or exceeding Nano Banana performance. Performance tends to improve substantially from ùëò = 1 to ùëò = 10 attempts, indicating that good solution can be found in reasonable number of tries. While image models are excellent zero-shot learners, too [7, 11, 34, 40], video models are the more general framework because of their ability to process both temporal and spatial information. 4.1. Perception: Edge detection Despite not being trained for it, Veo 3 can be prompted to detect, and therefore perceive, edges. Fig. 3 details edge detection performance (measured by OIS; details and prompt in Sec. B.1) for Veo 2 and Veo 3. While Veo 3 (0.77 pass@10) is not on par with task-specific SOTA (0.90) [72], 5 Video models are zero-shot learners and reasoners Original image Generated frame (Veo 3) Extracted masks Ground-truth masks Figure 4 Class-agnostic instance segmentation on subset of 50 easy images (1-3 large objects) from LVIS [31]. Prompt: [...] each distinct entity is overlaid in different flat color [...] the background fades to {white, green} [...] Details & full prompt: Sec. B.2. Figure 5 Object extraction on an animal dataset. Prompt: The background changes to white [...] all animals line up in row [...] Details & full prompt: Sec. B.3. its performance is remarkable for two reasons: First, it is zero-shot. Second, many of Veo 3s edge maps are more detailed than the ground truth. For example, Veo 3 correctly outlines foliage and tire profiles; this hurts performance on the dataset but seems more indicative of dataset limitation than model limitation (the annotators understandably did not bother to trace each and every edge). 4.2. Perception: Segmentation Instance segmentation asks to delineate (i.e., perceive) distinct objects in an image. Contrary to classic instance segmentation or promptable segmentation, we prompt models to segment all objects in scene, without specifying an object category or location. We report mean Intersection over Union (mIoU); experiment details are in Sec. B.2. As shown in Fig. 4, Veo 3 achieves an mIoU of 0.74 (best frame pass@10), comparable to Nano Bananas 0.73. Naturally, Veo 3 lacks behind the performance of bespoke model like SAMv2 [62], but nevertheless shows remarkable zero-shot segmentation abilities. Interestingly, the prompt really matters: Veo consistently performs better with green background than white one (0.74 vs. 0.66 best frame pass@10); possibly due to the widespread use of green screens. See also Sec. for prompting best practices. 4.3. Manipulation: Object extraction Can Veo perceive and extract (i.e., manipulate) all objects in scene? We test this using simple dataset depicting one to nine animals (details in Sec. B.3). Veo is asked to extract and line up all 6 Video models are zero-shot learners and reasoners Figure 6 Image editing on subset of Emu-edit [69]. Prompt: Create smooth, static animation that slowly changes the color of the fire hydrant to red. [...] Details & full prompt: Sec. B.4. Figure 7 Maze solving. Mazes of various sizes with start (red) and goal (green) locations. Prompt: [...] The red square slides smoothly along the white path, stopping perfectly on the green square [...] Details & full prompt: Sec. B.5. Veo 2 struggles to solve even small sizes, mostly due to illegal moves early in the generation. Veo 3 performs much better and benefits from multiple attempts. For reference, we evaluate Nano Banana and show Gemini 2.5 Pros performance on mazes presented as images (I2T) or ASCII text (T2T). animals in row (in some sense, visual tally), with white space between them. To assess whether the number of extracted animals is correct by counting connected components in the last frame. Fig. 5 shows sample and the results. While Veo 2 performs around chance, Veo 3 achieves up to 92 6% pass@10. Given the simplicity of the task, perfect model should easily achieve 100 0% accuracy. 4.4. Manipulation: Image editing Image editing requires manipulating images according to text instruction (e.g., adding/removing objects or changing their appearance). We evaluate whether Veo can edit images on random subset of 30 samples from Emu-edit [69]. Veo has strong bias for animated scenes and might introduce unintended changes (e.g., camera movement, animating people). We therefore ask three human raters to evaluate fidelity (correct edit) and precision (correct edit without unintended changes). An example edit and results are shown in Fig. 6. We find that Veo 3 especially excels in preserving details and textures across edits. With better control to curb unintended changes, video models could become capable, 3D-aware image editors (and likely video editors, too). If unintended changes such as camera movement or animating people can be controlled better, video models could become capable 3D-aware image and video editors (see also Molad et al. [58], Rotstein et al. [66]). 7 Video models are zero-shot learners and reasoners Figure 8 Visual symmetry. Prompt: Instantly reflect this pattern along the central, vertical axis while keeping the existing colored pattern without modification. [...] model has to color all cells correctly to pass. Details & full prompt: Sec. B.6. 4.5. Reasoning: Maze solving Maze solving tests models ability to plan path in constrained environment, key aspect of reasoning. In our setup, red circle needs to navigate to target location (green circle) without crossing any walls. We automatically verify path correctness (details in Sec. B.5) and present results for different mazes in Fig. 7. Veo 3 shows zero-shot maze solving abilities, significantly outperforming Veo 2 which often produces illegal moves. For instance, in the 55 grids, Veo 3 achieves pass@10 rate of 78% compared to Veo 2s 14%. The consistent performance gap highlights the advancing reasoning capabilities of the newer model. While Nano Banana matches or surpasses Veo 3s performance on rectangular mazes, it fails to solve irregular mazes entirely. Similarly, Gemini 2.5 Pro outperforms Veo 3 on small mazes when given an ASCII representation of the maze (T2T), but falls behind on 99 mazes, and generally struggles when the maze is represented as image (as opposed to text) input. Both comparisons highlight the advantages of solving visual task step-by-step in visual medium. 4.6. Reasoning: Visual symmetry solving Pattern completion for symmetry assess the ability to understand and apply spatial reasoning. We use custom dataset of shapes (e.g., heart, letters) and random patterns (see Sec. B.6 for details). Fig. 8 shows that Veo 3 outperforms Veo 2 and Nano Banana by large margin. We also use this task to systematically analyze how different prompts affect performance in Sec. C: The pass@1 difference between best and worst prompt is 40 percentage points on the shape split and 64 percentage points on the random split. 4.7. Reasoning: Visual analogy completion Visual analogies test models ability to understand transformations and relationships between objects, form of abstract reasoning. Concretely, we prompt the model to fill the missing quadrant of 22 grid to complete the analogy (e.g., is to as is to ?). We evaluate the correctness of the generated infill for four transformation types from KiVA [85], see Sec. B.7 for details. The results are summarized in Fig. 9. While Veo 2 struggles to understand any analogies, Veo 3 correctly completes examples for color and resize. However, both models perform below chance (0.33) on reflect and rotate analogies, indicating an erroneous, systematic bias. 8 Video models are zero-shot learners and reasoners Figure 9 Visual analogy solving on four transformations √† 50 samples from KiVA [85]. Prompt: [...] generate the missing object in the lower right region and solve the visual analogy. [...] Pass@1 is evaluated on the last frame, results up to pass@10 can be found in Sec. B.7. Takeaway 4 While far from perfect, Veo 3building on its ability to perceive, model and manipulate objectsshows emergent visual reasoning abilities. 5. Discussion Summary breakthrough in machine vision started the deep learning revolution in 2012 [44], but in recent years, natural language processing has seen the most rapid progress. This was driven by the rise of general-purpose LLMs, whose ability to solve novel tasks in zero-shot fashion has led them to replace most task-specific models in NLP. We here make the case that machine vision is on the cusp of similar paradigm shift, enabled by emergent abilities of large-scale video models. Our core finding is that Veo 3 can solve wide range of tasks in zero-shot manner, spanning the full vision stack from perception to modeling, manipulation and even early forms of visual reasoning. While its performance is not yet perfect, the massive and consistent improvement from Veo 2 to Veo 3 indicates that video models will become general-purpose foundation models for vision, just as LLMs have for language. Performance is lower bound Tasks can be represented in myriad of ways; maze, for example, can be presented as black-and-white grid, video game, or photorealistic scene, with the prompt requesting solution in the form of line, moving object, or glowing path. Moreover, visually, maze could be represented as black-and-white grid, Pac-Man game, or photorealistic top-down view of an apartment. This has three implications: First, prompt engineeringincluding the visual prompt a.k.a. starting frameis as important for visual tasks as it is for LLMs (see also Sec. and [46] for discussion). Second, we must distinguish between models task performance and its underlying ability (i.e., competence) to solve that task [18, 21]. Third, as consequence, the model performance reported here with given visual and textual prompt should be considered lower bound on the models true capabilities. This also holds for the tasks that we report as failure cases in Sec. D, such as providing visual instructions to fold laundry  (Fig. 76)  , planning to move sofa between rooms separated by small door  (Fig. 77)  , or certain visual puzzles  (Fig. 70)  . Video generation is expensive, but costs tend to fall While generating video is currently more expensive than running bespoke, task-specific model, the economics of general-purpose models are on predictable trajectory. Epoch AI [13] estimates that LLM inference costs are falling by factor of 9 to 900 per year for given performance level. In NLP, early generalist models were also considered prohibitively expensive (GPT-3s size makes it challenging to deploy [6, p. 8]). Nevertheless, rapidly falling inference costs, combined with the appeal of generalist models, have replaced most task-specific language models. If NLP is guide, the same trend will play out in vision. Jack of many trades, master of few? For many tasks, Veo 3s performance is below state of the art of specialized models. This mirrors the early days of LLMs; GPT-3 reported performance well below fine-tuned models on many tasks [6, cf. Tables 3.1, 3.3, 3.4, 3.5]). This did not stop language models from becoming foundation models, and we dont believe it will stop video models from becoming vision foundation models for two reasons. First, the step-change in performance from Veo 2 to Veo 3 9 Video models are zero-shot learners and reasoners is evidence of rapid progress over time. Second, our scaling results from Sec. 4 show pass@10 to be consistently higher than pass@1 with no signs of plateau. Therefore, inference-time scaling methods [e.g. 33, 56, 70, 75] in combination with the standard optimization toolkit like post-training with automatic verifiers are likely to boost performance. For the tasks we test here, Veo 3 is akin to pre-trained language model that has yet to undergo instruction tuning or RLHF [60, 61]. Outlook This is an exciting time for vision. Seeing NLPs recent transformation from task-specific to generalist models, it is conceivable that the same transformation will happen in machine vision through video models (a GPT-3 moment for vision), enabled by their emergent ability to perform broad variety of tasks in zero-shot fashion, from perception to visual reasoning. Acknowledgements We would like to thank Oyvind Tafjord, Mike Mozer, Katherine Hermann, Andrew Lampinen, Viorica Patraucean, Shiry Ginosar, Ross Goroshin, Abhijit Ogale, Claire Cui, Kun Zhang for helpful feedback/discussions, Anish Nangia for Vertex, Tuan Anh Le for suggesting the Omniglot evaluation, Medhini Narasimhan and Pieter-Jan Kindermans for Veo insights and discussion, Micromelon Robotics for permission to use the image in Fig. 58, Shelly Sheynin for permission to use the EMU Edit dataset, David Fleet and Jon Shlens for their support, and the Veo team for developing an incredible model. Author contributions Leads: TW, PJ, RG. Project idea: RG. Core contributors: YL, PV. Partial contributor: SG. Fig. 38: NM. Advisors: KS, BK. References [1] P. Acuaviva, A. Davtyan, M. Hassan, S. Stapf, A. Rahimi, A. Alahi, and P. Favaro. From generation to generalization: Emergent few-shot learning in video diffusion models. arXiv preprint arXiv:2506.07280, 2025. [2] M. Assran, A. Bardes, D. Fan, Q. Garrido, R. Howes, M. Muckley, A. Rizvi, C. Roberts, K. Sinha, A. Zholus, et al. V-JEPA 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [3] H. Bansal, Z. Lin, T. Xie, Z. Zong, M. Yarom, Y. Bitton, C. Jiang, Y. Sun, K.-W. Chang, and A. Grover. Videophy: Evaluating physical commonsense for video generation, 2024. [4] D. M. Bear, E. Wang, D. Mrowca, F. J. Binder, H.-Y. F. Tung, R. T. Pramod, C. Holdaway, S. Tao, K. Smith, F.-Y. Sun, L. Fei-Fei, N. Kanwisher, J. B. Tenenbaum, D. L. K. Yamins, and J. E. Fan. Physion: Evaluating physical prediction from vision in humans and machines, 2021. [5] A. Bhattad, K. Preechakul, and A. A. Efros. Visual jenga: Discovering object dependencies via counterfactual inpainting. arXiv preprint arXiv:2503.21770, 2025. [6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] R. Burgert, K. Ranasinghe, X. Li, and M. S. Ryoo. Peekaboo: Text to image diffusion models are zero-shot segmentors. arXiv preprint arXiv:2211.13224, 2022. [8] D. Campbell, S. Rane, T. Giallanza, C. N. De Sabbata, K. Ghods, A. Joshi, A. Ku, S. Frankland, T. Griffiths, J. D. Cohen, et al. Understanding the limits of vision language models through the lens of the binding problem. Advances in Neural Information Processing Systems, 37:113436 113460, 2024. 10 Video models are zero-shot learners and reasoners [9] A. Cherian, R. Corcodel, S. Jain, and D. Romeres. LLMPhy: Complex physical reasoning using large language models and world models. arXiv preprint arXiv:2411.08027, 2024. [10] F. Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. [11] K. Clark and P. Jaini. Text-to-image diffusion models are zero shot classifiers. Advances in Neural Information Processing Systems, 36:5892158937, 2023. [12] G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] B. Cottier, B. Snodin, D. Owen, and T. Adamczewski. LLM inference prices have fallen rapidly but unequally across tasks, march 2025. URL https://epoch.ai/data-insights/llm-i nference-price-trends. Accessed: 2025-09-12. [14] M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel, A. Kusupati, A. Fan, C. Laforte, V. Voleti, S. Y. Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [15] P. Doll√°r and C. L. Zitnick. Structured forests for fast edge detection. In ICCV, 2013. [16] P. Doll√°r and C. L. Zitnick. Fast edge detection using structured forests. ArXiv, 2014. [17] Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu, T. Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [18] C. Firestone. Performance vs. competence in humanmachine comparisons. Proceedings of the National Academy of Sciences, 117(43):2656226571, 2020. [19] Q. Garrido, N. Ballas, M. Assran, A. Bardes, L. Najman, M. Rabbat, E. Dupoux, and Y. LeCun. Intuitive physics understanding emerges from self-supervised pretraining on natural videos. arXiv preprint arXiv:2502.11831, 2025. [20] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel. ImageNettrained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International conference on learning representations, 2019. [21] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. [22] D. Geng, C. Herrmann, J. Hur, F. Cole, S. Zhang, T. Pfaff, T. Lopez-Guevara, C. Doersch, Y. Aytar, M. Rubinstein, C. Sun, O. Wang, A. Owens, and D. Sun. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. [23] N. Gillman, C. Herrmann, M. Freeman, D. Aggarwal, E. Luo, D. Sun, and C. Sun. Force prompting: Video generation models can learn and generalize physics-based control signals, 2025. URL https://arxiv.org/abs/2505.19386. [24] Google. Veo 2 announcement. https://blog.google/technology/google-labs/vide o-image-generation-update-december-2024/, 2024. Accessed: September 22, 2025. [25] Google. Nano Banana: Gemini Image Generation Overview. https://gemini.google/ov erview/image-generation/, 2025. Accessed: September 22, 2025. [26] Google. Veo 2 launch. https://developers.googleblog.com/en/veo-2-video-gen eration-now-generally-available/, 2025. Accessed: September 22, 2025. 11 Video models are zero-shot learners and reasoners [27] Google. Veo 3 announcement. https://blog.google/technology/ai/generative-m edia-models-io-2025/, 2025. Accessed: September 22, 2025. [28] Google. Veo 3 launch. https://cloud.google.com/blog/products/ai-machine-l earning/veo-3-fast-available-for-everyone-on-vertex-ai, 2025. Accessed: September 22, 2025. [29] Google Cloud. Vertex AI Veo Prompt Rewriter. https://cloud.google.com/vertex-ai/ generative-ai/docs/video/turn-the-prompt-rewriter-off#prompt-rewriter, 2025. Accessed: September 22, 2025. [30] R. L. Gregory. The intelligent eye, 1970. [31] A. Gupta, P. Dollar, and R. Girshick. LVIS: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. [32] M. Ivanitskiy. Maze dataset. https://pypi.org/project/maze-dataset/0.3.4/, 2025. Accessed: June 31, 2025. [33] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. OpenAI o1 system card. arXiv preprint arXiv:2412.16720, 2024. [34] P. Jaini, K. Clark, and R. Geirhos. Intriguing properties of generative classifiers. In The Twelfth International Conference on Learning Representations, 2023. [35] R. C. James. Sight for sharp eyes. LIFE, 58(7):120, 1965. [36] S. Jassim, M. Holubar, A. Richter, C. Wolff, X. Ohmer, and E. Bruni. GRASP: novel benchmark for evaluating language grounding and situated physics understanding in multimodal language models. arXiv preprint arXiv:2311.09048, 2023. [37] Y. Jeong, A. Uselis, S. J. Oh, and A. Rohrbach. Diffusion classifiers understand compositionality, but conditions apply. arXiv preprint arXiv:2505.17955, 2, 2025. [38] J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. [39] B. Kang, Y. Yue, R. Lu, Z. Lin, Y. Zhao, K. Wang, G. Huang, and J. Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. [40] L. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. [41] R. Khanam and M. Hussain. Yolov11: An overview of the key architectural enhancements. arXiv preprint arXiv:2410.17725, 2024. [42] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [43] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [44] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [45] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):13321338, 2015. Video models are zero-shot learners and reasoners [46] A. K. Lampinen, S. C. Chan, A. K. Singh, and M. Shanahan. The broader spectrum of in-context learning. arXiv preprint arXiv:2412.03782, 2024. [47] D. H. Le, T. Pham, S. Lee, C. Clark, A. Kembhavi, S. Mandt, R. Krishna, and J. Lu. One diffusion to generate them all. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26712682, 2025. [48] K. Leng, Z. Zhang, J. Liu, Z. Boukhers, W. Sui, C. Yang, and Z. Li. Superedge: Towards generalization model for self-supervised edge detection. CoRR, 2024. [49] Z.-Y. Li, R. Du, J. Yan, L. Zhuo, Z. Li, P. Gao, Z. Ma, and M.-M. Cheng. Visualcloze: universal image generation framework via visual in-context learning. arXiv preprint arXiv:2504.07960, 2025. [50] W. Lin, X. Wei, R. Zhang, L. Zhuo, S. Zhao, S. Huang, H. Teng, J. Xie, Y. Qiao, P. Gao, et al. Pixwizard: Versatile image-to-image visual assistant with open-language instructions. arXiv preprint arXiv:2409.15278, 2024. [51] Y. Lin, M. Huang, S. Zhuang, and Z. Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406, 2025. [52] D. Liu, J. Zhang, A.-D. Dinh, E. Park, S. Zhang, and C. Xu. Generative physical AI in vision: survey. arXiv preprint arXiv:2501.10928, 2025. [53] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM computing surveys, 55(9):135, 2023. [54] LMSYS ORG. Lmsys org text-to-video leaderboard. https://lmarena.ai/leaderboard/t ext-to-video, September 2025. Accessed: 2025-09-23. [55] C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha. The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. [56] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [57] F. Meng, J. Liao, X. Tan, W. Shao, Q. Lu, K. Zhang, Y. Cheng, D. Li, Y. Qiao, and P. Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation, 2024. [58] E. Molad, E. Horwitz, D. Valevski, A. R. Acha, Y. Matias, Y. Pritch, Y. Leviathan, and Y. Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. [59] S. Motamed, L. Culp, K. Swersky, P. Jaini, and R. Geirhos. Do generative video models understand physical principles? arXiv preprint arXiv:2501.09038, 2025. [60] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [61] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023. [62] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. R√§dle, C. Rolland, L. Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 13 Video models are zero-shot learners and reasoners [63] R. Ravishankar, Z. Patel, J. Rajasegaran, and J. Malik. Scaling properties of diffusion models for perceptual tasks. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1294512954, 2025. [64] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. [65] R. Riochet, M. Y. Castro, M. Bernard, A. Lerer, R. Fergus, V. Izard, and E. Dupoux. IntPhys: framework and benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018. [66] N. Rotstein, G. Yona, D. Silver, R. Velich, D. Bensaid, and R. Kimmel. Pathways on the image manifold: Image editing via video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 78577866, 2025. [67] S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, M. Moor, Z. Liu, and E. Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. [68] L. M. Schulze Buschoff, E. Akata, M. Bethge, and E. Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, pages 111, 2025. [69] S. Sheynin, A. Polyak, U. Singer, Y. Kirstain, A. Zohar, O. Ashual, D. Parikh, and Y. Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [70] C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [71] X. Soria, E. Riba, and A. Sappa. Dense extreme inception network: Towards robust CNN model for edge detection. In The IEEE Winter Conference on Applications of Computer Vision (WACV 20), 2020. [72] X. Soria, A. Sappa, P. Humanante, and A. Akbarinia. Dense extreme inception network for edge detection. Pattern Recognition, 139:109461, 2023. ISSN 0031-3203. doi: https://doi.org/10.1 016/j.patcog.2023.109461. URL https://www.sciencedirect.com/science/article/ pii/S0031320323001619. [73] H.-Y. Tung, M. Ding, Z. Chen, D. Bear, C. Gan, J. Tenenbaum, D. Yamins, J. Fan, and K. Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Advances in Neural Information Processing Systems, 36, 2024. [74] T. Wang, J. Chen, Q. Jia, S. Wang, R. Fang, H. Wang, Z. Gao, C. Xie, C. Xu, J. Dai, et al. Weaver: Foundation models for creative writing. arXiv preprint arXiv:2401.17268, 2024. [75] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [76] X. Wang, W. Wang, Y. Cao, C. Shen, and T. Huang. Images speak in images: generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. [77] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Video models are zero-shot learners and reasoners [78] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [79] L. Weihs, A. Yuile, R. Baillargeon, C. Fisher, G. Marcus, R. Mottaghi, and A. Kembhavi. Benchmarking progress to infant-level physical reasoning in ai. Transactions on Machine Learning Research, 2022. [80] S. Xiao, Y. Wang, J. Zhou, H. Yuan, X. Xing, R. Yan, C. Li, S. Wang, T. Huang, and Z. Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. [81] J. Xie, A. Tonioni, N. Rauschmayr, F. Tombari, and B. Schiele. Test-time visual in-context tuning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1999620005, 2025. [82] S. Xie and Z. Tu. Holistically-nested edge detection. In Proceedings of the IEEE international conference on computer vision, pages 13951403, 2015. [83] S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024. [84] W. Yang, W. Wang, H. Huang, S. Wang, and J. Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE Transactions on Image Processing, 30: 20722086, 2021. [85] E. Yiu, M. Qraitem, A. N. Majhi, C. Wong, Y. Bai, S. Ginosar, A. Gopnik, and K. Saenko. Kiva: Kidinspired visual analogies for testing large multimodal models. arXiv preprint arXiv:2407.17773, 2024. [86] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 37123722, 2018. [87] C. Zhang, D. Cherniavskii, A. Zadaianchuk, A. Tragoudaras, A. Vozikis, T. Nijdam, D. W. Prinzhorn, M. Bodracska, N. Sebe, and E. Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. [88] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are human-level prompt engineers. In The eleventh international conference on learning representations, 2022. [89] W. Zhu, H. Liu, Q. Dong, J. Xu, S. Huang, L. Kong, J. Chen, and L. Li. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675, 2023. [90] C. L. Zitnick and P. Doll√°r. Edge boxes: Locating object proposals from edges. In ECCV, 2014. 15 Video models are zero-shot learners and reasoners Appendix A. Qualitative results: Perception, Modeling, Manipulation, Reasoning A.1. Perception Figure 10 Edge detection. Prompt: All edges in this image become more salient by transforming into black outlines. Then, all objects fade away, with just the edges remaining on white background. Static camera perspective, no zoom or pan. Success rate: 0.92. Figure 11 Segmentation. Prompt: Create an animation of instance segmentation being performed on this photograph: each distinct entity is overlaid in different flat color [...] (full prompt: Sec. B.2). Success rate: 0.33. Figure 12 Keypoint localization. Prompt: Add bright blue dot at the tip of the branch on which the macaw is sitting. The macaws eye turns bright red. Everything else turns pitch black. Static camera perspective, no zoom or pan. Success rate: 0.58. Figure 13 Super-resolution. Prompt: Perform superresolution on this image. Static camera perspective, no zoom or pan. Success rate: 0.75. 16 Video models are zero-shot learners and reasoners Figure 14 Blind deblurring. Prompt: Unblur image including background. Static camera perspective, no zoom or pan. Success rate: 1.0. Figure 15 Blind denoising. Each quadrant was corrupted with different type of noise. Clockwise from top left: Gaussian noise, salt-and-pepper noise, speckle noise, shot noise. Prompt: Remove the noise from this image. Static camera perspective, no zoom or pan. Success rate: 1.0. Original low-light image Veo 3-generated lit image Ground-truth lit image Figure 16 Low-light enhancing. Prompt: Fully restore the light in this image. Static camera perspective, no zoom or pan. Success rate: 0.92. Image and ground-truth source: LOLv2 dataset [84]. Figure 17 Conjunctive search / binding problem. Prompt: The blue ball instantly begins to glow. Static camera perspective, no zoom no pan no movement no dolly no rotation. Success rate: 0.75. Inspiration: [8]. 17 Video models are zero-shot learners and reasoners Figure 18 Dalmatian illusion understanding. Prompt: Static camera perspective. Success rate: 1.0. Image credit: [30, 35]. Figure 19 Shape (cue-conflict) understanding. Prompt: Transform the animal in this image into sketch of the animal surrounded by its family. Success rate: 1.0. Image credit: [20]. Figure 20 Rorschach blot interpretation. Prompt: The patterns transform into objects. Success rate: undefined (1.0 for prompt following). Image credit: H. Rorschach, public domain via wikipedia. A.2. Modeling Figure 21 Material properties. Prompt: The bunsen burner at the bottom turns on. Sped up time lapse. Static camera, no pan, no zoom, no dolly. Success rate: 0.25. 18 Video models are zero-shot learners and reasoners Figure 22 Physics body transform. Rigid body (top). Prompt: person picks up the vase and puts it back on the table in sideways orientation. Static camera, no pan, no zoom, no dolly. Success rate: 1.0. Soft body (bottom). Prompt: person drapes thin silk scarf over the vase. Static camera, no pan, no zoom, no dolly. Success rate: 0.67. Figure 23 Gravity and air resistance. On earth (top). Prompt: The objects fall due to gravity. Static camera, no pan, no zoom, no dolly. Success rate: 0.5. On the moon (bottom). Prompt: The objects fall down on the moon due to gravity. Static camera, no pan, no zoom, no dolly. Success rate: 0.5. 19 Video models are zero-shot learners and reasoners Figure 24 Buoyancy. Prompt: The hand lets go of the object. Static camera, no pan, no zoom, no dolly. Success rate (bottle cap): 0.58; success rate (rock): 0.83. Figure 25 Visual Jenga, inspired by [5]. Prompt: hand quickly removes each of the items in this image, one at time. Success rate, based on removal of at least three objects: 0.5. Figure 26 Object packing. Prompt: person puts all the objects that can fit in the backpack inside of it. Static camera, no pan, no zoom, no dolly. Success rate: 0.75. 20 Video models are zero-shot learners and reasoners Figure 27 Material optics. Glass (top). Prompt: giant glass sphere rolls through the room. Static camera, no pan, no zoom, no dolly. Note that the image through the glass sphere is inverted. Success rate: 0.92. Mirror (bottom). Prompt: giant mirror-polish metal sphere rolls through the room. Static camera, no pan, no zoom, no dolly. Note that the image reflected off the sphere is not inverted). Success rate: 1.0. Figure 28 Color mixing. Additive (lights, top). Prompt: The spotlight on the left changes color to green, and the spotlight on the right changes color to blue. Success rate: 0.92. Subtractive (paints, bottom). Prompt: paintbrush mixes these colors together thoroughly until they blend completely. Static camera, no pan, no zoom. Success rate: 0.75. Figure 29 Categorizing objects. Prompt: person puts all the kids toys in the bucket. Static camera, no pan, no zoom, no dolly. Success rate: 0.33. 21 Video models are zero-shot learners and reasoners Figure 30 Character recognition, generation, and parsing, inspired by the Omniglot dataset [45]. Recognition (top). Prompt: The background of the grid cell with the same symbol as the one indicated on the right turns red. All other grid cells remain unchanged. After that, spinning color wheel appears in the top right corner. (Note: Veo 3 has prior to keep things moving, which is detrimental for tasks where the solution is obtained in an early frame. We observe that motion outlet, such as color wheel, can indicate task completion and freeze the solution.) Success rate: 0.33. Generation of variations (middle). Prompt: The page is filled line-by-line with hand-written practice variations of the symbol. Success rate: 0.25. Parsing into parts (bottom). Color and numbers in final frame are added post-hoc to show stroke order. Prompt: Stroke-by-stroke, replica of the symbol is drawn on the right. Success rate: 0.5. Figure 31 Memory of world states. Prompt: The camera zooms in to give close up of the person looking out the window, then zooms back out to return to the original view. Success rate: 1.0. A.3. Manipulation Figure 32 Background removal. Prompt: The background changes to white. Static camera perspective, no zoom or pan. Success rate: 0.83. 22 Video models are zero-shot learners and reasoners Figure 33 Style transfer. Prompt: The scene transforms into the style of Hundertwasser painting, without changing perspective or orientation; the macaw does not move. Static camera perspective, no zoom or pan. Success rate: 0.75. Figure 34 Colorization. Prompt: Perform colorization on this image. Static camera perspective, no zoom or pan. Success rate: 0.08. Figure 35 Inpainting. Prompt: The white triangles become smaller and smaller, then disappear altogether. Static camera perspective, no zoom or pan. Success rate: 1.0. Figure 36 Outpainting. Prompt: Rapidly zoom out of this static image, revealing whats around it. The camera just zooms back, while the scene itself and everything in it does not move or change at all, its static image. Success rate: 1.0. 23 Video models are zero-shot learners and reasoners Figure 37 Text manipulation. Prompt: Animation of the text rapidly changing so that it is made out of different types of candy (top left text) and pretzel sticks (bottom right text). Static camera perspective, no zoom or pan. Success rate: 0.33. Figure 38 Image editing with doodles. Prompt: Changes happen instantly. Success rate: 1.0. Figure 39 Scene composition. Prompt: smooth animation blends the zebra naturally into the scene, removing the background of the zebra image, so that the angle, lighting, and shading look realistic. The final scene perfectly incorporates the zebra into the scene. Success rate: 0.75. Figure 40 Single-image novel view synthesis. Prompt: Create smooth, realistic animation where the camera seems to rotate around the object showing the object from all the sides. Do not change anything else. No zoom. No pan. Success rate: 0.92. Image source: [14]. 24 Video models are zero-shot learners and reasoners Figure 41 3D-aware reposing. Prompt: The knight turns to face to the right and drops on one knee, lifting the shield above his head to protect himself and resting the hilt of his weapon on the ground. Success rate: 0.83. Figure 42 Transfiguration. Prompt: magical spell smoothly transforms the structure of the teacup into mouse. Success rate: 0.17. Figure 43 Professional headshot generation. Prompt: Turn this selfie into professional headshot for LinkedIn. Success rate: 0.42. Image credit: photo by George Pisarevsky on Unsplash. Video models are zero-shot learners and reasoners Figure 44 Dexterous manipulation. Jar opening (top). Prompt: Use common sense and have the two robot hands attached to robot arms open the jar, like how human would. Success rate: 1.0. Throwing and catching (middle). Prompt: Use common sense and have the two robot hands attached to robot arms throw the ball in the air, the ball goes up off the screen, hands move to positions to catch the ball, and catch the falling ball, like how human would. Success rate: 1.0. Rotating Baoding balls (bottom). Prompt: human hand holds two metal Baoding balls. The fingers, including the thumb, index, and middle finger, skillfully manipulate the balls, causing them to rotate smoothly like two planets orbiting around each other and continuously in the palm, one ball circling the other in fluid motion. Success rate: 0.08. Figure 45 Affordance recognition. Prompt: The robot hands mounted on robot arms pick up the hammer, naturally like how human would. Success rate: 0.5. Figure 46 Drawing. Prompt: person draws square. Static camera, no pan, no zoom, no dolly. Success rate: 0.33. 26 Video models are zero-shot learners and reasoners Figure 47 Visual instruction generation. Prompt: montage clearly showing each step to roll burrito. Success rate: 0.25. Inspiration: [83] and Reddit. A.4. Reasoning Figure 48 Graph traversal. Prompt: Starting from the blue well, an unlimited supply of blue water moves through the connected channel system without spilling into the black area. Success rate: 0.08. Figure 49 Tree BFS. Prompt: From the blue water basin, an unlimited supply of water flows at constant speed into the cave system until all caves are filled. Static camera perspective, no zoom or pan. Success rate: 0.17. Figure 50 Sequence completion inspired by Ravens progressive matrices. Each of the four pairs shows input (left) and generated output (right). Prompt: Draw the figure that completes the pattern in the rightmost box. The images in the boxes are static. Do not modify the existing images, only draw in the empty box. Static camera, no zoom, no pan, no dolly. Success rate: 0.33 for dots, 1.0 for arrows, 0.75 for shrinking circles, 0.83 for growing squares. Video models are zero-shot learners and reasoners Figure 51 Connecting colors. Prompt: Draw three curves, one connecting each pair of circles of the same color. Success rate: 0.25. Figure 52 Shape fitting. Prompt: The scene shows three colored pieces, and wooden panel with three holes. Each colored piece fits into one and only one hole. hand grabs each colored piece and puts it into an empty hole that has the exact same shape - if it doesnt fit, the hand tries another hole. All the objects must be placed in their respective holes. Success rate: 0.25. Figure 53 Sorting numbers. Prompt: The video starts with some numbered bubbles. The bubbles pop and disappear one at time, in numeric order, starting from the one with the smallest number. Success rate: 0.08. Figure 54 Tool use. Prompt: person retrieves the walnut from the aquarium. Success rate: 0.92 (retrieval via tool) and 0.08 (retrieval via tool without intersecting the glass). Figure 55 Simple Sudoku completion. Prompt: Create static, smooth, animation that solves the given 4x4 sudoku. Enter the missing numbers one by one. Do not change anything else in the picture. Only fill the numbers in the empty cells so the sudoku is solved properly. cursor moves and fills the correct number in the empty boxes. Success rate: 0.67. 28 Video models are zero-shot learners and reasoners Figure 56 Water puzzle solving. Prompt: The tap is turned on and water starts flowing rapidly filling the containers. Create smooth, static animation showing the containers getting filled with water in the correct order. (note: not all containers can be filled since some pipes are closed offVeo fills the correct containers, in the right order.) Success rate: 0.5. Figure 57 Maze solving. Prompt: Without crossing any black boundary, the grey mouse from the corner skillfully navigates the maze by walking around until it finds the yellow cheese. Success rate: 0.17. Figure 58 Robot navigation. Prompt: The robot drives to the blue area. Static camera perspective, no movement no zoom no scan no pan. Success rate: 0.58. Image credit: Micromelon Robotics website with permission from Tim Hadwen. Figure 59 Rule extrapolation inspired by ARC-AGI [10]. Prompt: Modify the lower-right grid to adhere to the rule established by the other grids. You can fill cells, clear cells, or change cells color. Only modify the lower-right grid, dont modify any of the other grids. Static scene, no zoom, no pan, no dolly. Success rate: 0.08. While Veo 3 doesnt follow the prompt perfectly, the output grid (bottom right) is completed correctly. 29 Video models are zero-shot learners and reasoners Figure 60 Graded Veo 3 edge map. While false negatives reflect genuine oversights of Veo 3 (e.g., cracks in the road, lettering on the car), many false positives correspond to actual image details that seem to be erroneously excluded from the ground truth (e.g., the outline of the trees, the reflection in the car window, and the tire profiles). B. Quantitative results: experimental details Table 1 Video count breakdown for quantitative tasks. For segmentation, 2 1 splits indicate one test set with two different background color prompts (white/green). For the prompt sensitivity study on the symmetry task (Sec. C), 2 10 splits indicate 2 splits (random/shape) across 10 tested prompt variations. For the qualitative tasks, we additionally generated 744 videos (62 tasks 12 samples). Task Splits Imgs/split Pass@ Video models Total videos 1 Edge 2 1 Segmentation 1 Object extraction 1 Editing 4 Maze Symmetry 2 Symmetry prompt analysis 2 10 4 Analogy Total 50 50 54 30 50 25 25 50 10 10 10 1 10 10 1 2 2 2 2 2 2 1 2 1000 2000 1080 60 8000 1000 500 4000 17640 B.1. Perception: Edge detection We provide details for the image editing task in Sec. 4.1. Evaluation As is standard in the literature, we refine and binarize predicted edges and allow for small local shifts compared to the ground truth [15, 16, 48, 90]. Concretely, we use non-maximum suppression, then binarize with one of 16 evenly-spaced thresholds, then thin the binary edge map. At each threshold, we find the optimal mapping between predicted and ground-truth edge pixels within radius of 0.75 % of the image diagonal (around 11 pixels). Fig. 60 shows an example rating of Veo 3-generated edge map. We report the best OIS over ùëò attempts (optimal image scale; the maximum ùêπ1-score over all thresholds) for the best/last frame. Dataset We used all 50 test images from BIPEDv2 [71, 72]. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-imagepreview through Google AI Studio. 30 Video models are zero-shot learners and reasoners Veo All edges in this image become more salient by transforming into black outlines. Then, all objects fade away, with just the edges remaining on white background. Static camera perspective, no zoom or pan. Nano Banana Outline all edges in the image in black, make everything else white. Sampling We generated 10 videos per sample with fixed prompt. B.2. Perception: Segmentation We provide details for the image editing task in Sec. 4.2. Evaluation Since the model is free to choose any colors for segmentation masks, we first determine the number and hue of each mask by considering the hue-difference histogram between the original image and the extracted frame. We smooth the histogram with scipy.ndimage.gaussian_filter1d with standard deviation of 2. Peaks with minimum height of 10% of the maximum and at least 10 hue steps apart are considered to correspond to predicted segmentation masks. We then map each pixel to the mask with the closest hue. Contrary to classic instance segmentation [31] or promptable segmentation [42, 62], our prompts do not specify class or list of possible classes, location prior (e.g., point or bounding box), or the number of instances in the image. This also means that mapping between predictions and annotated instances is established. Instead, we pair each ground-truth mask (including the background) with the predicted mask with the highest IoU (intersection over union), if any. We report mIoU as the average IoU over all pairs (excluding the background). Dataset We evaluated on 50 randomly chosen test images from LVIS [31] that contain one to three objects, each with at least 5000 pixels. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-imagepreview through Google AI Studio. Video models are zero-shot learners and reasoners Veo Create an animation of instance segmentation being performed on this photograph: each distinct entity is overlaid in different flat color. Scene: The animation starts from the provided, unaltered photograph. The scene in the photograph is static and doesnt move. First, the background fades to {white, green}. Then, the first entity is covered by flat color, perfectly preserving its silhouette. Then the second entity, too, is covered by different flat color, perfectly preserving its silhouette. One by one, each entity is covered by different flat color. Finally, all entities are covered with different colors. Camera: Static shot without camera movement. No pan. No rotation. No zoom. No glitches or artifacts. Nano Banana Perform instance segmentation on this image: Mask each distinct entity in different opaque flat color that only preserves the silhouette and turn the background green. Sampling We generated 10 videos per sample and prompt. B.3. Manipulation: Object extraction We provide details for the image editing task in Sec. 4.3. Evaluation We extract the last frame from each generated video; the resulting image is converted to greyscale, binary mask with threshold 200 is applied, and the number of connected components is extracted using scipy.ndimage.label, resulting in the count estimate. We also report the chance baseline which can be calculated as: random chance = 1 (1 ùëù)ùëò where ùëù is the probability to get the count correct via guessing (here: ùëù = 1 9 ) and ùëò [1, 10]. Dataset We generated an animal counting dataset using Nano Banana. Starting from white 16:9 image, we used the following prompt, where number is in [1, 9] and animal is in [dog, elephant, cat, brown bear, horse, rabbit, raccoon]. We manually evaluated the generated dataset for correctness; the resulting dataset has 54 images (exactly 6 per count). Nano Banana Exchange the white space with realistic photograph of: exactly {number} {animal}, outside, not overlapping, in natural landscape. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. 32 Video models are zero-shot learners and reasoners Veo The background changes to white. Then: If there is just single animal: the animal sits in the middle of the image, looking straight at the camera. If there are multiple animals: all animals line up in row, with ample white space between them. Sampling We generated 10 videos per sample with fixed prompt. B.4. Manipulation: Image editing We provide details for the image editing task in Sec. 4.4. Evaluation We perform human study with three human raters to evaluate fidelity (correct edit) and precision (correct edit with no unintended changes like zooming). Dataset We used random sample of 30 images from the test set of the Emu-edit dataset [69]. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. Veo Create smooth, static animation that slowly {image specific edit direction}. Do not change anything else. No zoom, no pan, no dolly. Sampling For each image, we generated two samples and use the first sample for human rating. B.5. Reasoning: Maze solving We provide details for the image editing task in Sec. 4.5. Evaluation Our evaluation process is tailored to the model type. For Veo, we analyze the generated video frame-by-frame, extracting the path taken by the agent (red circle). We check for any invalid moves, such as jumping over walls, clipping through boundaries, or any alteration of the goals position. We report the success rate as the fraction of ùëò attempts where the agent successfully reaches the goal (green circle) without any illegal moves. For Nano Banana, which generates the full path in one edit, we assess whether the drawn path connects the start and end points (allowing for minor discontinuities) and crucially, whether it intersects with any maze walls or goes off the valid path. For Gemini 2.5 Pro with maze input as an image (I2T) or as ASCII (T2T), we check whether the series of grid positions represents an uninterrupted path from the start position to the goal. Dataset For rectangular mazes, we generated 50 random mazes per size using maze-dataset [32], but replacing the square start and end with circles and swapping their colors. We also drew 10 irregular mazes by hand and flipped/rotated them to obtain 40 unique samples. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-imagepreview and Gemini 2.5 Pro gemini-2.5-pro through Google AI Studio. 33 Video models are zero-shot learners and reasoners Veo Create 2D animation based on the provided image of maze. The red square slides smoothly along the white path, stopping perfectly on the green square. The red square never slides or crosses into the black areas of the maze. The camera is static, top-down view showing the entire maze. Maze: The maze paths are white, the walls are black. The red square moves to the goal position, represented by green square. The red square slides smoothly along the white path. The red square never slides or crosses into the black areas of the maze. The red square stops perfectly on the green square. Scene: No change in scene composition. No change in the layout of the maze. The red square travels along the white path without speeding up or slowing down. Camera: Static camera. No zoom. No pan. No glitches, noise, or artifacts. 34 Video models are zero-shot learners and reasoners Gemini 2.5 Pro I2T system Think step by step as needed and output in xml format: <think>thinking process</think> <final_answer>final answer</final_answer> user The following image shows maze, represented by colored squares: Black squares represent walls and cannot be passed through. White squares are empty and can be passed through. The red square is the starting point. The green square is the end point. Please solve the maze by providing path from the starting point to the end point. The path should be provided as list of coordinates of each step, where each coordinate is (row, col) tuple, and row, col are 0-based indices. Consider the origin (0, 0) to be the top-left corner. Overall, the path should be provided in the format of [(row1, col1), (row2, col2), ...]. valid path must: Start at the starting point (the red square). End at the end point (the green square). Avoid the walls (the black squares). Pass only through empty space (the white squares). Move one square at time. Only move up, down, left, and right, not diagonally. Correct your answer if you spot any errors. Here is the maze image: {image} Nano Banana Mark the correct path from the red to the green circle through the maze in blue. 35 Video models are zero-shot learners and reasoners Gemini 2.5 Pro T2T system Think step by step as needed and output in xml format: <think>thinking process</think> <final_answer>final answer</final_answer> user The following is an ASCII-representation of maze: # represents walls which cannot be passed through. represents empty spaces that can be passed through. is the starting point. is the end point. Please solve the maze by providing path from the starting point to the end point. The path should be provided as list of coordinates of each step, where each coordinate is (row, col) tuple, and row, col are 0-based indices. Consider the origin (0, 0) to be the top-left corner. Overall, the path should be provided in the format of [(row1, col1), (row2, col2), ...]. valid path must: Start at the starting point S. End at the end point E. Avoid the walls #. Pass only through empty space . Move one square at time. Only move up, down, left, and right, not diagonally. Correct your answer if you spot any errors. Here is the maze in ASCII format: {maze} Sampling We generated 10 videos per sample with fixed prompt. B.6. Reasoning: Visual symmetry solving We provide details for the visual symmetry task in Sec. 4.6. Evaluation We prompt Veo with input images containing 1016 grid where pattern is drawn on the left half. The goal is to complete the pattern on the empty right half so that the final pattern is symmetrical along the central vertical axis. We compare Veos best-frame and last-frame solutions with the ground-truth symmetrical grid and compute the number of incorrectly-colored cells. cell is determined as incorrectly-colored if the average color across pixels in the cell is perceptually distinct from the ground-truth average color in the matching cell. We compute perceptual color differences of the average cell color in the CIELAB color space, with difference threshold of 15.0. In Fig. 8, we report the percentage of attempts in which the best or last frame solution has zero incorrect cells for ùëò = 1. Dataset We created synthetic grid coloring image dataset to evaluate visual symmetry. We generated 25 samples using common symmetrical symbols, objects and shapes such as english letters (e.g., A, H, M, X), geometric shapes (e.g., square, triangle), symmetrical objects (e.g., wineglass, balloon; together, the shape condition). We also generated 25 samples consisting of randomly-colored cells (the random condition). Video models are zero-shot learners and reasoners Figure 61 Visual analogy performance over 10 attempts. In contrast to other plots in this paper, we here report not the best performance over ùëò attempts, but instead the performance when choosing the majority vote from ùëò attempts. As result, performance is not necessarily monotonic in ùëò. In fact, for reflect and rotate, performance decreases with ùëò, indicating that both models have systematic, erroneous biases. In the case of Veo 3, the model tends to perform reflections and rotations, but not along the same axis as shown in the image. Veo 2 simply tends to copy the object without applying any transformation. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-imagepreview through Google AI Studio. Veo Instantly reflect this pattern along the central, vertical axis while keeping the existing colored pattern without modification. Static camera perspective, no zoom or pan. Sampling We generated 10 videos per sample with fixed prompt. B.7. Reasoning: Visual analogy completion We provide details for the visual analogy task in Sec. 4.7. Evaluation We prompt Veo to solve visual analogies with an input image showing reference object pair and test object. The object images are sourced from the Kid-inspired Visual Analogies benchmark [KiVA, 85]. Consistent with the multi-choice format in the KiVA benchmark, we evaluated Veos generation by cropping out the generated target object in the lower-right region of the last frame and compare Veos generated object with three candidate object choices using an autorater (see details below). In Fig. 9, we report the pass@1 accuracy across different conditions for both Veo 2 and Veo 3 for ùëò = 1. Fig. 61 shows performance for ùëò = 10. Dataset We used the test trials and choice images from the KiVA benchmark [85]. Models & prompts We tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generatepreview-001 through the Vertex AI API. Video models are zero-shot learners and reasoners We used Gemini 2.5 Pro gemini-2.5-pro through Google AI Studio to identify which image choice Veos generation is most similar with. To enhance the autoraters image comparison accuracy for this task, Gemini is prompted with privileged information about the values in the dataset conditions (see below for the full autorater prompt). If no object is visible in the lower-right region of Veos generated last frame or if the generated object is of different object type, we randomly sampled one of three choices as Veos choice. In pilot experiments, we found that the Gemini-assisted autoraters ratings achieve above 88% agreement with expert human ratings by the authors on 25 samples within each conditions. Note that in the prompt, words in { } are updated based on the test condition of the current generation (one of color, resize, reflect, and rotate) to provide more information of the feature name and values to direct the image comparison. Image choice orders are shuffled for each prompt. Veo Create smooth animation to generate the missing object in the lower right region and solve the visual analogy. The original three objects must remain still. Static shot, no zoom no pan no dolly. Gemini 2.5 Pro autorater system You are an expert visual judge. You will be presented with \"target image\" and three \"choice images\" labeled A, B, and C. Your goal is to identify the choice image that is most visually similar to the target image. Follow these steps: 1. Analyze each provided image and describe the objects shown. Focus on the object {color}. That is, if the objects appear {green}, {blue}, or {red}. 2. Determine if the primary object in the target image is of the same general category or type as the objects in the choice images. For example, if the target image shows dog, and the choices show cat, the object types are considered different. If no object is visible in the target image, the object type is considered to be mismatched. 3. If the object type matches between the target image and the choice images, identify the choice that is most visually similar to the target image in terms of the object {color}. Provide brief justification for your choice, explaining why it is the best match and why the others are less suitable. Conclude your response with the final answer on new line in the format: Final Answer: [answer] where answer is one of (A, B, C, or different object type). Do not use markdown format for the final answer line. user Please evaluate the following images. TARGET IMAGE {target object image} CHOICE IMAGES CHOICE A: {image choice} CHOICE B: {image choice} CHOICE C: {image choice} Which choice image is most similar to the target image? Sampling We generated 10 videos per sample with fixed prompt. 38 Video models are zero-shot learners and reasoners C. Prompting best practices Table 2 Prompt sensitivity study on the visual symmetry task. We report best frame pass@1 % and the average number of incorrectly-colored cells across 25 samples on each split (shape/random). No. Prompt 1 2 3 Instantly reflect this pattern along the central, vertical axis while keeping the existing colored pattern without modification. Instantly reflect this pattern along the central, vertical axis while keeping the existing colored pattern without modification. Static camera perspective, no zoom or pan. Instantly reflect this pattern along the central, vertical axis while keeping the existing colored pattern without modification. The result needs to be mirror-symmetrical along the vertical axis. Static camera perspective, no zoom or pan. Pass@1 Avg incorrect cells Shape Random Shape Random 48 36 68 4.16 7.00 65 5. 3.52 52 6.28 9.04 4 One by one, cells in the right half of the grid are filled in 12 10.76 14.08 to complete the pattern. The pattern is mirror-symmetrical along the central vertical line. Static shot; no zoom, no pan, no dolly. 7 5 Reflect this pattern along the central, vertical axis. 6 An animation showing the left half of the grid being mirrored onto the right half to create symmetrical pattern. Static shot; no zoom, no pan, no dolly. Youre master symmetry solver. Your task is to fill the cells on the right side of the grid to mirror the pattern on the left, such that its symmetrical along the vertical axis. Fill color in the appropriate cells on the right side of the grid to complete the pattern. The final image should be symmetrical along the central vertical line. Static shot, no zoom no pan no dolly. 8 9 Create static, smooth, realistic animation completing the pattern in the image by filling the grid on the right hand side. Do not change anything else. No zoom, no pan. 28 24 24 13 40 12 9.76 10.96 4.52 16.32 8 9 9. 17.72 10.30 14.74 4 14.88 21. 10 timelapse of professional pixel artist drawing 8 20 14.20 12.64 symmetrical pattern onto white canvas. Static shot; no zoom, no pan, no dolly. The results in Secs. 3 and 4 are best-effort estimates of Veos performance using carefully chosen prompts. Generally, performance varies greatly with the exact task description provided in the prompt, as illustrated by prompt sensitivity study on the visual symmetry task in Table 2. Here are best practices from this sensitivity analysis and our other experiments: Remove ambiguity. Tasks can be solved in variety of ways, and natural language descriptions tend to leave lot of room for interpretation. The goal should be formulated clearly, e.g., saying symmetrical along the central, vertical axis, rather than just symmetrical. Specify what shouldnt change. Veo has tendency to change any part of the input to create interesting, dynamic scenes. Including not only positive task description, but also specifying what not to change can help mitigate this, e.g., keep the existing colored pattern without modification. Providing an outlet. As mentioned above, Veo has strong prior to keep things moving. Providing motion outlet in the form of, e.g., spinning ball can help keep the rest of the scene static. Let the model decide when its done. The motion prior also means that Veo often keeps Video models are zero-shot learners and reasoners modifying the scene, even after solving the task. Providing visual indicator, e.g., add glowing red dot once the goal is reached allows for easy extraction of the solution from the generated video. Scene and camera controls. Phrases like static camera, no zoom, no pan, no dolly can help keeping the scene static, e.g., for image-to-image tasks. Speed control. Some tasks like maze solving benefit from being solved step-by-step. For other tasks, especially image-to-image tasks, specifying instant changes can help avoid artifacts. Realism. Veo was trained to generate plausible, realistic-looking videos. Translating an abstract task into realistic setting (including, but not limited to editing the original image to depict realistic, 3D scenes rather than abstract shapes) can improve generation results. similar effect was observed in [37], and we expect visual prompt engineering to emerge as powerful tool for video models. Video models are zero-shot learners and reasoners D. Failure cases Figure 62 Monocular depth estimation. Prompt: The image transitions to depth-map of the scene: Darker colors represent pixels further from the camera, lighter colors represent pixels closer to the camera. The exact color map to use is provided on the right side of the image. Static scene, no pan, no zoom, no dolly. Failure: Veo 3 seems generally unable to color pixels by depth beyond binary foreground/background mapping and specifically struggles with using provided color map. Figure 63 Monocular surface normal estimation. Prompt: The image transitions to surfacenormal map of the scene: the red/green/blue color channel specify the direction of the surface-normal at each point, as illustrated on the right side of the image on sphere. Static scene, no pan, no zoom, no dolly. Failure: While Veo 3 shows some promise in coloring surfaces according to their orientation (e.g., the cube in the front), coloration is inconsistent (compare the two cubes) and doesnt correctly interpolate colors (e.g., for the slope on the triangle). Figure 64 Force & motion prompting, inspired by [22, 23]. Force prompting (top). Prompt: The balls move in the direction indicated by the arrows. Balls without an arrow dont move. Static scene, no pan, no zoom, no dolly. Motion trajectory prompting (bottom). Prompt: Each car drives out of the frame following the indicated trajectory. Static camera, no zoom, no pan, no dolly. Failure: Veo 3 seems unable to follow force/motion annotations with any consistency. Providing annotations for the first frame and letting the model remove them before generating the scene in motion does not work, either. Video models are zero-shot learners and reasoners Figure 65 Tying the knot. Prompt: knot is tied connecting these two rope ends. Failure: physics violation, impossible rope movement. Figure 66 Connect the path puzzle.. Prompt: The path connecting the boy to the object starts glowing slowly. Nothing else changes. No zoom, no pan, no dolly.\" Failure: hallucinations, lighting up of all paths. Figure 67 Only five letter word search.. Prompt: Generate static video animation using the provided letter grid. The task is to highlight the only 5-letter English word CHEAT, which may be oriented in any direction (horizontally, vertically, or diagonally). The animation should consist of semi-transparent red rectangle with rounded corners smoothly fading into view, perfectly encapsulating the five letters of the word. The rectangle should have subtle, soft glow. Do not change anything else in the image. The camera must remain locked in place with no movement. No zoom, no pan, no dolly.\" Failure: does not recognize words; highlights individual letters randomly. Figure 68 Eulerian path.. Prompt: Create smooth animation where red pen traces all existing edges in continuous path without lifting the pen. All edges need to be traced. Do not visit any edge twice and do not lift the pen. No zoom, no pan.\" Failure: does not trace the edges exactly, traces non-existent edges. Video models are zero-shot learners and reasoners Figure 69 Solving system of linear equation.. Prompt: hand appears and solves the set of linear equations. It replaces the x, y, matrix with their correct values that solves the equation. Do not change anything else.\" Failure: hallucinations with text on the blackboard. Figure 70 Spot the difference.. Prompt: There are two images. The left image is different from the right image in 5 spots. Create static, realistic, smooth animation where cursor appears and points at each place where the left image is different from the right image. The cursor points one by one and only on the left image. Do not change anything in the right image. No pan. No zoom. No movement. Keep the image static.\" Failure: does not identify all the differences. Hallucinates differences. Figure 71 Perception IQ test.. Prompt: Create static, smooth, animation that solves the puzzle in the given image. The correct pattern should appear at the bottom right to solve the puzzle. Do not change anything else in the picture. No zoom, no pan, no dolly\" Failure: incorrect figure pattern. Figure 72 Glass falling. Prompt: The object falls. Static camera, no pan, no zoom, no dolly. Failure: physics violation, glass does not break, and orients itself to be vertical after landing on the floor. Figure 73 Collisions. Prompt: The two objects collide in slow motion. Static camera, no pan, no zoom, no dolly. Failure: not physically plausible, the objects pause at the moment of impact and then are pushed together by an invisible force. 43 Video models are zero-shot learners and reasoners Figure 74 Tiling puzzles. Jigsaw puzzle (top). Prompt: hand takes the fitting puzzle piece from the right, rotates it to be in the correct orientation, then puts it into the hole, completing the puzzle. Static scene, no pan, no zoom, no dolly. Failure: wrong piece orientation. Sliding puzzle (middle). Prompt: Slide the pieces of this sliding puzzle around one-at-a-time until all edges align. Failure: doesnt maintain piece integrity while sliding, hallucinates new pieces. Scrambled puzzle (bottom). Prompt: Unscramble this image. Failure: image details are inconsistent with original pieces. Figure 75 Bottleneck. Prompt: person tries to put the golf ball in the vase. Static camera, no pan, no zoom, no dolly.. Failure: not physically plausible, golf ball is too large to pass through the bottleneck of the vase. Figure 76 Laundry folding. Prompt: Generate video of two metal robotic arms properly folding the t-shirt on the table. Failure: physics violation, implausible folding movements. Video models are zero-shot learners and reasoners Figure 77 Motion planning; an instance of the piano movers problem. Prompt: The red couch slides from the left room over into the right room, skillfully maneuvering to fit through the doorways without bumping into the walls. The walls are fixed: they dont shift or disappear, and no new walls are introduced. Static camera, no pan, no zoom, no dolly. Failure: violating rigid-body integrity, not keeping to permissible transformations (rotation, translation). E. LLM use. Gemini 2.5 Flash and Gemini 2.5 Pro [12] were used for brainstorming task ideas, suggesting related work that we might have otherwise missed, coding support, and to polish human writing. F. Image sources. Where not stated in the figure caption, images were obtained as follows. Figs. 10 to 15, 32 to 38 and 74: The original macaw image was generated with Gemini and, depending on the figure, subsequently modified by the authors (e.g., conversion to grayscale, adding noise, adding the monkey with Nano Banana). Fig. 16: The input image was obtained from here (Apache 2.0 license) based on the LOLv2 dataset [84] and randomly selected. The image was slightly cropped to fit 16:9 aspect ratio. Figs. 17, 21 to 24, 26 to 29, 31, 39, 41, 42, 46, 47, 52, 54, 65, 69, 72, 73 and 75 to 77: generated with Gemini. Fig. 25: The input image was obtained from here (CC0 license). Fig. 30: hand drawn by us, inspired by Fig. 1 of the Omniglot paper [45]. Fig. 40: sample from Objaverse [14] Figs. 48 to 51, 53, 55 and 57: created by us. Figs. 56, 66, 67 and 70: original image from Reddit. Fig. 59: hand drawn by us, inspired by ARC-AGI [10]. Fig. 60: sample from BIPEDv2 [71, 72]. Figs. 62 to 64: generated with Gemini, then annotated by us. Figs. 68 and 71: hand drawn by us. Inspired by original images from Reddit. Figs. 44 and 45: The robot hands are extracted from frame in this video and were subsequently adapted with Nano Banana. The hands holding Baoding balls were obtained from here."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}