{
    "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
    "authors": [
        "Jing Bi",
        "Susan Liang",
        "Xiaofei Zhou",
        "Pinxin Liu",
        "Junjia Guo",
        "Yunlong Tang",
        "Luchuan Song",
        "Chao Huang",
        "Guangyu Sun",
        "Jinxi He",
        "Jiarui Wu",
        "Shu Yang",
        "Daoan Zhang",
        "Chen Chen",
        "Lianggong Bruce Wen",
        "Zhang Liu",
        "Jiebo Luo",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 5 1 3 0 . 4 0 5 2 : r Preprint. Under review. Why Reasoning Matters? Survey of Advancements in Multimodal Reasoning (v1) Jing Bi1, Susan Liang1, Xiaofei Zhou1, Pinxin Liu1, Junjia Guo1, Yunlong Tang1, Luchuan Song1, Chao Huang1, Guangyu Sun2, Jinxi He1, Jiarui Wu1, Shu Yang1, Daoan Zhang1, Chen Chen2, Lianggong Bruce Wen3 , Zhang Liu3, Jiebo Luo1, Chenliang Xu 1 1University of Rochester, 2University of Central Florida, 3Corning Inc. {jing.bi, sliang22, lsong11, yunlong.tang, daoan.zhang, chenliang.xu}@rochester.edu {jguo40, jhe44, jwu114, syang87}@ur.rochester.edu guangyu@ucf.edu, chen.chen@crcv.ucf.edu {Wenlb, liuz2}@corning.com"
        },
        {
            "title": "Abstract",
            "content": "Reasoning is central to human intelligence, enabling structured problemsolving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contextswhere models must integrate both visual and textual inputscontinues to be significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research."
        },
        {
            "title": "Introduction",
            "content": "Reasoning is fundamental aspect of human intelligence, enabling us to solve complex problems effectively. In LLMs, reasoning has shown promising capabilities, leading to significant advancements in various domains (Bi et al., 2024b), including arithmetic, commonsense, and symbolic reasoning. For instance, Chain-of-Thought (CoT) prompting mimics human stepwise problem-solving to boost LLM performance, especially in very large models. Beyond CoT, other prompting strategies, such as generated knowledge prompting and tree search algorithms (e.g., STAR search (Zelikman et al., 2022)), have been These incorporated into LLM training to further enhance reasoning capabilities. methods encourage the model to articulate its internal reasoning in natural language, which reinforces its understanding of the task. By laying out intermediate steps, the model can verify and adjust its logic before arriving at the final answer. 1 Preprint. Under review. While reasoning in LLMs has progressed significantly, extending these capabilities to multimodal tasks remains an emerging frontier, as multimodal large language models (MLLMs) must navigate the added complexity of interpreting and integrating information from both visual and textual modalities, often resolving ambiguities, inconsistencies, or gaps that arise when the two sources conflict or diverge. Effective reasoning is central to enabling MLLMs to achieve compositional understandingallowing them to deconstruct complex tasks into interpretable, modality-spanning steps. It also supports iterative processes such as error correction and self-refinement, where models can revise their outputs by reevaluating both visual and textual cues. Figure 1: Papers on visual reasoning per quarter over the last three years, with state computed using referenced papers. (Data current to mid-March 2025.) Moreover, strong reasoning capabilities help clear up confusion by inferring spatial relationships, handling counterfactuals, and choosing appropriate tools or actions. These mechanisms also play crucial role in mitigating hallucinations by grounding model outputs in cross-modal evidence, ultimately improving both accuracy and trustworthiness. Finally, reasoning extends the potential of MLLMs to handle hypothetical scenarios, helping them look ahead and explore what-if situations that span both language and vision. Initial research has demonstrated that integrating Chain-of-Thought (CoT) reasoning across different modalities can significantly enhance the performance of MLLMs. For example, Yao et al. (2024); Zhang et al. (2024a); Bi et al. (2024a) reports that the LLama series models reasoning abilities are greatly improved by solely utilizing filtered self-generated reasoning paths. Building on this momentum, our work provides comprehensive overview of reasoning in both LLMs and MLLMs, with focus on its techniques, applications, and future directions. We aim to bridge the gap between text-based and multimodal reasoning, offering insights into how reasoning can further enhance the capabilities of LLMs in multimodal contexts. In the following sections, we begin with clear problem formulation and definition of reasoning, followed by an in-depth analysis of reasoning techniquesfrom post-training strategies to test-time computation. We then examine recent trends in datasets and benchmarking. Our goal is to offer well-organized and accessible survey that supports both theoretical understanding and practical implementation of reasoning in LLMs and MLLMs."
        },
        {
            "title": "2 Background",
            "content": "In complex question-answering tasks, directly predicting an answer can be highly uncertain due to the vast range of possible responses. more effective approach involves breaking down the reasoning process into sequence of intermediate steps. This structured method not only improves interpretability but also helps reduce uncertainty at each inference step. Formally, let denote given question. Conventional language models typically aim to model the conditional probability P(A Q) of the answer given the question. However, as the complexity of increases, the prediction becomes more uncertainreflected by rise in the conditional entropy H(A Q). natural approach to mitigate this uncertainty is to decompose the reasoning process into sequence of intermediate steps. This leads to modeling the answer generation as structured chain of conditional probabilities: P(Step1 Q) P(Step2 Step1, Q) P(Stept Step1:t1, Q) P(A Step1:t, Q) (1) Such decomposition encourages the model to reduce uncertainty at each stage and improves interpretability and robustness in complex question answering. This step decomPreprint. Under review. Figure 2: Framework illustrating training and inference for reasoning optimization. virtuous cycle emerges as better policies generate improved trajectories, which in turn enhance the model through stronger supervision. position is useful because conditioning on prior steps reduces uncertainty. Specifically, the conditional entropy satisfies: H(Stept Step<t, Q) H(Stept Q), meaning each step becomes easier to predict as more context accumulates. From probabilistic standpoint, the entire reasoning process can be viewed as trajectory, denoted as τ = (Step1, . . . , Stept), which represents complete reasoning path. Using this, the overall probability of an answer given question can be written as marginal over all possible trajectories: P(A Q) = τ P(A τ, Q) P(τ Q). (2) However, this formulation presents challenge: enumerating all possible reasoning paths τ is intractable. In practice, CoT reasoning offers practical workaround by sampling single trajectory ˆτ from the distribution P(τ Q), and using it to approximate the answer: P(A Q) P(A ˆτ, Q), where ˆτ P(τ Q). (3) This is beneficial for many tasks if the model is capable of generating one coherent reasoning path that captures the correct structure of the solution, thereby reducing uncertainty and improving answer accuracy. More advanced methods such as Tree-of-Thought (ToT) expand on CoT by generating and evaluating multiple reasoning paths, and can be seen as multiple point estimation approach. In this case, the model generates set of reasoning paths {τ1, τ2, . . . , τn} and evaluates them to select the most promising one. Given the challenge of enumerating all possible reasoning paths, our objective becomes clear: to generate and identify only the most promising trajectories. Achieving this effectively requires two key components: (i) Generation strong model capable of producing highquality reasoning paths τ; (ii) Selection reliable mechanism for evaluating and choosing the best candidate path. These components are deeply interconnected: improved generation policies yield better trajectories, which in turn provide stronger supervision signals to further refine the model. This creates virtuous cycle of mutual enhancement, as illustrated in Figure 2. At inference time, we want to search for the best τ under the current model. During training, we aim to improve the model so that it generates reasoning paths that are more likely to lead to correct answers. Broadly speaking, recent approaches fall into two main categories: Post-training improvement involves optimizing the model policy π to better align outputs with desired utility, typically through techniques like fine-tuning or reinforcement learning. Formally, this corresponds to solving Equation (4), where the goal is to learn policy that maximizes expected utility over generated outputs τ. Test-time compute focuses on improving LLM performance during inference without modifying the models core parameters. This aligns with the objective in Equation (4), 3 Preprint. Under review. where the goal is to select the most useful output trajectory τ under fixed policy π, optimizing accuracy, coherence, and efficiency through dynamic strategies like CoT, ToT, and search-based reasoning. π = arg max π τ = arg max τπ Utility(τ) Eτπ[Utility(τ)] (Training: Learn better model) (Inference: Select best trajectory) (4) In the following sections, we will discuss these two components in detail."
        },
        {
            "title": "3 Post-training improvements",
            "content": "To enhance the quality of reasoning, we aim to learn model that generates high-utility reasoning trajectories. One effective approach frames this as Markov Decision Process (MDP), where the model is trained to maximize the expected return of reasoning path τ: max θ Eτπθ (cid:34) (cid:35) γtR(st, at) with θ J(πθ) = Eτ (cid:34) t=1 θ log πθ(at st)A(st, at) (5) (cid:35) st is the sequence of tokens up to time t, and at is the next token. The advantage function A(st, at) estimates the benefit of choosing at in state st, guiding the model toward higherutility token sequences. This objective can be directly optimized with respect to the model parameters using policy gradient methods (Williams, 1992). Notably, this formulation enables the training of LLMs not merely to predict the next token, but to generate complete reasoning trajectories that are optimized for long-term rewards. (cid:17) Policy optimization is still in its early stages, often relying on existing methods with primary focus on text token optimization. However, approaches from visual reinforcement learning are worth exploring to better align policies with perceptual and spatial dynamics. Reward alignment remains heavily reliant on textual signals, highlighting an opportunity to shift focus toward visual-based rewards. Developing methods that can interpret and adapt to visual feedbacksuch as spatial cues, object dynamics, and scene changeswill be key to achieving more grounded and effective visual decision-making. In terms of model architecture, improvements often stem from better attention to visual details that align with long-horizon reasoning steps. Spatial-temporal modeling offers unique opportunities for more creative action definition to manipulate the visual information for better reasoning. Similarly, during data curation, verifiers commonly depend on grounding models or language cuesindicating potential for more creative, vision-focused verifier designs. 3.1 Policy Optimization To solve the above reward maximization problem, recent advancements in visual reasoning have prominently focused on integrating reinforcement learning (RL) and imitation learning (IL) to align MLLMs more closely with human reasoning. IL approaches, such as Thought Cloning (Wei et al., 2025), surpass traditional cloning by aligning intermediate reasoning steps instead of final outputs alone. Coupled with iterative methods like DAgger, this reduces dataset shift and hallucination. LLaVA-Critic (Xiong et al., 2024) further generalizes reward signals via preference-based alignment, boosting multimodal reasoning effectiveness with minimal modality-specific fine-tuning. RL from Simulations (RLS3) (Waite et al., 2025), employing Soft Actor-Critic (SAC) agents, significantly enhances spatial reasoning accuracy and efficiency in models like PaliGemma, outperforming random exploration baselines (Waite et al., 2025). FuRL (Fu et al., 2024c) addresses RL reward misalignment through dual-stage alignment, effectively mitigating sparse reward challenges. Adaptive Reward Programming (ARP-DT+) (Kim et al., 2023), integrating CLIP representations with 4 Preprint. Under review. Decision Transformers, enhances reasoning robustness across domains by employing active inference to counter distractions and domain shifts. 3.2 Reward Alignment Recent approaches have explored iterative refinement and reflection mechanisms, tokenlevel rewards, and automated benchmarking to enhance reasoning quality and mitigate hallucinations. PARM++ (Guo et al., 2025) introduces reflection-based mechanism that iteratively refines outputs by identifying misalignments and actively correcting them through self-assessment loops, significantly improving test-time GenEval scores up to three reflection cycles (Guo et al., 2025). Similarly, REVERIE employs rationale-based training, explicitly supervising reasoning steps, thus improving coherence and reducing hallucinations compared to models without rationales (Zhang et al., 2024f). Fine-grained reward models, such as MMViG (Yan et al., 2024) and Reward Alignment via Preference Learning (RAPL) (Tian et al., 2024), emphasize granular feedback at individual reasoning steps, achieving precise improvements without manual reward tuning. Similarly, CLIP-DPO (Ouali et al., 2024) leverages pre-trained CLIP directly as reward function, streamlining reward modeling. Challenges such as constrained perceptual fields are addressed through dynamic evaluation strategies like zooming and shifting, emphasizing the importance of sequential decision-making in visual reasoning (Wang et al., 2024j). Structural issues like positional bias and patch-boundary effects further underscore the need for targeted architectural optimizations (Zhang et al., 2024d). Recent approaches integrate reward signals directly into generation. TLDR (Fu et al., 2024a) applies token-level binary rewards, enabling real-time feedback to mitigate hallucinations and boost both annotation efficiency and backbone model performance (Fu et al., 2024a). Complementarily, EACO enhances intermediate object recognition, significantly reducing hallucinations compared to earlier models like LLaVA-v1.6-7B (Wang et al., 2024g). Distillation frameworks also contribute: FIRE leverages iterative studentteacher feedback and structured evaluation to refine visual reasoning (Li et al., 2024d), while SILKIE demonstrates the scalability of GPT-4V-generated feedback in improving perception and cognition without human intervention (Li et al., 2023b). 3.3 Model Architecture Nested architectures, such as MaGNeTS, have emerged as effective means of balancing computational efficiency with model accuracy by employing parameter-sharing and caching mechanisms during inference (Goyal et al., 2025). Intrinsic activation approaches, notably ROSS, bypass external modules by integrating multimodal understanding directly into the models core, yielding improved adaptability to diverse visual inputs, including depth maps (Wang et al., 2024c). Additionally, DIFFLMM incorporates diffusion models and attention-based segmentation, enhancing visual grounding precision (Cao et al., 2024a), while Mini-Monkey targets the sawtooth effect in lightweight models through adaptive cropping and scale compression, achieving superior visual-text alignment without extensive computational resources (Huang et al., 2024b). SEA dynamically adjusts embedding alignment strategies to maintain performance across varying resolutions and model sizes (Yin et al., 2024). Concurrently, PAE-LaMM demonstrates the combined effectiveness of vision encoder adaptation and pixel reconstruction tasks, systematically enhancing visual detail awareness and question-answering performance (Gou et al., 2024). Conversely, LLAVIDAL and EventLens uniquely integrate domain-specific visual elements, such as object-centric embeddings and event-aware tokens, respectively, directly within LLM architectures to effectively reason about activities and temporal contexts (Reilly et al., 2024; Ma et al., 2024a). 3.4 Spatial Temporal Modeling Unlike pure language reasoning, visual reasoning involves richer action space, allowing operations such as zooming, cropping, and frame selection, which enables the model to interact with and manipulate visual inputs. The GeoGLIP pipeline utilizes geometric pre-training with dynamic feature router to advance fine-grained visual mathematical understanding by balancing symbolic and visual modalities based on task demands (Zhang et al., 2025). 5 Preprint. Under review. Concurrently, hallucination issues are mitigated via targeted fine-tuning and prompting strategies grounded in recent vision-language work like CapQA (Hu et al., 2025). For spatial reasoning, simpler graph topologies have been shown to enhance performance, while structural complexity reduces accuracy in edge-centric tasks (Zhu et al., 2024). Augmentation via uncertainty-aware active inference further boosts attribute detection precision (Zhang et al., 2024e). Intrinsic spatial-temporal modeling is advanced through architectures embedding multi-modal understanding without reliance on external depth tools (Wang et al., 2024c), and dual-branch structures improve video temporal grounding in multi-hop reasoning (Chen et al., 2024f). The shift toward multimodal benchmarks better reflects real-world spatial-temporal reasoning (Li et al., 2024a). Integrating rationalization with answer generation enhances temporal reasoning (Zohar et al., 2024), while automated annotation improves data scalability and bridges gaps between commercial and open-source Video-LLMs (Li et al., 2024e). Multi-modal integration and active inference optimize modality selection for multi-step spatial-temporal reasoning (Zhang et al., 2024l). Adjusting learning objectives to minimize hallucinations improves precision and EOS decision-making (Yue et al., 2024). Figure 3: Search framework where language models explore and refine reasoning paths. Trajectories are scored using reward models, based on expected utility or final output quality, and guided by feedback, world models, and evaluators to select the most promising steps."
        },
        {
            "title": "4 Test-Time compute",
            "content": "At test time, the goal is to find the reasoning trajectory τ that maximizes utility function as shown as red and green path in Figure 3 τ = arg max τ (τ), where (τ) = r(st, at) P(goal τ) P(τ τ) (rank(τ)) ... (MDP-style reward) (Goal likelihood) (Preference-based) (Ranking-based) (6) (Others: risk-sensitive, etc.) The utility (τ) can be defined in various ways, including cumulative rewards (MDP-style), goal likelihood, preference comparisons, and ranking-based scores. The choice of utility depends on the supervision available and the nature of the task. The core challenge remains sampling high-utility trajectories under the chosen formulation. Since directly modeling (τ) is often difficult, we use reward models to approximate it. These models can include (1) absolute rewards (as in standard RL), (2) pairwise preference models (e.g., DPO), and (3) ranking-based models. 6 Preprint. Under review. (cid:17) Search Strategies Current search algorithms primarily operate on distinct textual tokens. However, innovative approaches are emerging for raw visual tokens, termed Creative Visual Search, expanding search capabilities directly within visual spaces. Moreover, Multi-Granularity Spatial Search is gaining attention as an effective extension, enhancing exploration across diverse spatial granularities. Reward and Feedback Reward systems can significantly benefit from deeper visual insights. Leveraging visual prompts encourages models to engage in self-reflection grounded in visual information, substantially improving semantic alignment and decision-making effectiveness. Iterative Refinement Determining the alignment between textual tokens and visual information remains challenging. Consequently, model self-reflection can produce hallucinated interpretations. Nevertheless, these reflections offer valuable insights, highlighting new pathways to mitigate unnecessary or hallucinated reflections and enhance model reliability. 4.1 Search Strategies Monte Carlo Tree Search (MCTS) has emerged as powerful tool for managing uncertainty and sequential decision-making. Imam et al. (2025) applied MCTS to enhance temporal reasoning, reducing hallucinations in time-dependent data. Building on this, CoMCTS(Yao et al., 2024) unified multiple MLLMs within collaborative tree search, addressing model bias and improving accuracy and efficiency. Vision-specific strategies advanced with VisVM (Wang et al., 2024f), which estimates long-term candidate value to reduce hallucinations, outperforming immediate-alignment methods like CLIP-PRM. Video reasoning similarly benefited from reward modeling and adaptive exploration to prioritize key frames (Yang et al., 2024b). Structured search methods also gained traction. LLaVA-o1 (Xu et al., 2025) uses stage-level beam search for complex reasoning, while MVP (Qu et al., 2024) aggregates certainty across multi-perspective captions to resist adversarial inputs. DC2 (Wang et al., 2024e) applies MCTS-based cropping to focus on salient image regions for high-resolution reasoning. Multimodal and temporal search frameworks like VideoVista (Li et al., 2024e), WorldRetriver (Zhang et al., 2024l), and DynRefer (Zhao et al., 2024) surpass static baselines using adaptive sampling, fusion, and stochastic inference. Step-by-step comparative reasoning with LMs also enhances video QA (Nagarajan & Torresani, 2024). Innovations in context refinement (VURF (Mahmood et al., 2024)), image decomposition (V*(Wu & Xie, 2023)), and dynamic tool use (AVIS(Hu et al., 2023)) highlight the shift toward adaptive visual reasoning. FuRL (Fu et al., 2024c) aligns reward modeling with iterative fine-tuning for better performance. In embodied AI, combining chain-of-thought, self-verification, and MCTS-driven planning enables scalable, robust decision-making in dynamic environments (Shin et al., 2024). 4.2 Adaptive Inference Adaptive inference is reshaping vision-language reasoning by enabling dynamic, contextsensitive processing that improves both accuracy and efficiency. Central to this is the iterative evaluation and refinement of outputs, often through internal feedback and external verification. LOOKBACK enhances correction accuracy through iterative visual re-examination within each reasoning step (Wu et al., 2024b). IXC-2.5 balances performance and response length via Best-of-N sampling guided by reward models (Zang et al., 2025), while PARM++ uses reflection-based refinement to align generated images with prompts (Guo et al., 2025). Further innovations include ProgressCaptioners sliding-window approach for tracking action progress over time (Xue et al., 2024). To combat multi-modal hallucinations, MEMVR triggers visual retracing based on uncertainty, optimizing cost-accuracy tradeoffs (Zou et al., 2024a), while MVP aggregates certainty across diverse views (Qu et al., 2024). SID applies token-level contrastive decoding to filter irrelevant content (Huo et al., 2024). Models like DualFocus integrate macro and micro reasoning for fine-grained attention control (Cao et al., 2024b), and LISA++ enables test-time compute scaling without retraining (Yang et al., 2024a). PerceptionGPT accelerates inference via adaptive token embeddings that encode spatial cues and dynamically weigh layers (Pi et al., 2023). 7 Preprint. Under review. 4.3 Reward Guo et al. (2025) introduce ORM and PRM, showing that ORMs final-step evaluation significantly enhances image generation through ranking data. PARM++ further refines this method by enabling iterative reflection for self-correction, thus improving prompt fidelity. Complementarily, Hao et al. (2025) highlight persistent errors in VLM spatial reasoning and explanation, proposing reward re-ranking as an interim solution. To assess VL-GenRMs more comprehensively, Li et al. (2024b) propose VL-RewardBench, pinpointing perception and reasoning challenges while advocating critic training and co-evolutionary reward learning. Self-training methods also gain prominence: R3V (Cheng et al., 2024b) leverages synthesized rationales to enhance noise-robust Chain-of-Thought (CoT) refinement, while CECE (Cascante-Bonilla et al., 2024) enriches evaluation using LLM-generated entailment/contradiction captions. Enhanced supervision methods also emerge. Zhang et al. (2024k) enhance CoT reasoning through ShareGPT-4O distillation, Supervised Fine-Tuning (SFT), and Direct Preference Optimization (DPO). LLaVA-Critic (Xiong et al., 2024) builds upon this using critic-guided DPO without external human feedback. FuRL (Fu et al., 2024c) introduces two-stage tuning approach addressing misalignments and temporal errors, while HYDRA (Ke et al., 2024) employs dynamic loops for adaptive instruction ranking. Additionally, m&ms (Ma et al., 2024b) provides structured framework for evaluating multi-step planning via tool and argument accuracy. 4.4 Feedback VOLCANO employs three-stage iterative frameworkinitial response generation, visual self-assessment, and revisionto reduce hallucinations by emphasizing accurate image details (Lee et al., 2024). Similarly, LOOKBACK mandates explicit atomic verification against images to enhance both critique and correction processes for LVLMs (Wu et al., 2024b). LLaVA-Critic advances feedback by incorporating diverse critic instructions and iterative DPO training, leveraging internal data (Xiong et al., 2024). Additionally, tailored visual prompting with iterative binary verification effectively enhances semantic grounding in models like LLaVA-1.5, ViP-LLaVA, and CogVLM (Liao et al., 2024a). Minimalist reinforcement learning setups have shown effectiveness, particularly for multimodal mathematical reasoning, with larger models like InternVL2.5-38B benefiting from difficulty-based data filtering to stabilize training (Meng et al., 2025). Explicit visual CoT mechanisms, exemplified by VisCoT, integrate bounding box predictions and iterative cropping to enhance visual question-answering performance (Shao et al., 2024). In video understanding, VIDEOTREE utilizes adaptive hierarchical clustering for efficient navigation through structured treebased representations (Wang et al., 2024i). Lastly, prompt engineering strategies like MiHO and MiHI significantly reduce hallucinations without model retraining, post-processing evaluations with GPT-4 further enhancing reliability without architectural modifications (Han et al., 2024; Hu et al., 2025). 4.5 Iterative Refinement Adaptive inference transforms vision-language reasoning by facilitating dynamic, contextsensitive processing to enhance accuracy and efficiency. Central to this approach is iterative output refinement through internal feedback and external verification. For instance, IXC-2.5 employs Best-of-N sampling guided by reward models to optimize performance and response length (Zang et al., 2025), while PARM++ uses reflection-based refinement for better alignment between images and prompts (Guo et al., 2025). Further advancements include LLaVA-o1s structured perception and stage-level beam search for decomposing complex reasoning tasks (Xu et al., 2025). To mitigate multi-modal hallucinations, MEMVR implements visual retracing based on uncertainty to optimize the cost-accuracy tradeoff (Zou et al., 2024a), MVP aggregates certainty across diverse views (Qu et al., 2024), and SID employs token-level contrastive decoding to eliminate irrelevant content (Huo et al., 2024). Moreover, DualFocus integrates macroand micro-level reasoning for precise attention control (Cao et al., 2024b), LISA++ enables test-time compute scaling without retraining (Yang et al., 2024a), and PerceptionGPT accelerates inference through adaptive token embeddings encoding spatial cues and dynamically weighted layers (Pi et al., 2023). 8 Preprint. Under review. 4.6 Dataset Curation Recent work shows that high-quality, strategically curated datasets outperform large but noisy alternatives in guiding reasoning paths. R-CoT (Deng et al., 2024) enhances geometric reasoning via reverse generation and stepwise synthesis. Task-specific datasets such as SMIR (Li et al., 2025a) show up to 8% performance gains over generic datasets, demonstrating the value of targeted curation. Gradual complexity in dataset construction, as shown in VIREO (Cheng et al., 2024a), ensures foundational reasoning skills before introducing advanced tasks. Scalable dataset creation through automation also proves effective. VideoVista (Li et al., 2024e) uses auto-annotations for video reasoning, outperforming manually curated sets. DecoVQA+ (Zhang et al., 2024b) explicitly teaches when to apply question decomposition, aided by SelectiveVQD loss for better strategy selection. LocVLM (Ranasinghe et al., 2024) scales with pseudo-data and implicit feedback signals like location and relevance prediction. CogCoM (Qi et al., 2024) incorporates tasks like grounding and manipulation to reinforce step-by-step reasoning."
        },
        {
            "title": "5 Datasets and Benchmarks",
            "content": "Structured and Task-Specific Reasoning Datasets such as Visual-RFT(Liu et al., 2025c), CapQA(Hu et al., 2025), GUIDE(Liang et al., 2024a), STAR(Wu et al., 2024a), Visual Genome(Zhang et al., 2024f), VL-GPT(Zhu et al., 2023), and Interfacing (Zou et al., 2024b) have introduced structured prompting that significantly enhances models reasoning accuracy by explicitly guiding reasoning processes. In contrast to earlier benchmarks like STAR, these datasets place stronger emphasis on task-specific reward functions and structured inference, aiming to improve generalization across diverse visual and linguistic contexts. Temporal and Spatial Reasoning Temporal benchmarks like VisualQA, and TemporalVQA (Imam et al., 2025), TLQA (Swetha et al., 2025), REXTIME (Chen et al., 2024c), FrameCap (Xue et al., 2024), and VideoVista (Li et al., 2024e) have revealed substantial limitations in current multimodal language models, achieving accuracy significantly below human levels (15% versus 90%). These benchmarks emphasize the necessity for temporal logic annotations and highlight the difficulty models face when reasoning across video frames. In parallel, spatial datasets such as SpatialVLM (Chen et al., 2024a), WhatsUp (Kamath et al., 2023), DC2 (Wang et al., 2024e), and Grounded (Chen et al., 2024f) showcase improvements through extensive spatial reasoning QA pairs and challenging spatial configurations. Iterative and Reflective Reasoning Iterative reasoning capabilities have markedly improved through datasets like VISCO (Wu et al., 2024b), Mulberry-260k (Yao et al., 2024), FIRE (Li et al., 2024d), VLFeedback (Li et al., 2024c), Silkie (Li et al., 2023b), TIIL (Huang et al., 2024c), ConMe (Huang et al., 2024a), and Reflective (Zhang et al., 2024f), enabling models to learn from mistakes by both incorporating reflective and iterative feedback loops. For instance, VISCO demonstrated that human critiques significantly outperform model critiques (76% error correction), highlighting persistent gap in models ability to independently selfcorrect. Complex Multimodal Evaluation Complex multimodal datasets like EMMA (Hao et al., 2025), CoMT (Cheng et al., 2024c), SMIR (Li et al., 2025a), ProVision (Zhang et al., 2024e), MAGEBench (Zhang et al., 2024i), MM-Vet v2 (Yu et al., 2024), JourneyBench (Wang et al., 2025), VERIFY (Bi et al., 2025) and CompCap (Chen et al., 2024g) exposed limitations in current multimodal models integration capabilities. EMMA specifically shows that models struggle significantly with iterative multimodal interactions, particularly when tasks demand deep integration across domain-specific problems. Benchmarks like FineCops-Ref (Liu et al., 2025b), VL-RewardBench (Li et al., 2024b), MM-SAP (Wang et al., 2024h), PCA-Bench (Chen et al., 2024d), AttCoSeg (Pramanick et al., 2024), and M3CoT (Chen et al., 2024e) emphasize detailed evaluation metrics (Recall@k, AUROC). These benchmarks provide insights into models actual reasoning capabilities beyond traditional accuracy metrics. Counterfactual and Logical Reasoning Datasets like CounterCurate (Zhang et al., 2024c), C-VQA (Zhang et al., 2024g), CausalChaos! (Parmar et al., 2024), LogicAI (Xiao et al., 2024), Preprint. Under review. and MCDGRAPH (Zhu et al., 2024) explicitly test model reasoning robustness, showing improvements through challenging counterfactual examples and structured logical questions. Active Perception and Progressive Reasoning Active perception datasets such as ActiView (Wang et al., 2024j) and progressive reasoning benchmarks such as Blink (Fu et al., 2024b), ADL-X (Reilly et al., 2024), and GUIDE (Liang et al., 2024a) challenge models dynamic reasoning capabilities through active view adjustments and frame-level progression."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we presented comprehensive survey of existing work on reasoning in Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs), with focus on optimizing model performance and identifying the most effective reasoning trajectories. We introduced variety of post-training and test-time computation methods, discussing their potential to enhance model capabilities on complex, real-world tasks. Our analysis highlights the critical role of reasoning in improving the visual understanding abilities of MLLMs. Furthermore, we have provided insights and outlined potential directions for future research to guide the development of more robust and efficient reasoning frameworks."
        },
        {
            "title": "References",
            "content": "Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor. Mitigating Open-Vocabulary Caption Hallucinations, October 2024. URL http://arxiv. org/abs/2312.03631. arXiv:2312.03631. Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, and Chenliang Xu. Unveiling visual perception in language models: An attention head analysis approach, 2024a. URL https://arxiv.org/abs/2412.18108. Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Nguyen Nguyen, and Chenliang Xu. Eagle: Egocentric aggregated language-video engine. In Proceedings of the 32nd ACM International Conference on Multimedia, MM 24, pp. 16821691. ACM, October 2024b. doi: 10.1145/3664647.3681618. URL http://dx.doi.org/10.1145/3664647.3681618. Jing Bi, Junjia Guo, Susan Liang, Guangyu Sun, Luchuan Song, Yunlong Tang, Jinxi He, Jiarui Wu, Ali Vosoughi, Chen Chen, and Chenliang Xu. Verify: benchmark of visual explanation and reasoning for investigating multimodal reasoning fidelity, 2025. Kiran Bisra, Qing Liu, John Nesbit, Farimah Salimi, and Philip Winne. Inducing self-explanation: meta-analysis. Educational Psychology Review, 30:703725, 2018. Jerome Bruner. The process of education. Harvard university press, 2009. Shengcao Cao, Liang-Yan Gui, and Yu-Xiong Wang. Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision, October 2024a. URL http://arxiv. org/abs/2410.08209. arXiv:2410.08209. Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Jiaqi Wang. DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models, February 2024b. URL http://arxiv.org/abs/2402.14767. arXiv:2402.14767. Paola Cascante-Bonilla, Yu Hou, Yang Trista Cao, Hal Daume III, and Rachel Rudinger. Natural Language Inference Improves Compositionality in Vision-Language Models, October 2024. URL http://arxiv.org/abs/2410.22315. arXiv:2410.22315. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities, January 2024a. URL http://arxiv.org/abs/2401. 12168. arXiv:2401.12168. 10 Preprint. Under review. Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Weimo Deng, Ziyong Feng, Yongle Zhao, and Yin Xie. Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models, June 2024b. URL http://arxiv.org/abs/2403.19322. arXiv:2403.19322. Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, and Yu-Chiang Frank Wang. ReXTime: Benchmark Suite for Reasoning-Across-Time in Videos, July 2024c. URL http://arxiv.org/abs/2406.19392. arXiv:2406.19392. Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain, February 2024d. URL http://arxiv.org/abs/2402.15527. arXiv:2402.15527. Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M$ˆ3$CoT: Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought, May 2024e. URL http://arxiv.org/abs/2405.16473. arXiv:2405.16473. Qirui Chen, Shangzhe Di, and Weidi Xie. Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos, August 2024f. URL http://arxiv.org/abs/2408.14469. arXiv:2408.14469. Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang, David Yang, ShengYun Peng, Hanchao Yu, Shen Yan, Xuewen Zhang, and Baosheng He. CompCap: Improving Multimodal Large Language Models with Composite Captions, December 2024g. URL http://arxiv.org/abs/2412.05243. arXiv:2412.05243. Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. From the Least to the Most: Building Plug-and-Play Visual Reasoner via Data Synthesis, October 2024a. URL http://arxiv. org/abs/2406.19934. arXiv:2406.19934. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. VisionLanguage Models Can Self-Improve Reasoning via Reflection, October 2024b. URL http://arxiv.org/abs/2411.00855. arXiv:2411.00855. Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, and Libo Qin. CoMT: Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models, December 2024c. URL http://arxiv.org/abs/2412. 12932. arXiv:2412.12932. Michelene TH Chi, Nicholas De Leeuw, Mei-Hung Chiu, and Christian LaVancher. Eliciting self-explanations improves understanding. Cognitive science, 18(3):439477, 1994. Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu, Chengquan Zhang, Pengyuan Lyu, Ziyang Zhang, Gang Zhang, Errui Ding, Yingying Zhu, and Xiang Bai. R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models, October 2024. URL http://arxiv.org/abs/2410.17885. arXiv:2410.17885. Maksim Dzabraev, Alexander Kunitsyn, and Andrei Ivaniuta. VLRM: Vision-Language Models act as Reward Models for Image Captioning, April 2024. URL http://arxiv.org/ abs/2404.01911. arXiv:2404.01911. Wan-Cyuan Fan, Tanzila Rahman, and Leonid Sigal. MMFactory: Universal Solution Search Engine for Vision-Language Tasks, December 2024. URL http://arxiv.org/abs/ 2412.18072. arXiv:2412.18072. Keith Frankish. Dual-process and dual-system theories of reasoning. Philosophy Compass, 5 (10):914926, 2010. Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, and Lawrence Chen. TLDR: Token-Level Detective Reward Model for Large Vision Language Models, October 2024a. URL http://arxiv.org/abs/2410.04734. arXiv:2410.04734. 11 Preprint. Under review. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. BLINK: Multimodal Large Language Models Can See but Not Perceive, July 2024b. URL http://arxiv.org/abs/2404.12390. arXiv:2404.12390. Yuwei Fu, Haichao Zhang, Di Wu, Wei Xu, and Benoit Boulet. FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning, June 2024c. URL http://arxiv. org/abs/2406.00645. arXiv:2406.00645. Dedre Gentner and Francisco Maravilla. Analogical reasoning. In International handbook of thinking and reasoning, pp. 186203. Routledge, 2017. Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, and Mohamed Elhoseiny. How Well Can Vision Language Models See Image Details?, August 2024. URL http://arxiv.org/abs/2408.03940. arXiv:2408.03940. Sahil Goyal, Debapriya Tula, Gagan Jain, Pradeep Shenoy, Prateek Jain, and Sujoy Paul. Masked Generative Nested Transformers with Decode Time Scaling, February 2025. URL http://arxiv.org/abs/2502.00382. arXiv:2502.00382. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can We Generate Images with CoT? Lets Verify and Reinforce Image Generation Step by Step, January 2025. URL http://arxiv.org/abs/2501.13926. arXiv:2501.13926. Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, and Mike Zheng Shou. Skip n: Simple Method to Reduce Hallucination in Large Vision-Language Models, May 2024. URL http://arxiv.org/abs/2402.01345. arXiv:2402.01345. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark, January 2025. URL http://arxiv.org/abs/2501.05444. arXiv:2501.05444. Xin He, Longhui Wei, Lingxi Xie, and Qi Tian. Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models, January 2024. URL http: //arxiv.org/abs/2401.03105. arXiv:2401.03105. Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, and Jiajun Wu. What Makes Maze Look Like Maze?, September 2024. URL http://arxiv.org/abs/2409. 08202. arXiv:2409.08202. Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao, Qi Yang, and Changshui Zhang. Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild, January 2025. URL http://arxiv.org/abs/2501.02964. arXiv:2501.02964. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models, April 2024. URL http: //arxiv.org/abs/2312.03052. arXiv:2312.03052. Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A. Ross, Cordelia Schmid, and Alireza Fathi. AVIS: Autonomous Visual Information Seeking with Large Language Model Agent, November 2023. URL http://arxiv.org/abs/2306.08129. arXiv:2306.08129. Irene Huang, Wei Lin, M. Jehanzeb Mirza, Jacob A. Hansen, Sivan Doveh, Victor Ion Butoi, Roei Herzig, Assaf Arbelle, Hilde Kuehne, Trevor Darrell, Chuang Gan, Aude Oliva, Rogerio Feris, and Leonid Karlinsky. ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs, November 2024a. URL http://arxiv.org/abs/2406.08164. arXiv:2406.08164. 12 Preprint. Under review. Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid, October 2024b. URL http://arxiv.org/abs/2408.02034. arXiv:2408.02034. Mingzhen Huang, Shan Jia, Zhou Zhou, Yan Ju, Jialing Cai, and Siwei Lyu. Exposing Text-Image Inconsistency Using Diffusion Models, April 2024c. URL http://arxiv.org/ abs/2404.18033. arXiv:2404.18033. Zhijian Huang, Tao Tang, Shaoxiang Chen, Sihao Lin, Zequn Jie, Lin Ma, Guangrun Wang, and Xiaodan Liang. Making Large Language Models Better Planners with Reasoning-Decision Alignment, August 2024d. URL http://arxiv.org/abs/2408.13890. arXiv:2408.13890. Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, and Peilin Zhao. Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models, October 2024. URL http://arxiv.org/abs/2408.02032. arXiv:2408.02032. Mohamed Fazli Imam, Chenyang Lyu, and Alham Fikri Aji. Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!, January 2025. URL http://arxiv.org/abs/2501.10674. arXiv:2501.10674. Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen. From Training-Free to Adaptive: Empirical Insights into MLLMs Understanding of Detection Information, December 2024. URL http://arxiv.org/abs/2401.17981. arXiv:2401.17981. Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats up with vision-language models? Investigating their struggle with spatial reasoning, October 2023. URL http://arxiv. org/abs/2310.19785. arXiv:2310.19785. Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, and Ahmed Qureshi. ReMI: Dataset for Reasoning with Multiple Images, June 2024. URL http://arxiv.org/ abs/2406.09175. arXiv:2406.09175. Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, and Hamid Rezatofighi. HYDRA: Hyper Agent for Dynamic Compositional Visual Reasoning, July 2024. URL http://arxiv.org/abs/2403.12884. arXiv:2403.12884. Changyeon Kim, Younggyo Seo, Hao Liu, Lisa Lee, Jinwoo Shin, Honglak Lee, and Kimin Lee. Guide Your Agent with Adaptive Multimodal Rewards, October 2023. URL http: //arxiv.org/abs/2309.10790. arXiv:2309.10790. Emily Lai. Metacognition: literature review. 2011. Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision, April 2024. URL http://arxiv.org/abs/2311.07362. arXiv:2311.07362. Andrew Li, Rahul Thapa, Rahul Chalamala, Qingyang Wu, Kezhen Chen, and James Zou. SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning, January 2025a. URL http://arxiv.org/abs/2501.03675. arXiv:2501.03675. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while Reasoning in Space: Multimodal Visualization-of-Thought, January 2025b. URL http://arxiv.org/abs/2501.07542. arXiv:2501.07542. Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, and Jianfeng Gao. Visual In-Context Prompting, November 2023a. URL http://arxiv.org/abs/2311.13601. arXiv:2311.13601. Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wankou Yang, Yabiao Wang, and Chengjie Wang. Survey on Benchmarks of Multimodal Large Language Models, September 2024a. URL http://arxiv.org/abs/2408.08632. arXiv:2408.08632. 13 Preprint. Under review. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference Distillation for Large Visual Language Models, December 2023b. URL http://arxiv.org/abs/2312.10665. arXiv:2312.10665. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. VLRewardBench: Challenging Benchmark for Vision-Language Generative Reward Models, November 2024b. URL http://arxiv.org/abs/2411.17451. arXiv:2411.17451. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. VLFeedback: Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment, October 2024c. URL http://arxiv.org/abs/ 2410.09421. arXiv:2410.09421. Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash Harandi, Yunde Jia, Song-Chun Zhu, and Qing Li. FIRE: Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models, December 2024d. URL http://arxiv.org/abs/2407. 11522. arXiv:2407.11522. Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. VideoVista: Versatile Benchmark for Video Understanding and Reasoning, June 2024e. URL http://arxiv.org/abs/2406.11303. arXiv:2406.11303. Jiafeng Liang, Shixin Jiang, Zekun Wang, Haojie Pan, Zerui Chen, Zheng Chu, Ming Liu, Ruiji Fu, Zhongyuan Wang, and Bing Qin. GUIDE: Guideline-Guided Dataset for Instructional Video Comprehension, June 2024a. URL http://arxiv.org/abs/2406.18227. arXiv:2406.18227. Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, and Dongyan Zhao. ReasVQA: Advancing VideoQA with Imperfect Reasoning Process, January 2025. URL http://arxiv.org/abs/2501.13536. arXiv:2501.13536. Lili Liang, Guanglu Sun, Jin Qiu, and Lizhong Zhang. Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering, April 2024b. URL http://arxiv.org/abs/2404.04007. arXiv:2404.04007. Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?, April 2024a. URL http://arxiv. org/abs/2404.06510. arXiv:2404.06510. Zhaohe Liao, Jiangtong Li, Li Niu, and Liqing Zhang. Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video QuestionAnswering, July 2024b. URL http://arxiv.org/abs/2407.03008. arXiv:2407.03008. Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, and Lu Yuan. Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge, July 2024. URL http://arxiv.org/abs/2407.04681. arXiv:2407.04681. Huabin Liu, Filip Ilievski, and Cees G. M. Snoek. Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning, January 2025a. URL http://arxiv. org/abs/2501.05069. arXiv:2501.05069. Junzhuo Liu, Xuzheng Yang, Weiwei Li, and Peng Wang. FineCops-Ref: new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension, January 2025b. URL http://arxiv.org/abs/2409.14750. arXiv:2409.14750. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-RFT: Visual Reinforcement Fine-Tuning, March 2025c. URL http://arxiv.org/abs/2503.01785. arXiv:2503.01785. Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens, December 2023. URL http://arxiv.org/abs/2312.08870. arXiv:2312.08870. Preprint. Under review. Mingjie Ma, Zhihuan Yu, Yichao Ma, and Guohui Li. EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning, April 2024a. URL http://arxiv.org/abs/2404.13847. arXiv:2404.13847. Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. m&ms: Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks, September 2024b. URL http://arxiv.org/abs/2403.11085. arXiv:2403.11085. Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, and Fahad Shahbaz Khan. VURF: General-purpose Reasoning and Self-refinement Framework for Video Understanding, March 2024. URL http://arxiv.org/abs/2403.14743. arXiv:2403.14743. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning, March 2025. URL http://arxiv.org/abs/2503. 07365. arXiv:2503.07365. Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, and Cordelia Schmid. MoReVQA: Exploring Modular Reasoning Models for Video Question Answering, April 2024. URL http://arxiv.org/abs/2404.06511. arXiv:2404.06511. Tushar Nagarajan and Lorenzo Torresani. Step Differences in Instructional Video, June 2024. URL http://arxiv.org/abs/2404.16222. arXiv:2404.16222. Minheng Ni, Yutao Fan, Lei Zhang, and Wangmeng Zuo. Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning, October 2024. URL http://arxiv.org/abs/2410.03321. arXiv:2410.03321. Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. CLIP-DPO: Vision-Language Models as Source of Preference for Fixing Hallucinations in LVLMs, August 2024. URL http://arxiv.org/abs/2408.10433. arXiv:2408.10433. Fred Paas, Alexander Renkl, and John Sweller. Cognitive load theory: Instructional implications of the interaction between information structures and cognitive architecture. Instructional science, 32(1/2):18, 2004. Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?, January 2025. URL http://arxiv.org/abs/2501.02669. arXiv:2501.02669. Paritosh Parmar, Eric Peh, Ruirui Chen, Ting En Lam, Yuhan Chen, Elston Tan, and Basura Fernando. CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes, June 2024. URL http://arxiv.org/abs/2404.01299. arXiv:2404.01299. Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. PerceptionGPT: Effectively Fusing Visual Perception into LLM, November 2023. URL http://arxiv.org/abs/2311. 06612. arXiv:2311.06612. Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, and Amjad Almahairi. Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model, June 2024. URL http://arxiv.org/abs/2312.12423. arXiv:2312.12423. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, and Jie Tang. CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations, May 2024. URL http://arxiv.org/ abs/2402.04236. arXiv:2402.04236. Xiaoye Qu, Jiashuo Sun, Wei Wei, and Yu Cheng. Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning, August 2024. URL http://arxiv.org/abs/2408.17150. arXiv:2408.17150. Preprint. Under review. Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, and Tsung-Yu Lin. Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs, April 2024. URL http://arxiv.org/abs/2404.07449. arXiv:2404.07449. Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, and Srijan Das. LLAVIDAL: Large LAnguage VIsion Model for Daily Activities of Living, December 2024. URL http://arxiv.org/abs/ 2406.09390. arXiv:2406.09390. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual CoT: Advancing Multi-Modal Language Models with Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning, November 2024. URL http://arxiv.org/abs/2403.16999. arXiv:2403.16999. Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, and Byoung-Tak Zhang. Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following, April 2024. URL http://arxiv.org/abs/2404.15190. arXiv:2404.15190. Michal Shlapentokh-Rothman, Yu-Xiong Wang, and Derek Hoiem. Can We Generate Visual Programs Without Prompting LLMs?, December 2024. URL http://arxiv.org/abs/2412. 08564. arXiv:2412.08564. Hung-Ting Su, Chun-Tong Chao, Ya-Ching Hsu, Xudong Lin, Yulei Niu, Hung-Yi Lee, Investigating Video Reasoning Capability of Large Language and Winston H. Hsu. Models with Tropes in Movies, June 2024. URL http://arxiv.org/abs/2406.10923. arXiv:2406.10923. Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal Latent Language Modeling with Next-Token Diffusion, December 2024. URL http://arxiv.org/abs/2412.08635. arXiv:2412.08635. John Sweller. Cognitive load theory. In Psychology of learning and motivation, volume 55, pp. 3776. Elsevier, 2011. Sirnam Swetha, Hilde Kuehne, and Mubarak Shah. TimeLogic: Temporal Logic Benchmark for Video QA, January 2025. URL http://arxiv.org/abs/2501.07214. arXiv:2501.07214. Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, and Andrea Bajcsy. What Matters to You? Towards Visual Representation Alignment for Robot Learning, January 2024. URL http://arxiv.org/abs/2310.07932. arXiv:2310.07932. Janneke Van de Pol, Monique Volman, and Jos Beishuizen. Scaffolding in teacherstudent interaction: decade of research. Educational psychology review, 22:271296, 2010. Kurt VanLehn, Randolph Jones, and Michelene TH Chi. model of the self-explanation effect. The journal of the learning sciences, 2(1):159, 1992. Lev Vygotsky. Mind in society: The development of higher psychological processes, volume 86. Harvard university press, 1978. Joshua R. Waite, Md Zahid Hasan, Qisai Liu, Zhanhong Jiang, Chinmay Hegde, and Soumik Sarkar. RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception, January 2025. URL http://arxiv.org/abs/2501.18880. arXiv:2501.18880. Sandra Waite-Stupiansky. Jean piagets constructivist theory of learning. In Theories of early childhood education, pp. 318. Routledge, 2022. Dongsheng Wang, Jiequan Cui, Miaoge Li, Wang Lin, Bo Chen, and Hanwang Zhang. Instruction Tuning-free Visual Token Complement for Multimodal LLMs, August 2024a. URL http://arxiv.org/abs/2408.05019. arXiv:2408.05019. 16 Preprint. Under review. Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, and Liwei Wang. GiT: Towards Generalist Vision Transformer through Universal Language Interface, March 2024b. URL http://arxiv.org/abs/2403.09394. arXiv:2403.09394. Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive Visual Instruction Tuning, December 2024c. URL http://arxiv.org/abs/2410.09575. arXiv:2410.09575. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. CogVLM: Visual Expert for Pretrained Language Models, February 2024d. URL http://arxiv.org/abs/2311.03079. arXiv:2311.03079. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. Divide, Conquer and Combine: Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models, August 2024e. URL http: //arxiv.org/abs/2408.15556. arXiv:2408.15556. Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension, December 2024f. URL http://arxiv. org/abs/2412.03704. arXiv:2412.03704. Yongxin Wang, Meng Cao, Haokun Lin, Mingfei Han, Liang Ma, Jin Jiang, Yuhao Cheng, and Xiaodan Liang. EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation, December 2024g. URL http://arxiv.org/abs/2412.04903. arXiv:2412.04903. Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, and Yanfeng Wang. MMSAP: Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception, June 2024h. URL http://arxiv.org/abs/2401.07529. arXiv:2401.07529. Zhecan Wang, Junzhang Liu, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar, Rui Sun, Wenhao Li, Md Atabuzzaman, Hammad Ayyubi, Haoxuan You, Alvi Ishmam, Kai-Wei Chang, Shih-Fu Chang, and Chris Thomas. JourneyBench: Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images, January 2025. URL http://arxiv.org/abs/2409.12953. arXiv:2409.12953. Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos, October 2024i. URL http://arxiv.org/abs/2405.19209. arXiv:2405.19209. Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang, Yuzhuang Xu, Xiaolong Wang, Peng Li, and Yang Liu. ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models, October 2024j. URL http://arxiv.org/abs/2410.04659. arXiv:2410.04659. Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, and Deheng Ye. GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training, March 2025. URL http://arxiv.org/abs/2503.08525. arXiv:2503.08525. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229256, 1992. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, and Chuang Gan. STAR: Benchmark for Situated Reasoning in Real-World Videos, May 2024a. URL http: //arxiv.org/abs/2405.09711. arXiv:2405.09711. Penghao Wu and Saining Xie. V*: Guided Visual Search as Core Mechanism in Multimodal LLMs, December 2023. URL http://arxiv.org/abs/2312.14135. arXiv:2312.14135. 17 Preprint. Under review. Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang, and Nanyun Peng. VISCO: Benchmarking Fine-Grained Critique and Correction Towards SelfImprovement in Visual Reasoning, December 2024b. URL http://arxiv.org/abs/2412. 02172. arXiv:2412.02172. Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Yu-Jie Yuan, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaodan Liang. Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?, March 2025. URL http://arxiv. org/abs/2503.06252. arXiv:2503.06252. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts, July 2024. URL http://arxiv.org/abs/2407. 04973. arXiv:2407.04973. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. LLaVA-Critic: Learning to Evaluate Multimodal Models, October 2024. URL http://arxiv.org/abs/2410.02712. arXiv:2410.02712. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. LLaVA-CoT: Let Vision Language Models Reason Step-by-Step, January 2025. URL http://arxiv.org/abs/2411. 10440. arXiv:2411.10440. Zihui Xue, Joungbin An, Xitong Yang, and Kristen Grauman. Progress-Aware Video Frame Captioning, December 2024. URL http://arxiv.org/abs/2412.02071. arXiv:2412.02071. Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li. ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling, October 2024. URL http://arxiv.org/abs/2402.06118. arXiv:2402.06118. Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model, January 2024a. URL http://arxiv.org/abs/2312.17240. arXiv:2312.17240. Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, and Chuang Gan. VCA: Video Curious Agent for Long Video Understanding, December 2024b. URL http://arxiv.org/ abs/2412.10471. arXiv:2412.10471. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and Dacheng Tao. Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search, December 2024. URL http://arxiv.org/abs/2412.18319. arXiv:2412.18319. Yuanyang Yin, Yaqi Zhao, Yajie Zhang, Ke Lin, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, and Wentao Zhang. SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs, August 2024. URL http://arxiv.org/abs/2408. 11813. arXiv:2408.11813. Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, ChungChing Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. MM-Vet v2: Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities, December 2024. URL http://arxiv.org/abs/2408.00765. arXiv:2408.00765. Zihao Yue, Liang Zhang, and Qin Jin. Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective, May 2024. URL http://arxiv.org/abs/2402.14545. arXiv:2402.14545. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, and Jiaqi Wang. InternLM-XComposer2.5-Reward: Simple Yet Effective Multi-Modal Reward Model, January 2025. URL http://arxiv.org/abs/2501.12368. arXiv:2501.12368. 18 Preprint. Under review. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. STaR: Bootstrapping Reasoning With Reasoning, May 2022. URL http://arxiv.org/abs/2203.14465. arXiv:2203.14465. Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring, March 2024. URL http://arxiv.org/abs/2403.09333. arXiv:2403.09333. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning, November 2024a. URL http://arxiv.org/abs/2410.02884. arXiv:2410.02884. Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, and Jindong Gu. Visual Question Decomposition on Multimodal Large Language Models, October 2024b. URL http://arxiv.org/abs/2409.19339. arXiv:2409.19339. Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee. CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples, June 2024c. URL http://arxiv.org/abs/2402.13254. arXiv:2402.13254. Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, and Maosong Sun. Exploring Perceptual Limitation of Multimodal Large Language Models, February 2024d. URL http://arxiv.org/abs/2402.07384. arXiv:2402.07384. Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, Silvio Savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, and Ran Xu. ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models, December 2024e. URL http://arxiv.org/abs/2412.07012. arXiv:2412.07012. Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, and Feng Zheng. Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models, July 2024f. URL http://arxiv.org/abs/2407.11422. arXiv:2407.11422. Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, and Bingchen Zhao. What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models, April 2024g. URL http://arxiv.org/abs/2310.06627. arXiv:2310.06627. Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and Kyusong Lee. OmAgent: Multimodal Agent Framework for Complex Video Understanding with Task Divide-andConquer, November 2024h. URL http://arxiv.org/abs/2406.16620. arXiv:2406.16620. Miaosen Zhang, Qi Dai, Yifan Yang, Jianmin Bao, Dongdong Chen, Kai Qiu, Chong Luo, Xin Geng, and Baining Guo. MageBench: Bridging Large Multimodal Models to Agents, December 2024i. URL http://arxiv.org/abs/2412.04531. arXiv:2412.04531. Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. InternLM-XComposer: Vision-Language Large Model for Advanced Text-image Comprehension and Composition, December 2023. URL http: //arxiv.org/abs/2309.15112. arXiv:2309.15112. Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward, April 2024j. URL http://arxiv.org/abs/2404.01258. arXiv:2404.01258. 19 Preprint. Under review. Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve Vision Language Model Chain-of-thought Reasoning, October 2024k. URL http://arxiv.org/abs/2410.16198. arXiv:2410.16198. Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, Anton van den Hengel, and Yuan Xue. Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs, January 2025. URL http://arxiv.org/abs/ 2501.06430. arXiv:2501.06430. Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, and Ziwei Liu. WorldQA: Multimodal World Knowledge in Videos through LongChain Reasoning, May 2024l. URL http://arxiv.org/abs/2405.03272. arXiv:2405.03272. Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion Mamba: Efficient and Long Sequence Motion Generation, August 2024m. URL http: //arxiv.org/abs/2403.07487. arXiv:2403.07487. Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning, December 2023. URL http://arxiv.org/abs/2312.11420. arXiv:2312.11420. Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning, March 2025. URL http://arxiv.org/abs/ 2503.05379. arXiv:2503.05379. Yuzhong Zhao, Feng Liu, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, and Fang Wan. DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution, May 2024. URL http://arxiv.org/abs/2405.16071. arXiv:2405.16071. Haojie Zheng, Tianyang Xu, Hanchi Sun, Shu Pu, Ruoxi Chen, and Lichao Sun. Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination, November 2024. URL http://arxiv.org/abs/2411.12591. arXiv:2411.12591. Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. VL-GPT: Generative Pre-trained Transformer for Vision and Language Understanding and Generation, December 2023. URL http://arxiv.org/abs/ 2312.09251. arXiv:2312.09251. Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, and Min Zhang. Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning, December 2024. URL http://arxiv.org/abs/2412.13540. arXiv:2412.13540. Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, and Serena Yeung-Levy. VideoSTaR: Self-Training Enables Video Instruction Tuning with Any Supervision, July 2024. URL http://arxiv.org/abs/2407.06189. arXiv:2407.06189. Xin Zou, Yizhou Wang, Yibo Yan, Sirui Huang, Kening Zheng, Junkai Chen, Chang Tang, and Xuming Hu. Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models, October 2024a. URL http://arxiv.org/abs/2410.03577. arXiv:2410.03577. Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Junyi Wei, Zhengyuan Yang, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, and Lijuan Wang. Interfacing Foundation Models Embeddings, July 2024b. URL http://arxiv.org/abs/ 2312.07532. arXiv:2312.07532. 20 Preprint. Under review."
        },
        {
            "title": "A Implications from the Learning Science Perspective",
            "content": "The design of CoT reasoning for LLMs aligns with several key cognitive science and learning science theories that enhance structured reasoning, problem-solving, and knowledge construction. One key theory is Cognitive Load Theory (Sweller, 2011; Paas et al., 2004), which reduces humans working memory workload by breaking complex problems into manageable steps. CoT reasoning follows this approach by prompting models to generate step-by-step solutions rather than arriving at an answer in one leap. This structured decomposition mirrors how human learners handle intricate problems by offloading cognitive effort across multiple processing stages. Similarly, in the context of learning, scaffolding refers to offering temporary assistance to learners, enabling them to accomplish tasks that would be difficult or impossible for them to complete independently. This support is gradually removed as learners develop the skills and confidence required to perform the task on their own (Van de Pol et al., 2010). This supports CoTs design by guiding LLMs through intermediate steps, much like how educators provide structured support to help students grasp new concepts before transitioning to independent problem-solving. These theories are both rooted in the Zone of Proximal Development (ZPD), key construct in Lev Vygotskys theory of learning and development (Vygotsky, 1978). CoT reasoning also aligns with metacognition (Lai, 2011) and self-explanation (Bisra et al., 2018; Chi et al., 1994) by encouraging models to think about their thinking. Just as self-explanation improves human learning by prompting individuals to articulate their reasoning (Chi et al., 1994; VanLehn et al., 1992), CoT forces LLMs to justify their steps, reducing reliance on shallow heuristics. This ties into Dual-Process Theory (Frankish, 2010), where CoT shifts LLMs from fast, intuitive decision-making (System 1) to deliberate, analytical reasoning (System 2), leading to more logical and consistent outputs. Moreover, CoT fosters constructivist learning (Waite-Stupiansky, 2022; Bruner, 2009) by enabling LLMs to incrementally build knowledge structures through active reasoning. Instead of passively retrieving answers from training data, the model synthesizes prior knowledge with new information, improving adaptability. Additionally, analogical reasoning (Gentner & Maravilla, 2017) plays role in CoT by helping LLMs map relationships between concepts, allowing them to generalize problem-solving strategies across different contexts. By integrating these cognitive science and learning science principles, CoT reasoning enhances the interpretability, reliability, and generalization of LLM outputs, making them more aligned with human cognitive processes and educational best practices. Preprint. Under review. Adaptive Inference Reward Models Feedback Guo et al. (2025), Hu et al. (2025), Yang et al. (2024b), Xue et al. (2024), Wu et al. (2024b), Wang et al. (2024f), Zou et al. (2024a), Qu et al. (2024), Zhang et al. (2024k), Huo et al. (2024), Liao et al. (2024b), Su et al. (2024), Li et al. (2024e), Wang et al. (2024e), Xu et al. (2025), Zhao et al. (2024), Liao et al. (2024a), Mahmood et al. (2024), Cao et al. (2024b), Wu et al. (2024a), Yang et al. (2024a), Li et al. (2023a), Pi et al. (2023) Guo et al. (2025), Hao et al. (2025), Li et al. (2024b), Fu et al. (2024c), Ke et al. (2024), Xiong et al. (2024), Cascante-Bonilla et al. (2024), Cheng et al. (2024b), Zhang et al. (2024k), Guo et al. (2025), Wu et al. (2024b), Hu et al. (2025), Meng et al. (2025), Li et al. (2024d), Xiong et al. (2024), Cheng et al. (2024b), Zhang et al. (2024k), Zheng et al. (2024), Liao et al. (2024a), Han et al. (2024), Wang et al. (2024i), Shao et al. (2024), Liang et al. (2024b), Lee et al. (2024), Hu et al. (2024), Inference Multimodal Decomposition Xiang et al. (2025), Liang et al. (2025), Liu et al. (2025a), Shlapentokh-Rothman et al. (2024), Zhang et al. (2024b), Chen et al. (2024f), Hsu et al. (2024), Su et al. (2024), Reilly et al. (2024), Wang et al. (2024a), Zhang et al. (2024h), Min et al. (2024), Parmar et al. (2024), Wang et al. (2024b), Dzabraev et al. (2024), Wu et al. (2024a), Wu & Xie (2023), Zhang et al. (2023), Efficiency and Scalability Goyal et al. (2025), Li et al. (2025b), Sun et al. (2024), Fan et al. (2024), Park et al. (2025), Wang et al. (2024e), Wang et al. (2025), Huo et al. (2024), Lin et al. (2024), Ni et al. (2024), Wang et al. (2024i), Huang et al. (2024a), Zhang et al. (2024m), Zhan et al. (2024), Chen et al. (2024b), Wang et al. (2024b), Zhao et al. (2023), Pi et al. (2023), Chen et al. (2024a) Search Strategies Yao et al. (2024), Yang et al. (2024b), Wang et al. (2024f), Imam et al. (2025), Xu et al. (2025), Qu et al. (2024), Wang et al. (2024e), Li et al. (2024e), Kazemi et al. (2024), Shin et al. (2024), Zhang et al. (2024l), Zhao et al. (2024), Fu et al. (2024c), Mahmood et al. (2024), Wu & Xie (2023), Hu et al. (2023), Figure 4: Comprehensive Overview of Methods and Frameworks focus on test-time compute 22 Preprint. Under review. Policy Optimization Fu et al. (2024c), Zhang et al. (2024d), Hu et al. (2024), Tian et al. (2024), Kim et al. (2023), Yan et al. (2024), Ouali et al. (2024), Xiong et al. (2024), Wang et al. (2024j), Waite et al. (2025), Wei et al. (2025), Zhao et al. (2025), Xu et al. (2025), Deng et al. (2024), Model Architecture Reilly et al. (2024), Ma et al. (2024a), Zhang et al. (2024j), Zhan et al. (2024), Jiao et al. (2024), He et al. (2024), Ma et al. (2023), Ben-Kish et al. (2024), Hu et al. (2024), Wang et al. (2024d), Wang et al. (2024c), Cao et al. (2024a), Chen et al. (2024f), Huang et al. (2024d), Post Training Reward Alignment Hao et al. (2025), Li et al. (2024b), Xiong et al. (2024), Ke et al. (2024), Fu et al. (2024c), Dzabraev et al. (2024), Hu et al. (2024), Iterative Refinement Meng et al. (2025), Zheng et al. (2024), Liao et al. (2024a), Wu et al. (2024b), Shao et al. (2024), Han et al. (2024), Hu et al. (2024), Lee et al. (2024), Spatial Temporal Modeling Dataset Curation Wang et al. (2024i), Zhang et al. (2024m), Chen et al. (2024a), Wu & Xie (2023), Fan et al. (2024), Wang et al. (2025), Nagarajan & Torresani (2024), Zhang et al. (2024l), Lin et al. (2024), Zhao et al. (2023), Hu et al. (2023), Figure 5: Comprehensive Overview of Methods and Frameworks focus on post-training improvement"
        }
    ],
    "affiliations": [
        "Corning Inc.",
        "University of Central Florida",
        "University of Rochester"
    ]
}