{
    "paper_title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
    "authors": [
        "Cheng-De Fan",
        "Chen-Wei Chang",
        "Yi-Ruei Liu",
        "Jie-Ying Lee",
        "Jiun-Long Huang",
        "Yu-Chee Tseng",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes."
        },
        {
            "title": "Start",
            "content": "SPECTROMOTION: DYNAMIC 3D RECONSTRUCTION OF SPECULAR SCENES Cheng-De Fan1, Chen-Wei Chang1, Yi-Ruei Liu2, Jie-Ying Lee1, Jiun-Long Huang1, Yu-Chee Tseng1, Yu-Lun Liu1 1National Yang Ming Chiao Tung University, 2University of Illinois Urbana-Champaign https://cdfan0627.github.io/spectromotion/"
        },
        {
            "title": "ABSTRACT",
            "content": "We present SpectroMotion, novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing residual correction technique for accurate surface normal computation during deformation, complemented by deformable environment map that adapts to time-varying lighting conditions. We implement coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes."
        },
        {
            "title": "INTRODUCTION",
            "content": "3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has emerged as groundbreaking technique in 3D scene reconstruction, offering fast training and real-time rendering capabilities. By representing 3D space using collection of 3D Gaussians and employing point-based rendering approach, 3DGS has significantly improved efficiency in novel-view synthesis. However, extending 3DGS to accurately model dynamic scenes, especially those containing specular surfaces, has remained significant challenge. Existing extensions of 3DGS have made progress in either dynamic scene reconstruction or specular object rendering, but none have successfully combined both aspects. Methods tackling dynamic scenes often struggle with accurate representation of specular surfaces, while those focusing on specular rendering are limited to static scenes. This gap in capabilities has hindered the application of 3DGS to real-world scenarios where both motion and specular reflections are present. 4 2 0 2 2 2 ] . [ 1 9 4 2 7 1 . 0 1 4 2 : r Figure 1: Our method, SpectroMotion, recovers and renders dynamic scenes with higherquality reflections compared to prior work. It introduces physical normal estimation, deformable environment maps, and coarse-to-fine training strategy to achieve superior results in rendering dynamic scenes with reflections. Here we present rendered test image along with its corresponding normal maps and ground-truth image. For Deformable 3DGS, we use the shortest axes of the deformed 3D Gaussians as the normals. We have highlighted the specular regions for scene from the NeRF-DS dataset (Yan et al., 2023) to demonstrate the effectiveness of our approach. 1 We present SpectroMotion, novel approach that addresses these limitations by combining 3D Gaussian Splatting with physically-based rendering (PBR) and deformation fields. Our method introduces three key innovations: residual correction technique for accurate surface normal computation during deformation, deformable environment map that adapts to time-varying lighting conditions, and coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. Our evaluations demonstrate that SpectroMotion outperforms prior methods in view synthesis of scenes containing dynamic specular objects, as illustrated in Figure 1. It is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, surpassing state-ofthe-art techniques in rendering complex, dynamic, and specular content. This advancement represents significant leap in 3D scene reconstruction, particularly for challenging scenarios involving moving specular objects. In summary, we make the following contributions: We propose SpectroMotion, physically-based rendering (PBR) approach combining deformation fields and 3D Gaussian Splatting for real-world dynamic specular scenes. We introduce residual correction method for accurate surface normals during deformation, coupled with deformable environment map to handle time-varying lighting conditions in dynamic scenes. We develop coarse-to-fine training strategy enhancing scene geometry and specular color prediction, outperforming state-of-the-art methods."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Dynamic Scene Reconstruction. Recent works have leveraged NeRF representations to jointly solve for canonical space and deformation fields in dynamic scenes using RGB supervision (Guo et al., 2023; Li et al., 2021; Park et al., 2021a;b; Pumarola et al., 2020; Tretschk et al., 2021; Xian et al., 2021; Liu et al., 2023a). Further advancements in dynamic neural rendering include object segmentation (Song et al., 2023), incorporation of depth information (Attal et al., 2021), utilization of 2D CNNs for scene priors (Lin et al., 2022; Peng et al., 2023), and multi-view video compression (Li et al., 2022). However, these NeRF-based methods are computationally intensive. To address this, recent research has adapted 3D Gaussians for dynamic scenes (Yang et al., 2023c; Wu et al., 2023; Huang et al., 2024; Liang et al., 2023c; Wang et al., 2024; Mihajlovic et al., 2024; Stearns et al., 2024), primarily focusing on deforming spatial coordinates through deformation fields. Nevertheless, these approaches do not explicitly account for changes in object surface during the deformation process. Our work extends this line of research by combining specular object rendering based on normal estimation with deformation field, enabling each 3D Gaussian to effectively model dynamic specular scenes. Reflective Object Rendering. While significant progress has been made in rendering reflective objects, challenges arising from complex light interactions persist. Recent years have seen numerous studies addressing these issues, primarily by decomposing appearance into lighting and material properties (Bi et al., 2020; Boss et al., 2021; Li & Li, 2022; Srinivasan et al., 2020; Zhang et al., 2021b; Munkberg et al., 2022; Zhang et al., 2021a; Verbin et al., 2024a; Zhao et al., 2024). Building on this foundation, some research has focused on improving the capture and reproduction of specular reflections (Verbin et al., 2022; Ma et al., 2023; Verbin et al., 2024b), while others have leveraged signed distance functions (SDFs) to enhance normal estimation (Ge et al., 2023; Liang et al., 2023a;b; Liu et al., 2023b; Zhang et al., 2023). The emergence of 3D Gaussian Splatting (3DGS) has sparked new wave of techniques (Jiang et al., 2023; Liang et al., 2023d; Yang et al., 2024; Ye et al., 2024; Zhu et al., 2024; Shi et al., 2023) that integrate Gaussian splatting with physically-based rendering. Nevertheless, accurately modeling dynamic environments and time-varying specular reflections remains significant challenge. To address this limitation, our work introduces novel approach incorporating deformable environment map and additional explicit Gaussian attributes, specifically designed to capture specular color changes over time. 2 Figure 2: Method Overview. Our method stabilizes the scene geometry through three stages. In the static stage, we stabilize the geometry of the static scene by minimizing photometric loss Lcolor between vanilla 3DGS renders and ground truth images. The dynamic stage combines canonical 3D Gaussians with deformable Gaussian MLP to model dynamic scenes while simultaneously minimizing normal loss Lnormal between rendered normal map Nt and gradient normal map from depth map Dt, thus further enhancing the overall scene geometry. Finally, the specular stage introduces deformable reflection MLP to handle changing environment lighting, deforming reflection directions ωt to query canonical environment map for specular color ct s. It is then combined with diffuse color cd (using zero-order spherical harmonics) and learnable specular tint stint per 3D Gaussian to obtain the final color ct final. This approach enables the modeling of dynamic specular scenes and high-quality novel view rendering."
        },
        {
            "title": "3 METHOD",
            "content": "Overview of the approach. The overview of our method is illustrated in Fig. 2. Given an input monocular video sequence of frames and corresponding camera poses, we design three-stage approach to reconstruct the dynamic specular scene, as detailed in Section 3.2. Accurate specular reflection requires precise normal estimation, so Section 3.3 elaborates on how we estimate normals in dynamic scenes. Finally, we introduce the losses used throughout the training process in Section 3.4. 3.1 PRELIMINARY 3D Gaussian Splatting. Each 3D Gaussian is defined by center position R3 and covariance matrix Σ. 3D Gaussian Splatting (Kerbl et al., 2023) optimizes the covariance matrix using scaling factors R3 and rotation unit quaternion R4. For novel-view rendering, 3D Gaussians are projected onto 2D camera planes using differentiable splatting (Yifan et al., 2019): Pixel colors are computed using point-based volumetric rendering: Σ=JWΣWT JT . = (cid:88) iN Tiαici, αi = σie 1 2 (x)T Σ(x), (1) (2) where Ti = (cid:81)i1 Gaussian. j=1(1 αj) is the transmittance, σi is the opacity, and ci is the color of each 3D"
        },
        {
            "title": "3.2 SPECULAR RENDERING",
            "content": "Limitations of existing methods. The current Dynamic 3DGS-based methods (Wu et al., 2023; Yang et al., 2023c;b) encounter limitations in accurately modeling environments that include specular objects. This issue arises from the inherent low-frequency characteristics of low-order spherical harmonics (SH), which are inadequate for capturing complex visual effects such as specular highlights. In contrast, other specialized 3DGS-based methods for static specular object scenes (Jiang et al., 2023; Liang et al., 2023d) often incorporate environment maps to model lighting, which is then combined with BRDF to simulate the entire scene. However, vanilla environment maps are not suitable for modeling lighting scenarios that involve time-variant elements. This results in the existing 3DGS-based methods being insufficient for effectively modeling dynamic specular object scenes. Proposed solution overview. To address these challenges, we introduce physical normal estimation (Section 3.3) and deformable environment maps to model the specular color of real-world dynamic scenes. However, this approach alone is insufficient, as precise scene geometry is crucial for accurate reflections. Therefore, we introduce our coarse-to-fine training strategy, which helps stabilize scene geometry while simultaneously predicting accurate specular color. Our coarse-to-fine training strategy is divided into three stages: the static stage, the dynamic stage, and the specular stage. In the following paragraphs, we will introduce each of these stages in detail. 3.2.1 COARSE-TO-FINE TRAINING STRATEGY Static stage. In the static stage, we employ vanilla 3DGS (Kerbl et al., 2023) for static scene reconstruction to stabilize the geometry of the static scene. Specifically, we optimize the position x, scaling s, rotation r, opacity α, and coefficients of spherical harmonics (SH) of the 3D Gaussians by minimizing the photometric loss Lcolor between the rendered image and the corresponding image: Lcolor = (1 λD-SSIM)L1 + λD-SSIMLD-SSIM. (3) Dynamic stage. Following the static stage, we address dynamic objects using Deformable 3DGS (Yang et al., 2023c). For each 3D Gaussian in canonical 3D Gaussians G, we input its position and time into deformable Gaussian MLP with parameters θG to predict position, rotation, and scaling residuals: (xt, rt, st) = FθG (γ(x), γ(t)), where γ denotes positional encoding. Attributes of the corresponding 3D Gaussian in deformed 3D Gaussians Gt at time is obtained by: (4) (xt, rt, st) = (xt, rt, st) + (x, r, s). (5) This approach separates motion and geometric structural learning, allowing accurate simulation of dynamic behaviors while maintaining stable geometric reference. To further enhance scene geometry, we estimate normals of deformed 3D Gaussians and optimize them using: Lnormal = 1 Nt ˆNt, (6) where Nt is the rendered normal map and ˆNt is the normal map derived from the rendered depth map Dt. This process improves local associations among 3D Gaussians and optimizes both depth and normal information across the entire scene. Specular stage. We adopt an image-based lighting (IBL) model with learnable cube map. Inspired by the rendering equation (Kajiya, 1986), split-sum approximation (Karis & Games, 2013; Munkberg et al., 2022), and Cook-Torrance reflectance model (Cook & Torrance, 1982), we formulate the outgoing radiance of the specular component Ls as: Ls = (cid:90) DGF 4(ωt nt)(ωi nt) Ω (ωi nt)dωi (cid:90) Ω Li(ωi)D(ωi, ωt o)(ωi nt)dωi, (7) where Ω is the hemisphere around the surface normal nt. D, G, and represent the GGX normal distribution function (Walter et al., 2007), geometric attenuation, and Fresnel term, respectively. ωt is the view direction, and Li(ωi) is the incident radiance. The first term, representing the specular BSDF with solid white environment light, is precomputed and stored in look-up table. The second term is 4 pre-integrated in filtered cubemap, where each mip-level corresponds to specific roughness value. Roughness ρ [0, 1] is learnable parameter for each 3D Gaussian. After the static and dynamic stages, the geometry is well-defined. This allows us to accurately calculate reflection directions ωt r: (8) Reflection directions can query the environment map for the specular color of static environment light. To handle time-varying lighting in dynamic scenes, we introduce deformable environment map, detailed in the following section. nt)nt ωt o. = 2(ωt ωt"
        },
        {
            "title": "3.2.2 DEFORMABLE ENVIRONMENT MAP FOR DYNAMIC LIGHTING.",
            "content": "The concept of deformable environment map involves treating the vanilla environment map as canonical environment map and combining it with deformation field. This approach enables us to model time-varying lighting conditions effectively. To implement this, we first apply positional encoding to the reflection direction ωt and time t. These encoded values are then input into deformable reflection MLP with parameters θR. This process allows us to obtain the deformed reflection residual ωt for each specified time t: ωt = FθR(γ(ωt r), γ(t)). r. This can be expressed as: = ωt ωt + ωt Subsequently, we add the deformed reflection residual ωt the deformed reflection direction ωt to the reflection direction ωt We can then use this deformed reflection direction ωt allowing us to obtain time-varying specular color ct nature of lighting in the scene while maintaining stable canonical reference. (10) to query the canonical environment map, s. This approach effectively captures the dynamic (9) r, yielding 3.2.3 COLOR DECOMPOSITION AND STAGED TRAINING STRATEGY. We decompose the final color ct between high and low-frequency information: final into diffuse and specular components to better distinguish final = cd + ct ct stint, (11) where cd is the diffuse color (using zero-order spherical harmonics as view-independent color), stint [0, 1]3 is the learnable specular tint stored in each 3D Gaussian, and cs is the view-dependent color component. To manage the transition from spherical harmonics to ct final and mitigate potential geometry disruptions, in the early specular stage, we fix the deformable Gaussian MLP and most 3D Gaussian attributes, optimizing only zero-order SH, specular tint, and roughness. We temporarily suspend densification during this phase. As ct final becomes more complete, we gradually resume optimization of all parameters and reinstate the densification process. We further split the specular stage into two parts, applying coarse-to-fine strategy to the environment map. In the first part, we focus on optimizing the canonical environment map for time-invariant lighting. This establishes stable foundation for the overall lighting structure. In the second part, we proceed to optimize the deformable reflection MLP for dynamic elements. This approach ensures more robust learning process, allowing us to capture the static lighting conditions before introducing the complexities of dynamic components. 3.3 PHYSICAL NORMAL ESTIMATION Challenges in normal estimation for 3D Gaussians. Normal estimation is crucial for modeling specular objects, as it directly affects surface reflections. However, the discrete nature of 3D Gaussians makes this process challenging, as it typically requires continuous surface. GaussianShader (Jiang et al., 2023) observed that 3D Gaussians tend to flatten during training, leading to the use of the shortest axis as an initial approximation of the surface normal. To improve accuracy, they introduced residual normal for each 3D Gaussian to compensate for errors in this approximation. However, this method alone is insufficient for deformed 3D Gaussians, as the residual should vary at each time step. straightforward approach of rotating the residual based on the quaternion difference between canonical and deformed Gaussians proves inadequate, as it fails to account for shape changes during deformation. If the shortest axis of the canonical 3D Gaussian is no longer the shortest after deformation, this method results in incorrect rotation. Consequently, more sophisticated approach is needed to accurately model the normals of deformed 3D Gaussians. This approach must consider both the rotation and the change in shape during the deformation process, ensuring accurate normal estimation for dynamic specular objects. 5 Figure 3: Normal estimation. (a) shows that flatter 3D Gaussians align better with scene surfaces, their shortest axis closely matching the surface normal. In contrast, less flat 3D Gaussians fit less accurately, with their shortest axis diverging from the surface normal. (b) shows that when the deformed 3D Gaussian becomes flatter (t = t1), normal residual is rotated by Rt 1 and scaled down by β , as flatter Gaussians require smaller normal residuals. Conversely, when the deformation βt 1 results in less flat shape (t = t2), is rotated by Rt , requiring larger correction to align the shortest axis with the surface normal. (c) shows how γk changes with (where = vt ) for = 1, = 5, and = 50. Larger indicates less flat Gaussians, while smaller vt represents flatter Gaussians. As increases, γk decreases more steeply as rises. For = 5, we observe balanced behavior: γk approaches 1 for low and 0 for high w, providing nuanced penalty adjustment across different Gaussian shapes. 2 and amplified by β βt 2 Improved rotation calculation for deformed 3D Gaussians. To overcome the limitations of naive methods and accurately model the normal of deformed 3D Gaussians, we propose using both the shortest and longest axes of canonical and deformed Gaussians to compute the rotation matrix. This approach accounts for both rotation and shape changes during deformation. We first align the deformed Gaussians axes with those of the canonical Gaussian using the following method: vt = (cid:26)vt vt if vs vt otherwise. > 0, , vt = (cid:26)vt vt if vl vt otherwise. > 0, , (12) where vs and vl represent the shortest and longest axes of canonical 3D Gaussians, while vt and vt denote the same for deformed 3D Gaussians. We then construct orthogonal matrices using these aligned axes and their cross products: = [vs vl vs vl] , Vt = (cid:2)vt vt vt vt (cid:3) . (13) Finally, we derive the rotation matrix: Rt = VtU. This method provides robust solution for calculating the rotation of deformation process, ensuring accurate normal estimation for dynamic specular objects. (14) Adjusting normal residuals and ensuring accuracy. To account for shape changes during deformation, we scale the normal residual based on the ratio of oblateness β βt between canonical and deformed 3D Gaussians. β = vl vs vl , βt = vt vt vt , (15) where β and βt represent the oblateness of canonical and deformed 3D Gaussians, respectively. This is because flatter 3D Gaussians tend to align more closely with the surface, meaning their shortest axis becomes more aligned with the surface normal, as shown in Fig. 3(a). In such cases, less compensation from the normal residual is needed. Conversely, less flat Gaussians require more compensation, as illustrated in Fig. 3(b). We then obtain deformed normal residuals: nt = β βt Rtn. (16) The final normal nt is computed by adding this residual to the shortest axis and ensuring outward orientation: nt = nt + vt s, nt = (cid:26)nt nt if nt ωt otherwise. > 0, (17) This approach adjusts for Gaussian flatness and ensures accurate normal estimation. 6 Table 1: Quantitative comparison on the NeRF-DS (Yan et al., 2023) dataset. We report the average PSNR, SSIM, and LPIPS (VGG) of several previous models on test images. The best , the second best , and third best results are denoted by red, orange, yellow. Bell PSNR SSIM LPIPS Cup PSNR SSIM LPIPS Basin PSNR SSIM LPIPS As PSNR SSIM LPIPS Method Deformable 3DGS (Yang et al., 2023c) 26.04 24.85 4DGS (Wu et al., 2023) 21.89 GaussianShader (Jiang et al., 2023) 21.58 GS-IR (Liang et al., 2023d) 25.34 NeRF-DS (Yan et al., 2023) 17.59 HyperNeRF (Park et al., 2021b) 26.80 Ours 0.8805 0.8632 0.7739 0.8033 0.8803 0.8518 0.8851 0.1850 0.2038 0.3620 0.3033 0.2150 0.2390 0.1761 19.53 19.26 17.79 18.06 20.23 22.58 19. 0.7855 0.7670 0.6670 0.7248 0.8053 0.8156 0.7922 0.1924 0.2196 0.4187 0.3135 0.2508 0.2497 0.1896 23.96 22.86 20.69 20.66 22.57 19.80 25.46 0.7945 0.8015 0.8169 0.7829 0.7811 0.7650 0.8497 0.2767 0.2061 0.3024 0.2603 0.2921 0.2999 0.1600 24.49 23.82 20.40 20.34 24.51 15.45 24. 0.8822 0.8695 0.7437 0.8193 0.8802 0.8295 0.8879 0.1658 0.1792 0.3385 0.2719 0.1707 0.2302 0.1588 Method Plate PSNR SSIM LPIPS Press PSNR SSIM LPIPS Sieve PSNR SSIM LPIPS Mean PSNR SSIM LPIPS Deformable 3DGS (Yang et al., 2023c) 19.07 18.77 4DGS (Wu et al., 2023) 14.55 GaussianShader (Jiang et al., 2023) 15.98 GS-IR (Liang et al., 2023d) 19.70 NeRF-DS (Yan et al., 2023) 21.22 HyperNeRF (Park et al., 2021b) 20.84 Ours 0.7352 0.7709 0.6423 0.6969 0.7813 0.7829 0.8180 0.3599 0.2721 0.4955 0.4200 0.2974 0.3166 0.2198 25.52 24.82 19.97 22.28 25.35 16.54 26.49 0.8594 0.8355 0.7244 0.8088 0.8703 0.8200 0. 0.1964 0.2255 0.4507 0.3067 0.2552 0.2810 0.1889 25.37 25.16 22.58 22.84 24.99 19.92 25.22 0.8616 0.8566 0.7862 0.8212 0.8705 0.8521 0.8712 0.1643 0.1745 0.3057 0.2236 0.2001 0.2142 0.1513 23.43 22.79 19.70 20.25 23.24 19.01 24.17 0.8284 0.8235 0.7363 0.7796 0.8384 0.8167 0. 0.2201 0.2115 0.3819 0.2999 0.2402 0.2615 0.1778 Figure 4: Qualitative comparison on the NeRF-DS Yan et al. (2023) dataset. 3.4 LOSS FUNCTIONS Normal regularization. To allow the normal residual to correct the normal while not excessively influencing the optimization of the shortest axis towards the surface normal, we introduce penalty term for the normal residual: Lreg = γkn2 2 where γ = 1 (cid:115) vt s2 2 . vt (18) In our experiments, we set = 5. When = 5, less flatter 3D Gaussians have γk approaching 0. Their shortest axis aligns poorly with the surface normal, requiring more normal residual correction and smaller penalties. Conversely, flatter Gaussians have γk near 1. Their shortest axis aligns better with the surface normal, needing less normal residual correction and allowing larger penalties, as shown in Fig. 3(c). Total training loss. To refine all parameters in the dynamic and specular stages, we employ the total training loss: = Lcolor + λnormalLnormal + Lreg, (19) where Lcolor and Lnormal are obtained as described in Section 3.2.1. In our experiments, we set λnormal = 0.01."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EVALUATION RESULTS We evaluate our method on two real-world datasets: NeRF-DS dataset (Yan et al., 2023) and HyperNeRF dataset (Park et al., 2021b). 7 Table 2: Quantitative comparison on the NeRF-DS (Yan et al., 2023) dataset with our labeled dynamic specular masks. We report PSNR, SSIM, and LPIPS (VGG) of previous methods on dynamic specular objects using the dynamic specular objects mask generated by Track Anything (Yang et al., 2023a). The best , the second best , and third best results are denoted by red, orange, yellow. Method As PSNR SSIM LPIPS Basin PSNR SSIM LPIPS Bell PSNR SSIM Deformable 3DGS (Yang et al., 2023c) 4DGS (Wu et al., 2023) GaussianShader (Jiang et al., 2023) GS-IR (Liang et al., 2023d) NeRF-DS (Yan et al., 2023) HyperNeRF (Park et al., 2021b) Ours 24.14 22.70 19.27 19.32 23.67 17.37 24. 0.7432 0.6993 0.5652 0.5857 0.7478 0.6934 0.7534 Method Plate PSNR SSIM Deformable 3DGS (Yang et al., 2023c) 4DGS (Wu et al., 2023) GaussianShader (Jiang et al., 2023) GS-IR (Liang et al., 2023d) NeRF-DS (Yan et al., 2023) HyperNeRF (Park et al., 2021b) Ours 16.12 13.93 9.87 11.09 14.80 16.03 16.53 0.5192 0.4095 0.2992 0.3254 0.4518 0.4629 0. 0.2957 0.3517 0.5232 0.4782 0.3635 0.3834 0.2896 LPIPS 0.3544 0.4229 0.6812 0.6270 0.3987 0.3775 0.3041 17.45 16.61 15.71 15.21 17.98 18.75 17.71 0.5530 0.4797 0.4163 0.4009 0.5537 0.5671 0.5675 0.3138 0.4084 0.5941 0.5644 0.4211 0.4125 0. 19.42 14.64 12.10 12.09 14.73 13.93 19.60 0.5516 0.2596 0.1676 0.1757 0.2439 0.2292 0.5680 Press PSNR SSIM LPIPS Sieve PSNR SSIM 19.64 20.17 16.84 16.43 19.77 14.10 21.70 0.6384 0.5434 0.4408 0.4083 0.5835 0.5365 0. 0.3268 0.4339 0.6093 0.5776 0.5035 0.5023 0.3252 20.74 19.70 16.19 16.42 20.28 18.39 20.36 0.5283 0.4498 0.3241 0.3339 0.5173 0.5296 0.5089 LPIPS 0.2940 0.4467 0.6764 0.6722 0.5931 0.6051 0.2862 LPIPS 0.3109 0.3879 0.5862 0.5749 0.4067 0.3949 0.3190 Cup PSNR SSIM LPIPS 20.10 18.90 14.90 14.80 19.95 15.07 20.28 0.5446 0.4132 0.3634 0.3445 0.5079 0.4860 0.5473 0.3312 0.4032 0.6146 0.6046 0.3494 0.4183 0.3176 Mean PSNR SSIM LPIPS 19.66 18.09 14.98 15.05 18.74 16.23 20.10 0.5826 0.4649 0.3681 0.3678 0.5151 0.5007 0.5921 0.3181 0.4078 0.6121 0.5856 0.4337 0.4420 0.3066 Figure 5: Qualitative comparison on NeRF-DS dataset with labeled dynamic specular masks. Entire scene of the NeRF-DS dataset. The NeRF-DS dataset (Yan et al., 2023) is monocular video dataset comprising seven real-world scenes from daily life, featuring various types of moving or deforming specular objects. We compare our method with the most relevant state-of-the-art approaches. As shown in Tab. 1 and Fig. 4, the quantitative results demonstrate that our method decisively outperforms baselines in reconstructing and rendering real-world highly reflective dynamic specular scenes. Dynamic specular object of NeRF-DS dataset. Since each scene in the NeRF-DS dataset (Yan et al., 2023) contains not only dynamic specular objects but also static background objects, we use Track Anything (Yang et al., 2023a) to obtain masks for the dynamic specular objects. This allows us to evaluate only the dynamic specular objects. As shown in Tab. 2 and Fig. 5, our method outperforms baselines when evaluating the dynamic specular objects in these monocular sequences. HyperNeRF dataset. The HyperNeRF dataset, while also containing real-world dynamic scenes, does not include specular objects. As shown in Tab. 3 and appendix Fig. 14, the results demonstrate that we are on par with state-of-the-art techniques for rendering novel views and our methods performance is not limited to shiny scenes. This strong performance across different types of real-world datasets further confirms the effectiveness of our approach in handling wide range of scene characteristics. The success can be attributed to our key innovations: physical normal estimation, deformable environment map, and coarse-tofine training strategy, which together enable robust reconstruction and rendering of diverse scenes. Notably, unlike NeRF-DS, our approach does not require mask supervision to clearly distinguish between static and dynamic objects, as illustrated in Fig. 6. Additionally, Fig. 7 illustrates our methods decomposition results. As shown, our approach consistently achieves realistic separation of specular and diffuse components across different scenes in the NeRF-DS dataset. 8 Table 3: Quantitative comparison on the HyperNeRF (Park et al., 2021b) dataset. We report the average PSNR, SSIM, and LPIPS (VGG) of several previous models. The best , the second best , and third best results are denoted by red, orange, yellow. Broom PSNR SSIM LPIPS Mean PSNR SSIM LPIPS Chicken PSNR SSIM LPIPS Peel Banana PSNR SSIM LPIPS 3D printer PSNR SSIM LPIPS Method Deformable 3DGS (Yang et al., 2023c) 22.35 21.21 4DGS (Wu et al., 2023) 17.21 GaussianShader (Jiang et al., 2023) 20.46 GS-IR (Liang et al., 2023d) 22.37 NeRF-DS (Yan et al., 2023) 20.72 HyperNeRF (Park et al., 2021b) 22.04 Ours 0.4952 0.3555 0.2263 0.3420 0.4371 0.4276 0.5145 0.5148 0.5669 0.5812 0.5229 0.5694 0.5773 0. 21.47 21.90 17.31 18.24 22.16 21.94 19.96 0.6921 0.6993 0.5926 0.5745 0.6973 0.7003 0.6444 0.2147 0.3198 0.5054 0.5204 0.3134 0.3090 0.2397 23.55 28.69 19.70 20.64 27.32 27.40 22.20 0.6747 0.8143 0.6520 0.6592 0.7949 0.8013 0.6203 0.2334 0.2772 0.5004 0.4536 0.3139 0.3052 0. 21.28 27.77 19.99 20.15 22.75 22.36 27.34 0.5302 0.8431 0.7097 0.7159 0.6328 0.6257 0.8895 0.4472 0.2049 0.3308 0.3021 0.3919 0.3956 0.1290 22.16 24.89 18.55 19.87 23.65 23.11 22.89 0.5981 0.6781 0.5452 0.5729 0.6405 0.6387 0.6672 0.3525 0.3422 0.4795 0.4498 0.3972 0.3968 0. Table 4: Ablation studies on different coarse to fine training strategy stages. Stage PSNR SSIM LPIPS 20.26 0.7785 0.2953 Static St. + Dynamic 24.02 0.8508 0.1831 St. + Dy. + Specular 24.17 0.8529 0.1778 Table 5: Ablation study on coarse-to-fine and losses. C2F Lnormal Lreg γk PSNR SSIM LPIPS 23.16 0.8294 0.2156 23.40 0.8277 0.2278 24.15 0.8510 0.1845 24.09 0.8515 0.1818 24.17 0.8529 0.1778 Table 6: Ablation studies on SH, Static and Deformable environment map. PSNR SSIM LPIPS 23.63 0.8453 0.1844 SH Static Env. map 24.02 0.8508 0.1831 Deformable Env. map 24.17 0.8529 0.1778 Figure 6: Visualization our deformation magnitudes. (a) The left side shows the ground truth of the dynamic object, while (b) on the right side, we render the magnitude of the output of the position residual by our deformable Gaussian MLP. The brighter areas indicate greater movement of the 3D Gaussians. The figure shows that even without mask supervision, our method can still effectively distinguish which objects are dynamic. 4.2 ABLATION STUDY Different coarse to fine training strategy stages. As shown in the Tab. 4 and Fig. 8 , each stage contributes effectively to the models performance. The Dynamic stage enhances object stability compared to the Static stage alone, while the Specular stage improves reflection clarity beyond the combined Static and Dynamic stages. This coarse-to-fine approach establishes stable geometric foundation before addressing complex specular effects. Note that the total iterations for each row in the Tab. 4 are 40,000. Ablation study on coarse-to-fine, and loss function. The models performance was evaluated without key components: the coarse-to-fine training strategy, normal loss Lnormal, normal regularization Lreg, and γk. Fig. 9 and Tab. 5 illustrate the effects of these omissions. Without the coarse-to-fine approach, the model, trained directly at the specular stage, produces incomplete scene geometry, affecting environment map queries for specular color. Omitting normal loss Lnormal removes direct supervision on normals, impeding geometric refinement and reducing rendering quality. This also leads to inaccurate reflection directions and less precise specular colors. Removing normal regularization Lreg allows the normal residual to dominate normal optimization, resulting in insufficient optimization of the 3D Gaussians shortest axis towards the correct normal , which in turn reduces the rendering quality. Without γk in normal regularization, the normal residual decreases for both non-flattened and flat Gaussians. This particularly affects less flat 3D Gaussians whose shortest axis significantly deviates from the surface normal. The insufficient normal residual correction causes these 3D Gaussians shortest axes to deviate greatly from their original direction in an attempt to align with the surface normal, ultimately reducing rendering quality. Ablation study on SH, Static environment map, and Deformable enviorment map. Fig. 10 and Tab. 6 demonstrate the superiority of the deformable environment map over the static environment 9 Figure 7: Visualization our specular and diffuse color. Specular regions are emphasized while non-specular areas are dimmed to highlight the results of specular region color decomposition. Figure 8: Qualitative comparison of each training stage in our coarse-to-fine approach. Figure 9: Qualitative comparison of ablation study without different components. Figure 10: Qualitative comparison of ablation study on SH, Static environment map, and Deformable enviorment map. map, which in turn outperforms Spherical Harmonics (SH). SH struggles to accurately model highfrequency specular colors. While the static environment map can model high-frequency colors, it is best suited for static lighting conditions. In contrast, the deformable environment map excels in modeling time-varying lighting, offering superior performance for dynamic scenes."
        },
        {
            "title": "5 CONCLUSION",
            "content": "SpectroMotion enhances 3D Gaussian Splatting for dynamic specular scenes by combining specular rendering with deformation fields. Using normal residual correction, coarse-to-fine training, and deformable environment map, it achieves superior accuracy and visual quality in novel view synthesis, outperforming existing methods while maintaining geometric consistency. Limitations. Though we stabilize the entire scenes geometry using coarse-to-fine training strategy, some failure cases still occur. Please refer to the appendix for visual results of failure cases."
        },
        {
            "title": "REFERENCES",
            "content": "Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, and Matthew OToole. Torf: Time-of-flight radiance fields for dynamic scene view synthesis, 2021. 2 Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloˇs Haˇsan, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reflectance fields for appearance acquisition, 2020. 2 Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, and Hendrik P. A. Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition, 2021. 2 Robert Cook and Kenneth E. Torrance. reflectance model for computer graphics. ACM Transactions on Graphics (ToG), 1(1):724, 1982. Wenhang Ge, Tao Hu, Haoyu Zhao, Shu Liu, and Ying-Cong Chen. Ref-neus: Ambiguity-reduced neural implicit surface learning for multi-view reconstruction with reflection, 2023. 2 Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiaoqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, and Jingdong Wang. Forward flow for novel view synthesis of dynamic scenes, 2023. 2 Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes, 2024. 2 Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. arXiv preprint arXiv:2311.17977, 2023. 2, 4, 5, 7, 8, James Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, pp. 143150, 1986. 4 Brian Karis and Epic Games. Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice, 4(3):1, 2013. 4 Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering, 2023. 1, 3, 4, Junxuan Li and Hongdong Li. Neural reflectance for shape recovery with shadow handling, 2022. 2 Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. Neural 3d video synthesis from multi-view video, 2022. 2 Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes, 2021. 2 Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, and Nandita Vijaykumar. Envidr: Implicit differentiable renderer with neural environment lighting, 2023a. Ruofan Liang, Jiahao Zhang, Haoda Li, Chen Yang, Yushi Guan, and Nandita Vijaykumar. Spidr: Sdf-based neural point fields for illumination and deformation, 2023b. 2 Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis, 2023c. 2 Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. arXiv preprint arXiv:2311.16473, 2023d. 2, 4, 7, 8, 9 Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video, 2022. 2 Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023a. 2 11 Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images, 2023b. 2 Li Ma, Vasu Agrawal, Haithem Turki, Changil Kim, Chen Gao, Pedro Sander, Michael Zollhofer, and Christian Richardt. Specnerf: Gaussian directional encoding for specular reflections. arXiv preprint arXiv:2312.13102, 2023. 2 Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, and Edmond Boyer. Splatfields: Neural gaussian splats for sparse 3d and 4d reconstruction. arXiv preprint arXiv:2409.11211, 2024. Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 82808290, June 2022. 2, 4 Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields, 2021a. 2 Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields, 2021b. 2, 7, 8, 9, 15 Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Representing volumetric videos as dynamic mlp maps, 2023. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes, 2020. 2 Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, et al. Gir: 3d gaussian inverse rendering for relightable scene factorization. arXiv preprint arXiv:2312.05133, 2023. 2 Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerfplayer: streamable dynamic scene representation with decomposed neural radiance fields, 2023. 2 Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis, 2020. 2 Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. arXiv preprint arXiv:2406.18717, 2024. Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhofer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from monocular video, 2021. 2 Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 54815490. IEEE, 2022. 2 Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan Barron, Todd Zickler, and Pratul Srinivasan. Eclipse: Disambiguating illumination and materials using unintended shadows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7786, 2024a. 2 Dor Verbin, Pratul Srinivasan, Peter Hedman, Ben Mildenhall, Benjamin Attal, Richard Szeliski, and Jonathan Barron. Nerf-casting: Improved view-dependent appearance with consistent reflections. arXiv preprint arXiv:2405.14871, 2024b. 2 Bruce Walter, Stephen Marschner, Hongsong Li, and Kenneth Torrance. Microfacet models for refraction through rough surfaces. Rendering techniques, 2007:18th, 2007. 12 Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. 2 Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 2, 4, 7, 8, 9 Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video, 2021. Zhiwen Yan, Chen Li, and Gim Hee Lee. Nerf-ds: Neural radiance fields for dynamic specular objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82858295, 2023. 1, 7, 8, 9 Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv preprint arXiv:2304.11968, 2023a. 8 Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023b. 4 Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023c. 2, 4, 7, 8, 9 Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, and Xiaogang Jin. Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting, 2024. Keyang Ye, Qiming Hou, and Kun Zhou. 3d gaussian splatting with deferred reflection. In ACM SIGGRAPH 2024 Conference Papers, pp. 110, 2024. 2 Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG), 38 (6):114, 2019. 3 Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Neilf++: Inter-reflectable light fields for geometry and material estimation, 2023. 2 Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54535462, 2021a. 2 Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, and Jonathan T. Barron. Nerfactor: neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics, 40(6):118, December 2021b. ISSN 1557-7368. doi: 10.1145/3478513.3480496. URL http://dx.doi.org/10.1145/3478513.3480496. Xiaoming Zhao, Pratul Srinivasan, Dor Verbin, Keunhong Park, Ricardo Martin Brualla, and Philipp Henzler. Illuminerf: 3d relighting without inverse rendering. arXiv preprint arXiv:2406.06527, 2024. 2 Zuo-Liang Zhu, Beibei Wang, and Jian Yang. Gs-ror: 3d gaussian splatting for reflective object relighting via sdf priors. arXiv preprint arXiv:2406.18544, 2024. 2 APPENDIX / SUPPLEMENTAL MATERIAL A.1 IMPLEMENTATION DETAILS We use PyTorch as our framework and 3DGS (Kerbl et al., 2023) as our codebase. Our coarse-to-fine approach is divided into three sequential stages: static, dynamic, and specular. In the static stage, we 13 Figure 11: Architecture of the deformable Gaussian MLP Figure 12: Architecture of the deformable reflection MLP train the vanilla 3D Gaussian Splatting (3DGS) for 3000 iterations to stabilize the static geometry. The dynamic stage then introduces deformable Gaussian MLP to model dynamic objects. We first optimize both the canonical Gaussians and the deformable Gaussian MLP for 3000 iterations until the scene becomes relatively stable. Subsequently, we introduce Lnormal, enabling simultaneous optimization of the scenes normal and depth, further refining the geometry for another 3000 iterations. After the dynamic stage concludes, we transition to the specular stage, which involves changing the color representation from complete spherical harmonics to cfinal. To mitigate potential geometry disruptions due to the initially incomplete cfinal, we fix the deformable Gaussian MLP and all 3D Gaussian attributes except for zero-order SH, specular tint, and roughness, while temporarily suspending densification. After 6000 iterations, once cfinal becomes more complete, we resume optimization of all parameters and reinstate the densification process. Concurrently, during the first 2000 iterations of the specular stage, we optimize only the canonical environment map to learn time-invariant lighting. For the canonical environment, we use 6 128 128 learnable parameters. Subsequently, we begin optimizing the deformable reflection MLP to capture time-varying lighting effects. We train total iterations 40,000. All training and rendering are conducted on an NVIDIA RTX 4090 GPU. A.1.1 NETWORK ARCHITECTURE OF THE DEFORMABLE GAUSSIAN MLP AND DEFORMABLE REFLECTION MLP We use deformable Gaussian MLP to predict each coordinate of 3D Gaussians and time to their corresponding deviations in position, rotation, and scaling. As shown in Fig. 11, the MLP initially processes the input through eight fully connected layers that employ ReLU activations, featuring 256-dimensional hidden layers, and outputs 256-dimensional feature vector. This vector is then passed through three additional fully connected layers combined with ReLU activation to separately output the offsets over time for position, rotation, and scaling. Notably, similar to NeRF, the feature vector and the input are concatenated in the fourth layer. For the deformable reflection MLP, we utilize the same network architecture, as shown in Fig. 12. 14 A.2 ADDITIONAL EXPERIMENT RESULTS We provide an HTML interface in the supplementary material zip file for browser-rendered video results of all compared methods. This includes qualitative comparisons on the NeRF-DS dataset for each scene, as shown in Fig. 13, as well as qualitative comparisons on the NeRF-DS dataset for each scene with labeled dynamic specular masks, as shown in Fig. 15. Additionally, failure cases are presented in Fig. 16. Figure 13: Qualitative comparison on NeRF-DS dataset per-scene. Figure 14: Qualitative comparison on the HyperNeRF Park et al. (2021b) dataset. 15 Figure 15: Qualitative comparison on NeRF-DS dataset per-scene with labeled dynamic specular masks. Figure 16: Failure cases. In some dramatic scenes, relying solely on the Deformable Gaussian MLP is insufficient, such as when an arm enters or exits the scene, leading to many floaters occurring."
        }
    ],
    "affiliations": [
        "National Yang Ming Chiao Tung University",
        "University of Illinois Urbana-Champaign"
    ]
}