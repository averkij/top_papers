{
    "paper_title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning",
    "authors": [
        "Shenghao Fu",
        "Qize Yang",
        "Yuan-Ming Li",
        "Xihan Wei",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with a uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model that can adaptively zoom in on a video clip. The model is first provided with densely sampled frames but in a small resolution. If some spatial details are needed, the model can zoom in on a clip of interest with a large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as a multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves a great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 6 8 7 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "LOVE-R1: ADVANCING LONG VIDEO UNDERSTANDING WITH AN ADAPTIVE ZOOM-IN MECHANISM VIA MULTI-STEP REASONING Shenghao Fu1,2,4, Qize Yang2, Yuan-Ming Li1,2,4, Xihan Wei2, Xiaohua Xie1,4,5,6, Wei-Shi Zheng1,3,4,6 1School of Computer Science and Engineering, Sun Yat-sen University, China; 2Tongyi Lab, Alibaba Group; 3Peng Cheng Laboratory, China; 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China; 5Guangdong Province Key Laboratory of Information Security Technology, China; 6Pazhou Laboratory (Huangpu), China fushh7@mail2.sysu.edu.cn, qize.yqz@alibaba-inc.com Code: https://github.com/HumanMLLM/LOVE-R1 Figure 1: Illustration of the workflow of LOVE-R1. Our model first takes densely sampled smallresolution frames from the whole video as inputs to understand the video globally. If needed, it can adaptively zoom in on video clip to gain fine-grained spatial details. The workflow is implemented as multi-step reasoning process."
        },
        {
            "title": "ABSTRACT",
            "content": "Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, model that can adaptively zoom in on video clip. The model is first provided with densely sampled frames but in small resolution. If some spatial details are needed, the model can zoom in on clip of interest with large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance : Corresponding authors are Xiaohua Xie and Wei-Shi Zheng. Work was done when Shenghao Fu and Yuan-Ming Li were interns at Alibaba."
        },
        {
            "title": "Preprint",
            "content": "it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Video-Language Models (LVLMs) (Zhang et al., 2024b; Bai et al., 2025; Zhang et al., 2024c; Fu et al., 2025b) have achieved great progress in understanding temporal dynamics. However, long video understanding (LVU), owing to the long-form temporal dependency and the great variety of action sequences, still poses great challenges to them. When tackling long videos, mainstream LVLMs utilize uniform sampling strategy, in which frames are sampled with fixed interval and resolution. Constrained by the context length, LVLMs will face the dilemma of balancing spatial resolution and temporal sampling density. Sampling more frames can help better understand motion clues, while adopting larger frame resolution can preserve more spatial details. With limited context length, LVLMs with fixed sampling strategy fail to balance the spatial-temporal trade-off. However, only small number of keyframes are needed for large proportion of the questions. Suo et al. (2025) find that recent LVLMs can achieve more than 75% Pass@N accuracy with 32 randomly sampled frames on most long video understanding benchmarks when is larger than 40. Furthermore, ViLAMP (Cheng et al., 2025) also finds that around 90% of query-induced attention weights concentrate only on 5% of frames. These findings show that selecting high-quality keyframes is crucial for effective and efficient long video understanding. Inspired by the strong reasoning capacity demonstrated by recent reasoning models (Guo et al., 2025; Jaech et al., 2024), our objective is to train an LVLM with adaptive zoom-in ability. Specifically, three abilities are needed: 1) the model with the decision ability can decide whether the visual information is sufficient to answer the question, 2) if not, LVLMs can use the zoom-in ability to select the most relevant time span to zoom in, 3) when visual information is sufficient, LVLMs use the answering ability to provide answers through thinking. This adaptive frame selection mechanism allows LVLMs to attend to informative frames with large resolution while understanding the overall event with small resolution, thus preserving vision tokens within manageable context and balancing long-form temporal understanding and detailed spatial perception. Based on this motivation, we propose LOVE-R1, long video understanding model with slowfast-like dynamic frame processing mechanism, as shown in Figure 1. We first sample the video at high frame rate (e.g., 768 frames) but in small resolution (e.g., 32 tokens per frame) to provide the model with global view of the video without sacrificing temporal details. When some spatial details are needed, we provide the model with few high-resolution frames (e.g., 256 tokens per frame). The entire process is automatically decided by the model itself. The development of LOVER1 undergoes three-stage post-training: 1) slow-fast template finetuning: Instead of adopting fixed frame sampling strategy, LOVE-R1 processes the video into multiple segments with different frame rates, resolutions and timespans. We finetune the LVLMs with open-sourced video instruction data to adapt them to the new video template. 2) CoT cold start: We construct 38k CoT data with careful data selection, construction, cleaning, and filtering. After finetuning on the high-quality CoT data, the model is equipped with basic reasoning ability. 3) decoupled reinforcement finetuning: Reinforcement learning has been shown to be an effective method to boost reasoning capacity. However, most of the methods are based on outcome rewards, i.e., the final answer is correct or not, which can not provide fine-grained process rewards in our multi-turn scenario. Thus, we decouple the multi-turn conversations into multiple single-turn conversations and optimize the zoom-in ability separately, which is the key factor for long video understanding and can not be optimized effectively in the standard GRPO algorithm (Guo et al., 2025). With the ability to zoom in on the video, LOVE-R1 achieves state-of-the-art performance on common long video understanding benchmarks. Specifically, LOVE-R1 gets 48.2% on LVBench (Wang et al., 2025b), 60.1% on LongVideoBench (Wu et al., 2024), and 66.2% on VideoMME (Fu et al.,"
        },
        {
            "title": "Preprint",
            "content": "2025a), outperforming our baseline Qwen2.5-VL 7B (Bai et al., 2025) by 6.2%, 4.1%, and 1.0%. We hope our work can provide new paradigm to tackle the long video understanding problem."
        },
        {
            "title": "2.1 LONG VIDEO UNDERSTANDING WITH LARGE VIDEO-LANGUAGE MODELS",
            "content": "In order to unify image and video representation and pretraining, recent Large Video Language Models (LVLMs) (Bai et al., 2025; Zhao et al., 2025b; Peng et al., 2025; Zhang et al., 2024c) adopt uniform sampling strategy, in which frames are sampled from the video with fixed interval and resolution and frames are concatenated in order. Although this fixed dense sampling strategy is simple and effective in the short video scenario, the number of visual tokens will soon increase out of the budget when tackling long videos. To preserve informative visual information and reduce visual tokens, lot of video processing methods are proposed: 1) For token compression methods (Li et al., 2024d; Song et al., 2024; Shen et al., 2024; Man et al., 2025; Shu et al., 2025), they prune or merge visual tokens based on similarity or relation to the query. Tokens after pruning are poorly organized. 2) For keyframe selection methods (Zhang et al., 2025b; Wang et al., 2025a; Hu et al., 2025), they prune visual information at larger granularity. The most informative frames are selected for inference. Since the structure of frames is not changed, these methods can serve as plug-and-play module to other LVLMs without finetuning. 3) For long context methods (Chen et al., 2024b; Shen et al., 2025; Ren et al., 2025), they extend the context window to preserve as much information as possible. 4) For agent-based methods (Luo et al., 2024; Liu et al., 2025b; Wang et al., 2025c), they use tools to handle sub-videos separately and then use an LLM to select and merge the helpful information and then answer the question. In this work, we propose dynamic video processing method that can zoom in on subset of the video adaptively, striking balance between sampling density and frame resolution. 2.2 MULTIMODAL REASONING Recent reasoning models Guo et al. (2025); Jaech et al. (2024) show that generating long Chain-ofThoughts (CoT) at test time, which breaks hard problem into series of solvable sub-problems and then derives the final answer, can significantly enhance the performance. Early exploration of video reasoning (Feng et al., 2025; Yang et al., 2025; Chen et al., 2025b; Zhao et al., 2025a) also shows that taking visual information into thinking can boost performance in both perception and reasoning tasks. Different from reasoning on pure text, multimodal reasoning has greater flexibility to manipulate visual content to assist reasoning, such as zooming in on the image region (Su et al., 2025; Zhu et al., 2025) and grounding related objects (Zhang et al., 2025c; Fan et al., 2025). In this work, we aim to use the reasoning ability to decide which video clip to zoom in on so that models can process long videos in limited context. Very recently, two concurrent works VITAL (Zhang et al., 2025a) and Video-MTR (Xie et al., 2025) also share the idea of dynamically processing video information via CoTs. Differently, we adopt slow-fast video template rather than interleaving video clips with CoTs, which can balance the temporal density and frame resolution while preserving pretraining performance as much as possible. Further, we propose to provide fine-grained process rewards by decoupled reinforcement finetuning, achieving higher performance."
        },
        {
            "title": "3 A DYNAMIC FRAME PROCESSING MECHANISM",
            "content": "Previously, Large Video Language Models (LVLMs) usually adopt uniform frame sampling mechanism, in which frames are sampled at fixed sampling rate, and frames are of equal size. However, as the video becomes longer, the number of visual tokens will inevitably exceed the context length. Both decreasing the sampling rate or frame resolution can reduce visual tokens but at the cost of losing temporal or spatial details, failing to understand the long video fully and accurately. To enable better temporal-spatial perception with constrained context length, we propose using slow-fast-like template as shown in Figure 2(c). Specifically, for video with seconds, we first obtain the fast video by densely sampling the video at high frame rate psf but in small"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Different slow-fast video templates. Templates (a) and (b) will replace the original fast video segments with the slow videos. Template (a) treats multiple video segments as whole video while Template (b) explicitly separates them with identifiers (<vision start>, <vision end>). Template (c) appends the additional slow videos at the end of the fast video without removing the corresponding fast video segments. We adopt Template (c). max, i.e. = min (T pst, resolution rf . The total number of sampled frames does not exceed predefined maximum frame number max). This fast video provides the overall event of the video with rich temporal details. When the model needs some spatial details from specific clip [t1, t2], we sample the frames within this clip at high resolution rs and small number of frames max. The slow video is encoded separately and appended after the fast video. If the model zooms in on multiple clips, the slow videos will be organized in order. The overall video template is as follows: Full video [0,T]: fast video Subset zoom-in video clip [t1, t2]: slow video 1 Subset zoom-in video clip [t3, t4]: slow video 2 ... Subset zoom-in video clip [t2k1, t2k]: slow video where t1 t3 t2k1 and will be replaced with video tokens. We also ablate other video templates as shown in Figure 2 (a) and (b). These two templates will replace the corresponding fast video clips with slow videos. Template (a) views multiple segments as whole video, while Template (b) separates them explicitly. These two templates will break the fast video into multiple videos. Instead, our template (c) appends the slow videos behind the fast video. We find this template aligns well with the pretraining template, thus it can be adopted by the model with little finetuning data. With the dynamic frame processing mechanism, the model strikes balance between temporal sampling density and spatial resolution. However, which video clip is necessary to zoom in on is related to the user query and not easy to find. Thus, we aim to use the strong reasoning capacity of LLMs to determine it so that the model needs three reasoning capacities: the decision ability, the zoom-in ability, and the answering ability. Given specific query, the model should first decide whether the visual information is sufficient to answer the question (the decision ability). If not, the model can use the zoom-in ability to select the most relevant time span to zoom in. When visual information is sufficient, LVLMs use the answering ability to provide the answer through deep thinking. The overall pipeline works in multi-step manner. In the following, we show how to build base model to reasoning one via three-stage post-training."
        },
        {
            "title": "4 A THREE-STAGE POST-TRAINING RECIPE",
            "content": "4.1 STAGE 1: SLOW-FAST TEMPLATE FINETUNING In this work, the proposed dynamic zoom-in mechanism adopts new video template and requires strong temporal awareness to precisely localize the relevant video clip. The videos in our slowfast template will be represented in multiple segments with different frame rates, resolutions, and timespans, which is different from the pretraining one. Thus, we conduct an extra supervised finetuning stage before cold start to maintain the video understanding ability under the new template and enhance the temporal grounding ability. Specifically, we use FineVideo (Farre et al., 2024) and videos ranging from 2 to 3 minutes in LLaVA-Video-178k (Zhang et al., 2024c) as the general video instruction tuning dataset to enhance the perception ability and ET-Instruct (Liu et al., 2024) as the temporal grounding dataset to strengthen the temporal grounding ability. The total number of training data is around 153k. During training, we use the ground truth timespans or random times-"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Illustration of decoupled reinforcement finetuning. (a) For questions without ground truth timespans, we apply the standard GRPO algorithm to optimize multi-step CoTs as whole. (b) To provide fine-grained process rewards, we decouple multi-step reasoning into multiple single-step reasoning and optimize the single zoom-in step explicitly by appending the zoom-in prefix. pans to obtain several slow videos to simulate the slow-fast template. After finetuning, the model successfully adopts the new template without sacrificing performance. 4.2 STAGE 2: COT COLD START To facilitate training, we annotate 38k CoT data with strong proprietary reasoning models (Comanici et al., 2025; Hurst et al., 2024). The source videos are selected from two widely used grounded video question answering datasets NExT-GQA (Xiao et al., 2024) and CG-Bench (Chen et al., 2025a). Each question in these datasets is annotated with related timespans so that we can filter out CoTs with wrong zoom-in timespans. The entire construction pipeline undergoes strict cleaning, filtering, and prompt engineering to ensure data quality. The detailed construction pipeline can be found in Section B. And an example of our collected CoT is shown in Figure 1. With the 38k CoT data generated above, we finetune the base model on it to learn the decision ability, zoom-in ability, and answering ability. We treat each single step as sample and train the model on each single step. To ease learning difficulty and make flexible control during training and inference, we explicitly decouple the three abilities by adding prefix for each CoT. We add get the answer. for answer CoTs and need to zoom in on the video. for zoom-in CoTs. With the prefixes, we can precisely control the model behavior during training and testing by adding the corresponding prefix before generating CoTs. Further, to simulate the real-world decision scenarios, we select different slow videos during training. For zoom-in CoTs, we select no slow videos or wrong slow videos for training. For answer CoTs, we select slow videos containing ground truth timespans. Training with different slow videos for different CoTs, models can learn to zoom in on video clip when the information is not enough and provide final answer when key visual clues are gained. 4.3 STAGE 3: DECOUPLED REINFORCEMENT FINETUNING Although reinforcement algorithms based on rule-based outcome rewards, like GRPO (Guo et al., 2025), have shown great effectiveness and scalability, the sparsity of rewards can not provide accurate process supervision, which may hinder the performance, especially in our multi-step scenario. When the model zooms in on an error video clip but gets correct answer, the outcome rewards will still encourage this undesired behavior, preventing the model to learn the grounding ability. It will not be rare case since the model with only the fast video as input can also get high performance, as shown in Table 2c. With wrong slow videos, models may learn to answer the question based solely on the fast video while discarding the slow ones, which will greatly degrade the performance. To this end, in addition to the standard GRPO process which optimizes the multi-step CoT as whole, we propose to decouple the multi-step problem into multiple single-step reasoning and optimize the single zoom-in step with ground truth timespans. Specifically, two types of data are used for training: 1) local question data, each of which is annotated with ground truth timespans. 2) general question data, which are abundant in the community and have different question types. We use questions in CG-Bench (Chen et al., 2025a) for the first one while using MovieChat (Song et al., 2024) annotated by VideoMarathon (Lin et al., 2025) for the"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Evaluation results on long video understanding benchmarks. *: Reproduced by us using 16k context. Our LOVE-R1 is also evaluated with around 16k context. Since LOVE-R1 (stage 1) does not have the ability to zoom in, we uniformly sample 32 frames as the slow video. Models Duration Proprietary Models GPT4-V GPT4-o Open-Source Video MLLMs Video-LLaVA (Lin et al., 2024) LLaMA-VID (Li et al., 2024d) ShareGPT4Video (Chen et al., 2024a) LLaVA-NeXT-Video (Zhang et al., 2024b) VideoLLaMA2 (Cheng et al., 2024) LongVA (Zhang et al., 2024a) VideoChat2 (Li et al., 2024b) LLaVA-OneVision (Li et al., 2024a) Vamba (Ren et al., 2025) VideoChat-T (Zeng et al., 2025) Quicksviewer (Qi et al., 2025) Video-XL (Shu et al., 2025) Video-XL-Pro (Liu et al., 2025a) LongVILA (Chen et al., 2024b) LongVU (Shen et al., 2024) Hour-LLaVA (Lin et al., 2025) LongVITA-128k (Shen et al., 2025) VILAMP (Cheng et al., 2025) VideoChat-Flash (Li et al., 2024c) Open-Source Agent Video MLLMs VideoMind (Liu et al., 2025b) Video-RAG (Luo et al., 2024) Open-Source Reasoning Video MLLMs Video-MTR (Xie et al., 2025) Video-R1 (Feng et al., 2025) VITAL (Zhang et al., 2025a) LongVILA-R1 (Chen et al., 2025b) Ours Qwen2.5-VL* (Bai et al., 2025) LOVE-R1 (stage 1) LOVE-R1 (stage 2) LOVE-R1 Size #Frames Context MLVU 3120 min VideoMME (w/o sub) Overall 160 min Long 3060 min LongVideoBench LVBench 060 min 4101 sec - - 7B 7B 8B 7B 7B 7B 7B 7B 10B 7B 7B 7B 7B 7B 7B 7B 14B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 1fps 1fps 8 1fps 16 32 32 128 16 32 1024 12 1fps 256 240 256 1fps 1fps 256 1fps 512 - - 32 32 1024 512 128 768+32 768+32 768+32 - - - - - - - - - - - - - - - - - - - - - - - 4k 4k 32k 132k 16k 16k 16k 16k - 66.2 47.3 33.2 46.4 - 48.5 56.3 47.9 64.7 65.9 - 61.5 64.9 70.6 - 65.4 - - 72.6 74.7 64.4 72. 48.4 - - - 60.7 77.2 40.4 - 43.6 46.5 46.6 54.3 54.6 58.2 57.8 46.3 56.9 55.5 60.0 60.1 60.6 63.6 66.4 67.5 65.3 58.2 62.1 59.0 59.3 64.1 65.1 56.9 72. 38.1 - 37.9 - 43.8 47.6 39.2 46.7 - 41.9 - - - 53.0 59.5 55.0 58.8 57.8 55.4 49.2 59.8 51.0 - 54.0 55.2 - 66.7 39.1 - 39.7 43.5 - - - - 55.9 - - 50.7 56.7 57.1 - 60.4 60.9 61.2 - 56.3 58. - - - 58.0 - 34.7 - 23.9 - - - - - - 42.1 - - - - - - 45.6 - 45.2 48.2 40.8 - - - - - 66.4 68.5 66.7 67.4 (+1.0) 65.2 65.4 64.9 66.2 (+1.0) 54.6 56.0 53.3 57.7 (+3.1) 56.0 55.6 59.7 60.1 (+4.1) 42.0 44.7 46.2 48.2 (+6.2) latter one. As shown in Figure 3(a), for general question data, we use the standard GRPO algorithm for optimization. For each question, we generate multi-step CoTs until we get the final answer ans. The accuracy reward ra is determined by whether the predicted answer is correct or not: ra = (cid:26)1, 0, if ans is correct, otherwise, (1) The accuracy reward is shared by all steps in the CoT. As shown in Figure 3(b), for data with ground truth timespans, we explicitly optimize the single-step zoom-in ability by appending the prefix need to zoom in on the video. before generating the CoT. The accuracy reward rz is determined by whether the predicted zoom-in timespans [T pred ] overlaps with ground truth timespans [T gt , pred 1 1 , gt 2 ]: rz = (cid:40) 1, 0, if IoU([T pred 1 otherwise, , pred 2 ], [T gt 1 , gt 2 ]) > 0, (2) To align with the accuracy reward for general data and make it distinguishable, we use binary reward for rz instead of the IoU. After finetuning, the model has the ability to zoom in on clip of interest from long video."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5.1 EXPERIMENT SETUPS Implementation Details. Our model is finetuned from Qwen2.5-VL 7B (Bai et al., 2025) with our three-stage training recipe. To enhance temporal grounding ability, we add the frame number in"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Ablation studies. All experiments are tested on Video-MME (w/o subtitle). (a) Ablation studies on different video templates. Models are finetuned with part of data. Template Template (a) Template (b) Template (c) Setting zero-shot finetune zero-shot finetune zero-shot finetune Overall Long Medium Short 72.6 72.2 73.1 72.2 71.7 72.6 66.1 64.7 66.2 64.9 65.3 65.0 53.4 54.2 54.2 55.7 52.4 55.8 64.0 63.7 64.5 64.3 63.1 64.4 (b) Ablation studies on different RL methods. Multi-Step Single-Step Optimization Optimization Overall Long Medium Short 64.9 65.7 66.2 53.3 55.4 57.7 65.9 66.6 65. 75.6 75.2 75.3 (c) Ablation studies on different zoom-in video clips. (d) Ablation studies on different numbers of maximum inference iterations. Setting no zoom-in videos uniform zoom-in videos random zoom-in videos adaptive zoom-in videos Overall Long Medium Short 69.4 71.1 72.6 75.3 61.2 62.4 62.3 65. 52.1 51.9 51.9 57.7 60.9 61.8 62.3 66.2 #iteration Overall Long Medium Short 69.4 74.4 75.3 75.4 52.1 56.0 57.7 56.9 61.2 65.9 65.6 65.9 60.9 65.4 66.2 66. 1 2 3 4 the frames following NumPro (Wu et al., 2025; Ge et al., 2025). For the fast video, we sample at most 768 frames per video, each of which is encoded to 32 tokens (around 168*168 pixels). For each slow video, we sample at most 32 frames, each of which is encoded to 256 tokens (around 448*448 pixels). Due to memory constraints, we set the maximum number of steps to 3, which is around 16k context. During RL training, we mask the whole CoT if the model can not get final answer at the last step. During inference, the model can adaptively choose whether to zoom in on video clip or provide an answer before reaching the maximum number of reasoning steps. Upon reaching the final step, we prepend the prefix get the answer. to prompt the model to output the final response, thereby encouraging timely termination and preventing excessively long chains of thought. Evaluation is conducted with VLMEvalKit (Duan et al., 2024). Training Settings. In Stage 1, we fine-tune the model on 153k video instruction-following samples using learning rate of 1e5 and batch size of 128. Stage 2 involves further fine-tuning on 38k chain-of-thought (CoT) examples, with the same learning rate and batch size. In the final decoupled reinforcement finetuning stage, we select training samples from CG-Bench (Chen et al., 2025a) and MovieChat (Song et al., 2024) whose rollouts are neither entirely correct nor entirely incorrect for training. This stage uses learning rate of 1e6, batch size of 32, and performs 8 rollouts per sample."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "To demonstrate the long video understanding ability, we evaluate our model on common benchmarks Video-MME (Fu et al., 2025a), LongVideoBench (Wu et al., 2024), LVBench (Wang et al., 2025b), and MLVU (Zhou et al., 2025). These benchmarks contain videos exceeding one hour, posing great challenges to Video MLLMs. Results are shown in Table 1. First, compared with our baseline Qwen2.5-VL (Bai et al., 2025), LOVE-R1 outperforms it by an average of 3.1% points across 4 benchmarks, especially 6.2% points on LVBench. The strong performance shows that our adaptive zoom-in mechanism mitigates the dilemma between sampling density and spatial details faced by the uniform sampling mechanism. Second, compared with non-reasoning long video understanding models, which use complex token compression methods or rule-based key frame selection methods, our LOVE-R1 with only 16k context still achieves strong performance on VideoMME (66.2%), LongVideoBench (60.1%) and LVBench (48.2%). And we believe our method can combine with other token compression methods to process more frames and get higher performance. Further, compared with open-source reasoning video models, LOVE-R1 outperforms them by clear margin, demonstrating that simple reasoning can not tackle the long video understanding problem while our multi-step workflow works well on it. We also provide short video understanding benchmark results in Section A. And LOVE-R1 still achieves competitive performance compared with other models."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Comparisons with reasoning models with fixed frame sampling mechanisms. Reasoning Mode Single-step reasoning with 128 slow frames Single-step reasoning with 768 fast frames Multi-step reasoning with adaptive zoom-in ability"
        },
        {
            "title": "5.3 ABLATION STUDIES",
            "content": "Overall Long Medium Short 74.6 68.8 75.3 62.2 64.8 65.6 52.3 55.0 57.7 63.0 62.9 66.2 In Table 2a, we ablate different slow-fast templates. Effect of different video templates. In these experiments, we randomly zoom in on slow video. Although Templates (a) and (b) perform better in zero-shot evaluation, the performance even degrades after finetuning. We hypothesize it is because these templates are significantly different from the pretraining one. The single video in Template (a) has different resolutions and frame rates, while Template (b) segments the single video into multiple clips. The great template discrepancies can not be mitigated by small-scale finetuning thus degrading the performance. The fast video in Template (c) is still complete video which is in line with the pretraining one. The model only needs to link slow videos to the corresponding clips within the fast video. Thus, it can be adopted by finetuning on small scale of data and achieves the highest 64.4% score after finetuning. We use this template as the final template. Does decoupled reinforcement finetuning help? In multi-step reasoning, outcome rewards can not provide fine-grained process supervision, which hinders the zoom-in ability. Therefore, we propose decoupled reinforcement finetuning, which explicitly optimizes zoom-in CoTs by breaking down multi-step reasoning into multiple single-step reasoning. As shown in Table 2b, coupling multi-step and single-step optimization achieves the highest performance 66.2%, outperforming the standard GRPO algorithm by 0.5%. Performance comparisons across different training stages. To effectively build the new model while preserving the pretraining knowledge, we propose three-stage post-training recipe. Performance in each stage is shown at the bottom of Table 1. After slow-fast template finetuning, the model successfully adapts to the new video template with only 153k data. After the CoT cold start, the model is equipped with the basic adaptive zoom-in ability. The performance on LongVideoBench and LVBench is increased by 4.1% and 1.5% while preserving the performance on other benchmarks, showing the high-quality of our CoT data. Finally, after decoupled reinforcement finetuning, the overall performance is enhanced and the model in Stage 3 achieves the highest performance. Is the model really able to zoom in on an informative video clip? We provide three baselines: 1) no zoom-in videos, in which the model is only provided with the fast video; 2) unified zoom-in videos, in which we sample 32 uniform frames from the video as the slow video; 3) random zoomin videos, in which we randomly select 30-second clip as the slow video. As shown in Table 2c, compared to baselines, LOVE-R1 adaptively zooms in on clips of interest and achieves the highest performance, demonstrating that the model can truly select the clips related to the questions. Effect of different numbers of maximum inference iterations. In this work, we limited the number of reasoning steps to 3 due to resource constraints. In Table 2d, we study whether more reasoning steps can achieve higher performance. With only one reasoning step, the model can only provide the answer with the fast video and achieves the lowest performance 60.9%. With more reasoning steps, the model can zoom in on some clips of interest and further correct them by sampling other clips when selecting the wrong video clips. This multi-step reasoning paradigm greatly improves the model capacity and achieves higher performance. But the performance is saturated after 3 steps, possibly due to the training context length. We believe this paradigm has great scalability to more reasoning steps. Does the improved performance come from reasoning? We compare LOVE-R1 with single-step reasoning models which can not zoom in on video clips in Table 3. The two baselines are trained with standard single-step GRPO and the same dataset. The model with 128 slow frames gets rich spatial details while missing many temporal clues when tackling long videos. Thus, this model gets high short video performance and low long video results. In contrast, the model with 768 fast frames preserves more temporal clues and gets high performance on long videos but low performance on short videos. Our model with adaptive zoom-in ability achieves great balance between sampling density and spatial details thus achieving the highest performance. We also note that both baselines underperform Qwen2.5-VL (Bai et al., 2025). We hypothesize the reason is that Video-MME is"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Visualization of LOVE-R1 inference results. The video is taken from Video-MME (vid: edAu5 O4C54). dataset for evaluating perception ability rather than reasoning ability (unlike STEM). The same phenomenon can be found between Qwen3-VL-Instruct (Team, 2025) and Qwen3-VL-thinking. Visualization. We visualize three-step reasoning trace in Figure 4. The question asks which does not appear in the video. To answer the question, the model in the first step zooms in on the clip from 85 to 90 seconds and finds the Spider-Horse in it. And then, in the second step, the model zooms in on the clip from 30 to 40 seconds to find Spider-Dinosaur and Spider-Cat. Excluding these three options, the model gets the final correct answer in the third step. The adaptive zoom-in ability helps the model to find the most relevant video clips and contributes to the correctness of the final answer. More visualizations can be found in Section D."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose LOVE-R1, which formulates long video understanding as multi-step reasoning process. The model with the decision ability, zoom-in ability, and answering ability can adaptively zoom in on few video clips to get enough spatial details before providing the final answer. This slow-fast adaptive frame sampling mechanism achieves great trade-off between sampling density and spatial details. To provide fine-grained process rewards, we decouple the multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiment results show that our decouple reinforcement finetuning achieves higher performance than the standard GRPO algorithm, which is solely based on outcome rewards and the resulting"
        },
        {
            "title": "Preprint",
            "content": "LOVE-R1 achieves state-of-the-art performance on common long video understanding benchmarks. We hope our work can provide new paradigm to tackle the long video understanding problem. During the development of LOVE-R1, we find that the performance of recent models is largely constrained by the quality of existing long video understanding training data. Open-sourcing large-scale high-quality long video understanding datasets will make significant contribution to the community. Further, due to limited computation resources, our model context is limited to 16k. We believe that extending the context length, as demonstrated by LongVILA-R1 (Chen et al., 2025b), allows models to process more frames and conduct more reasoning steps, which can lead to further performance improvements and is left as future work."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cg-bench: Clue-grounded question answering benchmark for long video understanding. In ICLR, 2025a. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS, 2024a. Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Yihui He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024b. Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, and Song Han. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025b. Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. Scaling video-language models to 10k frames via hierarchical differential distillation. In ICML, 2025. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACM MM, 2024. Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. Miquel Farre, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https://huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Shenghao Fu, Qize Yang, Yuan-Ming Li, Yi-Xing Peng, Kun-Yu Lin, Xihan Wei, Jian-Fang Hu, Xiaohua Xie, and Wei-Shi Zheng. Vispeak: Visual instruction feedback in streaming videos. In ICCV, 2025b. Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Arc-hunyuan-video-7b: Structured video comprehension of real-world shorts. arXiv preprint arXiv:2507.20939, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Kai Hu, Feng Gao, Xiaohan Nie, Peng Zhou, Son Tran, Tal Neiman, Lingyun Wang, Mubarak Shah, Raffay Hamid, Bing Yin, et al. M-llm based video frame selection for efficient video understanding. In CVPR, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024b. Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024c. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, 2024d. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, 2024. Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Hao Chen, Jiebo Luo, Zicheng Liu, and Emad Barsoum. Unleashing hour-scale video training for long video-language understanding. arXiv preprint arXiv:2506.05332, 2025. Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025a. Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Chen. Et bench: Towards open-ended event-level video-language understanding. In NeurIPS, 2024. Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025b. Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, and Rongrong Ji. Video-rag: Visually-aligned retrieval-augmented long video comprehension. arXiv preprint arXiv:2411.13093, 2024. Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, and Miao Yin. AdacmË† 2: On understanding extremely long-term video with adaptive cross-modality memory reduction. In CVPR, 2025."
        },
        {
            "title": "Preprint",
            "content": "Yi-Xing Peng, Qize Yang, Yu-Ming Tang, Shenghao Fu, Kun-Yu Lin, Xihan Wei, and Wei-Shi Zheng. Actionart: Advancing multimodal large models for fine-grained human-centric video understanding. arXiv preprint arXiv:2504.18152, 2025. Ji Qi, Yuan Yao, Yushi Bai, Bin Xu, Juanzi Li, Zhiyuan Liu, and Tat-Seng Chua. Quicksviewer: An lmm for efficient video understanding via reinforced compression of video cubes. arXiv preprint arXiv:2504.15270, 2025. Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: Understanding hour-long videos with hybrid mamba-transformers. In ICCV, 2025. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Ran He, Caifeng Shan, Rongrong Ji, and Xing Sun. Long-vita: Scaling large multi-modal models to 1 million tokens with leading short-context accuracy. arXiv preprint arXiv:2502.05177, 2025. Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Videoxl: Extra-long vision language model for hour-scale video understanding. In CVPR, 2025. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. Yucheng Suo, Fan Ma, Linchao Zhu, Tianyi Wang, Fengyun Rao, and Yi Yang. From trial to triumph: Advancing long video understanding via visual context sample scaling and self-reward alignment. arXiv preprint arXiv:2503.20472, 2025. Qwen3-VL Team. Qwen3-vl. 99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research. latest-advancements-list, 2025. https://qwen.ai/blog?id= Shihao Wang, Guo Chen, De-An Huang, Zhiqi Li, Minghan Li, Guilin Liu, Jose M. Alvarez, Lei Zhang, and Zhiding Yu. Videoitg: Multimodal video understanding with instructed temporal grounding. arXiv preprint arXiv:2507.13353, 2025a. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. In ICCV, 2025b. Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, and Yali Wang. Videochat-a1: Thinking with long videos by chain-of-shot reasoning. arXiv preprint arXiv:2506.06097, 2025c. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In NeurIPS, 2024. Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, and Xu Yang. Number it: Temporal grounding videos like flipping manga. In CVPR, 2025. Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In CVPR, 2024. Yuan Xie, Tianshui Chen, Zheng Ge, and Lionel Ni. Video-mtr: Reinforced multi-turn reasoning for long video understanding. arXiv preprint arXiv:2508.20478, 2025."
        },
        {
            "title": "Preprint",
            "content": "Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, and Jingren Zhou. Humanomniv2: From understanding to omni-modal reasoning with context. arXiv preprint arXiv:2506.21277, 2025. Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, and Limin Wang. Timesuite: Improving mllms for long video understanding via grounded tuning. In ICLR, 2025. Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal tool-augmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416, 2025a. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024a. Shaojie Zhang, Jiahui Yang, Jianqin Yin, Zhenbo Luo, and Jian Luan. Q-frame: Query-aware frame selection and multi-resolution adaptation for video-llms. In ICCV, 2025b. Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025c. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024c. Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning. arXiv preprint arXiv:2503.05379, 2025a. Jiaxing Zhao, Qize Yang, Yixing Peng, Detao Bai, Shimin Yao, Boyuan Sun, Xiang Chen, Shenghao Fu, Xihan Wei, Liefeng Bo, et al. Humanomni: large vision-speech language model for humancentric video understanding. arXiv preprint arXiv:2501.15111, 2025b. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding. In CVPR, 2025. Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large language models with active perception via grpo. arXiv preprint arXiv:2505.21457, 2025."
        },
        {
            "title": "A MORE EXPERIMENT RESULTS",
            "content": "Table 4: Evaluation results on short video benchmarks. Models Size MVBench Video-MME (short) Video-LLaVA (Lin et al., 2024) LLaMA-VID (Li et al., 2024d) ShareGPT4Video (Chen et al., 2024a) LLaVA-NeXT-Video (Zhang et al., 2024b) VideoLLaMA2 (Cheng et al., 2024) VideoChat2 (Li et al., 2024b) LLaVA-OneVision (Li et al., 2024a) Vamba (Ren et al., 2025) VideoChat-T (Zeng et al., 2025) LongVILA (Chen et al., 2024b) LongVU (Shen et al., 2024) Video-R1 (Feng et al., 2025) LongVILA-R1 (Chen et al., 2025b) LOVE-R1 7B 7B 8B 7B 7B 7B 7B 10B 7B 7B 7B 7B 7B 7B 41.0 41.9 51.2 33.7 54.6 60.4 56.7 60.4 59.9 67.1 66.9 63.9 67.6 66. 45.3 - 48.3 - - 48.3 - - - 69.0 64.7 - 76.8 75.3 Experiment Results on Short Video Understanding Benchmarks. In Table 4, we compare LOVE-R1 with other models on common short video understanding benchmarks MVBench (Li et al., 2024b) and the short part of Video-MME (Fu et al., 2025a). Results show that LOVE-R1 also achieves competitive short video understanding performance."
        },
        {
            "title": "B COT DATA CONSTRUCTION",
            "content": "B.1 SOURCE DATA SELECTION To facilitate the evaluation of the accuracy of the zoom-in video intervals, we select two widely used grounded video question answering datasets NExT-GQA (Xiao et al., 2024) and CG-Bench (Chen et al., 2025a). Each question in these datasets is annotated with related timespans. NExT-GQA is short video dataset with videos within 3 minutes while CG-Bench is long video dataset with videos ranging from 10 to 60 minutes. Further, we select some videos ranging from 2 to 3 minutes in LLaVA-Video-178k (Zhang et al., 2024c) and use the pseudo timespans annotated by Wang et al. (2025a). To enhance the diversity of CoT data, in addition to the local question mentioned above, we also select some global questions in FineVideo (Farre et al., 2024) which have some keywords in the question, like main purpose, main characters, and main message. B.2 COT DATA CONSTRUCTION PIPELINE In this work, we decouple the whole pipeline as multiple single-step reasoning, thus we also collect single-step CoTs separately. For each question in NExT-GQA, CG-Bench, and LLaVA-Video-178k, we collect zoom-in CoT and an answer CoT. For FineVideo, we collect only an answer CoT for each question. The overall pipeline is shown in Figure 5: The prompt for filtering high-quality timespan annotations Question: {question} Answer: {answer} Do the provided frames contain the visual clues for the answer of the question? Yes or No?"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Our CoT data construction pipeline. To ensure the data quality, we perform strict data pre-processing and post-processing by filtering out low-quality annotations and CoTs. We also use strong reasoning model, Gemini 2.5 pro, to annotate CoT data, ensuring the content of CoTs is reasonable and high-quality. Question Cleaning. We empirically find that some questions in CG-Bench are extremely hard to answer and the corresponding timespan is inaccurate. We first prompt GPT-4o Hurst et al. (2024) to determine whether the timespan is relevant to the question with the prompt shown above. For questions with correct timespans, we then use Qwen2.5-VL 7B Bai et al. (2025) to filter out questions that can not be answered with ground truth video clips. The resulting data are used for annotation. The prompt for generating captions for short video clips Elaborate on the visual and narrative elements of the video briefly. Video Captioning. Since videos in CG-Bench are too long to localize the relevant video clips from the raw videos for recent APIs, we divide the long videos into multiple non-overlapped short clips (10s) and use Qwen2.5-VL 7B Bai et al. (2025) to caption each short video clip. The prompt for answer CoT construction Based on the video and the user question, first provide your reasoning, and then provide the option letter of your final answer within boxed{}. You can not use audio information during reasoning. The prompt for zoom-in CoT construction You do not know the answer and you should zoom in specific video segment to answer the question based on your reasoning following the instructions. ## INSTRUCTIONS - Based on the captions and the user question, first determine what information is needed to answer the question; then, provide your reasoning to localize the video segment that contains the key information and finally, provide the specific video segment within boxed{[start time, end time]}. - In the reasoning, you should specify how you localize the specific video segment. - The segment should be presented as [start time, end time] in integer seconds. For example, [110, 130]. - You do not know the question answer through the reasoning. - You can not use audio information during reasoning. CoT Construction. To ensure the high-quality of CoTs, we use the Gemini 2.5 pro (Comanici et al., 2025) as the annotator and use different prompts for answer CoTs and zoom-in CoTs as shown above. For short videos, we directly use the raw videos for annotation. For long videos, we use captions as input for zoom-in CoTs while ground truth video clips for answer CoTs. This design can efficiently collect high-quality CoTs under the constraints of APIs. Cleaning and Filtering. Data quality is essential for high-performance models. We perform some rule-based accuracy filtering and format filtering for the collected CoTs. For accuracy filtering, we"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Visualization of LOVE-R1 inference results. The video is taken from Video-MME (vid: -XpJeDGh8No). filter out CoTs with wrong answers or wrong zoom-in timespans (IoU < 0.1). For format filtering, CoTs with repeated patterns and undesired styles are filtered. For example, Gemini 2.5 pro may refer to the words in captions or the voice in videos during reasoning although we prompt it to act as if watching the true video and only focusing on visual information. To ensure consistency, the time representation is standardized to seconds. The resulting 38k CoTs are used for training. An example of our collected CoT is shown in Figure 1."
        },
        {
            "title": "C LLM USAGE",
            "content": "In this work, LLMs make contributions in two aspects: 1. We use LLMs to collect the CoT dataset as mentioned above. 2. We use LLMs to improve paper writing."
        },
        {
            "title": "D MORE VISUALIZATION",
            "content": "We provide more visualizations in Figure 6 and Figure 7. Results show that LOVE-R1 with adaptive zoom-in ability can select clips of interest to zoom in on, thus enhancing the long video understanding ability."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Visualization of LOVE-R1 inference results. The video is taken from Video-MME (vid: -qTAeVGl e8)."
        }
    ],
    "affiliations": [
        "Guangdong Province Key Laboratory of Information Security Technology, China",
        "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
        "Pazhou Laboratory (Huangpu), China",
        "Peng Cheng Laboratory, China",
        "School of Computer Science and Engineering, Sun Yat-sen University, China",
        "Tongyi Lab, Alibaba Group"
    ]
}