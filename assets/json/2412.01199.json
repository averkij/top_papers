{
    "paper_title": "TinyFusion: Diffusion Transformers Learned Shallow",
    "authors": [
        "Gongfan Fang",
        "Kunjun Li",
        "Xinyin Ma",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2$\\times$ speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/VainF/TinyFusion."
        },
        {
            "title": "Start",
            "content": "TinyFusion: Diffusion Transformers Learned Shallow Gongfan Fang*, Kunjun Li*, Xinyin Ma, Xinchao Wang National University of Singapore {gongfan, kunjun, maxinyin}@u.nus.edu, xinchao@nus.edu.sg 4 2 0 2 2 ] . [ 1 9 9 1 1 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, depth pruning method designed to remove redundant layers from diffusion transformers via endto-end learning. The core principle of our approach is to create pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce differentiable sampling technique to make pruning learnable, paired with co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft shallow diffusion transformer at less than 7% of the pretraining cost, achieving 2 speedup with an FID score of 2.86, outperforming competitors with comparable efficiency. Code is available at https://github.com/ VainF/TinyFusion 1. Introduction Diffusion Transformers have emerged as cornerstone architecture for generative tasks, achieving notable success in areas such as image [11, 26, 40] and video synthesis [25, 59]. This success has also led to the widespread availability of high-quality pre-trained models on the Internet, greatly accelerating and supporting the development of various downstream applications [5, 16, 53, 55]. However, pre-trained diffusion transformers usually come with con- *Equal contribution Corresponding author Figure 1. This work presents learnable approach for pruning the depth of pre-trained diffusion transformers. Our method simultaneously optimizes differentiable sampling process of layer masks and weight update to identify highly recoverable solution, ensuring that the pruned model maintains competitive performance after fine-tuning. siderable inference costs due to the huge parameter scale, which poses significant challenges for deployment. To resolve this problem, there has been growing interest from both the research community and industry in developing lightweight models [12, 23, 32, 58]. The efficiency of diffusion models is typically influenced by various factors, including the number of sampling steps [33, 43, 45, 46], operator design [7, 48, 52], computational precision [19, 30, 44], network width [3, 12] and depth [6, 23, 36]. In this work, we focus on model compression through depth pruning [36, 54], which removes entire layers from the network to reduce the latency. Depth pruning offers significant advantage in practice: it can achieve linear acceleration ratio relative to the compression rate on both parallel and non-parallel devices. For example, as will be demonstrated in this work, while 50% width pruning [12] only yields 1.6 speedup, pruning 50% of the layers results in 2 speedup. This makes depth pruning flexible and practical method for model compression. This work follows standard depth pruning framework: unimportant layers are first removed, and the pruned model is then fine-tuned for performance recovery. In the literature, depth pruning techniques designed for diffusion transformers or general transformers primarily focus on heuristic approaches, such as carefully designed importance scores [6, 36] or manually configured pruning 1 schemes [23, 54]. These methods adhere to loss minimization principle [18, 37], aiming to identify solutions that maintain low loss or error after pruning. This paper investigates the effectiveness of this widely used principle in the context of depth compression. Through experiments, we examined the relationship between calibration loss observed post-pruning and the performance after fine-tuning. This is achieved by extensively sampling 100,000 models via random pruning, exhibiting different levels of calibration loss in the searching space. Based on this, we analyzed the effectiveness of existing pruning algorithms, such as the feature similarity [6, 36] and sensitivity analysis [18], which indeed achieve low calibration losses in the solution space. However, the performance of all these models after finetuning often falls short of expectations. This indicates that the loss minimization principle may not be well-suited for diffusion transformers. Building on these insights, we reassessed the underlying principles for effective layer pruning in diffusion transformers. Fine-tuning diffusion transformers is an extremely time-consuming process. Instead of searching for model that minimizes loss immediately after pruning, we propose identifying candidate models with strong recoverability, enabling superior post-fine-tuning performance. Achieving this goal is particularly challenging, as it requires the integration of two distinct processes, pruning and fine-tuning, which involve non-differentiable operations and cannot be directly optimized via gradient descent. To this end, we propose learnable depth pruning method that effectively integrates pruning and fine-tuning. As shown in Figure 1, we model the pruning and finetuning of diffusion transformer as differentiable sampling process of layer masks [13, 17, 22], combined with co-optimized weight update to simulate future fine-tuning. Our objective is to iteratively refine this distribution so that networks with higher recoverability are more likely to be sampled. This is achieved through straightforward strategy: if sampled pruning decision results in strong recoverability, similar pruning patterns will have an increased probability of being sampled. This approach promotes the exploration of potentially valuable solutions while disregarding less effective ones. Additionally, the proposed method is highly efficient, and we demonstrate that suitable solution can emerge within few training steps. To evaluate the effectiveness of the proposed method, we conduct extensive experiments on various transformerbased diffusion models, including DiTs [40], MARs [29], SiTs [34]. The learnable approach is highly efficient. It is able to identify redundant layers in diffusion transformers with 1-epoch training on the dataset, which effectively crafts shallow diffusion transformers from pre-trained models with high recoverability. For instance, while the models pruned by TinyFusion initially exhibit relatively high calibration loss after removing 50% of layers, they recover quickly through fine-tuning, achieving significantly more competitive FID score (5.73 vs. 22.28) compared to baseline methods that only minimize immediate loss, using just 1% of the pre-training cost. Additionally, we also explore the role of knowledge distillation in enhancing recoverability [20, 23] by introducing MaskedKD variant. MaskedKD mitigates the negative impact of the massive or outlier activations [47] in hidden states, which can significantly affect the performance and reliability of fine-tuning. With MaskedKD, the FID score improves from 5.73 to 3.73 with only 1% of pre-training cost. Extending the training to 7% of the pre-training cost further reduces the FID to 2.86, just 0.4 higher than the original model with doubled depth. Therefore, the main contribution of this work lies in learnable method to craft shallow diffusion transformers from pre-trained ones, which explicitly optimizes the recoverability of pruned models. The method is general for various architectures, including DiTs, MARs and SiTs. 2. Related Works Network Pruning and Depth Reduction. Network pruning is widely used approach for compressing pre-trained diffusion models by eliminating redundant parameters [3, 12, 31, 51]. Diff-Pruning [12] introduces gradientbased technique to streamline the width of UNet, followed by simple fine-tuning to recover the performance. SparseDM [51] applies sparsity to pre-trained diffusion models via the Straight-Through Estimator (STE) [2], achieving 50% reduction in MACs with only 1.22 increase in FID on average. While width pruning and sparsity help reduce memory overhead, they often offer limited speed improvements, especially on parallel devices like GPUs. Consequently, depth reduction has gained significant attention in the past few years, as removing entire layers enables better speedup proportional to the pruning ratio [24, 27, 28, 36, 54, 56, 58]. Adaptive depth reduction techniques, such as MoD [41] and depth-aware transformers [10], have also been proposed. Despite these advances, most existing methods are still based on empirical or heuristic strategies, such as carefully designed importance criteria [36, 54], sensitivity analyses [18] or manually designed schemes [23], which often do not yield strong performance guarantee after fine-tuning. Efficient Diffusion Transformers. Developing efficient diffusion transformers has become an appealing focus within the community, where significant efforts have been made to enhance efficiency from various perspectives, including linear attention mechanisms [15, 48, 52], compact architectures [50], non-autoregressive transformers [4, 14, 38, 49], pruning [12, 23], quantization [19, 30, 44], feature Figure 2. The proposed TinyFusion method learns to perform differentiable sampling of candidate solutions, jointly optimized with weight update to estimate recoverability. This approach aims to increase the likelihood of favorable solutions that ensure strong post-finetuning performance. After training, local structures with the highest sampling probabilities are retained. caching [35, 57], etc. In this work, we focus on compressing the depth of pre-trained diffusion transformers and introduce learnable method that directly optimizes recoverability, which is able to achieve satisfactory results with low re-training costs. 3. Method 3.1. Shallow Generative Transformers by Pruning This work aims to derive shallow diffusion transformer by pruning pre-trained model. For simplicity, all vectors in this paper are column vectors. Consider L-layer transformer, parameterized by ΦLD = [ϕ1, ϕ2, , ϕL] , where each element ϕi encompasses all learnable parameters of transformer layer as D-dim column vector, which includes the weights of both attention layers and MLPs. Depth pruning seeks to find binary layer mask mL1 = [m1, m2, , mL] , that removes layer by: (cid:40) ϕi(xi), xi, xi+1 = miϕi(xi) + (1 mi)xi = if mi = 1, otherwise, (1) where the xi and ϕi(xi) refers to the input and output of layer ϕi. To obtain the mask, common paradigm in prior work is to minimize the loss after pruning, which can be formulated as minm Ex [L(x, Φ, m)]. However, as we will show in the experiments, this objective though widely adopted in discriminative tasks may not be well-suited to pruning diffusion transformers. Instead, we are more interested in the recoverability of pruned models. To achieve this, we incorporate an additional weight update into the optimization problem and extend the objective by: min Ex [L(x, Φ + Φ, m)] (cid:125) (cid:123)(cid:122) Recoverability: Post-Fine-Tuning Performance min Φ (cid:124) , (2) where Φ = {ϕ1, ϕ2, , ϕM } represents appropriate update from fine-tuning. The objective formulated by Equation 2 poses two challenges: 1) The non-differentiable nature of layer selection prevents direct optimization using gradient descent; 2) The inner optimization over the retained layers makes it computationally intractable to explore the entire search space, as this process necessitates selecting candidate model and fine-tuning it for evaluation. To address this, we propose TinyFusion that makes both the pruning and recoverability optimizable. 3.2. TinyFusion: Learnable Depth Pruning Probabilistic Perspective. This work models Equation 2 from probabilistic standpoint. We hypothesize that the mask produced by ideal pruning methods (might be not unique) should follow certain distribution. To model this, it is intuitive to associate every possible mask with probability value p(m), thus forming categorical distribution. Without any prior knowledge, the assessment of pruning masks begins with uniform distribution. However, directly sampling from this initial distribution is highly inefficient due to the vast search space. For instance, pruning 28-layer model by 50% involves evaluating (cid:0)28 (cid:1) = 40, 116, 600 possible solutions. To overcome this challenge, this work introduces an advanced and learnable algorithm capable of using evaluation results as feedback to iteratively refine the mask distribution. The basic idea is that if certain masks exhibit positive results, then other masks with similar pattern may also be potential solutions and thus should have higher likelihood of sampling in subsequent evaluations, allowing for more focused search on promising solutions. However, the definition of similarity pattern is still unclear so far. 14 3 Sampling Local Structures. In this work, we demonstrate that local structures, as illustrated in Figure 2, can serve as effective anchors for modeling the relationships between different masks. If pruning mask leads to certain local structures and yields competitive results after finetuning, then other masks yielding the same local patterns are also likely to be positive solutions. This can be achieved by dividing the original model into non-overlapping blocks, represented as Φ = [Φ1, Φ2, , ΦK] . For simplicity, we assume each block Φk = [ϕk1, ϕk2, , ϕkM ] contains exactly layers, although they can have varied lengths. Instead of performing global layer pruning, we propose an N:M scheme for local layer pruning, where, for each block Φk with layers, layers are retained. This results in set of local binary masks = [m1, m2, . . . , mK]. Similarly, the distribution of local mask mk is modeled using categorical distribution p(mk). We perform independent sampling of local binary masks and combine them for pruning, which presents the joint distribution: p(m) = p(m1) p(m2) p(mK) (3) If some local distributions p(mk) exhibit high confidence in the corresponding blocks, the system will tend to sample those positive patterns frequently and keep active explorations in other local blocks. Based on this concept, we introduce differential sampling to make the above process learnable. Differentiable Sampling. Considering the sampling process of local mask mk, which corresponds local block Φk and is modeled by categorical distribution p(mk). With the N:M scheme, there are (cid:0)M (cid:1) possible masks. We construct special matrix ˆmN :M to enumerate all possible masks. For example, 2:3 layer pruning will lead to the candidate matrix ˆm2:3 = [[1, 1, 0] , [1, 0, 1] , [0, 1, 1]]. In this case, each block will have three probabilities p(mk) = [pk1, pk2, pk3]. For simplicity, we omit mk and and use pi to represent the probability of sampling i-th element in ˆmN :M . popular method to make sampling process differentiable is Gumbel-Softmax [13, 17, 22]: (cid:32) (cid:33) = one-hot exp((gi + log pi)/τ ) exp((gj + log pj)/τ ) (cid:80) Figure 3. An example of forward propagation with differentiable pruning mask mi and LoRA for recoverability estimation. Notably, when τ 0, the STE gradients will approximate the true gradients, yet with higher variance which is negative for training [22]. Thus, scheduler is typically employed to initiate training with high temperature, gradually reducing it over time. Joint Optimization with Recoverability. With differentiable sampling, we are able to update the underlying probability using gradient descent. The training objective in this work is to maximize the recoverability of sampled masks. We reformulate the objective in Equation 2 by incorporating the learnable distribution: min {p(mk)} min Φ (cid:124) Ex,{mkp(mk)} [L(x, Φ + Φ, {mk}] (cid:125) (cid:123)(cid:122) Recoverability: Post-Fine-Tuning Performance , (6) where {p(mk)} = {p(m1), , p(mK)} refer to the categorical distributions for different local blocks. Based on this formulation, we further investigate how to incorporate the fine-tuning information into the training. We propose joint optimization of the distribution and weight update Φ. Our key idea is to introduce co-optimized update Φ for joint training. straightforward way to craft the update is to directly optimize the original network. However, the parameter scale in diffusion transformer is usually huge, and full optimization may make the training process costly and not that efficient. To this end, we show that ParameterEfficient Fine-Tuning methods such as LoRA [21] can be good choice to obtain the required Φ. For single linear matrix in Φ, we simulate the fine-tuned weights as: . (4) Wfine-tuned = + αW = + αBA, (7) where gi is random noise drawn from the Gumbel distribution Gumbel(0, 1) and τ refers to the temperature term. The output is the index of the sampled mask. Here StraightThrough Estimator [2] is applied to the one-hot operation, where the onehot operation is enabled during forward and is treated as an identity function during backward. Leveraging the one-hot index and the candidate set ˆmN :M , we can draw mask p(m) through simple index operation: = ˆm (5) where α is scalar hyperparameter that scales the contribution of W. Using LoRA significantly reduces the number of parameters, facilitating efficient exploration of different pruning decisions. As shown in Figure 3, we leverage the sampled binary mask value mi as the gate and forward the network with Equation 1, which suppresses the layer outputs if the sampled mask is 0 for the current layer. In addition, the previously mentioned STE will still provide non-zero gradients to the pruned layer, allowing it to be further updated. This is helpful in practice, since some layers 4 Method Depth #Param Iters IS FID sFID Prec. Recall Sampling it/s DiT-XL/2 [40] DiT-XL/2 [40] DiT-XL/2 [40] U-ViT-H/2 [1] ShortGPT [36] TinyDiT-D19 (KD) TinyDiT-D19 (KD) DiT-L/2 [40] U-ViT-L [1] U-DiT-L [50] Diff-Pruning-50% [12] Diff-Pruning-75% [12] ShortGPT [36] Flux-Lite [6] Sensitivity Analysis [18] Oracle (BK-SDM) [23] TinyDiT-D14 TinyDiT-D14 TinyDiT-D14 (KD) TinyDiT-D14 (KD) DiT-B/2 [40] U-DiT-B [50] TinyDiT-D7 (KD) 28 28 28 29 2819 2819 2819 24 21 22 28 28 2814 2814 2814 2814 2814 2814 2814 2814 12 22 147 675 7,000 278.24 675 2,000 240.22 675 1,000 157.83 500 265.30 501 100 132.79 459 100 242.29 459 500 251.02 459 458 1,000 196.26 300 221.29 287 400 246.03 204 100 186.02 338 100 169 83.78 66.10 100 340 54.54 100 340 100 340 70.36 100 141.18 340 100 151.88 340 500 198.85 340 100 207.27 340 500 234.50 340 130 1,000 119.63 85.15 400 500 166.91 - 173 2.27 2.73 5.53 2.30 7.93 2.90 2.55 3.73 3.44 3.37 3.85 14.58 22.28 25.92 21.15 7.43 5.73 3.92 3.73 2.86 10.12 16.64 5.87 4.60 4.46 4.60 5.60 5.25 4.63 4. 4.62 6.58 4.49 4.92 6.28 6.20 5.98 6.22 6.09 4.91 5.69 5.04 4.75 5.39 6.33 5.43 0.83 0.83 0.80 0.82 0.76 0.84 0.83 0.82 0.83 0.86 0.82 0.72 0.63 0.62 0.63 0.75 0.80 0.78 0.81 0.82 0.73 0.64 0.78 0.57 0.55 0.53 0.58 0.53 0.54 0. 0.54 0.52 0.50 0.54 0.53 0.56 0.55 0.57 0.55 0.55 0.58 0.54 0.55 0.55 0.63 0.53 6.91 6.91 6.91 8.21 10.07 10.07 10.07 9.73 13.48 - 10.43 13.59 13.54 13.54 13.54 13.54 13.54 13.54 13.54 13.54 28.30 - 26.81 Table 1. Layer pruning results for pre-trained DiT-XL/2. We focus on two settings: fast training with 100K optimization steps and sufficient fine-tuning with 500K steps. Both fine-tuning and Masked Knowledge Distillation (a variant of KD, see Sec. 4.4) are used for recovery. might not be competitive at the beginning, but may emerge as competitive candidates with sufficient fine-tuning. Pruning Decision. After training, we retain those local structures with the highest probability and discard the additional update Φ. Then, standard fine-tuning techniques can be applied for recovery. 4. Experiments 4.1. Experimental Settings Our experiments were mainly conducted on Diffusion Transformers [40] for class-conditional image generation on ImageNet 256 256 [8]. For evaluation, we follow [9, 40] and report the Frechet inception distance (FID), Sliding Frechet Inception Distance (sFID), Inception Scores (IS), Precision and Recall using the official reference images [9]. Additionally, we also extend our methods to other models, including MARs [29] and SiTs [34]. Experimental details can be found in the following sections and appendix. 4.2. Results on Diffusion Transformers DiT. This work focuses on the compression of DiTs [40]. We consider two primary strategies as baselines: the first Figure 4. Depth pruning closely aligns with the theoretical linear speed-up relative to the compression ratio. involves using manually crafted patterns to eliminate layers. For instance, BK-SDM [23] employs heuristic assumptions to determine the significance of specific layers, such as the initial or final layers. The second strategy is based on systematically designed criteria to evaluate layer importance, such as analyzing the similarity between block inputs and outputs to determine redundancy [6, 36]; this approach typically aims to minimize performance degradation after pruning. Table 1 presents representatives from both strategies, including ShortGPT [36], Flux-Lite [6], DiffPruning [12], Sensitivity Analysis [18] and BK-SDM [23], which serve as baselines for comparison. Additionally, 5 Method Depth Params Epochs FID IS MAR-Large MAR-Base TinyMAR-D16 SiT-XL/2 TinySiT-D 32 24 3216 28 2814 479 208 277 675 340 400 400 40 1,400 1.78 2.31 2.28 2.06 3.02 296.0 281.7 283.4 277.5 220.1 Table 2. Depth pruning results on MARs [29] and SiTs [34]. we evaluate our method against innovative architectural designs, such as UViT [1], U-DiT [50], and DTR [39], which have demonstrated improved training efficiency over conventional DiTs. Table 1 presents our findings on compressing pretrained DiT-XL/2 [40]. This model contains 28 transformer layers structured with alternating Attention and MLP layers. The proposed method seeks to identify shallow transformers with {7, 14, 19} sub-layers from these 28 layers, to maximize the post-fine-tuning performance. With only 7% of the original training cost (500K steps compared to 7M steps), TinyDiT achieves competitive performance relative to both pruning-based methods and novel architectures. For instance, DiT-L model trained from scratch for 1M steps achieves an FID score of 3.73 with 458M parameters. In contrast, the compressed TinyDiT-D14 model, with only 340M parameters and faster sampling speed (13.54 it/s vs. 9.73 it/s), yields significantly improved FID of 2.86. On parallel devices like GPUs, the primary bottleneck in transformers arises from sequential operations within each layer, which becomes more pronounced as the number of layers increases. Depth pruning mitigates this bottleneck by removing entire transformer layers, thereby reducing computational depth and optimizing the workload. By comparison, width pruning only reduces the number of neurons within each layer, limiting its speed-up potential. As shown in Figure 4, depth pruning closely matches the theoretical linear speed-up as the compression ratio increases, outperforming width pruning methods such as Diff-Pruning [12]. MAR & SiT. Masked Autoregressive (MAR) [29] models employ diffusion loss-based autoregressive framework in continuous-valued space, achieving high-quality image generation without the need for discrete tokenization. The MAR-Large model, with 32 transformer blocks, serves as the baseline for comparison. Applying our pruning method, we reduced MAR to 16-block variant, TinyMAR-D16, achieving an FID of 2.28 and surpassing the performance of the 24-block MAR-Base model with only 10% of the original training cost (40 epochs vs. 400 epochs). Our approach also generalizes to Scalable Interpolant Transformers (SiT) [34], an extension of the DiT architecture that employs flow-based interpolant framework to bridge data Figure 5. Distribution of calibration loss through random sampling of candidate models. The proposed learnable method achieves the best post-fine-tuning FID yet has relatively high initial loss compared to other baselines. Strategy Max. Loss Med. Loss Min. Loss Sensitivity ShortGPT [36] Flux-Lite [6] Oracle (BK-SDM) Learnable Loss 37.69 0.99 0.20 0.21 0.20 0.85 1.28 0.98 IS FID Prec. Recall NaN 149.51 73.10 70.36 66.10 54.54 141.18 151.88 NaN 6.45 20.69 21.15 22.28 25.92 7.43 5. NaN 0.78 0.63 0.63 0.63 0.62 0.75 0.80 NaN 0.53 0.58 0.57 0.56 0.55 0.55 0.55 Table 3. Directly minimizing the calibration loss may lead to non-optimal solutions. All pruned models are fine-tuned without knowledge distillation (KD) for 100K steps. We evaluate the following baselines: (1) Loss We randomly prune DiT-XL model to generate 100,000 models and select models with different calibration losses for fine-tuning; (2) Metric-based Methods such as Sensitivity Analysis and ShortGPT; (3) Oracle We retain the first and last layers while uniformly pruning the intermediate layers following [23]; (4) Learnable The proposed learnable method. and noise distributions. The SiT-XL/2 model, comprising 28 transformer blocks, was pruned by 50%, creating the TinySiT-D14 model. This pruned model retains competitive performance at only 7% of the original training cost (100 epochs vs. 1400 epochs). As shown in Table 2, these results demonstrate that our pruning method is adaptable across different diffusion transformer variants, effectively reducing the model size and training time while maintaining strong performance. 4.3. Analytical Experiments Is Calibration Loss the Primary Determinant? An essential question in depth pruning is how to identify redundant layers in pre-trained diffusion transformers. common approach involves minimizing the calibration loss, based on the assumption that model with lower calibration loss after pruning will exhibit superior performance. However, we demonstrate in this section that this hypothesis may not hold for diffusion transformers. We begin by examining the solution space through random depth pruning at 50% ratio, generating 100,000 candidate models with 6 Pattern IS FID sFID Prec. Recall 1:2 2:4 7: 1:2 2:4 7:14 1:2 2:4 7:14 LoRA LoRA LoRA Full Full Full Frozen Frozen Frozen 54.75 53.07 34. 53.11 53.63 45.03 45.08 48.09 34.09 33.39 34.21 49.41 35.77 34.41 38.76 39.56 37.82 49.75 29.56 27.61 28. 32.68 29.93 31.31 31.13 31.91 31.06 0.56 0.55 0.46 0.54 0.55 0.52 0.52 0.53 0.46 0.62 0.63 0. 0.61 0.62 0.62 0.60 0.62 0.56 Table 4. Performance comparison of TinyDiT-D14 models compressed using various pruning schemes and recoverability estimation strategies. All models are fine-tuned for 10,000 steps, and FID scores are computed on 10,000 sampled images with 64 timesteps. calibration losses ranging from 0.195 to 37.694 (see Figure 5). From these candidates, we select models with the highest and lowest calibration losses for fine-tuning. Notably, both models result in unfavorable outcomes, such as unstable training (NaN) or suboptimal FID scores (20.69), as shown in Table 3. Additionally, we conduct sensitivity analysis [18], commonly used technique to identify crucial layers by measuring loss disturbance upon layer removal, which produces model with low calibration loss of 0.21. However, this models FID score is similar to that of the model with the lowest calibration loss. Approaches like ShortGPT [36] and recent approach for compressing the Flux model [6], which estimate similarity or minimize mean squared error (MSE) between input and output states, reveal similar trend. In contrast, methods with moderate calibration losses, such as Oracle (often considered less competitive) and one of the randomly pruned models, achieve FID scores of 7.43 and 6.45, respectively, demonstrating significantly better performance than models with minimal calibration loss. These findings suggest that, while calibration loss may influence post-fine-tuning performance to some extent, it is not the primary determinant for diffusion transformers. Instead, the models capacity for performance recovery during fine-tuning, termed recoverability, appears to be more critical. Notably, assessing recoverability using traditional metrics is challenging, as it requires learning process across the entire dataset. This observation also explains why the proposed method achieves superior results (5.73) compared to baseline methods. Learnable Modeling of Recoverability. To overcome the limitations of traditional metric-based methods, this study introduces learnable approach to jointly optimize pruning and model recoverability. Table 3 illustrates different configurations of the learnable method, including the local pruning scheme and update strategies for recoverability estimation. For 28-layer DiT-XL/2 with fixed 50% Figure 6. Visualization of the 2:4 decisions in the learnable pruning, with the confidence level of each decision highlighted through varying degrees of transparency. More visualization results for 1:2 and 7:14 schemes are available in the appendix. 7 layer pruning rate, we examine three splitting schemes: 1:2, 2:4, and 7:14. In the 1:2 scheme, for example, every two transformer layers form local block, with one layer pruned. Larger blocks introduce greater diversity but significantly expand the search space. For instance, the 7:14 scheme divides the model into two segments, each retaining 7 layers, resulting in (cid:0)14 (cid:1) 2 = 6,864 possible solutions. Conversely, smaller blocks significantly reduce optimization difficulty and offer greater flexibility. When the distribution of one block converges, the learning on other blocks can still progress. As shown in Table 3, the 1:2 configuration achieves the optimal performance after 10K finetuning iterations. Additionally, our empirical findings underscore the effectiveness of recoverability estimation using LoRA or full fine-tuning. Both methods yield positive postfine-tuning outcomes, with LoRA achieving superior results (FID = 33.39) compared to full fine-tuning (FID = 35.77) under the 1:2 scheme, as LoRA has fewer trainable parameters (0.9% relative to full parameter training) and can adapt more efficiently to the randomness of sampling. Visualization of Learnable Decisions. To gain deeper insights into the role of the learnable method in pruning, we visualize the learning process in Figure 6. From bottom to top, the i-th curve represents the i-th layer of the pruned model, displaying its layer index in the original DiT-XL/2. This visualization illustrates the dynamics of pruning decisions over training iterations, where the transparency of each data point indicates the probability of being sampled. The learnable method shows its capacity to explore and handle various layer combinations. Pruning decisions for certain layers, such as the 7-th and 8-th in the compressed model, are determined quickly and remain stable throughout the process. In contrast, other layers, like the 0-th layer, require additional fine-tuning to estimate their recoverability. Notably, some decisions may change in the later stages 7 Figure 7. Images generated by TinyDiT-D14 on ImageNet 224224, pruned and distilled from DiT-XL/2. fine-tuning Strategy Init. Distill. Loss FID @ 100K fine-tuning Logits KD RepKD Masked KD (0.1σ) Masked KD (2σ) Masked KD (4σ) - - 2840.1 15.4 387.1 391. 5.79 4.66 NaN NaN 3.73 3.75 (a) DiT-XL/2 (Teacher) (b) TinyDiT-D14 (Student) Figure 8. Visualization of massive activations [47] in DiTs. Both teacher and student models display large activation values in their hidden states. Directly distilling these massive activations may result in excessively large losses and unstable training. Table 5. Evaluation of different fine-tuning strategies for recovery. Masked RepKD ignores those massive activations (x > kσx) in both teacher and student, which enables effective knowledge transfer between diffusion transformers. once these layers have been sufficiently optimized. The training process ultimately concludes with high sampling probabilities, suggesting converged learning process with distributions approaching one-hot configuration. After training, we select the layers with the highest probabilities for subsequent fine-tuning. 4.4. Knowledge Distillation for Recovery In this work, we also explore Knowledge Distillation (KD) as an enhanced fine-tuning method. As demonstrated in Table 5, we apply the vanilla knowledge distillation approach proposed by Hinton [20] to fine-tune TinyDiT-D14, using the outputs of the pre-trained DiT-XL/2 as teacher model for supervision. We employ Mean Square Error (MSE) loss to align the outputs between the shallow student model and the deeper teacher model, which effectively reduces the FID at 100K steps from 5.79 to 4.66. Masked Knowledge Distillation. Additionally, we evaluate representation distillation (RepKD) [23, 42] to transfer hidden states from the teacher to the student. It is important to note that depth pruning does not alter the hidden dimension of diffusion transformers, allowing for direct alignment of intermediate hidden states. For practical implementation, we use the block defined in Section 3.2 as the basic unit, ensuring that the pruned local structure in the pruned DiT aligns with the output of the original structure in the teacher model. However, we encountered significant training difficulties with this straightforward RepKD approach due to massive activations in the hidden states, where both teacher and student models occasionally exhibit large activation values, as shown in Figure 8. Directly distilling these extreme activations can result in excessively high loss values, impairing the performance of the student model. This issue has also been observed in other transformer-based generative models, such as certain LLMs [47]. To address this, we propose Masked RepKD variant that selectively excludes these massive activations during knowledge transfer. We employ simple thresholding method, µx < kσx, which ignores the loss associated with these extreme activations. As shown in Table 5, the Masked RepKD approach with moderate thresholds of 2σ and 4σ achieves satisfactory results, demonstrating the robustness of our method. Generated Images. In Figure 7, We visualize the generated images of the learned TinyDiT-D14, distilled from an 8 off-the-shelf DiT-XL/2 model. More visualization results for SiTs and MARs can be found in the appendix. 5. Conclusions This work introduces TinyFusion, learnable method for accelerating diffusion transformers by removing redundant layers. It models the recoverability of pruned models as an optimizable objective and incorporates differentiable sampling for end-to-end training. Our method generalizes to various architectures like DiTs, MARs and SiTs."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [2] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic arXiv preprint neurons for conditional computation. arXiv:1308.3432, 2013. [3] Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 821830, 2024. [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. [6] Javier Martın Daniel Verdu. Flux.1 lite: Distilling flux1.dev for efficient text-to-image generation. 2024. [7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [10] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael arXiv preprint Depth-adaptive transformer. Auli. arXiv:1910.10073, 2019. [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. 9 In Forty-first International Conference on Machine Learning, 2024. [12] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In Advances in Neural Information Processing Systems, 2023. [13] Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, and Xinchao Wang. Maskllm: Learnable semi-structured sparsity for large language models. arXiv preprint arXiv:2409.17481, 2024. [14] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024. [15] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang. Dimba: Transformermamba diffusion models. arXiv preprint arXiv:2406.01159, 2024. [16] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, MingMing Cheng, and Shuicheng Yan. Editanything: Empowering unparalleled flexibility in image editing and generation. In Proceedings of the 31st ACM International Conference on Multimedia, Demo track, 2023. [17] Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: series of lectures. US Government Printing Office, 1954. [18] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [19] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. DistillarXiv preprint ing the knowledge in neural network. arXiv:1503.02531, 2(7), 2015. [21] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Categorical arXiv preprint [22] Eric Jang, Shixiang Gu, and Ben Poole. reparameterization with gumbel-softmax. arXiv:1611.01144, 2016. [23] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [24] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened llama: simple depth pruning for large language models. arXiv preprint arXiv:2402.02834, 11, 2024. [25] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. [26] Black Forest Labs. FLUX, 2024. [27] Youngwan Lee, Yong-Ju Lee, and Sung Ju Hwang. Ditpruner: Pruning diffusion transformer models for text-toimage synthesis using human preference scores. [28] Youngwan Lee, Kwanyong Park, Yoorhim Cho, Yong-Ju Lee, and Sung Ju Hwang. Koala: self-attention matters in knowledge distillation of latent diffusion models for memory-efficient and fast image synthesis. arXiv e-prints, pages arXiv2312, 2023. [29] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [30] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. In Proceedings Q-diffusion: Quantizing diffusion models. of the IEEE/CVF International Conference on Computer Vision, pages 1753517545, 2023. [31] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. [32] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. [33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [34] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [35] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching, 2024. [36] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redunarXiv preprint arXiv:2403.03853, dant than you expect. 2024. [37] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016. [38] Zanlin Ni, Yulin Wang, Renping Zhou, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Shiji Song, Yuan Yao, and Gao Huang. Revisiting non-autoregressive transformers for efficient image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7007 7016, 2024. [39] Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routing for diffusion models. arXiv preprint arXiv:2310.07138, 2023. [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 10 [41] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. [42] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. [43] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [44] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. [45] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. [47] Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. [48] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. [49] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. [50] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. arXiv preprint arXiv:2405.02730, 2024. [51] Kafeng Wang, Jianfei Chen, He Li, Zhenpeng Mi, and Jun Zhu. Sparsedm: Toward sparse efficient diffusion models. arXiv preprint arXiv:2404.10445, 2024. [52] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [53] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and MingHsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4): 139, 2023. [54] Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui. Width & depth pruning for vision transformers. In Conference on Artificial Intelligence (AAAI), 2022. [55] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023. [56] Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024. [57] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. [58] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-to-image generation on mobile devices. arXiv preprint arXiv:2311.16567, 2023. [59] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 11 TinyFusion: Diffusion Transformers Learned Shallow"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Experimental Details Models. Our experiments evaluate the effectiveness of three models: DiT-XL, MAR-Large, and SiT-XL. Diffusion Transformers (DiTs), inspired by Vision Transformer (ViT) principles, process spatial inputs as sequences of patches. The DiT-XL model features 28 transformer layers, hidden size of 1152, 16 attention heads, and 2 2 patch size. It employs adaptive layer normalization (AdaLN) to improve training stability, comprising 675 million parameters and trained for 1400 epochs. Masked Autoregressive models (MARs) are diffusion transformer variants tailored for autoregressive image generation. They utilize continuousvalued diffusion loss framework to generate high-quality outputs without discrete tokenization. The MAR-Large model includes 32 transformer layers, hidden size of 1024, 16 attention heads, and bidirectional attention. Like DiT, it incorporates AdaLN for stable training and effective token modeling, with 479 million parameters trained over 400 epochs. Finally, Scalable Interpolant Transformers (SiTs) extend the DiT framework by introducing flow-based interpolant methodology, enabling more flexible bridging between data and noise distributions. While architecturally identical to DiT-XL, the SiT-XL model leverages this interpolant approach to facilitate modular experimentation with interpolant selection and sampling dynamics. Datasets. We prepared the ImageNet 256 256 dataset by applying center cropping and adaptive resizing to maintain the original aspect ratio and minimize distortion. The images were then normalized to mean of 0.5 and standard deviation of 0.5. To augment the dataset, we applied random horizontal flipping with probability of 0.5. To accelerate training without using Variational Autoencoder (VAE), we pre-extracted features from the images using pre-trained VAE. The images were mapped to their latent representations, normalized, and the resulting feature arrays were saved for direct use during training. Training Details The training process began with obtaining pruned models using the proposed learnable pruning method as illustrated in Figure 12. Pruning decisions were made by joint optimization of pruning and weight updates through LoRA with block size. In practice, the block size is 2 for simplicity and the models were trained for 100 epochs, except for MAR, which was trained for 40 epochs. To enhance post-pruning performance, the Masked Knowledge Distillation (RepKD) method was employed during the recovery phase to transfer knowledge from teacher modFigure 9. 1:2 Pruning Decisions Figure 10. 2:4 Pruning Decisions Figure 11. 7:14 Pruning Decisions els to pruned student models. The RepKD approach aligns the output predictions and intermediate hidden states of the pruned and teacher models, with further details provided in the following section. Additionally, as Exponential Moving Averages (EMA) are updated and used during image generation, an excessively small learning rate can weaken EMAs effect, leading to suboptimal outcomes. To address this, progressive learning rate scheduler was implemented to gradually halve the learning rate throughout training. The 1 Figure 12. Learnable depth pruning on local block corresponds to the masked distillation loss applied to the hidden states, as illustrated in Figure 13, which encourages alignment between the intermediate representations of the pruned model and the original model. The corresponding hyperparameters αKD, αDiff and αRep can be found in Table 6. Hidden State Alignment. The masked distillation loss LRep is critical for aligning the intermediate representations of the student and teacher models. During the recovery phase, each layer of the student model is designed to replicate the output hidden states of corresponding two-layer local block from the teacher model. Depth pruning does not alter the internal dimensions of the layers, enabling direct alignment without additional projection layers. For models such as SiTs, where hidden state losses are more pronounced due to their unique interpolant-based architecture, smaller coefficient β is applied to LRep to mitigate potential training instability. The gradual decrease in β throughout training further reduces the risk of negative impacts on convergence. Iterative Pruning and Distillation. Table 7 assesses the effectiveness of iterative pruning and teacher selection strategies. To obtain TinyDiT-D7, we can either directly prune DiT-XL with 28 layers or craft TinyDiT-D14 first and then iteratively produce the small models. To investigate the impact of teacher choice and the method for obtaining the initial weights of the student model, we derived the initial weights of TinyDiT-D7 by pruning both pre-trained model and crafted intermediate model. Subsequently, we used both the trained and crafted models as teachers for the pruned student models. Across four experimental settings, pruning and distilling using the crafted intermediate model yielded the best performance. Notably, models pruned from the crafted model outperformed those pruned from the pre-trained model regardless of the teacher model employed in the distillation process. We attribute this suFigure 13. Masked knowledge distillation with 2:4 blocks. details of each hyperparameter are provided in Table 6. 7. Visualization of Pruning Decisions Figures 9, 10 and 11 visualize the dynamics of pruning decisions during training for the 1:2, 2:4, and 7:14 pruning schemes. Different divisions lead to varying search spaces, which in turn result in various solutions. For both the 1:2 and 2:4 schemes, good decisions can be learned in only one epoch, while the 7:14 scheme encounters optimization diffi- (cid:1)=3,432 candidates, which is too culty. This is due to the (cid:0)14 huge and thus cannot be adequately sampled within single epoch. Therefore, in practical applications, we use the 1:2 or 2:4 schemes for learnable layer pruning. 8. Details of Masked Knowledge Distillation Training Loss. This work deploys standard knowledge distillation to learn good student model by mimicking the pre-trained teacher. The loss function is formalized as: = αKD LKD + αDiff LDiff + β LRep (8) Here, LKD denotes the Mean Squared Error between the outputs of the student and teacher models. LDiff represents the original pre-training loss function. Finally, LRep Model Optimizer Cosine Sched. Teacher αKD αGT β Grad. Clip Pruning Configs AdamW(lr=2e-4, wd=0.0) DiT-D19 AdamW(lr=2e-4, wd=0.0 DiT-D14 AdamW(lr=2e-4, wd=0.0) DiT-D7 AdamW(lr=2e-4, wd=0.0) SiT-D14 MAR-D16 AdamW(lr=2e-4, wd=0.0) ηmin = 1e-4 ηmin = 1e-4 ηmin = 1e-4 ηmin = 1e-4 ηmin = 1e-4 DiT-XL DiT-XL DiT-D14 SiT-XL MAR-Large 0.9 0.9 0.9 0.9 0.9 0.1 0.1 0.1 0.1 0.1 1e-2 0 1e-2 0 1e-2 0 2e-4 0 1e-2 0 1.0 1.0 1.0 1.0 1.0 LoRA-1:2 LoRA-1:2 LoRA-1:2 LoRA-1:2 LoRA-1:2 Table 6. Training details and hyper-parameters for mask training Teacher Model Pruned From IS FID sFID Prec. Recall"
        },
        {
            "title": "Learning Rate",
            "content": "IS"
        },
        {
            "title": "FID sFID",
            "content": "Prec."
        },
        {
            "title": "Recall",
            "content": "DiT-XL/2 DiT-XL/2 TinyDiT-D14 TinyDiT-D14 DiT-XL/2 TinyDiT-D14 DiT-XL/2 TinyDiT-D14 29.46 51.96 28.30 57.97 56.18 36.69 58.73 32.47 26.03 28.28 29.53 26.05 0.43 0.53 0.41 0. 0.51 0.59 0.50 0.60 lr=2e-4 lr=1e-4 lr=5e-5 207.27 194.31 161.40 3.73 4.10 6.63 5.04 5.01 6.69 0.8127 0.8053 0. 0.5401 0.5413 0.5705 Table 7. TinyDiT-D7 is pruned and distilled with different teacher models for 10k, sample steps is 64, original weights are used for sampling rather than EMA. Table 8. The effect of Learning rato for TinyDiT-D14 finetuning w/o knowledge distillation Learning Rate. We also search on some key hyperparameters such as learning rates in Table 8. We identify the effectiveness of lr=2e-4 and apply it to all models and experiments. 10. Visulization Figure 15 and 16 showcase the generated images from TinySiT-D14 and TinyMAR-D16, which were compressed from the official checkpoints. These models were trained using only 7% and 10% of the original pre-training costs, respectively, and were distilled using the proposed masked knowledge distillation method. Despite compression, the models are capable of generating plausible results with only 50% of depth. 11. Limitations In this work, we explore learnable depth pruning method to accelerate diffusion transformer models for conditional image generation. As Diffusion Transformers have shown significant advancements in text-to-image generation, it is valuable to conduct systematic analysis of the impact of layer removal within the text-to-image tasks. Additionally, there exist other interesting depth pruning strategies that need to be studied, such as more fine-grained pruning strategies that remove attention layers and MLP layers independently instead of removing entire transformer blocks. We leave these investigations for future work. Figure 14. FID and training steps. perior performance to two factors: first, the crafted models structure is better adapted to knowledge distillation since it was trained using distillation method; second, the reduced search space facilitates finding more favorable initial state for the student model. 9. Analytical Experiments Training Strategies Figure 14 illustrates the effectiveness of standard fine-tuning and knowledge distillation (KD), where we prune DiT-XL to 14 layers and then apply various fine-tuning methods. Figure 3 presents the FID scores across 100K to 500K steps. It is evident that the standard fine-tuning method allows TinyDiT-D14 to achieve performance comparable to DiT-L while offering faster inference. Additionally, we confirm the significant effectiveness of distillation, which enables the model to surpass DiTL at just 100K steps and achieve better FID scores than the 500K standard fine-tuned TinyDiT-D14. This is because the distillation of hidden layers provides stronger supervision. Further increasing the training steps to 500K leads to significantly better results. 3 Figure 15. Generated images from TinySiT-D14 Figure 16. Generated images from TinyMAR-D"
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}