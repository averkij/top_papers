{
    "paper_title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal",
    "authors": [
        "Wenhao Zeng",
        "Yaoning Wang",
        "Chao Hu",
        "Yuling Shi",
        "Chengcheng Wan",
        "Hongyu Zhang",
        "Xiaodong Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 8 9 5 0 . 8 0 5 2 : r Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal Wenhao Zeng1, Yaoning Wang2, Chao Hu1, Yuling Shi1, Chengcheng Wan3, Hongyu Zhang4, Xiaodong Gu1 * 1Shanghai Jiao Tong University, 2Fudan University, 3East China Normal University, 4Chongqing University zengwh cs@sjtu.edu.cn, xiaodong.gu@sjtu.edu.cn Abstract Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent tradeoffs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, SurprisAlbased Pruning), novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables logic-aware pruning by selecting logically essential reasoning steps based on novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4 v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving competitive accuracy of 36.19% in Pass@1. Our results highlight promising direction for building powerful and efficient LRMs.1 Figure 1: Illustration of CoT pruning by ASAP. The original CoT emitted by LRMs contains redundant branches (highlighted in red dashed boxes). ASAP prunes them in the first stage. It then identifies the first tokens of reasoning steps (marked in blue), whose surprisal scores are computed to guide the second-stage pruning."
        },
        {
            "title": "Introduction",
            "content": "Recently, the emergence of Large Reasoning Models (LRMs), including OpenAIs o1 (Jaech et al. 2024) and DeepSeek-R1 (Guo et al. 2025), has demonstrated remarkable capabilities across wide range of domains. LRMs empower LLMs with the Chain-of-Thought (CoT) prompting paradigm (Wei et al. 2022), which prompts models to think step by step. By generating longer reasoning traces during inference, LRMs are capable of handling complex code reasoning tasks (Shi et al. 2024). Despite achieving significant performance gains, the current LRMs come at heavy computational cost when the *Corresponding author 1Code and model available at https://github.com/Zengwh02/ ASAP. CoTs scale up. Long CoTs often contain substantial redundancy, particularly pronounced in code generation tasks, where LRMs frequently explore diverse strategies or engage in self-debugging (Bi et al. 2024; Xie et al. 2025; Wu et al. 2025; Qu et al. 2025). This not only increases inference and training costs but also suggests counterintuitive insight: removing redundant reasoning traces may improve both efficiency and performance. To improve the efficiency of CoT reasoning, growing body of research has emerged on efficient reasoning (Qu et al. 2025). prominent line of work is CoT compression for efficient model fine-tuning, which aims to distill concise yet effective reasoning patterns into the model. For example, TokenSkip (Xia et al. 2025a) adapts general-purpose context compressors such as Selective Context (Li et al. 2023) and LLMLingua-2 (Pan et al. 2024). However, these token-level compression methods are fundamentally ill-suited for code reasoning as they damage the logic of an entire code block within the trace, even if only variable name is removed. The compressed context makes it hard for models to learn effective reasoning patterns. more promising direction is step-level pruning. For example, SPIRIT (Cui et al. 2025) prunes the reasoning steps while preserving structural and semantic integrity. However, these approaches face deeper challenge: How to reliably estimate the logical importance of reasoning step. SPIRIT relies on perplexity (PPL), but we argue that PPL is an inadequate metric as it primarily reflects linguistic fluency rather than logical necessity and has shown limitations in long-context scenarios (Hu et al. 2024; Fang et al. 2024). To overcome these challenges, we propose ASAP: novel framework for Anchor-guided, SurprisAl-based Pruning of CoTs. Our core insight, illustrated in Figure 1, is identifying and pruning redundant steps in CoTs while preserving coherent logical path. In the first stage, ASAP leverages the inherent structural and logical nature of code to generate step-by-step reasoning path. The path serves as an anchor for coarse-grained pruning, efficiently identifying and preserving the core reasoning backbone. In the second stage, it enables fine-grained logic-aware pruning by selecting the first tokens of reasoning steps based on new First-Token Surprisal metric. The resulting CoT retains only the steps that introduce novel and meaningful information. By fine-tuning on the pruned CoTs, we teach large reasoning model to generate and leverage these compact reasoning patterns at inference time, enabling efficient reasoning in coding tasks. We validate our approach through extensive experiments on the DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1Distill-Llama-8B (Guo et al. 2025) across suite of challenging code generation benchmarks, including HumanEval (Chen et al. 2021), HumanEval+ (Liu et al. 2023), LiveCodeBench (Jain et al. 2024), and LeetCodeDataset (Xia et al. 2025b). Our resulting model establishes new state-of-the-art in the performance-efficiency trade-off. Notably, on the challenging LiveCodeBench v4 v5 benchmark, ASAP achieves 36.19% Pass@1 while reducing token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline. Our main contributions are summarized as follows: We propose ASAP, novel two-stage pruning framework that combines coarse-grained anchor-guided pruning with fine-grained surprisal-based pruning to identify and retain logically critical steps. We are the first to propose and empirically validate FirstToken Surprisal as superior metric to perplexity for measuring the logical importance of CoT steps in the code generation task. Extensive experiments on multiple code generation benchmarks demonstrate that models fine-tuned on CoTs pruned by ASAP achieve state-of-the-art accuracy while substantially reducing both training and inference costs."
        },
        {
            "title": "2.3 Efficient Reasoning via Fine-Tuning\nThe increasingly lengthy reasoning traces impose substan-\ntial costs, which have motivated a growing body of work\nfocused on efficient reasoning (Qu et al. 2025). Several re-\ncent approaches aim to compress CoTs while preserving\ntheir effectiveness. C3oT (Kang et al. 2025) uses GPT-4\nas a high-quality external compressor to generate shorter\nCoTs that retain essential reasoning content. TokenSkip (Xia\net al. 2025a) enables controllable compression by identify-\ning and removing less important tokens. SPIRIT (Cui et al.\n2025) identifies important reasoning steps by detecting per-\nplexity shifts when individual steps are removed. Another\nparadigm involves compressing CoTs into continuous latent\nrepresentations, as explored by Coconut (Hao et al. 2024),\nCCoT (Cheng and Van Durme 2024), CODI (Shen et al.",
            "content": "Figure 2: The overall framework of ASAP. The process begins with training sample consisting of question, an intermediate CoT, and the answer. In Stage 1, an LLM generates Direct CoT (Cdirect) from the question and answer. Cdirect acts as an anchor to prune the Original CoT (Corigin) into Coarse-grained Pruned CoT (Ccoarse). In Stage 2, we compute the first-token surprisal for each step in Ccoarse to remove less critical steps, yielding the final Fine-grained Pruned CoT (C ) for fine-tuning. 2025), etc. Our method, ASAP, distinguishes itself from these methods through its explicit two-stage pruning framework, which first generates an anchor for coarse-grained pruning, followed by fine-grained refinement based on first-token surprisal. By explicitly pruning at the step level, ASAP produces logically condensed human-readable CoTs."
        },
        {
            "title": "3 Methodology",
            "content": "i=1."
        },
        {
            "title": "3.1 Overall Framework\nGiven a training corpus of triples {(Qi, Ci, Ai)}N\ni=1—where\neach Qi denotes a question, Ci = {s1, . . . , sN } is the inter-\nmediate reasoning steps, known as chain-of-thoughts (CoT),\ngenerated by a large reasoning model L, and Ai represents\nthe predicted answer to the question. Our goal is to com-\npress each Ci into a concise one C′\ni| ≪ |Ci|\nwhile the LRM maintains the quality of generated reason-\ning steps and answers when fine-tuned on the compressed\ntriplets {Qi, C′",
            "content": "i such that i, Ai}N We propose ASAP, novel method for CoT compression, as illustrated in Figure 2. ASAP adopts coarse-to-fine pruning framework that firstly removes redundant steps from given reasoning trace and then refines it by selecting the most critical logical steps. In the first stage called Anchorguided Pruning, ASAP infers concise, anchor reasoning path called Direct Thoughts (Cdirect) based on the (Q, A) pairs. Guided by Cdirect as an anchor, ASAP prunes the original CoT into coarse-grained pruned CoT (Ccoarse) using an LLM, which retains the main logical backbone while preliminarily removing digressions and redundancies. In the second stage called Surprisal-based Refining, ASAP performs logic-aware refinement to Ccoarse. new metric called First-Token Surprisal is designed to measure the logical importance of reasoning step and then iteratively filter steps with low surprisal, yielding the final CoT (denoted as in our formulation). Finally, all {Q, , A} tuplets are used to fine-tune large reasoning model for downstream tasks, which internalizes concise reasoning patterns and substantially improves inference efficiency."
        },
        {
            "title": "3.2 Anchor-guided Pruning\nGenerating the Direct CoT Anchor We prompt an LLM\nto generate a concise and coherent anchor reasoning trace,\nCdirect, based on the question and answer. An anchor is de-\nfined as a structured, step-by-step explanation that outlines\nhow to derive the answer from the question. This leverages\nthe inherent logical and procedural nature of code reason-\ning. The resulting Cdirect serves as a logical backbone—an\nanchor—to guide the subsequent pruning of the more elab-\norate Corigin. The prompt used for the generation is shown\nin Appendix E.",
            "content": "Pruning with Pattern Matching Using Cdirect as reference, we prompt the LLM to prune the original CoT C. Specifically, the LLM is instructed to: 1) remove unnecessary reasoning steps from C; 2) retain all key supporting content that aligns with the logic of Cdirect; and 3) crucially, preserve the original wording without introducing new information. The prompt used for pruning is shown in Appendix E. To mitigate LLM hallucination, we incorporate validation step based on Gestalt Pattern Matching (Black 2004), which validates structural and semantic alignment with C. Specifically, we design pattern-matching algorithm that verifies whether each step in Ccoarse corresponds to matching step in while preserving their original order. The matching is performed using Gestalt Pattern Matching (Black 2004) as text similarity metric. pruning is considered valid only if all steps in Ccoarse achieve similarity score above predefined threshold τ when matched against sequential steps in C. This guarantees that Ccoarse forms consistent substructure of the original reasoning trace. The full pattern-matching algorithm is detailed in Algorithm 1. We leverage high-temperature sampling, which provides the necessary diversity to efficiently re-prompt failed cases, ensuring that valid Ccoarse can be eventually generated. Algorithm 1: Pattern Matching Require: Original CoT C, Coarse-grained Pruned CoT Ccoarse, Threshold τ ound match False while origin idx < Length(Sorigin) do sorigin Sorigin[origin idx] score GestaltSimilarity(sorigin, scoarse) if score τ then Ensure: True if Ccoarse is valid, False otherwise. 1: function PATTERNMATCH(C, Ccoarse, τ ) Sorigin SplitStepsByBlankLine(C) 2: Scoarse SplitStepsByBlankLine(Ccoarse) 3: origin idx 0 4: for each step scoarse in Scoarse do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end function ound match True origin idx origin idx + 1 break end if origin idx origin idx + 1 end while if not ound match then end for return True return False end if"
        },
        {
            "title": "3.3 Surprisal-based Refining\nFollowing the coarse-grained pruning, we perform a metic-\nulous, logic-aware refinement in Ccoarse to identify more\nsubtle redundancies within the core reasoning path.",
            "content": "First-Token Surprisal as Logical Importance We introduce First-Token Surprisal as novel metric to precisely quantify the logical importance of each step, enabling us to iteratively filter out the least informative ones and produce the final highly condensed CoT. In information theory, entropy is used to quantify the average level of uncertainty or information associated with variables potential states or possible outcomes (Shannon 1948). In the era of LLMs, this concept can be applied to each token generated by the model. Given an LLM generates sequence of tokens x1, x2, . . . , xN , the conditional probability of the n-th token is given by p(xnx<n), derived from the models logits. Token-level entropy measures the models uncertainty over the entire vocabulary for the next token (Malinin and Gales 2020; Kuhn, Gal, and Farquhar 2023). It is the surprisal over all possible outcomes: H(xnx<n) = (cid:88) vV p(vx<n) log p(vx<n) (1) In large reasoning models, the first token of each step often signals logical transition or continuation. High entropy indicates the models uncertainty, suggesting that the step may introduce semantically or logically significant information. Conversely, low entropy typically reflects predictable continuations or syntactic fillers, implying limited contribution to the overall reasoning process (Wang et al. 2025; Cheng et al. 2025). However, since the actual next token is known in the sequence, we directly compute the surprisal of the target token, more efficient and targeted metric that quantifies the unexpectedness of the target token xt: S(xtx<n) = log p(xtx<n) (2) Surprisal quantifies how unexpected given token is to the model. We focus specifically on the first-token surprisal of each reasoning step. high surprisal value at the beginning of step indicates logical saliencethat is, the step either initiates new line of reasoning or introduces novel information. In contrast, low surprisal value suggests continuation of prior context or redundant elaboration. Accordingly, we prune low-surprisal steps to obtain more concise and informative reasoning traces. Iterative Pruning Algorithm Based on the first-token surprisal metric, we propose length-controllable iterative pruning algorithm. As shown in Algorithm 2, it calculates the surprisal score for the first token of each reasoning step within the Ccoarse. It then iteratively removes the step with the lowest surprisal score until the total length of the CoT falls below predefined token budget Lmax. This process yields the final , which is highly compressed while retaining the most logically significant steps. Algorithm 2: Iterative Pruning via First-Token Surprisal Require: Coarse-grained Pruned CoT Ccoarse, Max Tokens return Ccoarse if Length(T (Ccoarse)) Lmax then end if SplitStepsByBlankLine(Ccoarse) SurprisalScores CalculateAll(S, M, ) StepsT oP rune SortByScore(S, SurprisalScores) Scurrent for each step sprune in StepsT oP rune do Lmax, Model , Tokenizer Ensure: Fine-grained Pruned CoT 1: function FINEGRAINEDPRUNE(Ccoarse, Lmax, M, ) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: Join(Scurrent) 18: return 19: 20: end function Stemp Scurrent {sprune} Ctemp Join(Stemp) if Length(T (Ctemp)) Lmax then end if Scurrent Stemp Scurrent Stemp break"
        },
        {
            "title": "3.4 Supervised Fine-tuning\nFollowing the two-stage pruning by our ASAP framework,\nwe construct the final training dataset using the compressed",
            "content": "triplets {(Qi, i, Ai)}N i=1. For each instance, we concatenate the pruned CoT (C i) and the final answer (Ai) to form the complete target response, denoted as Ri. We then finetune large reasoning model on this dataset. The training goal is to minimize the negative log-likelihood of the target response tokens, conditioned on the input question. Formally, the loss is defined as: = (cid:88) Ri (cid:88) i=1 j= log Pθ(ri,jQi, ri,<j) (3) where ri,j is the j-th token of the target response Ri, and θ represents the parameters of the model being fine-tuned. This supervised fine-tuning process effectively distills the knowledge from our pruning framework into the model. By training on these compact and logically salient examples, the model learns to internalize the efficient reasoning patterns."
        },
        {
            "title": "4.1 Experimental Setup\nModels and Datasets All experiments are conducted\non the DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-\nDistill-Llama-8B (Guo et al. 2025), with DeepSeek-R1-\nDistill-Qwen-7B as the default backbone across all settings.\nWe use the Python subset of the CodeForces-CoTs (Hug-\nging Face 2025) dataset for training. The dataset consists\nof high-quality Chain-of-Thought (CoT) samples generated\nby DeepSeek-R1, making it particularly suitable as training\ndata for competitive programming reasoning tasks.",
            "content": "Benchmarks We evaluate our method on suite of widely used code generation benchmarks that span range of difficulty levels: HumanEval (Chen et al. 2021) consists of 164 handcrafted programming problems. HumanEval+ (Liu et al. 2023) extends HumanEval with approximately 80 times more unique test cases. LiveCodeBench (Jain et al. 2024) is comprehensive, contamination-free benchmark that continuously aggregates new problems from competitive programming platforms. We use two subsets: v1 v3 (612 problems) and v4 v5 (268 problems). LeetCodeDataset (Xia et al. 2025b) provides highquality evaluation set of 228 problems collected from LeetCode. It covers over 90% of Python problems on the platform, with each problem featuring over 100 test cases. Baselines We compare our method against comprehensive set of baselines: Zero-shot: The original model without any task-specific fine-tuning. Original: The model is fine-tuned on the original, unSelective Context compressed CoTs from the training data. (Li et al. 2023): Identifies and prunes redundant lexical units in the input using selfinformation, aiming to retain only the most informative tokens. LLMLingua-2 (Pan et al. 2024): Distills GPT-4s token importance signals into lightweight Transformer encoder, which is trained as token classifier. This approach compresses content using the token classifier. TokenSkip (Xia et al. 2025a): Trains model to selectively skip less important tokens in CoT, allowing for controllable compression based on token importance scores. SPIRIT (Cui et al. 2025): Identifies critical reasoning steps by measuring perplexity shiftssteps are retained if their removal significantly increases the perplexity of the remaining text. Except for the zero-shot baseline, all other methods involve fine-tuning the model using CoTs that have been processed according to their strategies. Metrics To comprehensively evaluate each method, we report both accuracy and inference efficiency: Accuracy is measured by Pass@1, which denotes the percentage of problems correctly solved on the first attempt. Efficiency is evaluated using two metrics: the average number of generated tokens and the average generation latency measured in seconds per sample. Implementation Details All models are fully fine-tuned (i.e., full-parameter fine-tuning) using Unsloth (Daniel Han and team 2023) for efficiency. For inference, we utilize the vLLM (Kwon et al. 2023) engine to enable fast and efficient decoding. To ensure reproducibility, we use greedy decoding and disable prefix caching. The default generation token budget is adjusted based on task difficulty: 6K for HumanEval and HumanEval+, and 10K for LiveCodeBench and LeetCodeDataset. All experiments are conducted on NVIDIA H20 GPUs. Additional implementation details are provided in Appendix A."
        },
        {
            "title": "4.2 Main Results\nThe comprehensive results of our method compared to all\nbaselines are presented in Table 1. The results show that\nthe model fine-tuned on CoTs pruned by ASAP consistently\nachieves the best trade-off between accuracy and efficiency.\nIt outperforms all baselines in terms of accuracy, while gen-\nerating the fewest tokens and achieving the lowest genera-\ntion latency across all benchmarks. Statistical analysis using\nthe Wilcoxon signed-rank test confirms that the improve-\nments of our method over all baselines are statistically sig-\nnificant (p < 0.05).",
            "content": "A deeper analysis of the results reveals clear distinction between token-level and step-level pruning strategies. Token-level baselines like Selective Context, LLMLingua2, and TokenSkip exhibit significant performance degradation compared to the original CoTs. This is because the token removal disrupts the syntactic structure and semantic coherence of the original reasoning steps. Consequently, the fine-tuning data becomes fragmented and grammatically unnatural, making it difficult for the model to learn the intended logical flow of the CoT. Step-level methods, such as Methods Zero-shot Original HumanEval LiveCodeBench LeetCodeDataset HE HE+ v1 v3 v4 v5 All Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc Tok Lat 73.78 3051 1.16 78.05 2973 1.12 68.29 3051 1.16 75.61 2973 1.12 42.16 7088 3.59 52.12 6611 3.15 25.37 8336 5.15 30.97 8289 4.83 19.74 8680 4.95 25.00 8485 4.72 Selective Context LLMLingua-2 TokenSkip SPIRIT 59.76 2979 1.13 71.34 3075 1.19 76.83 2823 1.07 83.54 2764 1.07 54.88 2979 1.13 68.29 3075 1.19 73.78 2823 1.07 75.61 2764 1.07 30.23 7025 3.75 38.89 6953 3.60 32.35 7095 3.85 50.82 6524 3.09 16.79 8558 5.35 22.76 8474 5.31 20.15 8400 5.37 33.58 7892 4.62 15.79 8461 4.90 17.54 8513 4.81 18.42 8503 4.87 25.00 8186 4.45 ASAP 84.15 2464 0.98 78.66 2464 0.98 54.74 5177 2.09 36.19 6035 2.61 27.63 7541 3.48 Table 1: Experimental results of different methods with DeepSeek-R1-Distill-Qwen-7B. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. The best results are highlighted in bold, and the second-best are underlined. SPIRIT, perform significantly better than token-level pruning methods, due to the preservation of sentence-level integrity. While SPIRIT improves efficiency over the Original with comparable accuracy, our method achieves higher efficiency and accuracy at the same time. This improvement is particularly pronounced on the challenging LiveCodeBench v4 v5 benchmark: ASAP reduces the average number of generated tokens by 23.5% (from 7892 to 6035) and lowers generation latency by 43.5% (from 4.62s to 2.61s), while also achieving 7.8% improvement in accuracy (Pass@1 increases from 33.58% to 36.19%). These results verify our core hypothesis that pruning reasoning chains based on firsttoken surprisal is more effective than perplexity or tokenlevel pruning."
        },
        {
            "title": "4.3 Ablation Study\nTo validate the contribution and necessity of each compo-\nnent in our two-stage pruning framework, we conduct a de-\ntailed ablation study. Specifically, we evaluate the following\nthree variants:",
            "content": "ASAP w/o Coarse-grained Pruning: This variant skips Stage 1 and applies only the fine-grained surprisal-based pruning directly to the original CoT. ASAP w/o Fine-grained Pruning: This variant applies only the coarse-grained anchor-guided pruning from Stage 1, omitting the surprisal-based refinement in Stage 2. ASAP w/o Any Pruning: Equivalent to the Original baseline, where the model is fine-tuned on the full, uncompressed CoT without any pruning. Table 2 reports results on LiveCodeBench v4 v5, which is representative of the consistent trends observed across benchmarks. Additional results are included in Appendix B. The ablation study demonstrates that both pruning stages are essential and mutually complementary for achieving optimal accuracy and efficiency. First, removing the coarsegrained pruning stage (w/o Coarse-grained Pruning) leads to drop in both accuracy and efficiency. While the accuracy decrease is modest, the generation latency increases by substantial 76.2% (from 2.61s to 4.60s), underscoring the importance of stage 1 in eliminating redundancy and narrowing the search space for subsequent processing. Second, removing the fine-grained pruning stage (w/o Fine-grained Pruning) results in significant degradation across all metrics. The accuracy drops by 12.4% (Pass@1 decreases from 36.19% to 31.72%) relative to the ASAP, and efficiency improvements are largely lost. This highlights that Stage 2our surprisal-based pruning mechanismis central to precisely identifying and preserving the most critical logical steps. In summary, the two stages operate in synergy: the coarse-grained anchor-guided pruning preliminarily removes redundant steps, while the fine-grained surprisalbased pruning further extracts the logical core of the reasoning trace. This two-stage process is key to ASAPs strong performance, significantly outperforming any single-stage variant. Variants Acc Tok Lat ASAP w/o Coarse-grained Pruning w/o Fine-grained Pruning w/o Any Pruning 36.19 35.07 31.72 30.97 6035 7735 8061 8289 2.61 4.60 4.83 4.83 Table 2: Ablation study of different pruning strategies for ASAP on LiveCodeBench v4 v5. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds."
        },
        {
            "title": "4.4\nTo evaluate the performance scalability and resource sensi-\ntivity of our method, we analyze its behavior under varying\ninference-time token budgets (i.e., the maximum number of\ntokens the model is allowed to generate). We compare ASAP\nagainst the three strong baselines—SPIRIT, Original, and\nZero-shot—across all benchmarks, and observe consistent\ntrends. For clarity, we present results on LiveCodeBench\nv4 v5 under six budget settings ranging from 2K to 12K to-",
            "content": "kens. Results for other benchmarks and additional efficiency statistics are provided in Appendix C. As shown in Figure 3, our findings highlight three key advantages of ASAP: First, ASAP consistently outperforms all baselines across all budget settings on the benchmark. This demonstrates the robustness of our method under different resource constraints. Second, ASAP exhibits smooth performance scaling with respect to the token budget. Unlike baseline methods such as Zero-shot, whose accuracy can fluctuate as the budget increases (e.g., Zero-shot shows decline from 8K to 10K), ASAP reliably improves with more available tokens. This predictable scaling behavior enables users to confidently configure budget limits to meet specific accuracy or latency targets. Third, ASAP achieves superior performance-efficiency trade-offs. For example, ASAP with just an 8K token budget achieves higher accuracy than SPIRIT and Original at much larger 12K budget. These results further validate the practical utility of ASAP in realworld scenarios with constrained resources and varying performance requirements. Figure 3: Performance of ASAP on LiveCodeBench v4 v5 under different token budgets."
        },
        {
            "title": "4.5 Training Efficiency\nA central motivation for CoT compression is to reduce the\ncomputational burden during training and inference, specif-\nically shortening the lengthy reasoning traces typically re-\nquired by large reasoning models. Fine-tuning on com-\npressed CoTs not only lowers training-time resource con-\nsumption but also helps the model internalize efficient rea-\nsoning patterns. To this end, we aim to achieve a high com-\npression ratio across all methods while preserving the core\nlogical steps. More details are provided in Appendix A.",
            "content": "To quantify the training efficiency gains, we report two key metrics in Table 3: the average number of tokens per sample and the average training time measured in seconds per step. The results highlight the training efficiency advantage of ASAP. By generating the most compact yet logically rich CoTs, our approach significantly reduces training overhead. Compared to the uncompressed baseline (Original), our method reduces the number of training tokens by 75.6% and shortens training time by 60.7%. These savings substantially exceed those achieved by all other baselines. ASAP not only achieves more favorable accuracyefficiency trade-off at inference time, but also enables more resource-efficient training process, making it practical and cost-effective solution for real-world deployment. Methods Original Tokens Time 80.11 Selective Context LLMLingua-2 TokenSkip SPIRIT 6722 (-48.4%) 6919 (-46.9%) 9813 (-24.6%) 6082 (-53.3%) 63.41 (-20.9%) 65.25 (-18.6%) 77.27 (-3.6%) 57.45 (-28.3%) ASAP 3178 (-75.6%) 31.48 (-60.7%) Table 3: Training efficiency comparison. We report the average number of tokens per sample and training time measured in seconds per step. Percentages indicate the reduction relative to the Original baseline."
        },
        {
            "title": "4.6 Generalization to Different Architectures\nTo validate the generalizability of ASAP, we replicate our\nmain experiments on the DeepSeek-R1-Distill-Llama-8B.\nFollowing the same experimental protocol, we compare\nASAP against three strong baselines: Zero-shot, Original,\nand SPIRIT. We observe consistent trends across all bench-\nmarks, so for brevity, we present representative results on\ntwo key benchmarks: LiveCodeBench v4 v5 and LeetCode-\nDataset in Table 4. The results across all benchmarks are\nreported in the Appendix D.",
            "content": "The results on DeepSeek-R1-Distill-Llama-8B are highly consistent with our findings on the DeepSeek-R1-DistillQwen-7B, confirming that the effectiveness of ASAP generalizes beyond specific model family. As shown in the Table 4, ASAP achieves the highest accuracy on both benchmarks, and the improvements in efficiency are even more pronounced. On LiveCodeBench, for instance, ASAP not only surpasses the accuracy of the Original baseline (32.84% vs. 31.34%) but also generates 49.1% fewer tokens and reduces latency by over 3x (from 8.60s to 2.69s). Compared to the Zero-shot baseline, our method reduces token generation by 50.9% and latency by remarkable 69.8%. This suggests that ASAP is particularly effective in identifying and distilling the core reasoning patterns, validating its robustness and broad applicability for improving reasoning efficiency across different model families."
        },
        {
            "title": "5 Conclusion\nIn this paper, we addressed the challenge of balancing per-\nformance and efficiency in Chain-of-Thought (CoT) reason-\ning, where longer reasoning traces improve accuracy but in-\ntroduce significant redundancy and computational overhead.\nWe proposed ASAP, a novel two-stage pruning framework\nthat first generates an anchor for coarse-grained pruning,\nfollowed by a fine-grained refinement based on first-token\nsurprisal. Extensive experiments demonstrate that models\nfine-tuned on CoTs pruned by ASAP achieve state-of-the-\nart results on multiple code generation benchmarks while",
            "content": "Methods LiveCodeBench LeetCodeDataset Acc Tok Lat Acc Tok Lat Zero-shot Original SPIRIT 25.00 8508 8.90 31.34 8202 8.60 30.22 7913 8. 27.19 8358 8.65 26.32 8413 8.85 26.75 8449 8.73 ASAP 32.84 4175 2.69 27.63 3792 2.42 Table 4: Experimental results of different methods with DeepSeek-R1-Distill-Llama-8B. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. The best results are highlighted in bold. substantially reducing both training and inference costs. Although our current work focuses on competitive programming tasks within the code generation domain, we believe the framework is general and broadly applicable. References Bi, Z.; Zhang, N.; Jiang, Y.; Deng, S.; Zheng, G.; and Chen, H. 2024. When do program-of-thought works for reasoning? In Proceedings of the AAAI conference on artificial intelligence, volume 38, 1769117699. Black, P. E. 2004. Ratcliff/Obershelp pattern recognition. Dictionary of algorithms and data structures, 17. Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. D. O.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cheng, D.; Huang, S.; Zhu, X.; Dai, B.; Zhao, W. X.; Zhang, Z.; and Wei, F. 2025. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758. Cheng, J.; and Van Durme, B. 2024. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171. Cui, Y.; He, P.; Zeng, J.; Liu, H.; Tang, X.; Dai, Z.; Han, Y.; Luo, C.; Huang, J.; Li, Z.; et al. 2025. Stepwise perplexityguided refinement for efficient chain-of-thought reasoning in large language models. arXiv preprint arXiv:2502.13260. Daniel Han, M. H.; and team, U. 2023. Unsloth. Fang, L.; Wang, Y.; Liu, Z.; Zhang, C.; Jegelka, S.; Gao, J.; Ding, B.; and Wang, Y. 2024. What is Wrong with Perplexity for Long-context Language Modeling? arXiv preprint arXiv:2410.23771. Fang, Y.; Sun, T.; Shi, Y.; and Gu, X. 2025. Attentionrag: Attention-guided context pruning in retrieval-augmented generation. arXiv preprint arXiv:2503.10720. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Hao, S.; Sukhbaatar, S.; Su, D.; Li, X.; Hu, Z.; Weston, J.; and Tian, Y. 2024. Training large language models arXiv preprint to reason in continuous latent space. arXiv:2412.06769. Hu, Y.; Huang, Q.; Tao, M.; Zhang, C.; and Feng, Y. 2024. Can Perplexity Reflect Large Language Models Ability in Long Text Understanding? In The Second Tiny Papers Track at ICLR 2024. Hugging Face. 2025. Open R1: fully open reproduction of DeepSeek-R1. Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; El-Kishky, A.; Low, A.; Helyar, A.; Madry, A.; Beutel, A.; Carney, A.; et al. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Jain, N.; Han, K.; Gu, A.; Li, W.-D.; Yan, F.; Zhang, T.; Wang, S.; Solar-Lezama, A.; Sen, K.; and Stoica, I. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Jiang, H.; Wu, Q.; Lin, C.-Y.; Yang, Y.; and Qiu, L. 2023. LLMLingua: Compressing Prompts for Accelerated InferIn Proceedings of the ence of Large Language Models. 2023 Conference on Empirical Methods in Natural Language Processing, 1335813376. Jiang, H.; Wu, Q.; Luo, X.; Li, D.; Lin, C.-Y.; Yang, Y.; and Qiu, L. 2024. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt CompresIn Proceedings of the 62nd Annual Meeting of the sion. Association for Computational Linguistics (Volume 1: Long Papers), 16581677. Kang, Y.; Sun, X.; Chen, L.; and Zou, W. 2025. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 2431224320. Kimi; Du, A.; Gao, B.; Xing, B.; Jiang, C.; Chen, C.; Li, C.; Xiao, C.; Du, C.; Liao, C.; et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. SemanKuhn, L.; Gal, Y.; and Farquhar, S. 2023. tic uncertainty: Linguistic invariances for uncertainty esarXiv preprint timation in natural language generation. arXiv:2302.09664. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Lei, B.; Liao, C.; Ding, C.; et al. 2023. Boosting logical reasoning in large language models through new framework: The graph of thought. arXiv preprint arXiv:2308.08614. Li, Y.; Dong, B.; Guerin, F.; and Lin, C. 2023. Compressing Context to Enhance Inference Efficiency of Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 63426353. Li, Z.; Liu, Y.; Su, Y.; and Collier, N. 2025. Prompt Compression for Large Language Models: Survey. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao, Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36: 1180911822. Yu, Q.; Zhang, Z.; Zhu, R.; Yuan, Y.; Zuo, X.; Yue, Y.; Dai, W.; Fan, T.; Liu, G.; Liu, L.; et al. 2025. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Human Language Technologies (Volume 1: Long Papers), 71827195. Ling, Z.; Fang, Y.; Li, X.; Huang, Z.; Lee, M.; Memisevic, R.; and Su, H. 2023. Deductive verification of chain-ofthought reasoning. Advances in Neural Information Processing Systems, 36: 3640736433. Is Liu, J.; Xia, C. S.; Wang, Y.; and Zhang, L. 2023. your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36: 2155821572. Malinin, A.; and Gales, M. 2020. Uncertainty estimation in autoregressive structured prediction. arXiv preprint arXiv:2002.07650. Pan, Z.; Wu, Q.; Jiang, H.; Xia, M.; Luo, X.; Zhang, J.; Lin, Q.; Ruhle, V.; Yang, Y.; Lin, C.-Y.; et al. 2024. LLMLingua2: Data Distillation for Efficient and Faithful Task-Agnostic In Findings of the Association for Prompt Compression. Computational Linguistics ACL 2024, 963981. Qu, X.; Li, Y.; Su, Z.; Sun, W.; Yan, J.; Liu, D.; Cui, G.; Liu, D.; Liang, S.; He, J.; et al. 2025. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614. Shannon, C. E. 1948. mathematical theory of communication. The Bell system technical journal, 27(3): 379423. Shen, Z.; Yan, H.; Zhang, L.; Hu, Z.; Du, Y.; and He, Y. 2025. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074. Shi, Y.; Wang, S.; Wan, C.; and Gu, X. 2024. From code to correctness: Closing the last mile of code generation with hierarchical debugging. arXiv preprint arXiv:2410.01215. Wang, S.; Yu, L.; Gao, C.; Zheng, C.; Liu, S.; Lu, R.; Dang, K.; Chen, X.; Yang, J.; Zhang, Z.; et al. 2025. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837. Wu, Y.; Wang, Y.; Ye, Z.; Du, T.; Jegelka, S.; and Wang, Y. 2025. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266. Xia, H.; Leong, C. T.; Wang, W.; Li, Y.; and Li, W. 2025a. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. Xia, Y.; Shen, W.; Wang, Y.; Liu, J. K.; Sun, H.; Wu, S.; Hu, J.; and Xu, X. 2025b. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms. arXiv preprint arXiv:2504.14655. Xie, T.; Gao, Z.; Ren, Q.; Luo, H.; Hong, Y.; Dai, B.; Zhou, J.; Qiu, K.; Wu, Z.; and Luo, C. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Implementation Details Software and Hardware For fine-tuning, we utilized the unsloth library2 for its memory-efficient optimizations. For inference, we employed the vLLM engine3 to maximize throughput and efficiency. All experiments were conducted on NVIDIA H20 GPUs and Intel Xeon Platinum 8480+ CPUs. performed Configuration We Fine-tuning fullparameter fine-tuning for all models in our experiments. Key hyperparameters included precision set to bf16, num train epochs set to 10, and max seq length of 16384. We used per device train batch size of 1 with gradient accumulation steps set to 16, resulting in an effective batch size of 16. For the optimizer, we used AdamW with cosine with min lr learning rate scheduler. The warmup ratio was set to 0.03, and the schedulers min lr rate was 0.1 of the peak learning rate. To stabilize training, we applied gradient clipping with max grad norm of 0.2. Based on preliminary experiments, we set the peak learning rate to 4 105 for the DeepSeek-R1-Distill-Qwen-7B and 2 105 for the DeepSeek-R1-Distill-Llama-8B. Due to the high computational cost of full-parameter fine-tuning, the model is fine-tuned by single run with fixed random seed 42. Inference and Evaluation Protocol All inference benchmarks were run using the vLLM engine with dtype set to bfloat16 and gpu memory utilization set to 0.9. To ensure deterministic and reproducible outputs, we set the sampling temperature to 0.0 and set enable prefix caching to False. Baseline Details Following established practices, we used is consistent scoring model; as our primary model DeepSeek-R1-Distill checkpoints, we employed DeepSeekR1-Distill-Qwen-7B for all model-scoring tasks. To ensure fair comparison, we standardize the input format across all methods by preserving the original question and final answer, and applying compression only to the CoT reasoning steps. To balance compression ratio and content retention, we set the target compression ratio to 0.5 for all baseline methods, except for TokenSkip, where we follow its original design that allows controllable compression ratio between 0.5 and 1.0. Additionally, since the original SPIRIT method is computationally expensive when applied to extremely long CoTs, we adopt modified version to ensure fair comparison: specifically, we compute perplexity once per reasoning step and iteratively remove steps until the target ratio is met. This variant retains the core idea of SPIRIT while improving scalability in our evaluation setting. Hyperparameters for Our Method Our method involves several stages. For the LLM-guided Coarse-grained Pruning stage, we employed DeepSeek-V3 for economic reasons. When generating the Direct CoT, we used deterministic setting (temperature=0.0, top p=1.0), while 2https://pypi.org/project/unsloth/2025.5.6/ 3https://pypi.org/project/vllm/0.8.4/ for making the final pruning result, we increased exploration (temperature=2.0, top p=1.0). For Pattern Matching, the similarity threshold τ was set to 0.6. Finally, during Surprisal-based Fine-grained Pruning, the maximum token budget was set to 4096 to ensure deep level of compression."
        },
        {
            "title": "Strategies",
            "content": "To validate the contribution and necessity of each component in our two-stage pruning framework, we conduct detailed ablation study. Specifically, we evaluate the following three variants: ASAP w/o Coarse-grained Pruning, ASAP w/o Fine-grained Pruning and ASAP w/o Any Pruning. We present results on the HumanEval, HumanEval+, LiveCodeBench v1 v3, and LeetCodeDatsets benchmarks in Table 5, Table 6, Table 7, and Table 8. Variants Acc Tok Lat ASAP w/o Coarse-grained Pruning w/o Fine-grained Pruning w/o Any Pruning 84.15 82.32 71.34 78.05 2464 2839 2897 0.98 1.10 1.10 1.12 Table 5: Ablation study of different pruning strategies for ASAP on HumanEval. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. Variants Acc Tok Lat ASAP w/o Coarse-grained Pruning w/o Fine-grained Pruning w/o Any Pruning 78.66 78.05 67.07 75. 2464 2839 2897 2973 0.98 1.10 1.10 1.12 Table 6: Ablation study of different pruning strategies for ASAP on HumanEval+. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. Variants Acc Tok Lat ASAP w/o Coarse-grained Pruning w/o Fine-grained Pruning w/o Any Pruning 54.74 53.92 51.14 52.12 5177 6107 6599 6611 2.09 2.77 3.20 3.15 Table 7: Ablation study of different pruning strategies for ASAP on LiveCodeBench v1 v3. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. Variants Acc Tok Lat Prompt for Coarse-grained Pruning Compress the given thinking by referring to the provided solution. The goal is to remove irrelevant reasoning paths while retaining all content along the core reasoning path. Compression must be based on thinking, ensuring that the original wording and structure are preserved as much as possible. Follow these strict rules: 1. Use thinking as the foundation: Do not rewrite or replace its content with solutiononly use solution to determine which parts are relevant. 2. Remove unnecessary reasoning: Aggressively remove alternative paths that are not part of the core reasoning path. 3. Retain key supporting content: Keep examples, reflections, and tests that help illustrate, verify, or analyze the core reasoning path. 4. Preserve original words: Do not paraphrase, reorder, or change any words. 5. Do not add new words: Do not introduce new concepts, symbols, or abbreviations. If you understand, compress the following thinking based on the given solution. Solution: ``` {solution} ``` Thinking: ``` {think} ``` The compressed thinking is: ASAP w/o Coarse-grained Pruning w/o Fine-grained Pruning w/o Any Pruning 27.63 24.12 25.44 25.00 7541 7954 8326 8485 3.48 3.75 4.77 4. Table 8: Ablation study of different pruning strategies for ASAP on LeetCodeDataset. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds."
        },
        {
            "title": "Budgets",
            "content": "To evaluate the performance scalability and resource sensitivity of our method, we analyze its behavior under varying inference-time token budgets (i.e., the maximum number of tokens the model is allowed to generate). We compare ASAP with three strong baselinesSPIRIT, Original, and Zero-shoton HumanEval, LiveCodeBench, and LeetcodeDataset. For HumanEval, we evaluate the performance under four budget settings, ranging from 1K to 6K tokens. For LiveCodeBench and LeetCodeDataset, we evaluate the performance under six budget settings, ranging from 2K to 12K tokens. Results are shown in Table 9, Table 10 and Table 11."
        },
        {
            "title": "D Generalization to Different Architectures",
            "content": "To evaluate the generalizability of ASAP, we replicate our main experiments on the DeepSeek-R1-Distill-Llama8B. Following the same experimental protocol, we compare ASAP against three baselines: Zero-shot, Original, and SPIRIT. Results on the HumanEval, HumanEval+, LiveCodeBench v1 v3, LiveCodeBench v4 v5, and LeetCodeDataset benchmarks are shown in Table 12."
        },
        {
            "title": "E Prompts Used in Our Method",
            "content": "Prompt for Direct CoT Generation Given question, please tell me how to get this answer step by step. Question: ``` {question} ``` Answer: ```python {answer} ``` Only return detailed (with code) step-by-step solution (containing only Step-by-Step Solution and Final Code). The detailed step-by-step solution is: Budget Zero-Shot Original SPIRIT Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc 1K 2K 4K 6K 10.37 45.12 71.95 73.78 1007 1813 2561 3051 0.28 15.85 0.53 44.51 0.85 67.68 1.16 78.05 983 1702 2511 0.28 11.59 0.49 49.39 0.82 76.83 1.12 83.54 995 1690 2401 2764 0.28 25.00 0.49 56.71 0.80 75.00 1.07 84.15 (a) Results on HumanEval Budget Zero-Shot Original SPIRIT Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc 1K 2K 4K 6K 9.76 42.68 66.46 68.29 1007 1813 2561 0.28 14.63 0.53 43.29 0.85 65.85 1.16 75.61 983 1702 2511 2973 0.28 10.98 0.49 47.56 0.82 69.51 1.12 75.61 995 1690 2401 2764 0.28 23.78 0.49 54.88 0.80 71.34 1.07 78.66 ASAP Tok 946 1502 2116 2464 ASAP Tok 946 1502 2116 2464 Lat 0.27 0.44 0.72 0.98 Lat 0.27 0.44 0.72 0.98 (b) Results on HumanEval+ Table 9: Results of different methods under different budgets on HumanEval. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. Budget Zero-shot Original SPIRIT ASAP Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc . 2K 4K 6K 8K 10K 12K 16.50 32.68 39.05 44.28 42.16 43.95 1966 3499 4806 5903 7088 7988 0.52 17.16 1.06 30.72 1.70 42.65 2.46 47.71 3.59 52.12 5.10 54.41 1920 3432 4673 5723 6611 7473 0.51 18.95 1.05 34.80 1.67 43.14 2.43 51.80 3.15 50.82 4.22 51. 1908 3370 4605 5515 6524 7362 0.51 21.57 1.03 34.97 1.64 46.24 2.27 52.61 3.09 54.74 4.09 55.56 Tok 1833 3244 4358 4919 5177 5322 (a) Results on LiveCodeBench v1 v3 Budget 2K 4K 6K 8K 10K 12K Zero-shot Original SPIRIT ASAP Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc 6.72 16.79 23.13 25.37 25.37 25.75 2021 3820 5444 6927 8336 9706 0.59 6.34 1.22 15.67 2.07 22.76 3.27 25.74 5.15 30.97 7.44 32.46 1999 3799 5397 6882 8289 9567 0.57 8.21 1.20 20.15 2.00 26.49 3.24 30.60 4.83 33.58 7.10 34.33 1993 3712 5237 6634 7892 0.56 13.43 1.18 20.90 1.93 30.60 3.09 35.07 4.62 36.19 6.73 36.57 Tok 1930 3594 4988 5793 6035 6128 Lat 0.49 1.00 1.54 1.90 2.09 2.27 Lat 0.54 1.15 1.85 2.38 2.61 2.76 (b) Results on LiveCodeBench v4 v5 Table 10: Results of different methods under different budgets on LiveCodeBench. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. Budget 2K 4K 6K 8K 10K 12K Zero-Shot Original SPIRIT ASAP Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc 7.02 13.16 16.23 19.30 19.74 21.49 2028 3848 5553 7165 8680 0.53 6.14 1.21 13.16 2.04 16.67 3.27 22.37 4.95 25.00 7.58 28.07 2020 3854 5548 7104 8485 9717 0.53 7.02 1.21 16.23 2.04 18.86 3.18 22.37 4.72 25.00 7.09 26.32 2001 3789 5407 6882 8186 9354 0.53 10.09 1.19 15.79 2.00 19.30 3.04 23.25 4.45 27.63 6.86 27.63 Tok 1965 3758 5387 6722 7541 7902 Lat 0.53 1.19 2.00 2.88 3.48 3.83 Table 11: Results of different methods under different budgets on LeetCodeDataset. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds. HumanEval LiveCodeBench LeetCodeDataset Methods HE HE+ v1 v3 v4 All Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc Tok Lat Acc Tok Lat Zero-shot Original SPIRIT 68.29 3334 1.86 82.32 2978 1.63 76.22 3159 1.74 64.02 3334 1.86 76.22 2978 1.63 72.56 3159 1.74 44.12 7162 6.92 52.61 6614 6.16 52.61 6280 5. 25.00 8508 8.90 31.34 8202 8.60 30.22 7913 8.45 27.19 8358 8.65 26.32 8413 8.85 26.75 8449 8.73 ASAP 80.49 2494 1.30 76.83 2494 1.30 48.86 3605 2. 32.84 4175 2.69 27.63 3792 2.42 Table 12: Experimental results of different methods with DeepSeek-R1-Distill-Llama-8B. We report accuracy (Acc), average number of generated tokens (Tok), and average generation latency (Lat) measured in seconds."
        }
    ],
    "affiliations": [
        "Chongqing University",
        "East China Normal University",
        "Fudan University",
        "Shanghai Jiao Tong University"
    ]
}