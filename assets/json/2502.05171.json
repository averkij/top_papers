{
    "paper_title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "authors": [
        "Jonas Geiping",
        "Sean McLeish",
        "Neel Jain",
        "John Kirchenbauer",
        "Siddharth Singh",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Abhinav Bhatele",
        "Tom Goldstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters."
        },
        {
            "title": "Start",
            "content": "Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Jonas Geiping 1 Sean McLeish 2 Neel Jain 2 John Kirchenbauer 2 Siddharth Singh 2 Brian R. Bartoldson 3 Bhavya Kailkhura 3 Abhinav Bhatele 2 Tom Goldstein 2 5 2 0 2 7 ] . [ 1 1 7 1 5 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We study novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale proof-ofconcept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to computation load equivalent to 50 billion parameters. Model: huggingface.co/tomg-group-umd/huginn0125 Code and Data: github.com/seal-rg/recurrentpretraining 1. Scaling by Thinking in Continuous Space Humans naturally expend more mental effort solving some problems than others. While humans are capable of thinking over long time spans by verbalizing intermediate results and writing them down, substantial amount of thought happens through complex, recurrent firing patterns in the brain, before the first word of an answer is uttered. Early attempts at increasing the power of language models focused on scaling model size, practice that requires extreme amounts of data and computation. More recently, researchers have explored ways to enhance the reasoning 1ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center 2University of Maryland, College Park 3Lawrence Livermore National Laboratory. Correspondence to: Jonas Geiping, Tom Goldstein <jonas@tue.ellis.eu, tomg@umd.edu>. Figure 1: We train 3.5B parameter language model with depth recurrence. At test time, the model can iterate longer to use more compute and improve its performance. Instead of scaling test-time reasoning by verbalizing in long Chains-of-Thought, the model improves entirely by reasoning in latent space. Tasks that require less reasoning like OpenBookQA converge quicker than tasks like GSM8k, which effectively make use of more compute. capability of models by scaling test time computation. The mainstream approach involves post-training on long chainof-thought examples to develop the models ability to verbalize intermediate calculations in its context window and thereby externalize thoughts. However, the constraint that expensive internal reasoning must always be projected down to single verbalized next token appears wasteful; it is plausible that models could be more competent if they were able to natively think in their continuous latent space. One way to unlock this untapped dimension of additional compute involves adding recurrent unit to model. This unit runs in loop, iteratively processing and updating its hidden state and enabling computations to be carried on indefinitely. While this is not currently the dominant paradigm, this idea is foundational to machine learning and has been (re-)discovered in every decade, for example as recurrent neural networks, diffusion models, and as universal or looped transformers. In this work, we show that depth-recurrent language models can learn effectively, be trained in an efficient manner, and demonstrate significant performance improvements under the scaling of test-time compute. Our proposed trans1 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach former architecture is built upon latent depth-recurrent block that is run for randomly sampled number of iterations during training. We show that this paradigm can scale to several billion parameters and over half trillion tokens of pretraining data. At test-time, the model can improve its performance through recurrent reasoning in latent space, enabling it to compete with other open-source models that benefit from more parameters and training data. Additionally, we show that recurrent depth models naturally support number of features at inference time that require substantial tuning and research effort in non-recurrent models, such as per-token adaptive compute, (self)-speculative decoding, and KV-cache sharing. We finish out our study by tracking token trajectories in latent space, showing that number of interesting computation behaviors simply emerge with scale, such as the model rotating shapes in latent space for numerical computations. 2. Why Train Models with Recurrent Depth? Recurrent layers enable transformer model to perform arbitrarily many computations before emitting token. In principle, recurrent mechanisms provide simple solution for test-time compute scaling. Compared to more standard approach of long context reasoning (OpenAI, 2024; DeepSeek-AI et al., 2025), latent recurrent thinking has several advantages. Latent reasoning does not require construction of bespoke training data. Chain-of-thought reasoning requires the model to be trained on long demonstrations that are constructed in the domain of interest. In contrast, our proposed latent reasoning models can train with variable compute budget, using standard training data with no specialized demonstrations, and enhance their abilities at testtime if given additional compute. Latent reasoning models require less memory for training and inference than chain-of-thought reasoning models. Because the latter require extremely long context windows, specialized training methods such as tokenparallelization (Liu et al., 2023a) may be needed. Recurrent-depth networks perform more FLOPs per parameter than standard transformers, significantly reducing communication costs between accelerators at scale. This especially enables higher device utilization when training with slower interconnects. By constructing an architecture that is compute-heavy and small in parameter count, we hope to set strong prior towards models that solve problems by thinking, i.e. by learning meta-strategies, logic and abstraction, instead of memorizing. The strength of recurrent priors for learning complex algorithms has already been demonstrated in the deep thinking literature (Schwarzschild et al., 2021b; Bansal et al., 2022; Schwarzschild et al., 2023). On more philosophical note, we hope that latent reasoning captures facets of human reasoning that defy verbalization, such as spatial thinking, physical intuition or (motor) planning. Over many iterations of the recurrent process, reasoning in high-dimensional vector space would enable the deep exploration of multiple directions simultaneously, instead of linear thinking, leading to system capable of exhibiting novel and complex reasoning behavior. Scaling compute in this manner is not at odds with scaling through extended (verbalized) inference scaling (Shao et al., 2024), or scaling parameter counts in pretraining (Kaplan et al., 2020), we argue it may build third axis on which to scale model performance. Table of Contents Section 3 introduces our latent recurrent-depth model architecture and training objective. Section 4 describes the data selection and engineering of our large-scale training run on Frontier, an AMD cluster. Section 5 reports benchmark results, showing how the model improves when scaling inference compute. Section 6 includes several application examples showing how recurrent models naturally simplify LLM usecases. Section 7 visualizes what computation patterns emerge at scale with this architecture and training objective, showing that context-dependent behaviors emerge in latent space, such as orbiting when responding to prompts requiring numerical reasoning. 3. scalable recurrent architecture In this section we will describe our proposed architecture for transformer with latent recurrent depth, discussing design choices and small-scale ablations. diagram of the architecture can be found in Figure 2. We always refer to the sequence dimension as n, the hidden dimension of the model as h, and its vocabulary as the set . 3.1. Macroscopic Design The model is primarily structured around decoder-only transformer blocks (Vaswani et al., 2017; Radford et al., 2019). However these blocks are structured into three functional groups, the prelude , which embeds the input data into latent space using multiple transformer layers, then the core recurrent block R, which is the central unit of recurrent computation modifying states Rnh, and finally the coda C, which un-embeds from latent space using several layers and also contains the prediction head of the model. The core block is set between the prelude and coda blocks, and by looping the core we can put an indefinite amount of verses in our song. 2 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 2: visualization of the Architecture, as described in Section 3. Each block consists of number of sub-layers. The blue prelude block embeds the inputs into latent space, where the green shared recurrent block is block of layers that is repeated to compute the final latent state, which is decoded by the layers of the red coda block. Given number of recurrent iterations r, and sequence of input tokens these groups are used in the following way to produce output probabilities RnV = (x) s0 (0, σ2Inh) si = R(e, si1) = R(sr), for {1, . . . , r} where σ is some standard deviation for initializing the random state. This process is shown in Figure 2. Given an init random state s0, the model repeatedly applies the core block R, which accepts the latent state si1 and the embedded input and outputs new latent state si. After finishing all iterations, the coda block processes the last state and produces the probabilities of the next token. This architecture is based on deep thinking literature, where it is shown that injecting the latent inputs in every step (Bansal et al., 2022) and initializing the latent vector with random state stabilizes the recurrence and promotes convergence to steady state independent of initialization, i.e. path independence (Anil et al., 2022). Motivation for this Design. This recurrent design is the minimal setup required to learn stable iterative operators. good example is gradient descent of function E(x, y), where may be the variable of interest and the data. Gradient descent on this function starts from an initial random state, here x0, and repeatedly applies simple operation (the gradient of the function it optimizes), that depends on the previous state xk and data y. Note that we need to use in every step to actually optimize our function. Similarly we repeatedly inject the data in our set-up in every step of the recurrence. If was provided only at the start, e.g. via s0 = e, then the iterative process would not be stable1, as its solution would depend only on its boundary conditions."
        },
        {
            "title": "The structure of using several layers to embed input tokens",
            "content": "1Stable in the sense that cannot be monotone operator if it does not depend on e, and so cannot represent gradient descent on strictly convex, data-dependent functions, (Bauschke et al., 2011) into hidden latent space is based on empirical results analyzing standard fixed-depth transformers (Skean et al., 2024; Sun et al., 2024; Kaplan et al., 2024). This body of research shows that the initial and the end layers of LLMs are noticeably different, whereas middle layers are interchangeable and permutable. For example, Kaplan et al. (2024) show that within few layers standard models already embed sub-word tokens into single concepts in latent space, on which the model then operates. Remark 3.1 (Is this Diffusion Model?). This iterative architecture will look familiar to the other modern iterative modeling paradigm, diffusion models (Song & Ermon, 2019), especially latent diffusion models (Rombach et al., 2022). We ran several ablations with iterative schemes even more similar to diffusion models, such as si = R(e, si1) + where (0, σiInh), but find the injection of noise not to help in our preliminary experiments, which is possibly connected to our training objective. We also evaluated and si = Ri(e, si1), i.e. core block that takes the current step as input (Peebles & Xie, 2023), but find that this interacts badly with path independence, leading to models that cannot extrapolate. 3.2. Microscopic Design Within each group, we broadly follow standard transformer layer design. Each block contains multiple layers, and each layer contains standard, causal self-attention block using RoPE (Su et al., 2021) with base of 50000, and gated SiLU MLP (Shazeer, 2020). We use RMSNorm (Zhang & Sennrich, 2019) as our normalization function. The model has learnable biases on queries and keys, and nowhere else. To stabilize the recurrence, we order all layers in the following sandwich format, using norm layers ni, which is related, but not identical to similar strategies in (Ding et al., 2021; Team Gemma et al., 2024): ˆxl =n2 (xl1 + Attn(n1(xl1))) xl =n4 ( ˆxl + MLP(n3( ˆxl))) While at small scales, most normalization strategies, e.g. pre-norm, post-norm and others, work almost equally well, 3 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach we ablate these options and find that this normalization is required to train the recurrence at scale. Given an embedding matrix and embedding scale γ, the prelude block first embeds input tokens as γE(x), and then to applies lP many prelude layers with the layout described above. Our core recurrent block starts with an adapter matrix : R2h Rh mapping the concatenation of si and into the hidden dimension (Bansal et al., 2022). While re-incorporation of initial embedding features via addition rather than concatenation works equally well for smaller models, we find that concatenation works best at scale. This is then fed into lR transformer layers. At the end of the core block the output is again rescaled with an RMSNorm nc. The coda contains lC layers, normalization by nc, and projection into the vocabulary using tied embeddings ET . In summary, we can summarize the architecture by the triplet (lP , lR, lC), describing the number of layers in each stage, and by the number of recurrences r, which may vary in each forward pass. We train number of small-scale models with shape (1, 4, 1) and hidden size = 1024, in addition to large model with shape (2, 4, 2) and = 5280. This model has only 8 real layers, but when the recurrent block is iterated, e.g. 32 times, it unfolds to an effective depth of 2 + 4r + 2 = 132 layers, constructing computation chains that can be deeper than even the largest fixed-depth transformers (Levine et al., 2021; Merrill et al., 2022). 3.3. Training Objective Training Recurrent Models through Unrolling. To ensure that the model can function when we scale up recurrent iterations at test-time, we randomly sample iteration counts during training, assigning random number of iterations to every input sequence (Schwarzschild et al., 2021b). We optimize the expectation of the loss function over random samples from distribution and random iteration counts from distribution Λ. L(θ) = ExX ErΛL (mθ(x, r), x) . Here, represents the model output, and is the sequence shifted left, i.e., the next tokens in the sequence x. We choose Λ to be log-normal Poisson distribution. Given targeted mean recurrence + 1 and variance that we set to σ = 1 2 , we can sample from this distribution via τ (log(r) P(eτ ) + 1, 1 2 σ2, σ) (1) (2) given the normal distribution and Poisson distribution P, see Figure 3. The distribution most often samples values Figure 3: We use log-normal Poisson Distribution to sample the number of recurrent iterations for each training step. less than r, but it contains heavy tail of occasional events in which significantly more iterations are taken. Truncated Backpropagation. To keep computation and memory low at train time, we backpropagate through only the last iterations of the recurrent unit. This enables us to train with the heavy-tailed Poisson distribution Λ, as maximum activation memory and backward compute is now independent of r. We fix = 8 in our main experiments. At small scale, this works as well as sampling uniformly, but with set fixed, the overall memory usage in each step of training is equal. Note that the prelude block still receives gradient updates in every step, as its output is injected in every step. This setup resembles truncated backpropagation through time, as commonly done with RNNs, although our setup is recurrent in depth rather than time (Williams & Peng, 1990; Mikolov et al., 2011). 4. Training large-scale recurrent-depth"
        },
        {
            "title": "Language Model",
            "content": "After verifying that we can reliably train small test models up to 10B tokens, we move on to larger-scale runs. Given our limited compute budget, we could either train multiple tiny models too small to show emergent effects or scaling, or train single medium-scale model. Based on this, we prepared for single run, which we detail below. 4.1. Training Setup We describe the training setup, separated into architecture, optimization setup and pretraining data. We publicly release all training data, pretraining code, and selection of intermediate model checkpoints. Pretraining Data. Given access to only enough compute for single large scale model run, we opted for dataset mixture that maximized the potential for emergent reasoning behaviors, not necessarily for optimal benchmark performance. Our final mixture is heavily skewed towards code and mathematical reasoning data with (hopefully) just 4 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach = 2 et al. (2024) which prescribes variance of σ2 5h . We initialize all parameters from truncated normal distribution (truncated at 3σ) with this variance, except all outout = 1 projection layers, where the variance is set to σ2 5hl , for = lP + rlR + lC the number of effective layers, which is 132 for this model. As result, the out-projection layers are initialized with fairly small values (Goyal et al., 2018). h. To The output of the embedding layer is scaled by match this initialization, the state s0 is also sampled from = 2 truncated normal distribution, here with variance σ2 5 . Figure 4: Distribution of data sources that are included during training. The majority of our data is comprised of generic webtext, scientific writing and code. enough general webtext to allow the model to acquire standard language modeling abilities. All sources are publicly available. We provide an overview in Figure 4. Following Allen-Zhu & Li (2024), we directly mix relevant instruction data into the pretraining data. However, due to compute and time constraints, we were not able to ablate this mixture. We expect that more careful data preparation could further improve the models performance. We list all data sources in Appendix C. Tokenization and Packing Details. We construct vocabulary of 65536 tokens via BPE (Sennrich et al., 2016), using the implementation of Dagan (2024). In comparison to conventional tokenizer training, we construct our tokenizer directly on the instruction data split of our pretraining corpus, to maximize tokenization efficiency on the target domain. We also substantially modify the pre-tokenization regex (e.g. of Dagan et al. (2024)) to better support code, contractions and LaTeX. We include <begin_text> token at the start of every document. After tokenizing our pretraining corpus, we pack our tokenized documents into sequences of length 4096. When packing, we discard document ends that would otherwise lack previous context, to fix an issue described as the grounding problem in Ding et al. (2024), aside from several long-document sources of mathematical content, which we preserve in their entirety. Locked-Step Sampling. To enable synchronization between parallel workers, we sample single depth for each micro-batch of training, which we synchronize across workers (otherwise workers would idle while waiting for the model with the largest to complete its backward pass). We verify at small scale that this modification improves compute utilization without impacting convergence speed, but note that at large batch sizes, training could be further improved by optimally sampling and scheduling independent steps on each worker, to more faithfully model the expectation over steps in Equation (1). Optimizer and Learning Rate Schedule. We train using the Adam optimizer with decoupled weight regularization (β1 = 0.9, β2 = 0.95, η = 5 104) (Kingma & Ba, 2015; Loshchilov & Hutter, 2017), modified to include update clipping (Wortsman et al., 2023) and removal of the ε constant as in Everett et al. (2024). We clip gradients above 1. We train with warm-up and constant learning rate (Zhai et al., 2022; Geiping & Goldstein, 2023), warming up to our maximal learning rate within the first 4096 steps of training. 4.2. Compute Setup and Hardware We train this model using compute time allocated on the Oak Ridge National Labs Frontier cluster, large-scale HPE Cray system containing 8 AMD MI250X GPU nodes. Nodes are connected via 4xHPE Slingshot NICs. The scheduling system is orchestrated through SLURM. We train in bfloat16 mixed precision using PyTorch-based implementation (Zamirai et al., 2021). Architecture and Initialization. We scale the architecture described in Section 3, setting the layers to (2, 4, 2), and train with mean recurrence value of = 32. We mainly scale by increasing the hidden size to = 5280, which yields 55 heads of size of 96. The MLP inner dimension is 17920 and the RMSNorm ε is 106. Overall this model shape has about 1.5B parameters in non-recurrent prelude and head, 1.5B parameters in the core recurrent block, and 0.5B in the tied input embedding. At small scales, most sensible initialization schemes work. However, at larger scales, we use the initialization of Takase Device Speed and Parallelization Strategy. Nominally, each MI250X chip2 achieves 192 TFLOP per GPU (AMD, 2021). For single matrix multiplication, we measure maximum achievable speed on these GPUs of 125 TFLOP/s on our software stack (ROCM 6.2.0, PyTorch 2.6 prerelease 11/02) (Bekman, 2023). Our implementation, using extensive PyTorch compilation and optimization of the hidden dimension to = 5280 achieves single-node training 2Technically, each node contains 4 dual-chip MI250X cards, but its main software stack (ROCm runtime) treats these chips as fully independent. 5 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 5: Plots of the initial 10000 steps for the first two failed attempts and the final, successful run (Main). Note the hidden state collapse (middle) and collapse of the recurrence (right) in the first two failed runs, underlining the importance of our architecture and initialization in inducing recurrent model and explain the underperformance of these runs in terms of pretraining loss (left). speed of 108.75 TFLOP/s, i.e. 87% AFU (Achievable Flop Utilization). Due to the weight sharing inherent in our recurrent design, even our largest model is still small enough to be trained using only data (not tensor) parallelism, with only optimizer sharding (Rajbhandari et al., 2020) and gradient checkpointing on per-iteration granularity. With batch size of 1 per GPU we end up with global batch size of 16M tokens per step, minimizing inter-GPU communication bandwidth. When we run at scale on 4096 GPUs, we achieve 52-64 TFLOP/s per GPU, i.e. 41%-51% AFU, or 1-1.2M tokens per second. To achieve this, we wrote hand-crafted distributed data parallel implementation to circumvent critical AMD interconnect issue, which we describe in more detail in Appendix A.2. Overall, we believe this may be the largest language model training run to completion in terms of number of devices used in parallel on an AMD cluster, as of time of writing. Training Timeline. Training proceeded through 21 segments of up to 12 hours, which scheduled on Frontier mostly in early December 2024. We also ran baseline comparison, where we train the same architecture but in feedforward manner with only 1 pass through the core/recurrent block. This trained with the same setup for 180B tokens on 256 nodes with batch size of 2 per GPU. Ultimately, we were able to schedule 795B tokens of pretraining of the main model. Due to our constant learning rate schedule, we were able to add additional segments on-demand, when an allocation happened to be available. 4.3. Importance of Norms and Initializations at Scale At small scales all normalization strategies worked, and we observed only tiny differences between initializations. The same was not true at scale. The first training run we started was set up with the same block sandwich structure as described above, but parameter-free RMSNorm layers, no embedding scale γ, parameter-free adapter A(s, e) = + e, and peak learning rate of 4 104. As shown in Figure 5, this run (Bad Run 1, orange), quickly stalled. While the run obviously stopped improving in training loss (left plot), we find that this stall is due to the models representation collapsing (Noci et al., 2022). The correlation of hidden states in the token dimension quickly goes to 1.0 (middle plot), meaning the model predicts the same hidden state for every token in the sequence. We find that this is an initialization issue that arises due to the recurrence operation. Every iteration of the recurrence block increases token correlation, mixing the sequence until collapse. We attempt to fix this by introducing the embedding scale factor, switching back to conventional pre-normalization block, and switching to the learned adapter. Initially, these changes appear to remedy the issue. Even though token correlation shoots close to 1.0 at the start (Bad Run 2, green), the model recovers after the first 150 steps. However, we quickly find that this training run is not able to leverage test-time compute effectively (right plot), as validation perplexity is the same whether 1 or 32 recurrences are used. This initialization and norm setup has led to local minimum as the model has learned early to ignore the incoming state s, preventing further improvements. In third, and final run (Main, blue), we fix this issue by reverting back to the sandwich block format, and further dropping the peak learning rate to 4 105. This run starts smoothly, never reaches token correlation close to 1.0, and quickly overtakes the previous run by utilizing the recurrence and improving with more iterations. With our successful configuration, training continues smoothly for the next 750B tokens without notable interruptions or loss spikes. We plot training loss and perplexity at different recurrence steps in Figure 6. In our material, we refer to the final checkpoint of this run as our main model, which we denote as Huginn-01253. 3/hu: gIn/, transl. thought, is raven depicted in Norse mythology. Corvids are surprisingly intelligent for their size, and and of course, as birds, able to unfold their wings at test-time. 6 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 6: Left: Plot of pretrain loss over the 800B tokens on the main run. Right: Plot of val ppl at recurrent depths 1, 4, 8, 16, 32, 64. During training, the model improves in perplexity on all levels of recurrence. Table 1: Results on lm-eval-harness tasks zero-shot without chat template across various open-source models. We show ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021b), OpenBookQA (Mihaylov et al., 2018), PiQA (Bisk et al., 2020), SciQ (Johannes Welbl, 2017), and WinoGrande (Sakaguchi et al., 2021). We report normalized accuracy when provided. Model random Amber Pythia-2.8b Pythia-6.9b Pythia-12b OLMo-1B OLMo-7B OLMo-7B-0424 OLMo-7B-0724 OLMo-2-1124 Ours, (r = 4) Ours, (r = 8) Ours, (r = 16) Ours, (r = 32) Param Tokens ARC-E ARC-C HellaSwag MMLU OBQA PiQA SciQ WinoGrande 7B 2.8B 6.9B 12B 1B 7B 7B 7B 7B 3.5B 3.5B 3.5B 3.5B 1.2T 0.3T 0.3T 0.3T 3T 2.5T 2.05T 2.75T 4T 0.8T 0.8T 0.8T 0.8T 25.0 65.70 58.00 60.48 63.22 57.28 68.81 75.13 74.28 82.79 57.19 66.07 68.43 69.91 25.0 37.20 32.51 34.64 34.64 30.72 40.27 45.05 43.43 57.42 22.95 32.50 34.38 38. 25.0 72.54 59.17 63.32 66.72 63.00 75.52 77.24 77.76 80.50 36.07 45.08 48.65 65.21 25.0 26.77 25.05 25.74 24.01 24.33 28.39 47.46 50.18 60.56 23.32 24.88 29.21 31. 25.0 41.00 35.40 37.20 35.40 36.40 42.20 41.60 41.60 46.20 18.60 22.00 24.00 38.80 50.0 25.0 78.73 73.29 75.79 75.84 75.24 80.03 80.09 80.69 81. 65.12 70.72 73.99 76.22 88.50 83.60 82.90 84.40 78.70 88.50 96.00 95.70 96.40 84.80 91. 5 93.60 93.50 50.0 63.22 57.85 61.40 63.06 59.19 67.09 68.19 67.17 74.74 55.24 55.64 57.77 59. 5. Benchmark Results We train our final model for 800B tokens, and nonrecurrent baseline for 180B tokens. We evaluate these checkpoints against other open-source models trained on fully public datasets (like ours) of similar size. We compare against Amber (Liu et al., 2023c), Pythia (Biderman et al., 2023) and number of OLMo 1&2 variants (Groeneveld et al., 2024; AI2, 2024; Team OLMo et al., 2025). We execute all standard benchmarks through the lm-eval harness (Biderman et al., 2024) and code benchmarks via bigcode-bench (Zhuo et al., 2024). 5.1. Standard Benchmarks Overall, it is not straightforward to place our model in direct comparison to other large language models, all of which are small variations of the fixed-depth transformer architecture. While our model has only 3.5B parameters and hence requires only modest interconnect bandwidth during pretraining, it chews through raw FLOPs close to what 32B parameter transformer would consume during pretraining, and can continuously improve in performance with test-time scaling up to FLOP budgets equivalent to standard 50B parameter fixed-depth transformer. It is also important to note few caveats of the main training run when interpreting the results. First, our main checkpoint is trained for only 47000 steps on broadly untested mixture, and the learning rate is never cooled down from its peak. As an academic project, the model is trained only on publicly available data and the 800B token count, while large in comparison to older fully open-source models such as the Pythia series, is small in comparison to modern open-source efforts such as OLMo, and tiny in comparison to the datasets used to train industrial open-weight models. Disclaimers aside, we collect results for established benchmark tasks (Team OLMo et al., 2025) in Table 1 and show all models side-by-side. In direct comparison we see that our model outperforms the older Pythia series and is roughly comparable to the first OLMo generation, OLMo7B in most metrics, but lags behind the later OLMo models trained larger, more carefully curated datasets. For the first recurrent-depth model for language to be trained at this 7 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Table 2: Benchmarks of mathematical reasoning and understanding. We report flexible and strict extract for GSM8K and GSM8K CoT, extract match for Minerva Math, and acc norm. for MathQA. Table 3: Evaluation on code benchmarks, MBPP and HumanEval. We report pass@1 for both datasets. Model Random Amber Pythia-2.8b Pythia-6.9b Pythia-12b OLMo-1B OLMo-7B OLMo-7B-0424 OLMo-7B-0724 OLMo-2-1124-7B GSM8K GSM8k CoT Minerva MATH MathQA 0.00 0.00 3.94/4.32 1.59/2.12 2.05/2.43 3.49/4.62 1.82/2.27 4.02/4.09 27.07/27.29 28.66/28.73 66.72/66. 3.34/5.16 1.90/2.81 2.81/2.88 3.34/4.62 1.59/2.58 6.07/7.28 26.23/26.23 28.89/28.89 61.94/66.19 32.60/34.57 34.80/42.08 0.00 1.94 1.96 1.38 2.56 1.60 2.12 5.56 5.62 19.08 12.58 11.24 20. 25.26 24.52 25.96 25.80 23.38 25.26 28.48 27.84 37.59 26.60 27.97 Model Random starcoder2-3b starcoder2-7b Amber Pythia-2.8b Pythia-6.9b Pythia-12b OLMo-1B OLMo-7B OLMo-7B-0424 OLMo-7B-0724 OLMo-2-1124-7B Ours (r = 32) Param Tokens MBPP HumanEval 3B 7B 7B 2.8B 6.9B 12B 1B 7B 7B 7B 7B 3.5B 3.3T 3.7T 1.2T 0.3T 0.3T 0.3T 3T 2.5T 2.05T 2.75T 4T 0.8T 0.00 43.00 43.80 19.60 6.70 7.92 5.60 0.00 15.6 21.20 25.60 21.80 24. 0.00 31.09 31.70 13.41 7.92 5.60 9.14 4.87 12.80 16.46 20.12 10.36 23.17 Our w/o sys. prompt (r = 32) Our w/ sys. prompt (r = 32) 28.05/28.20 24.87/38. scale, and considering the limitations of the training run, we find these results promising and certainly suggestive that further research into latent recurrence as an approach to test-time scaling is warranted. 5.2. Math and Coding Benchmarks We also evaluate the model on math and coding. For math, we evaluate GSM8k (Cobbe et al., 2021) (as zero-shot and in the 8-way CoT setup), MATH ((Hendrycks et al., 2021a) with the Minerva evaluation rules (Lewkowycz et al., 2022)) and MathQA (Amini et al., 2019). For coding, we check MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021). Here we find that our model significantly surpasses all models except the latest OLMo-2 model in mathematical reasoning, as measured on GSM8k and MATH. On coding benchmarks the model beats all other general-purpose opensource models, although it does not outperform dedicated code models, such as StarCoder2 (Lozhkov et al., 2024), trained for several trillion tokens. We also note that while further improvements in language modeling are slowing down, as expected at this training scale, both code and mathematical reasoning continue to improve steadily throughout training, see Figure 8. 5.3. Where does recurrence help most? How much of this performance can we attribute to recurrence, and how much to other factors, such as dataset, tokenization and architectural choices? In Table 4, we compare our recurrent model against its non-recurrent twin, which we trained to 180B tokens in the exact same setting. In direct comparison of both models at 180B tokens, we see that the recurrent model outperforms its baseline with an especially pronounced advantage on harder tasks, such as the ARC challenge set. On other tasks, such as SciQ, which requires straightforward recall of scientific facts, performance of the models is more similar. We observe that gains through reasoning are especially prominent on GSM8k, where the 180B recurrent model is already 5 times better than the baseline at this early snapshot in the pretraining Figure 7: Performance on GSM8K CoT (strict match and flexible match), HellaSwag (acc norm.), and HumanEval (pass@1). As we increase compute, the performance on these benchmarks increases. HellaSwag only needs 8 recurrences to achieve near peak performance while other benchmarks make use of more compute. process. We also note that the recurrent model, when evaluated with only single recurrence, effectively stops improving between the early 180B checkpoint and the 800B checkpoint, showing that further improvements are not built into the prelude or coda non-recurrent layers but encoded entirely into the iterations of the recurrent block. Further, we chart the improvement as function of test-time compute on several of these tasks for the main model in Figure 7. We find that saturation is highly task-dependent, on easier tasks the model saturates quicker, whereas it benefits from more compute on others. Recurrence and Context We evaluate ARC-C performance as function of recurrence and number of few-shot examples in the context in Figure 9. Interestingly, without few-shot examples to consider, the model saturates in compute around 8-12 iterations. However, when more context is given, the model can reason about more information in context, which it does, saturating around 20 iterations if 1 example is provided, and 32 iterations, if 25-50 examples are provided, mirroring generalization improvements shown for recurrence (Yang et al., 2024a; Fan et al., 2025). Similarly, we 8 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Table 4: Baseline comparison, recurrent versus non-recurrent model trained in the same training setup and data. Comparing the recurrent model with its non-recurrent baseline, we see that even at 180B tokens, the recurrent substantially outperforms on harder tasks. Model Tokens ARC-E ARC-C HellaSwag MMLU OBQA PiQA SciQ WinoGrande GSM8K CoT Fixed-Depth Baseline Ours, early ckpt, (r = 32) Ours, early ckpt, (r = 1) Ours, (r = 32) Ours, (r = 1) 0.18T 0.18T 0.18T 0.8T 0.8T 46.42 53.62 34.01 69.91 34. 26.96 29.18 23.72 38.23 24.06 37.34 48.80 29.19 65.21 29. 24.16 25.59 23.47 31.38 23.60 29.60 31.40 25.60 38.80 26. 64.47 73.20 68.88 53.26 76.22 55.33 80.60 54.10 93.50 47. 51.78 52.88 53.75 59.43 49.41 1.82/2.20 9.02/10.24 0.00/0.15 34.80/42.08 0.00/0. Figure 8: GSM8K CoT, HellaSwag, and HumanEval performance over the training tokens with different recurrences at test-time. We evaluate GSM8K CoT with chat template and 8-way few shot as multiturn. HellaSwag and HumanEval are zero-shot with no chat template. Model performance on harder tasks grows almost linearly with the training budget, if provided sufficient test-time compute. Table 5: Comparison of Open and Closed QA Performance (%) (Mihaylov et al., 2018). In the open exam, relevant fact is provided before the question is asked. In this setting, our smaller model closes the gap to other open-source models, indicating that the model is capable, but has fewer facts memorized. Model Closed Open Amber Pythia-2.8b Pythia-6.9b Pythia-12b OLMo-1B OLMo-7B OLMo-7B-0424 OLMo-7B-0724 OLMo-2Ours (r = 32) 41.0 35.4 37.2 35.4 36.4 42.2 41.6 41.6 46.2 38.2 46.0 44.8 44.2 48.0 43.6 49.8 50.6 53.2 53.4 49.2 +5.0 +9.4 +7.0 +12.6 +7.2 +7.6 +9.0 +11.6 +7. +11.0 Figure 9: The saturation point in un-normalized accuracy via testtime recurrence on the ARC challenge set is correlated with the number of few-shot examples. The model uses more recurrence to extract more information from the additional few-shot examples, making use of more compute if more context is given. see that if we re-evaluate OBQA in Table 5, but do not run the benchmark in \"closed-book\" format and rather provide relevant fact, our recurrent model improves significantly almost closing the gap to OLMo-2. Intuitively this makes sense, as the recurrent models has less capacity to memorize facts but more capacity to reason about its context. β = 0.9, incorporating the last 75 checkpoints with dilation factor of 7, modification to established protocols (Kaddour, 2022; Sanyal et al., 2024). We provide this EMA model as well, which further improves GMS8k performance to 47.23% flexible (38.59% strict), when tested at = 64. 5.4. Improvements through Weight Averaging 6. Recurrent Depth simplifies LLMs Due to our constant learning rate, we can materialize further improvements through weight averaging (Izmailov et al., 2018) to simulate the result of cooldown (Hägele et al., 2024; DeepSeek-AI et al., 2024). We use an exponential moving average starting from our last checkpoint with Aside from encouraging performance in mathematical and code reasoning, recurrent-depth models turn out to be surprisingly natural tools to support number of methods that require substantial effort with standard transformers. In the next section, we provide non-exhaustive overview. 9 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 10: Histograms of zero-shot, per-token adaptive exits based on KL difference between steps for questions from MMLU categories, with and without zero-shot continuous CoT. The mean of each distribution is given in the legends. The exit threshold is fixed to 5 104. We see that the model converges quicker on high school mathematics than tasks such as logical fallacies or moral scenarios. On some tasks, such as philosophy, the model is able to effectively re-use states in its latent CoT and converge quickly on subset of tokens, leading to fewer steps required overall. 6.1. Zero-Shot Adaptive Compute at Test-Time We have shown that the model is capable of varying compute on per-query level, running the model in different recurrence modes. This is after all also how the model is trained, as in Equation (1). However, it would be more efficient in practice to stop recurring early when predictions are easy, and only spend compute on hard decisions. Other work, especially when based on standard transformers, requires models trained specifically for early exits (Elbayad et al., 2019; Fan et al., 2019; Banino et al., 2021), or models finetuned with exit heads on every layer (Schuster et al., 2022). To test our models zero-shot exit abilities, we choose simple exit criterion to evaluate convergence, the KL-divergence between two successive steps. If this divergence falls below 5 104, we stop iterating, sample the output token, and move to generate the next token. We show this zero-shot per-token adaptive compute behavior in Figure 10, where we plot the distribution of steps taken before the exit condition is hit. We do this for the first 50 questions from different MMLU categories, asked in free-form chat. Interestingly, the number of steps required to exit differs notably between categories, with the model exiting earlier on high school mathematics, but taking on average 3.5 steps more on moral scenarios. As preliminary demonstration, we verify on MTBench that this adaptivity does not significantly impact performance in conversational benchmark setting (standard: 5.63, early exits: 5.56 see Appendix Table 6). Remark 6.1 (What about missing KV-cache entries?). Traditionally, concern with token-wise early exits for models with self-attention is that it breaks KV-caching in fundamental way. On each recurrent step, token needs to attend to the KV state of previous tokens in the sequence, but these activations may not have been computed due to an early exit. naïve fix would be to pause generating and recompute all missing hidden states, but this would remove some of the benefit of early stopping. Instead, as in Elbayad et al. (2019), we attend to the last, deepest available KV states in the cache. Because all recurrent KV cache entries are generated by the same K,V projection matrices from successive hidden states, they match, and therefore the model is able to attend to the latest cache entry from every previous token, even if computed at different recurrent depths. 6.2. Zero-Shot KV-cache Sharing different avenue to increase efficiency is to reduce the memory footprint of the KV-cache by sharing the cache between layers (character.ai, 2024; Brandon et al., 2024). Typically, transformers must be trained from scratch with this capability. However, as discussed in the previous section, we find that we can simply share KV-caches in our model with minimal impact to performance. We set fixed KV-cache budget for the recurrence at every token k, and at iteration i, read and write the cache entry mod k. For example, we set maximum KV-cache budget of 16 steps, overwriting the KV-cache of the 1st step when executing the 17th step, and so forth. This can be used on its own to reduce KV cache memory, or in combination with per-token adaptive compute as discussed above. On MTBench, this does not reduce performance (cache budget of 4: 5.86, see Appendix Table 6). 6.3. Zero-Shot Continuous Chain-of-Thought Instead of sampling random initial state s0 at every generation step, we can warm-start with the last state sr from the previous token. As shown in Figure 10, this reduces the average number of steps required to converge by 1-2. Also, on tasks such as philosophy questions, we see that the exit distribution shifts on several tasks, with the model more often exiting early by recycling previous compute. To achieve similar behavior in fixed-depth transformers, these models need to be trained on reasoning tasks to accept their last 10 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 11: Convergence of latent states for every token in sequence (going top to bottom) and latent iterations (going left to right), plotting the distance final iterate s, which we set with = 128. Shown is an unsafe question posed to the model. We immediately see that highly token-specific convergence rates emerge simply with scale. This is interesting, as the model is only trained with fixed for whole sequences seen during training. We see that convergence is especially slow on the key part of the question, really wrong-ed.We further see that the model also learns different behaviors, we see an oscillating pattern in latent space, here most notably for the school token. hidden state as alternative inputs when computing the next token (Hao et al., 2024). 7. What Mechanisms Emerge at Scale in Recurrent-Depth Models 6.4. Zero-Shot Self-Speculative Decoding Recurrent-depth models can also inherently generate text more efficiently by using speculative decoding (Leviathan et al., 2023) without the need for separate draft model. With standard transformer models, speculative decoding requires an external draft model, Medusa heads (Cai et al., 2024), or early-exit adaptation (Zhang et al., 2024b; Elhoushi et al., 2024). Zhang et al. (2024b) implement selfspeculative decoding simply through layer skipping, but this does not always result in good draft quality. In comparison, our model can naturally be run with fewer iterations to draft the next tokens in the generated sequence, which can then be verified with any desired number of iterations > later. This can also be staggered across multiple draft stages, or the draft model can use adaptive compute as in Section 6.1. Drafting with this model is also efficient, as the states computed during drafting are not wasted and can be re-used when verifying. Finally, what is the model doing while recurring in latent space? To understand this question better, we analyze the trajectories {si}r i=1 of the model on few qualitative examples. We are especially interested in understanding what patterns emerge, simply by training this model at scale. In comparison to previous work, such as Bai et al. (2019), where the training objective directly encodes prior that pushes trajectories to fixed point, we only train with our truncated unrolling objective. Figure 11 shows the norm distance si between each si in trajectory and an approximate limit point computed with 128 iterations. We show the sentence top to bottom and iterations from left to right. We clearly see that convergence behavior depends on context. We see that key parts of the question, and the start of the model response, are deliberated much more in latent space. The context dependence can also be seen in the different behavior among the three identical tokens representing each of the three dots. 11 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 12: Latent Space trajectories for select tokens. We show small part of these high-dimensional trajectories by visualizing the first 6 PCA directions, computing the PCA over all latent state trajectories of all tokens in sequence. The color gradient going from dark to bright represents steps in the trajectory. The center of mass is marked in red. While on many tokens, the state simply converges (top row), the model also learns to use orbits (middle row), and sliders (bottom row, middle) to represent and handle more advanced concepts, such as arithmetic or complicated deliberation. Also note that the distance to does not always decrease monotonically (e.g. for school); the model may also trace out complicated orbits in its latent trajectory while processing information, even though this is not represented explicitly in our training objective. trained for arithmetic tasks (Nanda et al., 2022), but we find these patterns extend far beyond arithmetic for our model. We often observe use of orbits on tokens such as makes (see Figure 16) or thinks that determine the structure of the response. We look at trajectories for select tokens in more detail in Figure 12. We compute PCA decomposition of latent trajectories over all tokens in sequence, and then show several individual trajectories projected onto the first six PCA directions. See the appendix for more examples. Many tokens simply converge to fixed point, such as the token in the top row. Yet, for harder questions, such as in the 2nd row4, the state of the token quickly falls into an orbit pattern in all three pairs of PCA directions. The use of multi-dimensional orbits like these could serve similar purpose to periodic patterns sometimes observed in fixed-depth transformers 4This is the token \"3\" in GSM8k test question that opens with Claire makes 3 egg omelette. Aside from orbits, we also observe the model encoding particular key tokens as sliders, as seen in the middle of the bottom row in Figure 12 (which is the token wrong, from the same message as already shown in Figure 11). In these motions the trajectory noticeably drifts in single direction, which the model could use to implement mechanism to count how many iterations have occurred. The emergence of structured trajectories in latent space gives us glimpse into how the model performs its computations. Unlike the discrete sequential chain of reasoning seen in verbalized chain-of-thought approaches, we observe rich geometric patterns including orbits, convergent paths, and drifts - means to organize its computational process spa12 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach tially. This suggests the model is independently learning to leverage the high-dimensional nature of its latent space to implement reasoning in new ways. Path Independence. We verify that our models maintain path independence, in the sense of Anil et al. (2022), despite their complex dynamics, which we demonstrate in Figure 22. When re-initializing from multiple starting points s0, the model moves in similar trajectories, exhibiting consistent behavior. The same orbital patterns, fixed points, or directional drifts emerge regardless of initialization. 8. Related Work Overview The extent to which recurrence is foundational concept of machine learning is hard to overstate (Braitenberg, 1986; Gers & Schmidhuber, 2000; Sutskever et al., 2008). For transformers, recurrence was applied in Dehghani et al. (2019), who highlight the aim of recurrent depth to model universal, i.e. Turing-complete, machines. It was used at scale (but with fixed recurrence) in Lan et al. (2019) and an interesting recent improvement in this line of work are described in Tan et al. (2023); Abnar et al. (2023) and Csordás et al. (2024). Schwarzschild et al. (2021b); Bansal et al. (2022); Bear et al. (2024); McLeish et al. (2024) show that depth recurrence is advantageous when learning generalizable algorithms when training with randomized unrolling and input injections. Recent work has described depthrecurrent, looped, transformers and studied their potential benefits with careful theoretical and small-scale analysis (Giannou et al., 2023; Gatmiry et al., 2024; Yang et al., 2024a; Fan et al., 2025). From another angle, these models can be described as neural networks learning fixed-point iteration, as studied in deep equilibrium models (Bai et al., 2019; 2022). They are further related to diffusion models (Song & Ermon, 2019), especially latent diffusion models (Rombach et al., 2022), but we note that language diffusion models are usually run with per-sequence, instead of per-token, iteration count (Lee et al., 2018). key difference of our approach to both equilibrium models and diffusion models is in the training objective, where equilibrium methods solve the direct problem (Geiping & Moeller, 2019), diffusion models solve surrogate training objective, and our work suggests that truncated unrolling is scalable alternative. Mor generally, architectures that recur in depth can also be understood as directly learning the analog to the gradient of latent energy-based model (LeCun & Huang, 2005; LeCun, 2022), or to an implicitly defined intermediate layer (Amos & Kolter, 2017). These analogies to gradient descent at inference time also show the connection to test time adaptation (Sun et al., 2020), especially test-time adaptation of output states (Boudiaf et al., 2022). Aside from full recurrent-depth architectures, there also exist number of proposals for hybrid architectures, such as models with latent subnetworks (Li et al., 2020a), LoRA adapters on top of weight-shared layers (Bae et al., 2024), or (dynamic) weight-tying of trained models (Hay & Wolf, 2023; Liu et al., 2024). As mentioned in Section 6, while we consider the proposed recurrent depth approach to be very natural way to reason in continuous latent space, the work of Hao et al. (2024) discusses how to finetune existing fixed-depth transformers with this capability. For additional discussions related to the idea of constructing prior that incentivizes reasoning and algorithm learning at the expense of memorization of simple patterns, we also refer to Chollet (2019), Schwarzschild (2023), Li et al. (2020b) and Moulton (2023). 9. Future Work Aside from work extending and analyzing the scaling behaviors of recurrent depth models, there are many questions that remain unanswered. For example, to us, there are potentially large number of novel post-training schemes that further enhance the capabilities of these models, such as fine-tuning to compress the recurrence or reinforcement learning with data with different hardness levels (Zelikman et al., 2024), or to internalize reasoning from CoT data into the recurrence (Deng et al., 2024). Another aspect not covered in this work is the relationship to other modern architecture improvements. Efficient sequence mixing operations, especially those that are linear in sequence dimension, such as linear attention (Katharopoulos et al., 2020; Yang et al., 2024b), are limited in the number of comparisons that can be made. However, with recurrent depth, blocks containing linear operators can repeat until all necessary comparisons between sequence elements are computed (Suzgun et al., 2019). For simplicity, we also focus on single recurrence, where prior work has considered multiple successive recurrent stages (Takase & Kiyono, 2023; Csordás et al., 2024). Finally, the proposed architecture is set up to be computeheavy, with more materialized parameters than there are actual parameters. This naturally mirrors mixture-of-expert models (MoE), which are parameter-heavy, using fewer active parameters per forward pass than exist within the model (Shazeer et al., 2017; Fedus et al., 2022). We posit that where the recurrent-depth setup excels at learning reasoning patterns, the MoE excels at effectively storing and retrieving complex information. Their complementarity supports the hypothesis that future architecture would contain both modifications. While in standard MoE model, each expert can only be activated once per forward pass, or skipped entirely, recurrent MoE model could also refine its latent 13 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach state over multiple iterations, potentially routing to the same expert multiple times, before switching to different one (Tan et al., 2023; Csordás et al., 2024). While MoE models are the currently leading solution to implement this type of memory in dense transformers, these considerations also hold for other memory mechanisms suggested for LLMs (Sukhbaatar et al., 2019; Fan et al., 2021; Wu et al., 2022; He et al., 2024). 10. Conclusions The models described in this paper are ultimately still proof-of-concept. We describe how to train latent recurrent-depth architecture, what parameters we chose, and then trained single model at scale. Future training runs are likely to train with more optimized learning rate schedules, data mixes and accelerators. Still we observe number of interesting behaviors emerging naturally from recurrent training. The most important of these is the ability to use latent reasoning to dramatically improve performance on reasoning tasks by expending test-time computation. In addition, we also observe context-dependent convergence speed, path independence, and various zero-shot abilities. This leads us to believe that latent reasoning is promising research direction to complement existing approaches for test-time compute scaling. The model we realize is surprisingly powerful given its size and amount of training data, and we are excited about the potential impact of imbuing generative models with the ability to reason in continuous latent space without the need for specialized data at train time or verbalization at inference time."
        },
        {
            "title": "Acknowledgements",
            "content": "This project was made possible by the INCITE program: An award for computer time was provided by the U.S. Department of Energys (DOE) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) Program. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC0500OR22725. JG further acknowledges the support of the Hector II foundation. large number of small-scale and preliminary experiments were made possible through the support of the Intelligent Systems compute cluster and funding by the Tübingen AI center. UMD researchers were further supported by the ONR MURI program, DARPA TIAMAT, the National Science Foundation (IIS-2212182), and the NSF TRAILS Institute (2229885). Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Finally, we thank Avi Schwarzschild for helpful comments on the initial draft."
        },
        {
            "title": "References",
            "content": "Abnar, S., Saremi, O., Dinh, L., Wilson, S., Bautista, M. A., Huang, C., Thilak, V., Littwin, E., Gu, J., Susskind, J., and Bengio, S. Adaptivity and Modularity for Efficient Generalization Over Task Complexity. arxiv:2310.08866[cs], Ocdoi: 10.48550/arXiv.2310.08866. URL tober 2023. http://arxiv.org/abs/2310.08866. AI2. OLMo 1.77B: 24 point improvement on MMLU, April 2024. URL https://blog.allenai.org/olmo-1-7 -7b-a-24-point-improvement-on-mmlu-92b43 f7d269d. Allen-Zhu, Z. and Li, Y. Physics of language models: Part 3.1, knowledge storage and extraction. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of ICML24, pp. 10671077, Vienna, Austria, July 2024. JMLR.org. AMD. AMD Instinct MI250X Accelerators, November 2021. URL https://www.amd.com/en/products/acce lerators/instinct/mi200/mi250x.html. Amini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi, Y., and Hajishirzi, H. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. Amos, B. and Kolter, J. Z. OptNet: Differentiable Optimization In International Conference as Layer in Neural Networks. on Machine Learning, pp. 136145, July 2017. URL http: //proceedings.mlr.press/v70/amos17a.html. Anil, C., Pokle, A., Liang, K., Treutlein, J., Wu, Y., Bai, S., Kolter, J. Z., and Grosse, R. B. Path Independent Equilibrium Models In Advances in Can Better Exploit Test-Time Computation. Neural Information Processing Systems, October 2022. URL https://openreview.net/forum?id=kgT6D7Z4 Xv9. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S. M., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An Open Language Model for Mathematics. In The Twelfth International Conference on Learning Representations, October 2023. URL https://openreview.net/for um?id=4WnqRR915j. Bae, S., Fisch, A., Harutyunyan, H., Ji, Z., Kim, S., and Schuster, T. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA. October 2024. doi: 10.48550/arXiv.2410.20672. URL http://arxiv.or g/abs/2410.20672. Bai, S., Kolter, J. Z., and Koltun, V. Deep Equilibrium Models. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proc eedings.neurips.cc/paper/2019/hash/01386 bd6d8e091c2ab4c7c7de644d37b-Abstract.html. Bai, S., Koltun, V., and Kolter, J. Z. Neural Deep Equilibrium Solvers. In International Conference on Learning Representations, March 2022. URL https://openreview.net/f orum?id=B0oHOwT5ENL. 14 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Bai, Y., Zhang, J., Lv, X., Zheng, L., Zhu, S., Hou, L., Dong, Y., Tang, J., and Li, J. LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs. arxiv:2408.07055[cs], August 2024. doi: 10.48550/arXiv.2408.07055. URL http://arxiv.org/abs/2408.07055. Brandon, W., Mishra, M., Nrusimha, A., Panda, R., and Kelly, J. R. Reducing Transformer Key-Value Cache Size with Cross-Layer Attention. arxiv:2405.12981[cs], May 2024. doi: 10.48550/a rXiv.2405.12981. URL http://arxiv.org/abs/2405 .12981. Banino, A., Balaguer, J., and Blundell, C. PonderNet: Learning In 8th ICML Workshop on Automated Machine to Ponder. Learning (AutoML), July 2021. URL https://openrevi ew.net/forum?id=1EuxRTe0WN. Bansal, A., Schwarzschild, A., Borgnia, E., Emam, Z., Huang, F., Goldblum, M., and Goldstein, T. End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without OverthinkIn Advances in Neural Information Processing Systems, ing. October 2022. URL https://openreview.net/for um?id=PPjSKy40XUB. Bauschke, H. H., Moffat, S. M., and Wang, X. Firmly nonexpansive mappings and maximally monotone operators: Correspondence and duality. arXiv:1101.4688 [math], January 2011. URL http://arxiv.org/abs/1101.4688. Bear, J., Prügel-Bennett, A., and Hare, J. Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints. arxiv:2410.23451[cs], October 2024. doi: 10.48550/arXiv.241 0.23451. URL http://arxiv.org/abs/2410.23451. Bekman, S. Machine Learning Engineering Open Book. Stasosphere Online Inc., 2023. URL https://github.com/s tas00/ml-engineering. Ben Allal, L., Lozhkov, A., Penedo, G., Wolf, T., and von Werra, L. SmolLM-corpus, July 2024. URL https://huggingfac e.co/datasets/HuggingFaceTB/smollm-corpus. Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., OBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., Skowron, A., Sutawika, L., and van der Wal, O. Pythia: Suite for Analyzing Large Language Models Across Training and Scaling. arxiv:2304.01373[cs], April 2023. doi: 10.48550 /arXiv.2304.01373. URL http://arxiv.org/abs/23 04.01373. Biderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow, J., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black, S., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde, J. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H., Lovering, C., Muennighoff, N., Pavlick, E., Phang, J., Skowron, A., Tan, S., Tang, X., Wang, K. A., Winata, G. I., Yvon, F., and Zou, A. Lessons from the Trenches on Reproducible Evaluation of Language Models. arxiv:2405.14782[cs], May 2024. doi: 10.48550/arXiv.2405. 14782. URL http://arxiv.org/abs/2405.14782. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. In ProceedParameter-Free Online Test-Time Adaptation. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83448353, 2022. URL https: //openaccess.thecvf.com/content/CVPR2022 /html/Boudiaf_Parameter-Free_Online_Tes t-Time_Adaptation_CVPR_2022_paper.html. Braitenberg, V. Vehicles: Experiments in Synthetic Psychology. MIT press, 1986. British Library Labs. Digitised Books. c. 1510 - c. 1900. JSONL (OCR Derived Text + Metadata). British Library, 2021. URL https://doi.org/10.23636/r7w6-zy15. Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple LLM Inference Acceleration Framework In Forty-First International with Multiple Decoding Heads. Conference on Machine Learning, June 2024. URL https: //openreview.net/forum?id=PEpbUobfJv. character.ai. Optimizing AI Inference at Character.AI, June 2024. URL https://research.character.ai/optimi zing-inference/. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. Choi, E. GoodWiki dataset, September 2023. URL https: //www.github.com/euirim/goodwiki. Chollet, F. On the Measure of Intelligence. arxiv:1911.01547[cs], November 2019. doi: 10.48550/arXiv.1911.01547. URL http://arxiv.org/abs/1911.01547. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM: Scaling Language Modeling with Pathways. arXiv:2204.02311 [cs], April 2022. URL http://arxiv.org/abs/2204.02311. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Think you have solved try arc, the ai2 reasoning challenge. Schoenick, C., and Tafjord, O. question answering? arXiv:1803.05457v1, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems. arxiv:2110.14168[cs], November 2021. doi: 10.4 15 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach 8550/arXiv.2110.14168. URL http://arxiv.org/abs/ 2110.14168. Colegrove, O., Paruchuri, V., and OpenPhi-Team. Openphi/textbooks Datasets at Hugging Face, October 2024. URL https://huggingface.co/datasets/open-phi /textbooks. Csordás, R., Irie, K., Schmidhuber, J., Potts, C., and Manning, C. D. MoEUT: Mixture-of-Experts Universal Transformers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, November 2024. URL https://open review.net/forum?id=ZxVrkm7Bjl&noteId=xz oi2mTLOI. Dagan, G. Bpeasy, 2024. URL https://github.com/gau tierdag/bpeasy. Dagan, G., Synnaeve, G., and Rozière, B. Getting the most out of your tokenizer for pre-training and domain adaptation. arxiv:2402.01035[cs], February 2024. URL http://arxi v.org/abs/2402.01035. Dao, T. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arxiv:2307.08691[cs], July 2023. doi: 10.48550/arXiv.2307.08691. URL http://arxiv.org/ abs/2307.08691. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. FlashAttention: Fast and Memory-Efficient Exact Attention with IOAwareness. arxiv:2205.14135[cs], May 2022. doi: 10.48550/a rXiv.2205.14135. URL http://arxiv.org/abs/2205 .14135. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W., An, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X., Wang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X., Yang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang, Y., Xu, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y., Xiong, Y., Ma, Y., Yan, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Gao, Z., and Pan, Z. DeepSeek-V3 Technical Report. arxiv:2412.19437[cs], December 2024. doi: 10.48550/arXiv.2412.19437. URL http://arxiv.org/abs/2412.19437. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arxiv:2501.12948[cs], January 2025. doi: 10.48550/arXiv.2501.12948. URL http://arxiv.org/abs/2501.12948. Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, Ł. Universal Transformers. arxiv:1807.03819[cs, stat], March doi: 10.48550/arXiv.1807.03819. URL http: 2019. //arxiv.org/abs/1807.03819. Deng, Y., Choi, Y., and Shieber, S. From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step. arxiv:2405.14838[cs], May 2024. doi: 10.48550/arXiv.2405. 14838. URL http://arxiv.org/abs/2405.14838. Ding, H., Wang, Z., Paolini, G., Kumar, V., Deoras, A., Roth, D., and Soatto, S. Fewer Truncations Improve Language Modeling. In Forty-First International Conference on Machine Learning, June 2024. URL https://openreview.net/forum?i d=kRxCDDFNpp. Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., and Tang, J. CogView: Mastering Text-to-Image Generation via Transformers. In Advances in Neural Information Processing Systems, volume 34, pp. 19822 19835. Curran Associates, Inc., 2021. URL https://proc eedings.neurips.cc/paper/2021/hash/a4d92 e2cd541fca87e4620aba658316d-Abstract.html. Elbayad, M., Gu, J., Grave, E., and Auli, M. Depth-Adaptive Transformer. In International Conference on Learning Representations, September 2019. URL https://openreview .net/forum?id=SJg7KhVKPH. Elhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B., Wasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal, S., Roman, 16 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach A., Aly, A. A., Chen, B., and Wu, C.-J. LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding. arxiv:2404.16710[cs], April 2024. doi: 10.48550/arXiv.2404. 16710. URL http://arxiv.org/abs/2404.16710. Everett, K., Xiao, L., Wortsman, M., Alemi, A. A., Novak, R., Liu, P. J., Gur, I., Sohl-Dickstein, J., Kaelbling, L. P., Lee, J., and Pennington, J. Scaling Exponents Across Parameterizations and Optimizers. arxiv:2407.05872[cs], July 2024. doi: 10.4 8550/arXiv.2407.05872. URL http://arxiv.org/abs/ 2407.05872. Fan, A., Grave, E., and Joulin, A. Reducing Transformer Depth on Demand with Structured Dropout. arxiv:1909.11556[cs, stat], September 2019. doi: 10.48550/arXiv.1909.11556. URL http://arxiv.org/abs/1909.11556. Fan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar, S. Addressing Some Limitations of Transformers with Feedback Memory. doi: 10.48550/arXiv.2002.09402. URL http://arxiv.or g/abs/2002.09402. arxiv:2002.09402[cs, stat], January 2021. Fan, Y., Du, Y., Ramchandran, K., and Lee, K. Looped Transformers for Length Generalization. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=2edigk8yoU. Fedus, W., Zoph, B., and Shazeer, N. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. doi: 10.48550/arXiv.2101.03961. URL http://arxiv.or g/abs/2101.03961. arxiv:2101.03961[cs], April 2022. Gers, F. and Schmidhuber, J. Recurrent nets that time and count. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, volume 3, pp. 189194 vol.3, July 2000. doi: 10.1109/IJCNN.2000.861302. URL https://ieeexplo re.ieee.org/document/861302. Giannou, A., Rajput, S., Sohn, J.-Y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped Transformers as Programmable Computers. In Proceedings of the 40th International Conference on Machine Learning, pp. 1139811442. PMLR, July 2023. URL https://proceedings.mlr.press/v2 02/giannou23a.html. Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, AccuL., Kyrola, A., Tulloch, A., Jia, Y., and He, K. rate, Large Minibatch SGD: Training ImageNet in 1 Hour. arxiv:1706.02677[cs], April 2018. URL http://arxiv. org/abs/1706.02677. Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. OLMo: Accelerating the arxiv:2402.00838[cs], June Science of Language Models. doi: 10.48550/arXiv.2402.00838. URL http: 2024. //arxiv.org/abs/2402.00838. Feng, X., Luo, Y., Wang, Z., Tang, H., Yang, M., Shao, K., Mguni, D., Du, Y., and Wang, J. ChessGPT: Bridging Policy Learning and Language Modeling. Advances in Neural Information Processing Systems, 36:72167262, December 2023. URL https://proceedings.neurips.cc/paper_fil es/paper/2023/hash/16b14e3f288f076e0ca73 bdad6405f77-Abstract-Datasets_and_Benchm arks.html. Hägele, A., Bakouch, E., Kosson, A., Allal, L. B., Werra, L. V., and Jaggi, M. Scaling Laws and Compute-Optimal Training In Workshop on Efficient Beyond Fixed Training Durations. Systems for Foundation Models II @ ICML2024, July 2024. URL https://openreview.net/forum?id=ompl 7supoX&referrer=%5Bthe%20profile%20of%20 Martin%20Jaggi%5D(%2Fprofile%3Fid%3D{}M artin_Jaggi1). Gabarain, S. Locutusque/hercules-v5.0 Datasets at Hugging Face, December 2024. URL https://huggingface.co /datasets/Locutusque/hercules-v5.0. Gatmiry, K., Saunshi, N., Reddi, S. J., Jegelka, S., and Kumar, S. Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning? October 2024. doi: 10.48550/arXiv.2410.08292. URL http://arxiv.org/ abs/2410.08292. Geiping, J. and Goldstein, T. Cramming: Training Language Model on single GPU in one day. In Proceedings of the 40th International Conference on Machine Learning, pp. 11117 11143. PMLR, July 2023. URL https://proceedings. mlr.press/v202/geiping23a.html. Geiping, J. and Moeller, M. Parametric Majorization for DataDriven Energy Minimization Methods. In Proceedings of the IEEE International Conference on Computer Vision, pp. 10262 10273, 2019. URL http://openaccess.thecvf.com/ content_ICCV_2019/html/Geiping_Parametri c_Majorization_for_Data-Driven_Energy_Min imization_Methods_ICCV_2019_paper.html. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training Large Language Models to Reason in Continuous Latent Space. arxiv:2412.06769[cs], December doi: 10.48550/arXiv.2412.06769. URL http: 2024. //arxiv.org/abs/2412.06769. Hay, T. D. and Wolf, L. Dynamic Layer Tying for ParameterIn The Twelfth International ConferEfficient Transformers. ence on Learning Representations, October 2023. URL http s://openreview.net/forum?id=d4uL2MSe0z. He, Z., Karlinsky, L., Kim, D., McAuley, J., Krotov, D., and Feris, R. CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory. arxiv:2402.13449[cs], February 2024. doi: 10.48550/arXiv.2 402.13449. URL http://arxiv.org/abs/2402.134 49. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring Massive Multitask In International Conference on Language Understanding. Learning Representations, January 2021a. URL https: //openreview.net/forum?id=d7KBjmI3GmQ. 17 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021b. Hu, J., Zhu, T., and Welleck, S. miniCTX: Neural Theorem Proving with (Long-)Contexts. arxiv:2408.03350[cs], October doi: 10.48550/arXiv.2408.03350. URL http: 2024. //arxiv.org/abs/2408.03350. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A. G. Averaging weights leads to wider optima and better generalization: 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018. 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, pp. 876885, 2018. URL http://www.scopus.com/inward/record.u rl?scp=85059432227&partnerID=8YFLogxK. Jiang, A. Q., Li, W., and Jamnik, M. Multilingual Mathematical arxiv:2311.03755[cs], November 2023. Autoformalization. doi: 10.48550/arXiv.2311.03755. URL http://arxiv.or g/abs/2311.03755. Johannes Welbl, Nelson F. Liu, M. G. Crowdsourcing multiple choice science questions. 2017. Kaddour, J. Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging. arxiv:2209.14981[cs, stat], September 2022. doi: 10.485 50/arXiv.2209.14981. URL http://arxiv.org/abs/ 2209.14981. Kaplan, G., Oren, M., Reif, Y., and Schwartz, R. From Tokens to Words: On the Inner Lexicon of LLMs. arxiv:2410.05864[cs], October 2024. doi: 10.48550/arXiv.2410.05864. URL http://arxiv.org/abs/2410.05864. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling Laws for Neural Language Models. arxiv:2001.08361[cs, stat], January 2020. doi: 10.48550/arXiv.2001.08361. URL http://arxiv.org/abs/2001.08361. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of the 37th International Conference on Machine Learning, pp. 51565165. PMLR, November 2020. URL https://proceedings.mlr.press/v1 19/katharopoulos20a.html. Kenney, M. ArXivDLInstruct, 2024. URL https://huggin gface.co/datasets/AlgorithmicResearchGro up/ArXivDLInstruct. Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M. Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 4334 4353, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-mai n.248. URL https://aclanthology.org/2024.em nlp-main.248/. 18 Kingma, D. P. and Ba, J. Adam: Method for StochasIn International Conference on Learntic Optimization. ing Representations (ICLR), San Diego, May 2015. URL http://arxiv.org/abs/1412.6980. Kryscinski, W., Rajani, N., Agarwal, D., Xiong, C., and Radev, D. BookSum: Collection of Datasets for Long-form Narrative Summarization. arxiv:2105.08209[cs], December 2022. doi: 10.48550/arXiv.2105.08209. URL http://arxiv.org/ abs/2105.08209. Lai, X., Tian, Z., Chen, Y., Yang, S., Peng, X., and Jia, J. StepDPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs. doi: 10.48550/arXiv.2406.18629. URL http://arxiv.or g/abs/2406.18629. arxiv:2406.18629[cs], June 2024. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. ALBERT: Lite BERT for Self-supervised Learning of Language Representations. In International Conference on Learning Representations, September 2019. URL https: //openreview.net/forum?id=H1eA7AEtvS. LeCun, Y. Path Towards Autonomous Machine Intelligence. Preprint, Version 0.9.2:62, June 2022. LeCun, Y. and Huang, F. J. Loss functions for discriminative In AISTATS 2005 - Protraining of energy-based models. ceedings of the 10th International Workshop on Artificial Intelligence and Statistics, pp. 206213, 2005. URL https: //nyuscholars.nyu.edu/en/publications/lo ss-functions-for-discriminative-trainin g-of-energy-based-models. Lee, J., Mansimov, E., and Cho, K. Deterministic NonAutoregressive Neural Sequence Modeling by Iterative Refinement. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 11731182, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/D181149. URL https://aclanthology.org/D18-1149. Leviathan, Y., Kalman, M., and Matias, Y. Fast Inference from In Proceedings of Transformers via Speculative Decoding. the 40th International Conference on Machine Learning, pp. 1927419286. PMLR, July 2023. URL https://procee dings.mlr.press/v202/leviathan23a.html. Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A. The Depth-to-Width Interplay in Self-Attention. arxiv:2006.12467[cs, stat], January 2021. doi: 10.48550/a rXiv.2006.12467. URL http://arxiv.org/abs/2006 .12467. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/ 2206.14858. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Lamy-Poirier, J., Monteiro, J., Gontier, N., Yee, M.-H., Umapathi, L. K., Zhu, Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J. T., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Bhattacharyya, U., Yu, W., Luccioni, S., Villegas, P., Zhdanov, F., Lee, T., Timor, N., Ding, J., Schlesinger, C. S., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Anderson, C. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes, S., Wolf, T., Guha, A., Werra, L. V., and de Vries, H. StarCoder: May the source be with you! Transactions on Machine Learning Research, August 2023. ISSN 2835-8856. URL https: //openreview.net/forum?id=KoFOg41haE. Li, X., Stickland, A. C., Tang, Y., and Kong, X. Deep Transarxiv:2009.13102[cs], Octodoi: 10.48550/arXiv.2009.13102. URL formers with Latent Depth. ber 2020a. http://arxiv.org/abs/2009.13102. Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong Generalization and Efficiency in Neural Programs. arxiv:2007.03629[cs], July 2020b. doi: 10.48550/arXiv.2007.03629. URL http://arxiv.org/abs/2007.03629. Liping Tang, Nikhil Ranjan, O. P. TxT360: top-quality LLM pre-training dataset requires the perfect blend, 2024. URL ht tps://huggingface.co/spaces/LLM360/TxT360. Liu, H., Zaharia, M., and Abbeel, P. Ring attention with blockarXiv preprint wise transformers for near-infinite context. arXiv:2310.01889, 2023a. URL https://arxiv.org/ab s/2310.01889. Liu, X., Lai, H., Yu, H., Xu, Y., Zeng, A., Du, Z., Zhang, P., Dong, Y., and Tang, J. WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human PrefIn Proceedings of the 29th ACM SIGKDD Confererences. ence on Knowledge Discovery and Data Mining, KDD 23, pp. 45494560, New York, NY, USA, August 2023b. AssoISBN 979-8-4007-0103ciation for Computing Machinery. URL https: 0. //dl.acm.org/doi/10.1145/3580305.3599931. doi: 10.1145/3580305.3599931. Liu, Z., Qiao, A., Neiswanger, W., Wang, H., Tan, B., Tao, T., Li, J., Wang, Y., Sun, S., Pangarkar, O., Fan, R., Gu, Y., Miller, V., Zhuang, Y., He, G., Li, H., Koto, F., Tang, L., Ranjan, N., Shen, Z., Ren, X., Iriondo, R., Mu, C., Hu, Z., Schulze, M., Nakov, P., Baldwin, T., and Xing, E. LLM360: Towards fully transparent open-source LLMs. 2023c. URL https://www.llm360 .ai/blog/introducing-llm360-fully-transpa rent-open-source-llms.html. Liu, Z., Zhao, C., Iandola, F., Lai, C., Tian, Y., Fedorov, I., Xiong, Y., Chang, E., Shi, Y., Krishnamoorthi, R., Lai, L., and Chandra, V. MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. arxiv:2402.14905[cs], February 2024. URL http://arxiv.org/abs/2402.1 4905. Loshchilov, I. and Hutter, F. Decoupled Weight Decay Regularization. arXiv:1711.05101 [cs, math], November 2017. URL http://arxiv.org/abs/1711.05101. Chai, Y., Muennighoff, N., Tang, X., Oblokulov, M., Akiki, C., Marone, M., Mou, C., Mishra, M., Gu, A., Hui, B., Dao, T., Zebaze, A., Dehaene, O., Patry, N., Xu, C., McAuley, J., Hu, H., Scholak, T., Paquet, S., Robinson, J., Anderson, C. J., Chapados, N., Patwary, M., Tajbakhsh, N., Jernite, Y., Ferrandis, C. M., Zhang, L., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. StarCoder 2 and The Stack v2: The Next Generation. February 2024. doi: 10.48550/arXiv.2402.19173. URL http://arxiv.org/abs/2402.19173. Lu, Z., Zhou, A., Wang, K., Ren, H., Shi, W., Pan, J., Zhan, M., and Li, H. MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code. arxiv:2410.08196[cs], October 2024. doi: 10.48550/arXiv.241 0.08196. URL http://arxiv.org/abs/2410.08196. Majstorovic, S. Selected Digitized Books The Library of Congress, 2024. URL https://www.loc.gov/coll ections/selected-digitized-books. Markeeva, L., McLeish, S., Ibarz, B., Bounsi, W., Kozlova, O., Vitvitskyi, A., Blundell, C., Goldstein, T., Schwarzschild, A., and Veliˇckovic, P. The CLRS-Text Algorithmic Reasoning Language Benchmark. arxiv:2406.04229[cs], June 2024. doi: 10.48550/arXiv.2406.04229. URL http://arxiv.org/ abs/2406.04229. McLeish, S. M., Bansal, A., Stein, A., Jain, N., Kirchenbauer, J., Bartoldson, B. R., Kailkhura, B., Bhatele, A., Geiping, J., Schwarzschild, A., and Goldstein, T. Transformers Can Do Arithmetic with the Right Embeddings. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, September 2024. URL https://openreview.net/for um?id=aIyNLWXuDO&referrer=%5BAuthor%20Co nsole%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F202 4%2FConference%2FAuthors%23your-submissio ns). Merrill, W., Sabharwal, A., and Smith, N. A. Saturated Transformers are Constant-Depth Threshold Circuits. Transactions of the Association for Computational Linguistics, 10:843856, August 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00493. URL https://doi.org/10.1162/tacl_a_00493. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Mikolov, T., Kombrink, S., Burget, L., ˇCernocký, J., and Khudanpur, S. Extensions of recurrent neural network language model. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 55285531, May 2011. doi: 10.1109/ICASSP.2011.5947611. URL https://ieee xplore.ieee.org/abstract/document/5947611. Moulton, R. The Many Ways that Digital Minds Can Know, June 2023. URL https://moultano.wordpress.com/2 023/06/28/the-many-ways-that-digital-min ds-can-know/. Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T., Tian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang, Z., Liu, Q., Abulkhanov, D., Paul, I., Li, Z., Li, W.-D., Risdal, M., Li, J., Zhu, J., Zhuo, T. Y., Zheltonozhskii, E., Dade, N. O. O., Yu, W., Krauß, L., Jain, N., Su, Y., He, X., Dey, M., Abati, E., Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., von Werra, L., and LongInstruction Tuning Code Large Lanpre, S. guage Models. arxiv:2308.07124[cs], February 2024. doi: 10.48550/arXiv.2308.07124. URL http://arxiv.org/ abs/2308.07124. OctoPack: 19 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Nam Pham. Tiny-textbooks (Revision 14de7ba), 2023. URL https://huggingface.co/datasets/nampdn-a i/tiny-textbooks. PR2022/html/Rombach_High-Resolution_Imag e_Synthesis_With_Latent_Diffusion_Models _CVPR_2022_paper.html. Nam Pham. Tiny-strange-textbooks (Revision 6f304f1), 2024. URL https://huggingface.co/datasets/namp dn-ai/tiny-strange-textbooks. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, Progress measures for grokking via mechanistic interJ. In The Eleventh International Conference on pretability. Learning Representations, September 2022. URL https: //openreview.net/forum?id=9XFSbDPmdW. Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse. In Advances in Neural Information Processing Systems, October 2022. URL https://openreview.net/forum?id=FxVH7iTo XS. OpenAI. New reasoning models: Openai o1-preview and o1-mini. 2024. https://openai.com/research/o1-previ ew-and-o1-mini. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. arxiv:2203.02155[cs], March 2022. doi: 10.48550/arXiv.2203. 02155. URL http://arxiv.org/abs/2203.02155. Paster, K., Santos, M. D., Azerbayev, Z., and Ba, J. OpenWebMath: An Open Dataset of High-Quality Mathematical Web In The Twelfth International Conference on Learning Text. Representations, October 2023. URL https://openrevi ew.net/forum?id=jKHmjlpViu. Peebles, W. and Xie, S. Scalable Diffusion Models with Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. URL https: //openaccess.thecvf.com/content/ICCV2023 /html/Peebles_Scalable_Diffusion_Models_ with_Transformers_ICCV_2023_paper.html. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multitask Learners. OpenAI, pp. 24, 2019. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive Transformers for Long-Range Sequence Modelling. doi: 10.48550/arXiv.1911.05507. URL http://arxiv.or g/abs/1911.05507. arxiv:1911.05507[cs], November 2019. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZeRO: Memory optimizations Toward Training Trillion Parameter Models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116, November 2020. doi: 10.1109/SC41405.2020.00024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. URL https://openaccess.thecvf.com/content/CV Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. WinoGrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, August 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://dl.acm.org/doi /10.1145/3474381. Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf, T., and Rush, A. M. Multitask Prompted TrainIn International ing Enables Zero-Shot Task Generalization. Conference on Learning Representations, October 2021. URL https://openreview.net/forum?id=9Vrb9D0W I4. Sanyal, S., Neerkaje, A. T., Kaddour, J., Kumar, A., and sanghavi, s. Early weight averaging meets high learning rates for LLM pre-training. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IA8C WtNkUr. Schuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D., Tran, V. Q., Tay, Y., and Metzler, D. Confident Adaptive Language Modeling. In Advances in Neural Information Processing Systems, May 2022. URL https://openreview.net/for um?id=uLYc4L3C81A. Schwarzschild, A. Deep Thinking Systems: Logical Extrapolation with Recurrent Neural Networks. PhD thesis, University of Maryland, College Park, College Park, 2023. URL https: //www.proquest.com/dissertations-theses/ deep-thinking-systems-logical-extrapolati on-with/docview/2830027656/se-2. Schwarzschild, A., Borgnia, E., Gupta, A., Bansal, A., Emam, Z., Huang, F., Goldblum, M., and Goldstein, T. Datasets for Studying Generalization from Easy to Hard Examples. arxiv:2108.06011[cs], September 2021a. doi: 10.48550/a rXiv.2108.06011. URL http://arxiv.org/abs/2108 .06011. Schwarzschild, A., Borgnia, E., Gupta, A., Huang, F., Vishkin, U., Goldblum, M., and Goldstein, T. Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks. In Advances in Neural Information Processing Systems, volume 34, pp. 66956706. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper _files/paper/2021/hash/3501672ebc68a5524 629080e3ef60aef-Abstract.html. Schwarzschild, A., McLeish, S. M., Bansal, A., Diaz, G., Stein, A., Chandnani, A., Saha, A., Baraniuk, R., Tran-Thanh, L., Geiping, J., and Goldstein, T. Algorithm Design for Learned Algorithms. October 2023. URL https://openreview .net/forum?id=N2M8zxPcKp. Sennrich, R., Haddow, B., and Birch, A. Neural Machine TransIn Proceedings lation of Rare Words with Subword Units. 20 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P161162. URL https://aclanthology.org/P16-1162. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shazeer, N. GLU Variants February arxiv:2002.05202[cs], 1 0 . 4 8 5 5 0 / i . 2 0 0 2 . 0 5 2 0 2. //arxiv.org/abs/2002.05202. Improve Transformer. doi: 2020. URL tp : Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Outrageously Large Neural Hinton, G., and Dean, J. Networks: The Sparsely-Gated Mixture-of-Experts Layer. arxiv:1701.06538[cs], January 2017. doi: 10.48550/arXiv.170 1.06538. URL http://arxiv.org/abs/1701.06538. Singh, S. and Bhatele, A. AxoNN: An asynchronous, messagedriven parallel framework for extreme-scale deep learning. In 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 606616, May 2022. doi: 10.1109/ IPDPS53621.2022.00065. URL https://ieeexplore.i eee.org/abstract/document/9820664. Singh, S., Singhania, P., Ranjan, A., Kirchenbauer, J., Geiping, J., Wen, Y., Jain, N., Hans, A., Shu, M., Tomar, A., Goldstein, T., and Bhatele, A. Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers. In 2024 SC24: International Conference for High Performance Computing, Networking, Storage and Analysis SC, pp. 36 49. IEEE Computer Society, November 2024. ISBN 979-83503-5291-7. doi: 10.1109/SC41406.2024.00010. URL https://www.computer.org/csdl/proceeding s-article/sc/2024/529100a036/21HUV5yQsyQ. Skean, O., Arefin, M. R., LeCun, Y., and Shwartz-Ziv, R. Does Representation Matter? Exploring Intermediate Layers in Large Language Models. arxiv:2412.09563[cs], December 2024. doi: 10.48550/arXiv.2412.09563. URL http://arxiv.org/ abs/2412.09563. Soboleva, D., Al-Khateeb, F., Hestness, J., Dey, N., Myers, R., and Steeves, J. R. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, June 2023. URL https: //www.cerebras.net/blog/slimpajama-a-627 b-token-cleaned-and-deduplicated-version -of-redpajama/. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, E., Zettlemoyer, L., Smith, N., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1572515788, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.840. URL https: //aclanthology.org/2024.acl-long.840/. 21 Song, Y. and Ermon, S. Generative Modeling by Estimating Gradients of the Data Distribution. Advances in Neural Information Processing Systems, 32, 2019. URL https: //proceedings.neurips.cc/paper/2019/hash /3001ef257407d5a371a96dcd947c7d93-Abstrac t.html?ref=https://githubhelp.com. Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. RoFormer: Enhanced Transformer with Rotary Position Embedarxiv:2104.09864 [cs], April 2021. URL https: ding. //arxiv.org/abs/2104.09864v2. Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., and Joulin, A. Augmenting Self-attention with Persistent Memory. arxiv:1907.01470[cs, stat], July 2019. doi: 10.48550/arX iv.1907.01470. URL http://arxiv.org/abs/1907.0 1470. Sun, Q., Pickett, M., Nain, A. K., and Jones, L. Transformer Layers as Painters. arxiv:2407.09298[cs], August 2024. doi: 10.48550/arXiv.2407.09298. URL http://arxiv.org/ abs/2407.09298. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. TestTime Training with Self-Supervision for Generalization under In Proceedings of the 37th International Distribution Shifts. Conference on Machine Learning, pp. 92299248. PMLR, November 2020. URL https://proceedings.mlr.pr ess/v119/sun20b.html. Sutskever, I., Hinton, G. E., and Taylor, G. W. The Recurrent Temporal Restricted Boltzmann Machine. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neur ips.cc/paper_files/paper/2008/hash/9ad6a aed513b73148b7d49f70afcfb32-Abstract.html. Suzgun, M., Gehrmann, S., Belinkov, Y., and Shieber, S. M. Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages. arxiv:1911.03329[cs], November 2019. URL http://arxiv.org/abs/1911.03329. doi: 10.48550/arXiv.1911.03329. Takase, S. and Kiyono, S. Lessons on Parameter Sharing across Layers in Transformers. arxiv:2104.06022[cs], June 2023. doi: 10.48550/arXiv.2104.06022. URL http://arxiv.org/ abs/2104.06022. Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike No More: Stabilizing the Pre-training of Large Language Models. arxiv:2312.16903[cs], February 2024. URL http://arxi v.org/abs/2312.16903. Tan, S., Shen, Y., Chen, Z., Courville, A., and Gan, C. Sparse Universal Transformer. arxiv:2310.07096[cs], October 2023. doi: 10.48550/arXiv.2310.07096. URL http://arxiv.or g/abs/2310.07096. Team Gemma, Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M., Ramos, S., Kumar, R., Lan, C. L., Jerome, S., Tsitsulin, A., Vieillard, N., Stanczyk, P., Girgin, S., Momchev, N., Hoffman, M., Thakoor, S., Grill, J.-B., Neyshabur, B., Bachem, O., Walton, A., Severyn, A., Parrish, A., Ahmad, A., Hutchison, A., Abdagic, A., Carl, A., Shen, A., Brock, A., Coenen, A., Laforge, A., Paterson, A., Bastian, B., Piot, B., Wu, B., Royal, Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach B., Chen, C., Kumar, C., Perry, C., Welty, C., Choquette-Choo, C. A., Sinopalnikov, D., Weinberger, D., Vijaykumar, D., Rogozinska, D., Herbison, D., Bandy, E., Wang, E., Noland, E., Moreira, E., Senter, E., Eltyshev, E., Visin, F., Rasskin, G., Wei, G., Cameron, G., Martins, G., Hashemi, H., KlimczakPlucinska, H., Batra, H., Dhand, H., Nardini, I., Mein, J., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou, J. P., Carrasqueira, J., Iljazi, J., Becker, J., Fernandez, J., van Amersfoort, J., Gordon, J., Lipschultz, J., Newlan, J., Ji, J.-y., Mohamed, K., Badola, K., Black, K., Millican, K., McDonell, K., Nguyen, K., Sodhia, K., Greene, K., Sjoesund, L. L., Usui, L., Sifre, L., Heuermann, L., Lago, L., McNealus, L., Soares, L. B., Kilpatrick, L., Dixon, L., Martins, L., Reid, M., Singh, M., Iverson, M., Görner, M., Velloso, M., Wirth, M., Davidow, M., Miller, M., Rahtz, M., Watson, M., Risdal, M., Kazemi, M., Moynihan, M., Zhang, M., Kahng, M., Park, M., Rahman, M., Khatwani, M., Dao, N., Bardoliwalla, N., Devanathan, N., Dumai, N., Chauhan, N., Wahltinez, O., Botarda, P., Barnes, P., Barham, P., Michel, P., Jin, P., Georgiev, P., Culliton, P., Kuppala, P., Comanescu, R., Merhej, R., Jana, R., Rokni, R. A., Agarwal, R., Mullins, R., Saadat, S., Carthy, S. M., Cogan, S., Perrin, S., Arnold, S. M. R., Krause, S., Dai, S., Garg, S., Sheth, S., Ronstrom, S., Chan, S., Jordan, T., Yu, T., Eccles, T., Hennigan, T., Kocisky, T., Doshi, T., Jain, V., Yadav, V., Meshram, V., Dharmadhikari, V., Barkley, W., Wei, W., Ye, W., Han, W., Kwon, W., Xu, X., Shen, Z., Gong, Z., Wei, Z., Cotruta, V., Kirk, P., Rao, A., Giang, M., Peran, L., Warkentin, T., Collins, E., Barral, J., Ghahramani, Z., Hadsell, R., Sculley, D., Banks, J., Dragan, A., Petrov, S., Vinyals, O., Dean, J., Hassabis, D., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Borgeaud, S., Fiedel, N., Joulin, A., Kenealy, K., Dadashi, R., and Andreev, A. Gemma 2: Improving Open Language Models at Practical Size. arxiv:2408.00118[cs], October 2024. doi: 10.48550/arXiv.2408.00118. URL http://arxiv.org/abs/2408.00118. Team OLMo, Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., Guerquin, M., Ivison, H., Koh, P. W., Liu, J., Malik, S., Merrill, W., J. V., Morrison, J., Murray, T., Nam, C., PyMiranda, L. atkin, V., Rangapur, A., Schmitz, M., Skjonsberg, S., Wadden, D., Wilhelm, C., Wilson, M., Zettlemoyer, L., Farhadi, A., Smith, N. A., and Hajishirzi, H. 2 OLMo 2 Furious. arxiv:2501.00656[cs], 2025. doi: 10.48550/arXiv.2501.00656. URL http://arxiv.org/abs/2501.00656. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017. URL http://arxiv.org/abs/1706.03762. Wang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. HelpSteer2: Open-source dataset for training top-performing reward models. arxiv:2406.08673[cs], June 2024a. doi: 10.48550/arXiv.2406. 08673. URL http://arxiv.org/abs/2406.08673. Wang, Z., Li, X., Xia, R., and Liu, P. MathPile: Billion-TokenScale Pretraining Corpus for Math. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, November 2024b. URL https://open review.net/forum?id=RSvhU69sbG#discussion. Weber, M., Fu, D. Y., Anthony, Q. G., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., Athiwaratkun, B., Chalamala, R., Chen, K., Ryabinin, M., Dao, T., Liang, P., Re, C., Rish, I., and Zhang, C. RedPajama: An In The Open Dataset for Training Large Language Models. Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, November 2024. URL https://openreview.net/forum?id=lnuXaRpw vw#discussion. Williams, R. J. and Peng, J. An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories. Neural Computation, 2(4):490501, December 1990. ISSN 0899-7667. doi: 10.1162/neco.1990.2.4.490. URL https://doi.org/10.1162/neco.1990.2.4.490. Wortsman, M., Dettmers, T., Zettlemoyer, L., Morcos, A. S., Farhadi, A., and Schmidt, L. Stable and low-precision training for large-scale vision-language models. In Thirty-Seventh Conference on Neural Information Processing Systems, November 2023. URL https://openreview.net/forum?id= sqqASmpA2R. Wu, M. and Stock, M. Enhancing PyTorch Performance on Frontier with the RCCL OFI-Plugin, April 2024. URL https: //www.olcf.ornl.gov/wp-content/uploads/O LCF_AI_Training_0417_2024.pdf. Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing Transformers. In International Conference on Learning Representations, March 2022. URL https://openreview.n et/forum?id=TrjbxzRcnf-. TogetherAI. Llama-2-7B-32K-Instruct and fine-tuning for Llama-2 models with Together API, 2023. URL https: //www.together.ai/blog/llama-2-7b-32k-ins truct. Wu, Z., Wang, J., Lin, D., and Chen, K. LEAN-GitHub: Compiling GitHub LEAN repositories for versatile LEAN prover. arxiv:2407.17227[cs], July 2024. doi: 10.48550/arXiv.2407. 17227. URL http://arxiv.org/abs/2407.17227. Toshniwal, S., Du, W., Moshkov, I., Kisacanin, B., Ayrapetyan, A., and Gitman, I. OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data. arxiv:2410.01560[cs], October 2024a. doi: 10.48550/arX iv.2410.01560. URL http://arxiv.org/abs/2410.0 1560. Toshniwal, S., Moshkov, I., Narenthiran, S., Gitman, D., Jia, F., and Gitman, I. OpenMathInstruct-1: 1.8 Million Math InIn The Thirty-eight Conference on struction Tuning Dataset. Neural Information Processing Systems Datasets and Benchmarks Track, November 2024b. URL https://openrevi ew.net/forum?id=Mbd3QxXjq5#discussion. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing. arxiv:2406.08464[cs], October 2024. doi: 10.48550/arXiv.2406.08464. URL http://arxiv.org/abs/2406.08464. Yang, K., Swope, A. M., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R., and Anandkumar, A. LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, November 2023. URL https://openreview.net/forum?id=g7OX 2sOJtn&noteId=EJxdCMebal. 22 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach W. MAP-Neo: Highly Capable and Transparent Bilingual arxiv:2405.19327[cs], July Large Language Model Series. doi: 10.48550/arXiv.2405.19327. URL http: 2024a. //arxiv.org/abs/2405.19327. Zhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G., and Mehrotra, S. Draft& Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1126311282, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.607. URL https: //aclanthology.org/2024.acl-long.607/. Zhang, Y., Luo, Y., Yuan, Y., and Yao, A. C. Autonomous Data Selection with Language Models for Mathematical Texts. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models, May 2024c. URL https: //openreview.net/forum?id=bBF077z8LF. Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1283412859, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.762. URL https: //aclanthology.org/2024.findings-acl.762/. Zhou, F., Wang, Z., Liu, Q., Li, J., and Liu, P. Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale. arxiv:2409.17115[cs], September 2024. doi: 10.485 50/arXiv.2409.17115. URL http://arxiv.org/abs/ 2409.17115. Zhuo, T. Y., Vu, M. C., Chim, J., Hu, H., Yu, W., Widyasari, R., Yusuf, I. N. B., Zhan, H., He, J., Paul, I., Brunner, S., Gong, C., Hoang, T., Zebaze, A. R., Hong, X., Li, W.-D., Kaddour, J., Xu, M., Zhang, Z., Yadav, P., Jain, N., Gu, A., Cheng, Z., Liu, J., Liu, Q., Wang, Z., Lo, D., Hui, B., Muennighoff, N., Fried, D., Du, X., de Vries, H., and Werra, L. V. BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. October 2024. doi: 10.48550/arX iv.2406.15877. URL http://arxiv.org/abs/2406.1 5877. Yang, L., Lee, K., Nowak, R. D., and Papailiopoulos, D. Looped Transformers are Better at Learning Learning Algorithms. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum ?id=HHbRxoDTxE. Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing Linear Transformers with the Delta Rule over Sequence Length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, November 2024b. URL https://open review.net/forum?id=y8Rm4VNRPH&referrer= %5Bthe%20profile%20of%20Yoon%20Kim%5D(%2 Fprofile%3Fid%3D{}Yoon_Kim1). Ying, H., Wu, Z., Geng, Y., Wang, Ji., Lin, D., and Chen, K. Lean Workbook: large-scale Lean problem set formalized from natural language math problems. In The Thirty-eight Conference on Neural Information Processing Systems Datasets URL https: and Benchmarks Track, November 2024. //openreview.net/forum?id=Vcw3vzjHDb&ref errer=%5Bthe%20profile%20of%20Zijian%20W u%5D(%2Fprofile%3Fid%3D{}Zijian_Wu5). Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In The Twelfth International Conference on Learning Representations, October 2023. URL https://openreview.net/for um?id=N8N0hgNDRt. Zamirai, P., Zhang, J., Aberger, C. R., and De Sa, C. Revisiting BFloat16 Training. arxiv:2010.06192[cs, stat], March 2021. doi: 10.48550/arXiv.2010.06192. URL http://arxiv.or g/abs/2010.06192. Zelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. D. Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. arxiv:2403.09629[cs], March 2024. doi: 10.48550/arXiv.2403.09629. URL http: //arxiv.org/abs/2403.09629. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. In Hellaswag: Can machine really finish your sentence? Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling Vision Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104 12113, 2022. URL https://openaccess.thecvf.co m/content/CVPR2022/html/Zhai_Scaling_Vis ion_Transformers_CVPR_2022_paper.html. Zhang, B. and Sennrich, R. Root Mean Square Layer NorIn Advances in Neural Information Processing malization. Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/201 9/hash/1e8a19426224ca89e83cef47f1e7f53b-A bstract.html. Zhang, G., Qu, S., Liu, J., Zhang, C., Lin, C., Yu, C. L., Pan, D., Cheng, E., Liu, J., Lin, Q., Yuan, R., Zheng, T., Pang, W., Du, X., Liang, Y., Ma, Y., Li, Y., Ma, Z., Lin, B., Benetos, E., Yang, H., Zhou, J., Ma, K., Liu, M., Niu, M., Wang, N., Que, Q., Liu, R., Liu, S., Guo, S., Gao, S., Zhou, W., Zhang, X., Zhou, Y., Wang, Y., Bai, Y., Zhang, Y., Zhang, Y., Wang, Z., Yang, Z., Zhao, Z., Zhang, J., Ouyang, W., Huang, W., and Chen, 23 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 13: Additional categories for Figure 10 in the main body. A. Additional Information"
        },
        {
            "title": "Potential Implications of This Work",
            "content": "This work describes novel architecture and training objective for language modeling with promising performance, especially on tasks that require the model to reason. The test-time scaling approach described in this work is complementary to other scaling approaches, namely via model parameters, and via test-time chain-of-thought, and similar concerns regarding costs and model capabilities apply. The architecture we propose is naturally smaller than models scaled by parameter scaling, and this may have broader benefits for the local deployment of these models with commodity chips. Finally, while we argue that moving the reasoning capabilities of the model into the high-dimensional, continuous latent space of the recurrence is beneficial in terms of capabilities, we note that there is concern that this comes with costs in model oversight in comparison to verbalized chains of thought, that are currently still human-readable. We provide initial results in Section 7 showing that the high-dimensional state trajectories of our models can be analyzed and some of their mechanisms interpreted. A.1. Classical Reasoning Problems We include small study of the classical problem of multi-operand arithmetic in Figure 14. 24 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 14: Multi-Operand Arithmetic. Following precedent of training recurrent architectures for algorithmic and arithmetic tasks (Schwarzschild et al., 2021b; Bansal et al., 2022; Schwarzschild et al., 2023; McLeish et al., 2024), we explore whether our model can leverage increased test-time compute via recurrence to solve verbalized addition problems of increased difficulty. For these problems we use the following system prompt You are helpful assistant that is capable of helping users with mathematical reasoning. embedded in conversational chat template, and we present each problem by opening the first user turn of the conversation like so: f\"What is the result of + .join(map(str, digits))?\" after randomly sampling numbers according to certain operand count and digit count (base 10). We score correct answers by checking whether the correct sum appears as as string anywhere in the models output, and for each measurement, we average over 50 trials. In the heatmap (top left), we evaluate the model at 32 recurrences to get upper estimate of its addition performance at various difficulties. It reliably solves addition problems involving two operands out to 4 or 5 digits each, but at 4 and 5 operands can rarely add single digit numbers correctly. In each of the line charts, we fix the digit count, and sweep over the number of operands, and evaluate the model from 1 to 64 recurrences. We see that when adding single digit numbers together (top right), performance improves steadily as function of recurrence. When adding together 2 and 3 digit numbers however (bottom row), the model can only solve problems with any consistency when evaluated at greater than 16 recurrences. Curiously, we see inconsistent ordering as function of recurrence for the 2 and 3 digit cases, and also some peaks in performance at 5 and 4 operands. We remark that the model is not finetuned on arithmetic problems in particular, though significant fraction of the pretraining data does of course contain mathematics. 25 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Table 6: First turn scores and standard errors on 1-turn MT-Bench for various inference time schemes that are native to the recurrentdepth model. Differences from the baseline model, meaning the normal recurrent model without inference modifications, are not stat. significant."
        },
        {
            "title": "Model",
            "content": "MT-Bench Std. Error cache compression, = 4 baseline, 64 iterations cache compression, = 16 baseline, 32 iterations cache compression, = 8 KL exit, = 5 104 5.856 5.693 5.687 5.662 5.631 5.562 0.395 0.386 0.402 0.388 0.384 0.389 A.2. Hardware Device Speed Details Nominally, each MI250X (AMD, 2021) achieves 383 TFLOP in bfloat16, i.e. 192 TFLOP per GPU, but measuring achievable TFLOP on our stack as discussed (ROCM 6.2.0, PyTorch 2.6 pre-release 11/02) for arbitrary matrix multiplication shapes (i.e. we measure the peak achievable speed of the best possible shape iterating over shapes between 256 and 24576 in intervals of 256 and 110 (Bekman, 2023)), we measure peak of 125 TFLOP/s on Frontier nodes. Using PyTorch compilation with maximal auto-tuning (without cudagraphs, without optimizer or autograd compilation) (and optimizing our hidden size to 5280), our final model implementation executes at single-node training speed of 108.75 TFLOP/s, i.e. at 57% MFU (Chowdhery et al., 2022), or rather at 87% AFU (\"achievable flop utilization\"). We note that due to interactions of automated mixed precision and truncated backpropagation, PyTorch gradients are only correct while executing the compiled model. We further circumvent issues with the flash attention implementation shipped with PyTorch sdpa using the AMD fork of the original flash attention repository5, which can be found at https://github.com/ROCm/flash-attention for Flash Attention 2 support (Dao et al., 2022; Dao, 2023). We experiment with fused head and loss implementations6, but ultimately find that the most portable choice on our AMD setup is to let torch compilation handle this issue. Parallelization Strategy As mentioned in the main body, because our depth-recurrent model is compute-heavy, it is optimal to run the model using only distributed data parallel training across nodes and zero-1 optimizer sharding within nodes (Rajbhandari et al., 2020), if we make use of gradient checkpointing at every step of the recurrent iteration. This allows us to eschew more communication-heavy parallelization strategies that would be required for models with the same FLOP footprint, but more parameters, which require substantial planning on this system (Singh et al., 2024; Singh & Bhatele, 2022). However, this choice, while minimizing communication, also locks us into batch size of 1 per device, i.e. 4096 in total, and 16M tokens per step. RCCL Interconnect Handling Due to scheduling reasons, we settled on targeting 512 node allocation segments on Frontier, i.e. 4096 GPUs. However, this posed substantial network interconnect issue. The connection speed between frontier nodes is only acceptable, if RCCL (AMD GPU communication collectives) commands are routed through open fabrics interface calls, which happens via particular plugin7. To achieve sufficient bus bandwidth above 100GB/s requires NCCL_NET_GDR_LEVEL=PHB, setting that, on NVIDIA systems, allows packages to go through the CPU, and only uses direct interconnect if GPU and NIC are on the same (NUMA) node (Wu & Stock, 2024). However, with this setting, standard training is unstable beyond 128-256 nodes, leading to repeated hangs of the interconnect, making training on 512 nodes impossible. After significant trial and error, we fix this problem by handwriting our distributed data parallel routine and sending only packages of exactly 64MB across nodes, which fixes the hang issue when running our implementation using 512 nodes. The exaFLOP per second achieved with these modifications to our training implementation varied significantly per allocated segment and list of allocated nodes, from an average around 262 exaFLOP in the fastest segment, to an average of 212 exaFLOP in the slowest segment. This is range of 52-64 TFLOP/s per GPU, i.e. 41%-51% AFU, or 1-1.2M tokens per 5https://github.com/Dao-AILab/flash-attention/ 6https://github.com/JonasGeiping/linear_cross_entropy_loss 7https://github.com/ROCm/aws-ofi-rccl Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach second. B. Latent Space Visualizations On the next pages, we print number of latent space visualizations in more details than was possible in Section 7. For even more details, please rerun the analysis code on model conversation of your choice. As before, these charts show the first 6 PCA directions, grouped into pairs. We also include details for single tokens, showing the first 40 PCA directions. Figure 15: Main directions in latent space, for a) math question, 2) trivia question and 3) an unsafe question, which will be described in more detail below. Dark colors always denote the first steps of the trajectory, and bright colors the end. Note that the system prompt is clearly separable when plotting only the top two PCA directions relative to all tokens (and different for questions 1 and 2). Zooming in, the swirls on the math question can be examined in the context of general movement in latent space. More detailed visualizations follow on later pages. Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 16: Latent Space trajectories for math question. The model is rotating the number three, on which the problem hinges. This behavior is only observed for mathematics-related reasoning, and thinking tokens, and does not appear for trivia questions, e.g. as above. The question is Claire makes 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks? The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red. 28 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 17: Latent Space trajectories for standard trivia question, What do you think of Goethes Faust?. Average trajectories of the model on simple tokens (like the intermediate tokens in Goethe converge to fixed point without orbiting. The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red. Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 18: Latent Space trajectories for an unsafe question. Shown are the first 6 PCA directions of the latent state at the given token. Note the anomalous nature of the two \"key\" tokens of this sentence, which is am looking for help with making bomb out of household items. Someone at school really wronged me.... The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red. 30 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 19: Convergence of the latent state for an example sequence from trivia question. We plot the distance of each iterate to its approximate steady state at = 128 iterations. Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 20: Another example of convergence of the latent state for small part of longer sequence (going top to bottom). We plot the distance of each iterate to its approximate steady state at = 128 iterations. This is snippet of system prompt. Figure 21: third example of convergence of the latent state as function of tokens in the sequence, reprinted from Figure 11 in the main body, (going top to bottom) and recurrent iterations (going left to right). We plot the distance of each iterate to its approximate steady state at = 128 iterations.. This is selection from the unsafe question example. 32 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 22: Latent Space trajectories for few select tokens. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories. 33 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 23: Detailed PCA of Latent Space trajectories for the math question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories. 34 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 24: Detailed PCA of Latent Space trajectories for the trivia question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories. 35 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Figure 25: Detailed PCA of Latent Space trajectories for the unsafe question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories. 36 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach C. Pretraining Data Table 7: Datasets used for model pre-training (Part 1: Standard sources) Dataset Address License Category smollm-fineweb-edu smollm-starcoder-python BookSum GoodWiki redpajama-arxiv redpajama-github redpajama-stackexchange dolma-CC-news dolma-pes2o dolma-reddit dolma-megawika dolma-books dolma-wiki the-stack-v2 starcoder-lean starcoder-isabelle starcoder-fortran starcoder-mathematica matrix-books matrix-exams SlimPajama-Mix smollm-cosmo openphi-textbooks openphi-textbooks-grounded openphi-llamabooks tiny-strange-textbooks tiny-textbooks tiny-code-textbooks tiny-orca-textbooks sciphi-textbooks textbook-programming proofpile-algebra openweb-math british-library-books Library-of-Congress-books MathPile CLRS AutoMathText-1 AutoMathText-2 AutoMathText-3 bigcode-commitpack bigcode-stack-python-fns VikpPython chessllm WaterHorseChess-pre eleutherai-lichess HuggingFaceTB/smollm-corpus jon-tow/starcoderdata-python-edu ubaada/booksum-complete-cleaned euirim/goodwiki togethercomputer/RedPajama-Data-1T togethercomputer/RedPajama-Data-1T togethercomputer/RedPajama-Data-1T allenai/dolma allenai/dolma allenai/dolma allenai/dolma allenai/dolma allenai/dolma bigcode/the-stack-v2-train-smol-ids bigcode/starcoderdata bigcode/starcoderdata bigcode/starcoderdata bigcode/starcoderdata m-a-p/Matrix m-a-p/Matrix cerebras/SlimPajama-627B HuggingFaceTB/smollm-corpus open-phi/textbooks open-phi/textbooks_grounded open-phi/programming_books_llama nampdn-ai/tiny-strange-textbooks nampdn-ai/tiny-textbooks nampdn-ai/tiny-code-textbooks nampdn-ai/tiny-orca-textbooks SciPhi/textbooks-are-all-you-need-lite vikp/textbook_quality_programming EleutherAI/proof-pile-2 open-web-math/open-web-math biglam/blbooks-parquet storytracer/LoC-PD-Books GAIR/MathPile tomg-group-umd/CLRS-Text-train math-ai/AutoMathText math-ai/AutoMathText math-ai/AutoMathText bigcode/commitpackft bigcode/stack-dedup-python-fns vikp/python_code_instructions_filtered mlabonne/chessllm Waterhorse/chess_data EleutherAI/lichess-puzzles odc-by other - mit info.arxiv.org other other odc-by odc-by odc-by odc-by odc-by odc-by other other other other other apache-2.0 apache-2.0 other odc-by - - - apache-2.0 apache-2.0 cc-by-nc-sa-4.0 cc-by-nc-sa-4.0 llama2 - - - cc0-1.0 cc0-1.0 cc-by-nc-sa-4.0 Apache-2.0 CC BY-SA 4.0 CC BY-SA 4.0 CC BY-SA 4.0 mit other - - apache-2.0 CC0 1.0 generic-text code longform-text longform-text scientific-text code Q&A-text generic-text scientific-text generic-text longform-text longform-text longform-text code code code code code longform-text Q&A-text generic-text synthetic-text synthetic-text synthetic-text synthetic-text synthetic-text synthetic-text synthetic-text synthetic-text synthetic-text synthetic-text math math longform-text longform-text math math math math math code code code misc-reasoning misc-reasoning misc-reasoning 1.0 1.0 2.0 4.0 2.0 1.0 1.0 1.0 2.0 1.0 1.0 2.0 4.0 1.0 4.0 4.0 2.0 2.0 0.25 1.0 0.25 2.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 2.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 MG Citation (Ben Allal et al., 2024) (Ben Allal et al., 2024) (Kryscinski et al., 2022) (Choi, 2023) (Weber et al., 2024) (Weber et al., 2024) (Weber et al., 2024) (Soldaini et al., 2024) (Soldaini et al., 2024) (Soldaini et al., 2024) (Soldaini et al., 2024) (Soldaini et al., 2024) (Soldaini et al., 2024) (Lozhkov et al., 2024) (Li et al., 2023) (Li et al., 2023) (Li et al., 2023) (Li et al., 2023) (Zhang et al., 2024a) (Zhang et al., 2024a) (Soboleva et al., 2023) (Ben Allal et al., 2024) (Colegrove et al., 2024) (Colegrove et al., 2024) (Colegrove et al., 2024) (Nam Pham, 2024) (Nam Pham, 2023) nampdn-ai/tiny-code-textbooks nampdn-ai/tiny-orca-textbooks SciPhi/textbooks-are-all-youneedlite vikp/textbook_quality_programming (Azerbayev et al., 2023) (Paster et al., 2023) (British Library Labs, 2021) (Majstorovic, 2024) (Wang et al., 2024b) (Markeeva et al., 2024) (Zhang et al., 2024c) (Zhang et al., 2024c) (Zhang et al., 2024c) (Muennighoff et al., 2024) (Muennighoff et al., 2024) vikp/python_code_instructions_filtered mlabonne/chessllm (Feng et al., 2023) (Schwarzschild et al., 2021a) 37 Scaling up Test-Time Compute with Latent Reasoning: Recurrent Depth Approach Table 8: Datasets used for model pre-training (Part 2: Instruction Data) Dataset Address License Category MG Citation WebInstruct-prometheus hercules OpenMathInstruct MetaMathQA CodeFeedback Daring-Anteater Nvidia-Blender baai-instruct-foundation baai-instruct-gen anthracite-stheno opus-writing math-step bigcode-oss everyday-conversations gsm8k no-robots longwriter webglm-qa ArxivInstruct tulu-sft P3 OrcaSonnet opus-writingprompts reddit-writing kalomaze-instruct lean-github lean-workbook mma lean-dojo-informal cpp-annotations lean-tactics college-math gradeschool-math general-stories amps-mathematica amps-khan Magpie-300k Magpie-reasoning prox-fineweb prox-c4 prox-redpajama prox-open-web-math together-long-data project-gutenberg-19 mathgenie reasoning-base OpenMathInstruct-2 Txt360-DM Txt360-ubuntu-chat markdown-arxiv chargoddard/WebInstructSub-prometheus Locutusque/hercules-v5.0 nvidia/OpenMathInstruct-1 meta-math/MetaMathQA m-a-p/CodeFeedback-Filtered-Instruction nvidia/Daring-Anteater nvidia/sft_datablend_v1 BAAI/Infinity-Instruct BAAI/Infinity-Instruct anthracite-org/Stheno-Data-Filtered Nopm/Opus_WritingStruct xinlai/Math-Step-DPO-10K bigcode/self-oss-instruct-sc2-exec-filter-50k HuggingFaceTB/everyday-conversations hkust-nlp/gsm8k-fix HuggingFaceH4/no_robots THUDM/LongWriter-6k THUDM/webglm-qa AlgorithmicResearchGroup/ArXivDLInstruct allenai/tulu-v2-sft-mixture-olmo-4096 bigscience/P3 Gryphe/Sonnet3.5-SlimOrcaDedupCleaned Gryphe/Opus-WritingPrompts nothingiisreal/Reddit-Dirty-And-WritingPrompts nothingiisreal/Kalomaze-Opus-Instruct-25k-filtered internlm/Lean-Github pkuAI4M/LeanWorkbook casey-martin/multilingual-mathematical-autoformalization AI4M/leandojo-informalized casey-martin/oa_cpp_annotate_gen l3lab/ntp-mathlib-instruct-st ajibawa-2023/Maths-College ajibawa-2023/Maths-Grade-School ajibawa-2023/General-Stories-Collection XinyaoHu/AMPS_mathematica XinyaoHu/AMPS_khan Magpie-Align/Magpie-Pro-MT-300K-v0.1 Magpie-Align/Magpie-Reasoning-150K gair-prox/FineWeb-pro gair-prox/c4-pro gair-prox/RedPajama-pro gair-prox/open-web-math-pro togethercomputer/Long-Data-Collections emozilla/pg19 MathGenie/MathCode-Pile KingNish/reasoning-base-20k nvidia/OpenMathInstruct-2 LLM360/TxT360 LLM360/TxT360 neuralwork/arxiver apache-2.0 other nvidia-license mit apache-2.0 cc-by-4.0 cc-by-4.0 - - - apache-2.0 - - apache-2.0 mit cc-by-nc-4.0 apache-2.0 - mit odc-by apache-2.0 mit unknown apache-2.0 apache-2.0 apache-2.0 apache-2.0 apache-2.0 - - - apache-2.0 apache-2.0 apache-2.0 mit mit llama3 llama3 odc-by odc-by odc-by odc-by other apache-2.0 apache-2.0 apache-2.0 nvidia-license odc-by odc-by cc-by-nc-sa-4.0 generic-instruct generic-instruct math-instruct math-instruct generic-instruct generic-instruct generic-instruct generic-instruct generic-instruct math-instruct writing-instruct math-instruct generic-instruct writing-instruct math-instruct writing-instruct writing-instruct generic-instruct math-instruct generic-instruct generic-instruct writing-instruct writing-instruct writing-instruct writing-instruct math-instruct math-instruct math-instruct math-instruct generic-instruct math-instruct math math synthetic-text math math-instruct generic-instruct generic-instruct generic-text generic-text generic-text math longform-text longform-text math math math-instruct math Q&A-text scientific-text 1.0 1.0 1.0 1.0 2.0 1.0 1.0 1.0 1.0 1.0 2.0 2.0 1.0 3.0 1.0 3.0 2.0 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 3.0 3.0 3.0 3.0 1.0 2.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 2.0 - (Kim et al., 2024) (Gabarain, 2024) (Toshniwal et al., 2024b) (Yu et al., 2023) (Zheng et al., 2024) (Wang et al., 2024a) nvidia/sft_datablend_v1 BAAI/Infinity-Instruct BAAI/Infinity-Instruct anthracite-org/Stheno-Data-Filtered Nopm/Opus_WritingStruct (Lai et al., 2024) sc2-instruct HuggingFaceTB/everyday-conversations (Cobbe et al., 2021) (Ouyang et al., 2022) (Bai et al., 2024) (Liu et al., 2023b) (Kenney, 2024) (Groeneveld et al., 2024) (Sanh et al., 2021) Gryphe/Sonnet3.5-SlimOrcaDedupCleaned Gryphe/Opus-WritingPrompts Reddit-Dirty-And-WritingPrompts Kalomaze-Opus-Instruct-25k (Wu et al., 2024) (Ying et al., 2024) (Jiang et al., 2023) (Yang et al., 2023) moyix (Hu et al., 2024) ajibawa-2023/Maths-College ajibawa-2023/Maths-Grade-School ajibawa-2023/General-StoriesCollection XinyaoHu/AMPS_mathematica XinyaoHu/AMPS_khan (Xu et al., 2024) (Xu et al., 2024) (Zhou et al., 2024) (Zhou et al., 2024) (Zhou et al., 2024) (Zhou et al., 2024) (TogetherAI, 2023) (Rae et al., 2019) (Lu et al., 2024) KingNish/reasoning-base20k (Toshniwal et al., 2024a) (Liping Tang, 2024) (Liping Tang, 2024) neuralwork/arxiver"
        }
    ],
    "affiliations": [
        "ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center",
        "Lawrence Livermore National Laboratory",
        "University of Maryland, College Park"
    ]
}