{
    "paper_title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning",
    "authors": [
        "Xiaoxuan Wang",
        "Han Zhang",
        "Haixin Wang",
        "Yidan Shi",
        "Ruoyan Li",
        "Kaiqiao Han",
        "Chenyi Tong",
        "Haoran Deng",
        "Renliang Sun",
        "Alexander Taylor",
        "Yanqiao Zhu",
        "Jason Cong",
        "Yizhou Sun",
        "Wei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines."
        },
        {
            "title": "Start",
            "content": "ARLArena: Unified Framework for Stable Agentic Reinforcement Learning Xiaoxuan Wang1,*, Han Zhang1,*, Haixin Wang1,*, Yidan Shi1,, Ruoyan Li1,, Kaiqiao Han1,, Chenyi Tong2, Haoran Deng1, Renliang Sun1, Alexander Taylor1, Yanqiao Zhu1, Jason Cong1, Yizhou Sun1, Wei Wang1 1University of California, Los Angeles, 2University of WisconsinMadison These authors share first authorship. These authors share second authorship. Agentic reinforcement learning (ARL) has rapidly gained attention as promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, stable training recipe and systematic analysis framework that examines training stability in controlled and reproducible setting. ARLArena first constructs clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill unified perspective on ARL and propose SAMPO, stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines. GitHub: https://github.com/WillDreamer/ARL-Arena HuggingFace: https://huggingface.co/UCLA-SCAI/models 6 2 0 F 5 2 ] . [ 1 4 3 5 1 2 . 2 0 6 2 : r Figure 1 Overview of ARLArena. Part 1: standardized testbed via behavior cloning, format penalty, KL regularization, and hyperparameter search. Part 2: Policy gradient decomposition into four dimensions with representative methods mapped to each. Part 3: Key findings on training stability and collapse modes. Part 4: Insights unified into SAMPO for stable ARL training. Figure 2 Training curves on ALFWorld (left) and Sokoban (right). SAMPO (ours) achieves the highest success rates on both environments with stable, monotonic improvement throughout training, while baseline methods exhibit varying degrees of instability. These results demonstrate that principled integration of sequence-level clipping, advantage design, and dynamic filtering, as combined in SAMPO, is critical for both training stability and final performance in multi-turn agentic RL."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have been increasingly deployed as autonomous agents for complex, multi-step interactive tasks spanning web navigation (Zhou et al., 2024), embodied environments (Shridhar et al., 2020), games (Xi et al., 2024), and deep research (Guan et al., 2025; Jin et al., 2025). These tasks demand planning, tool use, and long-horizon decision-making, necessitating training objectives that capture multi-turn interactions. Reinforcement learning (RL) offers principled post-training framework for this purpose, building on its success in static reasoning tasks (e.g., DeepSeek-R1 (Guo et al., 2025), OpenAI o1 (Jaech et al., 2024)), and early results in the agentic setting are promising (Cheng et al., 2025; Jin et al., 2025; Xi et al., 2024). However, agentic RL (ARL) training remains highly unstable and prone to collapse (Xi et al., 2025). This instability arises from the interactive, multi-turn nature of agentic environments, which introduce compounding challenges such as invalid actions, sparse rewards, long-horizon credit assignment, and non-stationary agentenvironment dynamics (Wang et al., 2025b; Xu et al., 2026). Small deviations in early decisions can cascade across turns, causing distribution shifts that amplify credit-assignment noise and produce degenerate rollouts (Xia et al., 2026; Xie et al., 2026). Consequently, ARL outcomes are difficult to reproduce across runs and environments, and scaling to longer horizons or more complex interaction spaces remains severely limited (Abdulhai et al., 2023; Xi et al., 2025). These challenges underscore the need for stable and scalable training solutions for ARL. This paper addresses this gap by introducing ARLArena, stable training recipe and systematic analysis framework for agentic reinforcement learning. We first construct clean, standardized testbed through format correction, behavior cloning initialization, and KL-based regularization, establishing reliable baseline performance. We then decompose policy-gradientbased RL into four orthogonal design dimensions and evaluate the effectiveness and stability of each across diverse agentic tasks. Each dimension is examined in isolation using representative policy optimization (PO) methods; for methods that exhibit training collapse, we further diagnose the underlying failure modes and develop targeted stabilization strategies. This systematic analysis yields three key findings: (1) tolerant clipping induces training collapse, whereas sequence-level clipping ensures stable improvement; (2) incorporating environment-level information into advantage design improves both stability and performance; and (3) dynamic sampling combined with fine-grained advantage design further benefits ARL training. Motivated by these insights, we propose Stable Agentic Multi-turn Policy Optimization (SAMPO), unified PO method that directly addresses the dominant sources of instability identified in our analysis. SAMPO consistently improves training stability and performance, achieving an average 25.2% improvement over the GRPO baseline. We additionally study the impact of off-policy staleness in agentic environments and conduct comparative evaluations against proprietary models, demonstrating the robustness and generality of our approach. 2 Dynamical Sampling Loss Objective Advantage (Ai) (cid:88) Ti1 (cid:88) i=1 t=0 min(cid:0)wt Ai, clip(wt, 1ε)Ai (cid:1) 1 Ti Ti1 (cid:88) t=0 min(cid:0)wt Ai, clip(wt, 1ε)Ai (cid:1) ri mean(ri) std(ri) ri mean(ri) std(ri) IS (wt) Clipping Adv < 0 (cid:26) 1 ε, wt < 1 ε, otherwise. wt, (cid:26) 1 ε, wt < 1 ε, otherwise. wt, Adv > 0 (cid:26) 1 + ε wt > 1 + ε, otherwise. wt, (cid:26) 1 + ε, wt > 1 + ε, otherwise. wt, ri mean(ri) std(ri) (cid:26) 1 ε, wt < 1 ε, otherwise. wt, (cid:26) 1 + ε, wt > 1 + ε, otherwise. wt, ri mean(ri) std(ri) ri mean(ri) std(ri) σ(τneg(wt 1)) 4 τneg σ(τpos(wt 1)) 4 τpos (cid:26) 1 εlow, wt < 1 εlow, (cid:26) 1 + εhigh, wt > 1 + εhigh, sg(wt), otherwise. sg(wt), otherwise. ri mean(ri) std(ri) (cid:26) 1 ε, si, si < 1 ε, otherwise. (cid:26) 1 + ε, si, si > 1 + ε, otherwise. 1 (cid:80)G i=1 Ti (cid:88) 1 i=1 1 (cid:80)G i=1 Ti (cid:88) Ti1 (cid:88) i=1 t=0 Mi min(cid:0)wt Ai, clip(wt, 1ε)Ai (cid:1) 1 Ti Ti1 (cid:88) t=0 log πθold (ytx, y<t) πθ(ytx, y<t) δ Mi = 1 Ai 0 or (cid:88) i=1 Ti1 (cid:88) t=0 fi,t(wt)Ai Ti1 (cid:88) sg(wt)Ai log πθ i=1 t= 1 (cid:80)G i=1 Ti (cid:88) 1 (cid:80)G i=1 Ti (cid:88) Ti1 (cid:88) i=1 t=0 1 (cid:80)G i=1 Ti min(cid:0)si Ai, clip(si, 1ε)Ai (cid:1) si = exp (cid:16) 1 Ti Ti1 (cid:88) t= log πθ(yt x, y<t) πθold (yt x, y<t) (cid:17) Method GRPO GRPOST GRPOSM SAPO CISPO GSPO GIGPO EMPG DAPO 1 (cid:80)G i=1 Ti (cid:88) Ti1 (cid:88) i=1 t= min(cid:0)wt i,k, clip(wt, 1ε)A i,k (cid:1) Ai + ω Astep(ˆyi,k) min(cid:0)wt i, clip(wt, 1ε)A (cid:1) g(cid:0)Hk (cid:1)Ai + ζ (cid:0)Hk+ (cid:1) (cid:26) 1 ε, wt < 1 ε, otherwise. wt, (cid:26) 1 ε, wt < 1 ε, otherwise. wt, (cid:26) 1 + ε, wt > 1 + ε, otherwise. wt, (cid:26) 1 + ε, wt > 1 + ε, otherwise. wt, min(cid:0)wt Ai, clip(wt, 1ε)Ai (cid:1) ri mean(ri) std(ri) (cid:26) 1 εlow, wt < 1 εlow, (cid:26) 1 + εhigh, wt > 1 + εhigh, wt, otherwise. wt, otherwise. 1 (cid:80)G i=1 Ti 1 (cid:80)G i=1 Ti (cid:88) Ti1 (cid:88) i=1 t= (cid:88) Ti1 (cid:88) i=1 t=0 Table 1 summary of policy optimization methods studied in ARLArena, decomposed along four design dimensions: loss objective formulation, advantage (Ai), importance sampling (IS) clipping, and dynamic sampling. Colored entries highlight distinctive design choices: purple denotes modified loss aggregation (seq-mean-token-mean), violet indicates alternative IS clipping strategies (tolerant or sequence-level), and green marks novel advantage designs. The importance sampling weight is wt = πθ(yt x, y<t)/πθold (yt x, y<t), and sg() denotes the stop-gradient operator. In summary, our contributions are: (i) unifying policy gradient perspective and four-dimensional categorization of PO methods for ARL; (ii) standardized, reproducible testbed and diagnostic methodology for multi-turn ARL stability; (iii) principled, task-robust findings and remedies for common collapse modes; and (iv) SAMPO, new PO method that achieves both reliable training and strong final performance. We hope this study provides foundation for more reproducible and principled progress in LLM agent post-training."
        },
        {
            "title": "2.1 Policy Gradient for Agentic RL",
            "content": "During RL optimization for LLMs, the policy πθ generates response trajectory = (y0, . . . , yT ) conditioned on prompt x, which is subsequently used for policy updates (Ouyang et al., 2022). Following PPO-style optimization (Schulman et al., 2017), trajectories collected under behavior policy πθold are used to update the current policy πθ. The corresponding policy gradient can be written as: θL(θ) = Eyπθold (cid:34) (cid:88) t=0 wt(y)θ log πθ(yt x, y<t) A(x, y) (cid:35) , where the importance sampling weight is given by: wt(y) = Pθ(yt x, y<t) Pθold (yt x, y<t) = πθ(yt x, y<t) πθold (yt x, y<t) . Here, A(x, y) represents the advantage of the sampled sequence. (1) (2) Agentic RL. An agent interacts with the environment over turns, forming long-horizon decision-making process (Luo et al., 2026; Wei et al., 2026). At each turn, the policy conditions on the accumulated history to generate response, from which an action is extracted and executed to transition the environment state. The initial user prompt is x(1). At turn {1, . . . , K}, the policy generates response y(k) πθ( x(k)). Given the environment state s(k), actions a(k) are extracted from y(k), and the environment transitions to the next state s(k+1) according to an update function : s(k+1) = (cid:0)a(k), s(k)(cid:1), where () is the state transition function that incorporates tool calls, environment observations, or retrieved information. The user prompt for turn + 1, denoted x(k+1), is constructed from the updated state s(k+1). Finally, the complete multi-turn interaction trajectory is defined as τ = (cid:0)x(1), y(1), x(2), y(2), . . . , x(K), y(K)(cid:1). In the multi-turn agentenvironment setting described above, we decompose K-turn trajectory into single-turn updates. This yields the following policy gradient formulation for agentic LLM interaction: θL(θ) = Eτ πθold (cid:104) (cid:88) Tk(cid:88) k=1 t=0 wt(y(k)) (cid:124) (cid:123)(cid:122) (cid:125) IS θ log πθ (cid:124) x(k), y(k) <t (cid:16) y(k) (cid:123)(cid:122) Log prob (cid:17) (cid:125) A(x(k), y(k)) (cid:123)(cid:122) (cid:125) (cid:124) Advantage (cid:105) . (3)"
        },
        {
            "title": "2.2 Policy Gradient Decomposition Dimensions",
            "content": "According to Equation 3, the policy gradient formulation for agentic LLMs can be decomposed into four key research dimensions: Loss Aggregation, Importance Sampling (IS) clipping, Trajectory Filtering and Resampling, and Advantage Design. To study each dimension in isolation, we analyze the batch-level loss objective without loss of generality. We summarize mainstream PO algorithms across the different design dimensions of the policy gradient in Table 1. Loss Aggregation. In practice, we approximate the loss objective using different loss aggregation schemes. L(θ) = (cid:104) (cid:2)ℓi,t(θ)(cid:3)(cid:105) Et y(i)πθold (cid:88) Ti1 (cid:88) 1 Ti i=1 t= 1 ℓi,t(θ) (seq-mean-token-mean) 1 (cid:80)N i=1 Ti (cid:88) Ti1 (cid:88) i=1 t=0 ℓi,t(θ) (token-mean), (4) (5) (cid:1). denotes the total number of decomposed turns where ℓi,t(θ) := min(cid:0)wi,t(θ) Ai, clip(cid:0)wi,t(θ), 1 ε, 1 + ε(cid:1) Ai over trajectories. Ai denotes the advantage of sequence y(i), and wi,t(θ) is the importance sampling ratio at token of sequence y(i). Seq-mean-token-mean weights each token by the inverse of its trajectory length, biasing optimization toward shorter trajectories and potentially introducing response-level length bias. Token-mean assigns equal weight to all unmasked tokens in the batch. Additional aggregation strategies are provided in the Appendix A.1. IS Clipping. Clipping methods constrain the magnitude of policy updates by limiting the change in action probabilities relative to the old policy. By constraining the deviation between the new and old policies within bounded range, clipping mitigates performance degradation and instability caused by excessively large policy updates. The loss objective is formulated as follows: L(θ) = 1 (cid:80)N i=1 Ti (cid:88) Ti1 (cid:88) i=1 t=0 (cid:16) wi,t(θ) Ai, clip(cid:0)wi,t(θ), 1ε(cid:1) Ai (cid:17) . min (6) Within the GRPO (Guo et al., 2025) framework, several clipping variants are considered, including CISPO (Chen et al., 2025), SAPO (Gao et al., 2025), and GSPO (Zheng et al., 2025). CISPO employs stop-gradient mechanism to avoid hard clipping of out-of-bounds tokens while preserving their gradient information. SAPO adopts soft-clipping strategy, in which excessively large ratios are smoothly attenuated rather than truncated. GSPO performs clipping by using the sequence-level importance ratio as the clipping criterion. Detailed formulations of these variants are provided in Table 1 and further introduced in Appendix A.2. 4 Algorithm Strategy Task Score Success Rate GRPO GSPO DAPO SAPO + Behavior Cloning + Rformat + KL k3(x) ϵ : e2 e3 ϵ : e3 e4 Max_try: 2 3 Temperature: 1 2 Temperature: 2 3 + 2. + 0.49 + 0.95 + 0.70 1.16 + 0.59 1. 0.70 + 20.71 + 7.34 + 18.10 + 3.36 9. + 22.15 9.85 9.20 Table 2 Incremental stabilization strategies for constructing standardized testbed on ALFWorld, evaluated using GRPO as the base policy optimizer. Each row adds one stabilization technique or adjusts method-specific hyperparameter. Task Score and Success Rate report the absolute improvement (+) or degradation () relative to the preceding configuration. Trajectory Filtering and Resampling. Dynamic sampling addresses inefficiency caused by zero-gradient trajectories in long-horizon agent training (Yu et al., 2025a). L(θ) = s.t. (cid:88) Ti1 (cid:88) (cid:16) min 1 (cid:80)N i=1 Ti y(i) (cid:12) (cid:12) (cid:110) (cid:12) is_equivalent(a, y(i)) (cid:12) (cid:12) (cid:12) 0 < t=0 i=1 (cid:111)(cid:12) (cid:12) (cid:12) < G. wi,t(θ) Ai, clip(cid:0)wi,t(θ), 1ε(cid:1) Ai (cid:17) , (7) Here, denotes the ground-truth task completion target, and equivalence is determined by whether the agent successfully completes the task. It adaptively filters out trajectories whose sampled output groups receive identical rewards (e.g., all correct or all incorrect) and resamples additional trajectories to increase the proportion of samples with informative gradient signals. Advantage Design. Multi-turn agentic reinforcement learning introduces additional interaction steps and explicit agentenvironment state transitions, which motivates specialized advantage designs. GiGPO (Feng et al., 2025) defines advantages at the state level by grouping actions conditioned on the same preceding environment state and assigning them shared relative advantage. EMPG (Wang et al., 2025a) augments the advantage function with an entropy-dependent term, which modulates the learning signal at each turn to better account for uncertainty across interaction steps. Detailed formulations of these variants are provided in Table 1 and further introduced in Appendix A.3."
        },
        {
            "title": "3.1 Standardized Testbed",
            "content": "A primary challenge is constructing fair and effective testbed for comparing different algorithms. To address this issue, we progressively apply sequence of stabilization strategies shown in Table 2. Specifically, we start with behavior cloning, followed by format penalty enforcement and KL regularization when necessary, and finally PO-specific hyperparameter tuning. This process yields standardized and stable testbed that provides solid foundation for systematically comparing different policy optimization strategies. (1) Behavior Cloning. We first perform behavior cloning (BC) on supervised interaction traces to initialize the policy within reasonable behavioral manifold. Specifically, we construct multi-turn SFT dataset by deploying the Qwen3 series model (Yang et al., 2025) in the target training environments, collecting self-generated interaction trajectories, and retaining only high-scoring rollouts for supervision. This self-bootstrapped SFT stage initializes the policy within reasonable behavioral manifold aligned with the environment dynamics. 5 Dimension Method ALFWorld WebShop Sokoban TIR Math Score Success Success AIME AIME25 Avg Base GRPO 3.70 62. 83.90 49.96 30.78 46.16 (48.08) Loss Agg GRPOST 4.41 19.2% 72.61 16.4% 64.57 14.3% 51.29 11.1% 3.03 45.0% 68.73 18.1% 27.55 44.9% 21.63 29.7% 39.23 15.0% Score 75.32 Success 57.71 Score 5.51 Importance Sampling Advantage Design Dynamic Sampling SAPO CISPO GSPO GIGPO EMPG DAPOGRPO DAPOGIGPO 0.80 78.4% 25.16 59.7% 73.85 1.9% 52.10 9.7% 0.23 104% 30.25 63.9% 45.00 9.9% 30.85 0.2% 32.22 30.2% 2.16 41.6% 54.42 12.7% 67.96 9.8% 54.71 5.2% 0.47 109% 26.02 69.0% 36.53 26.9% 30.87 0.3% 34.03 26.3% 5.19 40.3% 78.61 26.1% 85.29 13.3% 72.48 25.6% 5.22 5.3% 82.22 1.7% 51.29 2.7% 37.95 23.3% 52.28 13.3% 4.97 34.3% 81.09 30.0% 67.76 10.0% 56.55 2.0% 5.19 5.8% 82.67 1.5% 3.32 10.3% 57.91 7.1% 79.16 5.1% 64.32 11.5% 4.48 18.7% 79.16 5.6% 49.71 3.4% 48.06 0.1% 1.95 47.3% 49.58 20.5% 62.43 17.1% 46.17 20.0% 5.16 6.4% 82.40 1.8% 54.66 9.4% 38.97 26.6% 42.67 7.6% Ours SAMPO 7.04 90.3% 92.72 48.7% 88.37 17.3% 77.73 34.7% 6.56 19.1% 88.86 5.6% 2.49 32.7% 60.55 2.9% 88.10 17.0% 76.82 33.1% 6.01 9.1% 86.20 2.7% 53.36 11.0% 60.21 25.2% Table 3 Performance comparison of policy optimization methods across four agentic tasks, evaluated on the SFT version of Qwen3-4B. Methods are organized by their primary design dimension: loss aggregation, importance sampling clipping, advantage design, and dynamic sampling. Green/red subscripts denote the percentage improvement/degradation relative to the GRPO baseline. SAMPO (ours) achieves the highest average score (59.55) with consistent gains across ALFWorld (92.72% success), WebShop (74.08% success), and Sokoban (88.86% success). The evaluation metric for TIR Math is Pass@4; indicates the method is not applicable. For GRPO, the value in parentheses reports the average over the first three tasks only. (2) Format Penalty. We incorporate Rformat that enforces structured outputs with explicit <think> </think> and <action> </action> tags. If the generated output violates this format (e.g., missing tags, malformed nesting, or extraneous content outside the tags), we apply fixed penalty to the final reward. This explicit structural constraint provides dense shaping signals during early training and substantially reduces invalid rollouts that would otherwise corrupt policy updates. (3) Auxiliary KL Loss. Unconstrained updates may cause the policy to drift excessively from the reference model. To regularize policy updates and preserve the pretrained knowledge embedded in the base model, we introduce KL divergence penalty between the current policy πθ and reference policy πref . This constraint encourages conservative policy improvement while still allowing sufficient exploration in the action space. We adopt the commonly used Bregman divergence estimator k3 for KL approximation, which leverages control variates to achieve unbiasedness and low variance (Schulman, 2017). Specifically, k3 is defined as k3(x) = δ(x) 1 log δ(x), where δ(x) = p(x) q(x) denotes the likelihood ratio. (4) PO-specific Hyper-parameter Grid Search. natural question is how to ensure that each PO method is fairly evaluated in the multi-turn setting. Our solution is to first run each method with its default configuration, and then perform PO-specific hyperparameter grid search. We continue tuning until the training trajectory becomes stable, measured by the variance of the success rate over the final 20% of training steps falling below predefined threshold. As shown in Table 1, hyperparameters related to IS clipping are particularly sensitive. The best-performing configurations and full results are reported in Appendix B."
        },
        {
            "title": "3.2 Tasks and Training Details",
            "content": "We adapt ALFWorld (Shridhar et al., 2020), WebShop (Yao et al., 2022), Sokoban (Schrader, 2018), and TIR Math (Xue et al., 2025) as the agentic tasks. Our entire codebase is built upon the verl RL framework (Sheng et al., 2024). We employ an agentic-loop architecture to coordinate rollouts and environment interactions, after which we segment each complete trajectory into multiple single-turn samples for policy optimization. For mathematical tasks, we use Qwen3-4B-base as the policy model, while for all other tasks we initialize from the SFT-tuned variant Qwen3-4B. For consistency validation, we additionally employed SFT-tuned Qwen3-8B, and the corresponding experimental results are provided in Appendix C. All experiments are conducted on NVIDIA H200 or B200 GPUs. Key hyperparameters and training details are reported in Appendix B. 6 Figure 3 Training dynamics of six IS variants on ALFWorld: GRPO, GSPO, SAPO, CISPO, and their sequence-masked counterparts SAPOSM and CISPOSM. Panels show (from left to right) success rate, off-policy KL divergence between the current and behavior policies, KL loss between the current and reference policies, gradient norm, and valid-format ratio of rollout actions."
        },
        {
            "title": "4 Exploring Gradient Dimensions on ARL",
            "content": "The experimental results for all policy optimization methods are reported in Table 3. GRPOST denotes GRPO with sequence-mean-token-mean loss aggregation. DAPOGRPO and DAPOGIGPO denote GRPO and GIGPO augmented with dynamic filtering, respectively."
        },
        {
            "title": "4.1 Impact of IS on ARL",
            "content": "We study GSPO, CISPO, and SAPO along the importance-sampling (IS) dimension. GSPO adopts sequence-level clipping, while CISPO and SAPO employ tolerant clipping techniques. For CISPO and SAPO, we further apply sequence masking (denoted as CISPOSM and SAPOSM) to improve training stability. Detailed training dynamics are reported in Figure 3, with IS token-level and sequence-level analyses presented in Figure 4. Table 3 shows that CISPO and SAPO perform substantially worse than GRPO across all tasks, achieving average scores of 34.03 and 32.22, respectively, compared to 46.16 for GRPO. In contrast, GSPO consistently outperforms all other policy optimization methods, achieving an average improvement of 13.3% compared to GRPO. To understand training behavior beyond final performance, we analyze training dynamics from multiple perspectives across several metrics. Different IS designs induce varying distances between the current policy and both the behavior and reference policies during training. These distance variations, in turn, influence optimization behavior (reflected by gradient norms), impact data quality (through the valid action ratio), and ultimately affect task success rates. Jointly examining these metrics enables more comprehensive understanding of training stability and failure modes. Figure 3 reports success rate, off-policy KL divergence (between the new and old policies), KL loss (between the new and reference policies), gradient norm, and the valid-format ratio of rollout action tokens. As shown in Figure 3, CISPO and SAPO with tolerant clipping exhibit rapid initial performance gains, characterized by higher success rates, larger policy updates relative to the reference model, and faster format ratio adaptation compared to GRPO and GSPO. This behavior indicates more aggressive optimization that departs quickly from the reference policy and adapts rapidly to the task. possible explanation is that tolerant clipping may preserve gradient contributions from tokens that deviate substantially from the current policy, resulting in overly exploratory updates. However, such aggressiveness leads to training instability, with collapse occurring around step 130. This collapse is marked by exploding gradient norms and KL divergence, accompanied by sharp drop in the valid-format ratio, ultimately resulting in severe degradation of success rate. In contrast, GSPO demonstrates substantially more stable training pattern, with gradual performance improvement accompanied by steady KL divergence and gradient norms. These results indicate that sequence-level clipping is effective for stabilizing training, while overly tolerant clipping thresholds may yield short-term gains at the cost of long-term stability. Furthermore, IS design substantially impacts both performance and training stability in ARL, making it an important dimension in ARL system design. Finding 1 ARL is highly sensitive to IS design: tolerant clipping yields fast early gains but causes training collapse, whereas sequence-level clipping ensures stable improvement. 7 Figure 4 Token-level and sequence-level IS analysis of SAPO and its sequence-masked variant SAPOSM. (a, b) Fraction of tokens with importance ratios outside the clipping range, decomposed into lower-bound (negative advantage) and upper-bound (positive advantage) portions. (c, d) Rollout groups partitioned by advantage sign, entropy level, and IS ratio magnitude, with KL divergence normalized for relative comparison. Rooted cause of training collapse. To investigate the root causes of training collapse along the IS dimension, we analyze token-level importance ratio statistics and stratify sequences by IS ratio, advantage, and entropy for SAPO and SAPOSM, where SAPOSM denotes stabilized variant of SAPO introduced later. Figure 4 reports token-level and sequence-level IS ratio analysis. Subfigures (a) and (b) present the statistics of tokens whose importance sampling ratios fall outside the standard clipping range. Specifically, we report the proportion of out-of-bounds tokens and decompose it into lowerand upper-bound portions. The lower-bound portion corresponds to negative-advantage tokens with importance ratios below ϵlow, while the upper-bound portion corresponds to positive-advantage tokens with ratios exceeding ϵhigh. As shown in Figure 4, During the collapse stage, SAPO exhibits rapidly growing number of out-of-bounds tokens, predominantly from negative-advantage sequences with small importance ratios (the lower-bound portion) In contrast, for stable training runs, the portion of out-of-bounds tokens remains fairly low, and lowerand upper-bound ratio portions remain relatively balanced. This growing pattern and imbalance during collapse suggests that negative-advantage samples with low IS ratios are the main contributors the observed training instability. Beyond token-level analysis, we conduct sequence-level comparison across training steps in Subfigures (c) and (d). Rollout samples are partitioned according to three factors: the sign of the advantage, whether the importance ratio is smaller or larger than one, and whether policy entropy falls below or exceeds predefined threshold. This yields eight groups per training step. The vertical area denotes the normalized KL divergence between the current policy and the reference policy. larger area therefore corresponds to greater deviation from the reference policy, indicating stronger contribution to policy shift during training. For collapsed experiments, the proportion of KL divergence attributed to sequences with negative advantages and low importance ratios increases abruptly, whereas for stable training this KL distribution remains relatively balanced across groups. Entropy is less impactful than advantage and IS ratio. This pattern further reinforces the conclusion that negative-advantage samples with low importance ratios are primary source of training instability. Stabilization Strategies for SAPO and CISPO. We explore several strategies to stabilize SAPO and CISPO training, reported in Table 4. First, we consider increasing the KL coefficient to regularize optimization, and enlarging the mini-update batch size to mitigate off-policy effects. As shown in Table 4, increasing the KL coefficient overly constrains training and yields limited performance gains (full success-rate plots reported in Appendix C). Similarly, increasing the mini-update batch size degrades performance. Motivated by the IS-token analysis during training collapse, we adopt sequence masking following (Liu et al., 2025) to directly control negative samples that induce instability. Specifically, sequences with negative advantages and low importance ratios are masked (see Table 1 for the detailed formulation), variant we denote as GRPOSM. We apply sequence masking to SAPO and CISPO, denoted as SAPOSM and CISPOSM. 8 Method Metric Original KL (0.05) Off-Policy (1024) Seq-Mask CISPO SAPO Score Success Score Success 2.16 54.42 0.80 25.16 1. 38.46 2.40 48.05 0.98 21.59 3. 64.30 5.25 78.88 4.88 76.92 Table 4 Effect of different stabilization strategies on CISPO and SAPO in ALFWorld. We evaluate three stabilization techniques applied to the tolerant-clipping methods CISPO and SAPO: increasing the KL penalty coefficient to 0.05, enlarging the off-policy mini-update batch size to 1024, and applying sequence-level masking (Seq-Mask). According to Figure 4 and Table 4, applying sequence masking improves the success rate from 54.12 to 78.88 for CISPO and from 25.16 to 76.92 for SAPO. SAPOSM and CISPOSM effectively stabilizes training, yielding success rates comparable to GSPO, along with steady KL divergence and gradient norms (Figures 3, 4). Finding 2 Training collapse is largely driven by the accumulation of negative-advantage sequences with low IS ratios. Sequence masking of such sequences stabilizes training."
        },
        {
            "title": "4.2 Impact of Advantage Design on ARL",
            "content": "We study GIGPO and EMPG along the advantage-design dimension. GIGPO incorporates both global and local advantage information from the environment, enabling fine-grained advantage estimation, while EMPG reshapes advantages by incorporating uncertainty information from the training data. Table 3 shows that GIGPO generally outperforms GRPO, achieving an average score of 49.71 compared to 48.08, with particularly strong improvement of 34.4% on ALFWorld. In addition, EMPG exhibits task-dependent performance, improving the success rate on WebShop by 11.5% while degrading performance on ALFWorld by 7.1%, resulting in an average score difference of 0.1 compared to GRPO. This suggests that fine-grained advantage design incorporating richer environmental information improves performance and alleviates reward sparsity in ARL, whereas advantage reshaping based on uncertainty signals has smaller effect. Finding 3 Incorporating fine-grained environmental advantage in ARL improves performance."
        },
        {
            "title": "4.3 Impact of Dynamic Filtering on ARL",
            "content": "Dynamic filtering is well known for delivering strong performance improvements on mathematical reasoning tasks (Xue et al., 2025; Yu et al., 2025a). However, we find that these gains do not always transfer to agentic reinforcement learning settings. As shown in Table 3, dynamic filtering improves performance more consistently when combined with GIGPO than with GRPO. This difference stems from how dynamic filtering interacts with format learning. In early training, many rollout groups fail entirely due to format errors, which amplifies the format penalty and produces strong implicit advantage signals for format correction. As result, the model rapidly acquires correct formatting from early rollouts. Meanwhile, dynamic filtering removes such all-failure groups. For GRPO, whose advantage signals have limited diversity, filtering substantially reduces format-related learning signals, leading to unstable format behavior and limited gains. In contrast, GIGPO produces more diverse advantage signals, which stabilize format learning even after filtering, allowing DAPOGIGPO to achieve better and more stable performance. The detailed evidence supporting the above analyses is provided in Appendix G. Finding 4 Dynamic filtering with GIGPO is beneficial for training stability and performance in ARL. 9 ALFWorld Math Degree Score Success AIME AIME25 k@ k@32 k@1 k@32 Low Medium High 3.50 3.83 2.33 60.80 58.38 52. 26.95 24.22 19.53 87.34 75.00 74. 24.61 17.97 16.41 50.00 48.59 43. Table 5 Effect of off-policy staleness on ALFWorld and MATH. We vary the degree of off-policy staleness (Low, Medium, High) and report task score, success rate (ALFWorld), and pass@k accuracy (AIME, AIME25)."
        },
        {
            "title": "4.4 Impact of Loss Aggregation on ARL",
            "content": "As shown in Table 3, sequence-mean-token-mean loss aggregation (GRPOST) degrades performance from 46.16 to 39.23 relative to token-mean aggregation (GRPO). Although GRPOST yields 16.4% improvement on ALFWorld, it leads to substantial decline on TIR-Math, with 44.9% decrease on AIME. Notably, math rollouts exhibit higher variance in sequence length compared to other tasks, ranging from brief solutions to extended reasoning traces. These findings suggest that the unbalanced token weighting induced by sequence-level aggregation may negatively affect ARL training, particularly in tasks characterized by high length variability."
        },
        {
            "title": "4.5 Further Stability Considerations",
            "content": "Exploration on Off-Policy Staleness. Due to infrastructure and efficiency constraints, policy training is typically performed in batched rollouts, where groups of trajectories are generated and updated sequentially before proceeding to the next rollout stage. Off-policy effects arise because later updates within the same rollout stage use data from an earlier policy while the current policy has already evolved. Such off-policy mismatch is further amplified in multi-turn settings, where turn-wise decomposition increases the number of samples subject to staleness. Experiment Setup and Results. We control off-policy degree through rollout configuration while holding the update batch size fixed. For TIR Math, rollout batch sizes of 128, 512, and 1024 correspond to low, medium, and high off-policy degrees, respectively. For ALFWorld, we vary the off-policy degree by adjusting the number of groups per rollout to 8, 16, and 32. The effects of off-policy staleness are summarized in Table 5. TIR Math achieves higher performance under low off-policy ratio (rollout batch size = 128), with 87.34% and 50.00% for avg@32, compared to 74.99% and 43.85% under high off-policy ratio. Similarly, ALFWorld attains its highest success rate of 60.80% under low off-policy settings, which decreases to 52.71% under high off-policy settings. These results suggest that policy gradient optimization for agentic tasks exhibits sensitivity to the off-policy ratio."
        },
        {
            "title": "5.1 Motivation",
            "content": "Can we derive unified understanding of ARL training based on these insights? By systematically analyzing POs along orthogonal design dimensions in ARL, we identify key factors that determine training stability and optimization efficacy. At initialization, formatting errors and invalid action tokens induce severe optimization noise. We eliminate these failure modes through behavior cloning and explicit format correction, constraining learning to valid behavioral manifold. Along the importance sampling dimension, sequence-level clipping, rather than token-wise constraints, is critical for long-horizon ARL. This mechanism addresses off-policy drift by suppressing harmful trajectories and yields substantial improvements in training stability. For advantage design, our analysis reveals that increasing advantage diversity across finer scales is essential to overcoming reward sparsity. Integrating global and local signals significantly enhances credit assignment. Finally, we show that dynamic trajectory filtering helps stabilize gradient updates by removing samples with degenerate advantages, leading to more informative and effective policy gradients."
        },
        {
            "title": "5.2 Our Method",
            "content": "Guided by this unified understanding, we propose SAMPO, new PO paradigm built on these principles. SAMPO integrates sequence-level clipping, fine-grained advantage estimation, and dynamic filtering into unified framework, yielding stable and scalable solution for ARL. It is formulated as: L(θ) = s.t. (cid:88) Ti1 (cid:88) (cid:16) 1 (cid:80)N i=1 Ti 0 < {y is_equivalent(a, y)} < G. si(θ) min t=0 i=1 i, clip(cid:0)si(θ), 1ε(cid:1) (cid:17) , (8) (cid:16) 1 Ti i,k = Ai + ω Astep(ˆyi,k), si(θ) = exp (cid:80)Ti1 Here, . Across all evaluated agentic tasks, t=0 SAMPO consistently achieves the strongest overall performance shown in Table 1. Compared to methods that modify only one dimension, SAMPO demonstrates that combining multiple design dimensions is necessary for stable and effective ARL. Notably, SAMPO delivers particularly large improvements on long-horizon interactive tasks such as ALFWorld, highlighting the importance of sequence-aware control in agentic settings. These results validate our central claim that stable agentic PO method requires satisfying multiple necessary conditions simultaneously, rather than relying on isolated algorithmic modifications. log πθ(ytx,y<t) πθold (ytx,y<t) (cid:17)"
        },
        {
            "title": "5.3 Benchmarking against Inference Paradigms",
            "content": "To further contextualize the performance of SAMPO and evaluate whether small open-source model trained with stable RL can compete with state-of-the-art inference strategies, we benchmark ARLArena against frontier closed-source models and complex multi-agent workflows. This comparison verifies key hypothesis: principled RL training may offer greater gains in agentic tasks than heavy inference-time engineering on generic models. Experiment Setup and Results. We evaluate GPT-5.2 (OpenAI, 2025a), o3 (OpenAI, 2025b), and Gemini 2.5 Pro (Comanici et al., 2025) on ALFWorld and WebShop, under two paradigms: (i) Single LLM as Agent (SLA), following standardized protocol; (ii) Multi-Agent System (MAS), with Debate and Aggressive Debate coordination strategies (details in Appendix F.5). Qwen3-4B-RFT post-trained with SAMPO achieves 92.72% all-task success on ALFWorld, outperforming GPT-5.2 (51.56%) and o3-based MAS (56.25%). Open-source models with SAMPO consistently exceed larger closed-source models, showing that scale and complex inference cannot replace stable, environment-aligned ARL training."
        },
        {
            "title": "6 Insights for Future Work",
            "content": "Based on our systematic dissection of policy gradient design choices in ARL, we identify several promising directions that merit deeper exploration. (1) Clean training recipes are foundational for complex reasoning. ARLArena reveals that ARL is extraordinarily sensitive to initialization and early-stage training dynamics. carefully constructed clean setting, combining short supervised cold-start SFT, format-enforcing structural constraints, and conservative KL regularization, proves essential for unlocking stable multi-turn reasoning behaviors. Without such controlled recipe, policy gradient signals are easily corrupted by malformed trajectories or premature collapse. This suggests that future research should treat training recipes not as auxiliary tricks, but as essential algorithmic components that define the feasible region in which sophisticated reasoning policies can emerge. Our codebase also provides detailed training recipes for reference. (2) IS clipping is highly sensitive, while advantage design offers comparatively stable gain. Among the policy gradient dimensions we examine, IS clipping strategies exhibit high sensitivity: minor changes in clipping thresholds or ratio parameterization can drastically affect stability. In contrast, advantage design tends to provide more stable but relatively modest improvements across tasks. These observations indicate that IS clipping strategy represents high-risk, high-reward direction, whereas advantage design offers more predictable but limited performance gains in ARL. 11 (3) Stable ARL unlocks long-horizon scaling opportunities. Once training collapse is mitigated, we observe that agentic policies can sustain performance improvements over substantially more optimization steps without degradation. This stability opens the door to scaling both interaction horizon and environment size, analogous to scaling laws in supervised pretraining. Consequently, future progress in the field will increasingly depend on scaling environment diversity, interaction data volume, and multi-task curricula."
        },
        {
            "title": "7 Conclusion",
            "content": "This work systematically analyzes how policy gradient design choices impact training stability for agentic LLMs in multi-turn environments. ARLArena demonstrates that sequence-level clipping is critical for stability, while advantage design and dynamic filtering offer smaller but consistent gains, and loss aggregation has limited effect. Based on these insights, we introduce SAMPO, unified policy optimization framework that achieves stable and effective agentic RL training. Overall, this study underscores the importance of principled policy design and reproducible evaluation for advancing ARL."
        },
        {
            "title": "References",
            "content": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Mingyue Cheng, Jie Ouyang, Shuo Yu, Ruiran Yan, Yucong Luo, Zirui Liu, Daoyu Wang, Qi Liu, and Enhong Chen. Agent-r1: Training powerful llm agents with end-to-end reinforcement learning. arXiv preprint arXiv:2511.14460, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Chang Gao, Chujie Zheng, Xiong-Hui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, and Junyang Lin. Soft adaptive policy optimization. arXiv preprint arXiv:2511.20347, 2025. Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. Deeprag: Thinking to retrieve step by step for large language models, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit Dhillon, David Brandfonbrener, and Rishabh Agarwal. The art of scaling reinforcement learning compute for llms. arXiv preprint arXiv:2510.13786, 2025. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31023116, 2023. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. Yinyi Luo, Yiqiao Jin, Weichen Yu, Mengqi Zhang, Srijan Kumar, Xiaoxiao Li, Weijie Xu, Xin Chen, and Jindong Wang. Agentark: Distilling multi-agent intelligence into single llm agent. arXiv preprint arXiv:2602.03955, 2026. OpenAI. Introducing GPT-5.2, December 2025a. Accessed: 2026-01-28. OpenAI. Introducing openai o3 and o4-mini, April 2025b. Accessed: 2026-01-28. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 13 Feng Peiyuan, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, and Hang Li. Agile: novel reinforcement learning framework of llm agents. Advances in Neural Information Processing Systems, 37:52445284, 2024. Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost Batenburg. Agentic large language models, survey. arXiv preprint arXiv:2503.23037, 2025. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, and Nicola Cancedda. Language models can teach themselves to use tools. arXiv preprint, 2023. arXiv:2302.04761. Max-Philipp B. Schrader. gym-sokoban. https://github.com/mpSchrader/gym-sokoban, 2018. John Schulman. Approximating kl divergence. http://joschu.net/blog/kl-approx.html, 2017. Blog post. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. In Proceedings of the International Conference on Learning Representations (ICLR), 2017. arXiv:1707.06347. Y. Sheng et al. Hybridflow: flexible and efficient rlhf training framework with 3d-hybridengine. arXiv preprint, 2024. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint, 2023. arXiv:2305.16291. Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, and Ke Wang. Harnessing uncertainty: Entropy-modulated policy gradients for long-horizon llm agents. arXiv preprint arXiv:2509.09265, 2025a. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang, Jiaru Zou, Zhichen Zeng, Ruizhong Qiu, Xiao Lin, Dongqi Fu, et al. Agentic reasoning for large language models. arXiv preprint arXiv:2601.12538, 2026. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Agentgym: Evolving large language model-based agents across diverse environments, 2024. Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, et al. Agentgym-rl: Training llm agents for long-horizon decision making through multi-turn reinforcement learning. arXiv preprint arXiv:2509.08755, 2025. Hanchen Xia, Baoyou Chen, Zelin Zang, Yutang Ge, Guojiang Zhao, and Siyu Zhu. Latent poincaré shaping for agentic reinforcement learning, 2026. Yutao Xie, Nathaniel Thomas, Nicklas Hansen, Yang Fu, Erran Li Li, and Xiaolong Wang. Tips: Turn-level informationpotential reward shaping for search-augmented llms. In International Conference on Learning Representations (ICLR), 2026. Tianshi Xu, Yuteng Chen, and Meng Li. Cleaner: Self-purified trajectories boost agentic reinforcement learning. arXiv preprint arXiv:2601.15141, 2026. Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. arXiv:2210.03629. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, and Lingjun Liu. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, and Mengdi Wang. Demystifying reinforcement learning in agentic reasoning. arXiv preprint arXiv:2510.11701, 2025c. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, et al. Agentrl: Scaling agentic reinforcement learning with multi-turn, multi-task framework. arXiv preprint arXiv:2510.04206, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents, 2024. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478, 2025."
        },
        {
            "title": "Supplementary Materials for ARLArena",
            "content": "A More Details on Research Dimension A.1 Loss Aggregation . . . . . . . . A.2 Importance Sampling Clipping . A.3 Advantage Design . . . . . . . . Key Hyper-parameter Additional Experiment Result C.1 Performance on 8B Model C.2 Additional Analysis Result C.3 Task Environment Details . . . . . . . . . . . . . . . . Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Another Roadmap of Building Agentic LLM: Multi-agent System E.1 Debate . . . . . . . E.2 Aggressive Debate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Experiment Results on SLA and MAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Failure Analysis F.1 Method: Sankey Graphs for Action-Transition Flows . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 WebShop: Action-Transition Patterns and Failure Modes . . . . . . . . . . . . . . . . . . . . . . . . F.3 WebShop: How RL Post-training Changes Behaviors . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 ALFWorld: Action-Transition Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Visualization G.1 Evidence of Format v.s. Dynamic Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case Study H.1 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Multi-turn State-Action Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 18 19 21 21 21 23 23 24 24 24 25 25 25 27 28 29 29 29 29"
        },
        {
            "title": "A More Details on Research Dimension",
            "content": "A.1 Loss Aggregation As discussed in Section 2.2, the policy gradient objective for agentic LLMs is implemented through batch-level loss aggregation over token-level surrogate losses. For batch of sampled trajectories {yi}N i=1, where trajectory has length Ti, we define the token-level loss as ℓi,t(θ) := min(wi,t(θ)Ai, clip(wi,t(θ), 1 ε, 1 + ε) Ai) , (S1) where wi,t(θ) = πθ(yi,t xi, yi,<t)/πθold (yi,t xi, yi,<t) and Ai denotes the (sequence-level) advantage associated with trajectory yi. Different loss aggregation strategies correspond to different empirical estimators of the expectation over trajectories and tokens. Below we summarize several commonly used schemes. Token-mean. The token-mean estimator averages the loss uniformly over all unmasked tokens in the batch: Ltoken-mean(θ) = 1 (cid:80)N i=1 Ti (cid:88) Ti1 (cid:88) i= t=0 ℓi,t(θ). (S2) This scheme assigns equal weight to each token across the entire batch and is invariant to trajectory length at the sequence level. Token-mean has been adopted in several recent works (e.g., DAPO) as means of stabilizing optimization. However, because trajectories with longer responses contribute more tokens, they implicitly receive larger total weight, which may bias optimization toward long trajectories. Sequence-mean token-mean (Seq-mean-token-mean). This estimator first averages over tokens within each trajectory and then averages across trajectories: Lseq-mean-token-mean(θ) = 1 (cid:88) i=1 1 Ti Ti1 (cid:88) t= ℓi,t(θ). (S3) Under this scheme, each trajectory contributes equally regardless of its length. Equivalently, each token is weighted by 1/Ti. As result, shorter trajectories assign larger per-token weight, while longer trajectories are relatively down-weighted. This behavior can introduce response-level length bias, rewarding short correct trajectories more strongly and penalizing long incorrect trajectories less. Sequence-mean token-sum (Seq-mean-token-sum). An alternative aggregation removes the per-trajectory normalization over tokens: Lseq-mean-token-sum(θ) = 1 N (cid:88) Ti1 (cid:88) i=1 t=0 ℓi,t(θ). (S4) This formulation corresponds to maximizing the expected cumulative surrogate objective over full trajectories. Compared to Seq-mean-token-mean, longer trajectories receive proportionally larger weight. Sequence-mean token-sum with length normalization (Seq-mean-token-sum-norm). tations normalize by fixed maximum generation length Tmax: In practice, some implemenLseq-mean-token-sum-norm(θ) = 1 Tmax (cid:88) Ti1 (cid:88) i=1 t=0 ℓi,t(θ). (S5) This estimator enforces uniform upper bound on the contribution of each trajectory and assigns equal weight to tokens across batches under fixed-length budget. 17 Discussion. These aggregation schemes differ primarily in how they trade off trajectory-level fairness, token-level weighting, and variance control. Seq-mean-token-mean and token-mean are the two most commonly used estimators in practice and are the focus of our empirical analysis in Section 4.4. The remaining variants are included here for completeness and to clarify their implicit inductive biases in agentic reinforcement learning. A.2 Importance Sampling Clipping As discussed in Section 2.2, importance sampling (IS) clipping plays central role in stabilizing off-policy policy optimization. While all methods considered in this work rely on the same token-level importance ratio wi,t(θ) = πθ(yi,t xi, yi,<t) πθold (yi,t xi, yi,<t) , (S6) they differ substantially in where and how clipping is applied. Below we summarize the clipping mechanisms of GRPO, CISPO, SAPO, and GSPO. A.2.1 GRPO Group Relative Policy Optimization (GRPO) adopts the standard PPO-style hard clipping applied independently at each token: ℓGRPO i,t (θ) = min(wi,t(θ)Ai, clip(wi,t(θ), 1 ε, 1 + ε) Ai) . (S7) Clipping is performed directly on the token-level importance ratio. When wi,t falls outside the clipping range, the gradient contribution of that token is truncated. A.2.2 CISPO Clipped Importance Sampling Policy Optimization (CISPO) modifies GRPO by clipping the importance ratio itself rather than the surrogate objective. Specifically, the clipped ratio is defined as (cid:40) wi,t(θ) = 1 + ε, wi,t(θ), wi,t(θ) > 1 + ε, otherwise, and is treated as stop-gradient quantity. The resulting loss takes the form ℓCISPO i,t (θ) = sg( wi,t(θ)) Ai log πθ(yi,t xi, yi,<t), (S8) (S9) where sg() denotes the stop-gradient operator. By avoiding hard truncation of token updates, CISPO preserves gradient flow for clipped tokens while still bounding their influence. However, clipping remains token-local and does not explicitly enforce sequence-level coherence. A.2.3 SAPO Soft Adaptive Policy Optimization (SAPO) replaces hard clipping with smooth, temperature-controlled gating function. The surrogate loss is defined as where ℓSAPO i,t (θ) = fi,t(wi,t(θ)) Ai, fi,t(x) = σ(cid:0)τi,t(x 1)(cid:1) 4 τi,t , τi,t = (cid:40) τpos, Ai > 0, τneg, Ai < 0. (S10) (S11) Here σ() denotes the sigmoid function. SAPO implements continuous trust region: near on-policy updates are preserved, while off-policy updates are smoothly attenuated rather than abruptly clipped. The asymmetric temperature design further suppresses high-variance negative-advantage updates. Despite improved smoothness, SAPO remains token-level method and does not explicitly prevent few extreme tokens from destabilizing full trajectory. A.2.4 GSPO Group Sequence Policy Optimization (GSPO) fundamentally changes the unit of clipping by operating at the sequence level. The sequence-level importance ratio is defined as si(θ) = exp (cid:32)"
        },
        {
            "title": "1\nTi",
            "content": "Ti1 (cid:88) t=0 (cid:33) log wi,t(θ) = (cid:18) πθ(yi xi) πθold (yi xi) (cid:19)1/Ti . Clipping is then applied once per sequence: ℓGSPO (θ) = min(si(θ)Ai, clip(si(θ), 1 ε, 1 + ε) Ai) . (S12) (S13) All tokens within trajectory share the same clipped update. This design aligns the unit of importance sampling with the unit of reward and enforces strong sequence-level coherence. As result, GSPO effectively suppresses high-variance token outliers and yields substantially more stable optimization in long-horizon agentic reinforcement learning. Summary. In summary, GRPO, CISPO, and SAPO apply clipping at the token level with increasing degrees of smoothness, whereas GSPO performs clipping at the sequence level. Our empirical results in Section 4.1 demonstrate that sequence-level clipping is key factor for stabilizing multi-turn agentic RL training. A.3 Advantage Design This section provides detailed formulations of the advantage designs introduced in Section 2.2, including Group-in-Group Policy Optimization (GiGPO) and Entropy-Modulated Policy Gradients (EMPG). Both methods extend standard group-based advantage estimation to better handle long-horizon agentic reinforcement learning. Notation. We consider batch of trajectories {τi}N under the behavior policy πθold . The total return of trajectory is denoted by i=1, where each trajectory τi = {(si,k, ai,k, ri,k)}Ki k=1 is generated R(τi) = Ti(cid:88) t=1 ri,k. (S14) A.3.1 Group-in-Group Policy Optimization (GiGPO) GiGPO introduces hierarchical advantage structure that combines trajectory-level and step-level relative advantages. The design preserves the critic-free and group-based nature of GRPO while enabling finer-grained credit assignment. Episode-level relative advantage. GiGPO first computes trajectory-level (episode-level) relative advantage by normalizing total returns within the rollout group: Ai = R(τi) mean (cid:0){R(τj)}N j=1 (cid:1) (cid:0){R(τj)}N Fnorm j=1 (cid:1) , (S15) where Fnorm() is normalization factor. In the original formulation, Fnorm may be chosen as the standard deviation or fixed constant. 19 Step-level relative advantage via anchor state grouping. To assign fine-grained credit within trajectory, GiGPO constructs step-level groups based on repeated environment states. Let denote the set of distinct environment states appearing in the trajectory batch. For each anchor state U, step-level group is defined as where Ri,k denotes the discounted return from step reward: GS(s) = (cid:8)(cid:0)ai,k, Ri,k (cid:1) (cid:12) (cid:12) si,k = s(cid:9) , Ri,k = Ti(cid:88) m=t γmtri,m. Within each step-level group, GiGPO computes relative advantage for individual actions: Astep(ˆyi,k) = Ri,k mean ({Rj,k (aj,k, Rj,k) GS(s)}) Fnorm ({Rj,k (aj,k, Rj,k) GS(s)}) . (S16) (S17) (S18) Combined advantage. The final advantage used for policy optimization is linear combination of episode-level and step-level components: i,k = Ai + ω Astep(yi,k), (S19) where ω 0 is weighting coefficient controlling the contribution of step-level credit. A.3.2 Entropy-Modulated Policy Gradients (EMPG) Entropy-Modulated Policy Gradients (EMPG) augments the advantage function by incorporating step-wise uncertainty measured via policy entropy. The method reshapes the learning signal at each decision step while preserving trajectory-level optimization objective, making it suitable for long-horizon agentic reinforcement learning. Step-level entropy. For trajectory τi and its t-th step, EMPG defines step-level entropy Hi,t as the average token-level entropy over the tokens generated at that step: Hi,t = 1 yi,t yi,t (cid:88) (cid:88) j=1 vV πθ(v yi,t,<j) log πθ(v yi,t,<j) , (S20) where yi,t is the number of tokens in step t, yi,t,<j denotes the prefix before token within that step, and is the vocabulary. Entropy-modulated advantage. Let A(τi) denote the trajectory-level advantage (e.g., computed via group-based normalization as described in Section 2.2). EMPG defines step-wise modulated advantage as Amod(i, t) = g(Hi,t) A(τi) + ζ (Hi,t+1) , (S21) where g() is self-calibrating scaling function based on current-step entropy, () is future-clarity bonus depending on the next step, and ζ 0 controls the contribution of the future-clarity term. Self-calibrating gradient scaling. The scaling function g() reweights the trajectory-level advantage according to the relative entropy of each step within batch: g(Hi,t) = (cid:80) 1 Tj (cid:16) exp (cid:17) Hi,t (cid:16) (cid:80) j,t exp Hj,t (cid:17) , (S22) where Hi,t denotes batch-normalized entropy value, Tj is the length of trajectory τj, and > 0 is temperature parameter. This normalization ensures that the average scaling factor over the batch equals one. Future clarity bonus. To encourage transitions toward lower-uncertainty future states, EMPG introduces future-clarity bonus defined as (Hi,t+1) = exp (cid:16) Hi,t+1 (cid:17) , (S23) where > 0 controls sensitivity to the entropy of the next step. Final advantage normalization. After computing Amod(i, t) for all steps in the batch, EMPG applies final batch-level normalization (e.g., zero-mean normalization) before using the resulting advantages in policy gradient updates. Key Hyper-parameter The hyperparameters reported in Table S1 are determined through task-specific grid search. For each policy optimization method and environment, we sweep over the method-relevant hyperparameters while keeping the remaining training and optimization settings fixed. The final configurations correspond to the stable settings selected from the grid search."
        },
        {
            "title": "C Additional Experiment Result",
            "content": "C.1 Performance on 8B Model To further investigate the scalability of our findings, we evaluate the 8B parameter model (Qwen3-8B) on AlfWorld, which serves as representative benchmark for complex, multi-turn agentic tasks. Given the substantial computational requirements for large-scale RL training, we focus on this environment to verify if the core design principles distilled from the 4B models remain consistent at larger scale. As shown in Table S2, the experimental results on AlfWorld demonstrate that the relative performance gains and stability trends are highly consistent with our observations in the 4B experiments Section 4. Specifically, the critical importance of sequence-level clipping is reaffirmed: even with increased model capacity, it remains the indispensable factor for preventing training collapse. Furthermore, we observe that the benefits of advantage design and dynamic filtering persist at this larger scale, providing consistent but incremental improvements to final performance. In contrast, the choice of loss aggregation continues to exhibit limited impact, echoing our findings on 4B models. These results collectively suggest that the hierarchical impact of policy design choicesand the resulting SAMPO recipeis robust and scale-invariant, effectively leveraging the enhanced reasoning capabilities of larger models while maintaining stable training dynamics. C.2 Additional Analysis Result Figure S1 Sequence-Level IS Analysis of CISPO and CISPOSM (CISPO with sequence masking) on ALFWorld. We further visualize the training dynamics of CISPO and CISPOSM on the AlfWorld task using diagrams. Specifically, following the same setup as in the main text, we categorize trajectories according to three factors: the sign of the advantage, whether the entropy exceeds predefined threshold, and whether the IS ratio is greater than zero. These criteria partition the samples into eight groups, which we use to analyze how the KL divergence evolves during training. 21 Table S1 Key training hyperparameters for agentic RL experiments across four tasks (ALFWorld, WebShop, Sokoban, TIR Math). indicates the method is not applicable to that task. Category ALFWorld WebShop Sokoban TIR Math Model and Environment Configuration Base model Qwen3-4B-RFT Qwen3-4B-RFT Qwen3-4B-VL-Instruct-RFT Qwen3-4B-Base Max interaction steps 50 Memory context window 2 (turns) Group rollout size Max prompt length Max response length Format penalty coefficient Group normalization mode Learning rate Mini-batch size KL coefficient Rollout engine Temperature (training) Temperature (validation) Top-p (validation) Top-k (validation) User Prompt Number Validation batch size Total epochs GPUs GRPO GIGPO EMPG GSPO CISPO SAPO DAPO εhigh εlow ε γ ω ε k, ζ εhigh εlow εhigh εlow τpos τneg εhigh εlow Noversample 8 2048 0.1 15 2 (turns) 8 4096 0.1 Training Optimization 15 2 (turns) 8 512 0.1 5 8196 (tokens) 5 4096 0.1 mean_std_norm 1 106 mean_std_norm 1 106 mean_std_norm 1 106 mean_std_norm 1 10 256 0.01 vLLM 1.0 0.6 0. 20 16 128 128 0.01 Rollout and Inference Configuration vLLM 1.0 0.6 0.95 20 Training and Batching 16 128 64 0.01 vLLM 1. 0.7 0.95 20 32 128 0 vLLM 1.0 0.6 0.95 512 128 200( 24h) 200( 22h) 200( 12h) 30( 80h) NVIDIA H200/B200 NVIDIA H200/B200 NVIDIA H200/B200 NVIDIA H200/B200 PO-specific Parameters 0.2 0. 0.2 0.95 1 0.2 1.0 0. 4e-3 3e-3 0.2 1 1.0 1. 0.2 0.2 3 0.2 0.2 0. 0.95 1 0.2 1.0 0.05 4e3e-2 0.2 1 1.0 1.05 0. 0.2 3 22 0.2 0.2 0. 0.95 1 0.2 1.0 0.05 4e3e-3 0.2 1 1.0 1.05 0. 0.2 3 0.28 0.2 4e-3 3e0.28 1 1.0 1.05 0.28 0. 2 Dimension Method"
        },
        {
            "title": "Dynamic\nSampling",
            "content": "Ours ALFWorld"
        },
        {
            "title": "Score",
            "content": "2."
        },
        {
            "title": "Success",
            "content": "50.92 1.68 29.1% 49.31 3.2% 0.08 96.6% 1.93 96.21% 0.80 66.2% 30.83 39.5% 5.05 113.1% 79.70 56.5% 4.10 73.0% 80.03 57.2% 4.51 90.3% 71.48 40.4%"
        },
        {
            "title": "DAPOGRPO\nDAPOGIGPO\nSAMPO",
            "content": "0.81 65.8% 38.11 25.16% 2.49 5.1% 60.27 18.4% 8.98 278.9% 97.71 91.9% Table S2 Performance on Qwen3-8B for ALFWorld. The overall trend on the 8B variant remains consistent, and SAMPO continues to achieve the best performance, indicating stable gains under model scaling. Consistent with our earlier findings, we clearly observe that after CISPO collapses, trajectories with negative advantages and low IS ratios (i.e., adv < 0 and IS < 1) rapidly dominate the distribution. This imbalance correlates strongly with the surge in KL divergence and subsequent training instability. This observation also explains why CISPOSM, which incorporates sequence-level masking, achieves substantially improved stability: by masking these harmful negative-advantage and low-ratio trajectories, the optimization process avoids pathological updates and maintains more balanced gradient signals. C.3 Task Environment Details ALFWorld (Shridhar et al., 2020): It provides text-based interactive setting in which LLM agents are required to complete goal-driven tasks that involve reasoning over multiple sequential decisions. The environment focuses on everyday household activities and evaluates an agents ability to plan and act through iterative interaction. WebShop (Yao et al., 2022): It is large-scale interactive environment that places agents in realistic e-commerce scenarios, requiring them to interpret user instructions and make sequential decisions to identify and purchase suitable products. Sokoban (Schrader, 2018): It is classic grid-based planning task where an agent navigates 2D environment to push all boxes onto designated target cells. The state is represented visually, and the agent selects from discrete movement actions TIR Math (Xue et al., 2025): This task focuses on standard mathematical question answering, where Python is used as tool for intermediate calculations and symbolic reasoning. The overall pipeline follows Xue et al. (2025). The training data are adapted from SimpleRL (Zeng et al., 2025), and evaluation is conducted on the AIME and AIME25 benchmarks. Performance is measured using avg@k, following the evaluation protocol in Yu et al. (2025b)."
        },
        {
            "title": "D Related Work",
            "content": "Large language models have demonstrated strong capabilities in agent-based environments and attracted increasing attention (Li et al., 2023; Shridhar et al., 2020; Yao et al., 2022). Prior studies investigate LLMs as agents in multi-turn, action-based environments, emphasizing long-horizon memory and explicit tool use for sequential decision making and reasoning (Schick et al., 2023; Wang et al., 2023; Yao et al., 2023). Recently, driven by the success of reinforcement learning in reasoning (Khatri et al., 2025; OpenAI, 2025a; Xu et al., 2025), RL has been extended to agentic settings (Abdulhai et al., 2023; Jin et al., 2025; Plaat et al., 2025; Yu et al., 2025c). Several representative RL frameworks 23 for LLM agents have emerged. AGILE (Peiyuan et al., 2024) proposes framework for LLM-driven conversational agents capable of planning, tool use, and expert consultation. SWEET-RL (Zhou et al., 2025) studies collaborative LLM agents that interact with simulated human partners in ColBench, where agents ask clarifying questions and learn from multi-turn feedback. Agent-R1 (Cheng et al., 2025) extends this paradigm to external tool-based environments and enables multi-turn reasoning with tool calls. Similarly, AgentGym-RL (Xi et al., 2025) presents an RL framework for autonomous LLM agents that supports multi-turn interactions, modular architectures, and real-world scenarios. AgentRL (Zhang et al., 2025) develops multi-turn, multi-task RL system and demonstrates superior performance relative to closed-source models. VerlTool (Jiang et al., 2025) focuses on tool-using LLM agents and aligns well with the VeRL codebase. Most prior work provides limited analysis of agentic RL training instability. In contrast, ARLArena offers unified training and analysis framework for examining how policy-gradient design choices relate to stability and performance across agentic tasks. Another Roadmap of Building Agentic LLM: Multi-agent System E.1 Debate Let = {A1, A2, . . . , AN } denote the set of agents, where is an odd integer to prevent tie-breaking scenarios during majority voting. Let denote the task prompt. In the initial round (t = 0), each agent Ai independently generates candidate solution c(0) based solely on the prompt x: c(0) = Ai(x), {1, . . . , } (S24) Let C(t) = {c(t) M() that returns the solution if it appears in more than half of the agent responses: } be the set of candidate solutions at round t. We define majority consensus function 2 , . . . , c(t) 1 , c(t) = M(C(t)) = (cid:40) ˆc (cid:12){c C(t) : = ˆc}(cid:12) (cid:12) > 2 if (cid:12) otherwise (S25) If M(C(0)) = , the process terminates and outputs y. Otherwise, the system enters the debate phase. The process iterates through debate rounds = 1, 2, . . . , Tmax. For each round, we construct the debate prompt for each agent, which includes the original prompt x, the set of unique candidate solutions from the previous round Unique(C(t1)), and agents reasoning in previous round R(t1). Let R(t) = {r(t) } be the agents reasoning at round t, and R(0) = . 2 , . . . , r(t) 1 , r(t) r(t) , c(t) = Ai (cid:16) x, Unique(C(t1)), R(t1)(cid:17) . (S26) At the end of each round t, we check for consensus again and output the solution if consensus is reached. This mechanism enables agents to either rectify perceived flaws by proposing new solution or align with peer by voting for an existing candidate. The debate terminates when majority consensus is achieved, M(C(t)) = . If the maximum iteration limit Tmax is reached without consensus, the final output is randomly sampled from the final set of candidates C(Tmax). E.2 Aggressive Debate We extend the Debate framework discussed above to build decisively goal-oriented variant designed to prioritize task completion over exhaustive exploration. While the standard framework seeks consensus on an optimal solution, the aggressive variant compels agents to accept partial success by securing the best available option within strict finite horizon. Formally, we modify the agent Ai by conditioning it on an additional constraint set Iagg. Unlike standard debate agents that aim for perfect solution, the aggressive agent Ai(Iagg) operates under modified utility function characterized by several governing principles: (1) Bounded Exploration: The agent must finalize the interaction within finite horizon. This constraint suppresses excessive exploration and ensures the agent commits to definitive outcome rather than prolonging the information-gathering phase; (2) Temporal Efficiency: The agent is encouraged to conclude the interaction as early as possible; (3) Incentive Awareness: The agent is explicitly informed that partial rewards are available. This awareness incentivizes the agent to accept high-utility suboptimal outcomes when perfect solution is unattainable; (4) Pragmatic Optimization: The agent prioritizes securing result that maximizes available partial rewards rather than seeking theoretical global optimum, thereby avoiding diminishing returns associated with perfecting the solution in complex environments. E.3 Experiment Results on SLA and MAS Method GPT-4o GPT-5.2 Debate ALFWorld WebShop Pick Look Clean Heat Cool Pick All Score Success 61.11 33.33 36. 50.00 45.45 63.64 50.00 13.60 70. 66.07 35.37 62.30 52.08 37.36 51. 26.56 67.74 64.28 33.33 60.00 52. 65.00 56.25 22.65 Aggressive Debate 28. Gemini-2.5-pro 84.97 61.61 63.94 22.22 62. 75.25 66.41 GRPO SAPO CISPO GSPO GIGPO EMPG DAPOGRPO DAPOGIGPO SAMPO 87.41 62.65 46.42 72.28 58.89 38. 72.61 75.32 34.49 32.19 24.13 24. 16.21 9.37 25.16 73.85 76.03 37. 58.56 50.97 57.88 23.68 54.42 67. 90.36 79.31 90.71 75.45 77.95 48. 78.61 85.29 94.80 83.03 86.37 81. 75.38 59.21 81.09 67.76 84.18 61. 69.83 72.49 46.51 81.28 37.57 53. 40.16 51.28 0.04 6.43 57.91 79. 49.58 62.43 85.04 55.26 65.35 58. 56.52 26.57 60.55 88.10 96.30 88. 93.65 92.42 92.70 88.35 92.72 88. 12.50 26.56 34.65 61.53 57. 52.10 54.71 72.48 56.55 64.32 46. 76.82 74.08 Table S3 Unified comparison across ALFWorld (six task types + overall) and WebShop (score and success rate). The upper block reports closed-source baselines and multi-agent strategies; the lower block reports policy optimization methods trained with Qwen3-4B."
        },
        {
            "title": "F Failure Analysis",
            "content": "F.1 Method: Sankey Graphs for Action-Transition Flows We analyze agent rollouts by visualizing step-wise action transitions with Sankey graphs. Each column corresponds to time step, node height indicates the empirical frequency of an action at that step, and edges represent transitions between consecutive steps. Compared with action histograms, Sankey graphs preserve temporal structure and thus reveal loop-like behaviors (e.g., repetitive pagination or oscillation between two actions) that dominate long-horizon failures. F.2 WebShop: Action-Transition Patterns and Failure Modes Overall flow (API agent). The API agent is single-agent baseline powered by GPT-4o via API under the same interaction protocol, without any task-specific training. Figure S2 summarizes WebShop trajectories of the API agent, where green links correspond to successful episodes and red links correspond to failures. large fraction of failures is characterized by repetitive next actions, suggesting exploration inefficiency where the agent keeps paginating without making progress toward constraint satisfaction. 25 Figure S2 WebShop action-transition Sankey for the API agent. Green flows denote successful trajectories and red flows denote failures. Failure-only flow with action coloring. Figure S3 focuses on failed trajectories and colors nodes by action type. Two dominant failure patterns are observed: (i) Pagination loops: long runs of next (and occasional search) that rarely transition into click_product (product-detail inspection); (ii) Backtracking oscillation: frequent alternation between click_product and back, suggesting repeated revisits to previously viewed product pages and limited progress toward constraint satisfaction. Notably, our API agent is provided with long interaction history (past actions and observations) in the prompt, so this pattern is unlikely to be explained by insufficient context alone. Instead, it may reflect limited effective memory usage: without structured tracking or summarization of verified attributes and visited items, the agent may fail to retrieve previously established evidence from long, unstructured context and thus re-check similar products. We emphasize that this is only one plausible factor; we find instruction ambiguity or conflicting constraints may also contribute. Figure S3 WebShop failure-only action-transition Sankey for the API agent. Nodes are colored by action type (e.g., search, click_product, click_other, buy, back, next). 26 F.3 WebShop: How RL Post-training Changes Behaviors Overall flow (RL-optimized agent). Figure S4 shows the same visualization for our RL-optimized agent (post-trained with RL). Compared with the API baseline, the RL agent exhibits fewer next-dominated failure paths and higher proportion of trajectories that transition into click_product and eventually attempt buy, consistent with more targeted product inspection and earlier decision making. Figure S4 WebShop action-transition Sankey for the RL-optimized agent. Green flows denote successful trajectories and red flows denote failures. Remaining failure modes after RL post-training. Figure S5 focuses on failed RL trajectories. While next-heavy pagination loops become less prominent, two residual issues remain: (i) Backtracking-heavy browsing: repeated click_other/back transitions, suggesting inefficient navigation; (ii) Premature purchase: occasional buy attempts that do not satisfy all constraints, suggesting incomplete constraint tracking. Figure S5 WebShop failure-only action-transition Sankey for the RL-optimized agent. Nodes are colored by action type (e.g., search, click_product, click_other, buy, back, next). 27 Figure S6 ALFWorld action-transition Sankey diagrams for the API agent. Top: Success (green) vs. failure (red) trajectories. Bottom: Failure trajectories with nodes colored by action type. F.4 ALFWorld: Action-Transition Patterns Figure S6 visualizes ALFWorld rollouts. Navigation actions (e.g., go, look) dominate early steps across episodes, whereas successful trajectories more often transition into object-centric interactions (e.g., examine, open/close, take, use) and explicit state-checking (inventory). In contrast, failed trajectories frequently exhibit prolonged navigation with comparatively fewer object interactions, which may reflect weak progression toward concrete object-level subgoals and imperfect tracking of what has already been tried or collected over long horizons. F.5 Implications Our analysis suggests two actionable directions: (1) Loop-aware control (e.g., detecting repeated next or click_product back cycles and triggering plan change); (2) Explicit constraint/state memory (e.g., introducing lightweight memory agent that maintains concise record of visited items and verified constraints, and feeds the acting agent with short summaries or retrieval results). Together, these mechanisms may further improve robustness beyond RL post-training."
        },
        {
            "title": "G Visualization",
            "content": "G.1 Evidence of Format v.s. Dynamic Filtering To support the analysis in Section 4.4, we report the format validity ratio during training for different policy optimization variants. The results illustrate that DAPO combined with GIGPO maintains more stable format behavior than DAPO+GRPO after dynamic filtering. Figure S7 Format validity ratio during training on AlfWorld and WebShop for GRPO, GIGPO, DAPOGRPO, and DAPOGIGPO. Applying dynamic filtering to GRPO leads to degraded format stability, whereas DAPOGIGPO maintains stable format behavior across training."
        },
        {
            "title": "H Case Study",
            "content": "H.1 Prompt Templates H.1.1 TIR Math TIR Math Prompt TIR_TEMPLATE = \"\"\" Solve the following problem step by step. You now have the ability to selectively write executable Python code to enhance your reasoning process. The Python code will be executed by an external sandbox, and the output (after \"Code execution result: \") is returned to aid your reasoning and help you arrive at the final answer. The Python code should be complete scripts, including necessary imports. Code Format: Each code snippet is wrapped between . You need to use print() to output intermediate results. Answer Format: You can use the final_answer() function in the code to return your final answer. 29 For example, to answer the User Question: What is the result of the 5 + 3 + 1294.678?, you can write: py answer = 5 + 3 + 1294.678 final_answer(answer) You can also use boxed to return your answer. The last part of your response should be: boxed{The final answer goes here.} User Question: \"\"\" H.1.2 WebShop WebShop Prompt WEBSHOP_TEMPLATE = \"\"\" You are an expert autonomous agent operating in the WebShop e-commerce environment. Your task is to: {task_description}. Prior to this step, you have already taken {step_count} step(s). Below are the most recent {history_length} observations and the corresponding actions you took: {action_history} You are now at step {current_step} and your current observation is: {current_observation}. Your admissible actions of the current situation are: [ {available_actions} ]. Now its your turn to take one action for the current step. You should first reason step-by-step about the current situation, then think carefully which admissible action best advances the shopping goal. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for the current step and present it within <action> </action> tags. \"\"\" H.1.3 ALFWorld ALFWorld Prompt ALFWORLD_TEMPLATE = \"\"\" You are an expert agent operating in the ALFRED Embodied Environment. Your task is to: {task_description} Prior to this step, you have already taken {step_count} step(s). 30 Below are the most recent {history_length} observations and the corresponding actions you took: {action_history} You are now at step {current_step} and your current observation is: {current_observation} Your admissible actions of the current situation are: [{admissible_actions}]. Now its your turn to take an action. You should first reason step-by-step about the current situation. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for the current step and present it within <action> </action> tags. \"\"\" H.1.4 Sokoban Sokoban Prompt SOKOBAN_TEMPLATE = \"\"\" You are an expert agent operating in the Sokoban environment. Your goal is to push all the boxes onto the target spots. Once all boxes are on the targets, you win! # Rules You can only push boxes. You cant pull them, so plan ahead to avoid getting stuck. You cant walk through or push boxes into walls. To avoid traps, do not push boxes into corners or against walls where they cant be moved again. # Visual Elements in the Image: Character: small, green alien-like figure with two antennae and black eyes. It represents you. Box: yellow crate marked with an orange \"X\" across its front. It is the box you need to push. Target: black tile outlined in red, with small red diamond shape in the center. It marks the destination where box should be pushed. # Current Step Your current observation is shown in the image: <image> Your admissible actions are [\"up\", \"down\", \"left\", \"right\"]. Now its your turn to make move (choose ONE action only for the current step). You should first reason step-by-step about the current situation observe the positions of boxes and targets, plan path to push box toward target, and avoid traps like corners or walls. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an 31 admissible action for the current step and present it within <action> </action> tags. \"\"\" H.2 Multi-turn State-Action Templates H.2.1 TIR Math TIR Math Rollout Example Input: user Solve the following problem step by step. You now have the ability to selectively write executable Python code to enhance your reasoning process. The Python code will be executed by an external sandbox, and the output (after \"Code execution result: \") is returned to aid your reasoning and help you arrive at the final answer. User Question: Each triangle is 30-60-90 triangle, and the hypotenuse of one triangle is the longer leg of an adjacent triangle. The hypotenuse of the larger triangle is 16 centimeters. What is the number of centimeters in the length of the longer leg of the smaller triangle? Output (Step-by-Step Reasoning): 1. Step 1: Recall 30-60-90 triangle ratios Sides are in ratio 1 : 3 : 2, where: Short side = (opposite 30) Long side = Hypotenuse = 2x 3 (opposite 60) 2. Step 2: Larger triangle properties Hypotenuse of larger triangle h1 = 16 cm, so 2x1 = 16 = x1 = 8 cm. Longer leg l1 = 3 = 8 3 cm. 3. Step 3: Relate smaller triangle Hypotenuse of one triangle = longer leg of adjacent triangle: l2 = h1 = 16 cm. Solve for x2: 3 = 16 = x2 = 16 3 cm. 4. Step 4: Final Answer Longer leg of smaller triangle: l2 = 16 cm. Answer: 16 H.2.2 WebShop WebShop Rollout Example at one turn Input: user You are an expert autonomous agent operating in the WebShop e-commerce environment. Your task is to: Find me machine washable mens t-shirts with short sleeve for tumble dry with color: 36 pack mix, and size: xx-large, and price lower than 160.00 32 dollars. Your current observation is: Search. Your admissible actions of the current situation are: [ search[<your query>], click[search], ]. Now its your turn to take one action for the current step. You should first reason step-by-step about the current situation, then think carefully which admissible action best advances the shopping goal. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags. Output: <think> Okay, so need to find machine washable mens t-shirts, short sleeve, for tumble dry, 36 pack mix, xx-large, under $160. The current observation is Search, so need to create specific query that includes all these parameters. </think> <action> search[mens short sleeve t-shirts, 36 pack mix, xx-large, machine washable, tumble dryable, color: 36 pack mix, price: $160 or less] </action> H.2.3 Alfworld Alfworld Rollout Example at one turn Input: user You are an expert agent operating in the ALFRED Embodied Environment. Your current observation is: -= Welcome to TextWorld, ALFRED! =- You are in the middle of room. Looking quickly around you, you see cabinet 13, cabinet 12, cabinet 11, cabinet 10, cabinet 9, cabinet 8, cabinet 7, cabinet 6, cabinet 5, cabinet 4, cabinet 3, cabinet 2, cabinet 1, coffeemachine 1, countertop 1, diningtable 1, drawer 1, fridge 1, garbagecan 1, microwave 1, shelf 3, shelf 2, shelf 1, sinkbasin 1, stoveburner 4, stoveburner 3, stoveburner 2, stoveburner 1, and toaster 1. 33 Your task is to: put cool egg in microwave. Your admissible actions of the current situation are: [ go to cabinet 1, go to cabinet 10, go to cabinet 11, go to cabinet 12, go to cabinet 13, go to cabinet 2, go to cabinet 3, go to cabinet 4, go to cabinet 5, go to cabinet 6, go to cabinet 7, go to cabinet 8, go to cabinet 9, go to coffeemachine 1, go to countertop 1, go to diningtable 1, go to drawer 1, go to fridge 1, go to garbagecan 1, go to microwave 1, go to shelf 1, go to shelf 2, go to shelf 3, go to sinkbasin 1, go to stoveburner 1, go to stoveburner 2, go to stoveburner 3, go to stoveburner 4, go to toaster 1, inventory, look ]. Now its your turn to take an action. You should first reason step-by-step about the current situation. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags. Output: <think> Okay, need to put cool egg in the microwave. First, where would find cool egg? Lets see. The fridge is typically where you keep cool items like eggs. So maybe should start by going to the fridge. Looking at the list of admissible actions, theres \"go to fridge 1\". Thats probably the first step. So should go to the fridge. Once there, can check the interior of the fridge to see if theres an egg. Alternatively, maybe should check the dining table or the countertops in case the egg is already on surface. But think the fridge is more likely to have an egg. Since the task is to put cool egg in the microwave, its probably in the fridge. So should go to the fridge first. </think> <action> go to fridge 1 </action> H.2.4 Sokoban Sokoban Rollout Example at one turn Input: user You are an expert agent operating in the Sokoban environment. Your goal is to push all the boxes onto the target spots. Once all boxes are on the targets, you win! 34 # Rules You can only push boxes. You cant pull them, so plan ahead to avoid getting stuck. You cant walk through or push boxes into walls. To avoid traps, do not push boxes into corners or against walls where they cant be moved again. # Visual Elements in the Image: Character: small, green alien-like figure with two antennae and black eyes. It represents you. Box: yellow crate marked with an orange \"X\" across its front. It is the box you need to push. Target: black tile outlined in red, with small red diamond shape in the center. It marks the destination where box should be pushed. # Current Step Your current observation is shown in the image: Your admissible actions are [\"up\", \"down\", \"left\", \"right\"]. Now its your turn to make move (choose ONE action only for the current step). You should first reason step-by-step about the current situation observe the positions of boxes and targets, plan path to push box toward target, and avoid traps like corners or walls. This reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible action for current step and present it within <action> </action> tags. Output : <think> To push the box to the target, need to move it to the right. The current position is corner, which is trap. should avoid pushing the box into corners or against walls. The path is straightforward: move right. This action will allow me to push the box towards the target without getting stuck. </think> <action> right </action>"
        }
    ],
    "affiliations": [
        "University of California, Los Angeles",
        "University of Wisconsin Madison"
    ]
}