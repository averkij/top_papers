{
    "paper_title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
    "authors": [
        "Peter Polák",
        "Sara Papi",
        "Luisa Bentivogli",
        "Ondřej Bojar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency--the delay between speech input and the translated output. While quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented. In this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes. We uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more accurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose SoftSegmenter, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while SoftSegmenter enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems."
        },
        {
            "title": "Start",
            "content": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation Peter Polák1 and Sara Papi2 and Luisa Bentivogli2 and Ondˇrej Bojar1 1Charles University, Czech Republic 2Fondazione Bruno Kessler, Italy 1{surname}@ufal.mff.cuni.cz, 2{spapi,bentivo}@fbk.eu 5 2 0 2 2 2 ] . [ 1 9 4 3 7 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "speech-to-text Simultaneous translation (SimulST) systems have to balance translation quality with latencythe delay between speech input and the translated output. While quality evaluation is well established, accurate latency measurement remains challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented. In this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both shortand long-form regimes. We uncover structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), refined latency metric that delivers more accurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose SOFTSEGMENTER, novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while SOFTSEGMENTER enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems."
        },
        {
            "title": "Introduction",
            "content": "Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speakers speech (Ren et al., 2020). SimulST models have to balance between quality and latency of the output, which is the time elapsed between when word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation (Freitag et al., 2022, 2023; Zouhar et al., 2024), there is no study on the reliability of latency metrics. The most common latency metrics in SimulST (Cho and Esipova, Figure 1: Ranking of the systems submitted to the IWSLT 2023 Simultaneous Speech Translation Track according to the True Latency, the proposed automatic metric YAAL, and the official five latency metrics. 2016; Ma et al., 2019; Cherry and Foster, 2019; Polák et al., 2022; Papi et al., 2022; Kano et al., 2023), even though with different approximations, base their calculation on simplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation. However, despite relying on the same assumptions, these metrics often produce very inconsistent assessments. This inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation (Agarwal et al., 2023), where different metrics produced substantially different rankings (see Figure 1). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems. Moreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., shortform SimulST, to unsegmented audio, i.e., longform SimulST, where information about sentence boundaries is not available, further complicating the evaluation (Papi et al., 2025). In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and shortand long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks (Anastasopoulos et al., 2022; Agarwal et al., 2023; Ahmad et al., 2024), we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to structural bias in how latency is measuredparticularly in how segmentation influences SimulST models behavior. Motivated by these findings, we propose YAAL (Yet Another Average Lagging), refined latency metric designed to mitigate the biases present in existing latency metrics. Our extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems. Furthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce SOFTSEGMENTER, new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community (Matusov et al., 2005a), SOFTSEGMENTER significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios."
        },
        {
            "title": "2 Background",
            "content": "Throughout the paper, we assume incremental SimulST systems, i.e., systems that cannot revise their outputs, as they are not affected by flickering problems, and are leading current research efforts in the topic (Papi et al., 2025)."
        },
        {
            "title": "2.1 Short-Form SimulST Latency Metrics",
            "content": "The short-form is the most common evaluation regime of SimulST (Anastasopoulos et al., 2022; Agarwal et al., 2023; Ahmad et al., 2024), where all recordings of the test set are divided, usually following sentence boundaries, into short segments of few seconds. Each segment consists of source audio = [x1, . . . , xX], where xi is small portion of raw audio, i.e., audio chunk, and reference translation YR = [yR YR]. Each audio chunk xi is incrementally fed to the system, which concurrently outputs translation token yj at timestamp dj, i.e., total duration of audio chunks up to and 1 , . . . , yR including the audio chunk xi. Under these settings, we describe the latency metrics operating in the short-form regime: Average Proportion (AP; Cho and Esipova, 2016) measures the average proportion of input speech read when emitting target token: AP = 1 XY (cid:88) i=1 di. (1) Average Lagging (AL; Ma et al., 2019) for simultaneous machine translation and modified for speech by Ma et al. (2020) defines the latency as the average delay behind an ideal policy: AL = 1 τ (X) τ (X) (cid:88) i=1 di , (2) where τ (X) = min{idi = X} is the index of the hypothesis token when the model reaches the end of the source sentence, also known as the cutoff point. AL considers delays up to and including the one associated with the token at the cutoff point. The i-th delay of the ideal policy is defined as = (i 1)/γ, where γ = YR/X. Length-Aware Average Lagging (LAAL) is an AL modification that is robust to overgeneration, i.e., when the hypothesis is much longer than YR, which makes the original AL produce negative delays when YR. To overcome this problem, which was unduly rewarding overgenerating systems, Polák et al. (2022) and Papi et al. (2022) proposed the modification γ = max(Y, YR)/X. Differentiable Average Lagging (DAL; Cherry and Foster, 2019) modifies AL by introducing minimal delay of 1/γ after each step. Unlike AL and LAAL, DAL considers all tokens in the hypothesis, without cutoff after > τ (X): DAL = 1 Y (cid:88) i= d , (3) and instead DAL penalizes each write operation by at least 1/γ: 1The code for YAAL, its long-form variant LongYAAL, and SOFTSEGMENTER will be released upon the paper acceptance under Apache 2.0 license. = (cid:40) di, max(di, i1 + 1/γ), if = 1 otherwise. (4) Average Token Delay (AP; Kano et al., 2023) assumes that the source speech, similar to the translation, consists of discrete tokens. ATD defines fixed duration for speech tokens of 300ms and divides the input speech and translation into chunks, where the c-th translation chunk yc is translated conditioned on the source chunk xc and previous translation chunks y1, . . . , yc1. ATD is then defined as the average delay between each translation and the corresponding source tokens: ATD = 1 Y (cid:88) i=1 (T (yt) (xa(t))), (5) where () is the end time of the source/translation token and a(t) = (cid:40) s(t), Lacc(xc(t)), ifs(t) Lacc(xc(t)) otherwise, (6) is an index of source token corresponding to translation token yt, where Lacc(xc) is the number of source tokens in the chunk xc and s(t) = tmax(0, Lacc(yc(t)1)Lacc(xc(t)1)) handles the case where more tokens are generated than read, i.e., yt is aligned with xt, < t."
        },
        {
            "title": "2.2 Long-Form SimulST Latency Metrics",
            "content": "The long-form evaluation regime evaluates SimulST systems more realistically (Papi et al., 2025), as it assesses their ability to handle long audio streams, often spanning several minutes. Since all metrics were developed for the short-form regime, recent studies (Papi et al., 2024; Polák and Bojar, 2024) resorted to resegmentation of translations and delays based on the reference translation (Matusov et al., 2005b), and computed the metrics on the segment level. We explain the long-form variant of LAAL (Papi et al., 2024) below. Streaming LAAL (StreamLAAL; Papi et al., 2024) extends the LAAL metric to unsegmented audio streams = [X1, ..., XS], paired with continuous stream of predicted translations YS. Since reference translations YR are only available at segment-level X1, ..., XS, prediction YS = [Y1, ..., YS] with the corresponding delays is segmented based on reference sentences YR to obtain segment-level predictions. Then, StreamLAAL is computed as: 1 , ..., YR Stream LAAL = 1 S (cid:88) s=1 1 τ (Xs) τ (Xs) (cid:88) i=1 di (7) = (i 1) Xs/max{Ys, YR Where } In practice, the LAAL metric is calculated for every speech segment Xs of the stream and its corresponding reference YR with the automatically aligned prediction Ys and then averaged over all the speech segments of the stream X1, ..., XS. Alternatively, Huber et al. (2023) proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segments start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the systems self-reported alignment might hinder the reliability. Second, as we empirically show in 5.1, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems."
        },
        {
            "title": "3.1 The Short-Form Regime",
            "content": "Figure 2: Translations and emission times of SimulST model. Words in column were emitted at once, last five words or tail words gemeinnützige Organisation namens Robin Hood. depend on the segmentation: Oracle Segmentation: The optimal segmentation is known beforehand. Once the model consumes the entire sentence, it is asked to finish the translationwithout additional delay. Simultaneous Segmentation: The evaluation uses an online segmenter that needs extra time (here: 0.5s) to decide when the sentence ends. The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in simulated environment where each segment is processed independently (Ma et al., 2020). When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay. This setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by human annotator or an automatic model with access to the full audio (Oracle Segmentation). Second, the model is allowed to generate the remaining translation (hereinafter, tail words) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in realistic setting, where the system must wait to confirm that the sentence has ended. In more realistic scenario, model has to rely on online segmentation (Simultaneous Segmentation) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency. This discrepancy is illustrated in Figure 2. Based on these observations, we categorize existing short-form latency metrics (2.1) into two main groups, depending on whether they include all translated words or only subset in their latency computation. The first groupAP, DAL, and ATDincludes all translated words in the calculation. DAL attempts to mitigate the impact of tail words by adding minimum delay of 1/γ after each generated word (also within the same step), thus spreading the tail beyond the sentence. However, 1/γ simply reflects the average source-totarget length ratio and does not accurately capture the system behavior for tail words in settings without segmentation. If multiple words are emitted as tail words, DAL can significantly overestimate latency. In the edge case of system that waits for an end-of-segment signal (i.e., an offline system), DAL returns the segment length, failing to capture the systems true behaviorin this case, undefined latency. AP assigns delay of 1 to each tail word as the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DALsince it operates on proportions rather than absolute delaysit still fails to capture system behavior faithfully for the tail words. ATD also considers all translated words. However, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics. The second groupAL and LAALcomputes latency only for words emitted up to and including the cutoff point τ (X), which marks the first word generated after the end of the input segment. This corresponds to the word gemeinnützige in Figure 2. As discussed, in the short-form regime with oracle segmentation, the τ (X)-th and following words are often translated earlier than in more realistic long-form scenario. As result, this cutoff introduces systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the systems policy. AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to systematic bias that undermines fair comparisons. To cope with this bias, we propose new metric derived from the LAAL metric: Yet Another Average Latency (YAAL) We refine the LAAL formulation to better isolate the portion of output that is actually produced in simultaneous settings. Specifically, we define new cutoff point τYAAL(X) = max{idi < X}, which includes only words generated strictly before the end of the input stream. This corresponds to words up to and including eine in Figure 2, thereby avoiding distortion from tail words and yielding more reliable latency estimate that remains consistent across different segmentation regimes."
        },
        {
            "title": "3.2 The Long-Form Regime",
            "content": "The long-form regime offers more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect realworld use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting. First, metrics such as AL, LAAL, and DAL rely on γ parameter, representing the average target-tosource length ratio. However, γ can vary substantially across different segments within the same audio, leading to inconsistent and unreliable latency estimates (Iranzo-Sánchez et al., 2021). Second, AP tends to converge toward 0 for long recordings, as typical speech inputs are significantly longer than the translations, i.e., Y, leading Equation (1) to approach 0. Finally, ATD assumes that each speech token has fixed duration and that source and target tokens align monotonically assumptions that are overly restrictive and especially unrealistic for long-form speech. To address these challenges, StreamLAAL has introduced resegmenting long inputs into short segments and computing latency on these units. While StreamLAAL provides adaptation of existing metrics to long-form input, it relies on the mWERSegmenter tool (Matusov et al., 2005a), which may introduce alignment errors (Amrhein and Haddow, 2022; Polák and Bojar, 2024), and computes latency up to the cutoff word τ (Xi) (Equation (7)), which can lead to the systematic bias (3.1) To overcome these limitations, we propose new resegmentation method and an extension of the YAAL metric for the long-form regime. SOFTSEGMENTER We introduce new resegmentation method inspired by Polák and Bojar (2024), employing softer alignment strategy to more accurately match the translation output with the reference segments. We start by lowercasing and tokenizing both the reference and the system hypotheses. This allows for more precise alignment around the sentence ends, especially in cases where the reference and the model differ in sentence segmentation. However, we still keep the original texts in memory so as not to interfere with the machine translation quality evaluation that is computed over the resegmented hypotheses. Additionally, we keep the delay together with each token and use it during the alignment process to prevent alignment of tokens to future segments, which leads to spurious negative latencies. For alignment, we maximize the following score: S(tr, th) = Schar(tr, th) sr dh, (tr) (rh), otherwise, (8) where tr and th, are the reference and hypothesis tokens, sr is the start of the reference segment, dh is the emission time of the hypothesis token, () is function that indicates if the token is punctuation, and finally Schar(tr, th) is the character-level similarity of the reference and hypothesis tokens, which we define as Schar(tr, th) = (tr th)/(tr th). In case of character-based languages such as Chinese, this reduces to an exact match. Long-Form YAAL (LongYAAL) We also extend YAAL to the long-form regimei.e., LongYAAL. Unlike StreamLAAL, LongYAAL includes all words in the latency computation, even those generated beyond the aligned segment boundaries Xs, i.e., all di for > τ (Xs). However, we exclude the final tail words produced after the end of the full stream S, i.e., di for > τ ((cid:80)X s=1 Xs). This ensures that we include all words emitted beyond the segment boundaries Xs, but we do not include the tail words generated at the end of the entire stream S. If the stream consists of single segment, LongYAAL coincides with YAAL."
        },
        {
            "title": "4 Experimental Settings",
            "content": "4.1 Data We use SimulEval (Ma et al., 2020) logs as submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023, and 2025. Detailed information on the datais presented in Section A. 4.2 Evaluation True Latency To enable fair comparisons across latency metrics, we require reference latency reflecting the user experience, i.e., how long the user needs to wait for translation. Since human evaluation is infeasible at scale, we adopt carefully designed automatic approximation, which we refer to as true latency. This is grounded in an intuitive and practical definition of latency in speech translation: On average, how long does user have to wait for given piece of source information to appear in the translation? Concretely, we define true latency as the average delay between each target word and its corresponding source word: = 1 YA YA (cid:88) i=1 di dsrc , (9) i = maxl {send where di is the emission time of the target word yi and dsrc is the corresponding source delay. We define the source delay as the time that the speaker finished the last word corresponding to the target word: dsrc (yi, sl) A(Y S)}, where send is the end timestamp of the source word sl and A(Y S) is the translation alignment between the target and the source. As discussed in 3.1, computing latency over all wordsincluding tail wordscan introduce systematic bias. To mitigate this, we restrict the true latency calculation to words generated strictly during simultaneous decoding, i.e., before the end-of-source signal. Additionally, we consider only the subset of target words YA that are aligned to at least one source word, thereby avoiding biases introduced by overor under-generation (Polák et al., 2022; Papi et al., 2022). The implementation details are provided in Section B. Score Difference For the main evaluation, we adopt the pairwise comparison approach (Mathur et al., 2020). Rather than evaluating each system independently, we examine the difference between the scores of two systems: = score(System A) score(System B). Pairwise comparison better reflects the typical use case of latency metricsdistinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair. Accuracy Following Kocmi et al. (2021), we evaluate the accuracy of binary comparisons: given pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to metric matches that of the true latency: Accuracy = sign(TL) = sign(M) all system pairs . Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales. However, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets by removing pairs that are not significantly different according to Mann-Whitney test on their true latencies.2 We use bootstrap resampling with = 10000 (Tibshirani and Efron, 1993) to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied."
        },
        {
            "title": "5.1 Short-Form Evaluation",
            "content": "We present pairwise comparison of all short-form systems in Figure 3. An important first observation is that significant portion of system pairs exhibit no or slightly negative correlationspoints that create almost vertical lines and lines far off the diagonal. These systems share anomalous simultaneous policy: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline. We assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the dead 2We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests. Figure 3: Each point represents the difference between the true latency (x-axis) and the automatic metric (yaxis) for two systems. Reported Pearson and Kendall rank correlations are illustrative, as each language pair has different scale. Direct comparison in Figure 5. Figure 4: Each point represents the actual and expected proportion of words translated online as observed on IWSLT 2022 and IWSLT 2023 EnDe test sets. Each color represents systems submitted by one team. Circles and crosses show the expected values computed based on YAAL and LAAL latencies, respectively. end of probable outputs and only emits the tail words at the signaled end of the sentence. This policy, coupled with bias in latency metrics, leads to severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy. Detecting Anomalous Policy To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set S, i.e., (cid:80)S = ds {ds (cid:80)S < Xs} Ys . (10) The expected fraction of words translated online based on the value of an automatic latency metric is Oe = (Xavg L)/Xavg, where Xavg is the average segment length. If the expected onlinetranslated word fraction significantly exceeds the observed one, i.e., Oe O, we conclude that the system follows the anomalous policy. In Figure 4, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow vertical line, i.e., Oe O. However, based on YAAL, the red and brown circles3 show strong disagreement between OYAAL and O. Based on LAAL (crosses), we observe some disagreement between OLAAL and O. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. This shows the importance of accurate evaluation of latency using YAAL. Which is the best Short-form Latency Metric? Moving to the metrics, we observe (Figure 3) that they all show positive correlations with the true latency, but each language pair has slightly different scale, which motivates the use of accuracy instead of simple correlation. Therefore, we further compare latency metrics in terms of accuracy in Table 1. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider subset that has p-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by margin of 25% absolute. If we remove systems with the anomalous policy, all metrics gain significant boost in accuracy (bottom part of Table 1). However, the YAAL metric remains the best metric in all subsets based on p-values, achieving 98 and 99% accuracyeven though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment. Based on these observations, we conclude that the automatic YAAL metric is almost as accurate as true latency. We include more accuracy evaluations by isolating different categories of systems in Section C. Should we use the Short-Form Regime? As discussed in 3 and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In Table 2, we present the p-val AL LAAL DAL ATD AP YAAL all <0.05 <0.001 0.001-0.05 all <0.05 <0.001 0.001-0. 0.66 0.67 0.68 0.40 0.95 0.96 0.96 0.71 all system pairs 0.69 0.70 0.70 0.46 0.59 0.59 0.59 0.40 0.56 0.56 0.56 0. w/o anomalous policy 0.97 0.97 0.98 0.74 0.95 0.96 0.97 0.66 0.92 0.92 0.93 0.74 0.74 0.75 0.76 0.42 0.85 0.85 0.85 0. 0.96 0.98 0.98 0.71 0.98 0.99 0.99 0.74 5326 5149 5048 101 2100 2060 2025 35 Table 1: Accuracy of systems in the short-form regime. Best scores in bold. Underlined scores are considered tied with the best metric. Latency regime [s] Tail Words [%] 1-2 41 2-3 49 363 4-5 72 Table 2: Average fraction of words generated after the end-of-segment signal under the short-form evaluation regime, averaged across all systems. average fraction of words translated after the endof-segment signal, i.e., words omitted during evaluation by most latency metrics. substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).4 Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics problematic handling of tail words, often misrepresents SimulST system behavior. This raises serious concerns about its reliability and underscores the need for long-form evaluation, which we analyze in 5.2."
        },
        {
            "title": "5.2 Long-Form Evaluation",
            "content": "Which Resegmentation is Better? In Table 3, we evaluate two resegmentation tools: mWERSegmenter and our proposed SOFTSEGMENTER. The evaluation is done on reconcatenated short-form outputs, allowing us to compare with gold segment boundaries. As we can see in Table 3, the accuracy of latency evaluation is significantly higher with the proposed segmenter. When filtering out comparable systems by the p-value, accuracy further decreases with mWERSegmenter, suggesting that the segmentation is not stable. Both segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment. 3All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon. 4Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration. Latency (StreamLAAL) MT Quality (COMET) p-value mWERSegmenter ours mWERSegmenter ours All <0.05 <0.001 0.001-0.05 86.4 86.3 86.1 92.9 94.0 94.3 94.3 94. 99.3 100.0 100.0 100.0 99.1 100.0 100.0 100.0 Table 3: Accuracy of latency and quality metrics after resegmentation. longform + unsegmented LAAL DAL ATD AP YAAL p-val all <0.05 <0.001 0.001-0.05 Stream LAAL AL 0.66 0.82 0.69 0.85 0.71 0.87 0.52 0.63 0.61 0.64 0.65 0.55 0.57 0.59 0.60 0.48 0.61 0.63 0.63 0. longform + resegmented p-val all <0.05 <0.001 0.001-0.05 Stream LAAL 0.82 0.85 0.87 0.63 Long AL 0.92 0.94 0.95 0. Long LAAL 0.95 0.96 0.97 0.90 Long DAL 0.94 0.97 0.98 0.85 Long ATD 0.93 0.97 0.99 0.82 0.39 0.36 0.34 0.47 Long AP 0.71 0.72 0.74 0.60 0.61 0.64 0.65 0. Long YAAL 0.95 0.98 0.99 0.87 594 523 461 62 594 523 461 62 Table 4: Accuracy of systems in the long-form regime. Best scores in bold. Underlined scores are considered tied with the best metric. All metrics in the bottom half use the proposed SOFTSEGMENTER, except for StreamLAAL that uses the original mWERSegmenter. Do we need Resegmentation? The upper part of Table 4 compares the accuracy of StreamLAAL that uses resegmentation and the original shortform metrics evaluated without resegmentation on long-form systems. We see that the accuracies are low, not exceeding 66% on all systems. Compared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points. The lower part of Table 4 reports the accuracy of latency metrics in long-form systems with resegmentation. We see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other Longmetrics) uses our SOFTSEGMENTER. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in Table 3. These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime. Additional observations are in Section D. Which is the best Long-form Latency Metric? Table 4 shows that the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not statistically significant. This contrasts with 5.1, where ATD and DAL perform worse than *AL metrics. We explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime. We attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter γ, and the difference in LongDAL is caused by the artificial minimum delay of 1/γ in Equation (4). LongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (p-val<0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see 5.1), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration (Polák and Bojar, 2024; Papi et al., 2024). Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metrics sensitivity to variable segment length. These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST."
        },
        {
            "title": "6 Conclusions",
            "content": "In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under shortand long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, new latency metric better aligned with the short-form evaluation regime. However, our analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as more reliable alternative. Moreover, we also demonstrated that resegmentation is necessary to conduct proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settingsLongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST."
        },
        {
            "title": "Acknowledgments",
            "content": "Sara and Luisa received funding by the European Unions Horizon research and innovation programme under grant agreement No 101135798, project Meetween (My Personal AI Mediator for Virtual MEETings BetWEEN People)."
        },
        {
            "title": "Limitations",
            "content": "While our study offers thorough evaluation of latency metrics for SimulST and introduces improved tools for both shortand long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed SOFTSEGMENTER improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings. Potential Risks Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is risk that over-reliance on specific metricseven improved ones like YAAL and LongYAALcould lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in realworld conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible. Computational Budget We did not train any models as part of this study. However, we used several evaluations that required computation. Most of the experiments were conducted on standard desktop computer equipped with an Intel i7 processor and 32GB of RAM. For forced alignments with neural models, machine translation alignment, and the COMET translation quality metric, we used GPU cluster. However, these evaluations can be done on desktop machine with slightly longer runtime. The proposed SOFTSEGMENTER, YAAL, and LongYAAL can be run efficiently on CPU. Use of AI Assistants We used AI-assisted coding (i.e, Copilot) with the bulk written by humans. For writing, we used AI to check grammar mistakes."
        },
        {
            "title": "References",
            "content": "Milind Agarwal, Sweta Agrawal, Antonios Anastasopoulos, Luisa Bentivogli, Ondˇrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qianqian Dong, Kevin Duh, Yannick Estève, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, Dávid Javorský, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Polák, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, and Rodolfo Zevallos. 2023. FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 161, Toronto, Canada (in-person and online). Association for Computational Linguistics. Ibrahim Said Ahmad, Antonios Anastasopoulos, Ondˇrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, William Chen, Qianqian Dong, Marcello Federico, Barry Haddow, Dávid Javorský, Mateusz Krubinski, Tsz Kim Lam, Xutai Ma, Prashant Mathur, Evgeny Matusov, Chandresh Maurya, John McCrae, Kenton Murray, Satoshi Nakamura, Matteo Negri, Jan Niehues, Xing Niu, Atul Kr. Ojha, John Ortega, Sara Papi, Peter Polák, Adam Pospíšil, Pavel Pecina, Elizabeth Salesky, Nivedita Sethiya, Balaram Sarkar, Jiatong Shi, Claytone Sikasote, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Brian Thompson, Alex Waibel, Shinji Watanabe, Patrick Wilken, Petr Zemánek, and Rodolfo Zevallos. 2024. FINDINGS OF THE IWSLT 2024 EVALUATION CAMPAIGN. In Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024), pages 111, Bangkok, Thailand (in-person and online). Association for Computational Linguistics. Chantal Amrhein and Barry Haddow. 2022. Dont discard fixed-window audio segmentation in speechIn Proceedings of the Seventh to-text translation. Conference on Machine Translation (WMT), pages 203219, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Antonios Anastasopoulos, Loïc Barrault, Luisa Bentivogli, Marcely Zanon Boito, Ondˇrej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Estève, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz, Barry Haddow, Benjamin Hsu, Dávid Javorský, Vera Kloudová, Surafel Lakew, Xutai Ma, Prashant Mathur, Paul McNamee, Kenton Murray, Maria Nˇadejde, Satoshi Nakamura, Matteo Negri, Jan Niehues, Xing Niu, John Ortega, Juan Pino, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Marco Turchi, Yogesh Virkar, Alexander Waibel, Changhan Wang, and Shinji Watanabe. 2022. Findings of the IWSLT 2022 evaluation campaign. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 98157, Dublin, Ireland (in-person and online). Association for Computational Linguistics. Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. 2023. Whisperx: Time-accurate speech transcription of long-form audio. In Interspeech 2023, pages 44894493. Roldano Cattoni, Mattia Antonino Di Gangi, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2021. Mustc: multilingual corpus for end-to-end speech translation. Computer speech & language, 66:101155. Colin Cherry and George Foster. 2019. Thinking slow about latency evaluation for simultaneous machine translation. arXiv preprint arXiv:1906.00048. Kyunghyun Cho and Masha Esipova. 2016. Can neural machine translation do simultaneous translation? arXiv preprint arXiv:1606.02012. Zi-Yi Dou and Graham Neubig. 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 21122128, Online. Association for Computational Linguistics. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pages 578628, Singapore. Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 4668, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Christian Huber, Tu Anh Dinh, Carlos Mullov, NgocQuan Pham, Thai Binh Nguyen, Fabian Retkowski, Stefan Constantin, Enes Ugan, Danni Liu, Zhaolin Li, Sai Koneru, Jan Niehues, and Alexander Waibel. 2023. End-to-end evaluation for low-latency simultaneous speech translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 1220, Singapore. Association for Computational Linguistics. Javier Iranzo-Sánchez, Jorge Civera Saiz, and Alfons Juan. 2021. Stream-level latency evaluation for simultaneous machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 664670, Punta Cana, Dominican Republic. Association for Computational Linguistics. Yasumasa Kano, Katsuhito Sudoh, and Satoshi Nakamura. 2023. Average token delay: latency metric for simultaneous translation. In INTERSPEECH 2023, pages 44694473. Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478494, Online. Association for Computational Linguistics. Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. 2019. STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 30253036, Florence, Italy. Association for Computational Linguistics. Xutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu, and Juan Pino. 2020. SIMULEVAL: An evaluation toolkit for simultaneous translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 144150, Online. Association for Computational Linguistics. Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49844997, Online. Association for Computational Linguistics. Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005a. Evaluating machine translation output with automatic sentence segmentation. In Evaluating multilingual speech translation under realistic conditions with resegmentation and terminology. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 6278, Toronto, Canada (in-person and online). Association for Computational Linguistics. Robert Tibshirani and Bradley Efron. 1993. An introduction to the bootstrap. Monographs on statistics and applied probability, 57(1):1436. Vilém Zouhar, Pinzhen Chen, Tsz Kin Lam, Nikita Moghe, and Barry Haddow. 2024. Pitfalls and outlooks in using COMET. In Proceedings of the Ninth Conference on Machine Translation, pages 1272 1288, Miami, Florida, USA. Association for Computational Linguistics. Proceedings of the Second International Workshop on Spoken Language Translation. Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005b. Evaluating machine translation output with automatic sentence segmentation. In Proceedings of the Second International Workshop on Spoken Language Translation, Pittsburgh, Pennsylvania, USA. Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. 2017. Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi. In Proc. Interspeech 2017, pages 498502. Sara Papi, Marco Gaido, Matteo Negri, and Luisa Bentivogli. 2024. StreamAtt: Direct Streaming Speechto-Text Translation with Attention-based Audio History Selection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand. Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi. 2022. Over-generation cannot be rewarded: Length-adaptive average lagging for simultaneous speech translation. In Proceedings of the Third Workshop on Automatic Simultaneous Translation, pages 1217, Online. Association for Computational Linguistics. Sara Papi, Peter Polak, Dominik Macháˇcek, and Ondˇrej Bojar. 2025. How real is your real-time simultaneous speech-to-text translation system? Transactions of the Association for Computational Linguistics, 13:281313. Peter Polák, Ngoc-Quan Pham, Tuan Nam Nguyen, Danni Liu, Carlos Mullov, Jan Niehues, Ondˇrej Bojar, and Alexander Waibel. 2022. CUNI-KIT system for simultaneous speech translation task at IWSLT 2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 277285, Dublin, Ireland (in-person and online). Association for Computational Linguistics. Peter Polák and Ondˇrej Bojar. 2024. Long-form end-toend speech translation via latent alignment segmentation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 10761082. Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2020. SimulSpeech: End-to-end simultaneous speech to text translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3787 3796, Online. Association for Computational Linguistics. Elizabeth Salesky, Kareem Darwish, Mohamed AlBadrashiny, Mona Diab, and Jan Niehues. 2023."
        },
        {
            "title": "A Evaluated Systems",
            "content": "Language Pair Dataset Teams Systems For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets (Anastasopoulos et al., 2022; Agarwal et al., 2023), and the logs of the tst-COMMON test set of the MuST-C data set (Cattoni et al., 2021) that were submitted to IWSLT 2022. For the longform regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset (Salesky et al., 2023), and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set (Ahmad et al., 2024) and the IWSLT 2025 test set. portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio. In Tables 5 to 7, we present the number of systems used in the shortand long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word. Language Pair Dataset Teams Systems ENDE ENJA ENZH IWSLT 22 test set IWSLT 23 test set tst-COMMON IWSLT 22 test set IWSLT 23 test set tst-COMMON IWSLT 22 test set IWSLT 23 test set tst-COMMON 5 5 7 3 4 3 3 3 3 68 5 75 9 4 14 14 3 Table 5: Overview of the short-form systems in our evaluation. ENDE ENJA ENZH IWSLT 22 test set IWSLT 23 test set tst-COMMON IWSLT 22 test set IWSLT 23 test set tst-COMMON IWSLT 22 test set IWSLT 23 test set tst-COMMON 4 4 6 3 4 3 3 3 3 40 4 47 7 4 14 3 14 Table 6: Overview of the short-form systems in our evaluation after filtering out systems with anomalous policy. Language Pair Dataset Teams Systems ENDE ENJA ENZH CSEN ACL 6060 dev set IWSLT 25 test set ACL 6060 dev set IWSLT 25 test set ACL 6060 dev set IWSLT 25 test set IWSLT 24 dev set IWSLT 25 test set 6 6 3 2 4 4 2 2 20 10 16 16 8 14 4 Table 7: Overview of the long-form systems in our evaluation."
        },
        {
            "title": "B True Latency",
            "content": "B."
        },
        {
            "title": "Implementation Details",
            "content": "Short-Form Regime To determine the true latency for each system, we follow the definition in 4. First, we tokenize the hypotheses, the reference transcript, and the reference translation using MosesTokenizer. For Chinese and Japanese, we split the text into characters. Second, we perform time alignment between the source speech and the golden source transcripts using Montreal Forced Aligner (McAuliffe et al., 2017). This gives us the precise start and end timestamps for every word in the source recording. Third, we use the awesome-align tool (Dou and Neubig, 2021) to map each hypothesis word with its most likely counterpart in the source transcript. Long-Form Regime Same as in the short-form evaluation, we follow the definition of the true latency in 4. However, there are two differences. After initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, Instead, we use the and non-native speech. alignment method implemented within WhisperX (Bain et al., 2023) for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperXs default settings, i.e., PyTorchs WAV2VEC2_ASR_BASE_960H for English comodoro/wav2vec2-xls-r-300m-cs-250 and for Czech speech forced alignments. Second, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the awesome-align tool uses the bert-base-multilingual-cased model for the alignment, and this model has maximum input length of 512 tokens, which is much lower than the system hypotheses. B.2 Why Not Use True Latency Directly? natural question arises: Why rely on automatic latency metrics at all, when true latency offers closer approximation of user experience? In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the caseparticularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in 5, several automatic metrics approximate true latency with high accuracy, making them practical and effective alternative in most evaluation scenarios. Short-Form Evaluation Additional Analysis In Figure 6, we illustrate the trends after filtering out the systems affected by the anomalous policy (see 5.1). Unlike in Figure 3, we see that all metrics and system pairs show positive correlation with the true latency. As mentioned in 5.1, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in 4.2. To this end, in Figures 7 and 8, we also offer the accuracy of subsets of system pairs based on the absolute difference in the true latency. p-val AL LAAL DAL ATD AP YAAL all <0.05 <0.01 <0.001 0.001-0.05 all <0.05 <0.01 <0.001 0.001-0.05 0.99 1.00 1.00 1.00 1.00 0.92 0.93 0.93 0.93 0.64 related systems 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 1.00 1.00 0.96 0.97 0.97 0.97 0.57 unrelated systems 0.95 0.96 0.96 0.96 0.68 0.92 0.94 0.94 0.94 0. 0.88 0.89 0.89 0.89 0.79 0.99 0.99 0.99 0.99 1.00 0.74 0.75 0.75 0.75 0.57 1.00 1.00 1.00 1.00 1.00 0.97 0.98 0.98 0.98 0.68 897 888 888 881 1203 1172 1158 1144 28 Table 8: Accuracy of systems in the short-form regime when comparing related and unrelated systems. Systems with the anomalous policy were omitted. Best scores in bold. Underlined scores are considered tied with the best metric. Comparing Related vs. Unrelated Systems We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.5 We also use only subset of the systems that were not affected by the anomalous simultaneous policy. The results are in Table 8. Surprisingly, when evaluating related systems, all metrics perform almost perfectly, reaching accuracy between 97% and 100%. In Figure 9, we report the accuracy of subsets based on the minimal difference in the true latency. Given difference of at least 250 ms, all metrics except AP achieve 100% accuracy, and AP achieves around 99% accuracy. The results on unrelated systems (bottom half of Table 8, and Figure 10) are generally similar to the observations in 5.1 and Table 1. All metrics show loss of accuracy of no more than 4% points compared to the results on all systems. The only exception seems to be AP, which loses up to 11% points. The order of the metrics remains the same. 5To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters. Figure 5: Direct comparison of all systems, i.e., not pairwise. Each point represents single system. In the upper left corner, we report the Pearson correlation coefficient ρ, and in the bottom right corner, we report the Kendall rank coefficient τ . The reported correlations are only for illustration, as different language pairs and test sets have different scales. Figure 6: Figure 3 excluding systems affected by the anomalous policy. Each point represents the difference between the true latency (x-axis) and the automatic metric (y-axis) for two systems. In the upper left corner, we report the Pearson correlation coefficient ρ, and in the bottom right corner, we report the Kendall rank coefficient τ . The reported correlations are only for illustration, as different language pairs and test sets have different scales. Figure 7: Metric accuracies based on the difference of two systems. Solid lines show the accuracy given the minimal difference in True Latency. The colored strips along the lines show the 95% confidence interval obtained with bootstrap resampling (N=10000). Figure 8: Figure 7 excluding systems affected by the anomalous policy. Metric accuracies based on the difference between two systems. Solid lines show the accuracy given the minimal difference in True Latency. The colored strips along the lines show the 95% confidence interval obtained with bootstrap resampling (N=10000). Long-Form Evaluation In Figure 11, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in Figure 12, we show the same systems evaluated in the long-form regime, but after resegmentation. In Figure 13, we report the accuracy of subsets based on the minimal difference in the true latency. Figure 9: Metric accuracies based on the difference of two related (coming from the same team) systems. Solid lines show the accuracy given the minimal difference in True Latency. The colored strips along the lines show the 95% confidence interval obtained with bootstrap resampling (N=10000). Figure 10: Metric accuracies based on the difference of two unrelated (each system is compared to system from different team) systems. Solid lines show the accuracy given the minimal difference in True Latency. The colored strips along the lines show the 95% confidence interval obtained with bootstrap resampling (N=10000). Figure 11: Automatic latency metrics when evaluating in the unsegmented regime without resegmentation. Figure 12: Automatic latency metrics when evaluating in the unsegmented regime without resegmentation. Figure 13: Metric accuracies based on the difference of two systems evaluated in the long-form regime. Solid lines show the accuracy given the minimal difference in True Latency. The colored strips along the lines show the 95% confidence interval obtained with bootstrap resampling (N=10000)."
        }
    ],
    "affiliations": [
        "Charles University, Czech Republic",
        "Fondazione Bruno Kessler, Italy"
    ]
}