{
    "paper_title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception",
    "authors": [
        "Jihao Zhao",
        "Zhiyuan Ji",
        "Yuchen Feng",
        "Pengnian Qi",
        "Simin Niu",
        "Bo Tang",
        "Feiyu Xiong",
        "Zhiyu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances performance and speed, and precisely identifies the boundaries of text chunks by analyzing the characteristics of context perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines PPL Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Furthermore, through the analysis of models of various scales and types, we observed that PPL Chunking exhibits notable flexibility and adaptability. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 2 8 8 7 2 1 . 0 1 4 2 : r Preprint. Under review. META-CHUNKING: LEARNING EFFICIENT TEXT SEGMENTATION VIA LOGICAL PERCEPTION Jihao Zhao1 Zhiyuan Ji1 Yuchen Feng2 Feiyu Xiong2 Zhiyu Li2 1Renmin University of China 2Institute for Advanced Algorithms Research, Shanghai Pengnian Qi2 Simin Niu1 Bo Tang"
        },
        {
            "title": "ABSTRACT",
            "content": "Retrieval-Augmented Generation (RAG), while serving as viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to granularity between sentences and paragraphs, consisting of collection of sentences within paragraph that have deep linguistic logical connections. To implement MetaChunking, we designed Perplexity (PPL) Chunking, which balances performance and speed, and precisely identifies the boundaries of text chunks by analyzing the characteristics of context perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose strategy that combines PPL Chunking with dynamic merging to achieve balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of singlehop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Furthermore, through the analysis of models of various scales and types, we observed that PPL Chunking exhibits notable flexibility and adaptability. Our code is available at https://github.com/IAAR-Shanghai/MetaChunking."
        },
        {
            "title": "INTRODUCTION",
            "content": "Retrieval-augmented generation (RAG), as cutting-edge technological paradigm, aims to address challenges faced by large language models (LLMs), such as data freshness (He et al., 2022), hallucinations (Benedict et al., 2023; Chen et al., 2023b; Zuccon et al., 2023; Liang et al., 2024), and the lack of domain-specific knowledge (Li et al., 2023; Shen et al., 2023). This is particularly relevant in knowledge-intensive tasks like open-domain question answering (Lazaridou et al., 2022). By integrating two key components: the retriever and the generator, this technology enables more precise responses to input queries (Singh et al., 2021; Lin et al., 2023). While the feasibility of the retrieval-augmentation strategy has been widely demonstrated through practice, its effectiveness heavily relies on the relevance and accuracy of the retrieved documents (Li et al., 2022; Tan et al., 2022). The introduction of excessive redundant or incomplete information through retrieval not only fails to enhance the performance of the generation model but may also lead to decline in answer quality (Shi et al., 2023; Yan et al., 2024). In response to the aforementioned challenges, current research efforts mainly focus on two aspects: improving retrieval accuracy (Zhuang et al., 2024; Sidiropoulos & Kanoulas, 2022; Guo et al., 2023) and enhancing the robustness of LLMs against toxic information (Longpre et al.; Kim et al., 2024). However, in RAG systems, commonly overlooked aspect is the chunked processing of textual content, which directly impacts the quality of dense retrieval (Xu et al., 2023). By delicately splitting long documents into multiple chunks, this module not only significantly improves the processing efficiency and performance of the system, reducing the consumption of computing resources, but also enhances the accuracy of retrieval (Besta et al., 2024). Meanwhile, the chunking strategy allows Corresponding author: lizy@iaar.ac.cn 1 Preprint. Under review. Figure 1: Overview of RAG pipeline, as well as examples based on rules, similarity, and PPL segmentation. The same background color represents being located in the same chunk. information to be more concentrated, minimizing the interference of irrelevant information, enabling LLMs to focus more on the specific content of each text chunk and generate more precise responses (Su et al., 2024). Traditional text chunking methods, often based on rules or semantic similarity (Zhang et al., 2021; Langchain, 2023; Lyu et al., 2024), provide some structural segmentation but are inadequate in capturing subtle changes in logical relationships between sentences. As illustrated in Figure 1, example sentences exhibit progressive relationship, yet their semantic similarity is low, which may result in their complete separation. The LumberChunker (Duarte et al., 2024) offers novel solution by utilizing LLMs to receive series of consecutive paragraphs and accurately identify where content begins to diverge. However, it demands high level of instruction-following ability from LLMs, necessitating the use of the Gemini model, which incurs significant resource and time costs. This raises practical question: How can we fully utilize the powerful reasoning capabilities of LLMs while efficiently accomplishing the text chunking task at lower cost? This paper introduces the concept of Meta-Chunking, which operates at granularity between sentences and paragraphs, aiming to enhance logical coherence in the process of text segmentation. Meta-Chunking consists of sets of sentences within paragraphs that share deep linguistic and logical connections. To address the limitations of traditional methods based on semantic similarity, we leverage the powerful comprehension and reasoning capabilities of LLMs to devise the MetaChunking strategy: Perplexity (PPL) Chunking. This method calculates the PPL of each sentence based on its context and identifies text chunk boundaries by analyzing the characteristics of PPL distribution. It effectively reduces the dependency of text chunking on model scale, enabling smaller language models with relatively weaker reasoning capabilities to adequately perform this task. Furthermore, PPL Chunking improves the efficiency of LLMs in handling chunking tasks, achieving both resource and time savings. This provides crucial support for LLMs to process text chunking in real-world scenarios. To comprehensively evaluate proposed methods, extensive experiments were conducted on eleven datasets across four benchmarks, involving both Chinese and English texts, ranging from brief to extensive documents, and measured through seven key metrics. In response to the inherent complexity of different datasets, we propose Meta-Chunking with dynamic combination strategy designed to achieve valid balance between fine-grained and coarse-grained text segmentation. Traditional chunking methods treat sentences as independent logical units, whereas we adopt meta-chunks as independent logical units. For instance, in the RAG system, if users opt for small model and set relatively low top value for recall, meta-chunks can be directly utilized. However, in cases 2 Preprint. Under review. where users employ LLMs with extended contexts and require larger text chunks, meta-chunks can initially be generated and subsequently merged based on the desired chunk size to achieve the final chunking outcome. Experimental results fully demonstrate that the Meta-Chunking strategy significantly improves performance compared to traditional rule-based and semantic chunking. More importantly, compared to the current LLMs approache, the method proposed in this paper exhibits superior performance in terms of efficiency and cost savings."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Text Segmentation It is fundamental task in NLP, aimed at breaking down text content into its constituent parts to lay the foundation for subsequent advanced tasks such as information retrieval (Li et al., 2020) and text summarization (Lukasik et al., 2020; Cho et al., 2022). By conducting topic modeling on documents, Kherwa & Bansal (2020) and Barde & Bainwad (2017) demonstrate the identification of primary and sub-topics within documents as significant basis for text segmentation. Numerous techniques exist for topic modeling, ranging from algorithms based on probabilistic methods, such as Latent Dirichlet Allocation (Blei et al., 2003) and Probabilistic Latent Semantic Analysis (Hofmann et al., 1999), to models that also consider semantic relationships between words and sentences, like Top2Vec (Angelov, 2020) and BERTopic (Grootendorst, 2022). Additionally, Zhang et al. (2021) frames text segmentation as sentence-level sequence labeling task, utilizing BERT to encode multiple sentences simultaneously. It calculates sentence vectors after modeling longer contextual dependencies and finally predicts whether to perform text segmentation after each sentence. Langchain (2023) provides flexible and powerful support for various text processing scenarios by integrating multiple text segmentation methods, including character segmentation, delimiter-based text segmentation, specific document segmentation, and recursive chunk segmentation. Although these methods better respect the structure of the document, they have limitations in deep contextual understanding. To address this issue, semantic-based segmentation (Kamradt, 2024) utilizes embeddings to aggregate semantically similar text chunks and identifies segmentation points by monitoring significant changes in embedding distances. Text Chunking in RAG LLMs have demonstrated remarkable capabilities in language-related tasks through their complex internal structures and reasoning mechanisms (Zheng et al., 2024). By expanding the input space of LLMs through introducing retrieved text chunks (Guu et al., 2020; Lewis et al., 2020), RAG significantly improves the performance of knowledge-intensive tasks (Ram et al., 2023). Text chunking plays crucial role in RAG, as ineffective chunking strategies can lead to incomplete contexts or excessive irrelevant information, thereby hurting the performance of QA systems (Yu et al., 2023). Besides typical granularity levels like sentences or paragraphs (Lyu et al., 2024; Gao et al., 2023), there are other advanced methods available. Chen et al. (2023a) introduced novel retrieval granularity called Proposition, which is the smallest text unit that conveys single fact. This method excels in fact-based texts like Wikipedia. However, it may not perform ideally when dealing with content that relies on flow and contextual continuity, such as narrative texts, leading to the loss of critical information. Meanwhile, LumberChunker (Duarte et al., 2024) iteratively harnesses LLMs to identify potential segmentation points within continuous sequence of textual content, showing some potential for LLMs chunking. However, this method demands profound capability of LLMs to follow instructions and entails substantial consumption when employing the Gemini model."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 META-CHUNKING Our main contribution is an innovative text segmentation technique named Meta-Chunking, which leverages the capabilities of LLMs to flexibly partition documents into logically coherent, independent chunks. Our approach is grounded in core principle: allowing variability in chunk size to more effectively capture and maintain the logical integrity of content. This dynamic adjustment of granularity ensures that each segmented chunk contains complete and independent expression of ideas, thereby avoiding breaks in the logical chain during the segmentation process. This not only enhances the relevance of document retrieval but also improves content clarity. 3 Preprint. Under review. As illustrated in Figure 2, our method integrates the advantages of traditional text segmentation strategies, such as adhering to preset chunk length constraints and ensuring sentence structural integrity, while enhancing the ability to guarantee logical coherence during the segmentation process. The key lies in introducing novel concept between sentence-level and paragraph-level text granularity: Meta-Chunking. meta chunk consists of collection of sequentially arranged sentences within paragraph, where the sentences not only share semantic relevance but, more importantly, contain deep linguistic logical connections, including but not limited to causal, transitional, parallel, and progressive relationships. These relationships go beyond mere semantic similarity. In order to achieve this goal, we have designed and implemented the following strategy. Perplexity Chunking: Given text, the initial step involves segmenting it into collection of sentences denoted as (x1, x2, . . . , xn), with the ultimate goal being to further partition these sentences into several chunks, forming new set (X1, X2, . . . , Xk), where each chunk comprises coherent grouping of the original sentences. We split the text into sentences and use the model to calculate the PPL of each sentence xi based on the preceding sentences: (cid:80)K PPLM (xi) = k=1PPLM (ti where represents the total number of tokens in xi, ti denotes the k-th token in xi, and t<i signifies all tokens that precede xi. To locate the key points of text segmentation, the algorithm further analyzes the distribution characteristics of PPLseq = (PPLM (x1), PPLM (x2), . . . , PPLM (xn)), particularly focusing on identifying minima: <k, t<i) kti (1) Minimaindex(PPLseq) = (cid:26) (cid:12) (cid:12) (cid:12) (cid:12) min(PPLM (xi1), PPLM (xi+1)) PPLM (xi) > θ, or PPLM (xi1) PPLM (xi) > θ and PPLM (xi+1) = PPLM (xi) (2) (cid:27) The meaning of the above formula include: when the PPL on both sides of point are higher than at that point, and the difference on at least one side exceeds the preset threshold θ; or when the difference between the left point and the point is greater than θ and the right point equals the point value. These minima are regarded as potential chunk boundaries. If the text exceeds the processing range of LLMs or device, we strategically introduce key-value (KV) caching mechanism. Specifically, the text is first divided into several parts according to tokens, forming multiple subsequences. As the PPL calculation progresses, when the GPU memory is about to exceed the server configuration or the maximum context length of LLMs, the algorithm appropriately removes KV pairs of previous partial text, thus not sacrificing too much contextual coherence. To address diverse chunking needs of users, merely adjusting the threshold to control chunk size sometimes leads to uneven chunking sizes as the threshold increases, as shown in Section 5.2.2 and 5.2.3. Therefore, we propose strategy combining Meta-Chunking with dynamic merging, aiming to flexibly respond to varied chunking requirements. Firstly, we set an initial threshold of 0 or specific value based on the PPL distribution and perform Meta-Chunking operations, preliminarily dividing the document into series of basic units (c1, c2, . . . , cα). Subsequently, according to the userspecified chunk length L, we iteratively merge adjacent meta-chunks until the total length satisfies or approximates the requirement. Specifically, if len(c1, c2, c3) = or len(c1, c2, c3) < while len(c1, c2, c3, c4) > L, then c1, c2, c3 are regarded as complete chunk. 3.2 THEORETICAL ANALYSIS OF PPL CHUNKING LLMs are designed to learn distribution that approximates the empirical distribution from sample texts. To quantify the closeness between these two distributions, cross-entropy is typically employed as metric. Under the discrete scenario, cross-entropy of relative to is formally defined as follows: H(P, Q) = Ep[logQ] = (x) log Q(x) = H(P ) + DKL(P Q) (3) where H(P ) represents the empirical entropy, and DKL(P Q) is the Kullback-Leibler (KL) divergence between and . The PPL of LLMs, mathematically speaking, is defined as: PPL(P, Q) = 2H(P,Q) (4) 4 (cid:88) Preprint. Under review. Figure 2: Overview of the entire process of Meta-Chunking. Each circle represents complete sentence, and the sentence lengths are not consistent. The vertical lines indicate where to segment. The two sides at the bottom of the figure reveal Margin Sampling Chunking and Perplexity Chunking. Circles with the same background color represent meta-chunk, which is dynamically combined to make the final chunk length meet user needs. It is essential to notice that, since H(p) is unoptimizable and bounded as shown in Appendix A.1, what truly impacts the discrepancy in PPL calculations across different LLMs is the KL divergence, which serves as metric to assess the difference between distributions. The greater the KL divergence is, the larger the disparity between two distributions signifies. Furthermore, high PPL indicates the cognitive hallucination of LLMs towards the real content, and such portions should not be segmented. On the other hand, Shannon (1951) approximates the entropy of any language through function GK = = (cid:88) Tk (cid:88) Tk (Tk) log2 (tkTk1) (Tk) log2 (Tk) + (cid:88) Tk1 (Tk1) log2 (Tk1) (5) where Tk represents consecutive tokens (t1, t2, . . . , tk) in text sequence, entropy can then be expressed as H(P ) = lim GK Then, based on the proof in Appendix A.1 that GK+1 GK for all 1, we can derive G1 G2 lim GK = H(P ) (6) (7) By combining formulas (3) and (7), we observe that for large-scale text processing tasks, increasing the context length tends to reduce the cross-entropy or PPL, phenomenon that reflects the ability of LLMs to make more effective logical inferences and semantic understandings after capturing broader contextual information. Consequently, during PPL Chunking experiments, we maximize the input of longer text sequences to LLMs, anticipating more substantial performance gains."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 DATASETS AND METRICS We conducted comprehensive evaluation on four benchmarks and comparison between MetaChunking and multiple baselines on series of question answering (QA) datasets, focusing on 5 Preprint. Under review. both Chinese and English languages, and covering multiple metrics such as the correctness of answers, factuality, and recall of retrieved texts. The CRUD benchmark (Lyu et al., 2024) is Chinese dataset containing single-hop, two-hop, and three-hop questions, evaluated using metrics including BLEU series, ROUGE-L, and BERTScore. We utilize the CUAD dataset from RAGBench benchmark (Friel et al., 2024), employing the same evaluation metrics as the CRUD. The MultiHop-RAG benchmark (Tang & Yang) assesses recall rates, with metrics such as Hits@ series, MAP@10, and MRR@10. LongBench benchmark (Bai et al., 2023) comprises various datasets, among which we exploit eight Chinese and English datasets covering single and multi-hop QA, evaluated based on F1 and ROUGE-L metrics."
        },
        {
            "title": "4.2 BASELINES",
            "content": "We primarily compared Meta-Chunking with two types of methods, namely rule-based chunking and dynamic chunking, noting that the latter incorporates both semantic similarity models and LLMs. The original rule-based method simply divides long texts into fixed-length chunks, disregarding sentence boundaries. However, the Llama index method (Langchain, 2023) offers more nuanced approach, balancing the maintenance of sentence boundaries while ensuring that token counts in each segment are close to preset threshold. On the other hand, similarity chunking (Xiao et al., 2023) utilizes sentence embedding models to segment text based on semantic similarity, effectively grouping highly related sentences together. Dense Retrieval (Chen et al., 2023a) introduces new retrieval granularity called propositions, which condenses and segments text by training an information extraction model. Alternatively, LumberChunker (Duarte et al., 2024) employs LLMs to predict optimal segmentation points within the text. These methods exhibit unique strengths in adapting to the context and structure of texts. It is noteworthy that LumberChunker encounters difficulties when applied to smaller models, thus impeding the comparison among different methods within the same model. To address this limitation, we introduced Margin Sampling (MSP) strategy to optimize the method, enhancing its adaptability to smaller models. This optimization enables more effective comparison of the performance and time consumption of various chunking methods. Margin Sampling Chunking: We split the text into collection of sentences denoted as (x1, x2, . . . , xn), and the method can be formulated as: MarginM (xi) = PM (cid:16) = k1Prompt(xi, (cid:17) ) PM (cid:16) = k2Prompt(xi, (cid:17) ) (8) ) represents forming an instruction between xi {xl}n where (k1, k2) indicates binary decision between yes or no for segmentation judgment. Prompt(xi, , regarding whether they should be merged, where encompasses either single sentence or multiple sentences. Through the probability PM obtained by model , we can derive the probability difference MarginM (xi) between the two options. Subsequently, by contrasting MarginM (xi) with the threshold θ, conclusion can be drawn regarding whether the two sentences should be segmented. For the setting of θ, we initially assign it value of 0 and then adjust it by recording historical MarginM (xi) and calculating their average. l=1 and 4.3 EXPERIMENTAL SETTINGS We primarily use Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B and Baichuan2-7B for Meta-Chunking (Yang et al., 2024; 2023). Without additional annotations, all language models used in this paper adopt chat or instruction versions. When chunking, the default parameter configurations of the models are adopted. For evaluation, Qwen2-7B is employed with the following settings: top = 0.9, top = 5, temperature = 0.1, and max new tokens = 1280. When conducting QA, the system necessitates dense retrievals from the vector database, with top set to 8 for CRUD and RAGBench, 10 for MultiHop-RAG, and 5 for LongBench. Text segmentation in the dataset is performed using NVIDIA H800, and evaluation is conducted using NVIDIA GeForce RTX 3090. To control variables, we maintain consistent chunk lengths for various chunking methods across each dataset. Detailed experimental setup information can be found in Appendix A.2. Preprint. Under review. Table 1: Main experimental results are presented in five QA datasets. The first four datasets are sourced from LongBench. Besides Dense Retrieval, we maintain consistent chunk length for various chunking methods in each dataset. Dataset 2WikiMultihopQA Qasper MultiFieldQA-en MultiFieldQA-zh MultiHop-RAG Chunking Method F1 Time F1 Time F1 Time F1 Time Hits@10 Hits@4 MAP@10 MRR@10 Baselines with rule-based or similarity-based chunking Original Llama index 11.89 11.74 0.21 8. Similarity Chunking 12.00 416.45 Dense Retrieval 5.49 57633. 9.45 10.15 9.93 8.23 0.13 5. 307.05 29.89 28.30 29.19 0.16 6. 318.41 22.45 21.85 22.39 0.06 5. 134.80 39762.54 29.72 41789.49 - - 0.6027 0.7366 0.7232 - 0.4523 0. 0.5362 - 0.1512 0.1889 0.1841 - 0.3507 0.4068 0.3934 - Chunking based on Qwen2-0.5B MSP Chunking PPL Chunking 11.74 13.56 788.30 140.54 9. 9.62 599.97 65.45 31.28 31.02 648. 79.72 23.35 23.52 480.35 64.02 0. 0.7215 0.5246 0.5583 0.1830 0.1925 0. 0.4186 MSP Chunking PPL Chunking 11.30 13.32 2189. 190.93 9.49 9.82 1487.27 32.81 1614. 122.44 31.30 136.96 22.08 22.57 1881. 107.94 0.7109 0.7366 0.5517 0.5570 0. 0.1979 0.4252 0.4300 Chunking based on Qwen2-1.5B"
        },
        {
            "title": "5 RESULTS AND ANALYSIS",
            "content": "5.1 MAIN RESULTS Comparison against Baselines. We systematically evaluated the performance of five baseline methods, as shown in Table 1 (top) and Table 2 (top). Notably, LumberChunker with Qwen2-7B achieved score of 10.65 and chunking time of 2883.43 seconds on the Qasper dataset but failed to work effectively on the other four datasets. This indicates significant limitations of this strategy in adapting to models with 7B parameters and below. Dense Retrieval condenses and segments text by training an information extraction model, which does not allow for specifying the chunk length. Aside from this method, we maintain consistent chunk length for various other chunking approaches across each dataset, which are enumerated individually in Appendix A.2. As shown in Table 1 (bottom), PPL Chunking provides notable improvements in the performance of QA systems and information retrieval when utilizing models with 0.5B and 1.5B parameter scales. Specifically, both model configurations show measurable improvements in accuracy and recall metrics compared to baseline tasks. Furthermore, they exhibit significant enhancements in processing speed when compared to dynamic chunking, thereby facilitating easier implementation of LLMs chunking in real-world scenarios. Table 2: Main experimental results of LLMs chunking using Qwen2-7B. Consistent chunk lengths were maintained for various chunking methods in each dataset. base represents the basic model, while inst. denotes the model fine-tuned with instructions. Dataset Efficiency and Accuracy Tradeoff. Margin Sampling Chunking addresses the current issue where LLMs chunking cannot be applied to models with weak instructionfollowing capabilities, and it demonstrates superior performance compared to LumberChunker, as illustrated in Table 2. However, this method exhibits chunking times similar to the LumberChunker algorithm, both reaching threshold ranges that are challenging for practical applications, highlighting inefficiencies of LLMs in handling chunking tasks. In contrast, PPL Chunking demonstrates significant advantages, not only excelling in maintaining or approaching the performance level provided by Margin Sampling Chunking, but also achieving substantial leap in processing efficiency compared to dynamic chunking strategies. Upon deeper examination between the base model and the instruction model, we found that PPL Chunking exhibits remarkable flexibility and MSP Chunkinginst. PPL Chunkingbase PPL Chunkinginst. LumberChunkerinst. Similarity Chunking 2WikiMultihopQA Chunking Method MultiFieldQA-en 2883. 8781.82 5755.79 6287.31 Qasper 416.45 530. 736.69 486.48 523.74 307.05 318.41 745. 493.43 10.11 12.00 13.41 32.35 29. 10.65 12.94 11.37 33.56 14.15 30. Time Time Time 9.93 9.39 F1 F1 - - - - 7 Preprint. Under review. adaptability, indicating that it does not have stringent requirement for the capacity of model to follow specific instructions. How Weak Can the Weaker LLM Be? As fundamental task, text chunking consumes large number of tokens when using LLMs like GPT-4 or Gemini, often leading to significant imbalance between resource utilization and task benefits. Therefore, using lightweight model is practical choice. Since our method is applicable to both large and small models, in addition to testing 1.5B and 7B models, we explored smaller models below 1B parameters. As the model size decreases, the execution time of the text chunking task significantly reduces, reflecting the advantage of small models in improving processing efficiency. Furthermore, our approaches do not suffer from significant performance degradation as the model size decreases, and it outperforms baselines on most datasets, which further demonstrates the superiority of our methods."
        },
        {
            "title": "5.2 ANALYSIS",
            "content": "5.2."
        },
        {
            "title": "IMPACT OF OVERLAPPING CHUNKING STRATEGIES",
            "content": "As we delve deeper into the influence of text chunking strategies on the performance of complex QA tasks, we further investigated the performance of various chunking strategies when overlapping chunks were employed. The original chunking overlap method uses fixed number of characters from the end of one chunk to overlap with the start of the next. The Llama index overlap approach builds upon this by additionally considering sentence integrity. The PPL Chunking overlap strategy, on the other hand, dynamically assigns sentences represented by minimal points of PPL to both the preceding and subsequent chunks, resulting in dynamic overlap. These approaches generally produce overlap lengths averaging around 50 Chinese characters. Table 3: Performance of different methods on CRUD QA datasets with overlapping chunks. ppl represents direct PPL Chunking, with threshold of 0.5. Precise chunk length and overlap length results are included in Appendix A.3. Chunking Method Overlap BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-Avg ROUGE-L BERTScore Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Fixed Dynamic Dynamic Dynamic Dynamic Fixed Dynamic Dynamic Dynamic Dynamic Fixed Dynamic Dynamic Dynamic Dynamic 0.3330 0.3326 0.3592 0.3582 0.3656 0. 0.2223 0.2295 0.2312 0.2336 0.2384 0. 0.2453 0.2447 0.2463 Single-hop Query 0.2641 0. 0.2888 0.2898 0.2952 0.2214 0.2214 0. 0.2450 0.2497 Two-hop Query 0.1300 0.1282 0. 0.1353 0.1350 0.0909 0.0896 0.0934 0. 0.0940 Three-hop Query 0.1268 0.1250 0.1319 0. 0.1324 0.0832 0.0825 0.0881 0.0891 0. 0.1881 0.1890 0.2081 0.2097 0.2143 0. 0.0677 0.0709 0.0719 0.0710 0.0602 0. 0.0643 0.0651 0.0651 0.2410 0.2413 0. 0.2657 0.2705 0.1114 0.1099 0.1143 0. 0.1154 0.1066 0.1049 0.1114 0.1122 0. 0.4060 0.4039 0.4332 0.4308 0.4393 0. 0.2555 0.2609 0.2638 0.2650 0.2546 0. 0.2599 0.2618 0.2596 0.8425 0.8439 0. 0.8548 0.8549 0.8747 0.8732 0.8700 0. 0.8754 0.8823 0.8796 0.8808 0.8817 0. As demonstrated in Table 3, PPL Chunking overlap strategy shows particularly notable performance in multi-hop QA scenarios. Specifically, except for the BERTScore metric, PPL Chunking overlap method achieves performance gain of 2%3% on the single-hop task. In the case of two-hop and three-hop tasks, although the rate of improvement slows slightly, consistent gain of 0.3%1% is maintained. Additionally, the performance across all three models exhibits an upward trend with the size of model parameters. Although the 1.5B model lags slightly behind the 7B model in terms of overall performance, it still demonstrates notable improvement over traditional chunking methods, further validating the effectiveness of PPL Chunking. 8 Preprint. Under review."
        },
        {
            "title": "5.2.2 COMPARATIVE ANALYSIS OF TWO PPL CHUNKING STRATEGIES",
            "content": "As shown in Figure 3, we compared two PPL Chunking strategies: direct PPL Chunking and PPL Chunking with dynamic combination, both of which are effective across the CRUD dataset. Through experimental analysis, we found that the latter demonstrates superior performance. This is primarily due to direct PPL Chunking, which may result in overly long chunks, whereas the PPL Chunking with dynamic combination method effectively maintains chunk length and logical consistency. In addition, PPL Chunking achieved significant performance improvements compared to traditional segmentation methods on BLEU series metrics and ROUGE-L. This indicates that our methods enhance the accuracy and fluency of the generated text to the reference text. Furthermore, this experiment reveals the delicate balance between model size and performance. Specifically, the performance of Qwen2-1.5B and Baichuan2-7B under this evaluation framework is closely matched, often surpassing the Qwen2-7B model across multiple metrics. Figure 3: Performance of different methods on single-hop query in the CRUD QA dataset. ppl represents direct PPL Chunking, with threshold of 0.5. comb. indicates PPL Chunking with dynamic combination, with threshold of 0 when performing PPL Chunking. Precise chunk length results and performance of remaining multi-hop scenarios are included in Appendix A.3. 5.2.3 LONG TEXT CHUNKING AND STRATEGY SELECTION When dealing with longer texts, we adopt the KV caching to calculate the PPL values of sentences under the premise of maintaining coherence, thereby optimizing the utilization of GPU memory and computational accuracy. Utilizing the CUAD dataset (average length 11k), we tested three models shown in Figure 4, which achieved appreciable improvements in BLEU-series metrics. Furthermore, it is noteworthy that both Qwen2-1.5B and Baichuan2-7B demonstrate comparable performance, which further confirms that the 1.5B model can maintain impressive balance between performance and efficiency when dealing with text chunking of varying lengths. Figure 4: Performance of different methods on CUAD QA datasets. ppl indicates direct PPL Chunking, with threshold of 0. On the other hand, we conducted an in-depth exploration of chunking in four long-text QA datasets of LongBench, and carried out gradient experiments (0 to 0.4, step 0.1) on the threshold of PPL Chunking, aiming to reveal the intrinsic relationship between PPL distribution and chunking effectiveness. The analysis of overall PPL distribution of datasets can be found in Appendix A.4. As shown in Figure 5, when chunk length is small, the direct PPL Chunking brings greater benefits, whereas when the chunk length is longer, PPL Chunking with dynamic combination performs better. 9 Preprint. Under review. Figure 5: Performance of different methods in four long-text QA datasets of LongBench is evaluated based on F1, F1, F1, and ROUGE-L. ppl represents direct PPL Chunking, and comb. indicates PPL Chunking with dynamic combination. Multi represents threshold values of the parallel method in four datasets, which are 0.5, 0.5, 1.34, and 0.5 respectively, resulting in chunk lengths of 87, 90, 71, and 262 in sequence. In addition, experimental results indicate that the optimal configuration of PPL Chunking relies on the PPL distribution of texts: when the PPL distribution is relatively stable, it is more appropriate to select lower threshold (such as setting the threshold to 0 in HotpotQA, MuSiQue, and DuReader); whereas when the PPL distribution exhibits large fluctuations, choosing higher threshold (such as setting the threshold to 0.4 in NarrativeQA) can effectively distinguish paragraphs with different information densities, improving the chunking effect. Therefore, when employing PPL for chunking, it is crucial to comprehensively consider the dual factors of chunk length and text PPL distribution to determine the relatively optimal configuration that maximizes performance. 5.2.4 EXPLORATION OF CHUNKING APPROACH FOR PERFORMANCE OF RE-RANKING To explore the impact of chunking strategies on the RAG system, we evaluated the combination of different chunking and re-ranking methods. Initially, top-10 set of relevant texts was filtered using dense retriever. We then compared two re-ranking strategies: (1) the BgeRerank method, leveraging the bge-reranker-large model (Xiao et al., 2023), and (2) the PPLRerank method with the Qwen2-1.5B model, utilizing the re-ranking method mentioned in the coarse-grained compression section in Jiang et al. (2023). Figure 6: Performance of re-ranking strategies combined with different chunking methods in the MultiHop-RAG benchmark. ppl represents direct PPL Chunking, with threshold of 0.5. The base reveals not utilizing re-ranking strategy. Precise chunk length results are included in Appendix A.5. Experimental results (see Figure 6) revealed that PPL Chunking and PPLRerank achieved the best overall performance across all metrics. Further analysis demonstrated that, compared to traditional chunking, PPL Chunking not only provided performance gains independently but also significantly enhanced the effectiveness of the subsequent re-ranking. Notably, while traditional chunking and re-ranking strategies already deliver performance improvements, PPL Chunking resulted in even greater re-ranking gains. For instance, in the Hits@8 metric, PPLRerank under the original chunking yielded 1.42% improvement, whereas PPLRerank under PPL Chunking achieved 3.59% improvement."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper proposes the concept of Meta-Chunking along with its implementation strategy, namely PPL Chunking, which enable more precise capture of the inherent logical structure of text, thereby providing powerful tool for optimizing text segmentation within the RAG pipeline. To balance the effectiveness of fine-grained and coarse-grained text segmentation, we present dynamic 10 Preprint. Under review. combination approach with Meta-Chunking to address the limitation when dealing with diverse texts. Our comprehensive evaluation using multiple metrics on eleven datasets demonstrates that Meta-Chunking significantly outperforms both rule-based and similarity-based chunking, while also achieving better balance between performance, time cost, and computational cost compared to current LLMs approaches."
        },
        {
            "title": "REFERENCES",
            "content": "Dimo Angelov. Top2vec: Distributed representations of topics. arXiv preprint arXiv:2008.09470, 2020. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Bhagyashree Vyankatrao Barde and Anant Madhavrao Bainwad. An overview of topic modeling methods and tools. In 2017 International Conference on Intelligent Computing and Control Systems (ICICCS), pp. 745750. IEEE, 2017. Garbiel Benedict, Ruqing Zhang, and Donald Metzler. Gen-ir@ sigir 2023: The first workshop on generative information retrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 34603463, 2023. Maciej Besta, Ales Kubicek, Roman Niggli, Robert Gerstenberger, Lucas Weitzendorf, Mingyuan Chi, Patrick Iff, Joanna Gajda, Piotr Nyczyk, Jurgen Muller, et al. Multi-head rag: Solving multiaspect problems with llms. arXiv preprint arXiv:2406.05085, 2024. David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):9931022, 2003. Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Dong Yu, and Hongming Zhang. Dense retrieval: What retrieval granularity should we use? arXiv preprint arXiv:2312.06648, 2023a. Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. Hallucination detection: Robustly discerning reliable answers in large language models. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 245255, 2023b. Sangwoo Cho, Kaiqiang Song, Xiaoyang Wang, Fei Liu, and Dong Yu. Toward unifying text segmentation and long document summarization. arXiv preprint arXiv:2210.16422, 2022. SS Dragomir and CJ Goh. Some bounds on entropy measures in information theory. Applied Mathematics Letters, 10(3):2328, 1997. Andre Duarte, Joao Marques, Miguel Graca, Miguel Freire, Lei Li, and Arlindo Oliveira. Lumberchunker: Long-form narrative document segmentation. arXiv preprint arXiv:2406.17526, 2024. Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrievalaugmented generation systems. arXiv preprint arXiv:2407.11005, 2024. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. Maarten Grootendorst. Bertopic: Neural topic modeling with class-based tf-idf procedure. arXiv preprint arXiv:2203.05794, 2022. Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, and Yang Liu. Prompt-guided retrieval augmentation for non-knowledge-intensive tasks. arXiv preprint arXiv:2305.17653, 2023. 11 Preprint. Under review. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 39293938. PMLR, 2020. Hangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303, 2022. Thomas Hofmann et al. Probabilistic latent semantic analysis. In UAI, volume 99, pp. 289296, 1999. Chip Huyen. Evaluation metrics for language modeling. The Gradient, 40, 2019. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839, 2023. Greg Kamradt. Semantic chunking. https://github.com/FullStackRetrieval-com/RetrievalTutorials, 2024. Kherwa and Bansal. Topic modeling: comprehensive review. eai endorsed transactions on scalable information systems, 7 (24), 116, 2020. Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, and Taeuk Kim. Adaptive contrastive decoding in retrievalaugmented generation for handling noisy contexts. arXiv preprint arXiv:2408.01084, 2024. Langchain. https://github.com/langchain-ai/langchain, 2023. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. survey on retrieval-augmented text generation. arXiv preprint arXiv:2202.01110, 2022. Jing Li, Billy Chiu, Shuo Shang, and Ling Shao. Neural text segmentation and its application to IEEE Transactions on Knowledge and Data Engineering, 34(2):828842, sentiment analysis. 2020. Xianzhi Li, Samuel Chan, Xiaodan Zhu, Yulong Pei, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? study on several typical tasks. arXiv preprint arXiv:2305.05862, 2023. Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, and Zhiyu Li. Internal consistency and self-feedback in large language models: survey. arXiv preprint arXiv:2407.14507, 2024. Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adri`a de Gispert, and Gonzalo Iglesias. Li-rage: Late interaction retrieval augmented generation with explicit signals for open-domain table question In Proceedings of the 61st Annual Meeting of the Association for Computational answering. Linguistics (Volume 2: Short Papers), pp. 15571566, 2023. Longpre, Yauney, Reif, Lee, Roberts, Zoph, Zhou, Wei, Robinson, Mimno, et al. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity, may 2023. URL http://arxiv. org/abs/2305.13169. Michal Lukasik, Boris Dadachev, Goncalo Simoes, and Kishore Papineni. Text segmentation by cross segment attention. arXiv preprint arXiv:2004.14535, 2020. 12 Preprint. Under review. Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. Crud-rag: comprehensive chinese benchmark for retrievalaugmented generation of large language models. arXiv preprint arXiv:2401.17043, 2024. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:13161331, 2023. Claude Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1): 5064, 1951. Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. In chatgpt we trust? measuring and characterizing the reliability of chatgpt. arXiv preprint arXiv:2304.08979, 2023. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 3121031227. PMLR, 2023. Georgios Sidiropoulos and Evangelos Kanoulas. Analysing the robustness of dual encoders for dense retrieval against misspellings. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 21322136, 2022. Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems, 34:2596825981, 2021. Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models. arXiv preprint arXiv:2403.10081, 2024. Chao-Hong Tan, Jia-Chen Gu, Chongyang Tao, Zhen-Hua Ling, Can Xu, Huang Hu, Xiubo Geng, and Daxin Jiang. Tegtok: Augmenting text generation via task-specific and open-world knowledge. arXiv preprint arXiv:2203.08517, 2022. Tang and Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries (2024). arXiv preprint arXiv:2401.15391. Shitao Xiao, Zheng Liu, Peitian Zhang, and Muennighof. C-pack: packaged resources to advance general chinese embedding. 2023. arXiv preprint arXiv:2309.07597, 2023. Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. Berm: Training the balanced and extractable representation for matching to improve generalization ability of dense retrieval. arXiv preprint arXiv:2305.11052, 2023. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884, 2024. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210, 2023. Qinglin Zhang, Qian Chen, Yali Li, Jiaqing Liu, and Wen Wang. Sequence model with self-adaptive In 2021 IEEE Automatic Speech sliding window for efficient spoken document segmentation. Recognition and Understanding Workshop (ASRU), pp. 411418. IEEE, 2021. 13 Preprint. Under review. Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Bo Tang, Feiyu Xiong, and Zhiyu Li. Attention heads of large language models: survey. arXiv preprint arXiv:2409.03752, 2024. Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Efficientrag: Efficient retriever for multi-hop question answering. arXiv preprint arXiv:2408.04259, 2024. Guido Zuccon, Bevan Koopman, and Razia Shaik. Chatgpt hallucinates when attributing answers. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region, pp. 4651, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THEORETICAL PROOF FOR PPL CHUNKING Firstly, we illustrate the relationship between cross-entropy and two distributions and in another way. Based on sequencing inequality (cid:88) i=1 aibi (cid:88) i=1 aibj(i) (cid:88) i=1 aibn+1i where a1 a2 an, b1 b2 bn and (j(1), j(2), . . . , j(n)) is an arbitrary sorting of (1, 2, . . . , n), it can be observed that the sum of products of larger numbers paired together is the maximum, while the sum of products of larger numbers paired with smaller numbers is the minimum. We desire the cross-entropy H(P, Q) to be as small as possible, which means that when (x) is relatively large, log Q(x) should be relatively small, thereby resulting in Q(x) also being relatively large. Therefore, smaller cross-entropy indicates that the prediction is closer to the actual label. Afterwards, inspired by insights provided in Huyen (2019), property of formula (7) is proved: GK+1 GK for all 1. Proof. GK GK+1 (Tk) loga (tkTk1) + (cid:88) Tk+ (Tk+1) loga (tk+1Tk) (cid:88) = Tk (cid:88) = (cid:88) Tk1 tk,tk+1 (cid:88) (cid:88) Tk1 tk,tk+1 (cid:88) = (cid:88) Tk1 tk,tk+ (cid:88) = (cid:88) Tk tk+1 (cid:88) = (cid:88) Tk1 tk+1 =0 (Tk+1) loga (tk+1Tk) (Tk) loga (tkTk1) (cid:88) tk (Tk+1) loga (tk+1Tk1) (Tk) loga (tkTk1) (cid:88) tk (Tk1, tk, tk+1) loga (tk+1Tk1) (cid:88) tk (Tk1, tk) loga (tkTk1) loga (tk+1Tk1) (cid:88) tk (Tk1, tk, tk+1) (cid:88) tk (Tk1, tk) loga (tkTk1) (Tk1, tk+1) loga (tk+1Tk1) (cid:88) tk (Tk1, tk) loga (tkTk1) The reason for the last equality is that tk+1 and tk belong to the same domain. Thus, the proof is complete. 14 Preprint. Under review. Eventually, we illustrate bounds of entropy, so as to demonstrate the positive correlation between H(P, Q) and DKL(P Q) in formula (3). Proof. Let be discrete random variable with finite range of values denoted by := {w1, w2, . . . , wl}. Set pi = {P = wi} for = 1, 2, . . . , l, and assume that pi > 0 for all {1, 2, . . . , l}. According to Lemma 2 in Dragomir & Goh (1997), if then γ := max i,j θi θj φ(ε) := 1 + ε ln + (cid:112)ε ln c(ε ln + 2) 0 logc (cid:33) pkθk (cid:32) (cid:88) k=1 (cid:88) k=1 pk logc θk ε where θk (0, +), pk 0 with (cid:80)l tioned inequality can be transformed into k=1pk = 1 and > 1. Given that θk = 1/pk, the aforemen0 logc Hc(P ) ε where ε > 0 satisfies the following conditions pi pj max i,j φ(ε) Furthermore, we can derive bounds for entropy as logc ε Hc(P ) logc l. The proof is concluded. A.2 MAIN EXPERIMENTAL DETAILS All language models utilized in this paper employ the chat or instruct versions where multiple versions exist, and are loaded in full precision (Float32). The vector database is constructed using Milvus, where the embedding model for English texts is bge-large-en-v1.5, and bge-base-zh-v1.5 for Chinese texts. When conducting QA, the system necessitates dense retrievals from the vector database, with top set to 8 for CRUD and RAGBench, 10 for MultiHop-RAG, and 5 for LongBench. In experiments, we utilized total of four benchmarks, and their specific configurations are detailed as follows: (a) Rule-based Chunking Methods Original: This method divides long texts into segments of fixed length, such as two hundred Chinese characters or words, without considering sentence boundaries. Llama index (Langchain, 2023): This method considers both sentence completeness and token counts during segmentation. It prioritizes maintaining sentence boundaries while ensuring that the number of tokens in each chunk are close to preset threshold. We use the SimpleNodeParser function from Llama index, adjusting the chunk size parameter to control segment length. Overlaps are handled by dynamically overlapping segments using the chunk overlap parameter, ensuring sentence completeness during segmentation and overlapping. (b) Dynamic Chunking Methods Similarity Chunking (Xiao et al., 2023): Utilizes pre-trained sentence embedding models to calculate the cosine similarity between sentences. By setting similarity threshold, sentences with lower similarity are selected as segmentation points, ensuring that sentences within each chunk are highly semantically related. This method employs the SemanticSplitterNodeParser from Llama index. For English texts, we exploit the bge-large-en-v1.5 model, and for Chinese texts, the bge-base-zh-v1.5 model. The size of the text chunks is controlled by adjusting the similarity threshold. LumberChunker (Duarte et al., 2024): Leverages the reasoning capabilities of LLMs to predict suitable segmentation points within the text. We utilize Qwen2 models with 1.5B and 7B parameters, set to full precision. 15 Preprint. Under review. Dense Retrieval (Chen et al., 2023a): Introduces new retrieval granularity called propositions, which condenses and segments text by training an information extraction model. In order to control variables during the experiment, we ensured that each dataset had approximately the same size when divided into chunks using different methods. The specific chunk lengths and corresponding thresholds for each dataset in the main experiment are shown in Table 4. We first explored direct chunking of Qwen2-72B, using the prompt displayed in Table 5, and found that it took too long. We then exploited this as comparison to explore other methods. Table 4: Chunk length and corresponding threshold settings for different methods. - indicates no relevant setting is involved. The first four datasets are sourced from LongBench. 0+comb. signifies that an initial chunking is performed using threshold of 0, followed by dynamic combination approach to derive the final chunks. In Llama index and Qwen2-72B, a(b) indicates that the chunk size of can be achieved by setting the chunking parameter to b. For other instances of a(b), it represents the dynamic combination of chunks where setting the combination length to results in final chunk size of a. Dataset 2WikiMultihopQA MultiFieldQA-en MultiFieldQA-zh MultiHop-RAG Qasper Chunking Method Length Threshold Length Threshold Length Threshold Length Threshold Length Threshold Original Llama index 122.61(215) - - 121 120.91(198) - - 113 112.59(208) - - 178 178.04(242) - - Similarity Chunking 125. 0.82 122.91 0.83 114.18 0.83 180. 0.73 78 79.68 80.13 - - 0.75 Baselines with rule-based or similarity-based chunking LLMs Direct Chunking Qwen2-72B 122.13(128) - 120.17(90) - 111.98(88) - 178.05(190) - - - Qwen2-0.5Bsent. Qwen2-0.5Bcomb. Qwen2-1.5Bchunk Qwen2-1.5Bcomb. Qwen2-7Bchunk Qwen2-7Bcomb. Qwen2-7B-basecomb. Chunking based on Qwen2-0.5B 122.33(148) 0+comb. 120.07(147) 0+comb. 112.46(136) 0+comb. 178.09(180) 0+comb. 78.04(91) 0+comb. 122.39(152) 0+comb. 120.04(155) 0+comb. 112.30(139) 0+comb. 178.36(160) 0+comb. 78.17(89) 0+comb. 121.99(148) 0+comb. 120.21(144) 0+comb. 111.52(134) 0+comb. 177.80(200) 0+comb. 78.16(97) 0+comb. 122.48(152) 0+comb. 120.56(156) 0+comb. 111.35(138) 0+comb. 178.00(159) 0+comb. 78.19(89) 0+comb. Chunking based on Qwen2-1.5B Chunking based on Qwen2-7B 121.81(138) 0+comb. 120.01(141) 0+comb. 111.56(129) 0+comb. 178.00(188) 0+comb. 77.49(95) 0+comb. 122.26(152) 0+comb. 120.26(155) 0+comb. 111.47(137) 0+comb. 177.80(156) 0+comb. 78.11(89) 0+comb. 122.34(152) 0+comb. 120.43(155) 0+comb. 112.76(139) 0+comb. - - - - Table 5: Prompt for direct chunking of Qwen2-72B. Chunking Prompt You are an expert in text segmentation, tasked with dividing given text into blocks. You must adhere to the following four conditions: 1. Aim to keep each block around 128 English words in length. 2. Segment the text based solely on its logical and semantic structures. 3. Do not alter the original vocabulary or structure of the text. 4. Do not add any new words or symbols. By solely determining the boundaries for text segmentation, divide the original text into blocks and output them individually, separated by clear delimiter Block Separator . Do not output any other explanations. If you understand, please proceed to segment the following text into blocks: [Text to be segmented] In the Margin Sampling Chunking method, we also use prompt, which mainly consists of two parts: instructions for guiding LLMs to perform chunking and two segmentation schemes. The specific form is shown in Table 6. 16 Preprint. Under review. Table 6: Prompt used in Margin Sampling Chunking. Chunking Prompt This is text chunking task. You are text analysis expert. Please choose one of the following two options based on the logical structure and semantic content of the provided sentence: 1. Split sentence1+sentence2 into sentence1 and sentence2 two parts; 2. Keep sentence1+sentence2 unsplit in its original form; Please answer 1 or 2. A.3 CHUNKING SITUATIONS OF THE CRUD DATASET A.3.1 FILTERING OF CORPORA RELATED TO QA TASKS IN THE CRUD DATASET In this experiment, we selected three QA datasets from the CRUD benchmark. Among them, the single-hop QA dataset consists of questions focused on extracting factual information from single document. These questions typically require precise retrieval of specific details such as dates, individuals, or events from the provided text. The two-hop QA dataset, on the other hand, evaluates integration capabilities and understanding of informational relationships between different documents. The more complex three-hop QA dataset often presents more intricate questions, demanding LLMs to process greater number of information sources to formulate complete and accurate response. Before the chunking phase, we collected original news articles used in all types of QA tasks in CRUD. Specifically, since CRUD provides evidence context snippets relied on by each QA pair, as well as the original news library where the context snippets are extracted, we can obtain the original news articles containing the context snippets through sentence matching. Taking the two-hop QA as an example, CRUD provides two news snippets, news1 and news2, which are necessary to answer questions. We then save the matched original news articles matched news1 and matched news2 that contain news1 and news2. Finally, from the original news library of 80,000 articles, we recall all 10,000 news articles containing context snippets as the initial text for chunking. A.3.2 EXPERIMENTAL SETUP FOR TWO RESEARCHES BASED ON THE CRUD DATASET We conducted two sets of experiments with overlapping and non-overlapping chunking on the CRUD dataset, respectively in Section 5.2.1 and 5.2.2. The chunk length and overlap length are shown in Table 7. Additionally, the specific values for the bar chart presented in Figure 3 are detailed in Table 8. Further analysis demonstrates that in single-hop and double-hop query scenarios presented in Table 8, PPL Chunking achieved significant performance improvements compared to traditional chunking methods on BLEU series metrics and ROUGE-L. This indicates that our methods enhance the accuracy and fluency of the generated text to the reference text. However, the relatively smaller margin of improvement observed on the BERTScore, BERT-based semantic similarity evaluation metric, may reflect lower sensitivity of deep semantic understanding to chunking, as well as the limitations of the current BERTScore models in capturing precise semantics. Finally, for three-hop query, although the performance of Qwen2-1.5B and Qwen2-7B using PPL Chunking was slightly lower than traditional methods, Baichuan2-7B performed comparably. However, when chunk overlap is introduced, the PPL Chunking method exhibits positive changes (as shown in Tables 3). This suggests that the effectiveness of segmentation strategies may be jointly influenced by query complexity and text characteristics. A.4 CHUNKING SITUATIONS OF LONG TEXT DATASETS We also conducted experiments on longer datasets. According to corresponding expressions in benchmarks, the average length of the CUAD dataset is 11k, and average lengths of four datasets in MultiHop-RAG are 9k, 11k, 18k, and 16k. The chunk lengths of these two sets of experiments are Preprint. Under review. Table 7: Settings of overlap length and chunk length for different chunking methods in the CRUD dataset. ppl represents direct PPL Chunking, with threshold of 0.5. comb. indicates PPL Chunking with dynamic combination, with threshold of 0 when performing PPL Chunking. Chunking Method Overlap Length Chunk Length Chunking with Overlap Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl 50 48.78 49.97 50.41 48. Chunking without Overlap Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Qwen2-1.5Bcomb. Qwen2-7Bcomb. Baichuan2-7Bcomb. 0 0 0 0 0 0 0 0 218 217.03 212.79 217.53 201.35 179 177.53 173.88 178.59 162.56 177.95 178.09 178.09 shown in Tables 9 and 10. Additionally, the specific values presented in Figures 4 and 5 correspond to Tables 11 and 12. According to Table 10, it can be observed that HotpotQA, MuSiQue, and DuReader achieve suitable chunk length with lower threshold, while NarrativeQA only reaches it when the threshold is set to 1.34. This indicates that PPL distribution of the first three datasets is relatively flat with small oscillations, whereas NarrativeQA exhibits significant fluctuations. Considering the chunking performance presented in Table 12, it suggests that direct PPL Chunking is more suitable when chunk length is small, while the combination of PPL Chunking and dynamic merging is preferable for larger chunk lengths. Furthermore, regarding the approach of PPL Chunking with dynamic combination, it is more appropriate to select smaller threshold when the PPL amplitude is small, and larger threshold when the PPL amplitude is significant. A.5 EXPERIMENTAL SETUPS FOR EXPLORING THE IMPACT OF CHUNKING ON RE-RANKING Tables 13 and 14 present chunk lengths that need to be set for Figure 6 and the specific values for drawing, respectively. Focusing on this batch of experiments, we first retrieve 10 relevant text chunks for each question through dense retriever, and then applied various re-ranking methods for secondary sorting to analyze changes in recall performance. 18 Preprint. Under review. Table 8: Performance of different methods on the CRUD QA dataset. ppl represents direct PPL Chunking, with threshold of 0.5. comb. indicates PPL Chunking with dynamic combination, with threshold of 0 when performing PPL Chunking. Chunking Method BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-Avg ROUGE-L BERTScore Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Qwen2-1.5Bcomb. Qwen2-7Bcomb. Baichuan2-7Bcomb. Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Qwen2-1.5Bcomb. Qwen2-7Bcomb. Baichuan2-7Bcomb. Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl Qwen2-1.5Bcomb. Qwen2-7Bcomb. Baichuan2-7Bcomb. 0.3515 0.3620 0.3714 0.3661 0.3725 0.3760 0.3724 0.3812 0.2322 0.2315 0.2328 0.2310 0.2350 0.2372 0.2364 0. 0.2494 0.2464 0.2402 0.2415 0.2460 0.2449 0.2408 0.2494 0.2788 0.2920 0.3013 0.2935 0.3011 0.3034 0.3012 0.3091 0.1324 0.1321 0.1326 0.1323 0.1341 0.1363 0.1360 0.1329 0.1317 0.1327 0.1260 0.1266 0.1293 0.1294 0.1274 0.1324 Single-hop Query 0.2340 0.2480 0.2569 0.2481 0.2558 0.2577 0.2561 0. 0.1997 0.2134 0.2223 0.2127 0.2207 0.2224 0.2206 0.2259 Two-hop Query 0.0919 0.0923 0.0918 0.0916 0.0924 0.0950 0.0945 0.0917 0.0695 0.0697 0.0694 0.0691 0.0695 0.0722 0.0713 0.0689 Three-hop Query 0.0869 0.0883 0.0827 0.0828 0.0851 0.0855 0.0837 0. 0.0636 0.0644 0.0596 0.0597 0.0615 0.0624 0.0610 0.0632 0.2548 0.2682 0.2778 0.2691 0.2772 0.2797 0.2774 0.2840 0.1133 0.1133 0.1133 0.1124 0.1141 0.1164 0.1161 0.1133 0.1110 0.1120 0.1054 0.1058 0.1084 0.1086 0.1068 0.1111 0.4213 0.4326 0.4426 0.4379 0.4429 0.4443 0.4445 0.4494 0.2613 0.2585 0.2611 0.2597 0.2637 0.2658 0.2661 0. 0.2595 0.2596 0.2531 0.2549 0.2568 0.2566 0.2551 0.2613 0.8489 0.8521 0.8563 0.8558 0.8562 0.8586 0.8584 0.8603 0.8768 0.8762 0.8749 0.8752 0.8772 0.8743 0.8761 0.8754 0.8827 0.8840 0.8802 0.8816 0.8828 0.8828 0.8825 0.8832 Table 9: Settings of overlap length and chunk length for different chunking methods in the CUAD dataset. ppl represents direct PPL Chunking, with threshold of 0. Chunking Method Overlap Length Chunk Length Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl 98.00 98.49 97.70 96.08 97.59 0 0 0 0 0 19 Preprint. Under review. Table 10: Chunk length and corresponding threshold settings for different chunking methods in four long-text QA datasets of LongBench. - indicates no relevant setting. In Llama index, a(b) represents that chunk length of can be obtained by setting the chunking parameter to b. The remaining a(b) indicates that final chunk length of is obtained by setting the combination length to b. Dataset Chunking Method Original Llama index Qwen2-1.5Bppl Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. HotpotQA MuSiQue NarrativeQA DuReader Length Threshold Length Threshold Length Threshold Length Threshold 87 86.73(154) 86.72 86.80(98) 86.52(96) 86.58(92) 86.77(85) 86.81(70) - - 0.5 0+comb. 0.1+comb. 0.2+comb. 0.3+comb. 0.4+comb. 90 89.94(157) 89.51 89.59(103) 89.60(100) 89.75(96) 89.60(88) 89.68(75) - - 0.5 0+comb. 0.1+comb. 0.2+comb. 0.3+comb. 0.4+comb. 71 70.35(139) 70.28 70.32(82) 70.47(82) 70.17(81) 70.19(79) 70.66(78) - - 1.34 0+comb. 0.1+comb. 0.2+comb. 0.3+comb. 0.4+comb. 262 262.06(330) 261.41 261.34(213) 261.98(200) 261.92(189) 261.06(170) 261.48(140) - - 0.5 0+comb. 0.1+comb. 0.2+comb. 0.3+comb. 0.4+comb. Table 11: Performance of different methods on CUAD QA datasets. ppl indicates direct PPL Chunking, with threshold of 0. Chunking Method BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-Avg ROUGE-L BERTScore Original Llama index Qwen2-1.5Bppl Qwen2-7Bppl Baichuan2-7Bppl 0.6845 0.6966 0.7098 0.7038 0.7195 0.4496 0.4573 0.4722 0.4670 0.4738 0.2997 0.3006 0.3180 0.3143 0.3160 0.1798 0.1730 0.1932 0.1911 0.1884 0.3513 0.3493 0.3677 0.3638 0. 0.4217 0.4137 0.4060 0.4070 0.4111 0.8043 0.8001 0.8006 0.8018 0.8025 Table 12: Performance of different methods in four long-text QA datasets of LongBench. ppl represents direct PPL Chunking, and comb. indicates PPL Chunking with dynamic combination. Multi represents threshold values of the parallel method in four datasets, which are 0.5, 0.5, 1.34, and 0.5 respectively, resulting in chunk lengths of 87, 90, 71, and 262 in sequence."
        },
        {
            "title": "Dataset\nThreshold",
            "content": "Original Llama index Qwen2-1.5Bppl Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. Qwen2-1.5Bcomb. - - Multi 0 0.1 0.2 0.3 0.4 HotpotQA MuSiQue NarrativeQA DuReader ROUGE-L F1 F1 7.21 8.19 8.39 8.08 7.48 7.31 7.92 8.05 5.72 5.03 6.12 4.93 4.91 5.20 5.08 5.80 20.69 21.41 20.77 20.77 20.33 20.95 21.22 21.65 15.79 15.72 17.74 17.47 17.19 17.70 17.46 16.44 20 Preprint. Under review. Table 13: Chunk length and its corresponding threshold settings when exploring the impact of chunking on re-ranking. - indicates no relevant setting. Chunking and Re-ranking Chunk Length Threshold Original Original and BgeRerank Original and PPLRerank Qwen2-1.5Bppl Qwen2-1.5Bppl and BgeRerank Qwen2-1.5Bppl and PPLRerank 78 78 77.60 77.60 77.60 - - - 0.5 0.5 0.5 Table 14: Performance of re-ranking strategies combined with different chunking methods in the MultiHop-RAG benchmark. ppl represents direct PPL Chunking, with threshold of 0.5. Chunking and Re-ranking Hits@8 Hits@6 Hits@4 Hits@2 MAP@10 MRR@ Original Original and BgeRerank Original and PPLRerank Qwen2-1.5Bppl Qwen2-1.5Bppl and BgeRerank Qwen2-1.5Bppl and PPLRerank 0.5627 0.5818 0.5769 0.6838 0.6927 0.7197 0.5180 0.5406 0.5521 0.6244 0.6435 0. 0.4523 0.4741 0.5055 0.5503 0.5721 0.6568 0.3499 0.3379 0.4102 0.4151 0.4381 0.5721 0.1512 0.1486 0.1849 0.1954 0.2075 0. 0.3507 0.3391 0.4147 0.4195 0.4413 0."
        }
    ],
    "affiliations": [
        "Institute for Advanced Algorithms Research, Shanghai",
        "Renmin University of China"
    ]
}