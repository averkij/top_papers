{
    "paper_title": "Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum",
    "authors": [
        "Zhuoning Guo",
        "Mingxin Li",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Pengjun Xie",
        "Xiaowen Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval."
        },
        {
            "title": "Start",
            "content": "Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum Zhuoning Guo1,2, Mingxin Li2, Yanzhao Zhang2, Dingkun Long2, Pengjun Xie2, Xiaowen Chu1 1AI Thrust, HKUST(GZ), 2Tongyi Lab, Alibaba Group The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRBs diagnostics, we introduce scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is dominant but overlooked scenario. Overall, our co-designed framework provides practical path to escape the limited scope and advance toward truly universal video retrieval. Date: November 3, 2025 Contact: Zhuoning Guo (zguo772@connect.hkust-gz.edu.cn) Project Leader: Dingkun Long (dingkun.ldk@alibaba-inc.com) Corresponding Author: Xiaowen Chu (xwchu@hkust-gz.edu.cn) Project Page: https://gzn00417.github.io/GVE/ 5 2 0 2 1 3 ] . [ 1 1 7 5 7 2 . 0 1 5 2 : r Figure 1 We propose Universal Video Retrieval (UVR) that retrieves videos with multi-task, cross-domain queries, which can be achieved via benchmark-data-model co-design in this work."
        },
        {
            "title": "1 Introduction",
            "content": "Video retrieval is critical, yet challenging task for modern search engines and recommendation systems, requiring effective video embedding models Zhu et al. (2023). Early efforts extended Contrastive LanguageImage Pretraining (CLIP) Radford et al. (2021) to video Ma et al. (2022). Now, paradigm shift is underway, 1 Figure 2 Model performance on UVRB for 16 datasets and 9 abilities (3 main tasks and 6 (sub-) domains). with Multimodal Large Language Models (MLLMs) rapidly displacing CLIP for their superior language understanding and visual generalization capabilities Kong et al. (2025). Current practice involves training these models on massive datasets with simple and noisy text annotations (e.g., WebVid Bain et al. (2021)) with strong results for coarse-grained text-to-video retrieval on benchmarks (e.g., MSRVTT Xu et al. (2016)). However, they struggle with the complexity of diverse video retrieval scenarios (Figure 1). First, narrow semantic distribution renders these models ineffective in fine-grained queries required to understand spatial relations or temporal dynamics Xu et al. (2025), as well as in long-context retrieval within lengthy videos Cai et al. (2025). Second, the scope of applicable tasks is restricted, with little support for diverse query formats beyond plain text, such as composed retrieval using text-and-image pairs and purely visual queries. Existing specialized models Hummel et al. (2024) are costly and hinder progress toward single, generalizable model across these emerging scenarios. Therefore, to establish framework for universal video retrieval that supports multi-domain, multi-granularity, and multi-task capabilities, three coupled challenges in evaluation, data, and modeling need to be addressed simultaneously: (1) Dimensional Diagnostic Evaluation: The foundational step is to define and measure universality quantitatively. This necessitates comprehensive evaluation framework capable not only of assessing performance across diverse tasks but, more critically, of diagnosing the intricate correlations and interferences between them. (2) Large-Scale Quality-Controlled Data Synthesis: Existing datasets are either too small or are biased, while collecting new, massive dataset is prohibitively expensive. Growing works attempt to synthesize data to address this challenge Chen et al. (2023); Chai et al. (2024); Ventura et al. (2024). However, these resources often exhibit uneven quality and distribution. Therefore, gaining precise control over the properties of large-scale, cross-domain, multi-task data via unified synthesis process is the second challenge. (3) Interconnected Multi-task Representation Learning: critical, often-ignored aspect is the inherent hierarchy among tasks. Foundational abilities such as spatial perception (e.g., object recognition) serve as building blocks for higher-order temporal reasoning (e.g., action recognition). This principle is evidenced by the remarkable success of image-only trained models like GME Zhang et al. (2025) on video retrieval tasks (see Figure 2), highlighting latent cross-task adaptability. Nevertheless, conventional models that assume task independence fail to capitalize on this structure, which is key to unlocking superior generalization. To this end, we propose holistic framework by co-designing evaluation, data, and modeling. Specifically, we first construct the Universal Video Retrieval Benchmark (UVRB), comprehensive suite with 16 test datasets 2 across diverse domains and tasks for 14 state-of-the-art models. More importantly, an in-depth analysis quantitatively exposes the limitations of current approaches. Second, based on the diagnostics, we design V-SynFlow, multi-stage data synthesis workflow that transforms massive, low-quality text-video pairs into high-quality and multi-task dataset, Universal Video Retrieval Dataset (UVRD). It consists of over 1.55 million video retrieval pairs with rich spatial-temporal details, diverse descriptive styles, and distinct task formats. Third, we devise the Modality Pyramid, customized curriculum that leverages inherent task and domain dependencies to optimize General Video Embedder (GVE) on the diverse synthesized data for advanced zero-shot task and domain adaptation. The bottom-up, pyramid-shaped curriculum prioritizes data-abundant, foundational tasks before progressing to more complex, dependent ones for progressive and stable knowledge acquisition. Extensive experiments on UVRB validate the effectiveness of GVE (Figure 2). Besides, the diagnostic analysis reveals underexplored findings. For example, conventional benchmarks are not representative of the overall retrieval ability, indicating the potential for the overfitting of existing models on in-domain data. Instead, partially relevant video retrieval, despite low research attention, is typical and generalizable scenario in this field. The contributions of this work are summarized as follows: (1) Benchmark: universal video retrieval benchmark with 16 test datasets for the multi-dimensional, diagnostic capability evaluation. (2) Data: scalable video data synthesis workflow, producing over 1.55 million cross-domain and multi-task pairs to establish high-quality training resource. (3) Training & Model: multimodal pyramid curriculum for learning generalizable video embeddings by modeling inherent knowledge dependencies across tasks. (4) Experiment & Analysis: Extensive experimental results and analysis, validating the superiority of our proposed methods among 14 state-of-the-art video retrievers and discovering unnoticed and insightful knowledge."
        },
        {
            "title": "2 Related Works",
            "content": "Video Retrieval. Text-to-video retrieval has progressed from matching coarse phrases to parsing fine-grained spatio-temporal descriptions Xu et al. (2016). While recent benchmarks have advanced beyond simple recognition by incorporating detailed annotations like spatio-temporal grounding (CaReBench Xu et al. (2025)), scene understanding (UltraVideo Xue et al. (2025)), and camera motion (CameraBench Lin et al. (2025)), they remain specialized. Concurrently, the scope of retrieval has expanded to include new paradigms like composed queries with text-image pairs (e.g., CoVR Ventura et al. (2024); Hummel et al. (2024)) and purely visual queries in egocentric contexts Liu et al. (2021). Despite these advances, the field remains fragmented, with models and evaluation siloed within specific tasks or domains. This prevents holistic understanding of models true generalization capabilities, gap our unified benchmark aims to fill. Video Embedding Models. Video embedding models have evolved from the adaptation of image-centric ones, such as CLIP Radford et al. (2021), to powerful, larger language-based encoders. Early methods such as CLIP4Clip Luo et al. (2022) and InternVideo2 Wang et al. (2024c) added temporal modules to CLIP but inherited its limitations in complex language understanding and long-context processing Wang et al. (2025); Li et al. (2025). To overcome these issues, recent work leverages Multimodal Large Language Models (MLLMs) as video embedders. Models like LLaVE Lan et al. (2025), UNITE Kong et al. (2025), and VLM2Vec-V2 Meng et al. (2025) achieve strong performance on benchmarks by training on text-image and text-video data. However, this data has so far been narrow for learning generalizable embeddings, and these video embedding models have not been systematically evaluated across more complex tasks and domains of video retrieval."
        },
        {
            "title": "3 Methodology",
            "content": "This section establishes new ecosystem to reshape the fragmented scope of video retrieval by co-designed, tripartite framework. The basis is the Universal Video Retrieval Benchmark (UVRB), which defines comprehensive suite of abilities and serves as diagnostic tool. Informed by UVRBs diagnostics, our V-SynFlow pipeline synthetically generates high-fidelity dataset, Universal Video Retrieval Dataset (UVRD), engineered to populate the identified semantic and structural gaps. Finally, the Modality Pyramid provides principled curriculum with adaptive task scheduling to train General Video Embedder (GVE). This tight integration of 3 Figure 3 V-SynFlow: multi-stage synthesis workflow for diverse video retrieval data. diagnostic evaluation, targeted data synthesis, and model optimization forms feasible solution for universal video retrieval."
        },
        {
            "title": "3.1 Unifying and Benchmarking Video Retrieval",
            "content": "Existing works are typically confined to coarse-grained text-to-video tasks, limiting their capacity to define and evaluate model generalization. To address this, we propose new paradigm to unify complex query formats and divergent data domains of video retrieval, namely Universal Video Retrieval (UVR), defined as below. Definition 1 (Universal Video Retrieval (UVR)). Given related pair with query and video v, UVR aims to learn θ-parameterized model Eθ() to compute relevance score between their embeddings, sqv = cos(Eθ(q), Eθ(v)), which should be higher than other irrelevant pairs. This condition can be satisfied for the given pair with different formats and in divergent domains. Specifically, (1) Query Format can be Textual (TXT, e.g., natural language), Composed (CMP, text+image/text+video), Visual (VIS, image/video). (2) Data Domain can be Coarse-grained (CG, high-level semantics), Fine-grained Spatial (S, object appearance), Temporal (T, event dynamics), Partially Relevant (PR, local or abstract information) and Long-context (LC, extended inputs). To systematically evaluate UVR performance, we introduce the Universal Video Retrieval Benchmark (UVRB), which evaluates model universality by covering 16 test datasets targeting distinct abilities1. Dataset statistics and construction details are presented in Appendix A.2. Specifically, coarse-grained tasks use MSRVTT Xu et al. (2016), DiDeMo Anne Hendricks et al. (2017), CRB-G Xu et al. (2025). Fine-grained: CRB-S Xu et al. (2025)/VDC-O Chai et al. (2024) (spatial), CRB-T Xu et al. (2025)/CMRB Lin et al. (2025) (temporal), DREAM-E Wang et al. (2024b)/LoVR-Theme2Clip Cai et al. (2025)/PEV-K Bolya et al. (2025) (partially relevant). Long-context: LoVR-V Cai et al. (2025), VDC-D Chai et al. (2024). Composed queries: MSTI/MS-TV (adapted from MomentSeeker Yuan et al. (2025)), MSRVTT-I2V Xu et al. (2016), LoVR-C2V Cai et al. (2025). To our knowledge, UVRB is the first benchmark to systematically span comprehensive video retrieval scenarios. Through extensive experiments on UVRB across 14 baselines in Section 4 to diagnose the strengths and weaknesses of existing works, which insightfully guide our design of data synthesis (Section 3.2) and model training (Section 3.3)."
        },
        {
            "title": "3.2 Scalable Synthesis of Cross-Domain Multi-Task Video Retrieval Data",
            "content": "Training universal video embedder is fundamentally impeded by deficiency of high-quality supervision for divergent tasks. Therefore, we introduce V-SynFlow (Figure 3) to transform weakly annotated web videos from raw datasets (e.g., PVD Bolya et al. (2025), InternVid-FLT Wang et al. (2023), and WebVid Bain et al. (2021)) into structured, high-fidelity, multi-task training instances. In this way, we obtain practical and diverse dataset, called Universal Video Retrieval Dataset (UVRD) with over 1.55 million descriptive pairs in total for dimensional training enhancement (see Appendix A.4 for details). V-SynFlow proceeds in three stages: 1We define an ability as proficiency in one query format or one data domain (e.g., VIS is an ability; is an ability). general embedding model should master multiple abilities and their combinations. 4 Figure 4 The architecture of GVE, MLLM-based embedding model. We only finetune the LLM part. GVE inputs compositional multimodal elements and outputs high-dimensional vector as an embedding. Figure 5 Modality Pyramid: simpler tasks lay the foundation for specific ones. We first construct clean, semantically coherent material pool by filtering noise at multiple granularities. Then we leverage an MLLM as conditional generative engine to enrich semantic dimensions. Lastly, we synthesize diverse instances across multiple retrieval tasks. We provide our details and applied prompts for synthesis in Appendix A.6. Multi-granular Quality Control. Given raw corpus = {(vi, ti)}, we produce high-fidelity asset pool, Atf c. The process applies filter cascade: Annotation Rectification to remove non-descriptive text; CrossModal Consistency Filtering, which discards pairs where the similarity from pretrained embedder Φ() (e.g., GME-7B Zhang et al. (2025)) is below threshold; and Temporal Dynamics Filtering to remove static content. The resulting asset pool Atf contains set of validated videos {vj}, their original captions {tj}, and corresponding sets of extracted frames {fjk} and cropped clips {cjl}. Multi-dimensional Information Enrichment. We leverage the filtered assets in Atf to generate richer data structures. To create an enriched text-video dataset D+, we use an MLLM, Mcap (e.g., Keye-VL-8B Team et al. (2025)), as conditional captioning engine. For each video vj, it synthesizes multiple captions {t jk} conditioned by randomly generated information profiles (30% spatial, 60% temporal, and 10% others). By sampling one of the captions for each video, we obtain set of new high-quality text-video pairs {(vj, jk)}. Besides, we form pairs of video and its visual components to construct collections of visual pairs Pfv and Pcv, resulting in frame-to-video {(fjk, vj)} and clip-to-video {(cjl, vj)} pairs, respectively. Multimodal Task Extension. The final stage assembles the unified training corpus, D. We synthesize complex composed retrieval tasks by leveraging Pfv and Pcv. For each visual pair (e.g., (fjk, vj), (cjl, vj)), Mcap generates query text tfjkvj describing the temporal evolution, forming training instance ((tfjkvj , fjk), vj). Besides, basic alignment tasks (e.g., text-image pairs) are also sampled from Atf c. For each video, two candidates from the unselected synthesized captions are mapped as text-to-text pairs. The resulting dataset provides comprehensive mix of tasks essential for training universal embedder."
        },
        {
            "title": "3.3 Modality Pyramid: Customized Curriculum Contrastive Learning for Generalizable Embed-",
            "content": "dings Our synthesized dataset provides rich mixture of retrieval tasks spanning diverse query formats (text, image, video, and their compositions) and data domains (coarseto fine-grained, long-context, etc., see Appendix A.4 for details). To embed multimodal inputs into unified space, we introduce General Video Embedder (GVE) (Figure 4), multimodal encoder derived from Qwen2.5-VL Bai et al. (2025) to inherit its pretrained vision-language aligned knowledge. For arbitrary modality combinations (e.g., image-only or text+video), GVE fuses the tokenized prompt and text inputs with projected visual features into joint input sequence. The LLM processes this sequence to autoregressively produce representations. We extract the final embedding via last token pooling and ℓ2-normalization for retrieval. Details are provided in Appendix A.7. 5 However, our diagnostics based on UVRB reveal that naively training single embedder on this heterogeneous data leads to suboptimal performance (Section 4.2). One of the potential reasons is that easy tasks dominate early optimization, while challenging ones receive insufficient gradient signal and converge poorly. Moreover, existing methods either train on single-task data or overlook the knowledge dependencies between heterogeneous domains of multi-task data, which can benefit the joint incorporation of model abilities. To address this, we propose Modality Pyramid (Figure 5), curriculum that schedules training from atomic to composite tasks for progressive knowledge acquisition. It guides the embedding model to master perceptual primitives first, then advance to complex integration, which preserves foundational knowledge while cultivating generalizable transfer. Alignment-aware dynamic scheduling. At the beginning of each training epoch t, we estimate the alignment level of every task using prober model Ψt. For = 1, Ψ1 is strong off-the-shelf embedder (e.g., GME-7B Zhang et al. (2025)); for > 1, Ψt is the GVE checkpoint from the end of epoch 1. For each task, we sample Np positive pairs and compute its relevance score as the average cosine similarity: R(t) cos(Ψt(xi), Ψt(yi)). Higher R(t) indicates better current alignment. During epoch t, tasks are sampled with probability (t)(k) exp(R(t) /σ(t)), where the temperature σ(t) increases linearly from σmin to σmax (σmin = 0.1 and σmax = 1.0 by default). This annealing schedule ensures initial focus on well-aligned tasks while progressively encouraging exploration of challenging ones. = 1 Np (cid:80) Unified Contrastive Optimization. GVE is trained with symmetric InfoNCE loss Oord et al. (2018) across all scheduled tasks. To strengthen discrimination, we augment in-batch negatives with hard negatives mined from large external corpus using the same prober Ψt. For querytarget pair (q, v+), the loss contrasts v+ against both other positives in the batch and the top-K retrieved hard negatives. The similarity is computed as sqv = cos(Eθ(q), Eθ(v)), and the final loss is symmetrized over query and target directions: L(qy) = log exp(sqiy+ /τl) + (cid:80) exp(sqiy+ /τl) /τl) + (cid:80) j=i exp(sqiy+ exp(sqiy /τl) , (1) Hi where sqv = cos(Eθ(q), Eθ(v)) and τl is pre-defined temperature. The total loss is the symmetric sum Li = 1 ). + L(yq) 2 (L(qy) i"
        },
        {
            "title": "4.1 Experimental Setups\nBaselines. Our evaluation benchmarks 14 prominent baselines. As detailed in Appendix A.9, our selection\nspans a wide range of architectures, parameter sizes (from 87M to 8.3B), and training data compositions.\nThe models are broadly divided into two categories: First, traditional CLIP-based embedding models\ninclude CLIP4Clip Luo et al. (2022), ViCLIP Wang et al. (2023), VideoCLIP-XL Wang et al. (2024a),\nLanguageBind Zhu et al. (2024), and the InternVideo2 series (1B and 6B) Wang et al. (2024c). Second,\na more recent category of MLLM-based embedding models includes GME-2B Zhang et al. (2025), Unite-\n2B Kong et al. (2025), VLM2Vec-V2 Meng et al. (2025), BGE-VL Zhou et al. (2024), UniME-7B Gu et al.\n(2025), B3-7B Thirukovalluru et al. (2025), GME-7B Zhang et al. (2025), and Unite-7B Kong et al. (2025).\nNote that the training data of baseline models may include in-domain data of test datasets in UVRB (e.g,\nMSRVTT, DiDeMo).",
            "content": "Metrics. Our primary metric is Recall@1 (R@1), which measures if the most relevant item is the correct one. For more challenging datasets with fuzzy queries (e.g., CMRB and LoVR-TH), we choose to report Recall@10 (R@10). Additionally, we use Precision@1 (P@1) for the MS-TI and MS-TV with multiple positive candidates. Evaluation Implementations. All models are evaluated within UVRB under controlled and consistent environment to ensure fairness and reproducibility. Model parameters are loaded in bf16 precision wherever 6 Table 1 Video retrieval performance for datasets of UVRB. The AVG values are averaged over 16 datasets. For each column: highest score is bolded, second-highest is underlined. Metrics: R@1 (Recall@1), R@10 (Recall@10), P@1 (Precision@1). Model AVG MSRVTT DiDeMo CRB-G CRB-S VDC-O CRB-T CMRB DREAM-E R@1 R@1 R@1 R@1 R@1 R@1 R@10 R@1 0.390 CLIP4Clip ViCLIP 0.352 VideoCLIP-XL 0.491 LanguageBind 0.487 InternVideo2-1B 0.404 InternVideo2-6B 0.427 0.488 GME-2B 0.480 Unite-2B 0.508 VLM2Vec-V2 0.443 BGE-VL 0.521 UniME-7B 0.511 B3-7B 0.530 GME-7B 0.538 Unite-7B GVE-3B GVE-7B Model 0.544 0.573 0.333 0.386 0.443 0.479 0.449 0.485 0.390 0.367 0.330 0.337 0.351 0.282 0.436 0.439 0.431 0. 0.297 0.306 0.403 0.421 0.404 0.418 0.303 0.298 0.299 0.318 0.335 0.350 0.377 0.386 0.511 0.447 0.828 0.716 0.586 0.608 0.690 0.699 0.828 0.690 0.815 0.815 0.740 0.798 0.497 0.437 0.839 0.687 0.568 0.612 0.718 0.723 0.843 0.688 0.827 0.825 0.767 0.804 0.620 0.530 0.735 0.759 0.644 0.650 0.715 0.727 0.775 0.639 0.743 0.768 0.731 0.753 0.289 0.280 0.349 0.229 0.487 0.274 0.466 0.290 0.470 0.355 0.455 0.346 0.400 0.298 0.409 0.284 0.410 0.286 0.359 0.225 0.476 0.317 0.415 0.312 0.442 0.304 0.472 0.351 0.191 0.235 0.263 0.280 0.242 0.271 0.240 0.223 0.228 0.212 0.293 0.216 0.274 0. 0.376 0.850 0.846 0.786 0.496 0.363 0. 0.433 0.865 0.847 0.794 0.539 0.398 0.302 LoVR-TH PEV-K LoVR-V VDC-D MS-TI MS-TV MSRVTT-I2V LoVR-C2V R@1 P@1 P@1 R@1 R@1 R@ R@1 R@1 0.338 CLIP4Clip 0.202 ViCLIP 0.439 VideoCLIP-XL LanguageBind 0.425 InternVideo2-1B 0.298 InternVideo2-6B 0.302 0.446 GME-2B 0.445 Unite-2B 0.492 VLM2Vec-V2 0.387 BGE-VL UniME-7B 0.504 0.462 B3-7B GME-7B 0.523 Unite-7B 0.555 GVE-3B GVE-7B 0.522 0. 0.360 0.230 0.380 0.540 0.280 0.330 0.530 0.570 0.610 0.550 0.480 0.590 0.179 0.075 0.229 0.303 0.026 0.086 0.354 0.355 0.324 0.184 0.323 0.387 0.396 0.710 0.440 0.620 0.610 0.330 0.680 0.413 0.566 0.173 0.183 0.395 0.283 0.243 0.820 0.230 0.223 0.679 0.228 0.233 0.485 0.265 0.230 0.516 0.235 0.205 0.839 0.350 0.340 0.792 0.250 0.233 0.913 0.275 0.250 0.722 0.303 0.233 0.847 0.310 0.305 0.853 0.275 0.265 0.865 0.348 0.333 0.871 0.278 0.230 0.918 0.340 0.268 0.948 0.343 0.280 0.924 0.846 0.861 0.827 0.794 0.868 0.827 0.863 0.841 0.779 0.867 0.884 0.860 0.883 0.891 0. 0.503 0.433 0.403 0.463 0.368 0.452 0.366 0.445 0.385 0.465 0.537 0.471 0.370 0.448 0.403 0.415 supported. All output embeddings are normalized to mitigate precision-induced variance. For retrieval tasks, we uniformly adopt cosine similarity as the relevance metric2, with no post-processing or re-ranking applied. Only raw visual frames are used, and audio, speech, and metadata are excluded. Each video is uniformly sampled into exactly 8 frames. CLIP-based models operate at 224 224 resolution, while MLLM-based models are constrained such that each frame encodes to fewer than 200 visual tokens3. Input sequences for queries or candidates are capped at 8192 tokens, with truncation applied beyond this limit. For models lacking native video support (e.g., BGE-VL), we implement multi-image embedding pipeline (usually by inserting multiple special tokens for image), treating each frame as an independent image input. Others. We provide complete details in the appendix, covering: (1) the construction and evaluation of UVRB (Appendix A.2-A.3); (2) the data synthesis pipeline with prompts (Appendix A.4-A.6); and (3) the GVE models architecture and training specifics (Appendix A.7-A.8). The appendix also includes baseline properties and more experimental results (Appendix A.9-A.13). 2Any learnable relevance estimation modules (e.g., the MLP head in InternVideo2) are removed and the last hidden state before them is used as embedding for fair comparison. 3Frame resolution is adaptively adjusted per models tokenization strategy to enforce the token limit. 7 Table 2 Video retrieval performance by specific abilities (tasks and domains) on UVRB. The AVG values are averaged over tasks (textual (TXT), composed (CMP), visual (VIS)) and domains (coarse-grained (CG), fine-grained (FG), long-context (LC)) video retrieval tasks. Besides, we provide sub-domain results, including spatial (S), temporal (T), partially relevant (PR). For each column: highest score is bolded, second-highest is underlined. Model CLIP4Clip ViCLIP VideoCLIP-XL LanguageBind InternVideo2-1B InternVideo2-6B GME-2B Unite-2B VLM2Vec-V2 BGE-VL UniME-7B B3-7B GME-7B Unite-7B AVG 0.416 0.375 0.510 0.508 0.420 0.445 0.416 0.507 0.538 0.480 0.542 0.538 0.562 0.559 TXT 0.401 0.336 0.550 0.543 0.422 0.448 0.539 0.536 0.587 0.497 0.561 0.570 0.604 0.609 GVE-3B GVE-7B 0. 0.619 0.600 0.657 Tasks CMP 0.178 0.263 0.227 0.231 0.248 0. 0.345 0.242 0.263 0.268 0.308 0.270 0.341 0.254 0.304 0.312 Domains Sub-domains VIS CG FG LC 0.714 0.640 0.632 0.645 0.581 0.660 0.597 0.654 0.613 0.622 0.702 0.678 0.615 0.666 0.647 0. 0.380 0.380 0.558 0.539 0.480 0.504 0.461 0.455 0.498 0.448 0.500 0.482 0.518 0.541 0.360 0.315 0.493 0.479 0.403 0.417 0.471 0.471 0.502 0.406 0.518 0.505 0.507 0.539 0.463 0.313 0.600 0.610 0.383 0.423 0.685 0.681 0.762 0.636 0.664 0.722 0.788 0.746 0.552 0.541 0. 0.587 0.570 0.814 0.559 0.484 0.787 0.723 0.606 0.631 0.716 0.725 0.809 0.664 0.785 0.797 0.749 0.779 0.816 0. 0.285 0.289 0.381 0.378 0.413 0.400 0.349 0.347 0.348 0.292 0.396 0.364 0.373 0.412 0.430 0.469 PR 0.236 0.171 0.310 0.336 0.189 0.220 0.347 0.341 0.348 0.261 0.373 0.355 0.398 0. 0.377 0.419 Table 3 Ablation study for synthesized data and modality pyramid curriculum. AVG of : average across datasets, AVG of : average across abilities. For each column of each size of model (3B or 7B): highest score is bolded, second-highest is underlined. A"
        },
        {
            "title": "Model",
            "content": "GVE-i-3B GVE-s-3B GVE-3B GVE-i-7B GVE-s-7B GVE-7B"
        },
        {
            "title": "AVG",
            "content": "0.528 0.537 0.544 0.563 0."
        },
        {
            "title": "AVG",
            "content": "0.558 0.564 0.571 0.587 0."
        },
        {
            "title": "TXT",
            "content": "0.620 0.617 0.619 0.643 0.648 0.573 0.600 0."
        },
        {
            "title": "Domains",
            "content": "Sub-domains"
        },
        {
            "title": "CMP",
            "content": "0.237 0."
        },
        {
            "title": "VIS",
            "content": "0.632 0.617 CG 0.532 0.539 FG 0.521 0.536 0. 0.647 0.552 0.541 0.274 0.313 0.312 0.678 0.662 0. 0.567 0.576 0.566 0.563 0.587 0.570 0.814 LC PR 0.808 0.775 0.764 0.795 0.804 0.781 0. 0.402 0.421 0.816 0.430 0.812 0.814 0.821 0.459 0. 0.469 0.379 0.377 0.377 0.426 0.418 0."
        },
        {
            "title": "4.2 Main Results\nOverall Performance. We evaluate GVE on the UVRB benchmark under a strictly zero-shot setting without\nany exposure to in-domain data during training. Although competing models may have an unfair advantage\nfor using training data corresponding to several test sets, our results in Table 1 and Table 2 show clear\nsuperiority and confirm the strong generalization of GVE. Specifically, GVE-7B achieves state-of-the-art\nresults with mean scores of 0.573 across datasets and 0.600 across task categories. It outperforms Unite-7B\nby +6.5% and +7.3% even though Unite-7B may have seen in-domain data. GVE-7B leads in every major\ndimension including TXT at 0.657 versus 0.609, CMP at 0.312 versus 0.254, CG at 0.587, FG at 0.570, and\nLC at 0.814 versus 0.746. It also leads in fine-grained subdomains with S at 0.821 versus 0.779 and T at\n0.469 versus 0.412. Unite-7B shows strength in VIS and PR tasks but underperforms in compositional and\ntemporal reasoning. Its performance is uneven and relies heavily on specific training data. The compact\nGVE-3B with 3.8B parameters scores 0.571, higher than Unite-7B at 0.559 with over 7B parameters. This\nshows our gains come from better data synthesis and curriculum design not from model size or data leakage.\nOur smaller models match or beat larger ones under fair zero-shot evaluation. This advantage translates to\ntop results while Unite-7B’s strength on a few datasets reflects narrow capability.",
            "content": "Ablation Study. We perform an ablation study to isolate the distinct contributions of our synthesized dataset (UVRD) and the Modality Pyramid curriculum, with results presented in Table 3. First, integrating 8 Figure 6 Performance effect from data scaling for GVE series. See Appendix A.11 for detailed results. UVRD (GVE-s vs. baseline GVE-i) demonstrates significant impact on acquiring complex abilities; notably, performance on the composed (CMP) task for the 3B model increases by 27% relative. Second, applying the Modality Pyramid (GVE vs. GVE-s) further refines model capabilities and enhances generalization, boosting the overall score from 0.594 to 0.600 and TXT score from 0.648 to 0.657 for the 7B model. In conclusion, UVRD provides the diverse knowledge required for complex tasks, while our curriculum optimizes its integration, culminating in total performance gain of 1.8% to 3.1% over the baseline. Data Scaling. Figure 6 shows the training-time scaling behavior that performance logarithmically improves with more data, but with diminishing returns. We quantify scaling efficiency by fitting logarithmic model = ln + (x: data size, y: performance) and report the absolute and relative gain per 10 data increase. On average across datasets (abilities), GVE-3B improves by +7.4% (+7.1%) per decade, while GVE-7B gains +5.4% (+5.4%). While GVE-3B exhibits higher scaling efficiency in relative terms, GVE-7B starts from higher baseline, suggesting trade-off between scaling slope and absolute capability. This result highlights the potential for scaling more powerful video embedding model with training data at larger scale. In addition, we also explore the test-time scaling of spatial and temporal density (the frame number and resolution) in Appendix A.12 as dimension of generalization."
        },
        {
            "title": "4.3 Analysis of Dimensional Capabilities",
            "content": "Figures 7 and 8 reveal patterns in how video retrieval models develop capabilities and what their relationships are, as measured primarily by Pearson correlation (ρ) for their performance results. Here, we present several discoveries from the analysis in underexplored perspectives. Finding 1: Partially Relevant Video Retrieval Better Reflects Universality than Traditional Benchmarks. Standard benchmarks such as MSRVTT show low correlation with average UVR performance (ρavg = 0.58), suggesting limited representativeness, likely due to overfitting or simplified task design. In contrast, finegrained (FG), partially relevant (PR), and long-context (LC) retrieval exhibit strong mutual correlations (ρ 0.90). More importantly, PR retrieval, although understudied, achieves the highest average correlation with overall performance (ρavg = 0.97), positioning it as promising proxy for robust model evaluation. Finding 2: Disentangled Spatial and Temporal Representations. Models exhibit marked decoupling between spatial (S) and temporal (T) representation (ρ = 0.12). This asymmetry is critical that temporal skills dominate fine-grained understanding (ρT-FG = 0.98), while spatial skills contribute minimally (ρS-FG = 0.39). This suggests current works fail to jointly model when and where, highlighting the need for inductive biases that encourage spatiotemporal integration. Finding 3: Performance Divergence Between CLIP and MLLM-based Models. Model failures are architecturedependent. For example, CLIP-based models are spatially biased (ρS-CG = 0.99) but temporally weak (ρT-CG = 0.46), leading to ability trade-offs. Compositional representation inversely correlates with visual 9 Figure 7 Correlation between dimensional abilities on UVRB for CLIP or MLLM-based models. Figure 8 Correlation between averaged performance and abilities or datasets on UVRB. accuracy (ρCMP-VIS = 0.71). And near-zero link between temporal and long-context skills (ρT-LC = 0.14). In contrast, MLLM-based models demonstrate more balanced and integrated learning: superior semantic matching (PR-CG: MLLM ρ = 0.98 vs. CLIP 0.70) and stronger temporal-long-context coupling (ρLC-T = 0.64). Therefore, architecture fundamentally shapes the development of capability, and MLLM has been increasingly popular because of its generalizability in video embedding modeling. Finding 4: Scaling Has Limited Impact on Visual Perception. Parameter scaling improves high-level semantic coherence, but yields negligible gains in low-level visual perception. Notably, the 87M-parameter CLIP4Clip (VIS: 0.714) outperforms the 8B-parameter Unite-7B (VIS: 0.702). Given the weak correlation between visual fidelity and overall retrieval success (ρAVG-VIS = 0.26), future progress requires targeted improvements in visual grounding."
        },
        {
            "title": "5 Conclusions",
            "content": "This work pioneers unified paradigm for video retrieval. We introduced the first benchmark to comprehensively evaluate dimensional video retrieval abilities. It provides diagnostics to guide us to generate 1.55 million highfidelity, multi-task training pairs to meet real-world complexity. In addition, we propose novel curriculum learning algorithm to take advantage of the inherent task-wise relational structure. Based on these, we train superior MLLM-based video embedding model, GVE. Our experiments validate the state-of-the-art generalization of GVE on UVRB and provide insightful findings in this field. Overall, this paper provides foundational framework with evaluation-data-training co-design toward more robust, versatile, and generalizable video retriever."
        },
        {
            "title": "References",
            "content": "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 58035812, 2017. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. Qifeng Cai, Hao Liang, Hejun Dong, Meiyi Qiang, Ruichuan An, Zhaoyang Han, Zhengzhou Zhu, Bin Cui, and Wentao Zhang. Lovr: benchmark for long video retrieval in multimodal contexts. arXiv preprint arXiv:2505.13928, 2025. Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audiosubtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36: 7284272866, 2023. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432, 2025. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Thomas Hummel, Shyamgopal Karthik, Mariana-Iuliana Georgescu, and Zeynep Akata. Egocvr: An egocentric benchmark for fine-grained composed video retrieval. In European Conference on Computer Vision, pages 117. Springer, 2024. Fanheng Kong, Jingyuan Zhang, Yahui Liu, Hongzhi Zhang, Shi Feng, Xiaocui Yang, Daling Wang, Yu Tian, Fuzheng Zhang, Guorui Zhou, et al. Modality curation: Building universal embeddings for advanced multimodal information retrieval. arXiv preprint arXiv:2505.19650, 2025. Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 780787, 2014. Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: large video database for human motion recognition. In 2011 International conference on computer vision, pages 25562563. IEEE, 2011. Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. arXiv preprint arXiv:2503.04812, 2025. Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. Improving llm video understanding with 16 frames per second. arXiv preprint arXiv:2503.13956, 2025. 11 Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023. Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, et al. Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376, 2025. Liu Liu, Jiangtong Li, Li Niu, Ruicong Xu, and Liqing Zhang. Activity image-to-video retrieval by disentangling In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages appearance and motion. 21452153, 2021. Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293304, 2022. Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. In Proceedings of the 30th ACM international conference on multimedia, pages 638647, 2022. Rui Meng, Ziyan Jiang, Ye Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, et al. Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents. arXiv preprint arXiv:2507.04590, 2025. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. doi: 10.48550/ARXIV.2210.07316. https://arxiv.org/abs/2210.07316. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra, et al. Breaking the batch barrier (b3) of contrastive learning via smart batch mining. arXiv preprint arXiv:2505.11293, 2025. Lucas Ventura, Antoine Yang, Cordelia Schmid, and Gül Varol. Covr-2: Automatic data construction for composed video retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, and Lianwen Jin. Videoclip-xl: Advancing long description understanding for video clip models. arXiv preprint arXiv:2410.00741, 2024a. Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024b. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024c. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 12 Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. Yifan Xu, Xinhao Li, Yichun Yang, Desen Meng, Rui Huang, and Limin Wang. Carebench: fine-grained benchmark for video captioning and retrieval, 2025. https://arxiv.org/abs/2501.00513. Zhucun Xue, Jiangning Zhang, Teng Hu, Haoyang He, Yinan Chen, Yuxuan Cai, Yabiao Wang, Chengjie Wang, Yong Liu, Xiangtai Li, and Dacheng Tao. Ultravideo: High-quality uhd video dataset with comprehensive captions. arXiv preprint arXiv:2506.13691, 2025. Youngjae Yu, Jongseok Kim, and Gunhee Kim. joint sequence fusion model for video question answering and retrieval. In Proceedings of the European conference on computer vision (ECCV), pages 471487, 2018. Huaying Yuan, Jian Ni, Zheng Liu, Yueze Wang, Junjie Zhou, Zhengyang Liang, Bo Zhao, Zhao Cao, Zhicheng Dou, and Ji-Rong Wen. Momentseeker: task-oriented benchmark for long-video moment retrieval. arXiv preprint arXiv:2502.12558, 2025. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Bridging modalities: Improving universal multimodal retrieval by multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 92749285, June 2025. Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475, 2024. Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining In The Twelfth International Conference on Learning to n-modality by language-based semantic alignment. Representations, 2024. Cunjuan Zhu, Qi Jia, Wei Chen, Yanming Guo, and Yu Liu. Deep learning for video-text retrieval: review. International Journal of Multimedia Information Retrieval, 12(1):3, 2023."
        },
        {
            "title": "Content",
            "content": "A.1 Overview of Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 UVRB Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Evaluation Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Dataset Construction Pipeline Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Prompt for Synthetic Video Retrieval Data Generation . . . . . . . . . . . . . . . . . . . . . . A.7 Model Architecture Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Training Implementation and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . A.9 Baseline Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.10 Training Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.11 Experiments of Training-time Scaling: More Results of Data Scaling . . . . . . . . . . . . . . A.12 Experiments of Test-time Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.13 Experiments of Video Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.14 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 14 16 16 18 23 24 24 24 24 29 29 13 A.1 Overview of Appendix This appendix provides supplementary material supporting the main paper, organized as follows: Methodology Supplementary. Details of Benchmarking (for Section 3.1): (1) UVRB Details (Appendix A.2): Provides statistics, construction strategies, and evaluation details for the UVRB datasets. This includes definitions for coarse-grained, fine-grained (spatial, temporal, partially relevant), long-context, composed, and visual video retrieval tasks. (2) Evaluation Pipeline (Appendix A.3): Offers an overview of the scalable, reproducible evaluation protocol. Details of Data Synthesis and Preparation (for Section 3.2): (1) Training Data (Appendix A.4): Details the composition and scaling of the multi-modal training mixture, including descriptions of specific datasets used. (2) Dataset Construction Pipeline (Appendix A.5): Explains the design principles (modularity, integrity) of the unified retrieval dataset framework. (3) Synthetic Data Prompts (Appendix A.6): Presents the structured prompts used for generating synthetic video retrieval data (covering captioning, composed retrieval, and frame-level tasks). Details of Model and Training (for Section 3.3): (1) Model Architecture (Appendix A.7): Gives detailed description of the GVE model derived from Qwen2.5-VL, including input fusion and embedding extraction. (2) Training Implementation (Appendix A.8): Outlines the parameter-efficient fine-tuning (LoRA) strategy, key hyperparameters, and the contrastive learning setup. Experimental Details and Results. Baseline Details (Appendix A.9): Lists specifications of the evaluated baseline models. Training Dynamics (Appendix A.10): Shows dynamics, status via metrics in training for GVE-3B and GVE-7B models. More Experiments: (1) Training-time Scaling (Appendix A.11): Presents the impact of scaling training data on performance in addition to Section 4.2. (2) Test-time Scaling (Appendix A.12): Analyzes the effect of scaling test-time parameters (number of frames, resolution) on performance extended from Section 4.2. (3) Video Classification (Appendix A.13): Reports model performance on standard video classification benchmarks. Others. Limitations (Appendix A.14): Claim existing limitations about scopes and settings, required to be addressed or extended in future works. A.2 UVRB Details This section provides details of UVRB, including statistics and construction strategies of datasets. First, the statistics for the number of queries and corpus, the video durations, and the text lengths are presented in Table 4. Second, we introduce the construction strategies of each dataset as follows. MSRVTT: We follow the data partition from JSFusion Yu et al. (2018), utilizing 1,000 clip-text pairs from the original MSRVTT dataset Xu et al. (2016) for evaluation. DiDeMo: Following the methodology in Liu et al. (2019), we concatenate all sentence descriptions associated with single video from the DiDeMo dataset Anne Hendricks et al. (2017) to create paragraph-level query for paragraph-to-video retrieval. CRB-G: We adhere to the CaReBench protocol Xu et al. (2025) and use the content from the caption field as the general query to retrieve videos. 14 Table 4 Statistics of datasets in the Universal Video Retrieval Benchmark (UVRB). All videos use 8 uniformly sampled frames. # Query: the number of queries; # Corpus: the number of corpus; Dur (s): Duration in seconds; # Word: text length in words. Dataset # Query # Corpus Dur (s) # Word 1,000 1,004 1,000 1,000 1,004 1,000 Textual Video Retrieval (Coarse-grained) MSRVTT Xu et al. (2016) DiDeMo Anne Hendricks et al. (2017) CaReBench-General (CRB-G) Xu et al. (2025) Textual Video Retrieval (Fine-grained) (a) Spatial CaReBench-Spatial (CRB-S) Xu et al. (2025) VDC-Object (VDC-O) Chai et al. (2024) (b) Temporal CaReBench-Temporal (CRB-T) Xu et al. (2025) CameraBench (CMRB) Lin et al. (2025) (c) Partially Relevant DREAM-1K-Event (DREAM-E) Wang et al. (2024b) LoVR-Theme2Clip (LoVR-TH) Cai et al. (2025) PE-Video-Keyword (PEV-K) Bolya et al. (2025) Textual Video Retrieval (Long-context) LoVR-Text2Video (LoVR-V) Cai et al. (2025) VDC-Detail (VDC-D) Chai et al. (2024) Composed Video Retrieval MomentSeeker-Text-Image (MS-TI) Yuan et al. (2025) MomentSeeker-Text-Video (MS-TV) Yuan et al. (2025) Visual Video Retrieval MSRVTT-ImageVideo (MSRVTT-I2V) Xu et al. (2016) LoVR-Clip-to-Video (LoVR-C2V) Cai et al. (2025) 1,000 1,027 1,000 728 6,251 8,854 14, 100 1,000 400 400 1,000 467 15.0 53.9 14.4 14.4 30.1 14.4 5. 8.8 16.9 16.9 9.4 29.1 232.2 115.0 91.4 103.2 24.8 6.5 48.1 45.5 1,000 1, 1,000 1,071 1,000 8,854 15,000 467 1,027 1560.3 30.1 17364.5 508.0 10 13.5 13.5 68.5 68.5 1,000 467 15.0 1560.3 - - CRB-S: Similar to CRB-G, we follow Xu et al. (2025) and select the text from the spatial_caption field to form queries focused on spatial descriptions. VDC-O: We utilize the VDC dataset Chai et al. (2024) and extract text from the main_object_caption field as an object-centric query (e.g., The main subject, worker dressed in gray sleeveless shirt and beige pants...). CRB-T: Similar to CRB-S, we again follow Xu et al. (2025), using the temporal_caption field to create queries based on temporal progression. CMRB: We use the detailed camera motion annotations from CameraBench Lin et al. (2025) as queries to retrieve videos (e.g., The camera smoothly dollies forward, maintaining steady and fluid motion...). DREAM-E: For event-based retrieval, we collect event descriptions from the DREAM-1K dataset Wang et al. (2024b) to serve as queries for event-to-video matching (e.g., Wooden trap launches purple squirrel into the air ). LoVR-TH: From the LoVR dataset Cai et al. (2025), we select the theme annotations of video clips as queries for theme-to-clip retrieval (e.g., The overall style of the animation is vibrant and whimsical...). PEV-K: We use the annotations from the keyword field in the PE-Video test set Bolya et al. (2025) to perform keyword-based partially relevant matching (e.g., colorful, paper, beautiful...). LoVR-V: We leverage the full-length video captions from the LoVR dataset Cai et al. (2025) to perform long-text to long-video retrieval. VDC-D: For this task, we extract the long, detailed descriptions from the detailed_caption field of the VDC dataset Chai et al. (2024) to serve as fine-grained queries. MS-TI: Following the adaptation method in Meng et al. (2025) for the MomentSeeker dataset Yuan et al. (2025), we create text-image composed retrieval task. The goal is to retrieve target video clip using combined text and image query. 15 MS-TV: Similar to MS-TI, the task for MS-TV is to use composed query of text description and reference video clip to retrieve the target clip. MSRVTT-I2V: We construct image-to-video retrieval pairs from the aforementioned MSRVTT test set. For each video, single frame is randomly sampled to serve as the image query for retrieving its source video. LoVR-C2V: We leverage the original clip-to-video structure of the LoVR dataset Cai et al. (2025). For each full-length video, one of its corresponding short clips is used as the query for clip-to-video retrieval. Third, we list the query prompt and metrics for datasets in evaluations in Table 5. Table 5 Query Prompts and Metrics for Datasets in UVRB. Dataset Query Prompt Metric MSR-VTT DiDeMo CRB-G CRB-S VDC-O CRB-T CMRB DREAM-E LoVR-TH PEV-K LoVR-V VDC-D MS-TI MS-TV Recall@1 Find the clip that corresponds to the described scene in the given video. Find video that includes the following described scenes. Find the video according to the general text description. Find the video according to the spatial description. Find the video according to the object description. Find the video according to the temporal description. Find the video according to the camera motion description. Find the video according to the text description. Find the video according to text description about video theme information. Find the video according to the text description of series of keywords. Recall@1 Recall@1 Find the long video according to the long text description. Recall@1 Find the video according to the detailed text description. Find the video clip that corresponds to the given text and the given Precision@1 image. Find the video clip that corresponds to the given text and the given video. Recall@1 Recall@1 Recall@1 Recall@1 Recall@1 Recall@10 Recall@1 Recall@10 Precision@ MSRVTT-I2V Find the video according to the image. LoVR-C2V Find the original long video according to the short video clip. Recall@1 Recall@1 A.3 Evaluation Pipeline. Our evaluation is built atop MTEB Muennighoff et al. (2022). It decouples the evaluation engine from model architecturesupporting everything from sentence transformers to custom multimodal encodersvia standardized interface. The pipeline operates in three phases: (1) dynamic task orchestration, (2) configurable model execution, and (3) generation of metrics and diagnostics. For instruction-sensitive tasks, it dynamically injects domain-specific prompts to mirror real-world conditions for actionable diagnostics. Custom benchmarks are integrated seamlessly through dedicated loaders. In this framework, expensive tasks are deferred for efficiency, model initialization (precision, multi-GPU) is abstracted via factory, and data loading supports both online and offline modes for robustness across environments. Engineering safeguards, including explicit multi-GPU management, clean shutdowns, and offline, first retry logicensure reliability at scale. A.4 Training Data To build robust and versatile multimodal retrieval system, our models are trained on large-scale, diverse mixture of text, image, and video data. All tasks are uniformly formulated as instruction-guided retrieval, strategy designed to foster unified representation space that can adeptly handle wide array of queries. The training data is organized into two primary collections: main mixture of widely-used public datasets  (Table 6)  , and synthesized set from our Universal Video Retrieval Dataset (UVRD) suite  (Table 7)  . We follow existing works Li et al. (2023); Zhang et al. (2025) to construct text-only (e.g., MSMARCO) and image-centric data (e.g., CIRR). In addition, we prepare video datasets using the following strategies. VAST Chen et al. (2023): We randomly select one sentence from the multiple vision captions as the text query for each video. 16 InternVid-FLT Wang et al. (2023): We drop about 300K low-quality videos and use the left text-video pairs. PE-Video Bolya et al. (2025): We choose the human_caption refined on model_caption as the textual description of videos. WebVid Bain et al. (2021): We refuse the queries captioning over one video to impede the generality of text, which excludes 5M videos in final training approximately. Table 6 Configuration of the main training data mixture for our 3B and 7B models. denotes thousands, denotes millions. Dataset Part 1: Text-only Data MSMARCO HotpotQA WebQA Part 2: Image-centric Data CIRR Fashion200K Nights OVEN OVEN VisualNews EDIS FashionIQ MSCOCO REMUQ WebQA LLAVA EVQA CC3M CC3M Laion Laion ImageNet VL3-Syn7M (short) VL3-Syn7M (short) VL3-Syn7M (detailed) VL3-Syn7M (detailed) VISTA VISTA Part 3: Video-centric Data VAST InternVid-FLT PE-Video WebVid Taska Sample Size Neg.b BS 3B 7B TT TT TT 300K 500K 69K 11K TIT TI II TITI TIT TI TTI TII IT TIT TTI TIT TITI TI IT TI IT IT TI IT TI IT TII TTI TV TV TV TV 16K 4K 13K 20K 20K 40K 30K 30K 60K 12K 4K 4K 5K 12K 20K 20K 30K 30K 200K 300K 100K 200K 300K 500K 200K 100K 200K 300K 500K 100K 200K 200K 300K 100K 200K 100K 20K 1.6M 1.7M 104K 5.4M 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 3 2 2 3 2 3 2 2 1 0 0 1 64 64 32 32 32 32 64 64 64 32 32 32 32 32 64 64 32 64 32 64 32 32 64 32 64 32 64 32 32 64 32 Task: T=Text, I=Image, V=Video. The format is QueryCorpus. Neg.: Number of explicit hard negatives per positive. 0 indicates use of in-batch negatives only. As the aforementioned methodology, we train our model based on contrastive learning with pre-mined explicit hard negatives and in-batch negatives. To maintain balanced training diet and manage computational load, we employ sophisticated data sampling strategy. For extremely large datasets, we sample fixed number of instances. To amplify the learning signal from certain high-value or complex datasets, we may apply an upsampling strategy by repeating their data. The scale of our training data is substantial. For our 3B parameter model, we prepared 12.55 million instances. To further leverage the capacity of our 7B parameter model, we increase the sampling rate for several large-scale datasets, bringing the total instances to 13.73 million. detailed breakdown of the data composition is provided in Table 8. 17 Table 7 Configuration of UVRD. Sample sizes are identical for both 3B and 7B models. Dataset UVRD-T2T UVRD-T2I UVRD-T2V UVRD-TI2V UVRD-TV2V UVRD-I2V UVRD-V2V Taska TT TI TV TIV TVV IV VV Sample Size 100K 210K 879K 89K 35K 200K 36K Neg.b 0 0 1 1 1 0 0 BS 64 32 64 64 64 64 64 Task: T=Text, I=Image, V=Video. The format is QueryCorpus. Neg.: Number of explicit hard negatives per positive. 0 indicates use of in-batch negatives only. Table 8 Total number of instances for our 3B and 7B models across all data categories."
        },
        {
            "title": "Data Category",
            "content": "Collected: Text-only Collected: Image-centric Collected: Video-centric Synthesized: UVRD (All Modalities)"
        },
        {
            "title": "Total Sample Size",
            "content": "3B Model 380K 1.82M 8.80M 1.55M 7B Model 580K 2.80M 8.80M 1.55M 12.55M 13.73M A.5 Dataset Construction Pipeline Details Current multimodal retrieval research is hindered by fragmented, ad-hoc dataset construction. We introduce unified, object-oriented framework that elevates this process to rigorous science, balancing conceptual clarity with engineering robustness for scalable, reproducible benchmarking. The framework is anchored in canonical tripartite abstraction: every retrieval task is decomposed into Corpus, set of Queries, and Relevance Mapping. This schema, enforced by an abstract base class, is conceptual invariant that ensures structural consistency across all datasets, from text-to-video to complex composed queries, enabling seamless model and evaluation compatibility. The expressiveness of this data construction pipeline stems from polymorphic specialization via inheritance, encapsulated in three core strategies: Modality as Configuration. Modality is dynamic parameter. By overriding single method, dataset effortlessly transitions between modalities (text, image, video, or composite), transforming benchmarks like MSRVTT into image-to-video tasks with minimal code. Task Derivation via Inheritance. Complex tasks are composed of simpler ones. text+image-to-video task inherits its base structure and extends the query schema. Variants are derived by overriding specific data-access methods, turning benchmark creation into modular, hypothesis-driven workflow. Engineering for Scale and Integrity. Scalability and data quality are first-class principles. Large datasets (e.g., InternVid-FLT) leverage chunked, parallel processing. Crucially, proactive curation is embedded: automated validators check video integrity (resolution, frame count), while repair mechanisms fix common errors using ffmpeg, ensuring failures reflect retrieval challenges, i.e, not data corruption. A.6 Prompt for Synthetic Video Retrieval Data Generation To ensure high-quality, diverse, and controllable synthetic video retrieval data generation at scale, we design and deploy suite of structured prompts to instruct MLLMs for captioning. Our pipeline operates in four parts: 18 1. Raw (Or Weakly Annotated) Video (Re-) Captioning: Enhance raw or uncaptioned videos with rich, diverse textual descriptions. 2. Text-Image Composed Retrieval: Generate queries that combine reference images with video content for fine-grained retrieval. 3. Text-Video Composed Retrieval: Generate queries that combine short reference clips with target videos for practical temporal or perspective-based retrieval. 4. Frame Image Captioning: Annotate individual video frames with dynamic-aware captions for auxiliary training signals. All prompts enforce strict output formatting, factual grounding, and stylistic diversity to ensure dataset quality and coverage. {raw_caption} represents an optional, potentially low-quality human-provided or auto-generated caption associated with the video. Placeholders (e.g., {readability} and {education_level}) are dynamically instantiated during batch generation. Besides, we can generate text-to-text retrieval data from the multiple video/frame captions by randomly matching any two sentences in the caption list. Synthetic Video Captioning Prompt Generate 5 distinct and high-quality ENGLISH captions for the provided video. Please first visually understand the video file and analyze the video frame-by-frame in depth before captioning. The original video caption is {raw_caption}. (Ignore if empty.) Each caption must be: 1. The final answer MUST only be JSON dict of captions where the key is the caption number and the value is singleparagraph caption. 2. Factually accurate and descriptive - include only what is clearly visible or reasonably inferable from the video. 3. Focus exclusively on visible content; do not mention absences or speculate about unseen elements. Content Requirements (per caption): 1. Spatial Details (30-60%): Describe location, setting, key environmental elements, objects, and their spatial relationships. Include notable visual features. 2. Temporal Flow (30-60%): Capture event sequence and action dynamics - movements, interactions, transitions - as they unfold over time. 3. Theme/Background/Style/Meaning/Highlight/Camera (0-20%): Describe observable emotional tone, narrative style, thematic elements, highlighting frames, or significant camera movements/angles. 4. Others (0-10%). Key Guidelines: 1. Do not invent fictional elements, dialogue, or backstory not visible in the video. 2. Use the following varied sentence styles (one per caption): - concise and punchy summary, - spatial-temporal richly descriptive, - abstract understanding, - keywords-only, - partially relevant information. 3. Ensure diversity: avoid repetition in wording, focus, or rhythm; at least one caption must be <20 words and one >100 words; balance objectivity with vivid sensory language; randomly omit minor details in 1 or 2 captions. 4. Ensure readability matches {readability} and is appropriate for {education_level} readers. Now generate these captions in strict JSON format: { \"1\": <caption text 1: concise and punchy summary, 10-25 words>, \"2\": <caption text 2: spatial-temporal richly descriptive, 80-200 words>, \"3\": <caption text 3: abstract understanding, 30-100 words>, \"4\": <caption text 4: keywords-only, 5-30 words>, \"5\": <caption text 5: partially relevant information, 10-70 words> } Synthetic Text-Image Composed Video Retrieval Prompt You are an information retrieval expert specializing in high-value text-image composed queries. Your sole objective is to generate queries that significantly enhance video retrieval performance by effectively combining reference image and video content. 19 ## What Makes HIGH-QUALITY Query (Non-Negotiable) truly valuable query must satisfy ALL of these criteria: 1. COMBINATION NECESSITY (Most Critical) - The query MUST become meaningless or significantly less specific if either the image or video is removed. - Example (HIGH-QUALITY): \"the person FROM REFERENCE IMAGE wearing red jacket now skiing\" - Example (LOW-QUALITY): \"a person skiing\" (works without image) 2. SEMANTIC PRECISION - Must accurately reflect BOTH the visual content of the reference image AND the video. - Must reference at least ONE specific visual attribute from the image (not generic descriptions). - Example (HIGH-QUALITY): \"matching the blue hat FROM PHOTO, now running through park\" - Example (LOW-QUALITY): \"someone similar to image moving\" (too vague) 3. RETRIEVAL EFFECTIVENESS - Must narrow search results by at least 50% compared to text-only queries. - Must contain actionable constraints that differentiate from 90% of videos in the database. - Example (HIGH-QUALITY): \"the woman WITH PONYTAIL FROM IMAGE entering building at 2PM\" - Example (LOW-QUALITY): \"a woman walking\" (too broad) 4. LOGICAL COHERENCE - Must maintain subject-verb-object consistency across modalities. - Must avoid semantic contradictions between image attributes and video actions. - Example (HIGH-QUALITY): \"the dog FROM REFERENCE PHOTO chasing ball\" - Example (LOW-QUALITY): \"the red jacket FROM IMAGE running down hill\" (jackets dont run) 5. PRACTICAL UTILITY - Must solve real-world ambiguity that neither text nor image could resolve alone. - Must reflect how actual users would express their information need. - Example (HIGH-QUALITY): \"same person AS IN PHOTO but wearing blue instead of red\" - Example (LOW-QUALITY): \"the image shows person and the video shows action\" (no real combination) ## High-Value Query Generation Framework Follow this structured approach: 1. DEEP ANALYSIS PHASE (Mandatory) a) Reference Image Analysis: - Identify 1-4 SPECIFIC visual attributes (e.g., \"red jacket\", \"ponytail\", \"blue hat\") - Determine primary subject with discriminative features - Note what CANNOT be determined from image (e.g., action, scene) b) Video Content Analysis: - Identify primary action using precise verbs (e.g., \"running\", \"entering\", \"chasing\") - Note scene context and temporal elements - Determine what CHANGES from the static image reference 2. COMBINATION STRATEGY SELECTION Choose ONE primary strategy: A) IDENTITY PRESERVATION + ACTION CHANGE - Structure: [Binding phrase] + [Image attribute] + [Video action] - Example: \"the man FROM REFERENCE IMAGE in blue shirt now running\" B) IDENTITY PRESERVATION + SCENE MIGRATION - Structure: [Binding phrase] + [Image attribute] + [Scene transition] - Example: \"same person AS IN PHOTO moving from office to park\" C) IDENTITY PRESERVATION + NEGATIVE CONSTRAINT - Structure: [Binding phrase] + [Negative constraint] + [Video state] - Example: \"not wearing red jacket FROM IMAGE but blue, walking\" D) RELATIONAL TRANSFER - Structure: [Binding phrase] + [Relationship description] - Example: \"the dog FROM REFERENCE PHOTO chasing ball\" 3. QUALITY ENHANCEMENT TECHNIQUES - Binding Precision: Use \"FROM REFERENCE IMAGE\", not \"like the picture\" - Attribute Specificity: Use concrete features (\"red jacket\", not \"clothing\") - Action Verbs: Use present continuous tense (\"running\", not \"runs\") - Context Enrichment: Add 1 relevant scene descriptor (\"in park\", \"near building\") - Noise Handling: For low-quality inputs, use \"resembling\" but maintain specificity 20 - Audience Adaptation: Ensure readability matches {readability} and suits {education_level} readers. 4. MANDATORY QUALITY CHECK Before finalizing, verify ALL: - [ ] Explicit binding to reference image - [ ] References SPECIFIC visual attribute from image - [ ] Describes DYNAMIC ELEMENT not in static image - [ ] Loses specificity if image removed - [ ] Contains actionable retrieval constraint - [ ] Maintains logical subject-action consistency - [ ] Solves real-world ambiguity ## What to AVOID - Generic descriptions ignoring image specificity - Redundant mentions of obvious image content - Semantic contradictions (e.g., \"the jacket is running\") - Overly precise details not visible in inputs - Standalone video descriptions without image binding - Vague terms: \"something\", \"thing\", \"area\" - Excessive length without added value (>50 words) ## Output Format - Output ONLY JSON object with one key: \"query\" - Value must be the generated query sentence - No additional text or formatting Generate the query for: Reference Image: <image_input> Video Clip: <video_input> Synthetic Text-Video Composed Video Retrieval Prompt You are an information retrieval expert specializing in practical text-video composed queries. Your objective is to generate queries that effectively combine short reference clip with target video for real-world retrieval. ## Practical Context Understanding In real-world scenarios: - Reference is typically SHORT CLIP (2-10 seconds) - Clip is usually HIGHLY RELEVANT to target video (same source or similar content) - Common relationships: temporal continuation, perspective variation, quality differences, minor action variations - Query should reflect how users actually search ## What Makes HIGH-QUALITY Query (Practical Focus) Must satisfy these criteria: 1. USEFUL COMBINATION (Most Important) - Leverages reference clip to specify what text alone cannot - Example (HIGH-QUALITY): \"the same person FROM REFERENCE CLIP continuing to run after the jump\" - Example (LOW-QUALITY): \"a person running\" (ignores reference) 2. PRACTICAL PRECISION - References at least ONE observable feature from reference clip - Example (HIGH-QUALITY): \"matching the red jacket FROM REFERENCE, now entering building\" - Example (LOW-QUALITY): \"someone similar to clip moving\" (too vague) 3. REAL-WORLD UTILITY - Helps find videos difficult to retrieve with text alone - Example (HIGH-QUALITY): \"same action AS IN REFERENCE but from front angle\" - Example (LOW-QUALITY): \"the clip shows action\" (no retrieval value) 4. NATURAL EXPRESSION - Sounds like how real user would phrase it - Example (HIGH-QUALITY): \"what happens right after this moment?\" - Example (LOW-QUALITY): \"temporal continuation of the current visual sequence\" (too academic) ## Practical Query Generation Framework 1. REFERENCE CLIP ANALYSIS - Identify 1-3 KEY OBSERVABLE FEATURES (e.g., \"red jacket\", \"starting pose\", \"mid-action\") - Determine most distinctive visual or action element - Note what is CLEARLY VISIBLE (avoid guessing) 21 2. TARGET VIDEO RELATIONSHIP ASSESSMENT Determine relationship type: A) TEMPORAL CONTINUATION - Reference is earlier part of same sequence - Query focus: \"what happens next\" or \"continuing action\" B) PERSPECTIVE VARIATION - Same action from different angle/view - Query focus: \"same action from different angle\" C) QUALITY/CONDITION VARIATION - Same action with different lighting/resolution - Query focus: \"same scene in better lighting\" D) MINOR ACTION VARIATION - Slightly different execution of similar action - Query focus: \"same person but running instead of walking\" 3. QUERY CONSTRUCTION - Start with binding phrase: \"FROM REFERENCE CLIP\", \"AS IN REFERENCE\", etc. - Reference 1-2 specific observable features from clip - Describe relationship to target video clearly - Keep natural and practical (5-15 words typically) - For temporal: focus on \"what happens next\" - For perspective: specify desired viewpoint - For quality: specify desired condition - For action: specify the change 4. PRACTICAL QUALITY CHECK Ask before finalizing: - Would this help me find what Im looking for? - Does it add value beyond describing the target? - Would real user phrase it this way? - Is everything mentioned clearly visible in reference? ## What to AVOID - Overly academic or technical language - References to features NOT clearly visible - Excessive precision about timing (\"exactly 3.2 seconds later\") - Queries that work equally well without reference - Generic descriptions: \"similar video\", \"related content\" - Making unsupported assumptions ## Output Format - Output ONLY JSON object with key: \"query\" - Value must be the generated query string - No additional text, explanations, or formatting Generate the query for: Reference Clip: <reference_clip_input> Target Video: <target_video_input> Synthetic Frame Captioning Prompt Generate 5 distinct and high-quality ENGLISH captions for the provided image (a frame extracted from video) based solely on visual content. Please first visually understand the image and analyze it in depth before captioning. Each caption must be: 1. The final answer MUST only be JSON dict of captions where the key is the caption number and the value is singleparagraph caption. 2. Factually accurate and descriptive - include only what is clearly visible; captions should also be consistent with the short-term video context. 3. Focus exclusively on visible content; do not mention absences or speculate about unseen context. Content Requirements (per caption): 1. Spatial Details (50-70%): Describe location, setting, key objects, spatial relationships, colors, lighting, composition. Include instantaneous action states derived from visible posture/motion cues. 2. Temporal Snapshots (0-30%): Describe the frozen moments temporal state (movements, interactions, transitions) ONLY if visually provable. Avoid implying sequence, duration, or speed. 22 3. Theme/Style/Composition (10-20%): Cover emotional tone, camera angle, lighting style, or artistic elements directly observable. 4. Others (0-10%). Key Guidelines: 1. Do not invent fictional elements or backstory. If action is ambiguous, describe neutrally. Never use future/past tense or speculative phrases. 2. Use varied sentence styles (randomly assign one per caption): - concise spatial-temporal snapshot, - spatially rich descriptive, - abstract spatial interpretation, - keywords with temporal anchors, - minimalist spatial focus. 3. Ensure diversity: avoid repetition in wording, focus, or rhythm; at least one caption <20 words and one >100 words; balance objectivity with vivid sensory language; randomly omit minor details in 1-2 captions. 4. Ensure readability matches {readability} and is appropriate for {education_level} readers. Now generate these captions in strict JSON format: { \"1\": <caption text 1: concise and punchy summary, 10-25 words>, \"2\": <caption text 2: spatial-temporal richly descriptive, 50-200 words>, \"3\": <caption text 3: abstract understanding, 30-100 words>, \"4\": <caption text 4: keywords-only, 5-30 words>, \"5\": <caption text 5: partially relevant information, 10-70 words> } Prompt Utilization. These prompts are designed for scalable, schema-constrained generation pipeline for synthesizing diverse video-centric annotations and queries. The system unifies four tasks under single modular framework. Each task is governed by structured prompt template with dynamic control slots (e.g., readability, education level), ensuring linguistic and semantic diversity. Input modalities (image, video, or both) are automatically routed and embedded via distributed multimodal LLM (Keye-VL-8B Team et al. (2025), 32K context), with outputs rigorously validated against JSON schemas for structural correctness. The pipeline supports sharded, resumable batch generation with quality-aware sampling, enabling the production of millions of grounded, stylistically varied synthetic instances. A.7 Model Architecture Details The GVE model is architecturally derived from Qwen2.5-VL Bai et al. (2025), repurposed as fixed-length multimodal encoder by removing its autoregressive head. Its core function is to map arbitrarily composed inputstext, image, or videointo shared d-dimensional embedding space, preserving cross-modal alignment inherited from pretraining. for images, and Tv = (THW/p2 Input fusion begins with tokenization and visual encoding. Text is converted into sequence of token IDs Xt ZBTt , while images and video frames are encoded into visual token sequences Xv RBTvd, where s) for videos with uniformly sampled frames. Note that Tv = THW/p2 frame embeddings will be added with absolute time encoding. Critically, must satisfy mod pt = 0 (default: pt = ps = 2) to maintain alignment with the vision encoders 3D spatiotemporal grid. The processor then injects these visual tokens into the textual sequence by replacing placeholder tokens (<image>, <video>), producing fused input Xfused RBT d, where = Tt + (cid:80) Tv. This scatter-based fusion preserves positional coherence and enables interleaved modality composition, which is essential for compositional query understanding in multimodal embeddings. The final embedding e(i) Rd for the i-th instance is extracted from the last attended token in the sequence: e(i) = h(i) pi h(i) pi , where pi = max{j M(i) = 1}, and M(i) is the attention mask for instance i. This position corresponds to the EOS token in left-padded sequences or the final non-pad token in right-padded ones. The choice is motivated by the observation that in instruction-tuned MLLMs, the final token often encapsulates the models response intent, making it semantically aligned with the users retrieval goal. 23 A.8 Training Implementation and Hyperparameters Parameter-Efficient Tuning. To facilitate parameter-efficient fine-tuning (PEFT), we employ Low-Rank Adaptation (LoRA) Hu et al. (2021). Our strategy involves targeted application of LoRA to the language components of the model, specifically the q_proj, v_proj, k_proj, up_proj, down_proj, and gate_proj modules. Crucially, the entire visual backbone and the base token embedding layer are kept frozen. This approach focuses adaptation on high-level semantic and cross-modal reasoning while preserving the powerful pretrained visual features. We enable FlashAttention-2 Dao (2023) to accelerate training and reduce memory. The final embedding for each input is obtained via last-token pooling on the last hidden layer, followed by L2 normalization to project embeddings onto the unit hypersphere for stable cosine similarity computations. Optimizer and Training Dynamics. The model is trained using the AdamW Loshchilov and Hutter (2017) optimizer with learning rate of 3 105 and weight decay of 0.1. cosine learning rate scheduler is used. Training is performed in BFloat16 (bf16) mixed-precision. The entire training process is managed under the DeepSpeed framework. For video inputs, we uniformly sample 8 frames per clip at rate of 1.0 FPS. By default, we use 32 NVIDIA A100 GPUs, each with 80GB of memory. Therefore, the overall batch size is at least 1024. Memory Optimization and Distributed Strategy. To manage GPU memory, we enable gradient checkpointing, which recomputes intermediate activations during the backward pass. While our framework supports more advanced techniques like Gradient Cache, it was disabled in favor of this standard approach. To leverage our multi-GPU setup, we enable cross-device negative sharing. This strategy gathers embeddings from all GPUs, effectively multiplying the pool of in-batch negatives by the number of devices. This enriches the negative set for the contrastive loss computation on each GPU, leading to stronger training signal without increasing per-device memory load. Contrastive Learning and Stability. The core of our training is an InfoNCE-style contrastive loss with temperature of 0.03. The use of cross-device negatives starts from the first step. We also enhance our training logs with contrastive-specific metrics, including the average scores of positive pairs and the average margin between positive and hard-negative pairs, providing crucial insights into the models learning dynamics. Hyperparameter Summary. comprehensive summary of all key hyperparameters is provided in Table 9. A.9 Baseline Details Here we present more details of baseline models tested on our benchmark in Table 10, including full model names, abbreviations, architectures, sizes, and training data types. Based on these properties, we analyze to discover potential performance knowledge and dependencies in our experimental part. A.10 Training Dynamics We monitor four key metrics during training: Training Loss, Mean Score, Max Negative Gap, and Mean Positive Score in Figure 9 and Figure 10. To ensure robust visualization, we mitigate outlier effects, followed by 200-step moving average smoothing. Original trajectories (subsampled every 100 steps) are plotted alongside smoothed trends, with 1 standard deviation bands indicating local volatility. A.11 Experiments of Training-time Scaling: More Results of Data Scaling Along with Section 4.2, we depict more experiments for six abilities in data scaling for GVE-3B and GVE-7B in Figure 11. Shaded bands show 1 std; dashed curves are log-linear fits for visual guidance. X-axis is log-scaled to reflect scaling law dynamics. We fit logarithmic scaling law = ln + to quantify gains per decade of data (10 increase). GVE-3B consistently shows higher relative gains: in compositional retrieval (CMP), it improves by +0.039 (14.7%), nearly double GVE-7Bs +0.025 (8.7%); in coarse-grained tasks (CG), it gains +0.057 (11.1%), far exceeding 24 Table 9 Key hyperparameters used for model training. Parameter Value Model & LoRA Architecture Base Model PEFT Method LoRA Rank (r) LoRA Alpha (α) LoRA Dropout LoRA Target Modules Frozen Components Optimizer & Training Optimizer Learning Rate Weight Decay LR Scheduler Training Epochs Precision Gradient Checkpointing Seed Contrastive Learning Temperature In-batch Negatives Cross-Device Negatives Data & Preprocessing Video Frames per Clip Video Sampling Rate Dataloader Workers Qwen2.5-VL-3B-Instruct or Qwen2.5-VL-7B-Instruct LoRA 16 32 0.1 q_proj, v_proj, k_proj, up_proj, down_proj, gate_proj Visual Backbone, Token Embeddings AdamW 3 105 0.1 Cosine 3 BF16 Enabled 0.03 Enabled Enabled 8 1.0 FPS 1 Table 10 Model Abbreviations, Architectures, Parameter Sizes, and Training Data Types. The checkmark () indicates the model was trained on the corresponding data pair types. Full Model Name Abbreviation Architecture Size Text-Text Text-Image Text-Video Contrastive Training Data Pairs CLIP-based CLIP4Clip CLIP4Clip ViCLIP CLIP-based ViCLIP VideoCLIP-XL VideoCLIP-XL CLIP-based LanguageBind-Video-Huge-V1.5 LanguageBind CLIP-based InternVideo2-1B CLIP-based InternVideo2-Stage2-1B InternVideo2-6B CLIP-based InternVideo2-Stage2-6B 87M 0.4B 0.4B 1.2B 1.4B 6.4B gme-Qwen2-VL-2B-Instruct Unite-Base-Qwen2-VL-2B VLM2Vec-V2.0 BGE-VL-v1.5-mmeb UniME-LLaVA-OneVision-7B B3-Qwen2-7B gme-Qwen2-VL-7B-Instruct Unite-Base-Qwen2-VL-7B GVE-3B GVE-7B MLLM-based 2.2B GME-2B Unite-2B MLLM-based 2.2B VLM2Vec-V2 MLLM-based 2.2B MLLM-based 7.6B BGE-VL MLLM-based 8.0B UniME-7B MLLM-based 8.3B B3-7B MLLM-based 8.3B GME-7B MLLM-based 8.3B Unite-7B GVE-3B GVE-7B MLLM-based 3.8B MLLM-based 8.3B - - - - - - - - - - - - - - - - - GVE-7Bs +0.037 (6.6%). Textual (TXT) and fine-grained (FG) retrieval follow similar trends, with GVE-3B improving by +0.042 (7.1%) and +0.040 (7.9%), respectively, versus GVE-7Bs +0.037 (5.8%) and +0.031 (5.6%). Visual retrieval (VIS) scales weakly for both (+0.024/3.8% for GVE-3B, +0.013/2.1% for GVE-7B). Notably, only in long-context retrieval (LC) does GVE-7B outperform GVE-3B in both absolute (+0.042 vs. +0.029) and relative gain (5.4% vs. 3.8%). This reveals task-dependent scaling trade-off: smaller models (GVE-3B) scale more efficiently in semantic and compositional tasks, while larger models (GVE-7B) uniquely excel in long-context modeling suggesting that model size should be chosen not only for capacity, but for alignment with the data-scaling profile of the 25 Figure 9 Training dynamics of GVE-3B across four metrics. Figure 10 Training dynamics of GVE-7B across four metrics. target task. 26 Figure 11 Performance effect from data scaling for GVE-3B and GVE-7B in detail. A.12 Experiments of Test-time Scaling We investigate the impact of scaling two test-time parameters: the number of sampled frames and the maximum tokens per frame (i.e., effective resolution). We analyze each parameter in isolation: when scaling the frame count from 8 to 48, the maximum token count is fixed at 200; conversely, when scaling the token count from 200 to 800, the frame count is fixed at 8. Temporal Scaling: the Number of Sampled Frames. As shown in Figure 12, increasing the number of sampled frames generally improves performance. The most substantial gains appear in the Long-Context (LC) task (GVE-3B: +19.6%, GVE-7B: +12.8%), underscoring the value of denser temporal sampling for long-range reasoning. However, the Compositional (CMP) task shows slight performance degradation, suggesting its sensitivity to potentially redundant visual cues from excessive frames. Furthermore, while GVE-7B consistently outperforms GVE-3B, the performance gap narrows as frames increase (from 0.029 to 0.024 on AVG-D), indicating that the larger model is more efficient at extracting information from sparser inputs. In contrast, increasing the token budget yields nonSpatial Scaling: the Maximum Tokens Per Frame. monotonic returns (Figure 13). Performance for most tasks peaks around 400 tokens and then declines. For instance, in the LC task, both models scores drop when moving from 400 to 800 tokens. This suggests that for long-context tasks, excessive per-frame detail can dilute attention from salient features rather than aid understanding. The CMP tasks performance again deteriorates with more tokens, reinforcing its sensitivity to input redundancy. Key Findings and Implications. Our scaling analysis reveals four key findings: Temporal Scaling (More Frames): Provides reliable, though diminishing, performance boost, especially for long-range tasks. Spatial Scaling (More Tokens): Exhibits clear optimal point (approx. 400 tokens). Exceeding it degrades performance, indicating trade-off between detail and distraction. Figure 12 Model performance as function of the number of sampled frames at inference time (max tokens fixed at 200). Figure 13 Model performance versus the maximum number of tokens per frame (frame count fixed at 8), determining the resolution of frames in resizing. Model Scale Efficiency: The larger model (GVE-7B) benefits less from input scaling, suggesting greater intrinsic capacity to process sparse inputs effectively. For test-time enhancement, temporal scaling is robust strategy, whereas naive spatial scaling is not. This distinction underscores critical insight: performance is not merely function of total input data, but of its effective composition. It strongly motivates shift from brute-force data scaling towards adaptive input mechanisms, such as dynamic token or frame selection, that can intelligently manage information density. A.13 Experiments of Video Classification We test the video embeddings for video classification by following the MMEB-V2 Meng et al. (2025). Note that we do not train on these datasets for zero-shot evaluations, while other baselines may include them in optimization. Specifically, we evaluate video embedding models on five diverse classification benchmarks, including Kinetics-700 (K700) Carreira et al. (2019), UCF101 Soomro et al. (2012), HMDB51 Kuehne et al. (2011), SomethingSomething-V2 (SSV2) Goyal et al. (2017), and Breakfast Kuehne et al. (2014), covering fine-grained interactions, open-domain actions, and procedural activities. Each task is cast as video-to-text retrieval over the full label set, using standardized validation/test splits. The results are provided in Table 11. Our results show that LanguageBind achieves the highest mean accuracy (0.553), followed closely by GVE7B (0.526) and InternVideo2-6B (0.526), demonstrating the efficacy of unified multimodal representations. However, performance remains suboptimal on temporally complex datasets such as Breakfast (best: 0.453) and fine-grained SSV2 (best: 0.569), revealing critical bottleneck in modeling dynamic physical interactions. Table 11 Video classification accuracy comparison across different models and datasets. For each column: highest score is bolded, second-highest is underlined. Model AVG UCF101 HMDB51 SSV2 Breakfast CLIP4Clip InternVideo2-6B VLM2Vec-V2 GME-7B UniME-7B Unite-7B GVE-3B GVE-7B 0.378 0.526 0.393 0.374 0.306 0.519 0.476 0.526 0.395 0.554 0.380 0.397 0.388 0. 0.489 0.540 0.596 0.572 0.600 0.547 0.377 0.752 0.661 0.757 0.413 0.480 0.409 0.479 0.407 0. 0.483 0.525 0.308 0.569 0.428 0.306 0.190 0.513 0.471 0.521 0.176 0.453 0.148 0.143 0.166 0. 0.277 0.289 A.14 Limitations Our study has several practical limitations that stem from scope boundaries and experimental design. First, all evaluations are conducted in vision-only setting, excluding audio, transcripts, or metadata. While this aligns with standard practice in video-text retrieval, it limits applicability to real-world scenarios where multimodal cues (e.g., sound) are essential for disambiguation. Second, inference uses fixed protocol: videos are uniformly sampled into exactly 8 frames, and input sequences are truncated at 8192 tokens. This may disadvantage tasks requiring adaptive frame selection (e.g., sparse event detection) or longer context (e.g., hour-long videos). Third, although UVRB covers 16 datasets across diverse tasks, it does not include specialized domains such as medical, industrial, or surveillance videos, where visual semantics and query intent differ significantly from general-domain content. Finally, training GVE, especially the 7B variant, requires substantial computational resources, limiting accessibility for researchers with constrained infrastructure. Efficient variants and training strategies are left for future work."
        }
    ],
    "affiliations": [
        "AI Thrust, HKUST(GZ)",
        "Tongyi Lab, Alibaba Group"
    ]
}