{
    "paper_title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation",
    "authors": [
        "Vadim Kurochkin",
        "Yaroslav Aksenov",
        "Daniil Laptev",
        "Daniil Gavrilov",
        "Nikita Balagansky"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 5 2 2 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Train Sparse Autoencoders Efficiently by Utilizing\nFeatures Correlation",
            "content": "Vadim Kurochkin1,2 Yaroslav Aksenov1 Daniil Laptev1,2 Daniil Gavrilov1 Nikita Balagansky1,2 Equal contribution 1T-Tech 2Moscow Institute of Physics and Technology"
        },
        {
            "title": "Abstract",
            "content": "Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework."
        },
        {
            "title": "Introduction",
            "content": "Interpreting dense embeddings from large language models remains central challenge for transparency and controllability in AI systems [Elhage et al., 2023, Heap et al., 2025]. Sparse autoencoders (SAEs) have emerged as powerful tools for uncovering human-interpretable features within neural activations by enforcing activation sparsity to induce discrete-style dictionaries [Elhage et al., 2023, Gao et al., 2025, Cunningham et al., 2024]. These dictionaries facilitate rigorous circuit-level semantic analysis and concept discovery, enabling fine-grained probing of model internals [Elhage et al., 2023, Cunningham et al., 2024]. However, naively scaling SAEs to the widths demanded by modern transformers leads to prohibitive compute costsboth in memory and per-step FLOPslimiting their applicability to frontier models. Gated SAEs [Rajamanoharan et al., 2024a] address this by learning continuous sparsity masks via lightweight gating networks, achieving Pareto improvement on the reconstructionsparsity trade-off. Switch SAEs [Mudide et al., 2025] leverage conditional computation by routing activations among smaller expert SAEs, reducing computation by activating only subset of experts per input. In work Gao et al. [2025] an acceleration of TopK SAE was proposed that utilizes an optimized kernel based on efficient sparsedense matrix multiplication. Despite these advances, the encoder remains largely unoptimized: it still performs dense projection into the full dictionary, incurring high computational cost and limiting scalability. Corresponding author: y.o.aksenov@tbank.ru Preprint. Under review. Figure 1: Isoflops Pareto fronts for KronSAE vs. TopK SAE on Qwen-1.5B for different dictionary sizes . 100 tokens: KronSAE improves explained variance while heavily reducing parameters count. 500 tokens: maintains gains across most sizes. 1000 tokens: matches baseline quality with significantly fewer parameters; Stars denote TopK baselines and markers denote KronSAE configurations (m, n, h). See details in section 4.1. In this paper, we introduce KronSAE, novel SAE encoder that directly tackles this bottleneck. By decomposing the latent space into head-wise Kronecker factors and embedding smooth, differentiable AND-like gating mechanism, KronSAE reduces both parameters and compute overhead while preserving reconstruction fidelity. This work makes three primary contributions: 1. We identify the encoder projection as the principal scalability bottleneck in sparse autoencoders, conducting FLOPs-aware analysis of its impact and demonstrating that targeted encoder optimizations can significantly improve computational performance while maintaining reconstruction quality. 2. We propose KronSAE, Kronecker-factorised sparse autoencoder equipped with the novel mAND activation function. The design significantly reduces encoder cost and is compatible with existing sparse decoder kernels. 3. We show on multiple language models that KronSAE improves reconstruction fidelity, decreases feature absorption, and yields more interpretable latents under fixed compute."
        },
        {
            "title": "2 Related Work",
            "content": "Sparse Autoencoders. Sparse autoencoders (SAEs) enforce sparsity constraint on latent code. Early work demonstrated that SAEs can uncover human-interpretable directions in deep models [Cunningham et al., 2024, Templeton et al., 2024], but extending them to the large latent sizes (F d) for modern language models with large hidden dimension is expensive: each forward pass still requires dense O(F d) encoder projection. Most prior optimization efforts focus on the decoder side. For example, Gao et al. [2025] introduce fused sparsedense TOPK kernel that reduces wall-clock time and memory traffic, while Rajamanoharan et al. [2024a] decouple activation selection from magnitude prediction to improve the ℓ0MSE trade-off. Separately, Rajamanoharan et al. [2024b] propose JUMPRELU, nonlinearity designed to mitigate shrinkage of large activations in sparse decoders. Conditional Computation. Conditional-computation schemes avoid instantiating full dictionary per token by routing inputs to subset of expert SAEs via lightweight gating network [Mudide et al., 2025] by employing the Mixture-of-Experts ideas [Shazeer et al., 2017], but still incur dense per-expert encoder projection, leaving the encoder as the primary bottleneck. Weight Factorization and Logical Activation Functions. Outside of sparse decoders and Mixtureof-Experts, tensor-factorization methods have been used to compress large weight matrices in language models [Edalati et al., 2021]. In parallel, differentiable-logic activations were introduced to approximate Boolean operators in smooth manner [Lowe et al., 2021]. Our method synthesizes 2 these lines of work: we embed differentiable AND-like gate into Kronecker-factorized encoder to build rich, compositional features while preserving end-to-end differentiability."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Let Rd denote an activation vector drawn from pretrained transformer. conventional TopK SAE [Gao et al., 2025] produces reconstruction ˆx of via = TopK(cid:0)Wencx + benc (cid:1), ˆx = Wdecf + bdec, (1) where Wenc RF and Wdec RdF are full-rank matrices and RF retains the largest activations. The encoder cost therefore scales as O(F d) per token. 3.2 KronSAE In contrast to prior work that (i) accelerates decoding or (ii) sparsifies computation through routing, KronSAE directly reduces the encoders computational complexity by decomposing the Wenc matrix into head-wise Kronecker factors and introducing multiplicative mAND kernel. We decompose the latent space into independent components (e.g. heads). Each head is parameterised by the composition of two thin matrices Rmd (composition base) and Qk Rnd (composition extension), with dims < and = n. The pre-latents pk = ReLU(P kx), qk = ReLU(Qkx) are combined through an element-wise interaction kernel independently in each head, i,j := mAND(pk zk , qk ) := 0, qk pk , pk > 0 and qk otherwise, > 0, (cid:40)(cid:113) (2) (3) where zk Rmn. Flattening and concatenating across heads yields post-latents RF , to which the usual TopK operator is applied. The mAND kernel smoothly approximates Boolean AND gate, ensuring non-zero activation only if both inputs are positive, while preserving gradient flow and activation magnitude for stable reconstruction. Further insights into the behavior of mAND are provided in Appendix C. The encoder cost per token drops from O(F d) to O(cid:0)h(m + n)d(cid:1) (see Appendix A.2). KronSAE thus reduces FLOPs and parameter count without routing overhead, and is orthogonal to existing sparse decoder kernels [Gao et al., 2025] and thus can be combined with them for end-to-end speed-ups."
        },
        {
            "title": "4 Experiments",
            "content": "We train SAEs on the residual streams of Qwen-2.5-1.5B-Base [Yang et al., 2024], Pythia-1.4Bdeduped [Biderman et al., 2023], and Gemma-2-2B [Team et al., 2024] language models. Activations are collected on FINEWEB-EDU, filtered subset of educational web pages from the FineWeb corpus [Penedo et al., 2024]. We measure reconstruction quality via explained variance (EV), EV = 1 Var(x ˆx) Var(x) , so that 1.0 is optimal, and use automated interpretability pipeline [Bills et al., 2023, Paulo et al., 2024] and SAE Bench [Karvonen et al., 2025] to evaluate properties of SAE features. Our experiments (see detailed setup in Appendices and D) address three questions: 1. Does KronSAE yield higher EV than TopK SAE under fixed compute? 2. Which design choices (nonlinearity, (m, n, h)) drive EV improvements? 3. How do these choices affect properties and interpretability of learned features?"
        },
        {
            "title": "Dictionary size m n",
            "content": "h"
        },
        {
            "title": "Explained Variance",
            "content": "32768 65536 2 4 4096 8 1024 2 4 8192 8 2048 mAND(u, v) ReLU(u) ReLU(v) mAND(u, v) ReLU(u) ReLU(v) mAND(u, v) ReLU(u) ReLU(v) mAND(u, v) ReLU(u) ReLU(v) 0.83356 0.82669 0.82373 0.82197 0.81912 0.81432 0.84445 0.83276 0.82974 0.83503 0.82968 0.8251 Table 1: Explained variance for KronSAE using different composition activations under 1B-token training budget. Bold values indicate the best performance of mAND across composition kernels. 4.1 Scaling Laws We examine the scaling behavior of KronSAE against conventional TopK SAE on Qwen-1.5B under equivalent FLOPs budgets. Models are grouped by dictionary size {215, 216, 217}, and each KronSAE configuration (m, n, h) matching hidden features is trained under token budgets adjusted to match the per-step FLOPs of training TopK SAE for 100 M, 500 M, and 1000 tokens (see FLOPs calculation details in Appendix A). Figure 1 plots explained variance versus trainable parameter count across these isoflops budgets. Under 100 token budget, all KronSAE variants exceed the TopK baseline in reconstruction quality, achieving up to 4.3% increase in explained variance with roughly 54.7% fewer parameters. At 500 tokens, KronSAE maintains 0.8% advantage and 43.8% fewer parameters across most model sizes, and the compositional head structure keeps smaller configurations on or above the baseline Pareto front. With 1000 tokens, although the TopK SAE narrows the gap, KronSAE still matches baseline reconstruction quality with approximately 46.1% fewer parameters. Smaller composition base dimension (m = 2) show especially strong returns under tight compute constraints. We observed similar results on Pythia-1.4B (see appendix A.3). 4.2 Ablations Figure 2: Iso-FLOPs ablation: EV against head count and base dimension for KronSAE under 100M, 500M, and 1B token budgets. Higher and smaller yield improved reconstruction quality. Composition activation. To isolate the impact of our mAND operator, we compare it to two simpler interaction kernels: (i) the element-wise product of ReLUs, ReLU(u) ReLU(v), and (ii) the raw product v. As reported in Table 1, under 1B-token training budget and for both (m, n, h) = (2, 4, 4096) and (4, 8, 1024) configurations, the mAND variant consistently achieves the highest explained variance, outperforming the alternatives by clear margin. Decomposition hyperparameters. Using the iso-FLOPs setup from Section 4.1, we systematically vary the number of heads and the per-head base dimension (with = F/(mh)). Figure 2 shows that for large training budgets (500M1B tokens), smaller (and thus larger n) yields higher reconstruction quality, since smaller base dimension frees capacity for more expressive extension features. Under the tighter 100M-token regime, however, configurations with = 4 outperform those with = 2 or = 8, indicating trade-off between per-head representational richness and data efficiency. Furthermore, fixing and increasing yields near-linear gains in explained variance. This corresponds to our intuition that each head acts as component of the feature-space decomposition, efficiently storing group of correlated features and allowing more fine-grained routing of activation patterns. Sparsity and depth-wise position. Finally, we assess KronSAEs robustness to different sparsity levels and layer depths. Figure 3a compares KronSAE and TopK SAE across range of L0 sparsity settings on the 14th layer of Qwen-2.5-1.5B and the 12th layer of Gemma-2-2B; Figure 3b evaluates performance across layers in Qwen-2.5-1.5B. In every case, KronSAE matches or exceeds the reconstruction quality of the TopK baseline under the same FLOPs budget, demonstrating that our Kronecker-factorized encoder maintains its advantages regardless of sparsity level or depth. 4.3 Absorption The primary challenge in interpretability is feature absorption, where one learned feature becomes strict subset of another (e.g. Lion feature is entirely subsumed by starts with feature) and consequently fails to activate on instances that satisfy the broader concept but not its superset representation [Chanin et al., 2024]. Figure 4 reports three absorption metrics across sparsity levels ℓ0 {16, 32, 64, 128, 256}: (1) the mean absorption fraction, measuring the proportion of features that are partially absorbed; (2) the mean full-absorption score, quantifying complete subsumption events; and (3) the mean number of feature splits, indicating how often single conceptual feature fragments into multiple activations. Across all ℓ0, KronSAE variants consistently reduce both the absorption fraction and the full-absorption score relative to the TopK SAE baseline, while maintaining similar rate of feature splits. We attribute KronSAEs improved disentanglement to two complementary design choices: 1. Smooth mAND activation. By emitting nonzero output only when both pre-latents are positive, we introduce differentiable AND gate that prevents broadly polysemantic primitive from entirely subsuming more specific one. Consequently, composite post-latents fire mainly in the intersection of their constituent concepts, encouraging each pre-latent to specialize on single semantic mode rather than inherit its parent activation region. 2. Head-wise Cartesian decomposition. Dividing the latent space into independent subspaces (each with its own grid of primitive interactions) ensures that specialized concepts (such as elephant) are confined to single head and cannot fully absorb more general concepts (such as starts with E) in another. Together, these mechanisms produce more monosemantic features, simplifying downstream causal interventions and targeted probing. Notably, the mean number of feature splits remains comparable to the TopK baseline, as Cartesian decomposition alone does not inherently alter the fragmentation of individual primitives (see Section 5.2)."
        },
        {
            "title": "5 Analysis",
            "content": "In this section we investigate the properties of our architecture and of learned features, and compare it to TopK architecture. We show that pre-latents interactions are closely resemble the logic AND gate. 5 (a) (b) Figure 3: (a) EV versus sparsity level L0 for KronSAE and TopK SAE on the 14th layer of Qwen2.5-1.5B and the 12th layer of Gemma-2-2B under iso-FLOPs constraints. (b) EV across layers of Qwen-2.5-1.5B, demonstrating that KronSAE matches TopK performance regardless of depth. Figure 4: Feature absorption metrics on Qwen-2.5-1.5B. KronSAE configurations (various m, n) exhibit lower mean absorption fractions and full-absorption scores across different ℓ0. 5.1 Toy Model of Superposition To evaluate how well different sparse autoencoder architectures recover known correlation patterns, we construct controlled experiment using synthetic, block-structured covariance model. Input vectors xsparse R256 sampled under heavy-tailed Bernoulli distribution with probability = 0.875 that value will be zero. We perform Cholesky decomposition = LL on the covariance matrix and set xsparse = xsparse, so that xsparse exhibits the desired structure. We train autoencoder (AE) to reconstruct xsparse. For this we use small toy model following the [Elhage et al., 2022]: ˆx = ReLU(W xsparse + b). (4) We collect hidden states (W xsparse) from AE and then train TopK SAE and our proposed KronSAE with = 256 and topk = 4 to reconstruct it. After training, we extract the decoder weight matrices Wdec from each model and compute the covariance Cdec = WdecW dec. To compare TopK SAE embeddings to the AE reference, we match atoms by minimal Euclidean distance, ensuring fair alignment before analysis. Result is shown in Figure 5. To quantify how closely each models feature correlations mirror the ground-truth structure S, we employ the RV coefficient, defined as RV (S, C) = trace(SC)/(cid:112)trace(S2) trace(C 2) and assess its significance via permutation test. In our experiments, KronSAE consistently achieves RV = 0.358 with p-value = 0.0002 while TopK SAE which achieved RV = 0.038 with p-value = 0.31 does not show any structure at all. Also, we try to find the nearest pattern in TopK SAE that matches its feature embeddings with learned features in AE. This setup has better score RV = 0.080 with p-value = 0.001, but still much less than KronSAE. This indicates that our compositional encoder more faithfully reconstructs the original feature relation. See additional experiments in appendix B. 6 Figure 5: We generate data with covariance matrix that consist of blocks with different sizes on diagonal and off diagonal (left panel). We then examine the decoder-weight covariance Wdec dec to assess feature-embedding correlations. Panels (middle panels) show that TopK SAE recovers these correlation structures only weakly, even after optimal atom matching. In contrast, KronSAE (right panel) more accurately reveals the original block patterns. Figure 6: Correlations between features in KronSAE with = 4, = 4 within head and with features from other heads. Our design induces higher correlations within group, which also gets stronger after training, although SAE have also learned correlated features from different heads. 5.2 Correlations in SAEs Trained on Language To examine the correlation structure of features learned in our SAE, we have calculated the correlations on 5k texts from the training dataset. For each feature we calculate the mean correlation with features within its group (head) and with all other features, and compare the randomly initialized KronSAE with = 4, = 4 with the trained one. It turns out, as shown in Figure 6, that correlations are indeed significantly higher within single head and higher than for random SAE, which suggest that our choice to impose the correlated structure in SAE latents works as intended. 5.3 Analysis of Learned Features We have compared our KronSAE and TopK architecture in terms of interpretability and feature properties, and we have analyzed the properties of groups in KronSAE. For this, we choose the 14th layer of Qwen2.5-1.5B and dictionary size of 32k features, of which the first 3072 were selected. KronSAE was chosen with = 4, = 4. We run for 24M tokens total to collect data. Our interpretation pipeline follows the common methodology: LLM interprets the activation patterns [Bills et al., 2023] and we evaluate obtained interpretations using the detection score and the fuzzing score Paulo et al. [2024]. For each selected feature, among the standard mean activation value and frequency, we calculate two additional metrics. Low values of token entropy suggest that feature activates more frequently on small number of tokens, thus it is token-specific; high value of multitoken ratio indicates that feature tends to activate multiple times in single sentence. We have observed that both these metrics have non-negligible negative correlation with the final interpretability scores. 7 Figure 7: Distribution of properties for TopK SAE and KronSAE (m = 4, = 4) with 32k dictionary size trained on Qwen2.5-1.5B. Pre and Post suffixes denote preand post-Cartesian product latents. Our SAE achieves better interpretability scores by learning specialized feature groups, indicated by lower activation frequency and lower variance in activated tokens. For more details on the data collection and interpretation pipeline, see Appendix D. For additional analysis of properties of learned features, see Appendix E. SAE properties and encoding mechanism. We observe that the features learned by KronSAE are more specific, indicated by lower values of the computed metrics and higher interpretability scores, as shown in Figure 7. Since post-latents are significantly more interpretable than corresponding prelatents, we hypothesize the hidden mechanism for encoding and retrieval of the required semantics. By examining activating examples and interpretations of latents, we observe that pre-latents may carry multiple distinct and identifiable modes of activation, such as composition base element 3 in head 23 shown in Table 2, and be very abstract compared to resulting post-latents. Polysemanticity of pre-latents is expected to be consequence of reduced \"working\" number of encoder latents, since we decompose the full dictionary size and reduce the encoder capacity. Thus, we hypothesize that the encoding of specific semantics in our SAE may be done via magnitude, which we validate by examining the activation examples. For the above mentioned pre-latent, the \"comparison\" part is encoded in the top 75% quantile, while the \"spiritual\" part is mostly met in the top 25% quantile, and the \"geographical\" part is mainly encoded in the interquartile range. We also consider but do not investigate the possibility that it may depend on the context, e.g. when the model uses the same linear direction to encode different concepts when different texts are passed to it. Semantic retrieval and interpretable interactions. Heads usually contain groups of semantically related pre-latents, e.g. in head 136 there are three base elements and one extension covering numbers and ordinality, two extension elements related to geographical and spatial matters, one questionrelated base and one growth-related extension. Interestingly, most post-latents for this head have higher interpretability score than both its parent pre-latents, which is unusual. The retrieval happens primarily via the mechanism closely resembling the logical AND circuit, where some pre-latent works as the bearer of multiple semantics, and the corresponding pre-latent (base or extension) works as specifier. An illustrative example is shown in Table 2: we see that the base contains three detectable sub-semantics, and each extension then retrieves the particular semantics. Other types of interaction may occur, such as appearance of completely new semantics, for example composition between base 3 and extension 1 in Table 2 where medical terminology arises and could not be interpreted as simple intersection between two pre-latents semantics. Another example is case of head 3 where base 3 has sub-semantics related to technical instruments and extension 2 have semantics related to the posession and necessity, and their combination gives the therapy and treatment semantics which looks more like addition than intersection. It is frequent case that post-latent inherit semantics of only one parent, or the impact of another parent is not detectable, which usually happens if parent has very broad interpretation and low score. However, it requires more sophisticated techniques to properly identify the fine-grained structure"
        },
        {
            "title": "Interpretation",
            "content": "Base 3 Suffix -like for comparative descriptors, directional terms indicating geographical regions, and concepts related to spiritual or metaphysical dimensions Extension elements and their compositions with base 3 Extension 0 Extension 1 Extension Interpretation: Comparative expressions involving than and as in contrastive or proportional relationships. Composition: Similarity or analogy through the suffix -like across diverse contexts. Interpretation: Specific terms, names, and abbreviations that are contextually salient and uniquely identifiable. Composition: Medical terminology related to steroids, hormones, and their derivatives. Interpretation: Spiritual concepts and the conjunction as in varied syntactic roles. Composition: Abstract concepts tied to spirituality, consciousness, and metaphysical essence. Extension 3 Interpretation: Directional and regional descriptors indicating geographical locations or cultural contexts. Composition: Directional terms indicating geographical or regional divisions."
        },
        {
            "title": "Score",
            "content": "0.84 0.87 0.89 0.66 0.84 0. 0.93 0.84 0.91 Table 2: Interactions between composition base element 3 in head 23 and all extension elements in that head. Interaction happens in way that closely resembles the Boolean AND operation: base pre-latent is polysemous, and the composition post-latent is the intersection, i.e. logical AND between parent pre-latents. See details in Section 5.3. of interactions than just looking at the resulting latent descriptions, so we leave it to further work. Despite this, the AND-like gate is very common behavior. See more examples in Appendix G. Geometry of post-latents. Each post-latent vector has vector representation in the residual stream represented by the corresponding column in Wdec, which is the approximation of overcomplete basis vectors we search for when training SAEs. We had not observed any notable differences in feature geometry between TopK and our SAEs, except that our architectural design leads to clustering so that post-latents produced by same head, base or extension elements are grouped in tight cluster, and the geometry is dependent on hyperparameters h, m, we choose, which is expected and may be useful for further applications such as steering. See more details in Appendix E."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "We introduce KronSAE, sparse autoencoder architecture design that directly tackles the longstanding encoder inefficiency bottleneck by combining head-wise Kronecker factorisation with mAND gating. This significantly lowers number of parameters compared to standard TopK SAEs, while improving reconstruction fidelity and yielding more interpretable features by utilizing their correlations. Our analysis links these gains to the complementary effects of compositional latent structure and logical AND-style interactions, offering new lens on how sparsity and factorisation can synergise in representation learning. Limitations. Despite its benefits, KronSAE has several limitations. Its gains depend on careful tuning of (m, n, h) and the mAND activationmisconfigured settings can negate both efficiency and quality improvements. Our evaluation is limited to mid-sized transformer models and single web corpus, so its applicability to larger models, other domains, or languages remains untested. 9 Future Work. We identify three directions for extending this work: (i) Transcoding. Treat transcoders [Dunefsky et al., 2024] as implicit routers of information and investigate alternative logical gating functions (e.g. XOR or composite gates) to improve interpretability and circuit analysis. (ii) Crosscoding. Generalize KronSAE to crosscoder setting [Lindsey et al., 2024] uncover interpretable, cross-level compositionality via logic operations. (iii) Dynamic Composition. Explore learnable tuning of both the number of attention heads and their dimensionality, enabling fine-grained decomposition into groups of correlated features at varying scales."
        },
        {
            "title": "References",
            "content": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: suite for analyzing large language models across training and scaling, 2023. URL https://arxiv.org/abs/2304.01373. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Language models URL https://openai.com/index/ Ilya Sutskever, can explain neurons in language models, 2023. language-models-can-explain-neurons-in-language-models/. and William Saunders. Jan Leike, Jeff Wu, Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, and Joseph Bloom. is for absorption: Studying feature splitting and absorption in sparse autoencoders, 2024. URL https://arxiv.org/abs/2409.14507. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=F76bwRSLeK. Jacob Dunefsky, Philippe Chlenski, and Neel Nanda. Transcoders find interpretable LLM feature circuits. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=J6zHcScAo0. Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, and Mehdi Rezagholizadeh. Kronecker decomposition for gpt compression, 2021. URL https://arxiv.org/ abs/2110.08152. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henigan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/ toy_model/index.html. Nelson Elhage et al. Decomposing language models with dictionary learning. Transformer Circuits, 2023. URL https://transformer-circuits.pub/2023/monosemantic-features. Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, In The Thirteenth Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=tcsZt9ZNKD. Thomas Heap et al. Sparse autoencoders can interpret randomly initialized transformers. arXiv preprint arXiv:2501.17727, 2025. URL https://arxiv.org/abs/2501.17727. Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Matthew Wearden, Arthur Conmy, Samuel Marks, and Neel Nanda. Saebench: comprehensive benchmark for sparse autoencoders in language model interpretability, 2025. URL https://arxiv.org/abs/2503.09532. 10 Jack Lindsey, Adly Templeton, Jonathan Marcus, Thomas Conerly, Joshua Batson, and Christopher Olah. Sparse crosscoders for cross-layer features and model diffing, 2024. URL https:// transformer-circuits.pub/2024/crosscoders/index.html. Scott C. Lowe et al. Logical activation functions: Logit-space equivalents of probabilistic boolean operators. arXiv preprint arXiv:2110.11940, 2021. URL https://arxiv.org/abs/2110. 11940. Anish Mudide, Joshua Engels, Eric Michaud, Max Tegmark, and Christian Schroeder de Witt. Efficient dictionary learning with switch sparse autoencoders. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= k2ZVAzVeMP. Gonçalo Paulo, Alex Mallen, Caden Juang, and Nora Belrose. Automatically interpreting millions of features in large language models, 2024. URL https://arxiv.org/abs/2410.13928. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. János Kramár, Rohin Shah, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Improving sparse deIn URL http://papers.nips.cc/paper_files/paper/2024/hash/ Varma, composition of NeurIPS, 2024a. 01772a8b0420baec00c4d59fe2fbace6-Abstract-Conference.html. language model activations with gated sparse autoencoders. and Neel Nanda. Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders, 2024b. URL https://arxiv.org/abs/2407.14435. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations (ICLR), 2017. URL https: //arxiv.org/abs/1701.06538. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, 11 Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv.org/abs/2408.00118. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/ index.html. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388."
        },
        {
            "title": "A More Details About Experiments",
            "content": "A.1 Experimental setup. Training details. All SAEs are optimized using AdamW with an initial learning rate of 8 104, cosine learning-rate schedule with minimum LR of 1 106, and linear warmup for the first 10% of total training steps. We use global batch size of 8,192. We sweep over dictionary (latent) sizes of 215, 216, and 217 features. For our KronSAE variant, we further sweep the number of heads and the per-head dimensions and such that equals the desired dictionary size. Regularization weights and auxiliary loss coefficients are kept constant throughout the runs to isolate the impact of architectural choices. For all experiments, we spent about 330 GPU days on NVIDIA H100 80GB GPUs, including preliminary research. SAE. For all experiments on Qwen-2.5, we train each SAE on activations from layer 14. Also for Pythia-1.4B we use layer 14 and for Gemma-2-2B we take activations from layer 12. For most of our experiments, we use sparsity level of ℓ0 = 50 non-zero activations per token. Initialization. As observed by Gao et al. [2025], initializing the decoder as the transpose of the encoder (Wdec = enc) provides strong metric improvement. We adopt this strategy within KronSAE by partitioning Wenc into head-wise blocks of shapes and d, denoted {Pi, Qi}h i=1. For each head k, we define its decoded rows via simple additive composition: Ck[i, j] = Pk,i + Qk,j, = 1, . . . , m, = 1, . . . , n. Finally, flattening the matrices {Ck} yields full decoder weight matrix Wdec RF d. A.2 FLOPs Calculation. For TopK SAE and KronSAE we compute FLOPs in the following way: FLOPSTopK(d, F, k) = dF + kd, FLOPSKronSAE(d, m, n, h, k) = dh(m + n) + mnh + kd dh(m + n) + kd. (5) We calculate flops for most effective variant of TopK where we perform vector matrix multipication only for nonzero activations. A.3 Pythia model. For Pythia-1.4B we train all SAEs on the 12th transformer layer. Figure 8 shows that, under larger equivalent token budgets (500M and 1000M), KronSAE achieves wider performance margin over the TopK SAE than we observed with Qwen-2.5. Figure 8: Isoflops comparison of KronSAE and TopK SAE on Pythia-1.4B across three token budgets."
        },
        {
            "title": "B More Results on Synthetic",
            "content": "We further evaluate KronSAE on several variant block-diagonal covariance matrices (Figure 9). In each case, the decoder-weight covariance Cdec = WdecW dec of KronSAE more faithfully reproduces the ground truth groupings than the TopK SAE. Notably, on the third covariance pattern (where some blocks are very small) TopKs learned correlations nearly vanish, whereas KronSAE still uncovers the correct block structure. For the first covariance matrix, KronSAE yields sharply elevated correlations in the regions corresponding to the true blocks, in line with our design goal of head-wise compositionality. Table 3 quantifies these observations via the RV coefficient and permutation tests. Even after optimally matching TopKs atoms to dense AE reference, TopK SAE attains only weak correlation alignment (RV 0.05 0.08) with non-significant or marginal p-values. In contrast, KronSAE configurations achieve RV values between 0.11 and 0.35 (all < 0.001), representing 36x improvement in correlation recovery. These results confirm that our compositional encoder not only accelerates training but also robustly captures the intended hierarchical feature interactions across diverse covariance regimes. Figure 9: Comparision of how different KronSAE try to reveal hidden structure of defined covariance matrix. The KronSAE models recover the underlying block-structured correlations more faithfully than the TopK baseline, with the finer head/composition split (h = 4, = 2) capturing smaller feature groups more accurately."
        },
        {
            "title": "C mAND as a Logical Operator",
            "content": "For KronSAE, we replace the original ANDAIL [Lowe et al., 2021] with more restrictive approximation, mAND. Since our objective is to drive each atom toward distinct, monosemantic feature, we found that tightening the logical conjunction encourages sharper feature separation. Moreover, q) rather than simple product or minimum, mAND preserves by using the geometric mean ( activation magnitudes. visual comparison of mAND and ANDAIL appears in Figure 10."
        },
        {
            "title": "Covariance matrix",
            "content": "1 2"
        },
        {
            "title": "TopK Matched",
            "content": "RV p-value 0.046 0.1324 0.051 0. KronSAE = 2, = 4, = 32 0.200 0.0002 KronSAE = 4, = 2, = 32 0.150 0."
        },
        {
            "title": "TopK Matched",
            "content": "0.033 0.2212 0.080 0.0002 KronSAE = 2, = 4, = 32 0. 0.0002 KronSAE = 4, = 2, = 32 0.319 0."
        },
        {
            "title": "TopK Matched",
            "content": "0.035 0.2490 0.043 0.0002 KronSAE = 2, = 4, = 32 0. 0.0002 KronSAE = 4, = 2, = 32 0.212 0.0002 TopK TopK Matched 0.034 0.4613 0.043 0.0002 KronSAE = 2, = 4, = 32 0. 0.0002 KronSAE = 4, = 2, = 32 0.334 0.0002 Table 3: RV coefficient and permutation p-values for correlation recovery on four synthetic covariance patterns. KronSAE outperforms both the standard and atom-matched TopK SAE by large margin, achieving statistically significant alignment (p < 103) across all cases. Figure 10: Comparison of the smooth mAND operator against the ANDAIL [Lowe et al., 2021]."
        },
        {
            "title": "D Feature Analysis Methodology",
            "content": "We analyze learned features using an established pipeline Bills et al. [2023], Paulo et al. [2024] consisting of three stages: (1) statistical property collection, (2) automatic activation pattern interpretation, and (3) interpretation evaluation. The following subsections detail our implementation. 15 D.1 Data collection Our collection process uses fixed-size buffer = 384 per feature, continuing until processing predetermined maximum token count Tmax. The procedure operates as follows: Initial processing batches generate large activation packs of 1M examples, where each example comprises 256-token text segments. When encountering feature activations, we add them to the buffer, applying random downsampling to maintain size when exceeding capacity. This approach enables processing arbitrary token volumes while handling rare features that may require extensive sampling. During collection, we compute online statistics including activation minimums, maximums, means, and frequencies. Post-processing yields two key metrics: token entropy and multitoken ratio. The token entropy is calculated as: token entropy = (cid:88) i=0 pi log(pi), pi = activations of token total amount of activations , where represents unique activated tokens. The multitoken ratio is: multitoken ratio = 1 (cid:88) i= number of activations in sequence total tokens in sequence , (6) (7) with < denoting collected context examples per feature. We then segment examples using 31-token context window (15 tokens before/after each activation), potentially creating overlapping but non-duplicated examples. Features with high multitoken ratio may have number of examples significantly exceeding B. separate negative examples buffer captures non-activating contexts. Future enhancements could employ predictive modeling (e.g., using frequent active tokens) to strategically populate this buffer with expected-but-inactive contexts, potentially improving interpretation quality. D.2 Feature interpretations For each feature, we generate interpretations by sampling 16 random activation examples above the median activation quantile and presenting them to Qwen3 14B [Yang et al., 2025] (AWQquantized with reasoning enabled). The model produces concise descriptions of the activation patterns. Empirical observations suggest reasoning mode improves interpretation quality, though we lack quantitative measurements. This aligns with findings in [Paulo et al., 2024], which compared standard one-sentence responses with Chain-of-Thought outputs, making model reasoning an interesting direction for future research. The interpretation process uses the system prompt presented in Figure 11. User prompts include all special characters verbatim, as some features activate specifically on these characters. representative (slightly abbreviated) user prompt example is presented on Figure 12. D.3 Evaluation pipeline We evaluate interpretations using balanced sets of up to 64 positive (activation quantile > 0.5) and 64 negative examples, employing the same model without reasoning to reduce computational costs. When insufficient examples exist, we maintain class balance by equalizing positive and negative counts. The evaluation uses modified system prompts from [Paulo et al., 2024], with added emphasis on returning Python lists matching the input example count exactly. We discard entire batches if responses are unparseable or contain fewer labels than the number of provided examples. We calculate two scores. Detection Score: After shuffling positive/negative examples, we present up to 8 unformatted text examples per batch to the model. The model predicts activations (1/0) for each example, generating 16 You are meticulous AI researcher conducting an important investigation into patterns found in language. analyze text and provide an explanation that thoroughly encapsulates possible patterns found in it. Your task is to Guidelines: You will be given list of text examples on which special words are selected and between delimiters like this. If sequence of consecutive tokens all are important, the entire sequence of tokens will be contained between delimiters just like this. How important each token is for the behavior is listed after each example in parentheses. - Your explanation should be concise STANDALONE PHRASE that describes observed patterns. - Focus on the essence of what patterns, concepts and contexts are present in the examples. - Do NOT mention the texts, examples, activations or the feature itself in your explanation. - Do NOT write \"these texts\", \"feature detects\", \"the patterns suggest\", \"activates\" or something like that. - Do not write what the feature does, e.g. heart diseases in medical reports\" write \"heart diseases in medical reports\". - Write explanation in the last line exactly after the [EXPLANATION]: instead of \"detects Figure 11: System prompt for feature interpretations. Examples of activations: Text: Activations: Text: Activations: Leno, San Francisco Democrat, said in statement. said (22.74), statement (27.84), in (27.54) city spokesman Tyler Gamble said in an email. said (2.92), in (12.81), an (14.91) Text: Activations: Speaking (3.48) towpath at Brentford Lock. Speaking on BBC London 94 Text: Activations: told (4.05) Michelle, quadriplegic, told DrBicuspid.com Text: Activations: CEO Yves Carcelle said in statement. said (19.64), in (29.09), statement (29.39) Figure 12: Example of user prompt passed to LLM. This feature with 16 examples received the interpretation \"Structural elements in discourse, including speech attribution, prepositional phrases, and formal contextual markers\" with detection score of 0.84 and fuzzing score of 0.76. 17 Figure 13: Correlation coefficients (Pearson and Spearman) between properties of TopK and KronSAE latents. Token entropy emerges as strong predictor of interpretability scores, while higher mean activation and lower frequency also indicate more interpretable features. up to 128 true/predicted label pairs. The score calculates as: score = (cid:18) correctly predicted positives total positives 1 2 + correctly predicted negatives total negatives (cid:19) . (8) Fuzzing Score: We highlight activated tokens on sampled examples, from which 50% are correctly labeled positive examples, 25% are mislabeled positive examples, and 25% are randomly labeled negative examples. We present batches of up to 8 examples and the model identifies correct/incorrect labeling, with scoring following Equation 8."
        },
        {
            "title": "E Additional Feature Analysis Results",
            "content": "Feature property correlations. Our analysis reveals significant correlations between feature properties and interpretability scores (Figure 13). Notably, token entropy and mean activation show substantial correlations with interpretability scores, suggesting their potential as proxies for assessing feature quality without running the full interpretation pipeline. These findings are based on analysis of the first 3072 features from 32k TopK and KronSAE (m=4, n=4) trained on 24M tokens, warranting further validation with larger-scale studies. Pre-latent to post-latent relationships. We investigate how post-latent properties correlate with various combinations of pre-latent properties, including individual values, means, products, and the mAND operation (product followed by square root). Figure 14 demonstrates that post-latent multitoken ratio, token entropy, and frequency show stronger correlations with pre-latent products or mAND values than with individual pre-latent properties or their means. Basis geometry. As noted in Section 5.3, latent embeddings primarily exhibit clustering within their originating groups (head, base, extension). With the support of observations reported in Sections 4.2 and 5.3, we find that models with more heads achieve better reconstruction while producing more diverse basis vectors. This suggests that fine-grained architectures yield more expressive 18 Figure 14: Correlation patterns between properties of post-latents and pre-latents. Figure 15: UMAP visualization of post-latent clustering patterns by head, base, and extension group membership. We observe tight clusters by base for < and by extension for n. representations, although they may also exhibit undesired challenging behavior like feature splitting [Bricken et al., 2023] or absorption [Chanin et al., 2024]. Figure 15 visualizes this structure through UMAP projections (n_neighbors=15, min_dist=0.05, metric=cosine) of decoder weights from the first 8 heads of 32k SAEs with varying m,n configurations. The plots reveal distinct clustering patterns: for < we observe tight base-wise clustering with weaker grouping by extension, and for extension-wise clustering is stronger. This asymmetry suggests that pre-latent capacity requirements directly manifest in the embedding geometry - components with lower polysemanticity (extensions when < n) exhibit greater geometric diversity. We expect symmetric behavior for reciprocal configurations (e.g., m=4,n=8 vs. m=8,n=4), merely swapping the roles of bases and extensions."
        },
        {
            "title": "F KronSAE in Terms of Tensor Diagram",
            "content": "The proposed encoder architecture can be visualized as tensor diagram (Figure 16). Notably, this formulation draws connection to quantum mechanics, where represents the (unnormalized) state of two disentangled qubits described by and q. If we were to sum the outputs of the encoders heads instead of concatenating them, would correspond to separable quantum state. This scenario can be expressed via the Schmidt decomposition: = (cid:88) ph qh , where denotes the Kronecker product. However, preliminary experiments revealed that this alternative design results in poorer performance compared to the concatenation-based approach. Figure 16: For single head, the KronSAE encoder architecture separates the input into two distinct components, and q, via matrix multiplications with enc accordingly. These components are then combined via the outer product q, resulting in matrix representation. To produce the final output vector , this matrix is flattened into single vector using multi-index mapping. enc and q"
        },
        {
            "title": "G Analysis of Compositional Structure",
            "content": "Here we analyze more examples of interactions in various heads. Head 3. For this head we have selected all base elements and extension 2, shown in Table 4. Extension element 2 shows moderate interpretability with clear AND-like interactions: with base 1 (semantic inheritance through shared pre-latent semantics) and base 2 (retaining only instrumentrelated semantics). Notable interactions occur with base 0 (acquiring medical semantics while preserving metric/number aspects) and base 3 (combining instrument semantics with necessity to yield therapy/treatment concepts). The high interpretability scores suggest potential additional encoding mechanisms beyond simple intersection, possibly related to activation magnitude, though dataset or interpretation artifacts cannot be ruled out without further validation. Head 136. This head exhibits higher interpretability in post-latents than pre-latents. Key observations from the Table 5 include: extension 2 with base 0 narrows semantics to Illinois (likely inheriting geographical subsemantics), while interactions with bases 2-3 demonstrate complexity beyond simple intersection, often introducing additional semantics requiring deeper investigation. Head 177. Latents presented in Table 6 emonstrates more consistent AND-like behavior than Heads 3 and 136, closely matching the interaction pattern shown in Figure 2, supporting our theoretical framework."
        },
        {
            "title": "Interpretation",
            "content": "Extension 2 Scientific instruments, acronyms, and critical numerical values in technical and astronomical contexts Base elements and their compositions with extension 2 Base 0 Base 1 Base Base 3 Interpretation: Punctuation marks and line breaks serving as structural separators in text. Composition: Health-related metrics focusing on survival rates, life expectancy, and longevity. Interpretation: Numerical values, both in digit form and as spelled-out numbers, often accompanied by punctuation like decimals or commas, in contexts of measurements, statistics, or quantitative expressions. Composition: Numerical digits and decimal points within quantitative values. Interpretation: Nuanced actions and adverbial emphasis in descriptive contexts. Composition: Astronomical instruments and their components, such as space telescopes and their acronyms, in scientific and observational contexts. Interpretation: Forms of the verb \"to have\" indicating possession, necessity, or occurrence in diverse contexts. Composition: Antiretroviral therapy components, viral infection terms, and medical treatment terminology. Table 4: Interactions between extension 2 in head 3 and all base elements in that head. Component Interpretation Extension 2 Hierarchical scopes, geographic references, and spatial dispersal terms Base elements and their compositions with extension 2 Base 0 Base Base 2 Base 3 Interpretation: Numerical decimal digits in quantitative expressions and proper nouns. Composition: The state of Illinois in diverse contexts with high significance. Interpretation: The number three and its various representations, including digits, Roman numerals, and related linguistic forms. Composition: Geographic place names and their linguistic variations in textual contexts. Interpretation: Ordinal suffixes and temporal markers in historical or chronological contexts. Composition: Terms indicating layers, degrees, or contexts of existence or operation across scientific, organizational, and conceptual domains. Interpretation: Question formats and topic introductions with specific terms like \"What\", \"is\", \"of\", \"the\", \"Types\", \"About\" in structured text segments. Composition: Spatial spread and occurrence of species or phenomena across environments."
        },
        {
            "title": "Score",
            "content": "0.71 0.66 0.88 0.80 0.86 0. 0.90 0.91 0.87 Score 0.78 0. 0.95 0.84 0.91 0.87 0.82 0. 0.87 Table 5: Interactions between extension 2 in head 136 and all base elements in that head. 21 Component Interpretation Score Extension 1 Geographical mapping terminology and institutional names, phrases involv0.90 ing spatial representation and academic/organizational contexts Base elements and their compositions with extension 1 Base 0 Base Base 2 Base 3 Interpretation: Proper nouns, abbreviations, and specific named entities. Composition: Geographical or spatial references using the term \"map\". Interpretation: Emphasis on terms indicating feasibility and organizations. Composition: Specific organizations and societal contexts. Interpretation: Institutional names and academic organizations, particularly those containing \"Institute\" or its abbreviations, often paired with prepositions like \"of\" or \"for\" to denote specialization or affiliation. Composition: Institutional names containing \"Institute\" as core term, often followed by prepositions or additional descriptors. Interpretation: Closure and termination processes, initiating actions. Composition: Initiating or establishing state, direction, or foundation through action. 0.64 0.93 0.80 0.89 0.89 0.92 0.79 0.85 Table 6: Interactions between extension 1 in head 177 and all base elements in that head."
        }
    ],
    "affiliations": [
        "Moscow Institute of Physics and Technology",
        "T-Tech"
    ]
}