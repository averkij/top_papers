{
    "paper_title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
    "authors": [
        "Cong Wei",
        "Bo Sun",
        "Haoyu Ma",
        "Ji Hou",
        "Felix Juefei-Xu",
        "Zecheng He",
        "Xiaoliang Dai",
        "Luxin Zhang",
        "Kunpeng Li",
        "Tingbo Hou",
        "Animesh Sinha",
        "Peter Vajda",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization."
        },
        {
            "title": "Start",
            "content": "MoCha: Towards Movie-Grade Talking Character Synthesis Cong Wei1,2, Bo Sun2, Haoyu Ma2, Ji Hou2, Felix Juefei-Xu2, Zecheng He2, Xiaoliang Dai2, Luxin Zhang2, Kunpeng Li2, Tingbo Hou2, Animesh Sinha2, Peter Vajda2, Wenhu Chen1 1University of Waterloo, 2GenAI, Meta https://congwei1230.github.io/MoCha 5 2 0 2 0 3 ] . [ 1 7 0 3 3 2 . 3 0 5 2 : r Figure 1. MoCha is an end-to-end talking character video generation model that takes only speech and text as input, without requiring any auxiliary conditions. More videos are available on our website: https://congwei1230.github.io/MoCha"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in video generation have achieved impressive motion realism, yet they often overlook characterdriven storytelling, crucial task for automated film, animation generation. We introduce Talking Characters, more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of In this one or more characters beyond the facial region. paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose speechvideo window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of largescale speech-labeled video datasets, we introduce joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turnbased dialogueallowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization. 1. Introduction Automating film production holds immense commercial potential, promising to democratize cinematic-level storytelling by enabling content creators to effortlessly generate films through natural language [15, 36, 41, 47]. Ideally, creators should be able to specify rich narratives involving multiple characterseither realistic humans or stylized cartoonsthat engage in meaningful dialogues, expressive emotional portrayals, synchronized speech, and realistic full-body actions. Crucially, talking characters serve as powerful mediums for delivering impactful messages, clearly communicating ideas, and deeply engaging audiences. In films especially, dialogue serves as key vehicle to effectively convey narratives, with vast downstream applications including digital assistants, virtual avatars, advertising, and educational content. However, existing video foundation models are far from achieving this vision. Despite significant advancements in visually compelling content and dynamic environments, models such as SoRA, Pika, Luma, Hailuo, and Kling [2, 3, 57, 19, 27, 42] primarily generate characters with limited speech capabilities. Typically, these models exhibit simplified mouth movements and emotional expressions detached from meaningful dialogue, lacking control over actual spoken content. Consequently, their practical usability is severely restricted for speech-driven interactions essential to cinematic and interactive applications. On the other hand, recent speech-driven video generation methods, such as Loopy, Hallo3, and EMO [18, 24, 3335, 40], predominantly focus on synthesizing talking-head videos confined to facial regions. These approaches neglect essential fullbody movements and multi-character interactions critical for expressive storytelling, thus significantly limiting their applicability in realistic and interactive cinematic scenarios. To bridge these gaps, we introduce the novel task: Talking Characters, defined as generating characters from natural language and speech input that naturally express synchronized speech, realistic emotions and full-body actions. We further propose MoCha, the first-of-its-kind diffusion transformer (DiT) model trained end-to-end to achieve high-quality, movie-grade talking character generation. MoCha introduces several key technical innovations tailored specifically for this task: End-to-End Training Without Auxiliary Conditions: Unlike prior works such as EMO [33, 35], SONIC [17], Echomimicv2 [24], Loopy [18], and Hallo3 [40], which rely heavily on external control signals (e.g., reference images, skeletons, keypoints), MoCha is trained directly on text and speech without any auxiliary conditioning. This simplifies the model architecture and improves motion diversity and generalization. Speech-Video Window Attention: We propose novel attention mechanism that aligns speech and video inputs through localized temporal conditioning (see Sec. 3.2). This design significantly improves lip-sync accuracy and speech-video alignment. Joint Speech-Text Training Strategy: To address the scarcity of large-scale speech-labeled video datasets, we introduce joint training framework that leverages both speech-labeled and text-labeled video data. This strategy enhances the models ability to generalize across diverse character actions and enables universal controllability through natural language prompts, enabling nuanced control of character expressions, actions, interactions, and environments without auxiliary signals. Multi-Character Conversation Generation: For the first time, MoCha enables coherent multi-character conversations in dynamic, turn-based dialogues, overcoming the single-character limitation of prior methods and supporting cinematic, story-driven video synthesis. To evaluate MoChas performance, we curated MoChaBench, benchmark tailored for Talking Characters generation tasks. Both human evaluations and automatic metrics demonstrate that MoCha set new standard for talking character video generation and represents significant step toward achieving controllable, narrative-driven video synthe2 Figure 2. MoCha Architecture. MoCha is end-to-end Diffusion Transformer model that generates video frames from the joint conditioning of speech and text, without relying on any auxiliary signals. Both speech and text inputs are projected into token representations and aligned with video tokens through cross-attention. sis, with broad applications in film production, animation, virtual assistants, and beyond. 5. Visual Quality: The entire video is visual consistency and temporal coherence without visual artifacts. 2. Task: Talking Characters 3. Model: MoCha We introduce the novel task of Talking Characters, which aims to generating characters from natural language and speech input that mimicking realistic human-like behaviors. In contrast to talking-head (Close-up shot), Talking Characters aims at generating digital characters at any camera shot size (From close-up shot to wide-shot), covering one or more characters beyond the facial region. Input: Talking Character system takes as input: 1. text prompt describing the character, environment, facing direction(optional), position in the actions, frame(optional) and camera framing(optional). 2. speech audio for driving the character mouth, facial expression and body motion. Output: The output is video featuring one or more talking characters, which can be human, 3D cartoon, or animal. Evaluation: The generated characters are expected to perform well across the following five axes: 1. Lip-Sync Quality: Speak the provided audio with accurate and temporally aligned lip synchronization. 2. Facial Expression Naturalness: Express natural and coherent facial emotions that align with both the speech content and the text prompt. 3. Action Naturalness: Perform natural and fluid body movements corresponding to the actions described in the text, with gestures synchronized to the speech. 4. Text Alignment: Appear in scene and context that are consistent with the descriptions provided in the prompt. In this section, we introduce the MoCha model, the first model to generate talking characters. We begin by outlining its architecture in subsection 3.1, followed by the speechvideo window attention mechanism in subsection 3.2. Next, we describe the method of generating multiple clips in subsection 3.3. Finally we provide explanation of the training strategy in subsection 3.4. 3.1. Speech + Text to Video Diffusion Transformers Figure 2 presents the overall framework of MoCha. Unlike prior works that employ text-to-image (T2I) U-Net [10, 33, 35, 40] for talking head generation, MoCha is built on diffusion transformer (DiT) [26]. By incorporating text and speech conditions sequentially via cross-attention, it effectively captures both semantics and temporal dynamics. Model Architecture. Given an RGB video ν RT HW 3 with frames, we encode it into latent representation x0 Rτ hwc using 3D VAE, which downsamples the video spatially and temporally. We define the temporal down-sampling ratio as = τ . Next, x0 is flattened into sequence of tokens of size (τ w) and passed to the DiT model fθ(). Within each DiT block, the model first applies self-attention to the tokens, followed by sequential cross-attention with the text condition tokens and audio condition tokens α. The audio condition α RT is derived from raw waveforms using Wav2Vec2 [1] 3 Figure 3. MoChas Speech-Video Window Cross Attention MoCha generates all video frames in parallel using window cross-attention mechanism, where each video token attends to local window of audio tokens to improve alignment and lip-sync quality. and processed through an single layer MLP to align its feature dimension with the latent video tokens. Training Objective. We adopt Flow Matching [21], which enables efficient simulation of continuous-time dynamics, to train our model. Given latent video representation x1 Rτ hwc (encoded from the input video), random noise ϵ (0, I), and continuous time step [0, 1], we construct an intermediate latent xt by interpolating between ϵ and x1: xt = (1 t) ϵ + x1. (1) The model is trained to predict the velocity, defined as the difference between the data and noise: vt = dxt dt = x1 ϵ. (2) The training loss is then: = EϵN (0,I), x1, c, α, t[0,1] (cid:13) 2 (cid:0)xt, c, α, t(cid:1) (x1 ϵ) (cid:13) (cid:13) 2 (3) where x1 is the encoded latent video, and α are text and audio conditions, and fθ() is the DiT model. (cid:13) (cid:13) (cid:13)fθ , 3.2. Speech-Video Window Attention Most talking head generation methods employ 2D diffusion models (e.g., U-Net) that auto-regressively generate video frames conditioned on audio tokens α RT c. When generating frame νi, the model is provided only with the corresponding audio token αi. This design inherently preserves speech-video synchronization, ensuring accurate alignment between lip movements and the corresponding speech. However, when using DiT-style architectures, two key differences emerge that disrupt this alignment: 4 1. Temporal Compression: Videos are compressed using 3D VAE with downsampling ratio (typically = 4 or 8 in modern T2V models [19, 27]), resulting in latent representations of length τ = /r. While audio remains at the original resolution (T ), video tokens operate at the compressed scale (τ ), degrading lip synchronization. 2. Parallel Generation: Unlike autoregressive models, DiT generates all τ latent frames in parallel. However, naıve cross-attention allows each video token to attend to all audio tokens. As result, latent frames may incorrectly associate with phonemes from unrelated timesteps. To address this, we propose Speech-Video Window Attention mechanism that enforces localized conditioning. This design is motivated by the observation that lip movements depend on short-term audio cues (12 phonemes), whereas body motions align with longer-term text descriptions. To capture this distinction, we constrain each video token to attend only to temporally bounded audio window. As illustrated in Figure 3, for each latent video frame x(i) Rhwc (i {1, . . . , τ }), attention is computed over audio tokens αj, where spans: [max(1, (i 1)r 1), min(T, ir + 1)] . (4) This window encompasses + 2 audio tokens, covering the frames corresponding to the latent x(i), plus one token on either side to ensure contextual continuity, thereby enhancing local smoothness between adjacent latents. 3.3. Multi-character Conversation MoCha generates multi-clip videos simultaneously in the same manner as single-clip generation, with no additional architectural modifications. As illustrated in Figure 4, instead of auto-regressive generation in video extension methods which requiring conditioning on previous generated results, MoCha leverages self-attention across video tokens to ensure consistency of characters appearing in multiple clips, Figure 4. Multi-character Conversation and Character Tagging. MoCha supports generates multi-character conversion with scene cut. We design specialized prompt template: it first specifies the number of clips, then introduces the characters along with their descriptions and associated tags. Each clip is subsequently described using only the character tags, simplifying the prompt while preserving clarity. MoCha leverages self-attention across video tokens to ensures character and environment consistency. The audio conditioning signal implicitly guides the model on when to transition between clips. as well as coherence in the surrounding environment. We assume that only one character speaks at single time, so the changes in the speaker in the audio conditioning implicitly guide the MoCha on when to transition between clips without any additional guidance signal condition such as clip tokens [39]. Binding attributes and actions to the correct characters using only text is particularly challenging in multi-clip settingsespecially when multiple characters interact or when the same character appears across clips. Naive captioning models typically rely on visual descriptions to refer to characters. As result, they must repeat detailed appearance descriptions each time character is mentioned, leading to long, redundant, and confusing prompts. For example: The girl aged 10-15 in yellow dress waves to another girl dressed in green shirt with braided hair holding book... the green shirt with braided hair responds with smile... Nearby, another girl aged 10-15 in blue hoodie points toward the whiteboard while looking at the girl dressed in green shirt with braided hair... The girl in This verbosity not only increases the risk of exceeding token limits (e.g., 256 tokens) but also confuses the model during generationespecially in multi-clip scenarios. As shown in Figure 4, we address this by introducing structured prompt template with fixed keywords and character tagging mechanism that promotes clarity, compactness, and consistency: Two video clips Specifies the number of clips. Characters Introduces list of characters, each described by visual attributes and assigned unique tag [20] (e.g., Person1, Person2). First clip, Second clip, etc. Each video segment is described using only the defined character tags. This design significantly reduces redundancy and helps the model reliably associate visual attributes with character actions, even across multiple clips. 3.4. MoCha Training Strategy Joint Training of (Speech+Text)-to-Video (ST2V) and Text-to-Video (T2V) Speech-annotated video datasets are significantly smaller in scale and less diverse compared to standard text-to-video (T2V) datasets, making it challenging to train high-quality (Speech+Text)-to-Video (ST2V) models directly. Relying solely on speech-annotated data limits the models ability to generalize across varied visual and semantic contexts. To address this, we propose joint training strategy that integrates both speech-annotated and text-only video dataset: 80% ST2V Data: The model is primarily trained on speech-conditioned video data, leveraging both speech and text modalities. 20% T2V Data: To enhance diversity, we incorporate text-only video data, where speech conditioning is absent. In these cases, Wav2Vec2 embeddings are replaced with zero vectors, allowing the model to generalize across textonly prompts after training. Multi-Stage Human Video Learning Speech condition5 Figure 5. Qualitative results of MoCha on MoCha-Bench. MoCha not only generates lip movements that are well-synchronized with the input speech, but also produces natural facial expressions that reflect the prompt along with realistic hand gestures and action movements of speech-conditioned data simultaneously can lead to inefficiencies. As illustrated in Figure 6, we address this challenge with multi-stage training framework that categorizes data based on shot types, ranging from close-up to medium shots: We begin training with close-up shots, which contain the strongest speech-video correlation. At each subsequent stage, we reduce the data from the previous stage by 50% while introducing harder tasks with weaker speech conditioning. We maintain the 80% ST2V and 20% T2V data ratio across all stages to ensure balanced training. In Stage 0, we pretrain MoCha exclusively on textconditioned video data (T2V) to establish strong foundational prior for video generation before incorporating speech-conditioning signals. 4. Experiment In this section, we first describe our training data processing pipeline in subsection 4.1, followed by the details of our model in subsection 4.2. We then introduce MoChaBench for Talking Characters task and benchmark MoCha against baseline methods in subsection 4.3, and finally, we present an ablation study to analyze the impact of key design choices in subsection 4.4. Figure 6. Multi-Stage Training Strategy for MoCha. TextSpeech Joint training starts with close-up shots where speech conditioning has the strongest influence. At each stage, previous data is reduced by 50%, and harder tasks with weaker speech conditioning are introduced. Stage 0 uses text-only video data to establish foundation for the future stages. ing in human video generation exhibits diminishing influence as we progress from low-level to high-level motion: it strongly governs lip movements and facial expressions, but its effect weakens for co-speech gestures and full-body actions. Meanwhile, generating these higher-level motions is inherently more difficult. As result, training on all types 6 Figure 7. Qualitative comparison between MoCha and baselines on MoCha-Bench. MoCha not only produces lip movements that align closely with the input speechenhancing the clarity and naturalness of articulationbut also generates expressive facial animations and realistic, complex actions that faithfully follow the textual prompt. In contrast, SadTalker and AniPortrait exhibit minimal head motion and limited lip synchronization. Hallo3 mostly follows the lip-syncing but suffers from inaccurate articulation, erratic head movements, and noticeable visual artifacts. Since the baselines operate in an image-to-video (I2V) setting, we provide them with the first frame generated by MoCha as input for comparison. The first frame is cropped and resized as needed to meet the requirements of each baseline. 4.1. Training Data Processing Pipeline To construct high-quality training data, we employ multistage filtering and annotation pipeline. Speech Scene Filtering: We first segment videos into scenes using PySceneDetect [4]. Each scene undergoes speech detection, where non-speech segments and those heavily influenced by background noise or music are dis7 Figure 8. Human evaluation scores on MoCha-Bench. Scores range from 1 to 4 across five evaluation axes, where score of 4 reflects performance that is nearly indistinguishable from real video or cinematic production. MoCha significantly outperforms all baselines across all axes. SadTalker and AniPortrait consistently received score of 1 for action naturalness, as these methods only perform head movements. Text alignment is marked as not applicable (N/A) for these baselines since they do not accept text input. carded. For each valid segment, we perform music and noise removal and Wav2Vec2 [1, 29] is used to extract speech embeddings for valid segments. Prominent Character Filtering: To ensure the dataset focuses on scenes with clear, prominent human characters, we employ an LLM-based filtering mechanism. The model analyzes each scene and removes those lacking central characters. Motion and Lip-Sync Filtering: We further refine the dataset by applying motion and lip-sync filters, ensuring that the extracted speech corresponds to meaningful human expressions and actions. Scene Captioning: Each processed scene is captioned using an LLM [25], describing character appearances, positions, speech activity, emotions, and body language in highly detailed structured format. Following this pipeline, we curate high-quality dataset consisting of 300 hours (O(500)K samples) of speechconditioned video data. 4.2. Implementation Details Our model follows design similar to MovieGen [27] and HunyuanVideo [19], utilizing DiT architecture. MoCha is built on pretrained 30-B DiT model. All models are trained with spatial resolution of approximately 720720, accommodating multiple aspect ratios. The model is optimized to generate 128-frame videos at 24 frames per second, resulting in outputs with duration of 5.3 seconds. Our dataset [9] for text conditioning comprises approximately O(100)M samples, while the speech conditioning video dataset consists of around O(500)K samples. Training is conducted on 64 nodes to support the scale of our model. 4.3. Evaluation Baselines. We compare our approach against several representative audio-driven talking face generation methods with publicly available source code or implementations, including SadTalker [44], AniPortrait [38], and Hallo3 [40]. These baselines span both end-to-end architectures and those utilizing intermediate facial representations, enabling comprehensive evaluation of our method across diverse generation paradigms. Benchmark. We introduce MoCha-bench, benchmark tailored for the Talking Character generation task. It comprises 150 diverse examples, each consisting of text prompt and corresponding audio clip. The dataset includes both close-up and medium-shot compositions: close-ups emphasize facial expressions and lip synchronization, while medium shots highlight hand gestures and broader body movements. The scenes span wide range of human activities and interactions with objects (e.g., chef chopping vegetables, musician playing an instrument) and the character speaking with various emotions and facing directions. All text prompts were manually curated and further enriched using the publicly released LLaMA-3 [12] model to enhance expressiveness and variety. As MoCha directly generates videos from speech and text inputs, while all baseline models operate in an image-to-video (I2V) setting, we ensure fair comparison by providing each I2V method with the first frame of the MoChas generation as the input. Qualitative Experiments We presents qualitative results of MoCha-30B in Fig. 1 and Fig. 5 showcasing its ability to generate diverse and realistic human motion while synchronizing speech with complex actions. Fig. 7 presents direct comparison between MoCha and baseline methods on MoCha-Bench. All baselines require reference image as an auxiliary input. To ensure fairness, we first generate video using MoCha and then use its first frame as the reference image for all baseline models. For models that do not support arbitrary aspect ratios, we crop the first frame to focus on the head region before feeding it into their networks. We provide two groups of qualitative comparisons: one featuring close-up shots and the 8 Method Lip-Sync Quality Facial Expression Naturalness Action Naturalness Text Alignment Visual Quality Hallo3 [40] SadTalker [44] AniPortrait [38] 2.45 1.21 1.16 2.25 1.14 1.12 2.13 1.00 1. 2.35 N/A N/A 2.36 2.95 1.45 MoCha (Ours) 3.85 (+1.40) 3.82 (+1.57) 3.82 (+1.69) 3.85 (+1.50) 3.72 (+1.36) Table 1. Human evaluation scores on MoCha-Bench. Scores range from 1 to 4 across five evaluation axes, where score of 4 reflects performance that is nearly indistinguishable from real video or cinematic production. Participants rated each method on five aspects: lip-sync quality, facial expression naturalness, action naturalness, text-prompt alignment, and visual quality. MoCha significantly outperforms prior methods across all categories. Green numbers indicate absolute improvements () over the second-best method (underlined). SadTalker and AniPortrait consistently received score of 1 for action naturalness, as these methods only perform head movements. Method Sync-C Sync-D SadTalker [44] AniPortrait [38] Hallo3 [40] Ours 4.727 1.740 4.866 6.037 (+1.17) 9.239 11.383 8.963 8.103 (-0.86) Comparison with State-of-the-Art Methods on Table 2. MoCha-Bench. We report synchronization metrics: Sync-C (higher is better) and Sync-D (lower is better). MoCha outperforms all baselines, indicating superior lip-sync quality. Ablation Ours w/o Joint ST2V + T2V Training w/o Speech-Video Window Attention Sync-C Sync-D 6.037 5.659 5.103 8. 8.435 8.851 Table 3. Ablation Study of MoCha on MoCha-Bench We analyze the impact of different components by disabling them and measuring the effect on key metrics. Removing speech-video window attention degrades synchronization, joint ST2V and T2V training improves generalization. other medium shots. The close-up group emphasizes lipsync quality, head movement, and facial expressions, while the medium shot group focuses on hand movements during speech. MoCha not only produces lip movements that closely align with the input speechenhancing both articulation and naturalnessbut also generates expressive facial animations and realistic, coordinated actions that accurately follow the textual prompt. In contrast, SadTalker and AniPortrait exhibit minimal head motion and limited lip synchronization. While Hallo3 achieves mostly consistent lipsyncing, it suffers from inaccurate articulation and erratic head movements. In the medium shot comparisons, Hallo3 also introduces noticeable visual artifacts, particularly during complex actions. Quantitative Experiments We evaluate video quality using the automatic metrics to measure the lip-sync quality. Table 2 presents comparison on the MoCha-Bench. Our model achieves the best scores across lip-sync metrics. Human Evaluations. We conduct comprehensive human evaluation to compare MoCha against baseline methods on the MoCha-Bench dataset. The evaluation is based on five axes tailored for the Talking Characters task(See 2): Lip-Sync Quality: Measures how accurately the characters lip movements align with the spoken audio. Scale: 1 Not aligned at all, 2 Weak alignment, 3 Mostly aligned, 4 Perfectly aligned. Facial Expression Naturalness: Evaluates whether the facial expressions and lip-sync appear natural and contextually coherent, without seeming robotic or exaggerated. Scale: 1 Completely unnatural, 2 Noticeably synthetic or stiff, 3 Mostly natural and believable, 4 Indistinguishable from real or cinematic performance. Action Naturalness: Assesses how naturally the characters body movements and gestures align with the audio. Scale: 1 Completely unnatural, 2 Noticeably unnatural, 3 Mostly natural, 4 Indistinguishable from real movie or TV characters. Text Alignment: Measures how well the generated actions and expressions follow the behaviors described. Scale: 1 No alignment, 2 Partial alignment, 3 Mostly aligned, 4 Perfect alignment with the prompt. Visual Quality: Evaluates visual quality by checking for issues such as artifacts, discontinuities, or glitches. Scale: 1 Severe artifacts, 2 Noticeable artifacts, 3 Mostly artifact-free, 4 Flawless visuals. Each model output received 5 independent ratings per example, resulting in over 750 responses per model. MoCha significantly outperforms all baselines across all five axes, with average scores approaching 4indicating performance that is nearly indistinguishable from real video or cinematic production. 4.4. Ablation Studies We conduct ablation studies to analyze the contribution of key components in MoCha. Table 3 presents the impact of each design choice. Speech-Video Window Attention Ablation: We disable our speech-video window attention mechanism to analyze 9 its effect on speech-video alignment. This results in noticeable drop in Sync-C (6.037 5.103) and increased Sync-D (8.103 8.851), confirming that our method significantly enhances lip synchronization. Joint ST2V and T2V Training Ablation: We train MoCha exclusively on ST2V data (removing text-only video training). This leads to This results in noticeable drop in Sync-C (6.037 5.659) and increased Sync-D (8.103 8.435), indicating degraded generalization due to reduced dataset diversity. These findings confirm that both our speech-video window attention and joint training strategy are essential for achieving high-quality motion, realistic speech alignment, and overall superior generation performance. 5. Related Work 5.1. Talking Head Generation Given an audio sequence and reference face, pioneer talking-head generation works typically utilize biometric signals such as facial keypoints [22, 28, 30, 45], or 3D priors [11, 13, 16, 23, 32, 43, 44] as intermediate motion representation to animate the reference face while ensuring lip synchronization. For example, SadTalker [44] first extracts 3DMM coefficients from audio and then renders the face in 3D-aware manner. AniPortrait [38] predicts 2D facial landmarks from audio and then utilizes diffusion models to generate portrait video from the 2D landmarks maps. VLOGGER [8] predicts both 3D expression coefficient and 3D body pose from speech and enables the simultaneous generation of talking-face animations and upper-body gestures. Although effective, videos generated by these methods often lack expressiveness and naturalness due to the limited representation of 2D/3D priors. Recently works, such as EMO [34] and Hallo [40], generate audio-driven portrait videos end-to-end using diffusion models, which eliminate intermediate facial representations and learn natural motion from data [10, 18, 34, 37, 40]. Hallo3 [9] builds upon pretrained transformer-based video diffusion models to animate faces with dynamic head poses and background elements. Although these methods can generate natural expressions, they rely on complex auxiliary signalssuch as reference images or keypointswhich not only limit the naturalness and flexibility of facial expressions and body movements but also limit the generalization ablity of those methods. 5.2. Diffusion-Based Video Generation Diffusion models have emerged as powerful approach for video synthesis, demonstrating state-of-the-art results in text-to-video generation. Methods like Make-A-Video [31], MagicVideo [46], and AnimateDiff [14] leverage pretrained text-to-image (T2I) models and extend them to the temporal domain to synthesize coherent motion sequences. Recent advances in DiT-based architectures, such as CogVideoX [42] and MovieGen [27], have further improved video fidelity and controllability by integrating spatial and temporal constraints. Despite these advancements, existing diffusion-based methods primarily focus on scene dynamics and global motion synthesis, lacking explicit modeling of speech-driven facial and body gestures of characters in the video. Our proposed MoCha framework extends diffusion models to jointly condition on speech and text, enabling the generation of lifelike character animations with natural conversation. 6. Conclusion In summary, our work pioneers the task of Talking Characters Generation, pushing beyond traditional talking head synthesis to enable full-body, multi-character animations directly driven by speech and text. We present MoCha, the first framework to address this challenging task, introducing key innovations such as the speech-video window attention mechanism for precise audio-visual alignment and joint training strategy that leverages both speechand text-labeled data for enhanced generalization. Additionally, our structured prompt design unlocks multi-character, turn-based dialogues with contextual awareness, marking significant step toward scalable, cinematic AI storytelling. Through comprehensive experiments and human evaluations, we demonstrate that MoCha delivers state-of-the-art performance in terms of realism, expressiveness, and controllability, setting solid foundation for future research in generative character animation. 7. Acknowledgment We thank Xinyi Ji, Tianquan Di, Anqi Xu, Matthew Yu, Emily Luo for providing speech samples used in the MoCha demo."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. 3, 8 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. 2 [4] Brandon Castellano. PySceneDetect. 7 10 [5] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 2 [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [7] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. [8] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, and Cristian Sminchisescu. Vlogger: Multimodal diffusion for embodied avatar synthesis. arXiv preprint arXiv:2403.08764, 2024. 10 [9] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv preprint arXiv:2412.00733, 2024. 8, 10 [10] Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, and Jingdong Wang. Hallo2: Long-duration and high-resolution audio-driven portrait image animation. ICLR, 2025. 3, 10 [11] Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia Sharmanska. Headgan: One-shot neural head synthesis and editing. In Proceedings of the IEEE/CVF International conference on Computer Vision, pages 1439814407, 2021. 10 [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 8 [13] Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, and Yi Yang. Efficient emotional adaptation for audio-driven In Proceedings of the IEEE/CVF talking-head generation. International Conference on Computer Vision, pages 22634 22645, 2023. 10 [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [16] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, and Feng Xu. Audio-driven emotional video portraits. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1408014089, 2021. 10 [17] Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. Sonic: Shifting focus to global arXiv preprint audio perception in portrait animation. arXiv:2411.16331, 2024. 2 [18] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven In The portrait avatar with long-term motion dependency. Thirteenth International Conference on Learning Representations, 2025. 2, 10 [19] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 4, 8 [20] Feng Liang, Haoyu Ma, Zecheng He, Tingbo Hou, Ji Hou, Kunpeng Li, Xiaoliang Dai, Felix Juefei-Xu, Samaneh Azadi, Animesh Sinha, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Movie weaver: Tuning-free multi-concept video personalization with anchored prompts. CVPR, 2025. [21] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [22] Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, and Yu Li. Moda: Mapping-once audio-driven portrait animation with In Proceedings of the IEEE/CVF Internadual attentions. tional Conference on Computer Vision, pages 2302023029, 2023. 10 [23] Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767, 2023. 10 [24] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. Echomimicv2: Towards striking, simplified, and semi-body human animation. arXiv preprint arXiv:2411.10061, 2024. 2 [25] AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2(5):6, 2024. [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. ICCV, 2023. 3 [27] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 4, 8, 10 [28] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. 10 [29] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. arXiv preprint arXiv:1904.05862, 2019. 8 [30] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion 11 Jinglin Liu, et al. Real3d-portrait: One-shot realistic 3d talking portrait synthesis. ICLR, 2024. 10 [44] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86528661, 2023. 8, 9, 10 [45] Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei, IdentityGangming Zhao, Liang Lin, and Guanbin Li. preserving talking face generation with landmark and apIn Proceedings of the IEEE/CVF Conferpearance priors. ence on Computer Vision and Pattern Recognition, pages 97299738, 2023. 10 [46] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. 10 [47] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2024. models for generalized audio-driven portraits animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19821991, 2023. 10 [31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 10 [32] Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, and Xun Cao. Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior. arXiv preprint arXiv:2312.01841, 2023. 10 [33] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260, 2024. 2, 3 [34] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. 10 [35] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. Emo2: End-effector guided audio-driven avatar video generation. arXiv preprint arXiv:2501.10687, 2025. 2, 3 [36] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. [37] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 10 [38] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 8, 9, 10 [39] Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, and Sergey Tulyakov. Mind the time: Temporally-controlled multi-event video generation. In CVPR, 2025. 5 [40] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 2, 3, 8, 9, 10 [41] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 2 [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, [43] Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang,"
        }
    ],
    "affiliations": [
        "GenAI, Meta",
        "University of Waterloo"
    ]
}