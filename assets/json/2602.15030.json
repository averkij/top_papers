{
    "paper_title": "Image Generation with a Sphere Encoder",
    "authors": [
        "Kaiyu Yue",
        "Menglin Jia",
        "Ji Hou",
        "Tom Goldstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io ."
        },
        {
            "title": "Start",
            "content": "Kaiyu Yue 1 2 Menglin Jia 1 Ji Hou 1 Tom Goldstein 2 6 2 0 2 6 1 ] . [ 1 0 3 0 5 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in single forward pass and competing with manystep diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto spherical latent space, and decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with small fraction of the inference cost. Project page is available at sphere-encoder.github.io. 1. Introduction Most generative image models rely on either diffusion (Ho et al., 2020; Lipman et al., 2022) or autoregressive nexttoken prediction (Tian et al., 2024). With either paradigm, image generation is extremely slow and costly, requiring many forward passes to produce single image. We propose an alternative paradigm that is capable of generating sharp images with as little as one forward pass. Our approach, which we call sphere encoder, works by training two complementary models: an encoder model that maps the distribution of natural images uniformly onto the sphere, and decoder that maps points on the sphere back to natural images (Figure 2). The term aligns with the autoencoder convention, reflecting its encoder-decoder architecture. At test time, an image is generated quickly by sampling random point on the sphere and passing it through the decoder. Although the sphere encoder does not employ diffusion processes explicitly, it supports several key capabilities commonly associated with its diffusion-based cousins (Dhariwal & Nichol, 2021; Rombach et al., 2022; Esser et al., 2024). Equal contribution. 1Meta 2University of Maryland. Figure 1. Selected images generated by the Sphere Encoder in one-step for CIFAR-10 (32 32) and Animal-Faces, two-steps for Oxford-Flowers, and four-steps for ImageNet (256 256). These include conditional generation using AdaLN (Perez et al., 2018; Peebles & Xie, 2023), classifier-free guidance (CFG) (Ho & Salimans, 2022), and few-step iteration to enhance sample quality (Goodfellow et al., 2014; Kingma"
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 2. sphere encoder maps the natural image distribution uniformly onto global sphere S. The decoder then generates realistic image by decoding random point on the sphere. & Dhariwal, 2018; Song et al., 2023). Experiments demonstrate that our approach achieves competitive one-step generation, and state-of-the-art performance in few-step regimes (e.g., fewer than 5 steps) on range of datasets."
        },
        {
            "title": "Motivation and Relation to Autoencoders",
            "content": "Autoencoders (LeCun, 1987; Bourlard & Kamp, 1988; Hinton & Zemel, 1993) have been widely used in representation learning and generative modeling. lower-dimensional latent bottleneck between the encoder and decoder forces the model to learn an undercomplete representation of the input (Goodfellow et al., 2016). To regularize the latent space, variational autoencoders (VAEs) (Kingma & Welling, 2013; 2019; Tolstikhin et al., 2018; Davidson et al., 2018; Ke & Xue, 2025) minimize the divergence between the latent distribution and (typically) Gaussian prior. Unfortunately, in the standard VAE formulation, the divergence loss and image reconstruction loss are at odds with one another; zero divergence loss cannot be achieved simultaneously with perfect image reconstruction. As result, the learned posterior fails to strongly match the prior an issue known as the posterior hole problem (Makhzani et al., 2015; Rezende & Viola, 2018; Tomczak & Welling, 2018; Dai & Wipf, 2019; Ghosh et al., 2020; Aneja et al., 2021). Direct samples from the Gaussian prior fail to yield valid images. Realistic images are currently possible only by decoding samples from the posterior (i.e., adding noise to latents derived from real images), as illustrated in Figure 3. Our approach does not suffer from this problem. Like classical VAE, our approach relies on an autoencoder. Unlike the VAE, which tries to force the latent vectors into Gaussian distribution, we instead force latents to be uniformly distributed on sphere. Due to the bounded and rotationally symmetric nature of the sphere, this can be achieved simply by forcing embeddings of natural images away from one another, causing them to spread throughout the sphere. Moreover, this objective is not in contradiction with the image reconstruction objective; we can achieve 2 Figure 3. Posterior hole problem in VAEs. Columns: (1) Input images; (2) Autoencoder reconstructions; (3) Samples from standard Gaussian prior; and (4) Samples from estimated Gaussian posterior on Animal-Faces training set. Unlike modern FLUX.1/2 (Labs et al., 2025) and SD-VAE (Podell et al., 2024), our sphere encoder produces realistic images by decoding random points sampled from the sphere. both uniformity and accurate reconstruction simultaneously. Many contemporary state-of-the-art diffusion models are actually latent diffusion models (Rombach et al., 2022; Peebles & Xie, 2023; Liu et al., 2023; Ma et al., 2024; Esser et al., 2024; Podell et al., 2024; Wan et al., 2025) hybrid models built on top of VAEs. The VAE partially Gaussianizes the image distribution, but not well enough to be sampled. diffusion pipeline picks up the slack in the VAE, going the last mile of producing valid latent sample for the decoder. Concurrent works have shown that more powerful representation encoders (Yu et al., 2024; Tong et al., 2026), and even spherical manifold encoders (Zheng et al., 2025), result in faster training of the diffusion layer. In our work, we show that spherical latent space1 can be learned so precisely that the expensive diffusion step is irrelevant. 2. Method 2.1. Spherical Latent Space We employ an encoder based on Transformer (Dosovitskiy, 2020; Vaswani et al., 2017) to map an input image RHW 3 into latent representation Rhwd. The latent resolution is determined by the patch size , such that = H/P and = W/P , with denoting the channel depth. To construct global spherical latent space, we define spherifying function, denoted as . This function flattens into vector of dimension = and then projects 1In contrast to prior vMF-based approaches, we create our spherical space using simple vector RMS normalization."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 4. Spherifying latent with noise. Encoder maps image to latent, which projects to on sphere S. During training, random Gaussian noise σ is added to v, where σ is jittered magnitude. Decoder reconstructs the image ˆx from the re-projected noisy latent (v + σ e). it onto sphere with radius via RMS normalization: uniformly from [0, 1], we compute σ as: = (z) = (E(x)) . (1) σ = σmax , (5) Subsequently, decoder reconstructs the image from v: ˆx = D(v) , (2) where ˆx denotes the reconstructed image. If the encoder maps images uniformly onto sphere, then we can generate images by decoding random points on the sphere: ˆx = D(f (e)) , (3) where (0, I) RL is random anisotropic Gaussian and (e) is uniformly distributed on the sphere. For simplicity, we use ˆx to denote the decoder output in both reconstruction and generation scenarios. 2.2. Spherifying with Noise Our training process uses embedding vectors of natural images, and also noisy versions of those embedding vectors. The purpose of training with noisy vectors is two-fold. First, noisy clouds of vectors densely cover the latent space, enabling us to train the decoder on the continuous global latent sphere, rather than only on the finite set of embedding vectors. Second, by using loss that promotes accurate decoding of noisy latent vectors, we force the noisy clouds produced by each training image to spread apart and cover the entire latent sphere. From Normal distribution, we randomly sample noise vector (0, I) RL to perturb the direction of v: where the σmax is the maximum noise limit. The case of = 0 reduces to the naive spherifying in Equation (1). Later we determine the optimal value for σmax with experiments. This core design is illustrated in Figure 4. 2.3. Training Objective Consider two perturbed latent vectors, vNOISY and vnoisy, with large and small noise. vNOISY is defined as in Equation (4) with σ [0, σmax]. The other perturbed vnoisy has less jitter: vnoisy = (v + σsub e) , (6) where σsub = σ, and is uniformly sampled from [0, 0.5]. Note that vnoisy shares the same noise direction as vNOISY. Pixel Reconstruction Loss. This loss ensures that the decoder is an approximate inverse of the encoder, and that the decoder creates valid images. We have the standard pixellevel reconstruction loss, which combines of smoothed L1 loss (Girshick, 2015) and perceptual loss (Johnson et al., 2016). This loss encourages the decoder to reconstruct the input image from its noisy latent representation vnoisy: Lpix-recon = LL1 + perceptual (D(vnoisy), x) . (7) Pixel Consistency Loss. This consistency loss ensures that the latent space is smooth and well structured by promoting that nearby latent vectors produce similar images: vNOISY = (v + σ e) , (4) Lpix-con = LL1 + perceptual(D(vNOISY), sg(D(vnoisy))) , (8) where the scalar σ controls the noise magnitude. Note that is applied again here to project the perturbed vector back onto the spherical surface. Jittering Sigma. To cover diverse directions on the sphere, we jitter σ during the training. By sampling scalar which also uses the combination of smooth L1 loss and perceptual loss, and sg() denotes stop-gradient operation. Latent Consistency Loss. It is well known that image similarity is better measured in latent space than in pixel space (Zhang et al., 2018; Radford et al., 2021). This is"
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "the reason why our pixel similarities use perceptual loss, which relies on features produced by static VGG model. To achieve stronger consistency loss, we also measure image similarity using the latent space of our own encoder. We want natural image and its noisy decoded representation D(vNOISY) to be semantically similar. The semantic similarity is measured by applying the encoder to both, and computing the cosine similarity between their latent representations. This yields the following loss: Llat-con = Lcosine similarity(v, E(D(vNOISY))) . (9) This loss serves an additional important purpose: It improves the iterative generation process we discuss later by encouraging the encoder to map distorted images, D(vNOISY), that may be off the image manifold to cleaned up latent vectors that reflect on-manifold images. Algorithm 1 pseudocode of generation forward pass. = Normal(0, 1).sample([L]) # random noise vector # one-step generation = spherify(e, sampling=False) # Eq.(1) w/o noise = D(v, y) # is class embedding if do_dec_cfg: # in pixel space x_uncond = D(v, y_null) # null class embedding = x_uncond + cfg * (x - x_uncond) # few-step refinement for _ in range(T - 1): # steps in total = E(x, y) if do_enc_cfg: # in latent space z_uncond = E(x, y_null) = z_uncond + cfg * (z - z_uncond) = spherify(z, sampling=True) # Eq.(4) w/ noise = D(v, y) if do_dec_cfg: x_uncond = D(v, y_null) = x_uncond + cfg * (x - x_uncond) Overall Loss. The overall training loss is weighted sum of the three components: return = Lpix-recon + Lpix-con + Llat-con . (10) More details about loss weights and training hyperparameters are provided in Appendix D. 2.4. Model Architecture Our architecture employs the standard ViT (Dosovitskiy, 2020) for both encoder and decoder. We insert 4-layer MLPMixers (Tolstikhin et al., 2021) in the end of the encoder (before spherification) and the beginning of the decoder. This aims to improve cross-token mixing and globalization of features without the expense of linear layers on the full flattened vector. final RMSNorm layer (Zhang & Sennrich, 2019) with learned affine parameters is added to each MLP-Mixer to bound the latent magnitude ( L). This regularization proves critical for stabilizing training, especially when there is dramatic divergence between the decoder outputs of vnoisy and vNOISY. We use both RoPE (Su et al., 2024) positional embedding and sinusoidal absolute positional encoding. We found that removing the sinusoidal positional embedding hurts generation quality. For class-conditional generation, we implement AdaLNZero (Perez et al., 2018; Peebles & Xie, 2023) in both the encoder and decoder using separate class embeddings. learned null embedding is trained for classifier-free guidance (CFG) (Ho & Salimans, 2022) with probability of 0.1. For image reconstruction tasks, we found using either random class embeddings or the null embedding is effective in the conditional setting. We default to the null embedding for simplicity. In addition, CFG can be applied in either the latent space (after the encoder), or the pixel space (after the decoder), or both. Algorithm 1 summarizes the forward pass for one-step or few-step generation at inference time. We count D(f (e)) as 4 one-step generation, and few-step generation as iteratively encoding and decoding 1 times 2. We fix = 1.0 in Equation (5) to use the maximum noise magnitude across all steps. For reconstruction task, no noise is added (r = 0.0). 3. Quantitative Experiments We adopt generation FID (gFID) (Heusel et al., 2017) and Inception Score (IS) (Salimans et al., 2016) to assess generation quality, while reconstruction FID (rFID) measures reconstruction quality. All metrics are computed using 50K randomly sampled training images. For class-conditional generation, we have balanced distribution with an equal number of random samples per class. We perform experiments on CIFAR-10 (Krizhevsky et al., 2009) with small image size 32 32, as well as ImageNet (Russakovsky et al., 2015), Animal-Faces (Choi et al., 2020), and Oxford-Flowers (Nilsback & Zisserman, 2008) with large image size 256 256. Center cropping and horizontal flipping with 0.5 probability are the only data augmentation. 3.1. Small Image Size We first comprehensively test our method with small image size (32 32) on CIFAR-10 (Krizhevsky et al., 2009). We build encoder and decoder with ViT Large, which consists of 24 layers and yields latent dimension = 16 16 8. The model, indicated as Sphere-L, is trained for 5000 epochs for conditional generation, 10000 epochs for unconditional generation, following the other setup in Appendix D. 2While step represents single model iteration regardless of CFG, NFE (number of function evaluation) counts the dual forward passes required by CFG."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 5. Uncurated CIFAR-10 conditional generation with different sampling steps and with/without CFG. Convincing images can be formed with single forward pass, with reliability and gFID improving with up to 4 steps. Table 1. Few-step generation results on CIFAR-10. Table 2. Few-step generation results (gFID ) on Animal-Faces (AF) and Oxford-Flowers (OF). METHOD SPHERE-L STEPS RFID GFID IS CONDITIONAL GENERATION W/O CFG 1 2 4 6 1 0.59 - - - - - STYLEGAN2 (KARRAS ET AL., 2020) STYLEGAN2 + ADA (KARRAS ET AL., 2020) UNCONDITIONAL GENERATION SPHERE-L DDIM (SONG ET AL., 2021) DDPM (HO ET AL., 2020) IMPROVED-DDPM (NICHOL ET AL., 2021) 1 2 4 6 1K 1K 4K 0.53 - - - - - - 18.68 8.28 2.72 1.65 6.96 3. 35.67 14.13 4.31 2.34 3.17 3.17 2.90 9.1 9.9 10.5 10.7 9.53 10.2 6.7 8.4 9.8 10.2 - 9.4 - Table 1 presents both conditional and unconditional generation results. For conditional generation, our method achieves strong results in both one-step and few-step generation, even without CFG. For example, with less than 10 sampling steps, sphere encoder yields gFID way below 2.0 and IS above 10. For unconditional generation task, our method achieves better gFID and IS with less than 10 sampling steps, 100 reduction in sampling steps, comparing to diffusion-based methods (Ho et al., 2020). Figure 5 depicts qualitative results with different steps and CFG. Visually, our generation results without CFG look the same as those using CFG. Appendix provides quantitative results on CIFAR-10 with CFG. Additionally, we discuss the memorization risk when training on small datasets like CIFAR-10 with extensive epochs in Appendix B. 5 SPHERE-L MODEL STEPS 1 2 6 OF W/ CFG=1.6 25.12 14.08 11.25 10. OF AF 27.12 21.70 16.60 19.29 12.98 18.23 12.26 17.97 STYLEGAN2 (KARRAS ET AL., 2020) AF - CAT AF - DOG AF - WILD 5.13 19.37 3.48 - - - - - - - - - 3.2. Large Image Size We then evaluate our method with large image size (256 256) on Oxford-Flowers (Nilsback & Zisserman, 2008) (8K images), Animal-Faces (Choi et al., 2020) (16K images), and ImageNet (Russakovsky et al., 2015) (1.2M images). Animal-Faces and Oxford-Flowers. We employ ViT Large for encoder and decoder, i.e., Sphere-L, with latent dimension of = 32 32 128, training for 1000 epochs. Due to the low diversity of Animal-Faces (3 classes), we train unconditional models, while for Oxford-Flowers, we utilize conditional generation for the 102 classes. For evaluation, we adhere to the standard protocol of randomly sampling 50K images, even though the training sets are relatively small. Table 2 reports quantitative results on both datasets. We report metrics only up to 6 sampling steps, as performance saturates beyond this point. Uncurated qualitative results are provided in Figures 17 and 18. ImageNet. We train class-conditional ViT-Large/-XLarge models on ImageNet for 800 epochs, utilizing latent dimension of = 32 32 64. Table 3 evaluates our approach alongside GANs, diffusion models, and other direct pixel-space generation frameworks. At comparable"
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 6. Latent interpolation on Animal-Faces and Oxford-Flowers. Images are generated in 4 steps without CFG. (left) Interpolation in 2D space that spans 4 synthetic images. (right) Each row interpolates between random vector on the sphere and class conditional vector y. Note that our model exhibits fast/sudden transitions between image classes rather than producing hybrid images that unrealistically merge properties of different object types. This property is necessary for model to reliably convert random samples from the sphere into realistic images, as it makes the probability of observing hybrid image small. parameter counts, our sphere encoder achieves competitive FID scores while requiring fewer sampling steps; its performance falls within the range of recent high-performing models, outperforming several established baselines. 3.3. Lower FID scores? Because low FID scores do not always align with perceptual realism (Stein et al., 2023), Table 3 reports FIDs for our qualitatively strongest models, prioritizing visual quality over the optimization of single numerical metric. Lower FID scores are possible with some tradeoffs. For example, while training on CIFAR-10 for 10K epochs can reduce FID to 0.94, it does not yield proportional gain in visual clarity. On ImageNet, increasing sampling steps beyond 4 improves the FID to 3.9 and sharpens local edges, yet this numerical gain can introduce more abstract object structures. This phenomenon where FID rewards local texture refinement even at the cost of global semantic coherence (Jayasumana et al., 2024) highlights nuanced trade-off in generative modeling that warrants further investigation. Still, our reported FID scores trail some recent highperformance generative models. We suspect this may be due to our use of pixel-space similarity losses, which likely contributes to the subtle edge blurring observed in our uncurated results (Figures 15 and 16). In contrast, other recent works achieve high sharpness and low-FID through similarity metrics based purely on latent-space representations or multi-stage GAN losses direction that should be evaluated in future work. Finally, note that our training and conditioning methods do not rely on the discreteness of the ImageNet ontology. This generality opens the door for our methods to be transferred to the text-to-image setting. Table 3. Few-step generation results on ImageNet 256 256. METHOD PARAMS STEPS GFID IS BIGGAN-DEEP (B21) STYLEGAN-XL (S22) GIGAGAN (K23) S S I ADM-G (D21) ADM-G SID (H23) SID2 (H24) JIT-H (L25) JIT-G JETFORMER (T25) FRACTALMAR-H (L25) SPHERE-L SPHERE-XL 56M 166M 569M 554M 554M 2B - 953M 2B 2.8B 848M 950M 1.3B 1 1 1 250 25 1000 512 100 100 - 96 4 6.90 2.30 3.45 4.59 5.44 2.44 1.38 1.86 1.82 6.64 6.15 4.76 4.02 171.4 265.1 225.5 186.7 - 256.3 - 303.4 292. - 348.9 301.8 265.9 4. Qualitative Experiments Latent Interpolation. In Figure 6, we interpolate between latent vectors on the Animal-Faces and Oxford-Flowers models to investigate the learned latent manifold. On Animal-Faces, we randomly sample four noise vectors in Equation (3) and visualize their corresponding images at the corners of the figure. We interpolate the latent space bilinearly to fill in the other images. On Oxford-Flowers, we randomly sample noise vector and class for each side of each row. Since the model is class-conditional, we interpolate both input noise and class embeddings linearly as we move horizontally across each row. We see that the model exhibits fast transitions between object types as we move through latent space. For example, starting with the bottom-left image of cheetah, we observe sudden transition from cheetah to cat as we move vertically, and from cheetah to dog as we move horizontally. The model does not linger in half-cheetah / half-dog state that is absent in the training data. These fast-transitions are"
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "cover the sphere, as do cats. As result, any uniformly sampled vector can be decoded to create the desired object. To better understand the latent space learned by our method, we visualize it in Figure 7. We extract in Equation (1) for all CIFAR-10 50K training samples. We project latents to 3D space using random Gaussian matrix, and then normalize each projected vector to have unit length. We visualize the results separately for three random classes (other classes look similar). We see that the latent space achieves conditional uniformity we get even coverage of the sphere for each class in isolation. 5. Image Editing This section presents two training-free editing applications that leverage the expressivity and robustness of our latent space: simple semantic manipulation and image crossover. Conditional Manipulation. Given an out-of-domain image, e.g., woolly panda in Figure 8, we can manipulate it by conditioning on different ImageNet classes. We simply encode and decode the image with multiple steps, using the Sphere-L model (in Table 3) trained on ImageNet. We set fixed noise strength = 1.0 in Equation (5) and γ = 0 in Equation (13) across all steps, and do not apply CFG. Figure 8 demonstrates that single step effectively captures the primary structure of the input while adapting its texture to align with the target class. Subsequent iterations (e.g., 4-step generation) further refine these class-specific characteristics and textures, achieving semantic alignment while preserving the original images structural integrity. Figure 7. Latent space visualization using random projection on CIFAR-10 training set. Each sphere shows the latent vectors of different class. The conditional latent distributions appear approximately uniform. required of reliable generative model, as the probability of sampling an impossible/hybrid image should be minimized. This important property of the sphere encoder differentiates it from other latent models. GANs, for example, tend to exhibit slow transitions, resulting in frequent production of distorted objects, e.g., Figure 8 and 9 in (Brock et al., 2019). Conditional Uniformity. The latent distribution of our model should be uniform on the sphere. For conditional models, the latent distribution must be conditionally uniform. To understand why, consider conditional model with two classes, cat and dog. Suppose we have an unconditional encoder and conditional decoder. It is likely that an unconditional encoder would structure latent space with cats in one region and dogs in another. Even if the union of the two classes achieves uniform coverage, conditional generation may fail; we cannot reliably decode dog from random vector, as half the time this vector will represent cat. We avoid this pitfall by using conditional encoder. In this case, our training objective naturally creates latent distribution that is conditionally uniform, i.e., dogs alone uniformly Figure 8. Conditional manipulation via iterative encoding and decoding on ImageNet model. We demonstrate the models expressivity using an out-of-domain input (a woolly panda top-left). Each row shows the result of conditioning the iterative process on different ImageNet classes without CFG. For example, the first row is conditioned on class 580 (greenhouse, nursery, glasshouse)."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 9. Image crossover using the sphere encoder trained on Animal-Faces. composite of two images (A and B) is iteratively processed through the encoder-decoder pipeline until it converges to coherent sample on the learned image manifold. Image Crossover. We further demonstrate the models capability for image crossover by processing manually stitched composites of distinct source images (Figure 9). This process similarly operates without CFG. By repeatedly encoding and decoding the stitched composite (up to 10 steps), the model naturally harmonizes the content and smooths boundary discontinuities. For these experiments, we set the noise magnitude = 0.25 (Equation (5)) and apply decay schedule (Equation (13)) with γ = 1, which we found yields the most seamless blending. This iterative refinement forces the manipulated image to converge toward valid point on the learned spherical manifold. Notably, unlike diffusion models, e.g., Figure 12 in (Liu et al., 2023), that require noise injection before projecting onto the image manifold, our encoder directly projects the stitched image into the latent space without adding noise (through the encoder). This deterministic path preserves the semantic integrity of the original sources while ensuring fluid, natural transition between them. 6. Main Ablations This section presents ablation studies on key design choices of our method. Additional ablations regarding CFG strategies, noise distribution, uniform regularization, volume compression ratio, and model size are provided in Appendix C. Determining Noise Magnitude. In this section, we analyze the maximum noise magnitude σmax in Equation (4) from geometric perspective to understand its impact, and empirically determine its optimal value. We begin with the noise-to-signal ratio (NSR) η as the ratio of the expected noise magnitude to the signal magnitude in highdimensional space: η = v = σmax = σmax . (11) 8 Because of the concentration of measure phenomenon, is nearly orthogonal to v, i.e., ve = 0 when . Thus, the angle α formed between + σ and satisfies: tan(α) v σmax = η , (12) which gives σmax an interpretable geometric meaning. The noise magnitude σmax can be equivalently expressed by either the angle α or the NSR η. We build conditional encoder and decoder using classic ViT with Base size (12 layers), and train them on ImageNet for 200 epochs. The latent dimension is = 16 16 256. We vary α from 45 to 88, corresponding to σmax = tan(α) from 1 to 28.6. Our first ablations are done to select the noise level. The NSR η guides the difficulty of the decoders task. The decoder aims to generate the same image from vNOISY as from the clean v. When α 45 (equivalently η 1), the noise does not overwhelm and the latent clouds generated by each training point are well separated. The decoder reconstructs images well, but the noisy latents fail to cover the sphere. In this case, generation from random sampling using Equation (3) fails, with high gFID in Figure 10. As α 90, the latent representations of images are forced apart and the decoder starts to generate images from random latents. The gFID drops significantly in Figure 10. Figure 11 shows some sampled images with different α, demonstrating that lower α leads to abstract and blurry images, while higher α produces more realistic details. Once we dial in α, we find that we can fix it for various latent dimensions because of the dimension-invariant property of angle α in Equation (11). However, we found that the optimal α varies slightly with image size. For small image size, e.g., 32 32 on CIFAR-10, we found α = 80 works best. For large image size, e.g., 256 256 on ImageNet, we found α = 85 works best."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 10. Quantitative impact of the angle α on ImageNet. Figure 11. Qualitative impact of the angle α on ImageNet with 4-step generation. Table 4. Ablation of loss terms on ImageNet. Loss terms are added incrementally from top to bottom. W/O CFG W/ CFG = 1.6 LOSS STEPS RFID GFID IS GFID IS LPIX-RECON + LPIX-CON + LLAT-CON 1 4 1 4 1 1.70 - 0.89 - 1.32 - 25.35 13.58 17.19 10.12 15.40 7. 168.8 162.9 183.8 189.9 188.3 176.4 20.68 11.76 14.74 10.47 13.14 7. 204.4 231.7 218.3 247.1 225.1 243.1 Training Loss. We evaluate the individual contribution of each proposed loss term in Section 2.3 to generation quality. Adopting the same ImageNet setup as the previous ablation, Table 4 reports the results of adding each term incrementally. Starting with only the pixel reconstruction loss Lpix-recon, we observe the model can generate images, but the quality is suboptimal with serious waffle artifact. Adding the pixel consistency loss significantly improves generation quality by removing the artifact, as it encourages the decoder to produce consistent images from perturbed latents. Including the latent consistency loss yields the best performance, demonstrating its effectiveness in guiding the encoder-decoder pair to learn coherent latent manifold. Figure 12 visualizes the optimization path from noisy latent to clean latent on the sphere for those two consistency losses during training. Overall, each loss contributes positively to the models ability to generate high-quality images. Latent Spatial Resolution. In these ablations, we keep the latent dimension constant and vary the latent spatial resolution by adjusting channel depth of latent dim L. Table 5 presents the results on ImageNet, suggesting that higher latent spatial resolution with volume compression ratio 3.0 works best on ImageNet, i.e., = 322 64. On CIFAR-10, Animal-Faces, and Oxford-Flowers, we also observed that higher latent spatial resolution yields better 9 Figure 12. Consistency optimization path from noisy latent to clean latent on the sphere, pushing the decoder to generate consistent and diverse images from right to left. Table 5. Ablation on latent spatial resolution with two optimal compression ratios on ImageNet. W/O CFG W/ CFG = 1. LATENT STEPS RFID GFID IS GFID IS 162 322 128 162 256 322 64 VOLUME COMPRESSION RATIO = 1.5 1 4 1 0.64 - 0.60 - 16.06 10.03 20.63 15.45 169.8 153.0 145.1 127. 14.25 8.71 17.69 12.75 187.6 180.8 164.9 154.6 VOLUME COMPRESSION RATIO = 3.0 1 1 4 0.74 - 0.61 - 16.93 8.30 16.80 7.65 206.3 202. 227.8 219.4 15.39 7.85 15.33 7.48 227.7 232.3 267.0 252.4 generation quality, but with lower compression ratio 1.5. The finalized latent dimensions are detailed in Appendix D. Sampling Schemes. In Algorithm 1, the few-step generation involves adding noise in spherifying at each step following Equation (4). We investigate two key aspects of this noise injection mechanism: strength schedule and sampling schedule. First, regarding the noise strength controlled by in Equation (5), we compare two cases: (a) fixed = 1.0 for all steps, and (b) decaying schedule, where decreases from 1.0 to minimum value. Second, we also have two options for sampling the noise vector in Equation (4): (a) sampling independent noise for each step, versus (b) sharing the same noise across all steps. We qualitatively and quantitatively evaluate both aspects without CFG. We simply decay the noise strength with linear schedule: (cid:18) = 1 (cid:19)γ , 1 1 (13) where is the current step from 2 to in the loop, and decay rate γ = 1. The = 1 is the first step of decoder forward pass, which is not involved with spherifying process. When γ = 0, it corresponds to the fixed schedule, i.e., = 1.0. Table 6 presents the results on ImageNet with those four sampling schemes. The fixed schedule (γ = 0) outperforms the decaying schedule (γ = 1) across all sampling steps. In addition, sharing the same noise across all steps consistently yields better results than using independent noise for each step. We hypothesize that sharing the same noise helps maintain consistent direction during the optimization path on the sphere, leading to more stable and effective generation. Overall, the best sampling scheme is using fixed"
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Table 6. Ablation on sampling schemes with few-step generation and no CFG. Results are reported with gFID on ImageNet. γ 0 1 SHARE 2 4 - - 16.57 16.66 16.73 16. 7.99 7.97 8.12 8.02 7.68 5.99 9.02 8.53 6 7.49 7. 12.17 12.32 8 10 7.61 10.11 15.08 17.50 7.71 12. 18.09 22.78 noise strength with shared noise across all steps. Figure 13 shows generated images on both CIFAR-10 and ImageNet under different sampling schemes. Visual inspection confirms that sharing the same noise across steps yields significantly more coherent and higher-quality results than using independent noise. Notably, on ImageNet, we observe that sharing noise with decay schedule γ = 1 produces exceptionally sharp images as the number of sampling steps increases. For example, with 10 steps, the images exhibit distinct, hyper-sharp aesthetic reminiscent of paper art. Achieving Uniform Distribution. Our method does not employ explicit regularization to achieve uniform latent distribution on the sphere. Comprehensive ablation studies in Appendix C.3 demonstrate that such regularization is unnecessary, as our method naturally achieves the desired latent properties. 7. Related Work Spherical latent space has been explored primarily through variational inference, using priors such as the von MisesFisher distribution (Xu & Durrett, 2018; Davidson et al., 2018; De Cao & Aziz, 2020; Ke & Xue, 2025). However, these approaches inherit limitations from VAEs, including significant posterior-prior mismatch. These issues are compounded in high-dimensional latent spaces, where sampling relies on intricate reparameterization or rejection sampling techniques. Although (Zhao et al., 2019) drew inspiration from StyleGAN (Karras et al., 2019; 2020; 2018) to enable direct sampling in high-dimensional unit spherical space via simple normalization, their experiments were limited to toy datasets like MNIST. Since VAEs can already perform direct sampling on MNIST (Dai & Wipf, 2019; Rezende & Viola, 2018), the advantages of spherical latent spaces in this context remain unclear. Crucially, the potential of highdimensional spherical latent spaces (Kumar & Patel, 2026) for image generation remains significantly underexplored. Few-step generation has been extensively studied in both GANs and diffusion models. GANs (Goodfellow et al., 2014) are inherently created for one-step generation. While approaches like ProgressiveGAN (Karras et al., 2018) introduce multi-stage refinement, these primarily serve as training strategies (Denton et al., 2015; Zhang et al., 2017; Brock et al., 2019) rather than altering the fundamental single-pass Figure 13. Qualitative impact of sampling schemes on generation without CFG for CIFAR-10 (top) and ImageNet (bottom). The indicates sharing the same noise across steps. Shared noise with γ = 1 yields superior coherence and sharp paper art aesthetic on ImageNet as sampling steps increase. inference process. Conversely, diffusion models rely on an iterative generation process, typically requiring hundreds to thousands of steps. Although distillation (Frans et al., 2024; Yin et al., 2024; Xie et al., 2024; Salimans & Ho, 2022; Geng et al., 2025; Sauer et al., 2024) and consistency techniques (Song et al., 2023; Geng et al., 2024; Yang et al., 2024; Lu & Song, 2025) have been proposed to accelerate this to few steps, their core insight aims to approximate the original diffusion trajectory. Pixel-space generation is common for GANs (Karras et al., 2020; Kang et al., 2023; Sauer et al., 2022; Brock et al., 2019; 2018) but challenging for diffusion models due to the high dimensionality of pixel space (Child, 2019; Van den Oord et al., 2016; Chen et al., 2020; Hawthorne et al., 2022; Yu et al., 2023). Recent advances based on diffusion mechanisms have made strides in pixel-space generation (Li et al., 2025; Li & He, 2025; Yu et al., 2025). Our sphere encoder diverges fundamentally from both paradigms. Its few-step mechanism iteratively traverses between latent and pixel spaces, grounded in spherical latent space. Signal processing. Concepts from this work take inspiration from sphere encoders/decoders in wireless networking that distribute codewords uniformly across sphere (Studer et al., 2008; Studer & Bolcskei, 2010). 8. Conclusion The Sphere encoder is novel generative framework that enables few-step, high-fidelity image synthesis. Our key observation is that the distribution of latents can be tightly controlled on uniform sphere, enabling the training of an autoencoder that can be directly sampled."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "This work is intended to be proof-of-concept for direct conditional and unconditional generation from an autoencoder, and we suspect this first implementation is far from optimal. To illustrate the average generation quality, Figure 14 shows randomly selected ImageNet results, with comprehensive uncurated examples provided in Figures 15 and 16. One drawback of our approach is that parameters must be allocated for both the encoder and decoder, and two passes are needed through the encoder during training, i.e., one for latent encoding and another for consistency loss. Interestingly, improvements that enable single-pass generation for more complex distributions would eliminate the need for the encoder at inference time, and possibly at training time as well. We think there are many avenues for future research that could unlock this and other capabilities, e.g., text-to-image generation."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Tan Wang, Chenyang Zhang, Tian Xie, Wei Liu, Felix Juefei Xu, and Andrej Risteski for their valuable discussions and feedback. Figure 14. Randomly selected images generated by the Sphere encoder for ImageNet (256 256). Results are generated using Sphere-XL with 4-step sampling and CFG = 1.4."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 15. Uncurated results on ImageNet (256 256). Results are generated using Sphere-XL with 4-step sampling and CFG = 1.4."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 16. Uncurated results on ImageNet (256 256). Results are generated using Sphere-XL with 4-step sampling and CFG = 1.4."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 17. Uncurated qualitative results on CIFAR-10 (32 32), Oxford-Flowers and Animal-Faces (256 256). Results are 2-step generation without CFG."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Figure 18. Uncurated qualitative results on CIFAR-10 (32 32), Oxford-Flowers and Animal-Faces (256 256). Results are 4-step generation without CFG. 15 A. Additional Results on CIFAR-10 Table 7. Few-step conditional generation results on CIFAR-10 with longer 10K training epochs."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Table 8 shows that using CFG = 1.2 yields slight improvement over the baseline in Table 1. For example, with 6-step sampling, gFID decreases from 1.65 (no CFG) to 1.41 (with CFG), and IS increases from 10.7 to 10.8. B. Memorization Risk on CIFAR-10 In the case of training longer on CIFAR-10, e.g., around 10K epochs following common practice of diffusion models (Song et al., 2023; Geng et al., 2025), we found our model sometimes generates near-duplicate samples. Figure 19 presents near-duplicate samples, i.e., the bird, from different sampling runs. This phenomenon indicates our model may memorize some training samples when trained excessively to overfit the small-scale data distribution. However, our model generates the flipped version of the bird, which is not exactly the same as the real image, suggesting that the model does not simply copy the training image but rather learns transformation of it. This aligns with known memorization issue in diffusion models (Somepalli et al., 2023; Wen et al., 2024). This longer training improves generation quality further, as shown in Table 7, e.g., gFID = 0.94 and IS = 11.1 with 10-step sampling with CFG. Figure 19. Memorization risk with longer 10K training epochs on CIFAR-10. Each row is different sampling run showing nearduplicate birds of the real training image (in red box). C. Additional Ablations C.1. CFG Position In Algorithm 1, we have three options to apply CFG: (1) only in the pixel space after decoding; (2) only in the latent space after encoding; (3) in both latent and pixel spaces, termed as combo. Since the combo option applies CFG twice, we halve the CFG scale for each application to keep the overall strength consistent. For example, if the overall CFG scale is s, we use s1/2 for each position. Table 9 presents the results on ImageNet with different CFG positions and scales. We observe that applying CFG in the pixel space consistently outperforms applying it in the latent space. The combo option with CFG = 1.6 achieves the best results for 4-step sampling on ImageNet. Unless otherwise specified, we apply combo for all experiments. 16 METHOD STEPS RFID GFID IS SPHERE-L SPHERE-L WITHOUT CFG 1 2 4 6 8 10 0.68 - - - - - WITH CFG = 1.2 1 2 4 6 8 10 - - - - - - 16.02 4.92 1.24 1.01 1.01 1.00 16.57 5.17 1.20 0.96 0.94 0.94 9.5 10.3 10.7 10.9 10.8 10. 9.5 10.4 10.8 10.9 10.9 11.1 Table 8. Few-step generation results on CIFAR-10 with CFG. METHOD CFG STEPS RFID GFID IS SPHERE-L 1.2 1 2 4 6 8 10 0.59 - - - - - 18.82 8.41 2.53 1.41 1.18 1.12 9.1 10.0 10.6 10.8 10.8 10.9 C.2. Dialing in the Noise Distribution Because the noise magnitude has strong impact on image quality, we dial this in by considering few more sophisticated sampling strategies for the noise magnitude. After determining the optimal maximum angle α in the previous section, we further explore whether mixing range of larger angles during training can enhance generation quality. We conduct experiments on CIFAR-10 with latent size = 16 16 8. We train the model for 2000 epochs. In Table 10, we compare three settings: (1) first row: base case where angles are sampled uniformly from [0, 80]; (2) second row: the base case mixing with larger angles from [80, 85] with 0.1 probability; and (3) third row: the base case mixing with even larger angles from [80, 89] with 0.1 probability. We observed that mixing in constrained range of larger angles (e.g., [80, 85]) improves generation quality for both one-step and four-step sampling. However, including excessively high angles, e.g., [80, 89], degrades generation quality and also causes unstable training with gradient explosions. We therefore conclude that augmenting the training data with moderate band of large angles enhances quality. Accordingly, we adopt this mixing strategy for all experiments: for CIFAR-10 with small image size 32, we add angles from [80, 85] with 0.1 probability to the base range of [0, 80]; for ImageNet, Animal-Faces, Oxford-Flowers with large image size 256, we add angles from [85, 89] with 0.1 probability to the base range of [0, 85]."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Table 9. Ablation on CFG position. Results are reported with gFID on ImageNet. The sampling scheme is the fixed strength with sharing noise in spheriying process across all steps. STEPS W/ CFG = 1.0 STEPS W/ CFG = 1.2 STEPS W/ CFG = 1.4 STEPS W/ CFG = 1.6 CFG POSITION ENC DEC COMBO 1 16.7 16.7 16.7 2 7.9 8.0 7.9 6.0 5.9 6.0 6 7.9 7.8 7.8 8 1 10.0 10.1 10. 16.7 15.0 15.3 2 8.0 8.3 8.0 4 6.1 4.7 5.1 7.8 5.1 6.1 8 1 10.3 6.2 7.9 16.8 15.4 15.1 7.9 9.3 8.2 4 6.1 5.0 4.7 6 7.9 4.5 5.2 1 2 4 10.5 5.0 6.5 16.8 16.4 15.0 7.9 10.2 8. 6.11 5.7 4.7 6 8.1 4.7 4.7 8 10.7 4.8 5.7 Table 10. Impact of mixing range of larger angles during training on CIFAR-10. All settings yield rFID scores in the range of 0.460.48. Table 11. Ablation on explicit uniform regularization on the spherical latent space. W/O CFG W/ CFG = 1.6 ADD-ONS STEPS RFID GFID IS GFID IS W/O CFG W/ CFG = 1. BASE MAX α MIX RANGE STEPS GFID IS GFID IS 80 80 80 - - [80, 85] [80, 89] 1 4 1 4 1 4 21.17 8.48 19.82 7.98 29.60 9. 8.8 10.0 8.7 10.1 8.8 10.1 18.93 7.47 18.39 7.05 30.23 8. 9.3 10.1 9.1 10.2 9.2 10.1 - + BN + LSWD 1 4 1 4 1 4 1.20 - 0.94 - 1.01 - 17.56 8.73 17.28 8.39 18.95 8.41 182.5 177.6 181.6 171.3 164.4 165. 14.88 8.05 14.71 7.65 15.47 7.46 217.8 245.0 216.0 240.3 202.1 235. C.3. Explicit Distribution Regularization The ideal distribution on the sphere is uniform. To form such distribution, we could add regularization term to the encoder output. We investigate two options: (1) Batch Normalization (BN) (Ioffe, 2015) on the encoder output before spherifying; Since we sample noise from Normal distribution, BN encourages the encoder output to be Gaussian, which is close to the distribution of noise. (2) SWD loss (Rowland et al., 2019; Kolouri et al., 2019; Wu et al., 2019; Deshpande et al., 2019), which is sliced Wasserstein distance between the encoder output and uniform distribution on the sphere. We implement the loss LSWD by constructing orthogonal random projections following (Rowland et al., 2019) Algorithm 3 to the encoder output before spherifying. Starting with the baseline model without any regularization, we then gradually add BN and SWD loss to evaluate their effects on generation quality. As shown in Table 11, applying BN marginally improves generation quality for both one-step and four-step sampling. However, adding SWD loss on top of BN slightly degrades generation quality. This suggests that our noisy spherifying method already encourages near-uniform distribution on the sphere, and additional BN or SWD regularization may not be necessary. There are downsides to these regularizations as well. BN introduces extra calibration steps during inference to adjust batch statistics, which complicates the generation process (Brock et al., 2018) (see details in Appendix C.4). And SWD is expensive with large latent dimensions, requiring latent caching and memory for random projection matrices. Table 12. Recalibrating BN stats for inference on ImageNet. W/O CFG W/ CFG = 1.6 CALIB BY STEPS RFID GFID IS GFID IS ON-THE-FLY GENERATED IMAGES REFERENCE IMAGES 1 1 4 1 4 0.99 - 0.99 - 1.00 - 14.21 6. 14.21 7.30 14.21 7.46 214.2 202.3 214.2 211.1 214.2 209.3 13.02 7. 13.02 8.08 13.02 8.06 246.0 268.2 246.0 273.9 246.0 273.6 C.4. BatchNorm Recalibration common issue from BatchNorm (Ioffe, 2015) is the traintest discrepancy for the batch statistics, i.e., running mean and variance. Directly using the training statistics during inference may lead to performance degradation. Using batch norm, we need to calibrate the stats of bn layers during inference. DCGANs (Radford et al., 2016) does not calibarate it. However, BigGANs (Brock et al., 2018) found the calibration affect the generation quality significantly, and run the testing samples to get the BN stats. In Table 12, we compare the results of using training stats and calibrated stats for BN on ImageNet. For calibration, we have three options: (1) no calibration: reset the BN training stats and then directly use the model for generation, the BN stats are updated on-the-fly. (1) calibrate with reference images: run the model on the training set to accumulate BN statistics prior to generation. (2) calibrate with generated images: sample 500 images to get the BN stats before generating the final samples for evaluation. The results indicate that explicit calibration is not strictly necessary."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Table 13. Ablation on pixel/latent volume compression ratio on ImageNet without CFG. COMPRESSION RATIO LATENT STEPS RFID GFID IS 24 12 6.0 3.0 1. 1.0 162 32 162 64 162 128 162 256 162 162 768 1 4 1 4 1 4 1 4 1 1 4 1.61 - 1.38 - 1.15 - 1.01 - 0.69 - 0.64 - 27.99 15.34 20.82 10.28 17.26 7.72 14.20 6.80 14.63 9. 18.36 14.03 195.6 245.1 233.8 259.0 225.0 229.4 214.2 201.7 136.0 125. 97.6 85.6 Table 14. Ablation of noise prior distribution on ImageNet. W/O CFG W/ CFG = 1.2 TRUNC α STEPS GFID IS GFID IS - 0.05 0. 2.0 1 4 1 4 1 4 1 4 17.81 5. 17.89 5.88 17.96 5.89 17.82 5.85 233.9 192.3 231.7 192.2 231.4 193. 233.5 191.7 16.30 5.02 15.81 5.08 15.73 5.02 15.66 5.06 261.0 229. 277.0 228.2 275.8 229.7 281.0 228.3 Table 15. Detailed quantitative results for the impact of the angle α on ImageNet for Figure 10. W/O CFG W/ CFG = 1. ANGLE α 45 80 81 83 84 85 86 87 88 σ 1. 5.7 6.3 8.1 9.5 11.4 14. 19.1 28.6 STEPS RFID GFID IS GFID 1 4 1 4 1 4 1 4 1 1 4 1 4 1 4 1 4 0.69 - 0.87 - 0.94 0.99 - 1.15 1.32 - 1.26 - 1.29 - 1.53 - 69.57 80.32 60.14 42.72 47.68 37.64 20.15 13.05 13.96 11. 15.40 7.53 15.37 9.96 16.40 9.99 23.27 13.15 31.3 18.7 33.4 39. 40.4 43.4 107.7 102.2 161.7 116.9 188.3 176.4 232.7 229.7 245.0 257. 203.5 241.5 59.09 73.04 51.73 34.14 40.11 28.88 16.21 8.70 12.30 8. 13.14 7.28 14.15 11.14 14.94 11.50 17.72 12.24 IS 39.9 21. 41.3 54.3 51.1 62.2 135.1 151.0 186.0 171.5 225.1 243.1 267.1 292. 291.3 315.1 268.7 304.4 Figure 20. Quantitative impact of volume compression ratio on ImageNet. Details in Table 13. C.5. Volume Compression Ratio Distinct from classical VAEs, our encoder outputs high latent dimension. Our pixel/latent volume compression ratio is much smaller than that of standard VAE, for example, 1.5 for CIFAR-10 and 3.0 for ImageNet, compared to 48 for standard VAE (e.g., from pixel 2562 3 to latent 322 4) (Rombach et al., 2022; HaCohen et al., 2024). We furthur study the impact of pixel/latent volume compression ratio for the latent dim L. It is well known that, for autoencoders, higher compression ratio typically leads to worse reconstruction quality but eases diffusion models to fit in (Yao et al., 2025; Zheng et al., 2025). However, this is not necessarily true for our method. We vary the channel depth of latent dim to adjust the volume compression ratio while keeping the spatial resolution fixed, i.e., 162 for image size 256 on ImageNet. The results are plotted in Figure 20, demonstrating two optimal compression ratios around 1.5 and 3.0 for few-step generation. This ratio is way lower than that of typical autoencoders used in diffusion models, e.g., 48 in (Rombach et al., 2022; Zheng et al., 2025). Unless otherwise specified, we use the optimal compression ratio 3.0 for ImageNet and 1.5 for CIFAR-10, Animal-Faces and Oxford-Flowers. C.6. Noise Prior Distribution Algorithm 1 samples noise in Equation (3) from isotropic Gaussian (0, I) as the input latent for generation. Since we spherify it before feeding into the decoder, we assume the generation should be insensitive to the specific choice of noise prior distribution. This is contrast to GANs (Brock et al., 2019; Karras et al., 2020; Sauer et al., 2022), which apply truncation tricks (Karras et al., 2019) to the latent prior distribution to improve generation fidelity. We compare two noise prior distributions: (1) the standard Gaussian (0, I); and (2) truncated Normal (0, I) truncated to [α, α], where α is the truncation threshold. Table 14 confirms the generation quality is insensitive to the choice of noise prior distribution, as the gFID and IS are similar across different α values."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Table 16. Training hyperparameters and model details for main experiments. HYPERPARAMETER DATASET CIFAR-10 ANIMAL-FACES OXFORD-FLOWERS IMAGENET IMAGE SIZE BATCH SIZE LEARNING RATE (LR) LR DECAY SCHEDULE MIN LR WEIGHT DECAY OPTIMIZER WARMUP EPOCHS TOTAL EPOCHS MODEL ENCODER / DECODER SIZE NUMBER OF TRANSFORMER BLOCKS NUMBER OF ATTENTION HEADS TRANSFORMER HIDDEN SIZE MLP-MIXER DEPTH CLASS CONDITION MODEL PARAMS VOLUME COMPRESSION RATIO LATENT DIM ANGLE α JITTER RANGE ANGLE α MIX RANGE ANGLE α MIX PROBABILITY LPIX-RECON SMOOTH L1 LOSS WEIGHT LPIX-RECON PERCEPTUAL LOSS WEIGHT LPIX-CON SMOOTH L1 LOSS WEIGHT LPIX-CON PERCEPTUAL LOSS WEIGHT LLAT-CON LOSS WEIGHT 32 256 1 104 COSINE 1 106 0.0 ADAMW 10 5K SPHERE-L LARGE 24 16 1024 2 921M 1.5 162 [ 0, 80] [80, 85] 0.1 1.0 1.0 0.5 0.5 0.1 256 256 1 104 COSINE 1 106 0.0 ADAMW 10 1K SPHERE-L LARGE 24 16 1024 4 - 642M 1.5 322 128 [ 0, 85] [85, 89] 0.1 25.0 1.0 1.0 1.0 0.1 256 256 1 104 COSINE 1 106 0.0 ADAMW 10 1K SPHERE-L LARGE 24 16 1024 4 948M 1.5 322 [ 0, 85] [85, 89] 0.1 25.0 1.0 1.0 1.0 0.1 256 256 1 104 COSINE 1 106 0.0 ADAMW 5 800 SPHERE-L, -XL LARGE, XLARGE 24, 28 16 1024, 1152 4 950M 3.0 322 64 [ 0, 85] [85, 89] 0.1 50.0 1.0 25.0 1.0 0.1 D. Hyperparameters Table 16 lists the training hyperparameters and model details for our main experiments. In addition, for unconditional generation on CIFAR-10, the only difference is removing the class condition and training for 10K epochs. We found EMA for smoothing weights in checkpoints may not noticeably change FID or sample quality, likely because we use cosine annealing learning rate schedule, which already provides smoothing effect."
        },
        {
            "title": "Impact Statement",
            "content": "Our work proposes new generative framework that offers fresh perspective on image generation and yields various benefits, including fast sampling and high-dimensional latent space for image generation. Although this specific method does not raise unique ethical challenges, we acknowledge ongoing general concerns inherent to the field, and we encourage continued communiy discussion to ensure responsible development and mitigation of potential risks."
        },
        {
            "title": "References",
            "content": "Aneja, J., Schwing, A., Kautz, J., and Vahdat, A. contrastive learning approach for training variational autoencoder priors. In NeurIPS, 2021. Bourlard, H. and Kamp, Y. Auto-association by multilayer perceptrons and singular value decomposition. Biological cybernetics, 1988. Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. In ICLR, 2018. Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. In ICLR, 2019. Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In ICML, 2020. Child, R. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choi, Y., Uh, Y., Yoo, J., and Ha, J.-W. Stargan v2: Diverse image synthesis for multiple domains. In CVPR, 2020. Dai, B. and Wipf, D. Diagnosing and enhancing vae models. In ICLR, 2019. Davidson, T. R., Falorsi, L., De Cao, N., Kipf, T., and Tomczak, J. M. Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018. De Cao, N. and Aziz, W. The power spherical distribution. arXiv preprint arXiv:2006.04437, 2020. Denton, E. L., Chintala, S., Fergus, R., et al. Deep generative image models using Laplacian pyramid of adversarial networks. In NeurIPS, 2015. Deshpande, I., Hu, Y.-T., Sun, R., Pyrros, A., Siddiqui, N., Koyejo, S., Zhao, Z., Forsyth, D., and Schwing, A. G. Max-sliced wasserstein distance and its use for gans. In CVPR, 2019. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Frans, K., Hafner, D., Levine, S., and Abbeel, P. One arXiv preprint step diffusion via shortcut models. arXiv:2410.12557, 2024. Geng, Z., Pokle, A., Luo, W., Lin, J., and Kolter, J. Z. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean In NeurIPS, flows for one-step generative modeling. 2025. Ghosh, P., Sajjadi, M. S., Vergari, A., Black, M., and Scholkopf, B. From variational to deterministic autoencoders. In ICLR, 2020. Girshick, R. Fast r-cnn. In ICCV, 2015. Goodfellow, I., Bengio, Y., and Courville, A. Deep http://www. MIT Press, 2016. Learning. deeplearningbook.org. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In NeurIPS, 2014. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Hawthorne, C., Jaegle, A., Cangea, C., Borgeaud, S., Nash, C., Malinowski, M., Dieleman, S., Vinyals, O., Botvinick, M., Simon, I., et al. General-purpose, long-context autoregressive modeling with perceiver ar. In ICML, 2022. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, volume 30, 2017. Hinton, G. E. and Zemel, R. Autoencoders, minimum description length and helmholtz free energy. In NeurIPS, 1993. Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2022."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Toronto, ON, Canada, 2009. Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. Kumar, A. and Patel, V. M. Learning on the manifold: unlocking standard diffusion transformers with representation encoders. arXiv preprint arXiv:2602.10099, 2026. Hoogeboom, E., Mensink, T., Heek, J., Lamerigts, K., Gao, R., and Salimans, T. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. Ioffe, S. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. Jayasumana, S., Ramalingam, S., Veit, A., Glasner, D., Chakrabarti, A., and Kumar, S. Rethinking fid: Towards better evaluation metric for image generation. In CVPR, 2024. Johnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In CVPR, 2023. Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018. Karras, T., Laine, S., and Aila, T. style-based generator architecture for generative adversarial networks. In CVPR, 2019. Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T. Training generative adversarial networks with limited data. In NeurIPS, 2020. Ke, G. and Xue, H. Hyperspherical latents improve arXiv continuous-token autoregressive generation. preprint arXiv:2509.24335, 2025. Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. In NeurIPS, 2018. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Kingma, D. P. and Welling, M. An introduction to variational autoencoders. arXiv preprint arXiv:1906.02691, 2019. Kolouri, S., Pope, P. E., Martin, C. E., and Rohde, G. K. Sliced-wasserstein autoencoder: An embarrassingly simple generative model. In ICLR, 2019. Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. LeCun, Y. Phd thesis: Modeles connexionnistes de learning models). lapprentissage https://api.semanticscholar.org/CorpusID:151887454, 1987. (connectionist Li, T. and He, K. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. Li, T., Sun, Q., Fan, L., and He, K. Fractal generative models. In TMLR, 2025. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. Lu, C. and Song, Y. Simplifying, stabilizing and scaling continuous-time consistency models. In ICLR, 2025. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. In ECCV, 2024. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B. Adversarial autoencoders. In ICLR, 2015. Nichol, A. Q., Dhariwal, P., and et al. Improved denoising diffusion probabilistic models. In ICML, 2021. Nilsback, M.-E. and Zisserman, A. Automated flower classification over large number of classes. In ICCV, 2008. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Perez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with general conditioning layer. In AAAI, 2018. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. Studer, C., Burg, A., and Bolcskei, H. Soft-output sphere decoding: Algorithms and vlsi implementation. IEEE Journal on selected areas in Communications, 2008. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Rezende, D. J. and Viola, F. Taming vaes. arXiv preprint arXiv:1810.00597, 2018. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Rowland, M., Hron, J., Tang, Y., Choromanski, K., Sarlos, T., and Weller, A. Orthogonal estimation of wasserstein distances. In AISTATS, 2019. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. In IJCV, 2015. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Improved techniques for Radford, A., and Chen, X. training gans. In NeurIPS, 2016. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In SIGGRAPH, 2022. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In ECCV, 2024. Somepalli, G., Singla, V., Goldblum, M., Geiping, J., and Goldstein, T. Understanding and mitigating copying in diffusion models. In NeurIPS, 2023. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In ICLR, 2021. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In ICML, 2023. Stein, G., Cresswell, J., Hosseinzadeh, R., Sui, Y., Ross, B., Villecroze, V., Liu, Z., Caterini, A. L., Taylor, E., and Loaiza-Ganem, G. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. In NeurIPS, 2023. Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasserstein auto-encoders. In ICLR, 2018. Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A. Mlpmixer: An all-mlp architecture for vision. In NeurIPS, 2021. Tomczak, J. and Welling, M. Vae with vampprior. In AISTATS, 2018. Tong, S., Zheng, B., Wang, Z., Tang, B., Ma, N., Brown, E., Yang, J., Fergus, R., LeCun, Y., and Xie, S. Scaling text-to-image diffusion transformers with representation autoencoders. arXiv preprint arXiv:2601.16208, 2026. Tschannen, M., Pinto, A. S., and Kolesnikov, A. Jetformer: An autoregressive generative model of raw images and text. In ICLR, 2025. Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. Conditional image generation with pixelcnn decoders. In NeurIPS, 2016. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In NeurIPS, 2017. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wen, Y., Liu, Y., Chen, C., and Lyu, L. Detecting, explaining, and mitigating memorization in diffusion models. In ICLR, 2024. Wu, J., Huang, Z., Acharya, D., Li, W., Thoma, J., Paudel, D. P., and Gool, L. V. Sliced wasserstein generative models. In CVPR, 2019. Studer, C. and Bolcskei, H. Softinput softoutput single tree-search sphere decoding. IEEE Transactions on Information Theory, 2010. Xie, S., Xiao, Z., Kingma, D., Hou, T., Wu, Y. N., Murphy, K. P., Salimans, T., Poole, B., and Gao, R. Em distillation for one-step diffusion models. In NeurIPS, 2024."
        },
        {
            "title": "Image Generation with a Sphere Encoder",
            "content": "Xu, J. and Durrett, G. Spherical latent spaces for stable variational autoencoders. In EMNLP, 2018. Yang, L., Zhang, Z., Zhang, Z., Liu, X., Xu, M., Zhang, W., Meng, C., Ermon, S., and Cui, B. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. Yao, J., Yang, B., and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In CVPR, 2024. Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte In NeurIPS, sequences with multiscale transformers. 2023. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Yu, Y., Xiong, W., Nie, W., Sheng, Y., Liu, S., and Luo, J. Pixeldit: Pixel diffusion transformers for image generation. arXiv preprint arXiv:2511.20645, 2025. Zhang, B. and Sennrich, R. Root mean square layer normalization. In NeurIPS, 2019. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., and Metaxas, D. N. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Zhao, D., Zhu, J., and Zhang, B. Latent variables on spheres for autoencoders in high dimensions. arXiv preprint arXiv:1912.10233, 2019. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025."
        }
    ],
    "affiliations": [
        "Meta",
        "University of Maryland"
    ]
}