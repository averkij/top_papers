{
    "paper_title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
    "authors": [
        "Jian Hu",
        "Dimitrios Korkinof",
        "Shaogang Gong",
        "Mariano Beguerisse-Diaz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are sparsely distributed and not pre-segmented. Moreover, long-form video understanding often relies on supervised hierarchical training that needs extensive annotations which are costly, slow and prone to inconsistency. With ViSMaP we bridge the gap between short videos (where annotated data is plentiful) and long ones (where it's not). We rely on LLMs to create optimised pseudo-summaries of long videos using segment descriptions from short ones. These pseudo-summaries are used as training data for a model that generates long-form video summaries, bypassing the need for expensive annotations of long videos. Specifically, we adopt a meta-prompting strategy to iteratively generate and refine creating pseudo-summaries of long videos. The strategy leverages short clip descriptions obtained from a supervised short video model to guide the summary. Each iteration uses three LLMs working in sequence: one to generate the pseudo-summary from clip descriptions, another to evaluate it, and a third to optimise the prompt of the generator. This iteration is necessary because the quality of the pseudo-summaries is highly dependent on the generator prompt, and varies widely among videos. We evaluate our summaries extensively on multiple datasets; our results show that ViSMaP achieves performance comparable to fully supervised state-of-the-art models while generalising across domains without sacrificing performance. Code will be released upon publication."
        },
        {
            "title": "Start",
            "content": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting Jian Hu1,2* Dimitrios Korkinof2 1Queen Mary University of London Shaogang Gong1 Mariano Beguerisse-DÃ­az2 2Spotify jian.hu@qmul.ac.uk, dkorkinof@spotify.com, s.gong@qmul.ac.uk, marianob@spotify.com 5 2 0 A 2 2 ] . [ 1 1 2 9 5 1 . 4 0 5 2 : r Abstract We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, system to summarise hour long videos with nosupervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are sparsely distributed and not pre-segmented. Moreover, long-form video understanding often relies on supervised hierarchical training that needs extensive annotations which are costly, slow and prone to inconsistency. With ViSMaP we bridge the gap between short videos (where annotated data is plentiful) and long ones (where its not). We rely on LLMs to create optimised pseudo-summaries of long videos using segment descriptions from short ones. These pseudo-summaries are used as training data for model that generates long-form video summaries, bypassing the need for expensive annotations of long videos. Specifically, we adopt meta-prompting strategy to iteratively generate and refine creating pseudo-summaries of long videos. The strategy leverages short clip descriptions obtained from supervised short video model to guide the summary. Each iteration uses three LLMs working in sequence: one to generate the pseudo-summary from clip descriptions, another to evaluate it, and third to optimise the prompt of the generator. This iteration is necessary because the quality of the pseudo-summaries is highly dependent on the generator prompt, and varies widely among videos. We evaluate our summaries extensively on multiple datasets; our results show that ViSMaP achieves performance comparable to fully supervised state-of-the-art models while generalising across domains without sacrificing performance. Code will be released upon publication. CCS Concepts Computing methodologies Motion capture; Unsupervised learning; Video summarization. Keywords Transfer Learning, Long Video Summary, Video Understanding"
        },
        {
            "title": "1 Introduction\nVideo captioning models are typically trained on large datasets\ncomposed of short videos along with their corresponding captions;\nthese videos are usually shorter than three minutes [10, 43]. While\nthis approach equips models to describe simple actions like walking\nor talking, it struggles with the complexity of long-form videos\n(videos longer than three minutes). These include videos, such as\nvlogs, sporting events, or movies, which are often complex and\ncould be an hour or longer in duration. When used on hour-long",
            "content": "Partly done during Spotify internship. Corresponding author. (a) Verb frequency: Segments vs Summaries. Illustration of the semantic gap between segment descriptions and video summaries in Ego4D-HCap [13] dataset. (b) t-SNE visualization between Ego4D-HCap and Youcook2 datasets. Illustration of the distributional shift between these two dataset. Figure 1: Two main challenges we address with our approach: (a) Bridging the semantic gap between short-form segment descriptions and hour-long video summaries descriptions (b) Overcoming the domain shift between the source domain and the target domain. videos, short-form video models are only able to produce descriptions for short segments of video that represent isolated actions without capturing the underlying story, resulting in extraneous details [13, 19]. For example, model may capture individual actions in biographical film, but it will struggle to summarise the significant events of persons life. To extend the capabilities of video captioning to longer durations, MA-LMM [9] and LaViLa [61] use LLMs to describe long-form videos with duration up to 10 minutes. These models still struggle with the extensive content of hour-long videos, for which there is notable lack of suitable training datasets. Addressing this gap, Ego4D [8] introduced the first extensive hour-long video dataset; however, these videos are mostly first-person, and they differ from the typical third-person videos (e.g., vlogs, documentaries, etc), which may limit their broader use in video understanding tasks. To better summarise hour-long videos, the authors of Video Recap [13] Conference17, July 2017, Washington, DC, USA Jian et al. Figure 2: Motivation of our VisMaP. Most existing video summarisation models focus on minute-level short-form videos, while hour-long videos, which are more common in real-world scenarios, are often overlooked due to their length, content complexity, and the prohibitively high cost of manual annotation. We propose cross-domain unsupervised approach for hour-long video summarisation. It leverages the inductive power of multiple LLMs to generate high-quality pseudo-summaries from short video segments via meta-prompting. These pseudo-summaries are then used to train model, enabling effective summarisation of long videos without costly human annotations. introduced recursive video captioning model trained on an hourlong video dataset with multi-granularity annotations. It identifies key moments from redundant information to generate effective summaries. However, annotating hour-long videos at multiple levels of granularity is not only costly but also prone to annotator inconsistencies [13]; this creates the need for careful quality control of the annotations, which imposes additional cost and scaling constraints. In contrast, extensive collections of annotated short-form videos are readily available for training models. In this work, we propose scalable approach to generate unsupervised hour-long video summaries. Our aim is to train lightweight summarisation model from annotated short-form videos in source domain, and then generalise to unannotated hour-long videos in target domain. For this purpose, we develop methods to transition from recognising single actions in short videos and summarising the more complex activities in unannotated hour-long videos (See Fig.2). There are three main challenges in understanding complex behaviours in hour-long videos from their associated atomic actions. The first challenge is identifying the salient atomic actions within large collection of redundant information. The second is bridging the semantic gap between atomic actions in short-form segments and their corresponding complex behaviour within long videos (see Fig. 1a). complex behaviour may consist of many atomic actions (where order may also matter) that collectively identify the behaviour, thus certain level of reasoning is required to bridge the semantic gap between those atomic actions and the complex behaviour. Finally, we also address the semantic gap the may exist between the source and target domains due to differences in the content, as is the case for instance between the Ego4D-HCap and Youcook2 datasets (see Fig. 1b). To address these challenges we introduce Video Summarisation by Meta Prompting (VisMaP). An overview of the framework is shown in Fig. 3, and consists of three stages: First, we use minutelong annotated videos as the source domain to construct lightweight short-form video summary model. Second, we treat unsupervised hour-long videos as the target domain and split them into minute-long segments to reduce the semantic gap caused by video length. We then use the summary model from Stage 1 to generate pseudo-captions for these segments transfering the visual information into the text domain. After that, we use these pseudo-captions along with meta-prompting to construct optimised pseudo-summaries (i.e., summaries of long videos generated without human intervention) for hour-long target domain videos. Using the contextual reasoning and reflection capabilities of LLMs, our method iteratively discovers and selects relevant semantic tokens associated with atomic activities in the source domain and complex activities in the target domain, aiming to construct optimised pseudo-summaries. Finally, the third challenge is to fine-tune the foundational model from Stage 1 to generate hour-long video summaries from minute-long segment pseudo-captions, using the optimised pseudo-summaries as supervision. We evaluate the performance of VisMaP on five datasets, and we provide theoretical results showing that VisMaP reduces the error upper bound due to the domain gap caused by both video length and semantic distribution. Our contributions are the following: To our knowledge, we are the first to present and address the challenge of unsupervised hour-long video summary. We formulate mechanism to extend video understanding from annotated short-form videos to unlabelled hour-long ones. We do this by optimising semantic alignment, and bridging the gap caused by longer video lengths and semantic distribution. Specifically, we present meta-prompting, using LLMs to optimise the selection and localisation of key events in hour-long videos. With pseudo captions from models trained on short-form videos, our approach uses sequences of atomic action descriptions to iteratively obtain more accurate holistic summaries of hour-long videos. We conduct experiments on five datasets to demonstrate the effectiveness and robustness of ViSMaP. These experiments show that we can attain comparable performance to fully supervised state of the art models, in an unsupervised and, importantly, domain adaptable manner. ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting Conference17, July 2017, Washington, DC, USA Figure 3: An overview of our VisMaP. (a) First stage: we use 180-second source video ğ‘£ğ‘  for supervised pretraining to establish basic summary capabilities. (b) Second stage: we split hour-long target videos ğ‘£ğ‘¡ into 3-minute segments set Vğ‘¡ ğ‘– and process them through the first-stage summary model to generate pseudo captions (cid:98)ğ¶ğ‘¡ . (cid:98)ğ¶ğ‘¡ are then refined through meta-prompting process with ğ¾ iterations, using Gemini as the evaluator and GPT-3.5 as the optimiser and the generator, to create more tailored prompts ğ‘ƒğ‘Ÿ ğ‘¡ and summaries (cid:98)ğ‘Œ ğ‘¡ . (c) Third stage: Refined (cid:98)ğ‘Œ ğ‘¡ pseudo-summaries are utilised to fine-tune the summary model for effective hour-long video summary."
        },
        {
            "title": "2 Related Works\nVisual-Language Models enhance cross-modality understanding\nand reasoning [2, 45, 57]. CLIP [37] and ALIGN [14] were the first\nto expand language models to vision-language tasks; LLaVA [30]\nand MiniGPT-4 [64] combine LLM and image embeddings to give\nLLMs the ability to understand images. Subsequently, Refs. [22, 26]\nenable video understanding by modelling the temporal sequences\nof extracted video embeddings. Refs. [20, 23] present further im-\nprovements on the video understanding capabilities by constructing\nhigher-quality large-scale video datasets for training. In spite of\ntheir good results, one obstacle limiting their performance is the\nneed for large long-form video datasets with extensive annota-\ntions, which are difficult and expensive to obtain. VisMaP sidesteps\nthis challenge by requiring only short-form video annotations and\nadapting them to long-form videos.\nShort-form video models typically process videos under 3 min-\nutes and include tasks like: a) Video Question Answering [24, 51],\nwhich answers questions based on an understanding of the entire\nvideo clip; b) video captioning [12, 43] goes further, requiring more\ndetailed descriptions for different clips of a video; c) video ground-\ning [54, 60] locates the temporal moments in a video that correspond\nto a given text description. These tasks only require temporal or\nspatial localisation and understanding; hour-long video summarisa-\ntion, on the other hand, is more complex because it needs to identify\ncrucial frames within a vast amount of redundant information and\nto comprehend them.\nLong-form Video Models normally handle videos between 3 and\n10 minutes; for example, Refs. [9, 49] optimise memory to retain\ncontext for longer video analysis; Refs. [23, 46] filter visual tokens to\nexpand the range of video lengths that can be processed; LLoVi [55]\nuses LLMs to summarise short video clip captions. All of these",
            "content": "models struggle with hour-long videos because identifying sporadically dispersed important moments among extensive information is difficult for them. LongVA [58] and LLaVA-Video [59] are capable of performing VQA on hour-long videos, but they require largescale annotated long video datasets for training and attain limited performance on video summarisation tasks. Video ReCap [13] uses recursive supervised training for hour-long first-person videos to identify important moments and create summaries, but requires extensive annotations and struggles with videos in third-person perspective. VisMaP avoids these problems by relying on metaprompting to perform spatio-temporal modelling, and generate summaries from redundant information in long videos. Domain Adaptation (DA) transfers useful information from labelled source domain to an unlabelled target domain [31]. For example, conventional DA approaches require simultaneous access to both source and target domain data during training, and typically rely on adversarial learning [11, 41] or high-order moment matching [32, 62]. In contrast, source-free DA methods only need model previously trained on the source domain for target adaptation [17, 25]; VisMaP falls into this category. While previous work focus mostly on classification and segmentation, VisMaP is the first to our knowledge that applies DA methods to generate unsupervised hour-long video summaries. Furthermore, VisMaP not only identifies target atomic activities in source domains (with domain gaps), but also discovers undefined new complex activities."
        },
        {
            "title": "3 Methodology\n3.1 Problem Definition\nWe tackle a cross-domain video summarization task using a la-\nğ‘›ğ‘ \nbelled short-form dataset Dğ‘  = {ğ‘‰ ğ‘ \nğ‘–=1 as our source do-\nmain. Here, ğ‘‰ ğ‘ \nğ‘– denotes",
            "content": "ğ‘– represents 3-minute short-form video, ğ¶ğ‘  ğ‘– , ğ‘Œ ğ‘  ğ‘– } ğ‘– , ğ¶ğ‘  Conference17, July 2017, Washington, DC, USA Jian et al. ğ‘›ğ‘¡ ğ‘–= captions for each 4-second segment within the video, and ğ‘Œ ğ‘  is ğ‘– the summary of the entire 3-minute video. In addition, we employ an unlabelled hour-long dataset Dğ‘¡ = {ğ‘‰ ğ‘¡ as our target doğ‘– } main. The variables ğ‘›ğ‘  and ğ‘›ğ‘¡ indicate the number of videos in the source and target domains, respectively. We make no assumptions about the distributions of Dğ‘  and Dğ‘¡ ; that is, data in the two domains can originate from different distributions (e.g., the source domain could be first-person shopping videos, and the target domain is third-person cooking videos). After being trained on the fully-labelled source dataset Dğ‘  , the model needs to generate ğ‘Œ ğ‘¡ ğ‘– caption ğ‘Œ ğ‘¡ ğ‘—=1 that summarises video ğ‘‰ ğ‘¡ ğ‘– = [ğ‘¦ğ‘¡ ğ‘–,ğ‘— is the ğ‘—-th token in the caption. ğ‘– , where ğ‘¦ğ‘¡ ğ‘–,ğ‘— ]"
        },
        {
            "title": "3.2 Short-form Video Learning\nFirst we develop a primary short-form video summary model that\nidentifies key information and summarises it. Inspired by multi-\nlevel annotations in the Ego4D-HCap dataset [13], we divide Ego4D\nvideos into 3-minute segments ğ‘‰ ğ‘ \n(which we use as the source do-\nğ‘–\nmain Dğ‘  ). These segments have detailed descriptions ğ¶ğ‘ \nğ‘– for every\n4 seconds of video from Ego4D, and captions ğ‘Œ ğ‘ \nğ‘– for the entire 3-\nminute segment from Ego4D-HCap. During training, the model\nreceives 3-minute video segments ğ‘‰ ğ‘ \nğ‘– along with the corresponding\n4-second descriptions ğ¶ğ‘ \nğ‘– . The model learns to summarise 3-minute\nsegments in a supervised manner using the segment captions ğ‘Œ ğ‘ \nğ‘– . We\nencode video segments ğ‘‰ ğ‘ \nğ‘– using the frozen temporal visual trans-\nformer TimeSFormer [5] as feature encoder ğ¹ . A visual-language\nalignment module ğ´ğ‘£ğ‘™ then aligns semantics between ğ‘‰ ğ‘ \nğ‘– and ğ¶ğ‘ \nğ‘– .\nFinally, a language model as text decoder ğ· generates 3-minute\nsegment descriptions as predictions. This model is trained with 3-\nminute segment descriptions ğ‘Œ ğ‘ \nğ‘– (See Fig.3(a)). We use cross-entropy\nand contrastive loss as follows:\nğ½\nâˆ‘ï¸",
            "content": "ğ¿pre (ğ‘‰ ğ‘ , ğ¶ğ‘ , ğ‘Œ ğ‘  ) = log ğ‘ƒ (ğ‘¦ğ‘  ğ‘— = ğ‘Œ ğ‘  ğ‘— ğ‘¦ğ‘  < ğ‘— , ğ‘‰ ğ‘ , ğ¶ğ‘  ) 1 2ğ‘ 2ğ‘ ğ‘=1 log ğ‘— =1 exp (cid:19) (cid:18) ğ‘§ğ‘‡ ğ‘ ğ‘§ğ‘+ ğœ exp (cid:19) (cid:18) ğ‘§ğ‘‡ ğ‘ ğ‘§ğ‘ ğœ (cid:205)2ğ‘ ğ‘=1 ğ‘ğ‘ (cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:171) (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) (1) , where ğ½ is the number of tokens in ğ‘Œğ‘— ; ğ‘§ğ‘ is clips feature vector and ğ‘§+ ğ‘ is the one from the immediate next clip, which is positive sample; ğ‘§ğ‘ are feature vectors from other clips in the same batch, which we use as negative samples; ğœ is scalar, and ğ‘ is the batch size. The first term in Eq. 1 is the cross-entropy loss, which helps the model learn video summary. The second term is temporal contrastive learning loss comparing sequential clips, helping the model find semantic connections between clips without supervision."
        },
        {
            "title": "3.3 Hour-long Video Summary with LLMs\nThe training in Sec. 3.2 gives the model basic video summary ca-\npabilities. However, directly applying this model to summarise\nhour-long videos yields poor results due to a domain gap caused\nby the difference in video length between the training source data\nand the target videos. We sidestep this difficulty by first segment-\ning long target videos ğ‘‰ ğ‘¡\ninto a collection of 3-minute segments\nğ‘–\nğ‘€ğ‘–\nğ‘š=1, where ğ‘€ğ‘– is the number of segments for video ğ‘–. Vğ‘¡\nVğ‘¡\nğ‘– = {ğ‘£ğ‘¡\nğ‘–\ncan eliminate the semantic gap caused by the video length, ensuring\nthat the source pre-trained model can effective summarise these",
            "content": "ğ‘–,ğ‘š } target segments (i.e., short video summary model struggles to summarise an entire football match but can effectively summarise the players short-term behaviours.). This way we obtain pseudo caption Ë†ğ‘ğ‘¡ ğ‘– from the pre-trained model: ğ‘–,ğ‘š for each segment Vğ‘¡ ğ‘–,ğ‘š = ğ· (ğ´ğ‘£ğ‘™ (ğ¹ (ğ‘£ğ‘¡ Ë†ğ‘ğ‘¡ ğ‘–,ğ‘š))), (2) ğ‘–,ğ‘š } With Eq. 2, we convert hour-long videos into collection of shortğ‘€ğ‘– ğ‘– = { Ë†ğ‘ğ‘¡ form segment descriptions (cid:98)ğ¶ğ‘¡ ğ‘š=1. Our working assumption is that in hour-long videos, only few key segments are relevant to the underlying narrative and identifying these segments from all the redundant information is particularly challenging task. Fortunately, this is one of the tasks that LLMs excel at, due to their text reasoning and reflection capabilities [40]. Still, one remaining challenge is that summaries from LLMs (cid:98)ğ‘Œ ğ‘¡ ğ‘– can be sensitive to the input prompt ğ‘ƒğ‘Ÿ ğ‘¡ ğ‘– . In addition, the optimal prompt is unknown, and may even differ between models, model versions or even across different videos. Inspired by studies of LLMs as optimisers [52], we propose ğ¾-iteration meta-prompting setup with multiple LLMs for iteratively refining the prompt ğ‘ƒğ‘Ÿ ğ‘¡ ğ‘– , which includes three components: generator, an evaluator and an optimiser (see the right half of Fig. 3). Generator LLM. Given an initial prompt ğ‘ƒğ‘Ÿğ‘¡ from the ğ‘˜-th itğ‘–,ğ‘˜ eration, the generator LLM produces summary capturing key information within (cid:98)ğ¶ğ‘¡ ğ‘– : (cid:98)ğ‘Œ ğ‘¡ ğ‘–,ğ‘˜ = ğ¿ğ¿ğ‘€gen ( (cid:98)ğ¶ğ‘¡ ğ‘– , , ğ‘ƒğ‘Ÿğ‘¡ ğ‘–,ğ‘˜ ). (3) (4) ğ‘–,ğ‘˜ ). ğ‘– , as follows: We can then iteratively refine the prompt to improve the summary, progressively distilling essential content from extensive textual input. This way we leverage the LLMs ability to efficiently extract key insights from from long-form texts often containing rich but sparse information. Evaluator LLM. The evaluator LLM evaluates the relevance and accuracy of the summary produced by the generator during the , given the captions (cid:98)ğ¶ğ‘¡ current iteration ğ‘†ğ‘Ÿ ğ‘¡ ğ‘–,ğ‘˜ ğ‘–,ğ‘˜ = ğ¿ğ¿ğ‘€eval ( (cid:98)ğ¶ğ‘¡ Sğ‘¡ ğ‘– , (cid:98)ğ‘Œ ğ‘¡ This assesses how well the summary (cid:98)ğ‘Œ ğ‘¡ captures the essence of ğ‘–,ğ‘˜ the captions, guiding the optimisation of the prompt in the next step. higher score indicates that the summary better reflects the key information in the captions. Optimiser LLM. The optimiser refines the generators prompt ğ‘ƒğ‘Ÿ ğ‘¡ using all previously evaluated prompts and their evaluation scores. We optimise the prompt by minimizing the domain gap and improving the summary in the next iteration: ğ‘–,0), ..., (ğ‘ƒğ‘Ÿğ‘¡ ğ‘–,ğ‘˜+1 = ğ¿ğ¿ğ‘€opt ((ğ‘ƒğ‘Ÿğ‘¡ ğ‘–,0, Sğ‘¡ We evaluated several combinations of LLMs in meta-prompting (see in Tab. 7) and in our final implementation we use OpenAIs GPT-3.5-Turbo [35] as the optimiser and generator, and Googles Gemini-1.5-Flash [38] as the evaluator. Using different LLMs helps reduce biases that might come from the LLMs and could be amplified during the optimisation process. Note that while some basic parts of the task description in ğ‘ƒğ‘Ÿğ‘¡ stay fixed, the flexible elements are ğ‘–,ğ‘˜ updated with each iteration, improving the summary (cid:98)ğ‘Œ ğ‘¡ ğ‘–,ğ‘˜ Early stopping strategy. We iteratively refine the meta-prompting ğ‘–,ğ‘˜, Sğ‘¡ ğ‘–,ğ‘˜ )). ğ‘–,ğ‘˜+1 ğ‘ƒğ‘Ÿ ğ‘¡ (5) . ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting Conference17, July 2017, Washington, DC, USA for each long video. However, given that videos differ in length and content, not all videos need the same number of iterations. If the summary score does not improve after few iterations ğ‘™ (ğ‘™ < ğ¾), we stop refining the prompt. Algorithm 1 Iterative Meta-Prompting for Hour-long Video Summarisation 1: Input: Labelled source short-form videos ğ‘‰ ğ‘  ; Unlabelled target hour-long video ğ‘‰ ğ‘¡"
        },
        {
            "title": "3.4 Hour-long Video Adaptation\nAfter obtaining an optimised set of pseudo-summaries (cid:98)ğ‘Œ ğ‘¡ for hour-\nlong videos ğ‘‰ ğ‘¡ , we use them to fine-tune the source pre-trained\nsummary model. We also use the pseudo-descriptions (cid:98)ğ¶ğ‘¡ as inputs\nfor 3-minute video segments, so at this stage, we no longer rely\non human annotations and source data. To account for potential\ninaccuracies in the pseudo-summaries (cid:98)ğ‘Œ ğ‘¡ , we adopt a learning-\nwith-noisy-labels strategy using a symmetric cross-entropy (SCE)\nloss [48] as follows:",
            "content": "ğ¿SCE (ğ‘‰ ğ‘¡ , (cid:98)ğ¶ğ‘¡ , (cid:98)ğ‘Œ ğ‘¡ ) = ğ¿CE (ğ‘‰ ğ‘¡ , (cid:98)ğ¶ğ‘¡ , (cid:98)ğ‘Œ ğ‘¡ ) + ğ¿RCE (ğ‘‰ ğ‘¡ , (cid:98)ğ¶ğ‘¡ , (cid:98)ğ‘Œ ğ‘¡ ), ğ‘ƒ (ğ‘¦ğ‘¡ ğ‘— = ğ‘¤ ğ‘¦ğ‘¡ < ğ‘— , ğ‘‰ ğ‘¡ , (cid:98)ğ¶ğ‘¡ ) log ğ‘ (ğ‘¤ ) (6) ğ½ ğ‘Š = ğ‘¤=1 ğ‘— =1 ğ½ ğ‘— =1 log ğ‘ƒ (ğ‘¦ğ‘¡ ğ‘— = (cid:98)ğ‘Œ ğ‘¡ ğ‘— ğ‘¦ < ğ‘—, (cid:98)ğ‘Œ ğ‘¡ ), ğ‘— = ğ‘¤ ğ‘¦ğ‘¡ where ğ‘ƒ (ğ‘¦ğ‘¡ < ğ‘— , ğ‘‰ ğ‘¡ , (cid:98)ğ¶ğ‘¡ ) is the probability predicted by the model that the ğ‘—-th token in the summary is ğ‘¤, given the previous output ğ‘¦ğ‘¡ < ğ‘— and the inputs, and ğ‘ (ğ‘¤) is the token probability of ğ‘¤ in pseudo annotations (cid:98)ğ‘Œ ğ‘¡ , represented as smoothed one-hot vector."
        },
        {
            "title": "4 Theoretical Analysis\nSuppose we have a collection of unlabelled hour-long videos Dğ‘¡ ,\nthat are annotated with pseudo-captions (cid:98)ğ‘Œ ğ‘¡ via meta-prompting,\nas outlined in the previous section. Then Dğ‘¡ can be partitioned\ninto two subsets: D+\nğ‘¡ , which includes videos with accurate pseudo-\ncaptions, and D âˆ’\nğ‘¡ , which contains videos with noisy pseudo cap-\ntions (i.e., summaries that are misaligned with the video content,\ndenote the label noise rate.\nincomplete or incorrect). Let ğœ‚ =\nğ‘¡ , denoted by U+\nWe assume that samples from D+\nğ‘¡ and\nU âˆ’\nğ‘¡ respectively, are independent and identically distributed (i.i.d.)\nof size ğ‘š each.",
            "content": "ğ‘¡ Dğ‘¡ ğ‘¡ and 2: Step 1: Short-form Video Learning 3: Extract visual features using frozen visual transformer ğ¹ . 4: Train the summary model with video ğ‘‰ ğ‘† and its fine-grained captions ğ¶ğ‘  ğ‘– (one every 4 seconds) as input, and the overall short-form summary ğ‘Œ ğ‘  as supervision, using the alignment module ğ´ğ‘£ğ‘™ and language model ğ·, optimised by loss Lğ‘ğ‘Ÿğ‘’ . 5: Obtain short-form video summary model. 6: Step 2: Pseudo Summary Generation 7: Divide unlabelled target video ğ‘‰ ğ‘¡ into short segments {ğ‘£ğ‘¡ 1, ğ‘£ğ‘¡ 2, . . . , ğ‘£ğ‘¡ ğ‘š }. 8: Generate segment pseudo-descriptions ğ¶ğ‘¡ = {ğ¶ğ‘¡ 1, ğ¶ğ‘¡ 2, . . . , ğ¶ğ‘¡ ğ‘š } using frozen models ğ¹, ğ´ğ‘£ğ‘™ , ğ·. 9: Initialize prompt ğ‘ƒğ‘Ÿ ğ‘¡ based on generated pseudo-descriptions ğ¶ğ‘¡ . 10: for iteration ğ‘˜ = 1, 2, . . . , ğ¾ do 11: Generator: Generate summary Ë†ğ‘Œ ğ‘¡ from prompt ğ‘ƒğ‘Ÿ ğ‘¡ ğ‘˜ ğ‘˜ Evaluator: Evaluate summary and output score ğ‘†ğ‘¡ . ğ‘˜ Optimizer: Optimize prompt to obtain improved prompt ğ‘ƒğ‘Ÿ ğ‘¡ if Converged or scores ğ‘†ğ‘¡ ğ‘˜ 1 ğ‘˜ over two consecutive iterations then show no improvement and ğ‘†ğ‘¡ ğ‘˜+1 . . 12: 13: 14: 15: Break 16: end if 17: end for 18: Obtain optimized summary Ë†ğ‘Œ ğ‘¡ . 19: Step 3: Hour-long Video Adaptation 20: Fine-tune modules ğ´ğ‘£ğ‘™ , ğ· using video ğ‘‰ ğ‘¡ , optimized pseudosummary Ë†ğ‘Œ ğ‘¡ with ğ¿ğ‘†ğ¶ğ¸ . 21: Output: Model for summarising hour-long videos Theorem 1. Let â„ be captioning hypothesis trained on Dğ‘¡ . With at least 1 ğ›¿ probability, the true error ğœ–ğ‘¡ (â„) is bounded by: (7) ğœ–ğ‘¡ (â„) 2ğœ–+ ğ‘‘ğ» Î”ğ» ( D+ ğ‘¡ (â„) + 1 2 ğ‘‘ log(2ğ‘š) + log (cid:16) 2 ğ‘š ğ›¿ ğ‘¡ , ğ‘¡ ) 1 2 (cid:17) + ğœ†, (cid:170) (cid:174) (cid:174) (cid:172) where ğœ† = ED+ ğ‘¡ + 2 (cid:169) (cid:173) (cid:173) (cid:171) ğ‘˜ğ‘¦ ğœ‚ğ‘¦ğ‘˜ â„“rce (ğ‘“ (ğ‘£), ğ‘˜ ) ğœ‚ğ‘¦â„“rce (ğ‘“ (ğ‘£), ğ‘¦) is the expected additional loss due to label noise. The left-hand side term in Eq. 7, 2ğœ–+ ğ‘¡ (â„), is constrained by supervised training on correctly labelled pseudo-captions. The second and third terms capture the domain divergence, and are regulated through the second stage of the long video summary using LLMs. The last term ğœ†, is governed by the SCE loss in Eq. 6 during the target adaptation phase, ensuring effective learning under noisy label conditions. Proof. Consider the unlabelled video domain Dğ‘¡ , which is partitioned into two subsets: D+ ğ‘¡ , containing accurately pseudo-captioned videos, and ğ‘¡ , containing noisy pseudo-captioned videos. The Conference17, July 2017, Washington, DC, USA Jian et al. Table 1: Domain adaptation results to Ego4D-HCap dataset Ego4D-HCap [13] results Model Video Encoder Text Decoder Human Ann. Hier. Train Train Params. Segment Description Video Summary M Fully supervised Unsupervised 258M 336M 586M 113M 24.6 38.2 39.1 46.9 33.3 38.1 38.8 39.7 15.3 16.6 16.9 18.6 6.5 18.0 20.1 29.3 24.0 29.5 30.1 32.6 11.0 12.8 13.2 14. LaViLa [61] LaViLa [61]+GPT2 [18] LaViLa [61]+FLANT5 [6] Video ReCap [13] Zero-Shot BLIP2 [21]+GPT3.5 [35] LaViLa [61]+GPT3.5 [35] Domain Adaptation VisMaP (ours) TSF-B TSF-B TSF-B TSF-B VIT-G TSF-B GPT2 GPT2 FT5-XL GPT FT5-XL GPT2 TSF-B GPT2 total error on the target domain can be expressed as: ğœ–ğ‘¡ (â„) = Eğ‘£ Dğ‘¡ [â„(ğ‘£) ğ‘“ (ğ‘£)] = ğœ–+ ğ‘¡ (â„) + ğœ– ğ‘¡ (â„). (8) By adding and subtracting appropriate terms and applying the triangle inequality, we derive: ğœ–ğ‘¡ (â„) = ğœ–+ ğ‘¡ (â„) ğ‘¡ (â„) ğœ–+ ğ‘¡ (â„) + ğœ–+ ğ‘¡ (â„) + ğœ– + ğœ–+ 2ğœ–+ + ğœ–+ ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„) + ğœ– ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„, ğ‘“ ) ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„, ğ‘“ +), ğ‘¡ (â„, ğ‘“ ) (9) where ğ‘“ + and ğ‘“ denote the true and noisy target functions, respectively. Using the Î”H -divergence [4], we have: ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğœ– ğ‘¡ (â„, ğ‘“ ) 1 2 ğ‘‘ HÎ”H (D+ ğ‘¡ , ğ‘¡ ). (10) Given empirical samples U+ ğ‘¡ and generalisation bounds, with probability at least 1 ğ›¿, we obtain: ğ‘¡ , and applying VC-dimension ğœ–ğ‘¡ (â„) 2ğœ–+ ğ‘¡ (â„) + 1 ğ‘‘ HÎ”H (U+ ğ‘¡ , ğ‘¡ ) 1 2 (cid:17) ğ‘‘ log(2ğ‘š) + log (cid:16) 2 ğ‘š + 2 (cid:169) (cid:173) (cid:173) (cid:171) ğ‘¡ (â„, ğ‘“ ) ğœ–+ + ğœ–+ (cid:170) (cid:174) (cid:174) (cid:172) ğ‘¡ (â„, ğ‘“ +), ğ›¿ (11) where ğ‘‘ is the VC-dimension of the hypothesis class . The final term quantifies the expected loss introduced by asymmetric label noise. Evaluated using the symmetric cross-entropy (SCE) loss, it corresponds to: ğœ–+ ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„, ğ‘“ +) = E(ğ‘£,ğ‘¦)D+ ğ‘¡ [â„“ (ğ‘“ (ğ‘£), ğ‘¦) â„“ (ğ‘“ (ğ‘£), ğ‘¦)] . (12) Thus, we recover the bound in Theorem 1, completing the proof. more detailed derivation is in Supplementary Material. 0 0 5.7 5.8 16.9 19.8 13.5 13.5 11.1 12.2 22.4 24. 12.1 12.5 113M 45.6 39.6 18.6 26. 29.9 13."
        },
        {
            "title": "5 Experiments\n5.1 Evaluation, Data and Testing Assumptions\nWe evaluate our approach in three scenarios: (1) We evaluate VisMaPâ€™s\nsummarisation ability under a video length gap using the Ego4D-\nHCap [13] dataset. (2) We measure VisMaPâ€™s ability to bridge cross-\ndomain gaps under varying content conditions on the short-form\nvideo caption datasets MSRVTT [50], MSVD [16] and YouCook2 [63].\n(3) We test VisMapâ€™s (trained on hour-long videos) cross-length gen-\neralisation ability on the short-form video dataset EgoSchema [34],\nwhich contains videos that are on average 3 minutes long.\nEgo4D-HCap [13] is an hour-long video dataset with three levels\nof detail: second-length clips, minute-length segments, and full\nvideos. We use the annotated minute-length video segments as\nthe source domain Dğ‘  and the unlabelled full videos as the target\ndomain Dğ‘¡ We compare VisMaP with Video ReCaP [13], the only\nmodel specifically designed for hour-long video summaries, and\nLaViLa [61], a video captioning model that can be extended for\nlong-form videos, both of which were trained on the Ego4D-HCap\ndataset. The Video Recap model uses additional 4-second video clips\nfor training. We also compare VisMaP against zero-shot approaches\nusing BLIP2 [21] combined with GPT 3.5 [35] and LaViLa with GPT\n3.5. We generated captions with BLIP2 and LaViLa respectively,\nand then processed these captions with GPT 3.5 to generate the\nsummaries. We also compared VisMaP with LaViLa+GPT2 [18],\nLaViLa+FLAN-T5 [6], both are fine-tuned on annotated 3-minute\nvideo segments and full videos.\nMSRVTT [50], MSVD [16] and Youcook2 [63] are video caption-\ning datasets featuring clips from various scenes, usually under 5\nminutes long, all shot from a third-person perspective. We use these\nvideos to evaluate the semantic generalisation ability of VisMaP\n(source pre-trained on 4-second video clips from the Ego4D-HCap\ndataset) under similar duration settings. We also compared VisMaP\nwith leading supervised methods [9, 28, 33, 44, 55].\nEgoSchema [34] includes over 5,000 curated multiple-choice ques-\ntions and answers over real-world videos with average duration\nof 3 minutes. We use this dataset to evaluate how well VisMaP\n(trained on hour-long videos) can perform on shorter video clips.\nWe segment videos into various clip lengths to generate hierarchical",
            "content": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting Conference17, July 2017, Washington, DC, USA Table 2: Short-form VideoQA on EgoSchema Input Ego4D QA Feature Pretain Acc Model GPT3.5 [35] FrozenBiLM [51] mPLUG-Owl [53] InternVideo [47] EgoVLP [29] EgoVLPv2 [36] Question Video Video Video Video Video 19.6 26.9 31.1 32.1 34. 34.2 44.3 50.2 52.5 53.4 LaViLa [61]+GPT3.5 [35] Captions Video ReCap [13] + GPT3.5 [35] Hier. Captions VisMaP + GPT3.5 [35] VisMaP + GPT4 [1] Hier. Captions Hier. Captions Table 4: Ablation study of different modules. Here TPL means target pseudo labelling, SCL is source contrastive learning, CSG is cycle summary generation, CPG is cycle prompt generation, and SCE is Symmetric Cross Entropy. Ego4D-HCap 26.9 28.0 28.8 29.8 29.9 Models variants SCL CSG CPG SCE 22.6 24.5 24.6 25.8 26.0 11.7 11.8 12.5 12.8 13.1 TPL 1 2 3 4 5 detailed video captions at different levels. We feed these captions into GPT3.5/4 to produce the final summary. We compared VisMaP with leading methods [29, 36, 47, 53] to highlight its adaptability and performance across various video formats and topical complexities. Metrics. To measure performance across all tests, we rely on three widely-used captioning metrics: CIDEr (C) [42], which quantifies the similarity between candidates captions and reference captions based on the consensus of ğ‘›-grams; ROUGE-L (R) [27], which measures the longest common subsequence between the generated text and the reference to assess fluency and text coherence; and METEOR (M) [3], which evaluates translation hypotheses by aligning them to reference translations based on their semantic and syntactic features. For the EgoSchema dataset, we use QA-accuracy to evaluate on the VQA task. Higher scores on these metrics indicate better performance. Implementation Details. Our model is trained on one NVIDIA A100 GPU for 25 epochs during each of the two phases. frozen TimeSformer [5] is used as an encoder to generate video embeddings. DistilBERT [39], language model, aligns video embeddings with text inputs. We aggregate the 4-second embeddings from TimeSFormer to form fixed-length embedding for various video lengths. GPT2 text decoder is used for the final output. We used the Adam optimiser [7] with learning rate of 3e-5 and weight decay of 0.01. We used GPT 3.5 Turbo as the optimiser and generator, Gemini 1.5 Flash as the evaluator, with 5 cycles meta-prompting to generate pseudo-summary (see Fig. 3). Table 3: Short-form Video Captioning Results Short-form Video Captioning Model Supervised UniVL [33] SwinBERT [28] GIT [44] Video-LLaMA [56] MA-LMM [9] Unsupervised Video ReCap [13] VisMaP (Ours) MSR-VTT MSVD YouCook2 C C 28.2 29.9 32.9 32.9 33.4 49.9 53.8 73.9 71.6 74.6 29.3 41.3 51.1 49.8 51.0 52.8 120.6 180.2 175.3 179.1 15.6 17.3 16.5 17. 127.0 109.0 129.8 123.7 131.2 7.1 29.1 5.5 52.8 10.1 38.9 8.2 113.3 7.5 15. 8.9 117.1 Table 5: Effect of iteration count on pseudo label generation. 24.5 24.9 25.1 25.8 26.0 25.8 iteration ğ¾ 1 2 3 4 5 6 11.8 12.2 12.6 12.9 13.1 13.0 28.0 28.6 29.4 29.6 29.9 29.9 Table 6: Evaluating LLMs in meta-prompting. Four LLM combinations Evaluator Optimiser Generator 1 Gemini [38] GPT3.5 [35] Mixtral [15] 2 3 4 5 6 7 Gemini GPT3.5 GPT4 [1] GPT3.5 GPT4 Gemini Mixtral Mixtral GPT3.5 GPT3.5 GPT4 GPT3. GPT3.5 Mixtral GPT3.5 GPT3.5 GPT4 GPT3.5 Ego-4D HCap 11.6 27.9 24.9 11.4 27.6 24.7 11.5 27.9 24.6 12.5 29.3 25.4 12.6 29.5 25.6 29.4 25.7 12.6 13.1 29.9 26."
        },
        {
            "title": "5.2 Results\nResults on Ego4D-HCap. Tab. 1 shows the performance of VisMaP\ncompared to supervised and unsupervised methods on the Ego4D-\nHCap dataset. Overall, zero-shot methods perform significantly\nworse than supervised methods, which underscores the complex-\nity of long-form summarisation. Our results show that VisMap\noutperforms methods that use video segments and full videos for\nsupervised learning. The one exception is the state-of-the-art Video\nRecap, which outperforms VisMap in summarisation because it is\nfully supervised; even then, VisMapâ€™s performance is comparable,\nwhich is impressive given that it is an unsupervised method. These\nresults show that using LLM-generated pseudo-summaries with\nmeta-prompting is an effective strategy for long-form video sum-\nmarisation, especially given that it sidesteps the need for costly\nannotations.\nResult on EgoSchema. Tab. 2 shows the evaluation of VisMap\ntrained on hour-long videos directly on the EgoSchema dataset\n(average duration of 3-minutes) to evaluate how well our model can",
            "content": "Conference17, July 2017, Washington, DC, USA Jian et al. Figure 4: An example of summaries from ViSMaP on the Ego4D-HCap dataset. generalise going from hour-long videos to short videos. VisMaP significantly outperforms long-form video methods and classical approaches, further proving its superior performance and generalisation ability across various video analysis tasks. The consistent strong performance across datasets highlights our methods robustness and adaptability to diverse videos and tasks. Results on short video captioning. Tab. 3 shows short video captioning evaluation on the MSRVTT, MSVD, and YouCook2 datasets. We pit unsupervised VisMaP against supervised learning approaches. Results show that VisMaP can effectively caption short videos with significant differences in perspective and content, and achieves results that are close to or even surpass supervised models. Qualitative Results on the Ego4D-HCap dataset. Fig. 4 shows an example of the type of summaries that VisMap can generate without supervision on 63 minute long video from the Ego4DHCap dataset. Following the flow in Fig. 3, the source summary model, which was trained on short-form videos, produces captions for each 30-second long segment. The collection of segment descriptions for this video has total of 3480 words. After adaptation, the target model, which is adjusted to content variations, selects the key information (shown in red boxes in the figure) to create an accurate and concise summary that is specific to the input video (third row in Fig. 4), which in this example is 19 words long. These summaries can be readily compared to ground truth summaries generated by human annotators (bottom row in Fig. 4). Module Analysis. To understand the contribution of the constituent components of our method to its performance we carry out an ablation study. The first row of Tab. 4 contains basic baseline approach that uses model pretrained on the source domain, and employs GPT-3.5 to create summaries. We use these summaries as pseudo-labels to train new captioning model. This baseline, similar to LLoVi [55], is less effective because it cannot accurately identify key information, resulting in summaries filled with unnecessary details. Row 2 contains model enhanced by incorporating source contrastive learning during pre-training, which enables the extraction of information from the source domain, and improves the accuracy of the target pseudo-summaries. Rows 3 and 4 contain models that introduce cycle summary and prompt generation strategies, respectively. These techniques use meta-learning to iteratively improve summaries and prompts, leading to more accurate pseudo-summaries and validating our meta-prompting approach. Finally, row 5 adds the SCE loss during target adaptation to reduce the impact of noisy labels and ensure effective semantic learning. Analysis of the number of iterations. In Tab. 5, we present an evaluation of the impact that the number of iterations ğ¾ has on performance. These results clearly show the benefit from each iteration up to the fifth, after which it stabilises. Therefore, we have set ğ¾ = 5 in our evaluations. LLMs in meta prompting. To understand which combination of LLMs as optimiser, evaluator and generator works best, we evaluate different combinations and report our results in Tab. 7. The results reveal that different LLM choices significantly impact performance. While GPT-4 demonstrates strong overall capabilities, it does not consistently outperform all configurations. Notably, the combination of GPT-3.5 as both the optimiser and generator, with Gemini as the evaluator, achieves the highest scores across all metrics, suggesting that Gemini provides effective evaluation feedback while GPT-3.5 generates outputs that align well with the evaluation criteria. In contrast, Mixtral-based setups generally yield lower performance, indicating that its effectiveness may be role-dependent. These findings highlight the importance of selecting LLMs based on their specific strengths rather than defaulting to single powerful model for all tasks. Supplementary material Sections S1 and S2 provide code information and implementation details. Section S3 presents an in-depth theoretical analysis, Sections S4 and S5 contain an analysis of the LLMs in meta-prompting and detailed limitations."
        },
        {
            "title": "6 Conclusion\nWe introduce ViSMaP, an unsupervised method for summarising\nhour-long videos by leveraging annotated short-video datasets and\na meta-prompting strategy. We first generate high-quality sum-\nmaries via meta-prompting and then train an end-to-end summari-\nsation model, reducing reliance on extensive annotations. Exper-\niments show ViSMaP achieves performance comparable to fully\nsupervised methods and adapts effectively to diverse video datasets.\nOne limitation of ViSMaP is that it relies on pseudo labels generated\nby a source-domain model, which may limit performance under\nlarge domain shifts. The current approach also uses only visual\ninformation, without incorporating audio or transcripts, which can\naffect summary quality. TFuture work can explore multimodal in-\nputs, hierarchical summarisation at different resolutions, and more\ngeneralisable meta-prompting methods for broader use.",
            "content": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting Conference17, July 2017, Washington, DC, USA References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large visionlanguage model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023). [3] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 6572. [4] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. theory of learning from different domains. Machine learning 79 (2010), 151175. [5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is space-time attention all you need for video understanding?. In ICML, Vol. 2. 4. [6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research 25, 70 (2024), 153. [7] Kingma Diederik. 2014. Adam: method for stochastic optimization. (No Title) (2014). [8] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. 2022. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18995 19012. [9] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. 2024. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1350413514. [10] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret Harsham, John Hershey, Tim Marks, and Kazuhiko Sumi. 2017. Attention-based multimodal fusion for video description. In Proceedings of the IEEE international conference on computer vision. 41934202. [11] Jian Hu, Hongya Tuo, Chao Wang, Lingfeng Qiao, Haowen Zhong, Junchi Yan, Zhongliang Jing, and Henry Leung. 2020. Discriminative partial domain adversarial network. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16. Springer, 632648. [12] Vladimir Iashin and Esa Rahtu. 2020. Multi-modal dense video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 958959. [13] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. 2024. Video ReCap: Recursive Captioning of Hour-Long Videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1819818208. [14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and visionlanguage representation learning with noisy text supervision. In International conference on machine learning. PMLR, 49044916. [15] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024). [16] Muhammad Umer Khan and Mustafa AH Hasan. 2020. Hybrid EEG-fNIRS BCI fusion using multi-resolution singular value decomposition (MSVD). Frontiers in Human Neuroscience 14 (2020), 599802. [17] Jogendra Nath Kundu, Naveen Venkat, Venkatesh Babu, et al. 2020. Universal source-free domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 45444553. [18] Klemens Lagler, Michael Schindelegger, Johannes BÃ¶hm, Hana KrÃ¡snÃ¡, and Tobias Nilsson. 2013. GPT2: Empirical slant delay model for radio space geodetic techniques. Geophysical research letters 40, 6 (2013), 10691073. [19] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg, and Mohit Bansal. 2020. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning. arXiv preprint arXiv:2005.05402 (2020). [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 (2024). [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning. PMLR, 1973019742. [22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023). [23] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2025. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision. Springer, 323340. [24] Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. 2022. Invariant grounding for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 29282937. [25] Jian Liang, Dapeng Hu, and Jiashi Feng. 2020. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International conference on machine learning. PMLR, 60286039. [26] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122 (2023). [27] Chin-Yew Lin. 2004. Rouge: package for automatic evaluation of summaries. In Text summarization branches out. 7481. [28] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. 2022. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1794917958. [29] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Xu, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. 2022. Egocentric video-language pretraining. Advances in Neural Information Processing Systems 35 (2022), 75757586. [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems 36 (2024). [31] Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, JeWon Kang, Jonghye Woo, et al. 2022. Deep unsupervised domain adaptation: review of recent advances and perspectives. APSIPA Transactions on Signal and Information Processing 11, 1 (2022). [32] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael Jordan. 2016. Unsupervised domain adaptation with residual transfer networks. arXiv preprint arXiv:1602.04433 (2016). [33] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020. Univl: unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353 (2020). [34] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2023. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems 36 (2023), 46212 46244. [35] OpenAI. 2021. GPT-3.5: Language Models are Few-Shot Learners. https://beta. openai.com. Accessed: [Insert date here]. [36] Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, and Pengchuan Zhang. 2023. Egovlpv2: Egocentric video-language pre-training with fusion in the backbone. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 52855297. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [38] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). [39] Sanh. 2019. DistilBERT, Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. arXiv preprint arXiv:1910.01108 (2019). [40] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. 2023. survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562 (2023). [41] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. 2017. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 71677176. [42] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 45664575. [43] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. 2018. Reconstruction network for video captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition. 76227631. [44] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100 (2022). [45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). [46] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. 2024. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889 (2024). [47] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. 2022. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022). [48] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. 2019. Symmetric cross entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF international conference on computer vision. 322330. [49] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. 2024. VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges. arXiv preprint arXiv:2409.01071 (2024). [50] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition. 52885296. [51] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems 35 (2022), 124141. [52] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023. Large Language Models as Optimizers. ArXiv abs/2309.03409 (2023). https://api.semanticscholar.org/CorpusID:261582296 [53] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023). [54] Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, and Chuang Gan. 2020. Dense regression network for video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10287 10296. [55] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. 2023. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235 (2023). [56] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instructiontuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023). [57] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024). [58] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. 2024. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852 (2024). [59] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713 (2024). [60] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. 2020. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1066810677. [61] Yue Zhao, Ishan Misra, Philipp KrÃ¤henbÃ¼hl, and Rohit Girdhar. 2023. Learning video representations from large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 65866597. [62] Haowen Zhong, Hongya Tuo, Chao Wang, Xuanguang Ren, Jian Hu, and Lingfeng Qiao. 2019. Source-constraint adversarial domain adaptation. 2019 IEEE International Conference on Image Processing (ICIP) (2019), 24862490. [63] Luowei Zhou, Chenliang Xu, and Jason Corso. 2018. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. [64] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023). Conference17, July 2017, Washington, DC, USA Jian et al. Supplementary Materials for ViSMaP: Unsupervised Hour-long Video Summary by Meta-Prompting Jian Hu1,2* Dimitrios Korkinof2 1Queen Mary University of London Shaogang Gong1 Mariano Beguerisse-DÃ­az2 2Spotify jian.hu@qmul.ac.uk, dkorkinof@spotify.com, s.gong@qmul.ac.uk, marianob@spotify.com Abstract We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, system to summarise hour long videos with nosupervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are sparsely distributed and not pre-segmented. Moreover, long-form video understanding often relies on supervised hierarchical training that needs extensive annotations which are costly, slow and prone to inconsistency. With ViSMaP we bridge the gap between short videos (where annotated data is plentiful) and long ones (where its not). We rely on LLMs to create optimised pseudo-summaries of long videos using segment descriptions from short ones. These pseudo-summaries are used as training data for model that generates long-form video summaries, bypassing the need for expensive annotations of long videos. Specifically, we adopt meta-prompting strategy to iteratively generate and refine creating pseudo-summaries of long videos. The strategy leverages short clip descriptions obtained from supervised short video model to guide the summary. Each iteration uses three LLMs working in sequence: one to generate the pseudo-summary from clip descriptions, another to evaluate it, and third to optimise the prompt of the generator. This iteration is necessary because the quality of the pseudo-summaries is highly dependent on the generator prompt, and varies widely among videos. We evaluate our summaries extensively on multiple datasets; our results show that ViSMaP achieves performance comparable to fully supervised state-of-the-art models while generalising across domains without sacrificing performance. Code will be released upon publication. CCS Concepts Computing methodologies Motion capture; Unsupervised learning; Video summarization; Keywords Transfer Learning, Long Video Summary, Video Understanding Contents Introduction Related Works Methodology Abstract 1 2 3 3.1 3.2 3.3 3.4 3.5 4 Problem Definition Short-form Video Learning Hour-long Video Summary with LLMs Hour-long Video Adaptation Algorithm Overview Theoretical Analysis 1 1 3 3 3 4 4 5 5 5 5 5.1 5.2 6 References Abstract Contents 7 8 9 10 Experiments Evaluation, Data and Testing Assumptions Results Conclusion Code Release More Implement Details Theoretical Details Evaluating LLMs in meta-prompting. Limitations 6 6 7 8 9 2 2 2 2 3"
        },
        {
            "title": "8 More Implement Details\nModule details. Our experiments (training and evaluation) are\nconducted using Pytorch on a single NVIDIA A100 GPU. Specifi-\ncally, the feature encoder is a TimeSformer, which processes videos\nresized to 224x224 pixels. This encoder inherits parameters from a\nmodel pre-trained as detailed in [61]. During training, the visual\nencoder remains frozen, and we use the output from the final cls\nlayer as the input for our visual language alignment module. In our\nVideo-Language (VL) Alignment module, we employ DistilBERT, a\npretrained 6-layer transformer encoder model. We maintain the self-\nattention blocks as frozen and introduce a trainable cross-attention\nmodule to each layer. The model processes video features from\nthe video encoder and captions produced in the preceding hier-\narchy as inputs. We employ a pretrained GPT2 model as a text\ndecoder, featuring a 12-layer transformer structure. Within each\ntransformer layer, we incorporate a gated cross-attention block,\ntraining only these blocks while freezing the remaining components.\nEach block comprises a cross-attention layer and a feed-forward\nlayer, enhanced by tanh gating initially set to zero.\nTraining details. The training of VisMaP involves three phases.\nThe first phase trains on supervised video segments from the Ego4d-\nHCap dataset, with a 3-minute duration per segment. During segments-\nlevel training, the model received two inputs, fine-grained descrip-\ntions ğ¶ğ‘  for every 4 seconds of video, along with a visual embedding\nfor every 3 minute of video generated by the feature encoder after\nsubsampling to a fixed dimension. The model is supervised with a\ntextual description ğ‘Œ ğ‘  for the entire 3-minute long video segment.\nThe fine-grained video captions and visual embedding are then\ninputted into a VL Alignment module for cross-modal semantic",
            "content": "Supplementary Materials for ViSMaP: Unsupervised Hour-long Video Summary by Meta-Prompting Conference17, July 2017, Washington, DC, USA Figure 5: Qualitative Results on Ego4D dataset. alignment. The aligned embeddings are subsequently fed into decoder, which acquires basic video summary and understanding capabilities from the ground-truth segment descriptions ğ‘Œ ğ‘  . This phase requires training for up to 50 epochs with batch size of 64. In the second phase, all model parameters are frozen, and the model processes unlabeled target dataset videos, split into 3-minute segments. More specifically the pretrained model from the first phase is used to generate collection of segment descriptions, one for each 3-minute segment of the long-form videos. These captions are then inputted into our meta-prompting with GPT-3.5 as both generator and optimizer, and Gemini-1.5 Flash as the evaluator, iterating through the prompt and summary optimization for up to maximum of 5 iterations to produce pseudo-summaries for each long video in the target domain. We also employ early-stopping for our iterative prompt adaptation, whereby we terminate the loop if there is no improvement. The third phase involves using the pseudo-summaries generated by our iterative prompt adaptation process to train full video summary model on unlabelled long-form videos. During this phase, the model inputs are segment descriptions for every 3 minutes of video, generated with the phase one segment-level model, and full video visual embeddings generated using the features encoder. The model inputs are fed into VL alignment module and the resulting embeddings are then inputted into decoder and supervised using the pseudo-summaries for up to 200 epochs, with the batch size set to 32. We use AdamW optimizer with learning rate of 3ğ‘’ 5 and weight decay of 0.01."
        },
        {
            "title": "9 Theoretical Details\nHypothesize a function represented as â„ : V â†’ {0, 1}. The proba-\nbility according to the distribution D that a hypothesis â„ disagrees\nwith a labeling function ğ‘“ is defined as:",
            "content": "by ğœƒ , trained on Dğ‘¡ , and belonging to hypothesis space with VC-dimension ğ‘‘, then with probability at least 1 ğ›¿, the following inequality holds: ğœ–ğ‘¡ (â„) 2ğœ–+ 1 2 ğ‘‘ HÎ”H (D+ ğ‘¡ (â„) + (cid:118)(cid:116) ğ‘‘ log(2ğ‘š) + log (cid:16) 2 ğ‘š ğ›¿ (cid:17) + 2 ğ‘¡ , ğ‘¡ ) (14) + ğœ†, ğ‘¡ (cid:104)(cid:205)ğ‘˜ğ‘¦ ğœ‚ğ‘¦ğ‘˜ â„“rce (ğ‘“ (ğ‘£), ğ‘˜) ğœ‚ğ‘¦â„“rce (ğ‘“ (ğ‘£), ğ‘¦) where ğœ† = ED+ the expected additional loss due to label noise, and ğ‘‘ HÎ”H (D+ denotes the distribution divergence. Proof. Recall that ğœ–ğ‘¡ (â„) = Eğ‘£ Dğ‘¡ [â„(ğ‘£)ğ‘“ (ğ‘£)], and Dğ‘¡ = {D+ Therefore: (cid:105) represents ğ‘¡ , ğ‘¡ ) ğ‘¡ , ğ‘¡ }. ğœ–ğ‘¡ (â„) = ğœ–+ ğ‘¡ (â„) ğ‘¡ (â„) ğœ–+ ğ‘¡ (â„) + ğœ–+ ğ‘¡ (â„) + ğœ– + ğœ–+ 2ğœ–+ + ğœ–+ ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„) + ğœ– ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„, ğ‘“ ) ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğ‘¡ (â„, ğ‘“ +), ğ‘¡ (â„, ğ‘“ ) (15) where ğ‘“ + and ğ‘“ are the true and noisy labeling functions, respectively. Following [4], the -divergence between distributions ğ· and ğ· over domain is: ğ‘‘ (ğ·, ğ·) = 2 sup â„ Pr ğ· (ğ¼ (â„)) Pr ğ· (ğ¼ (â„)), (16) ğœ– (â„, ğ‘“ ) = Eğ‘£D [â„(ğ‘£) ğ‘“ (ğ‘£)], (13) For any two hypotheses â„, â„ : Theorem 1. Consider an unlabeled hour-long video domain Dğ‘¡ , to which pseudo-captions (cid:98)ğ‘Œ ğ‘¡ are assigned by an initial captioning process. Then Dğ‘¡ can be partitioned into two disjoint subsets: D+ ğ‘¡ : instances correctly labeled by the pseudo-captioning process, ğ‘¡ : instances labeled with noise by the pseudo-captioning process. The label noise rate is ğœ‚. Suppose we independently and identically sample (i.i.d.) ğ‘š instances from D+ ğ‘¡ and ğ‘¡ , respectively. Let â„“ (, ) be loss function applicable to hypothesis and dataset (for empirical error estimation) or distribution (for generalization error estimation). Given hypothesis â„ parameterized ğ‘¡ , forming samples U+ ğ‘¡ and ğœ–ğ‘  (â„, â„) ğœ–ğ‘¡ (â„, â„) sup â„,â„ 1 2 = ğ‘‘ HÎ”H (ğ·ğ‘ , ğ·ğ‘¡ ), ğœ–ğ‘  (â„, â„) ğœ–ğ‘¡ (â„, â„) Applying this, we have: ğœ–ğ‘¡ (â„) 2ğœ–+ ğ‘‘ HÎ”H (D+ ğ‘¡ , ğ‘¡ ) 1 ğ‘¡ (â„) + 2 ğ‘¡ (â„, ğ‘“ ) ğœ–+ + ğœ–+ ğ‘¡ (â„, ğ‘“ +), (17) (18) Conference17, July 2017, Washington, DC, USA Jian et al. robustness under greater domain shifts remains an important direction for future research. Another limitation is that our method is able to produce summaries without incorporating context and other modalities, akin to describing film watched on mute. Incorporating context and other modalities such as audio and transcripts will improve the quality of the summaries and broaden their usefulness to other tasks, such as search and VQA. Other interesting directions include tunable hierarchical summarisation, so that we can extract summaries at any resolution, from short clip all the way to the full video. Our LLM-based meta prompting component can also be more broadly investigated and made more general and reusable to other applications. In this sense, ViSMaP is first step in journey to develop accessible and robust video understanding techniques. = ğœ† Table 7: Evaluating LLMs in meta-prompting. Four LLM combinations Generator Evaluator Optimizer GPT3.5 [35] GPT3.5 [35] GPT4 [1] Gemini [38] GPT3.5 [35] Mixtral [15] GPT3.5 [35] Gemini [38] Mixtral [15] Mixtral [15] GPT3.5 [35] Mixtral [15] Mixtral [15] Gemini [38] Mixtral [15] GPT3.5 [35] GPT3.5 [35] GPT3.5 [35] GPT4 [1] GPT3.5 [35] Gemini [38] GPT3.5 [35] GPT4 [1] GPT4 [1] Ego-4D HCap 29.30 27.93 27.56 27.88 27.79 29.48 29.44 29.85 12.47 11.46 11.39 11.47 11.52 12.64 12.60 13.05 25.39 24.85 24.73 24.61 24.65 25.62 25.67 26.04 Letting U+ ğ‘¡ , sation bounds: ğ‘¡ be samples from D+ ğ‘¡ , ğ‘¡ , and using VC generaliğœ–ğ‘¡ (â„) 2ğœ–+ ğ‘¡ (â„) + 1 ğ‘‘ HÎ”H (U+ ğ‘¡ , ğ‘¡ ) ğ‘‘ log(2ğ‘š) + log( 2 ğ›¿ ) ğ‘š ğ‘¡ (â„, ğ‘“ +), ğ‘¡ (â„, ğ‘“ ) ğœ–+ Evaluating the final term using the noise model: + ğœ–+ + 2 (19) ğ‘¡ (â„, ğ‘“ ) ğœ–+ ğœ–+ ğ‘¡ (â„, ğ‘“ +) = ED+ ğ‘˜ğ‘¦ Hence, the final bound becomes: ğ‘¡ ğœ–ğ‘¡ (â„) 2ğœ–+ ğ‘¡ (â„) + 1 2 ğ‘‘ HÎ”H (U+ ğ‘¡ , ğ‘¡ ) + ğœ‚ğ‘¦ğ‘˜ â„“rce (ğ‘“ (ğ‘£), ğ‘˜) ğœ‚ğ‘¦â„“rce (ğ‘“ (ğ‘£), ğ‘¦) ğ‘‘ log(2ğ‘š) + log( 2 ğ›¿ ) ğ‘š (20) + ğœ† (21)"
        }
    ],
    "affiliations": [
        "Queen Mary University of London",
        "Spotify"
    ]
}