{
    "paper_title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs",
    "authors": [
        "Mengzhao Chen",
        "Yi Liu",
        "Jiahao Wang",
        "Yi Bin",
        "Wenqi Shao",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at \\url{https://github.com/ChenMnZ/PrefixQuant}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 5 6 2 5 0 . 0 1 4 2 : r Under review PREFIXQUANT: STATIC QUANTIZATION BEATS DYNAMIC THROUGH PREFIXED OUTLIERS IN LLMS Mengzhao Chen1,2, Yi Liu, Jiahao Wang1, Yi Bin3, Wenqi Shao2, Ping Luo1 1The University of Hong Kong 2Shanghai AI Laboratory 3Tongji University chenmnz@connect.hku.hk, randall.liu30@gmail.com, shaowenqi@pjlab.org.cn,pluo@cs.hku.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60 to 2.81 faster than FP16 models and exceeds QuaRot models by 1.2 to 1.3. Our code is available at https://github.com/ChenMnZ/PrefixQuant."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, Large Language Models (LLMs)(Touvron et al., 2023; Bubeck et al., 2023) demonstrate remarkable capabilities across various tasks, significantly improving the convenience of daily work and life. However, their large parameters and computational demands pose significant challenges for deployment. This makes quantization (Frantar et al., 2022; Lin et al., 2023; Shao et al., 2023) crucial technology for reducing memory usage and speeding up inference (Yuan et al., 2024). Despite advancements, large outliers in LLMs activations can lead to significant quantization errors and accuracy loss. Many current methods address this by focusing on alleviating channel-wise outliers (Dettmers et al., 2022) through techniques like channel-wise scaling (Xiao et al., 2023a; Shao et al., 2023; Wei et al., 2023a), mixed-precision quantization (Dettmers et al., 2022; Zhao et al., 2023), Hadamard rotation (Ashkboos et al., 2024b; Liu et al., 2024a), and channel-level assembly (Liu et al., 2023). However, activations of LLMs include not only channel-wise but also token-wise outliers. For example, Figure 1 (a) shows that some tokens, can be termed as outlier tokens, have extreme values exceeding 1,000, making it impractical to share quantization scales between outlier and normal tokens. The current leading method, Hadamard rotation (Ashkboos et al., 2024b), redistributes outlier values across all channels, reducing the maximum value in outlier tokens from over 1,000 to about 15 (see Figure 1 (b)). Nevertheless, the magnitude of outlier tokens remains hundreds of times greater than that of normal tokens, still suffering significant performance degradation when sharing quantization scales across different tokens. Due to such dramatic discrepancies between normal and outlier tokens, previous quantization methods have to rely on per-token dynamic quantization to adjust quantization scales on-the-fly for each token. While per-token dynamic quantization adapts better to distribution changes, it faces more Corresponding author: shaowenqi@pjlab.org.cn, pluo@cs.hku.hk 1 Under review Figure 1: Comparison of PrefixQuant with existing methods. This figure shows the input activation of the down proj linear layer in Llama-2-7B using different methods. Perplexity is measured with Llama-2-7B under 16-bit weight and 4-bit activation using per-tensor static quantization without any re-training. The original distribution has significant outliers larger than 1,000 (left). The previous method with Hadamard rotation (Ashkboos et al., 2024b) reduces outliers to nearly 15 (middle) but still suffers from poor perplexity due to non-uniform distribution. We propose PrefixQuant (right), which prefixes some specific tokens in KV cache to isolate outliers, reducing the maximum to nearly 0.07, significantly improving quantization performance. computational effort (Xiao et al., 2023a) and less compatible with operator fusion (Nagel et al., 2021) than per-tensor static quantization which use fixed quantization parameter for all token. This leads to an important question: Can we eliminate token-wise outliers to enhance the precision of efficient per-tensor static quantization? In this paper, we propose PrefixQuant, an efficient solution for static activation quantization in LLMs. PrefixQuant is based on key observation: outlier tokens usually appear at fixed positions in the token sequence (such as the initial token) or in tokens with low semantic value (such as n, ., the, etc). Based on this observation, PrefixQuant pre-processes the outlier tokens offline in the KV cache to prevent generate new outlier tokens during inference. Specifically, given LLM, PrefixQuant firstly counts the number of outlier tokens, and selects the Top-N high-frequency outlier tokens to prefix in the KV cache. This process is efficient and does not require any retraining, unlike previous methods (Sun et al., 2024; Bondarenko et al., 2024), and can be completed quickly, such as in 12 seconds for Llama-2-7B. As illustrated in Figure 1 (c), PrefixQuant effectively eliminates outlier tokens, achieving excellent performance with per-tensor static activation quantization. For example, with 4-bit per-tensor static activation quantization on Llama-2-7B, PrefixQuant achieves 5.91 perplexity, significantly outperforms QuaRot which has perplexity of 17.95. Furthermore, we introduce block-wise fine-tuning optimization (Shao et al., 2023; Chen et al., 2024a) to improve performance by simultaneously training the quantization parameters of both weight and activation. Additionally, we also find that isolating the outlier tokens enhances the convergence stability of training through avoiding large outliers magnitude during the calculation of Mean Square Error (MSE) loss. Thus, the proposed method of prefixed outliers can also serve as plug-and-play enhancement for existing optimization-based quantization methods (Shao et al., 2023; Chen et al., 2024a). Experiments demonstrate that, without any fine-tuning, PrefixQuant achieves comparable or better performance than previous per-token dynamic quantization methods (Ashkboos et al., 2024b; Xiao et al., 2023a; Lin et al., 2024b) using coarser per-tensor static quantization. Furthermore, fine-tuning significantly enhances PrefixQuants performance. For example, PrefixQuant with finetuning achieves 7.43 WikiText2 (Merity et al., 2016) perplexity and 71.08% average accuracy across five common-sense reasoning tasks in W4A4KV4 Llama-3-8B, significantly outperforming previous QuaRot (Ashkboos et al., 2024b) with 0.98 perplexity benefit and +5.98 points accuracy. To the best of our knowledge, PrefixQuant is the first to outperform previous per-token dynamic quantization methods (Ashkboos et al., 2024b; Xiao et al., 2023a; Lin et al., 2024b) using coarse per-tensor static quantization. We also benchmark the end-to-end inference of W4A4 quantization, where PrefixQuant achieves 1.60 to 2.81 speedup over FP16 models, and surpasses QuaRot models by 1.2 to 1.3. We hope PrefixQuant inspires future developments in LLM compression."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "In this section, we discuss works related to outliers in LLMs, including quantization methods that enhance performance by eliminating activation outliers. 2 Under review Channel-Wise Outliers. Dettmers et al. (2022) identifies that outliers in activation consistently occur in the same channels across different input tokens and proposes isolating these outlier channels with 16-bit precision. Other works, such as Atom (Zhao et al., 2023) and QUIK (Ashkboos et al., 2023), follow similar mixed-precision approach to handle outliers. Instead of introducing mixedprecision matrix manipulation, which lacks native hardware support, another line of work addresses outliers through mathematically equivalent transformations. For example, SmoothQuant (Xiao et al., 2023a), OmniQuant (Shao et al., 2023), and Outlier Suppression (Wei et al., 2022; 2023b) mitigate outliers by scaling activations to weights on channel-wise basis. QLLM (Liu et al., 2023) reduces outlier values by dividing each outlier channel into multiple sub-channels. Recently, QuaRot (Ashkboos et al., 2024b) proposed simple and effective method, random Hadamard rotation, to redistribute outliers across all channels. Building on QuaRot, SpinQuant (Liu et al., 2024a) suggests training the orthogonal matrix instead of using random Hadamard matrix to further enhance performance. DuQuant (Lin et al., 2024a) leverages channel permutation to evenly distribute outlier to each block and uses block-rotation to smoothen outliers. Although these methods significantly improve activation quantization performance, they all rely on fine-grained per-token dynamic quantization, which incurs additional overhead to manage token-wise fluctuations. Token-Wise Outliers. The SoftMax function used in the self-attention mechanism naturally prevents producing zero attention scores. As result, the model tends to assign unnecessary scores to special tokens, leading to token-wise outliers (or termed as massive activation) (Sun et al., 2024; Xiao et al., 2023b). Based on this, StreamingLLM (Xiao et al., 2023b) and LM-infinite (Han et al., 2023) support infinite sequences by retaining the initial token. Unlike StreamingLLM and LMinfinite, which simply preserve initial tokens in the KV-cache for long-context generation, our PrefixQuant carefully selects prefixed tokens in the KV-cache to isolate outliers for quantization. Some studies explore eliminating outliers with training techniques. For example, Bondarenko et al. (2024) allows SoftMax to produce zero values, and Sun et al. (2024) shows that adding attention bias in the KV cache during training can effectively reduce outliers. Our PrefixQuant efficiently isolates outlier tokens without needing retraining. The works closest to our approach are QFeP (Yang et al., 2024) and CushionCache (Son et al., 2024), which also set prefixed tokens in the KV cache. However, CushionCache (Son et al., 2024) takes 12 hours to find the prefixed tokens for Llama-3-8B through greedy search, while our method completes this process in 12 seconds. QFeP (Yang et al., 2024) fixes the outlier token number for all models at 3, which lacks flexibility. Additionally, both QFeP and CushionCache suffer significant performance degradation when using per-tensor static quantization instead of per-token dynamic quantization. Our PrefixQuant is the first to make per-tensor static quantization outperform per-token dynamic quantization."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Quantization in LLMs involves weight, activation, and KV cache quantization. Weight quantization (Chen et al., 2024a) and KV cache quantization (Liu et al., 2024b) reduce memory usage and speed up memory-bound computations (Yuan et al., 2024). Combining weight and activation quantization enables low-bit matrix manipulation to accelerate computation-bound tasks (Yuan et al., 2024). Specifically, the symmetric quantization process is: XINT = clamp( sX , 2N 1, 2N 1 1), (1) where denotes rounding operation, is the target bit number, XINT and are the quantized integer and full-precision activation, respectively. sX is the step size. Full precision weight can also be quantized into WINT and sW similarly. Then, full-precision matrix manipulation transfer into efficient low-bit matrix manipulation: XW (sW sX) XINTWINT (2) Granularity. Finer granularity in quantization results in more overhead but less information loss. Per-tensor quantization shares across the entire tensor. Per-channel quantization of weight and per-token quantization of activation means is shared within each row of the tensor. Dynamic and Static. Activation quantization divides into dynamic and static quantization based on how quantization parameters are calculated. Specifically, dynamic quantization calculates 3 Under review sX = max(X) 2N 11 during inference, offering better adaptability to different distributions. In contrast, static quantization precomputes sX and (sW sX) in Eq.(2) offline, leading to more efficient inference and more feasible operator fusion (Nagel et al., 2021). Table 8 shows that the overhead of static quantization is nearly 3 lower than dynamic quantization. Additionally, we initialize both sW and sX through grid search (Lin et al., 2023; Gong et al., 2024) on small calibration dataset for all experiments with static quantization. Hadamard Rotation. Random Hadamard rotation (Ashkboos et al., 2024b; Liu et al., 2024a) addresses channel-wise outliers. Our method focus on removing token-wise outliers. Therefore, We build our method upon the Hadamard rotation technique, and the detailed is provided in Sec. C. (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 2: Distribution of token-wise maximum values for linear layers inputs in Llama-2-7B. Top-N indicates the -th largest value, Min-N indicates the -th smallest value. We also report the maximum ratio between Top-1 value and median value, as well as the maximum ratio between median value and Min-1 value (Ratios greater than 10 are marked with red, and the rest are green). Lower ratio indicate similar maximum values across different tokens, leading compatibility with per-tensor static activation quantization."
        },
        {
            "title": "4 DIFFICULTY OF STATIC QUANTIZATION",
            "content": "Both channel-wise and token-wise outliers can cause information loss during quantization. While channel-wise outliers have been thoroughly explored and addressed in prior research (Ashkboos et al., 2024b), this discussion focuses on token-wise outliers, which occur within specific tokens. Let RT represent the token sequence, with tokens and dimension size of C. We calculate token-wise maximum values RT , indicating the maximum value of each token. Per-tensor static quantization uses one pre-computed scale for all tokens. If the token-wise maximum values vary significantly across tokens, this can lead to substantial information loss after per-tensor static quantization. To analyze the distribution of token-wise maximum values and understand the challenges for per-tensor static quantization, we define top-1, median, and min-1 as the largest, median, and smallest values of M, respectively. We then measure discrepancies using the ratios top-1 median and median min-1 represents lower outliers. Both ratios highlight the variability in M. Specifically, we identify the following patterns that motivate our method. median indicates upper outliers, while larger median min-1 . Specifically, larger top-1 1. Upper Outlier Tokens in Inputs. As shown in Figure 2a, the input activation of down proj layers exhibits significant outliers with top-1 median = 4127. Although Hadamard rotation (Figure 2b) 4 Under review (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 3: Distribution of token-wise maximum values for Q/K/V in Llama-2-7B. Same present rules as Figure 2a except that ratios greater than 5 are marked with red. (a) Content (b) Index (c) Index with prefixed tokens Figure 4: Illustration the content and index of outlier token in the input sequence of Llama-27B. (a) counts the outlier tokens except in the initial token, shows that the outliers only exit in . and tokens. (b) illustrates the sequence index of outlier tokens. (c) demonstrates that prefix the input sequence with .n[BOS] can constraint the outlier token in the first three tokens. reduces the ratio to 478, it remains impractical to share quantization scaling factor across tokens due to the large gap in maximum values. 2. Lower Outlier Tokens in Q/K/V. We also investigate the distribution of Q/K/V within the self-attention mechanism. We only quantize the KV cache for fair comparisons with previous works (Ashkboos et al., 2024b; Lin et al., 2024b). However, quantization of is also crucial, as used in FlashAttention-3 (Shah et al., 2024). In Figure 3, Q/K/V display different outlier pattern than the inputs of linear layers, with some tokens having extremely small magnitudes instead of large ones. Specifically, Q/K have top-1 min-1 > 9. Additionally, as shown in Figure 3b, Hadamard rotation has no effect on these outliers. median 1.5, but median 3. Outlier Tokens in Initial or Low-Semantic Tokens. Though outlier tokens occur in different patterns, we find that they are the same tokens in inputs of linear layers and Q/K/V. Consistent with Massive Attention (Sun et al., 2024), we find that outlier tokens appear only in small fractions (nearly 1 to 4 tokens in the input sequence) with fixed patterns. For example, Llama-2-7B has outlier tokens in both initial and delimiter tokens (. or as shown in Figure 4a). However, unlike outlier channels that exist in some fixed channel indexes (Dettmers et al., 2022), the position indexes of outlier tokens relate to the input sequence and are diverse, as shown in Figure 4b. Therefore, it is not 5 Under review (a) Original (b) PrefixQuant Figure 5: Comparison of Original and PrefixQuant Inference. Both methods use Hadamard rotation to remove channel-wise outliers. PrefixQuant differs by setting specific prefixed tokens in the KV cache, which eliminates token-wise outliers in linear inputs and Q/K/V, enhancing compatibility with per-tensor static quantization. Llama-2-7B serves as an example; additional prefixed tokens for other models are listed in Table 1. feasible to decide offline on the outlier token to achieve mixed-precision quantization like previous works on outlier channels (Dettmers et al., 2022; Zhao et al., 2023). Previous works (Ashkboos et al., 2024b; Lin et al., 2024b; Liu et al., 2024b) take per-token dynamic quantization for inputs of linear layers and KV cache to deal with outlier tokens. In this paper, we focus on eliminating outlier tokens to facilitate per-tensor static quantization."
        },
        {
            "title": "5 PREFIXQUANT",
            "content": "As shown in Figure 5, we propose prefixing outlier tokens in the KV cache to improve the performance of more efficient per-tensor static quantization, instead of using costly per-token dynamic quantization. Section 5.1 explains how to find these prefixed outliers. Section 5.2 introduces blockwise fine-tuning to further enhance performance. 5.1 PREFIXED OUTLIERS Definition of Outlier Token. Given that both upper outlier tokens in the inputs of the linear layer and lower outlier tokens in Q/K/V are same tokens, we choose to identify outlier tokens using the upper outliers in the inputs of the down proj layers due to the outlier in down proj is more highlight and easier to be detected. Given token-wise maximum values RT , which represents the maximum values of each token. Then, outlier token in the i-th index of token sequence is identified when the ratio of their maximum values to the median of all maximum values exceeds threshold η: Mi median(M) > η, (3) Table 1: Prefixed tokens in KV cache across different models. [BOS] indicates the special token for beginning of sequence(e.g. <s> for Llama2 and begin of text for Llama-3). Note that the following represents space. Model Llama-2-7B Llama-2-13B Llama-2-70B Llama-3-8B(-Instruct) Llama-3-70B(-Instruct) Mistral-v0.3-7B Qwen-2-7B Prefixed token Number 3 3 4 1 3 4 1 Content .n[BOS] the.[BOS] n[BOS] [BOS] , [BOS] n.to[BOS] [BOS] where Mi is the maximum value of the i-th token, median() denotes the function to find the median value from the vector, and the threshold η is empirically set to 64 in our experiments. Number of Outlier Tokens. We determine the number of outlier tokens by calculating the average number of outlier tokens in small calibration dataset. Specifically, we compute the average outlier 6 Under review PPL FP16 W16A4KV16 (static) W16A16KV4 (static) original + rotation + prefixed original + rotation + prefixed Llama-2-7B 5.47 3024.77 Llama-3-8B 6.14 1816.57 17.95 22.14 5.91 7.23 6.46 7. 5.95 8.12 5.56 6.30 Table 2: Proposed prefixed outliers in KV cache significantly improves the performance of the static quantized models over hadamard rotation Ashkboos et al. (2024b); Liu et al. (2024a). W16A4KV16 indicates 4-bit per-tensor static quantization of all linear layer inputs. W16A16KV4 indicates 4-bit per-head static KV cache quantization. WikiText2 perplexity with 2048 context length is reported. token count Rb for each transformer block according to Eq (3), where is the total number of transformer blocks. Since outlier tokens are nearly consistent across layers that contain them, we set the number of outlier tokens as = max(O). Which Tokens to Prefix? Outlier tokens act as attention sinks (Xiao et al., 2023b), occupying only few tokens (1 4) to help the attention mechanism do nothing (Bondarenko et al., 2024; Sun et al., 2024). Given the outlier token number o, we find that prefixing the top-o high-frequency* outlier tokens and the special [BOS] token can successfully constrains the outliers in prefixed tokens as shown in Figure 4c. For special models (such as Llama-3-8B and Qwen-2-7B) with outlier tokens only in the initial tokens, we simply set the prefix token as [BOS]. The detailed prefixed tokens for different models are illustrate in Table 1. Considering the auto-regressive inference pipeline of LLMs, we store these prefix tokens in the KV cache to prevent generating new outlier tokens during inference. As shown in Figure 2c and Figure 3c, prefixing outliers in the KV cache reduces the top-1 median ratio of down proj inputs from 476 to 2.7 and the median Quantitative Analysis. Table 2 presents separate performance of static quantization on input activation and KV cache quantization. We can find that the model suffers significant performance degradation with static quantization becuase of outlier tokens. For example, in Llama-3-8B, WikiText2 perplexity increases from 6.14 to 22.14 with 4-bit per-tensor activation quantization and from 6.14 to 8.12 with 4-bit per-head static KV cache quantization even with Hadamard rotation. After further setting prefix outliers in the KV cache, performance significantly improves: perplexity of 4-bit per-tensor activation decreases to 7.23 and perplexity of 4-bit per-head static KV cache quantization decreases to 6.30, demonstrating the effectiveness of prefixed outlier tokens for static quantization. min-1 ratio of Q/K from > 9 to < 3.5. 5.2 BLOCK-WISE FINE-TUNING Recent studies demonstrate that block-wise fine-tuning (Shao et al., 2023; Chen et al., 2024a) enhances performance by considering inter-layer interactions (Li et al., 2021). We initialize all quantization parameters using grid search (Lin et al., 2023; 2024b) and then fine-tune each transformer block with mean square error loss sequentially. For trainable parameters, we follow EfficientQAT (Chen et al., 2024a) by activating the training of all quantization parameters and original fullprecision weights. Additionally, unlike dynamic activation quantization, the offline pre-computed quantization parameters of static activation quantization are inherently trainable. To maintain simplicity, we use block-wise quantization in this work and leave the end-to-end finr-tuning of EfficientQAT (Chen et al., 2024a) for future performance improvements."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "6.1 SETUPS Baseline. PrefixQuant is versatile method applicable to any precision. We conduct experiments on three precisions: W8A8KV8, W4A8KV4, and W4A4KV4. In PrefixQuant, weight uses per-channel symmetric quantization. KV cache uses per-head symmetric static quantization for 4-bit and per- *The frequencies are calculated without considering initial token. 7 Under review Table 3: W4A4KV4 results. Perplexity is measured with context length 2048. Avg. Acc. indicates the average zero-shot accuracy on 5 common-sense reasoning tasks. Quant Type is used to indicate whether the activation and kv cache quantization are dynamic or static. Model Llama-2-7B Llama-2-70B Llama-2-13B Method FP16 Atom QuaRot DuQuant SpinQuant PrefixQuant w/o FT PrefixQuant FP16 Atom QuaRot DuQuant SpinQuant PrefixQuant w/o FT PrefixQuant FP16 Atom QuaRot DuQuant SpinQuant PrefixQuant w/o FT PrefixQuant FP16 Atom QuaRot DuQuant SpinQuant PrefixQuant w/o FT PrefixQuant FP16 QuaRot DuQuant PrefixQuant w/o FT PrefixQuant * Grayed results use Wikitext2 as calibaration dataset. Atom apply 128 group size quantization to both weight and activations. Quant Type - dynamic dynamic dynamic dynamic static static - dynamic dynamic dynamic dynamic static static - dynamic dynamic dynamic dynamic static static - dynamic dynamic dynamic dynamic static static - dynamic dynamic static static Wiki PPl 5.47 6.12 6.19 6.20 5.95 6.22 6.01 4.88 5.31 5.45 5.39 5.24 5.50 5.32 3.32 3.73 3.83 3.77 3.70 4.41 3.81 6.14 7.76 8.41 8.14 7.36 7.93 7.43 2.85 6.82 5.67 5.23 4.41 Llama-3-70B Llama-3-8B Avg. Acc. 69.04 59.73 64.69 66.25 65.35 66.84 66.37 71.73 63.51 69.01 69.13 69.24 69.92 70.36 76.72 67.52 75.43 74.75 75.19 73.29 75.48 72.71 - 65.15 67.13 68.23 68.37 71.08 80.03 68.39 74.89 76.40 77.18 tensor symmetric static quantization for 8-bit. Activation (inputs of linear layers) uses per-tensor static quantization. We compare PrefixQuant with QuaRot (Ashkboos et al., 2024b), Atom (Zhao et al., 2023), DuQuant (Lin et al., 2024a), QoQ (Lin et al., 2024b), SmoothQuant (Xiao et al., 2023a) and SpinQuant (Liu et al., 2024a). Following QoQ, we reproduce all these methods except SpinQuant with Pile (Gao et al., 2020) calibration dataset to avoid over-fitting for fair comparisons. The detailed quantization configuration and results sources of these comparison methods can be found at Sec. B. Note that all comparison methods use dynamic quantization if without specific mentioned, and would suffer dramatic performance degeneration likes + static quantization in Table 6. Models and datasets. We evaluate PrefixQuant on the Llama-2, Llama-3, Llama-3-Instruct families, Mistral-7B-v0.3, and Qwen-2-7B models. Following previous literature (Shao et al., 2023; Lin et al., 2024b), we assess PrefixQuant quantized models on language modeling and zero-shot tasks. Specifically, we evaluate on WikiText2 (Merity et al., 2016) with 2048 context length for perplexity, and on PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), and WinoGrande (Sakaguchi et al., 2021) using lm eval v0.4.2 (Gao et al., 2024). For accuracy, we report acc for WinoGrande and acc norm for HellaSwag, Arc Challenge, Arc Easy, and PIQA, following Qserve (Lin et al., 2024b). Some weight-only quantization works such as EfficientQAT (Chen et al., 2024a) and QuiP# (Tseng et al., 2024) report acc for all tasks. 8 Under review Table 4: W4A8KV4 results. Refer Table 3 for the metric setting and performance of full-precision models. Model Llama-2-7B Llama-2-13B Llama-2-70B Llama-3-8B Llama-3-70B Method QoQ QuaRot PrefixQuant w/o FT PrefixQuant QoQ QuaRot PrefixQuant w/o FT PrefixQuant QoQ QuaRot PrefixQuant w/o FT PrefixQuant QoQ QuaRot PrefixQuant w/o FT PrefixQuant QoQ QuaRot PrefixQuant w/o FT PrefixQuant Activation Quant Wiki PPl Avg. Acc. dynamic dynamic static static dynamic dynamic static static dynamic dynamic static static dynamic dynamic static static dynamic dynamic static static 5.75 5.73 5.76 5.68 5.12 5.07 5.08 5.07 3.52 3.46 3.60 3.50 6.89 6.80 6.90 6.62 4.36 3.73 3.55 3.43 67.22 67.11 67.86 68.90 70.56 69.96 71.07 71.25 75.91 76.31 75.00 76.50 71.35 71.68 70.29 72.46 78.12 78.92 77.82 78.70 Grid Search Setting. For all experiments with static quantization, we initialize the quantization parameters through grid search on 8 Pile (Gao et al., 2020) samples with 1024 sequence length. We minimize the layer outputs for fine-grained quantization (per-channel/per-head) and block outputs for per-tensor quantization. In the performance comparison tables, PrefixQunt w/o FT indicates finishing the quantization only with grid search and without fine-tuning. Fine-Tuning Setting. During fine-tuning, we optimize block output mean square error following existing works (Shao et al., 2023; Chen et al., 2024a). The dataset for fine-tuning consists of 512 samples from Pile with 1024 context length. The learning rates for quantization parameters (step sizes) and full-precision weights are set to 5e-5 and 5e-6, respectively, and to 2e-5 and 2e-6 for Llama-3-70B(-Instruct) models. The fine-tuning batch size is set to 4, and the number of epochs is set to 10 for W4A8KV4 and 20 for W4A4KV4. 6.2 COMPARISON RESULTS Results on W4A4KV4. Table 3 shows the comparison results for W4A4KV4. PrefixQuant with static quantization significantly outperforms the previous state-of-the-art QuaRot, which uses dynamic quantization. For instance, in Llama-3-8B, PrefixQuant without fine-tuning surpasses QuaRot by 0.48 in WikiText2 perplexity and +3.22 points in average accuracy. Fine-tuning further improves these results to 0.98 in WikiText2 perplexity and +5.98 points in average accuracy. Results on W4A8KV8. Table 4 presents the comparison results for W4A8KV8. Without finetuning, PrefixQuant performs comparably to QoQ (Lin et al., 2024b). After fine-tuning, PrefixQuant outperforms both QoQ and QuaRot in most models. For instance, PrefixQuant surpasses QoQ (Lin et al., 2024b) by 0.27 perplexity and +1.11 accuracy points in Llama-3-8B. Results on W8A8KV8. Table 18 includes the comparison with various methods in W8A8KV8 quantization. We can find that SmoothQuant, QuaRot, and PrefixQuant all attain near lossless performance without fine-tuning. Notably, our PrefixQuant is unique in employing static quantization, which enhances inference efficiency. Additionally, earlier methods like CushionCache (Son et al., 2024) and QFeP (Yang et al., 2024), despite also using prefixed tokens in the KV cache to support coarser quantization, exhibit marked performance decline even under W8A8 as illustrated in Table 17. 9 Under review Results on more models. The results in Table 19 demonstrate that PrefixQuant consistently achieves excellent performance on other models such as Mistral-7b-v0.3 and Qwen-2-7B, as well as instruction-tuned models like Llama-3-{7B,70B}-Instruct. Results on weight-only quantization. PrefixQuant can also improve existing weight-only quantization methods by reducing outlier noise in MSE loss calculations. As shown in Table 16, PrefixQuant enhances the average accuracy by +5.05 and +4.73 points on W2A16g128 Llama-3-8B and Llama3-70B, respectively, based on the state-of-the-art uniform quantization method EfficientQAT (Chen et al., 2024a). See Sec. for more details. 6.3 INFERENCE SPEED In this section, we evaluate the end-to-end inference speed of PrefixQuant in the W4A4 quantization scenario. We do not consider KV quantization here because it saves memory footprint through more computation overhead and only achieves speedup with large batch sizes (Liu et al., 2024b). Table 5 shows the speedup of W4A4 quantization in the prefilling stage. Our PrefixQuant improves the QuaRot speedup from 2.30 to 2.81 on the A100-80GB GPU, and from 1.31 1.39 to 1.60 1.82 on the RTX 3090 GPU. In Sec. D, we also provide comprehensive apple-to-apple comparisons of submodules, such as quantization kernels and quantized linears, demonstrating the significant superiority of PrefixQuant over the existing dynamic quantization approach QuaRot (Ashkboos et al., 2024b). Table 5: Time-to-first-token (prefilling) speedup of W4A4 Llama-3-8B model over the FP16 model. We use 2048 sequence length with different batch size. Batchsize 1 4 on RTX 3090 GPU (ms) FP16 509 Quarot (W4A4) 221 (2.30x) 851 PrefixQuant (W4A4) 181 (2.81x) 725 on an A100-80GB GPU (ms) 661 FP16 172 OOM Quarot (W4A4) 130 (1.31x) 477 (1.39x) PrefixQuant (W4A4) 107 (1.60x) 362 (1.82x) Table 6: Ablation study on quantization techniques used in PrefixQuant. The model used here is Llama-3-8B, and WikiText2 perplexity with 2048 context length is reported. Method QuaRot RTN + rotation + grid search + static quantization + prefixed outliers + block-wise fine-tuning Activation Quant. W8A8KV8 W4A8KV4 W4A4KV4 dynamic dynamic dynamic dynamic static static static 6.17 6.26 6.17 6.17 7.27 6.17 6.17 6.75 12.66 10.88 8.91 29.07 6.90 6. 8.33 1282.34 24.98 16.47 141.02 7.93 7.41 6.4 ABLATION STUDIES We examine the impact of various quantization techniques implemented in PrefixQuant. Our analysis starts with W4A4KV4 round-to-nearest (RTN) quantization on Llama-3-8B, involving perchannel weight quantization, per-token dynamic activation quantization, and per-head dynamic KVcache quantization. We apply different techniques step-by-step and report the WikiText2 perplexity in Table 6. We find that both Hadamard rotation and grid search initialization improve performance. Then, perplexity collapses due to static quantization of activation and KV cache, but introducing prefixed outliers significantly recovers performance, even surpassing results before introducing static quantization. These benefits arise not only by reducing information loss from outlier tokens but also by helping to find accurate quantization parameters in grid searches through isolating extremely large outliers (> 1e3) in activation. Additionally, block-wise fine-tuning improves performance except on W8A8KV8, which is nearly lossless without fine-tuning. More ablation results related to the training dataset, training epochs, dynamic quantization, the number of prefixed tokens, and the content of prefixed tokens are in Sec. in the Appendix. 10 Under review"
        },
        {
            "title": "7 CONCLUSION",
            "content": "We propose PrefixQuant, which enables static quantization to outperform dynamic quantization by effectively handling token-wise outliers through novel prefixing approach. This technique also stabilizes model training, making it plug-and-play module that enhances the performance of other optimization-based methods. The simplicity and broad applicability of PrefixQuant make it promising direction for future LLM compression and optimization research."
        },
        {
            "title": "REFERENCES",
            "content": "Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024a. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024b. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pp. 74327439, 2020. Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing outliers by helping attention heads do nothing. Advances in Neural Information Processing Systems, 36, 2024. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062, 2024a. Tianqi Chen, Zhe Li, Weixiang Xu, Zeyu Zhu, Dong Li, Lu Tian, Emad Barsoum, Peisong Wang, and Jian Cheng. Ternaryllm: Ternarized large language model. arXiv preprint arXiv:2406.07177, 2024b. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35: 3031830332, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. 11 Under review Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Yunchen Zhang, Xianglong Liu, and Dacheng Tao. Llm-qbench: benchmark towards the best practice for post-training quantization of large language models. arXiv preprint arXiv:2405.06001, 2024. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms. arXiv preprint arXiv:2406.01721, 2024a. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: arXiv preprint Activation-aware weight quantization for llm compression and acceleration. arXiv:2306.00978, 2023. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. Qllm: Accurate and efficient low-bitwidth quantization for large language models. arXiv preprint arXiv:2310.08041, 2023. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquantllm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024a. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024b. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, arXiv preprint and Tijmen Blankevoort. white paper on neural network quantization. arXiv:2106.08295, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. Under review Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, and Jaeho Lee. Prefixing attention sinks can mitigate activation outliers for large language model quantization. arXiv preprint arXiv:2406.12016, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:1740217414, 2022. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023a. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Jaewoo Yang, Hayun Kim, and Younghoon Kim. Mitigating quantization errors due to activation spikes in glu-based llms. arXiv preprint arXiv:2405.14428, 2024. Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. 13 Under review"
        },
        {
            "title": "OVERVIEW OF APPENDIX",
            "content": "We detailed the content of Appendix here: Sec gives the reproducibility statement to summarize the information related to the reproduction of our method. Sec details the quantization configuration and data sources of comparison methods. Sec. illustrates the detailed image of hadamaed rotation within transformer block. Sec. presents the apple-to-apple sub-module comparisons of PrefixQuant and QuaRot. Sec. details the quantization time of PrefixQuant. Sec. gives more ablation studies of PrefixQuant, including the fine-tuning dataset, training epoch, dynamic quantiztaion and number of prefixed tokens. Sec. demonstrates that proposed PrefixQuant can also play as plug-in to enhance the performance of existing weight-only quantization methods. Sec. presents the detailed accuracy number of each zero-shot task, and provide more results of PrefixQuant on Mistral-v0.3-7B, Qwen-2-7B, and Llama-3-{8B,70B}-Instruct. Sec. illustrate more visualization of inputs of linear layer and Q/K/V on more models, including Llama-3-{8B,70B}, Mistral-7B-v0.3, Qwen-2-7B."
        },
        {
            "title": "A REPRODUCIBILITY STATEMENT",
            "content": "In this section, we summarize the necessary information to reproduce our results. First, PrefixQuant is based on Hadamard rotation, as detailed in Sec.C. Our main contribution, setting prefixed outliers, is discussed in Sec.5.1. After configuring prefixed outliers in the KV-cache, we initialize the quantization parameters using grid search. We also offer optional block-wise fine-tuning to enhance performance. Detailed setups for grid search and fine-tuning are available in Sec.6.1. Additionally, we provide the source of detailed results for each compared method in Sec.B."
        },
        {
            "title": "B CONFIGURATION AND DATA SOURCES OF COMPARISON METHODS",
            "content": "Quantization Configurations. In this study, we establish the quantization granularity for each comparison method based on the specifications provided in the original papers. Details on these settings are given in Table 7. Table 7: Detailed quantization setting of comparison methods. All per-group quantization set group size as 128. Method Weight Activation KV Cache SmoothQuant per-channel symmetric per-token symmetric dynamic per-tensor symmetric static Atom QoQ QuaRot DuQuant per-group symmetric per-group symmetric dynamic per-group asymmetric dynamic per-channel asymmetric per-token symmetric dynamic per-group asymmetric dynamic per-channel symmetric per-token dynamic symmetric per-group asymmetric dynamic per-channel asymmetricper-token dynamic asymmetric per-token asymmetric dynamic PrefixQuant per-channel symmetric per-tensor symmetric static per-head symmetric static Data Sources. We compare our proposed PrefixQuant with several other methods: QuaRot (Ashkboos et al., 2024b), Atom (Zhao et al., 2023), QoQ (Lin et al., 2024b), SmoothQuant (Xiao et al., 2023a), SpinQuant (Liu et al., 2024a), and EfficientQAT (Chen et al., 2024a). The data for our comparisons either come directly from the official publications of these methods, from other papers, or from our own reproduction of the methods. The source of the data for each method is outlined as follows: 14 Under review QuaRot: We present the performance of QuaRot using the Pile calibration dataset. The results for Llama-2 models with W4A4KV4 come from QoQ (Lin et al., 2024b), while the rest are reproduced using the official open-source code. DuQuant: We reproduce DuQuant with Pild calibration dataset through their official opensource code. Note that we change the evaluation toolbox to lm-eval v0.4.2 for more accurate evaluation. Atom: We present the performance of Atom using the Pile calibration dataset. The results are sourced from QoQ (Lin et al., 2024b). QoQ: We present the performance of QoQ using the Pile calibration dataset. The results for Llama-2 come from QoQ (Lin et al., 2024b), and the Llama-3 results are reproduced using the official open-source code. SmoothQuant: We present the performance of SmoothQuant using the Pile calibration dataset. All results are reproduced using the open-source code from QoQ (Lin et al., 2024b). SpinQuant: All results are reproduced using the official open-source code and the pretrained rotation matrix. Note that SpinQuant directly trains on the WikiText2 dataset. EfficientQAT: All results are reproduced using the official open-source code and the prequantized models."
        },
        {
            "title": "C DETAILS OF ROTATION",
            "content": "Hadamard rotation (Ashkboos et al., 2024b; Liu et al., 2024a) redistributes outlier channels across all channels, achieving uniform distribution within each token. The Hadamard matrix is an orthogonal matrix with HHT = I, and its entries are {+1, 1} at the same scale. Hadamard rotation can be applied to all activations and use inverse rotation on corresponding weights to maintain computational invariance (Ashkboos et al., 2024a). Specifically, the rotation includes absorbable and online rotations. As shown in Figure 6, we follow SpinQuant (Liu et al., 2024a) to set R1, R2, R3 and R4 rotations, details as follows. Absorbable Rotation. Hadamard rotation of activation can be absorbed into the previous linear layer if there is no intervening non-linear operation. Thus, the rotation of input activations for q/k/v/gate/up proj (R1) and head-wise rotation for proj input activations (R2) can be fully absorbed without adding computation during inference. Online Rotation. Some rotations must be executed online, including output activations of proj and proj after RoPE (Su et al., 2024) (R3), and the input activation of down proj (R4). These online rotations are efficiently implemented using the Walsh-Hadamard transform without significant overhead. If not specifically mentioned, we activate all rotation (R1, R2, R3 and R4) in weight-activation quantization scenes, and only activate absorbable rotation (R1 and R2) in weight-only quantization. Figure 6: Illustrate of hadamard rotation within transformer block of Llama (Touvron et al., 2023) model."
        },
        {
            "title": "D INFERENCE EFFICIENCY DETAILS",
            "content": "15 Under review Table 8: Speedup of per-tensor static quantization compared to per-token dynamic quantization in 4-bit activation quantization. (a) Nvidia RTX 3090 GPU (Seq len,dimension) Quantization Time (ms) Speedup Per-token Dynamic Per-tensor static (1,4096) (1,8192) (2048,4096) (2048,8192) Average Speedup 0.0127 0.0144 0.1073 0.2084 0.0038 0.004 0.0344 0. (b) Nvidia A100-80GB GPU 3.34x 3.60x 3.12x 3.19x 3.31x (Seq len,dimension) Quantization Time (ms) Speedup Per-token Dynamic Per-tensor static (1,4096) (1,8192) (2048,4096) (2048,8192) Average Speedup 0.020 0.022 0.095 0.157 0.0072 0.0075 0.033 0.058 2.81x 2.88x 2.88x 2.71x 2.82x Table 9: Speedup of W4A4 quantized linear layers compared to FP16 linear layer. Numbers in brackets indicate the speedup compared to FP16. (a) Nvidia RTX 3090 GPU (Seq len,input c, output c) FP16 (ms) W4A4 (ms) Quarot + static quant + improved GEMV (1,4096,4096) (1,4096,14336) (1,8192,8192) (1,8192,28672) (2048,4096,4096) (2048,4096,14336) (2048,8192,8192) (2048,8192,28672) 0.0512 0.1548 0.1080 0.5762 1.0666 3.5766 3.9986 13.1607 0.0578 (0.89x) 0.0641 (2.42x) 0.0957 (1.77x) 0.2087 (2.76x) 0.3699 (2.88x) 0.9358 (3.93x) 1.0211 (4.03x) 2.8609 (4.74x) 0.0472 (1.08x) 0.0549 (2.83x) 0.0863 (1.97x) 0.1977 (2.91x) 0.2965 (3.59x) 0.8618 (4.27x) 0.8718 (4.72x) 2.7177 (4.99x) 0.0223 (2.30x) 0.0475 (3.27x) 0.0561 (3.02x) 0.1503 (3.83x) - - - - (b) Nvidia A100-80GB GPU (Seq len,input c, output c) FP16 (ms) INT4 (ms) Quarot + static quant + improved GEMV (1,4096,4096) (1,4096,14336) (1,8192,8192) (1,8192,28672) (2048,4096,4096) (2048,4096,14336) (2048,8192,8192) (2048,8192,28672) 0.0418 0.1026 0.1080 0.3036 0.2762 1.0092 1.0583 3.6897 0.0588 (0.71x) 0.0679 (1.51x) 0.0888 (1.22x) 0.1668 (1.82x) 0.2408 (1.15x) 0.5461 (1.85x) 0.5298 (2.00x) 1.4686 (2.51x) 0.0455 (0.92x) 0.0556 (1.85x) 0.0735 (1.47x) 0.1534 (1.97x) 0.1799 (1.54x) 0.4850 (2.08x) 0.4349 (2.43x) 1.3857 (2.66x) 0.0235 (1.78x) 0.0441 (2.33x) 0.0508 (2.13x) 0.1114 (2.72x) - - - - In this section, we examine the inference efficiency of PrefixQuant. We conduct tests on Nvidia RTX 3090 and A100-80GB GPUs, considering sequence lengths of 1 and 2048, with batch size of 16 Under review 1. We detail the speedup ratios for quantization overhead, quantized linear layers, and end-to-end inference below. Reduced Quantization Overhead. Activations are quantized and packed into low-bit formats for matrix manipulations. We define the time for this process during inference as quantization overhead. The per-token dynamic quantization kernel is sourced from QuaRot (Ashkboos et al., 2024b). Table 8 shows the speedup of per-tensor static quantization over per-token dynamic quantization. We can find that the per-tensor static quantization kernel achieves speedups of 3.31 on the RTX 3090 and 2.82 on the A100-80GB. Accelerated Quantized Linear Layer. The quantized linear layer consists of quantization, low-bit matrix multiplication, and de-quantization. The speedup for the quantization process is in Table 8. For low-bit matrix multiplication, we use the 4-bit GEMM kernel from CUTLASS and design custom kernel for W4A4 GEMV. We also integrate the de-quantization process into the GEMM and GEMV kernels. Table 9 presents the speedup ratios of the QuaRot kernel and our kernel compared to FP16. With sequence length of 1, our quantized linear layer improves the QuaRot speedup from 0.89 2.76 to 2.30 3.83 on the RTX 3090, and from 0.71 1.82 to 1.78 2.72 on the A100-80GB. With sequence length of 2048, our layer enhances the QuaRot speedup from 2.88 4.74 to 3.59 4.99 on the RTX 3090, and from 1.15 2.51 to 1.54 2.66 on the A100-80GB."
        },
        {
            "title": "E QUANTIZATION TIME",
            "content": "Table 10 shows the quantization time for PrefixQuant. PrefixQuant identifies prefixed tokens quickly, taking only 0.2 minutes for Llama-3-8B and 1 minute for Llama-3-70B. In contrast, the recent CushionCache (Son et al., 2024) requires 12 hours for the same task on Llama-3-8B. Additionally, the grid-search initialization is efficient, taking 0.7 minutes for Llama-3-8B and 12 minutes for Llama-3-70B. Experiments in Tables 3 and 4 demonstrate that PrefixQuant, even without finetuning, outperforms previous methods (Lin et al., 2024b; Ashkboos et al., 2024b). Fine-tuning requires more time, taking 2.2 hours for Llama-3-8B and 17 hours for Llama-3-70B, but it can successfully enhances the potential of low-bit quantization. Table 10: The quantization time of PrefixQuant on single NVIDIA-A100-80GB GPU. Fine-tuning indicates the time of 20 fine-tuning epochs of W4A4KV4. Model Find Prefixed Outliers Grid-search initialization Fine-tuning Llama-3-8B Llama-3-70B 0.2 1 0.7 12 2.2 17 h"
        },
        {
            "title": "F MORE ABLATION RESULTS",
            "content": "Table 11: Ablation studies on calibration dataset, including (a) Dataset type, (b) Training sequence length and (c) Total training tokens. indicates number of training samples, and is the length of each samples. The model used here is Llama-3-8B with W4A4KV4 quantization. Our default settings are marked in gray . (a) Dataset (b) Sequence length (c) Total token number Dataset Wiki PPL Wiki PPL Wiki PPL C4 RedPajama Pile 7.60 7.49 7.42 256 2048 5121024 1024512 7.65 7.42 7.65 256 1024 512 1024 1024 7.46 7.42 7.41 Fine-tuning Datasets. Table 11a shows results with different fine-tuning datasets, including C4 (Raffel et al., 2020), RedPajama (Computer, 2023), and Pile (Gao et al., 2020). We find that Pile 17 Under review Table 12: Ablation study about training epochs. The model used here is Llama-3-8B, and WikiText2 perplexity with 2048 context length is reported. Our default settings are marked in gray . Epochs W4A8KV4 W4A4KV 0 (w/o FT) 5 10 20 30 6.90 6.66 6.63 6.63 6.63 7.93 7.53 7.47 7.42 7.41 Table 13: Ablation study about quantization type of activation. The model used here is Llama-3-8B with W4A4KV4 quantization. WikiText2 perplexity with 2048 context length is reported. Fine-Tuning Quant Type W4A8KV4 W4A4KV4 No No Yes Yes token-wise dynamic tensor-wise static token-wise dynamic tensor-wise static 6.84 6.90 6.60 6.63 8.29 7.93 7.88 7.41 Table 14: Ablation study about the number of prefixed tokens. WikiText2 perplexity with 2048 context length and W4A4KV4 quantization is reported. Number indicates the first tokens in Table 1 are set as the prefixed tokens. Model Method 0 Llama-2-7B Llama-2-7B Mistral-7B-v0.3 Mistral-7B-v0.3 PrefixQuant w/o FT 333.52 17.63 90.02 15.97 PrefixQuant PrefixQuant w/o FT PrefixQuant 1 74.37 10.71 6.12 7.08 2 6.21 6.01 5.84 5.83 3 6.22 6.01 6.43 5. 4 - 5.89 5.79 achieves the best performance. Additionally, we ablate the sequence length of each training sample and the total training tokens. Table 11b shows that sequence length of 1024 achieves the best performance. Table 11c demonstrates that fine-tuning on 512 1024 tokens achieves satisfactory performance, with further increases in training samples only marginally improving performance. Note that the optimal token number for fine-tuning datasets may change with quantization precision. Generally, lower precision requires more training data. For example, EfficientQAT shows that 40962048 tokens are needed for W2A16 quantization, while our paper shows that only 5121024 tokens are needed for W4A4 quantization. Training Epochs. Table 12 demonstrates that 10 and 20 epochs are sufficient for the convergence of fine-tuning on W4A8KV4 and W4A4KV4. Dynamic Quantization. Tables 3 and 4 show that PrefixQuant with static quantization can surpass previous state-of-the-art methods (Xiao et al., 2023a; Ashkboos et al., 2024b; Lin et al., 2024b) with dynamic quantization. Note that without prefixed outliers, per-token dynamic quantization consistently outperforms per-tensor static quantization across different precisions, as shown in Table 6. Therefore, question arises: can dynamic quantization further improve the performance of PrefixQuant? We replace per-tensor static activation quantization in PrefixQuant with per-token dynamic quantization and report the results in Table 13. We find that the winner varies with different precision. Specifically, per-token dynamic quantization marginally surpasses per-tensor static quantization in W4A8KV4 quantization, while per-tensor static quantization significantly outperforms per-token dynamic quantization in W4A4KV4 quantization. This is because, in high-precision quantization such as 8-bit, clipping is not necessary (Gong et al., 2024), and the MAX-MIN initialization of dynamic quantization adapts to more diverse range flexibly. However, in low-precision quantization such as 4-bit, clipping is crucial to balance clipping error and rounding error (Lin et al., 2023), resulting in per-tensor static quantization outperforming per-token dynamic quantization. 18 Under review Table 15: Ablation study about the content of prefixed tokens. WikiText2 perplexity with 2048 context length and W4A4KV4 quantization is reported. default refers to the prefixed tokens obtained through the proposed method. random represents the average performance of 10 times with randomly selected prefixed tokens. Model Type Prefixed Wiki PPL (PrefixQuant w/o FT) Llama-2-7B Llama-2-7B Llama-2-7B Mistral-7B-v0.3 Mistral-7B-v0.3 Mistral-7B-v0.3 default only highest frequency random default only highest frequency random .n[BOS] ... - n.to[BOS] nnnn - 6.22 12.07 66.51 5.89 6.23 80.05 Number of Prefixed Tokens. In Sec. 5.1, we determine the number of prefixed tokens by calculating the average number of outlier tokens and adding an additional [BOS] token. Table 1 illustrates the specific number and content of these tokens. We use Llama-2-7B (3 outlier tokens) and Mistral-7Bv0.3 (4 outlier tokens) to study the impact of the number of prefixed tokens. Table 14 shows that the adaptively calculated number of prefixed tokens achieves the best performance. Notably, for models like Llama-2-7B, using 2 prefixed tokens without the additional [BOS] token also yields excellent performance. For consistency and simplicity, we include the [BOS] token in the prefixed tokens in our experiments. Content of Prefixed Tokens. PrefixQuant determines the number of outlier tokens and designates the top-N high-frequency outlier tokens as prefixes in the KV cache. Table 15 examines various prefixed tokens with the same token count. The results show that using the top-N high-frequency tokens as prefixed tokens significantly outperforms using only the highest-frequency or randomly selected tokens. Table 16: Weight-only quantization results. indicates group size for weight quantization. EfficientQAT only execute Block-AP and without E2E-QP for the fair comparisons in block-wise reconstruction scenario. We providing WikiText2 perplexity with 2048 context length and detailed zero-shot accuracy of weight-only quantization by lm eval v0.4.2. We report acc for WinoGrande and acc norm for HellaSwag, ArcC, ArcE, and PIQA. Model Method Precision Wiki PPL WinoGrande HellaSwag ArcC ArcE PiQA Avg. Acc. 3-8B 3-70B Baseline FP16 EfficientQAT W3A16g128 PrefixQuant W3A16g128 EfficientQAT W2A16g128 PrefixQuant W2A16g128 Baseline FP16 EfficientQAT W3A16g128 PrefixQuant W3A16g128 EfficientQAT W2A16g128 PrefixQuant W2A16g 6.14 7.34 7.17 13.55 11.97 2.85 4.89 4.79 16.79 11.01 72.61 70.48 72.38 62.04 66.22 80.51 78.77 78.22 66.14 72.3 79.17 75.09 76.54 62.49 66.54 84.9 83.74 84.03 73.01 78. 53.41 77.69 80.69 51.37 77.9 79.16 52.65 78.37 80.58 36.6 60.44 73.18 41.81 69.61 75.84 64.33 85.9 84.49 55.03 78.66 82.05 60.15 83.00 83.35 48.21 73.57 78.45 53.67 77.9 80.63 72.71 70.80 72.10 58.95 64.00 80.03 75.65 77.75 67.88 72.61 EXTEND TO WEIGHT-ONLY QUANTIZATION In addition to static activation quantization, setting prefixed outliers in the KV-cache improves training stability (Chen et al., 2024b) and reduces information loss from outlier tokens, can also enhancing weight-only quantization performance. To verify this, we compare PrefixQuant with the recent state-of-the-art weight-only quantization method, EfficientQAT (Chen et al., 2024a), in blockwise fine-tuning scenario. Following EfficientQAT, we use 4096 RedPajama (Computer, 2023) with 2048 context length to train for 2 epochs. The learning rates for quantization parameters and full-precision weights are set to 5e-5 and 5e-6, except for W2A16g128 Llama-3-8B, where they are 1e-4 and 2e-5, respectively. As shown in Table 16, PrefixQuant significantly surpasses EfficientQAT 19 Under review with +5.05 and +4.73 points in average accuracy on W2A16g128 Llama-3-8B and Llama-3-70B, respectively. FULL RESULTS OF WEIGHT-ACTIVATION QUANTIZATION Table 17: W8A8 performance comparisons with other methods that also set prefixed tokens in KV cache. Model Method Activation Quant Wiki PPL 2-7B 2-13B 2-70B 3-8B QFeP CushionCache PrefixQuant QFeP PrefixQuant QFeP PrefixQuant CushionCache PrefixQuant per-tensor dynamic per-tensor static per-tensor static per-tensor dynamic per-tensor static per-tensor dynamic per-tensor static per-tensor static per-tensor static 5.75 5.87 5.48 6.00 4.89 6.01 3.39 7.37 6.17 H.1 COMPARISONS WITH RELATED WORKS CushionCache (Son et al., 2024) and QFeP (Yang et al., 2024) also set prefixed tokens in the KV cache to reduce outliers. However, they experience significant performance degradation even with W8A8 quantization. Table 17 shows that PrefixQuant outperforms QFeP by 2.62 perplexity on Llama-2-70B and surpasses CushionCache by 1.20 perplexity on Llama-3-8B. H.2 DETAILED ACCURACY RESULTS In the main paper, we present the average accuracy of five common reasoning tasks for brevity. Here, we provide detailed results for each task in Table 18. H.3 RESULTS ON MORE MODELS Table 19 shows the effectiveness of the proposed PrefixQuant in other models, including Mistralv0.3-7B and Qwen-2-7B. It also includes instruction-tuned models such as Llama-3-{8B,70B}- Instruct."
        },
        {
            "title": "I MORE VISUALIZATIONS",
            "content": "I.1 OUTLIER TOKEN In Figure 7, we showcase the four most frequently occurring outlier tokens in Llama-2-{13B,70B}, Llama-3-70B, and Mistral-7B-v0.3. Specifically, Table 1 selects the top-o high-frequent outlier tokens as the prefixed tokens. It is important to note that we do not visualize the outlier tokens in Llama-3-8B and Qwen-2-7B because all the outlier tokens in these two models appear in the initial tokens. I.2 MAGNITUDE DISTRIBUTION We illustrate more token-wise maximum values distribution of other models. Details are as follows: Llama-2-13B: Figure 8 and Figure 9 illustrate the distribution of input activation and Q/K/V, respectively. 20 Under review Table 18: Continuation of Table 3 and Table 4, providing detailed zero-shot accuracy of weightactivation quantization of Llama models by lm eval v0.4.2. We report acc for WinoGrande and acc norm for HellaSwag, ArcC, ArcE, and PIQA.). Precision WinoGrande HellaSwag ArcC ArcE PiQA Avg. Acc. Model 2-7B 2-13B 2-70B 3-8B 3-70B Method Baseline Atom QuaRot DuQuant SpinQuant FP16 W4A4KV4 W4A4KV4 W4A4KV4 W4A4KV4 PrefixQuant w/o FT W4A4KV4 W4A4KV4 W4A8KV8 W4A8KV8 PrefixQuant w/o FT W4A8KV8 PrefixQuant W4A8KV8 SmoothQuant W8A8KV8 W8A8KV8 PrefixQuant w/o FT W8A8KV PrefixQuant QoQ QuaRot QuaRot Baseline Atom QuaRot DuQuant SpinQuant FP16 W4A4KV4 W4A4KV4 W4A4KV4 W4A4KV4 PrefixQuant w/o FT W4A4KV4 W4A4KV4 W4A8KV8 W4A8KV8 PrefixQuant w/o FT W4A8KV8 W4A8KV8 PrefixQuant SmoothQuant W8A8KV8 W8A8KV8 PrefixQuant w/o FT W8A8KV8 PrefixQuant QoQ QuaRot QuaRot Baseline Atom QuaRot DuQuant SpinQuant FP16 W4A4KV4 W4A4KV4 W4A4KV4 W4A4KV4 PrefixQuant w/o FT W4A4KV4 W4A4KV4 W4A8KV8 W4A8KV8 PrefixQuant w/o FT W4A8KV8 PrefixQuant W4A8KV8 SmoothQuant W8A8KV8 W8A8KV8 PrefixQuant w/o FT W8A8KV8 PrefixQuant QoQ QuaRot QuaRot Baseline QuaRot DuQuant SpinQuant FP16 W4A4KV4 W4A4KV4 W4A4KV4 PrefixQuant w/o FT W4A4KV4 W4A4KV4 W4A8KV8 W4A8KV8 PrefixQuant w/o FT W4A8KV8 PrefixQuant W4A8KV8 SmoothQuant W8A8KV8 W8A8KV8 PrefixQuant w/o FT W8A8KV PrefixQuant QoQ QuaRot QuaRot Baseline QuaRot DuQuant SpinQuant FP16 W4A4KV4 W4A4KV4 W4A4KV4 PrefixQuant w/o FT W4A4KV4 W4A4KV4 W4A8KV8 W4A8KV8 PrefixQuant w/o FT W4A8KV8 PrefixQuant W4A8KV8 SmoothQuant W8A8KV8 W8A8KV8 PrefixQuant w/o FT W8A8KV8 PrefixQuant QoQ QuaRot QuaRot 69.22 62.75 64.40 67.09 66.54 67.80 66.54 68.03 66.77 69.14 69.06 69.22 68.98 70.48 72.22 67.40 67.88 68.9 67.88 72.06 72.53 70.96 70.24 72.77 72.77 72.14 71.98 72.53 79.48 74.27 76.24 75.45 75.85 75.45 77.35 77.51 77.03 77.35 79.08 77.03 77.82 79.16 72.61 65.98 68.59 69.22 69.14 71.9 73.4 72.74 71.19 72.53 73.01 72.53 74.11 80.51 68.51 70.8 76.4 77.43 77.35 80.11 80.35 79.23 79.48 79.40 80.66 79.40 76.00 69.37 72.3 72.53 73.15 73.75 73.42 74.00 74.56 75.12 75.25 76.32 75.96 76.62 79.37 73.84 75.28 76.65 77.01 76.54 76.12 77.80 78.21 77.49 77.54 79.34 79.35 78.38 84.31 79.06 81.82 81.95 82.36 80.51 82.3 82.78 83.30 82.79 83.56 83.38 83.8 84.14 79.17 72.38 74.27 74.83 75.46 75.44 77.23 77.35 77.65 77.97 78.99 78.99 79.25 84.9 76.75 79.89 80.9 83.48 83.79 83.7 84.03 84.71 84.86 84.64 84.84 85.5 46.25 74.62 79.11 38.40 52.99 75.14 41.47 68.06 76.17 43.26 71.38 76.99 41.64 69.32 76.12 43.94 71.51 77.2 43.09 71.17 77.64 43.60 72.81 77.64 43.86 72.39 77.97 44.45 73.06 77.53 44.8 73.19 78.13 45.56 74.71 78.78 46.59 74.41 79.11 45.65 73.91 78.18 49.06 77.48 80.52 42.32 57.49 76.50 45.65 72.35 77.48 47.7 74.24 78.18 46.76 75.97 78.56 46.67 75.8 78.51 47.70 76.09 79.38 48.38 75.97 79.71 47.01 74.49 79.87 48.12 77.06 79.92 48.72 76.81 80.41 48.89 77.31 80.2 49.23 77.4 80.47 48.98 76.81 80.9 56.91 80.30 82.54 46.08 58.25 79.92 56.23 80.43 82.43 55.03 82.32 79 56.31 79.17 81.61 52.3 77.06 81.12 56.4 79.29 82.05 56.83 79.80 82.64 57.08 81.27 82.86 54.35 78.28 82.21 57.42 80.39 82.05 56.91 80.72 82.92 57.34 80.93 82.75 55.8 78.87 82.59 53.41 77.69 80.69 44.45 67.3 75.63 46.5 70.41 75.9 45.99 74.07 77.04 47.1 72.94 77.2 50.68 78.32 79.05 50.87 75.59 79.65 51.62 77.48 79.22 48.98 73.99 79.65 52.65 79.25 79.92 53.07 77.82 80.74 53.67 78.03 80.63 53.75 78.03 80.36 64.33 85.9 84.49 47.01 72.31 77.37 59.04 82.91 81.83 80.8 77.3 58.87 79.88 82.32 60.15 81.31 83.3 83 61.01 82.79 62.12 84.64 83.46 59.39 81.57 84.22 62.29 82.53 84.33 63.14 85.35 83.9 63.65 85.56 84.44 61.43 82.49 84.22 56 69.04 59.73 64.48 66.25 65.35 66.84 66.37 67.22 67.11 67.86 68.09 68.92 69.01 68.97 71.73 63.51 67.73 69.13 69.24 69.92 70.36 70.56 69.96 71.07 71.25 71.58 71.69 71.52 76.71 67.52 75.43 74.75 75.19 73.29 75.48 75.91 76.31 75.00 76.50 76.19 76.53 76.11 72.71 65.15 67.13 68.23 68.37 71.08 71.35 71.68 70.29 72.46 72.73 72.77 73.10 80.03 68.39 74.89 74.28 76.40 77.18 78.12 78.92 77.82 78.70 79.29 79.83 78.61 Llama-3-8B: Figure 10 and Figure 11 illustrate the distribution of input activation and Q/K/V, respectively. 21 Under review Table 19: Results of proposed PrefixQuant on other models. Model Precision Wiki PPL WinoGrande HellaSwag ArcC ArcE PiQA Avg. Acc. Mistral-v0.3-7B Qwen-2-7B Llama-3-8B-Instruct Llama-3-70B-Instruct FP16 W8A8KV8 W4A8KV4 W4A4KV4 FP16 W8A8KV8 W4A8KV4 W4A4KV FP16 W8A8KV8 W4A8KV4 W4A4KV4 FP16 W8A8KV8 W4A8KV4 W4A4KV4 5.32 5.34 5.51 5.79 7.14 7.15 8.04 8.37 8.29 8.21 8.74 8.96 5.33 5.40 5.96 6. 73.88 74.03 73.88 71.51 72.3 72.22 71.43 68.75 71.82 71.35 70.17 69.53 75.69 78.06 77.74 75.93 80.43 80.8 79.8 78.12 78.96 78.88 76.77 74. 75.81 75.54 74.6 74.66 82.58 82.67 81.97 80.64 52.3 78.28 82.26 53.5 79.76 81.72 52.05 79.42 80.79 49.66 78.03 79.92 52.65 78.75 80.96 52.9 78.49 80.85 53.67 77.95 78.45 48.21 74.75 79.49 56.83 79.76 78.51 56.31 78.75 79.16 54.44 77.65 77.97 52.65 76.35 76.66 64.42 84.97 82.15 66.72 84.89 82.21 65.87 84.93 81.56 64.76 83.88 81. 73.43 73.96 73.19 71.45 72.72 72.67 71.65 69.22 72.55 72.22 70.97 69.97 77.96 78.91 78.41 77.29 (a) Llama-2-13B (b) Llama-2-70B (c) Llama-3-70B (d) Mistral-7B-v0.3 Figure 7: Content of outlier tokens in different models. Note that we do not count the outlier tokens situated at the initial token. Llama-3-70B: Figure 12 and Figure 13 illustrate the distribution of input activation and Q/K/V, respectively. Qwen-2-7B: Figure 14 and Figure 15 illustrate the distribution of input activation and Q/K/V, respectively. Mistral-7B-v0.3: Figure 16 and Figure 17 illustrate the distribution of input activation and Q/K/V, respectively. 22 Under review (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 8: Distribution of token-wise maximum values for linear layers inputs in Llama-2-13b. (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 9: Distribution of token-wise maximum values for Q/K/V in Llama-2-13b. Same present rules as Figure 8a except that ratios greater than 5 are marked with red. 23 Under review (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 10: Distribution of token-wise maximum values for linear layers inputs in Llama-3-8b. (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 11: Distribution of token-wise maximum values for Q/K/V in Llama-3-8B. Under review (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 12: Distribution of token-wise maximum values for linear layers inputs in Llama-3-70B. (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 13: Distribution of token-wise maximum values for Q/K/V in Llama-3-70B. 25 Under review (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 14: Distribution of token-wise maximum values for linear layers inputs in Qwen-2-7B. (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 15: Distribution of token-wise maximum values for Q/K/V in Qwen-2-7B. 26 Under review (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 16: Distribution of token-wise maximum values for linear layers inputs in Mistral-7Bv0.3. (a) Original distribution (b) Rotation (c) PrefixQuant (ours) Figure 17: Distribution of token-wise maximum values for Q/K/V in Mistral-7b-v0.3."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The University of Hong Kong",
        "Tongji University"
    ]
}