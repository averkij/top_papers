{
    "paper_title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "authors": [
        "Lingjie Jiang",
        "Xun Wu",
        "Shaohan Huang",
        "Qingxiu Dong",
        "Zewen Chi",
        "Li Dong",
        "Xingxing Zhang",
        "Tengchao Lv",
        "Lei Cui",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 1 3 6 4 1 . 5 0 5 2 : r Think Only When You Need with Large Hybrid-Reasoning Models Lingjie Jiang Xun Wu Shaohan Huang Qingxiu Dong Zewen Chi Li Dong Xingxing Zhang Tengchao Lv Lei Cui Furu Wei Microsoft Research Peking University https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce metric called Hybrid Accuracy to quantitatively assess the models capability for hybrid thinking. Expensive experimental results show that LHRMs can adaptively performs hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for reconsideration of the appropriate use of extended thinking processes, and provides solid starting point for building hybrid thinking systems."
        },
        {
            "title": "Introduction",
            "content": "Recent remarkable advancements in Large Reasoning Models (LRMs), such as DeepSeekR1 [GYZ+25], OpenAI o1/o3 series [Ope25] and others [Tea24, MYS+25], have catalyzed significant paradigm shift from conventional Large Language Models (LLMs) toward LRMs. Compared to LLMs, LRMs exhibit substantially improved reasoning capabilities and generalization across tasks such as programming, mathematics, and commonsense reasoning [XHZ+25, GYZ+25, XCM+25]. These improvements are primarily attributed to the generation of extended thinking traces, often marked by special tokens such as <think>, prior to producing the final responses. Despite the effectiveness of LRMs, existing work primarily focuses on converting LLMs into LRMs to enhance performance across various domains, typically through reinforcement learning algorithms such as GRPO [GYZ+25] and REINFORCE++ [Hu25], or by fine-tuning on distilled thinking traces [MHS+25]. However, broader implications regarding the potential challenges faced by LRMs in real-world applications remain underexplored. prominent issue is the problem of overthinking [CXL+24, SCW+25], where excessive computational resources are expended on simple queries with negligible performance gains, even for trivial inputs such as single word (e.g. Hello). These findings highlight several pressing limitations of current LRMs and raise critical question: Equal contribution. Corresponding author. Question 1: Let a, b, be real numbers such that ax2 + bx + 1for all 0 1. Find the largest possible value of + + c. Lets think step by step and output the final answer within boxed{}. Ground Truth: 17 Qwen2.5-7B-Instruct: To find the largest possible value of + + given that ax2 + bx + 1 for all 0 1, we need to analyze the constraints imposed by this inequality at specific points. ... All conditions are satisfied. Now, calculate + + c: + + = 2 + 2 + 1 = 2 + 2 + 1 = 5 Thus, the largest possible value of + + is 5 . DeepSeek-R1-Distill-Qwen-7B: <think> Okay, so have this problem where need to find the largest possible value of + + given that ax2 + bx + 1 for all between 0 and 1. Hmm, let me think about how to approach this. ... </think> The maximum value of + + is achieved when the quadratic function touches 1 at x=0 and x=1, and -1 at x=1/2. ... For 0 1, 0 (cid:0)x 1 2 4 , so 1 (x) 1. Therefore, the largest possible value of + + is 17 (cid:1)2 LHRMs-7B: <think> Okay, so need to find the largest possible value of + + given that the quadratic function ax2 + bx + is bounded by 1 in absolute value for all between 0 and 1. Hmm, lets start by understanding the problem. ... </think> Given the problem of finding the largest possible value of + + under the constraint that ax2 + bx + 1 for all 0 1, we start by considering the Chebyshev polynomials and their properties. ... After verifying that this polynomial indeed satisfies the given constraint and considering other potential polynomials, we conclude that the maximum value of + + is achieved by the polynomial 8x2 8x + 1. The largest possible value of + + is 17 . Question 2: Can you help me please? Qwen2.5-7B-Instruct: Of course! Id be happy to help you. Could you please provide some more information about what you need assistance with? DeepSeek-R1-Distill-Qwen-7B: <think> Okay, the user just said \"Can you help me please?\" need to respond in friendly and helpful way. should acknowledge their request, show willingness to assist, and ask them to specify what they need help with. Keeping it open-ended will encourage them to provide more details. Hmm, maybe start with positive affirmation. Something like, \"Yes, can help! What do you need assistance with today?\" That sounds good. Its polite and inviting them to share more about their request. should make sure my response is clear and concise. No need for complicated words. Just straightforward offer to help. Let me put that together. </think> Of course! Im here to help. What do you need assistance with today? LHRMs-7B: <no_think> Of course! What do you need help with?</no_think> Figure 1: Example responses from Qwen2.5-7B-Instruct, DeepSeek-R1-Distill-Qwen-7B, and our LHRMs-7B on reasoning related task (top) and daily QA task (bottom). While LLMs respond concisely to simple queries, but struggle with complex reasoning. LRMs handle reasoning well via explicit thinking steps but often overuse them on simple tasks, resulting in slower responses and degraded user experience. In contrast, LHRMs adaptively determines when to engage in thinking, preserving strong reasoning ability while enabling faster, more natural interactions in everyday scenarios. Additional examples are provided in Appendix E. How to build hybrid thinking system that can achieve an optimal balance between system 2 reasoning and system 1 ability? To fill these gaps and make LRMs more efficient, intelligent, and better aligned with human usage requirements, we draw inspiration from human cognitive processes, where complex problems often require deliberate thinking while simple ones can be handled intuitively. Building on this insight, we introduce Large Hybrid-Reasoning Models (LHRMs), novel class of models that, to the best of our knowledge, are the first to explicitly address the limitations of existing LRMs by dynamically deciding whether to invoke extended thinking based on the semantic and contextual characteristics of user queries. As shown in Figure 1, this adaptive thinking mechanism allows LHRMs to balance computational efficiency with task complexity, better aligning with realworld deployment scenarios and enhancing user experience without compromising performance. To achieve this, we propose two-stage training pipeline. The first stage, Hybrid Fine-Tuning (HFT), provides an effective cold start by enabling the model to learn to support two distinct thinking modes on the same query without mode collapse. This is followed by Hybrid Group Policy Optimization, variant of online RL designed to teach the model when to engage in extended thinking while simultaneously generating more helpful and harmless responses. Furthermore, we introduce metric termed Hybrid Accuracy, which correlates strongly with human expert judgment, to quantitatively evaluate the models capability for hybrid thinking. Extensive experimental results and human studies conducted on Qwen-2.5 series models ranging from 1.5B to 7B parameters across multiple domains (including mathematics, programming, and general tasks) demonstrate that our LHRMs effectively performs hybrid thinking by adapting to queries of varying difficulty and types. Moreover, LHRMs outperforms existing LRMs and LLMs 2 in both reasoning and general capabilities, while significantly improving efficiency. In general, our main contribution is as follows: 1. We introduce Large Hybrid-Reasoning Models, the first kind of model which can adaptively determine whether to perform thinking based on the contextual information of user queries, mimicking human reasoning patterns and mitigating the over-thinking problem. 2. We develop two-stage training pipeline comprising Hybrid Fine-Tuning as cold start and novel RL method, Hybrid Group Policy Optimization, which enables the model to learn when to engage in thinking while simultaneously generating more helpful and harmless responses. 3. Extensive experiments across diverse model scales and domains demonstrate that LHRMs accurately performs hybrid thinking and significantly improves both efficiency and general capabilities compared to existing LRMs and LLMs."
        },
        {
            "title": "2 Large Hybrid-Reasoning Models",
            "content": "2.1 Problem Formulation Specifically, let be an input query. The LHRMs has access to two thinking modes: Thinking and No-Thinking, denoted as = {, }, respectively. Each mode defines inducing conditional distribution P(a q, m) over answers A, i.e., P(a q, m), M. (1) For each query q, the model aims to select the optimal reasoning mode m(q) from the candidate set M, such that task-specific utility function U(q, a) is maximized in expectation: m(q) = arg max mM EaP(aq,m) (cid:104) (cid:105) U(q, a) . (2) The overall objective is to learn policy π : to select mode that maximizes the expected utility U(q, A) over the query distributions Θ = {(Di, Ui)}N i=1 : max π 1 (cid:88) i=1 (cid:34) EDiΘ, DiUi EaP(aq,π(q)), qDi (cid:104) (cid:105) Ui(q, a) (cid:35) . (3) We summarize two key challenges and our corresponding solutions as follow: C1. How to learn the policy π based on the Ui. We first propose two-stage training phase to lean effective LHRMs by: Hybrid Fine-Tuning as code start ( 2.2), following by online reinforcement learning with Hybrid Group Policy Optimization ( 2.3) to guides LHRMs to more effectively select between two reasoning modes to generate more appropriate and high-quality responses, respectively. C2. How to evaluate the hybrid thinking capability of policy π. We propose evaluation metric named hybrid accuracy, which is designed to assess the hybrid thinking ability of models in 2.4. 2.2 Stage I: Hybrid Fine-Tuning We begin with hybrid-formatted supervised fine-tuning stage named Hybrid Fine-Tuning (HFT) that integrates both reasoning-intensive (Thinking) and direct-answer (No-Thinking) data. This approach mitigates the instability often observed in cold-start scenarios [GYZ+25], and establishes robust initialization for next stage reinforcement learning. Data Construction. The hybrid fine-tuning dataset consists of both reasoning-intensive and directanswer examples. The think-style subset includes high-quality math, code, and science questions sourced from existing datasets [MJB+25, Fac25, PLK+25, Tea25, XLY+25], with answers generated by Deepseek-R1 [GYZ+25] and verified for correctness. Each instance is annotated with <think> and </think> tags to mark intermediate reasoning steps. For the non-think-style subset, we collect simple queries from WildChat-1M [ZRH+24] using FastText-based classifier [JGB+16] to exclude complex reasoning tasks. The remaining factual and conversational examples are wrapped with <no_think> and </no_think> tags to indicate that direct answers are sufficient. After deduplication and the removal of overlaps with evaluation benchmarks, we obtain final set of 1.7M hybrid-formatted training examples. This curated collection provides high-quality, 3 Algorithm 1 Hybrid Group Policy Optimization Input model trained at Stage πθHFT ; reward models Rϕ; queries P; hyperparameters ϵ, β, µ 1: policy model πθ πθHFT 2: for iteration = 1, . . . , do 3: 4: 5: 6: 7: Sample batch Pb from Update the old policy model πθold πθ Sample outputs πθold ( q, m) for reference model πref πθ for step = 1, . . . , do Pb and Compute rewards r(om ) for om by to rinter(om ) and running Rϕ rintra(om Assign each r(om ) ) using Eq. 9 Compute At for the t-th token of om through Eq. 10 for iteration = 1, . . . , µ do Update the πθ by maximizing the objec8: 9: 10: 11: 12: tive JHGPO in Eq. 11 end for end for 13: 14: 15: end for Output πθ Figure 2: Demonstration of Hybrid Group Policy Optimization. HGPO proceeds by: (1) sampling multiple responses for each query using both reasoning modes; (2) scoring the responses with the reward model and assigning these rewards based on Eq. 9; and (3) computing the advantage and policy loss, followed by updating the policy model. AE denotes advantage estimator and reward assignment denotes Eq. 9. format-consistent foundation for cold-start training, enabling the model to effectively handle both reasoning-heavy and direct-response tasks. (cid:88) Optimize Objective. HFT trains the model to predict the next token based on prior context. For the constructed dataset DHFT = {(xi, yi)}N i=1, the objective is: LHFT(θ) = E[(x,y)DHFT] log πθ(yt x, y1:t1) , (4) t=1 2.3 Stage II: Hybrid Group Policy Optimization After Stage I, LHRMs acquires an initial ability to support two distinct reasoning modes in on the same query without collapsing. However, our ultimate goal is to enable the LHRMs to adaptively select the most appropriate reasoning mode m(q) (Eq. 2) for each query to improve its applicability in real-world scenarios. To this end, we propose effective RL algorithm named Hybrid Group Policy Optimization (HGPO) to explicitly teach the model how to adaptively select between reasoning modes, i.e., to learn the policy π in Eq. 3, while simultaneously improving the models fundational ability (e.g., helpfulness and harmlessness). HGPO is illustrated in Figure 2 and Algorithm 1. Following prior RL approaches [ACG+24, Hu25, SWZ+24], HGPO discards the critic model and instead estimates the values using multiple samples for each prompt to reduce the computational cost training. Specifically, for each question in the task prompt set P, HGPO samples two groups of outputs from the old policy πθHFT , using Thinking and No-Thinking mode, respectively. Sampling Strategy. Given query P, we sample candidate responses from the old policy πθHFT under two distinct reasoning modes = {, }. Specifically, {o }N/2 i=1 πθHFT ( q, = ), {o }N/2 i=1 πθHFT ( q, = ) We define the complete output candidate set as: O(q) = (cid:8)o (cid:9)N/2 i=1 (cid:8)o (cid:9)N/2 i=1 . 4 (5) (6) Reward Scoring and Assignment. reward function Rϕ yielding two reward sets: 2 is applied to each candidate output, = (cid:8)r(o )(cid:9)N/2 i=1 , = (cid:8)r(o )(cid:9)N/2 i=1 . (7) Since the reward scores assigned by the reward model may vary significantly across different domains, we apply rule-based assignment scheme to normalize the existing reward scores. Specifically, we compute two types of binary reward: inter-group rewards rinter and intra-group rewards rintra to jointly capture both the relative quality across different reasoning modes and the answer quality within each individual reasoning mode. Given the average reward for each mode computed as ="
        },
        {
            "title": "2\nN",
            "content": "N/2 (cid:88) i=1 r(o ), ="
        },
        {
            "title": "2\nN",
            "content": "N/2 (cid:88) i=1 r(o ), (8) we have rinter and rintra assigned as: if = arg max 1, m{,} rinter(om ) = 0, otherwise { R, + δ} , rintra(om ) = 1, if = arg max j{1,...,N/2} rm . (9) 0, otherwise Here margin δ is hyperparameter that governs the trade-off between two reasoning modes. Advantage Estimation. After getting the final two kinds of rewards for O, we can adopt advantage estimators like REINFORCE++ [Hu25], GRPO [SWZ+24], and RLOO [ACG+24] to estimate the advantages. Here we use GRPO as the default advantage estimator, i.e., At = (cid:20) rintra (oi) mean(rintra (oj)) std(rintra (oj)) (cid:123)(cid:122) GRPO for intra-group advantage Aintra (cid:124) (cid:21) (cid:125) (cid:124) + 1{ot Φ} α (cid:20) rinter(oi) mean (rinter (oj)) std (rinter (oj)) (cid:123)(cid:122) GRPO for inter-group advantage Ainter (cid:21) , (10) (cid:125) where At is the final estimated per-token advantage for each response in O, Φ = {<think>, <no_think>} and α is the hyperparameter for controlling the weight of inter-group advantages. We also explore the impact of using different estimators in 3.3. Optimize Objective. Following [ACG+24, Hu25, SWZ+24], HGPO optimizes the policy model πθ by maximizing the following objective: [qP, {om i=1πθHFT (Oq), mM] JHGPO (θ) = (cid:88) (cid:34) oi (cid:88) (cid:32) min 1 }N (cid:0)om,t (cid:0)om,t πθ πθHFT t= i=1 where (cid:1) q, om,<t q, om,<t (cid:32) (cid:1) At i, clip πθ πθHFT (cid:1) (cid:0)om,t (cid:0)om,t q, om,<t q, om,<t (cid:33) (cid:33) (cid:1) , 1 ϵ, 1 + ϵ At (cid:35) βDKL (πθπref) , (11) πref(om πθ(om Here πθHFT denotes the model we get from Stage in 2.2 and ϵ and β are hyper-parameters. DKL (πθπref) = πref(om πθ(om q) q) q) q) log 1. (12) 2.4 Evaluating Hybrid Thinking Capability To more comprehensively evaluate the performance of LHRMs, beyond conventional downstream task metrics, we propose new metric called Hybrid Accuracy (HAcc), which aims to measure the LHRMs ability to correctly select the appropriate reasoning pattern. Given the task prompt set = {pi}K i=1, LHRMs is first applied to sample responses under two distinct reasoning modes and . reward model is then employed as scorer to evaluate and assign scores to both sets of generated responses. The mode with the higher average score is regarded as the ground-truth preferred reasoning mode for each prompt, denoted as mgt. In cases where the average scores of the two modes are equal or the difference between them falls below predefined margin, the mode yielding the shorter response is selected as mgt. Subsequently, we allow LHRMs to autonomously select reasoning mode mp for each prompt. The proportion of prompts for which LHRMs selected mode matches the ground-truth mode is reported as Hacc = 1 (cid:88) (cid:104) 1 i=1 Equal (mgt, mp) (cid:105) s.t. mgt, mp {, }. (13) 2For queries with definitive answers, we use rule-based rewards [SWZ+24, GYZ+25] for better reward estimation; otherwise, trained parametric reward model is applied."
        },
        {
            "title": "3 Experimental Results",
            "content": "3.1 Experimental Setup Compared Baselines. To validate the effectiveness of our proposed method LHRMs, we conduct comprehensive comparison against state-of-the-art LLMs and LRMs derived from the same base models. Specifically, we build our LHRMs on both 1.5B and 7B parameter versions of Qwen-2.5math-base [YZH+24], and compare our method with several Qwen-2.5-derived variants, including: LLMs: we compare our model with Qwen-2.5 Math series [YZH+24] and Instruct series [YYZ+24], which show great capabilities in coding, mathematics and general tasks. LRMs: Here we use DeepSeek-R1-Distill series [DA25], which are distilled using the reasoning data generated by DeepSeek-R1[DA25] and attain strong reasoning ability. Hybrid: Due to the absence of existing models capable of hybrid thinking, we compare our final model against the baseline we obtained in our Stage ( 2.2) (denoted as HFT). Additionally, we construct two further baselines by applying Direct Preference Optimization (DPO) [RSM+23] and Rejection Sampling Fine-Tuning (RFT) [YYL+23] to the checkpoint πθHFT from Stage I, using the same training data as in Stage II ( 2.3). These baselines are referred to as HFT-DPO and HFT-RFT, respectively. Implementation details of DPO and RFT can be found in Appendix A. Evaluation Settings. We primarily evaluate model performance based on the following aspects: Reasoning Capabilities. We evaluate models on comprehensive set of widely-used reasoning benchmarks, including math related like MATH500 [LKB+23], AIME24 [AM24], AMC23, Olympaid Bench [HLB+24], and code related like LiveCodeBench [JHG+24], MBPP [AON+21] and MBPP+ [LXW+24]. General Capabilities. We assess the models general capabilities through open-ended generation tasks using LLMs as judges. Specifically, we adopt AlpacaEval 2.0 [DGLH24] and ArenaHard [LCF+24] to assess instruction-following ability and alignment with human preferences. Hybrid Thinking Capabilities. We compute Hybrid Accuracy (HAcc) ( 2.4) on MATH500 to evaluate the models ability to correctly select the appropriate reasoning mode. More details about the evaluation settings can be found in Apendix D. Training Settings. For stage I, we finally obtained 1.7 SFT data and details about construction pipeline, sources and statistics can be found in 2.2 and Appendix C. All models are trained for 3 epochs with the AdamW optimizer, employing 10% linear warmup followed by cosine learning rate decay schedule. The maximum learning rate is set to 1e4, with batch size of 128 and maximum sequence length of 32k tokens. Training the 7B model in the SFT phase takes approximately 2.5 days on 4 nodes of NVIDIA 8H100 stations. For stage II, we construct the training dataset by randomly sampling 76K queries from Deepscaler [LTW+25] and Tülu3 [LMP+24] (details can be found in Appendix C). We use Llama-3.1Tulu-3-8B-RM 3 as the parametric reward model in Eq. 7. We use VeRL [SZY+24] to conduct experiments. By default, we use constant 1 106 learning rate together with AdamW optimizer for policy model, and use batch size of 256 and micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each prompt. We set α = 1.0 and margin = 0.2 for RL training. We set KL coefficient to 0.001 and ϵ = 0.5 in Eq. 11 in all experiments. The RL phase takes 2 days on NVIDIA 4H100 Stations. 3.2 Main Results Overall Performance. The overall results of different models on the aforementioned benchmarks are presented in Table 1. We observe that LHRMs consistently outperforms all comparable baselines across both the 1.5B and 7B model scales, We observe that LHRMs consistently outperforms all comparable baselines across both the 1.5B and 7B model scales, achieving average improvements of 9.2% and 7.1% compared to HFT-DPO, and 18.3% and 11.5% compared to HFT-RFT, at the 1.5B and 7B scales, respectively. Specifically, it surpasses the strongest competing baseline, HFT-DPO, by 10.3% and 13.6% on the AIME24 benchmark at 1.5B and 7B scales, respectively. On the 7B 3https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-RM 6 Table 1: Performance comparison across different tasks. - indicates that the model does not support hybrid thinking and is therefore left blank. The last column (Avg.) reports the average performance across all evaluation tasks, excluding the Hacc metric. Bold numbers indicate the best performance. Type refers to the models reasoning mode, where Hybrid denotes models that adaptively select between Thinking and No-Thinking modes for each query. MATH Code General Methods Type MATH AIME24 AMC23 Olympiad LiveCode"
        },
        {
            "title": "MBPP",
            "content": "MBPP+ Alpaca Arena Hacc Avg. LLMs Qwen2.5-Math-1.5B LLMs Qwen2.5-1.5B-Instruct Qwen2.5-Math-1.5B-Instruct LLMs DeepSeek-R1-Distill-Qwen-1.5B LRMs Hybrid HFT-1.5B Hybrid HFT-RFT-1.5B Hybrid HFT-DPO-1.5B Hybrid LHRMs-1.5B Qwen2.5-Math-7B Qwen2.5-7B-Instruct Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-Qwen-7B HFT-7B HFT-RFT-7B HFT-DPO-7B LHRMs-7B LLMs LLMs LLMs LRMs Hybrid Hybrid Hybrid Hybrid 1.5B size model 3.3 3.3 6.7 28.9 32.7 22.0 32.6 35.3 22.5 52.8 60.0 62.9 75.0 67.5 75.0 75.0 16.7 38.7 38.1 43.3 48.9 44.1 48.7 50.4 7B size model 13.3 13.3 10.0 55.5 56.7 55.3 58.7 66.7 22.5 52.8 62.5 91.5 95.0 82.5 92.5 95.0 21.8 29.1 41.6 58.1 58.5 55.0 60.6 61.2 42.4 51.0 72.0 83.9 87.8 82.2 86.8 87.8 57.0 77.0 82.4 92.8 93.6 87.8 93.8 93.8 0.4 2.2 3.7 16.8 15.7 14.2 17.2 17. 6.0 14.6 2.6 37.6 34.7 35.8 38.8 38.8 16.1 60.1 26.7 54.2 54.8 49.7 50.5 61.1 31.5 79.9 40.2 74.3 70.6 81.0 80.1 81.5 14.3 51.9 23.8 46.3 47.4 42.6 42.6 54.0 27.3 67.5 34.7 64.3 59.8 68.8 68.3 69.6 0.1 8.8 2.8 5.6 13.1 13.6 13.3 16. 2.0 36.2 3.8 19.1 23.7 28.1 23.3 35.0 1.8 1.1 4.7 2.7 6.9 8.5 6.9 10.4 7.0 25.8 10.0 17.9 14.0 14.0 13.0 26.0 - - - - 13.1 30.0 26.5 38.3 41.4 42.5 48.1 38.3 45.8 41.5 54.4 45.3 - - - - 20.9 44.0 32.0 56.8 34.2 56.4 49.7 56.6 37.1 58.9 71.9 63.1 scale, LHRMs further outperforms HFT-DPO by 50.2% on Alpaca and 93.4% on Arena-Hard. Notably, AIME24 and Arena-Hard represent the most challenging benchmarks in the math and general domains, respectively. These results demonstrate the strong reasoning and general capability of our LHRMs. Furthermore, we find that our LHRMs achieves the best hybrid thinking performance, as measured by Hacc, significantly outperforming all baselines. For example, it exceeds HFT-DPO by 93.8% and RFT by 44.7%. This further demonstrates that our HGPO effectively enables the model to learn correct hybrid thinking behaviors, providing promising and practical pathway for building hybrid thinking systems. Next, we provide detailed investigation into each of the two training stages. Effect of HFT at Stage I. By comparing HFT with the Qwen2.5 and Deepseek-R1-Distill series, we observe that HFT significantly enhances both reasoning and general capabilities, while demonstrating robust cold-start performance by maintaining stability during hybrid reasoning without failure or collapse. These results validate the effectiveness of the proposed HFT ( 2.2) framework. Effect of HGPO at Stage II. By comparing LHRMs with HFT, we find that HGPO further substantially improves the models reasoning and general capabilities, while enabling more effective selection of thinking modes (over 31.4% and 110.2% Hacc improvements at 1.5B and 7B size, respectively). These findings demonstrate the effectiveness of HGPO. Furthermore, when comparing LHRMs with HFT-DPO and HFT-RFT, we observe that LHRMs achieves superior downstream performance on both reasoning and general tasks, along with higher accuracy in reasoning mode selection. This highlights the effectiveness of HGPO. Notably, we also observe that LHRMs exhibits stronger cross-domain generalization capabilities. Although RL training is conducted solely on math and general-domain data, LHRMs-1.5B achieves substantial improvements on code-related tasks, with gains of 11.1% on MBPP and 13.9% on MBPP+. In contrast, both HFT-DPO-1.5B and HFT-RFT-1.5B show experience performance drops (7.8% and 9.3%, respectively) on the same tasks. This indicates that LHRMs is able to generalize learned hybrid thinking patterns across domainsa property not observed in DPO or RFT. 3.3 Analysis Effect of Different Advantage Estimators. To investigate the impact of different advantage estimators on HGPO training, we replace GRPO in Eq. 10 with RLOO [ACG+24] and REIN7 (a) Ablation study on advantage estimators. (b) Ablation study on margin δ. Figure 3: Ablation study on the effects of advantage estimators and margin δ. δ in Eq. 9. FORCE++ [Hu25]. Implementation details can be found in Appendix B. As shown in Figure 3 (a), all estimators yield competitive performance, indicating that HGPO is robust to the choice of advantage estimator. Effect of Margin δ in Eq. 9. We investigate how varying the margin δ influences the models bias toward the two reasoning modes. As shown in Figure 3 (b), different values of δ result in distinct hybrid reasoning behaviors. Specifically, larger δ encourages the model to favor the No-Thinking mode. This suggests that δ can serve as control knob for tailoring hybrid reasoning behavior to specific application needs. For instance, higher δ may be preferred when real-time responsiveness is prioritized, whereas lower δ is more suitable when reasoning quality is the primary concern. Figure 4: Analysis of thinking ratio of LHRMs within single domain. Thinking Ratio Study within Domain. Figure 4 illustrates the distribution of thinking ratio across varying difficulty levels in the MATH500 benchmark for both HFT-7B and LHRMs-7B. We observe that HFT-7B maintains consistently high thinking ratio across all difficulty levels. In contrast, after applying the Stage II HGPO, LHRMs-7B exhibits decreasing thinking ratio as the difficulty level decreases while getting higher overall performance (see in Table 1). This trend suggests that our HGPO can effectively enables the model to adaptively perform hybrid thinking based on the input query. Specifically, the model learns to respond to simpler queries using No-Thinking strategy, thereby reducing reasoning cost and inference latency without sacrificing accuracy. For more challenging queries, the model consistently engages in full thinking, achieving higher reasoning accuracy where it matters most. Figure 5: Analysis of thinking ratio of LHRMs across different domains. Specifically, the model learns to respond to simpler queries using No-Thinking strategy, thereby reducing reasoning cost and inference latency without sacrificing accuracy. For more challenging queries, the model consistently engages in full thinking, achieving higher reasoning accuracy where it matters most. Thinking Ratio Study across Domain. We investigate how HGPO affects model performance on unseen domains not encountered during the RL phase. Specifically, we conduct RL using data from the math and general domains, and then evaluate the resulting models on the code domain. As shown in Figure 5, we observe that the model attained from HFT exhibits relatively low think ratio in the code domain. However, as RL progressesdespite being conducted on unrelated domains such as maththe think ratio in the code domain gradually increases, accompanied by improved performance. This suggests that our HGPO is capable of transferring hybrid thinking patterns learned in specific domain (e.g., math) to other domains (e.g., code), demonstrating strong generalization and transferability. Thinking Ratio Study across Model Size. We analyze how model size influences hybrid thinking behavior during the RL phase. In Figure 6, we present the think ratio of the 1.5B and 7B models 8 Question: If = 9a + 2b ab + 5, what is the value of 5 1? DeepSeek-R1-Distill-Qwen-7B: <think> Okay, lets tackle this problem. So, we have this operation defined as star equals 9a plus 2b minus ab plus 5. And we need to find the value of 5 star 1. Hmm, let me make sure understand the question... (2941 characters) </think> To solve 5 1 using the defined operation = 9a + 2b ab + 5: 1. Substitute = 5 and = 1 into the expression: 5 1 = 9(5) + 2(1) (5)(1) + 5 2. Calculate each term: - 9 5 = 45 - 2 1 = 2 - 5 1 = 5 3. Substitute these values back into the expression: 45 + 2 5 + 5 4. Perform the arithmetic operations step-by-step: - 45 + 2 = 47 - 47 5 = 42 - 42 + 5 = 47 Thus, the value of 5 1 is 47 LHRMs-7B: <no_think> To solve the problem, we need to substitute = 5 and = 1 into the given function = 9a + 2b ab + 5. Step-by-step solution: 1. Start with the given function: = 9a + 2b ab + 5 2. Substitute = 5 and = 1 into the function: 5 1 = 9(5) + 2(1) 5(1) + 5 3. Perform the multiplication operations: 5 1 = 45 + 2 5 + 5 4. Simplify the expression by performing the addition and subtraction operations: 5 1 = 45 + 2 5 + 5 = 47 Therefore, the final answer is: 47 . </no_think> Figure 7: Example outputs from DeepSeek-R1-Distill-Qwen-7B and our LHRMs-7B model. For easy questions, LHRMs selects the No-Thinking mode. on the MATH500 benchmark. We observe that, as RL training progresses, the think ratio of the 7B model gradually decreases, while that of the 1.5B model increases. This indicates that hybrid thinking behavior during RL is strongly correlated with model capacity and knowledge storage. Specifically, larger models with stronger knowledge reserves tend to shift towards using No-Thinking strategies on solvable problems to reduce reasoning overhead and improve response efficiency. In contrast, smaller models are more inclined to invoke Thinking more frequently, as way to compensate for their limited capabilities through enhanced reasoning. Case Analysis. We present representative model responses in Figure 1 and 7. We observe that for relatively simple queriessuch as everyday questions and basic math questions, LHRMs tends to autonomously select the No-Thinking mode, achieving fast and accurate responses. In contrast, for more complex problems that require deeper reasoning, LHRMs adaptively switches to the Thinking mode to produce more precise solutions. More example responses can be found in Appendix E."
        },
        {
            "title": "4 Related Works",
            "content": "Figure 6: Ablation study on model size. Test-Time Scaling (TTS). Test-time scaling [Fac25, GYZ+25, MYS+25, IBR+23] has been validated as promising approach to enhance model performance beyond scaling model size [SLXK24]. There are two primary approaches to implementing TTS [MYS+25]: Parallel and Sequential. The former is achieved by sampling multiple solutions and select the one by selector (e.g., an outcome reward model), like Best-of-N [BJE+24, IBR+23, Lev24], and Monte-Carlo Tree Search (MCTS) [LCP+24, ZCS+23, ZYSR+24, CFWS23]. The latter aims to achieve TTS by enabling the model to generate longer outputs, i.e., Chain-of-Thoughts (CoT) within single sampling pass by prompting, finetuning or reinforcement learning. Beyond the field of NLP, TTS has also been shown to effectively improve the test-time performance of trained models in other domains, like image generation, video generations [LWC+25] and multi-modality learning [FGL+25, HJZ+25, CLZ+25]. Large Reasoning Models. Recent advances in Large Reasoning Models (LRMs), such as DeepSeekR1 [DA25], OpenAI o1/o3 series [Ope25], and others [Tea24, MYS+25, Ant25, Goo25], have led to growing focus on Large Reasoning Models (LRMs). Compared to general LLMs, LRMs extend the capabilities of LLMs by generating long chains of reasoning steps with reflection before outputting the final answers. LRMs are typically developed by applying reinforcement learning such as GRPO [GYZ+25] and REINFORCE++ [Hu25], or distilled from stronger models [Tea25, MJB+25, Fac25, ZWP+25, YHX+25]."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we focus on building large language model that effectively balances reasoning capabilities and general-purpose performance. To this end, we propose novel evaluation metric, Hacc, designed to consistently assess models ability to perform hybrid thinking across diverse tasks. We further introduce two-stage training pipeline consisting of HFT and HGPO. Experimental results demonstrate that this pipeline significantly improves performance on both reasoning-centric and general downstream tasks. Moreover, it enhances the models hybrid thinking capabilities, leading to better user experience by reducing unnecessary reasoning on simple queries commonly observed in LRMs, and mitigating the insufficient reasoning capabilities in conventional LLMs."
        },
        {
            "title": "References",
            "content": "[ACG+24] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human Feedback in LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1224812267. Association for Computational Linguistics, 2024. [AM24] AI-MO. Aime 2024, 2024. [Ant25] Anthropic. Claude https://www.anthropic.com/news/claude-3-7-sonnet, 2025. sonnet 3. and claude code. [AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [BJE+24] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. [CFWS23] Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. Kcts: Knowledgeconstrained tree search decoding with token-level hallucination detection, 2023. [CLZ+25] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github. com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [CXL+24] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [DA25] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [DGLH24] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Lengthcontrolled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [Fac25] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [FGL+25] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [Goo25] Google. Gemini 2.5 flash. https://deepmind.google/technologies/gemini/ flash/, 2025. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [HJZ+25] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [HLB+24] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. 11 [Hu25] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [IBR+23] Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. Rewarding chatbots for real-world engagement with millions of users, 2023. [JGB+16] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016. [JHG+24] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [KKVR+23] Andreas Köpf, Yannic Kilcher, Dimitri Von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36:4766947681, 2023. [LCF+24] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. [LCP+24] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2024. [Lev24] Noam Levi. simple model of inference scaling laws, 2024. [LKB+23] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [LMP+24] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tülu 3: Pushing frontiers in open language model post-training, 2024. [LTW+25] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [LWC+25] Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1: Test-time scaling for video generation. arXiv preprint arXiv:2503.18942, 2025. [LXW+24] Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. arXiv preprint arXiv:2408.06450, 2024. [LXWZ23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:21558 21572, 2023. [MHS+25] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. [MJB+25] Justus Mattern, Sami Jaghouar, Manveer Basra, Jannik Straube, Matthew Di Ferrante, Felix Gabriel, Jack Min Ong, Vincent Weisser, and Johannes Hagemann. Synthetic-1: Two million collaboratively generated reasoning traces from deepseek-r1, 2025. [MLL+25] Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Christos Thrampoulidis, Leonid Sigal, and Renjie Liao. Leveraging online olympiad-level math problems for llms training and contamination-resistant evaluation. arXiv preprint arXiv:2501.14275, 2025. [MYS+25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [Ope25] OpenAI. Openai gpt-4.5 system card. OpenAI Publication, 2025. [PLK+25] Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/ open-r1/codeforces, 2025. [RSM+23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [SCW+25] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [SLXK24] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [SZY+24] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [Tea24] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [Tea25] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [XCM+25] Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492, 2025. [XHZ+25] Xu, Hao, Zong, Wang, Zhang, Wang, Lan, Gong, Ouyang, Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arxiv. 10.48550. arXiv preprint arXiv.2501.09686, 2025. 13 [XLY+25] Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding, 2025. [YHX+25] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [YYL+23] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. [YYZ+24] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [YZH+24] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [ZCS+23] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation, 2023. [ZRH+24] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. In The Twelfth International Wildchat: 1m chatGPT interaction logs in the wild. Conference on Learning Representations, 2024. [ZWP+25] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training, 2025. [ZYSR+24] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2024. [ZZZ+24] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Large Hybrid-Reasoning Models"
        },
        {
            "title": "2.1 Problem Formulation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.4 Evaluating Hybrid Thinking Capability . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Experimental Results"
        },
        {
            "title": "3.2 Main Results",
            "content": "3.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Related Works 5 Conclusion Implement Details for DPO and RFT Implement Details for RLOO and Reinforce++ Dataset Statistics Evaluation Settings Example Outputs 1 3 3 4 5 6 6 7 9 10 16 16 17"
        },
        {
            "title": "A Implement Details for DPO and RFT",
            "content": "In this section, we detail the constrution pipeline of raining data for both DPO and RFT used in 3. To construct datasets for DPO and RFT, we adopt the same set of queries used in HGPO for offline sampling. Specifically, for each query q, we sample two responses using each of the two thinking modes {, }, resulting in set of four responses per query as Then the reward function Rϕ O(q) = (cid:8)o (cid:9)2 i=1 (cid:8)o 4 in Eq. 15 is applied to score these responses as: i=1 , = (cid:8)r(o = (cid:8)r(o (cid:9)2 i=1 . )(cid:9)2 )(cid:9)2 i=1 . i"
        },
        {
            "title": "Given the average reward for each mode computed as",
            "content": "R = 1 2 2 (cid:88) i=1 r(o ), = 1 2 2 (cid:88) i=1 r(o ), (14) (15) (16) DPO. For DPO, the training dataset which contains win and lose sample is constructed as following: (cid:40) DDPO = (q, ow, ol) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ow = arg max }2 o{omw i= (r (o)) , ol = arg min ml }2 o{o i=1 (r (o)) , (17) (cid:41) while mw = arg max m{,} Rm, ml = arg min m{,} Rm. (18) After getting DDPO, we optimize the model πθ initialized from πθHFT by using training objective: max πθ E(x,ow,ol)DDPO (cid:20) (cid:18) log σ β log πθ(ow x) πθHFT (ow x) β log πθ(ol x) πθHFT (ol x) (cid:19)(cid:21) . RFT. For RFT, the training dataset is constructed as following: (cid:26) DRFT = (q, o) (cid:12) (cid:12) (cid:12) (cid:12) = arg max (cid:0)R R(cid:1) (cid:27) . oO The training objectives for RFT are presented as: LRFT(θ) = E[(x,o)DRFT] (cid:88) log πθ(ot x, o1:t1) , (19) (20) (21) For implementations, we use LLaMA-Factory [ZZZ+24] 5 as the codebase for both DPO and RFT. To ensure fair comparison, we maintain the same learning rate, batch size, and total number of training samples as used in Stage II of HGPO. t=1 Implement Details for RLOO and Reinforce++ First of all, We compare different advantage estimators including REINFORCE++ [Hu25], GRPO [SWZ+24], and RLOO [ACG+24], toggling the existence of our HGPO. To make different algorithms compatible with the compound of intra-group rewards and inter-group rewards, we accordingly make adaptions similar to Eq. 10. For Reinforce++, we have At = oi (cid:88) γst rintra (oi) + s=t (cid:124) (cid:123)(cid:122) REINFORCE++ for intra-group advantage Aintra (cid:125) 1{ot (cid:124) Φ} α [rinter (oi)] (cid:125) (cid:123)(cid:122) REINFORCE++ for inter-group advantage Ainter , (22) 4Same with 2.3, for queries with definitive answers, we use rule-based rewards [SWZ+24, GYZ+25]; otherwise, trained parametric reward model [LMP+24] is applied. 5https://github.com/hiyouga/LLaMA-Factory.git 16 Here is hyperparameter representing the decay factor, which is set to 0.99 in our experiments. For RLOO, we have At = rintra (oi) 1 1 rintra (oj) + 1{ot Φ} α rinter(oi) (cid:88) j=i 1 1 (cid:88) j=i (rinter (oj)) . (23) (cid:124) (cid:123)(cid:122) RLOO for intra-group advantage Aintra (cid:125) (cid:124) (cid:123)(cid:122) RLOO for inter-group advantage Ainter (cid:125) From 3.3, We show that HGPO is general plug-in for almost any advantage estimators., which largely extends the use cases of HGPO. We implemnent both RLOO and Reinforce++ on VeRL [SZY+24] 6."
        },
        {
            "title": "C Dataset Statistics",
            "content": "Stage I. Figure 8 shows the token length distributions for the Thinking and No-Thinking datasets we construct in Stgae I. The Thinking data has an average length of 575 tokens, with the 25th and 75th percentiles at 362 and 672 tokens, respectively, and maximum length of 9,148 tokens. In contrast, the No-Thinking data exhibits significantly higher average length of 4,897 tokens, with the 25th and 75th percentiles at 1,719 and 6,738 tokens, and maximum of 23,997 tokens. We present the scores and statistics of our constructed dataset for HFT in Table 2. The dataset covers diverse range of domains, primarily including reasoning-intensive tasks such as mathematics and code, as well as general-purpose question answering. Stage II. We report the details of training data used in Stage II at Table 3. Table 2: Data distribution and source of Stage I. Category General MATH Coding Others Total Source WildChat-1M [ZRH+24] OSSAT2 [KKVR+23] SYNTHETIC-1 [MJB+25] OpenR1-Math [Fac25] OpenThought [Tea25] AoPS [MLL+25] AIME SYNTHETIC-1 [MJB+25] OpenR1-Codeforces [PLK+25] OpenThought [Tea25] KodCode [XLY+25] SYNTHETIC-1 [MJB+25] Data Size Total Link 649,569 25,339 343,988 93,533 55,566 137,497 741 107,543 8,926 19,447 245,929 674,908 https://huggingface.co/datasets/allenai/WildChat-1M https://huggingface.co/datasets/OpenAssistant/oasst 631,325 https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-1 https://huggingface.co/datasets/open-r1/OpenR1-Math-220k https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k https://huggingface.co/datasets/di-zhang-fdu/AOPS https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024 381,845 https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-1 https://huggingface.co/datasets/open-r1/codeforces https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k https://huggingface.co/datasets/KodCode/KodCode-V1 6,508 6, https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-1 1,694,586 1,694,586 Figure 8: Token length distributions of Thinking and No-Thinking data in Stage I. General MATH Table 3: Data distribution and source of Stage II. Note that our experiments are conducted on subsets of the two datasets rather than their full versions. Category Data Size Used Source Tülu 3 [LMP+24] Tülu 3 preference dataset DeepScaler [LTW+25] DeepScaleR-Preview-Dataset 337,000 50,000 40,300 40,"
        },
        {
            "title": "D Evaluation Settings",
            "content": "In this section, we introduce the details for downstream tasks evaluation in 3. During inference, we set the temperature to 0.6, the top-p value to 0.95, and the maximum generation length to 35,000 tokens. 6https://github.com/volcengine/verl.git 17 For MATH500, AIME24, GPQA Diamond, and LiveCodeBench, we adopt the OpenR1 [Fac25] evaluation framework. For HumanEval+ and MBPP+, we use the EvalPlus [LXWZ23, LXW+24] framework with Chain-of-Thought (CoT) [WWS+22] prompting. We report pass@1 for all reasoning related benchmarks. For AlpacaEval 2.0 and Arena-Hard, we use GPT-4-Turbo and GPT-4-0314 as the respective baseline models, with GPT-4o serving as the judge in both evaluations. To reduce length-related bias, we feed only the final generated summary into the evaluation in think mode. We provide summarization and links of evaluation tools we used in experiments in Table 4. Table 4: Summarization of evaluation tools we used in experiments. Task MATH500 [LKB+23], AIME24 [AM24], Livecodebench [JHG+24] Olympaid Bench, AMC23 MBPP [AON+21, LXW+24], MBPP+ [AON+21, LXW+24] AlpacaEval [DGLH24] ArenaHard v2.0 [LCF+24] Tool Name Source open-r1 [Fac25] math-evaluation-harness evalplus [LXWZ23, LXW+24] aplaca_eval arena-hard-auto https://github.com/huggingface/open-r1 https://github.com/ZubinGou/math-evaluation-harness https://github.com/evalplus/evalplus https://github.com/tatsu-lab/alpaca_eval https://github.com/lmarena/arena-hard-auto"
        },
        {
            "title": "E Example Outputs",
            "content": "Question: What is the value of 5132 4872? Lets think step by step and output the final answer within boxed{}. Ground Truth: 26000 DeepSeek-R1-Distill-Qwen-7B: <think> Okay, so need to calculate 513 squared minus 487 squared. Hmm, let me think about how to approach this. remember that theres formula for the difference of squares, which is a² - b² = (a - b)(a + b). Maybe can use that here instead of calculating each square separately, which might be time-consuming. ... (3837 characters) </think> To solve 5132 4872, we use the difference of squares formula a2 b2 = (a b)(a + b). 1. Identify = 513 and = 487. 2. Calculate b: 3. Calculate + b: 4. Multiply the results from steps 2 and 3: 513 487 = 26 513 + 487 = 1000 26 1000 = Verification by direct calculation: - 5132 = 263169 - 4872 = 237169 - Subtracting these values: 263169 237169 = 26000 Both methods confirm the result. Therefore, the final answer is 26000 LHRMs-7B: <no_think> To solve 5132 4872, we can use the difference of squares formula, which states that a2 b2 = (a+b)(ab). Here, = 513 and = 487. Step 1: Calculate + b: Step 2: Calculate b: Step 3: Use the difference of squares formula: 513 + 487 = 1000 513 487 = Step 4: Perform the multiplication: 5132 4872 = (513 + 487)(513 487) 5132 4872 = 1000 26 1000 26 = 26000 Therefore, the final answer is: 26000 </no_think> Figure 9: LHRMs-7B uses the No-Thinking mode to solve easy math problems. 18 Question: You are given an array of integers nums of length and positive integer k. The power of an array is defined as: its maximum element if all of its elements are consecutive and sorted in ascending order, and -1 otherwise. You need to find the power of all subarrays of nums of size k. Return an integer array results of size - + 1, where results[i] is the power of nums[i..(i + - 1)]. HFT-7B: <no_think> To solve this problem, we need to evaluate the power of all subarrays of given size from the input array nums. The power of subarray is defined as its maximum element if all elements are consecutive and sorted in ascending order; otherwise, it is -1. Heres step-by-step plan to solve the problem: 1. Iterate through each subarray of size k: We will slide window of size through the array nums and examine each subarray. 2. Check if the subarray is sorted and consists of consecutive integers: First, check if the subarray is sorted in ascending order. Then, check if the elements of the subarray are consecutive integers. 3. Calculate the power of each subarray: If the subarray meets the criteria, the power is the maximum element; otherwise, it is -1. Here is the Python code that implements this logic: class Solution: def resultsArray(self, nums: List[int], k: int) -> List[int]: = len(nums) result = [] for in range(n - + 1): subarray = nums[i:i + k] sorted_subarray = sorted(subarray) max_element = subarray[-1] if subarray == sorted_subarray and max_element - subarray[0] == - 1: result.append(max_element) else: result.append(-1) return result </no_think> LHRMs-7B: <think> Okay, need to solve this problem where have to find the power of all subarrays of size in the given array. The power is defined as the maximum element if all elements are consecutive and sorted in ascending order. Otherwise, its -1. Hmm, lets break down the problem. So for each subarray of length k, have to check two things: whether the elements are consecutive and whether they are sorted in ascending order. ... (2645 characters) </think> To solve this problem, we need to determine the power of all subarrays of given size from an array of integers. The power of subarray is defined as the maximum element if the subarray is both sorted in ascending order and consists of consecutive integers. Otherwise, the power is -1. Approach 1. Iterate through each possible subarray: For each starting index from 0 to - k, extract the subarray of length k. 2. Check if the subarray is sorted: Verify if each element in the subarray is less than the next element. If not, the power is -1. 3. Check if the subarray consists of consecutive integers: Ensure each element is exactly one more than the previous element. If not, the power is -1. 4. Determine the power: If both conditions are met, the power is the last element of the subarray (since it is the maximum in sorted consecutive sequence). Otherwise, the power is -1. class Solution: def resultsArray(self, nums: List[int], k: int) -> List[int]: result = [] = len(nums) for in range(n - + 1): sub = nums[i:i+k] # Check if the subarray is sorted in ascending order is_sorted = True for in range(len(sub) - 1): if sub[j] >= sub[j+1]: is_sorted = False break if not is_sorted: result.append(-1) continue # Check if the subarray consists of consecutive integers is_consecutive = True for in range(len(sub) - 1): if sub[j+1] != sub[j] + 1: is_consecutive = False break if is_consecutive: result.append(sub[-1]) else: result.append(-1) return result Figure 10: LHRMs-7B learned to select the Thinking mode to solve hard code problems."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Peking University"
    ]
}