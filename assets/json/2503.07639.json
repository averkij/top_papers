{
    "paper_title": "Mixture of Experts Made Intrinsically Interpretable",
    "authors": [
        "Xingyi Yang",
        "Constantin Venhoff",
        "Ashkan Khakzar",
        "Christian Schroeder de Witt",
        "Puneet K. Dokania",
        "Adel Bibi",
        "Philip Torr"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neurons in large language models often exhibit \\emph{polysemanticity}, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present \\textbf{MoE-X}, a Mixture-of-Experts (MoE) language model designed to be \\emph{intrinsically} interpretable. Our approach is motivated by the observation that, in language models, wider networks with sparse activations are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches."
        },
        {
            "title": "Start",
            "content": "Xingyi Yang 1 2 Constantin Venhoff 1 Ashkan Khakzar 1 Christian Schroeder de Witt 1 Puneet K. Dokania 1 Adel Bibi 1 Philip Torr 1 5 2 0 2 5 ] . [ 1 9 3 6 7 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with sparse activations are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer scalable alternative by activating only subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches. 1. Introduction Transformer-based large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Waswani et al., 2017) have achieved remarkable progress. However, their internal workings remain poorly understood. This lack of understanding often leads to unexpected and potentially harmful 1University of Oxford 2National University of Singapore. Preprint Figure 1. MoE-X introduces sparse and wide network architecture designed for interpretability. Compared to dense MLPs, it incorporates both sparsity and wider structure. Unlike traditional MoE models, it enforces sparsity within each expert and routes tokens to the sparsest experts. behaviors (Hendrycks et al., 2023; Ngo et al., 2022), posing risks in their deployment. To address this, mechanistic interpretability (Elhage et al., 2022c) seeks to uncover how these models process information and reduce potential risks. central obstacle to mechanistic interpretability is polysemanticity, where individual neurons encode multiple, unrelated concepts (Olah et al., 2020). Specifically, we refer to the hidden neurons from the multi-layer perceptron (MLPs) in Transformers. Such polysemantic neurons lack clear, singular roles, making it difficult to identify disentangled features or factors in neural networks. common strategy to address this issue is to decompose entangled neuron activity into interpretable vectors using posthoc methods like Sparse Auto-Encoders (SAEs) (Huben et al., 2023). However, these approaches are computationally expensive (Gao et al., 2024; Lieberum et al., 2024), require additional analysis after training, and often do not explain all the features of the model (Menon et al., 2024). Instead, we advocate for designing interpretability directly into the model architecture in which the resultant discourages polysemanticity during training. While some works explore architectural changes for interpretability, they often focus on toy-scale tasks (Pearce et al., 2024; Agarwal et al., 2021; Sharkey, 2023; Jermyn et al., 2022) or compromise Mixture of Experts Made Intrinsically Interpretable performance (Elhage et al., 2022a). To achieve built-in interpretability, as shown in Figure 1, we identify two key factors that influence it, (1) increasing the size of the MLP layers, i.e., number of hidden activations, and (2) increasing the sparsity of these activations. That is to say, making the MLP layers in transformer wider and sparser should encourage more disentangled internal representations. To test this beyond toy experiments (Jermyn et al., 2022; Elhage et al., 2022b), we conduct experiments on GPT-2-like (Radford et al., 2019) models on chess gameplay data (Karvonen et al., 2024). Chess provides an excellent natural ground truth for interpretability because the board state can be used as reference for understanding how each neuron represents and predicts chess moves. Our experiments show that sufficiently wide-and-sparse MLP in transformer indeed yields more interpretable neurons, leading to 25% increase in F1 score for chess move prediction, supporting our hypothesis. Motivated by these findings, we propose to leverage mixtureof-experts (MoE) architectures (Fedus et al., 2022; Shazeer et al., 2017) as natural fit for intrinsic interpretability. While different from standard MLP, MoE can be rewritten as wide and sparse MLP, whose neurons are each experts neuron weighted by the sparse gating scores. This structure allows MoE to increase the width while maintaining controlled sparsity, leading to inherently interpretable models. Yet, typical MoE models are not perfect for interpretability. First, each expert is still dense MLP, which can suffer from polysemanticity. Second, standard top-k routing (Fedus et al., 2022) primarily targets performance rather than expert sparsity. As result, the gating decisions made by the routing mechanism are often misaligned with the goals of interpretability. To bridge this gap and align MoE with interpretability, we propose MoE-X, which includes two key designs: ReLU Experts. We use ReLU activation within each expert. This simple yet effective modification promotes intrinsic activation sparsity in the expert (Awasthi et al., 2024), which in turn helps to disentangle the feature representations. Sparsity-Aware Routing. We introduce gating function that predicts which experts would produce the most sparse activations. To avoid expensive computations, we develop method to estimate each experts sparsity without explicitly computing all activations. This ensures that the sparsest experts are chosen during inference, promoting disentangled representations. Together, these modifications enable MoE-X to maintain competitive performance while providing more transparent, semantically meaningful internal representations. Our experiments on chess and language tasks confirm that MoE-X matches or exceeds the performance of dense transformers while eliminating the need for expensive post-hoc interpretability methods. In summary, our main contributions are: a) We systematically how architectural analyze choicesparticularly width and sparsityinfluence interpretability in transformer-based language models. b) We introduce MoE-X, redesigned MoE layer that functions as wide, sparse, and more interpretable MLP within large language models. c) We incorporate ReLU Experts and Sparsity-Aware Routing to tightly connect gating decisions with the expected sparsity of activations. d) Our experiments on chess tasks demonstrate that MoE-X models achieve strong performance while offering clear and interpretable representations. 2. Related Work Mechanistic Interpretability & Polysemantics. Mechanistic interpretability (Olah, 2022) aims to understand deep neural networks by analyzing individual units (e.g., neurons) reverse-engineering their computations (Elhage et al., 2021). Although this approach provides insights into large language models (LLMs) (Zhong et al., 2024; Wang et al., 2022), many neurons remain polysemantic, activating for multiple concepts (Elhage et al., 2022b), making interpretation difficult. Post-hoc methods like Sparse Auto-Encoders (SAEs)(Gao et al., 2024; Huben et al., 2023) attempt to address this but are computationally expensive and incomplete (Menon et al., 2024). In this paper, we introduce MoE architecture to reduce polysemanticity. This approach promotes more interpretable internal representations without the need for extensive post-hoc methods. Intrinsic Interpretability. Intrinsic interpretability aims to design neural networks that are inherently easier to understand without sacrificing performance. These methods enforce sparsity, modularity, and monosemanticity through architectural and training constraints. For example, (Liu et al., 2023a;b) use brain-inspired modular training to enhance anatomical modularity in RNNs, while (Jermyn et al., 2022; Elhage et al., 2022a) explore structural choices for monosemanticity, and (Sharkey, 2023) employs bilinear layers for interpretability. We take different approach by leveraging MoE for intrinsic interpretability. Mixture of Experts. Mixture-of-Experts (MoE) models dynamically route input to specialized experts to reduce computation (Jacobs et al., 1991; Shazeer et al., 2017). 2 Mixture of Experts Made Intrinsically Interpretable Recent work focus on replacing MLP layers in LLMs with MoE layers, achieving better performance at lower cost (Jiang et al., 2024; Fedus et al., 2022). major challenge in MoE is designing the routing function (Zhou et al., 2022), which typically requires an auxiliary loss to balance expert usage. Regarding interpretability, previous studies observed that MoE models tend to exhibit increased monosemanticity (Park et al., 2024; Oldfield et al., 2024). but these studies offer limited explanations for why this occurs. In this work, we clarify its underlying mechanisms and propose redesigned routing function that prioritizes experts with more interpretable activations, rather than focusing solely on performance. 3. Preliminary Study: What Architectural Choices Enhance Interpretability? To design more interpretable architectures, it is essential to identify what is the key influencing factors. In this section, we conduct series of toy experiments by training LLMs on chess gameplay data and evaluating their interpretability. Through extensive ablations of various design choices, we identify two key factors that significantly enhance inherent interpretability in language models: a) MLP Hidden Size: Larger hidden states result in better interpretability. b) Sparsity of Hidden Activations: Lower numbers of nonzero neurons lead to more interpretable representations. In the next section, we will use these findings to design intrinsic interpretable architectures. 3.1. Measuring interpretability on Chess Games Designing and evaluating the interpretability of language models is challenging due to the absence of universal metric. In our experiments, we use chess game dataset to assess interpretability (McGrath et al., 2022; Toshniwal et al., 2022; He et al., 2024). Specifically, we measure how well the models internal activations align with semantically meaningful chess board state properties (BSP), using the metrics described in (Karvonen et al., 2024). Dataset and Metrics. As shown in Figure 2, we trained LLMs on chess Portable Game Notation (PGN), treating it as language, and analyzed the interpretability of MLP hidden activations. Specifically, we trained an 8-layer GPT2like model (Radford et al., 2019). Each character in the PGN is treated as token, and we conducted next-token prediction training. Figure 2. Illustration of using chess game to evaluate the LLMs interpretability. Figure 3. Comparision BSP Coverage score v.s. the Model size. for BSPs. More information can be found in Section 5.1 and Appendix. We report the core observations on model hidden size and activation sparsity. Study I: Model Hidden Size. In this study, we train models with different MLP hidden widths D. As shown in Figure 2, the width is determined by two factors: the input dimension of the MLP as d, and the hidden size multiplier α. Together, the hidden size is = αd. We vary either or α. With α = 4 fixed, we test {256, 512, 1024, 2048}. With = 512 fixed, we test α {2, 4, 8, 16}. Figure 3 compares BSP coverage for fixed and fixed α, with model size on the x-axis. As baseline, we plot the SAE score with dictionary size of 4096, trained on post-res for model (α = 4, = 512). This results in nearly 3 increase in model parameters. We make three key observations. First, increasing both and α improves the interpretability, as indicated by higher coverage score. Second, scaling α is better than scaling d. This implies that, instead of increasing the overall model size, we can efficiently scale the hidden size multiplier α for better interpretability. Third, by increasing the model size, we can eventually outperform the SAE features in terms of interpretability. For example, model with (α = 16, = 512) achieves score of 0.53, compared to 0.45 for (α = 4, = 512)+SAE, with similar overall parameter. After training, we calculated the BSP Coverage Score on layer-6 MLP hidden activation. This score measures the average best F1 score when each feature is used as classifier Study II: Activation Sparsity. In this study, we explore how the activation sparsity affect the model interpretability. we explore different techniques to control the sparsity Mixture of Experts Made Intrinsically Interpretable of this, we propose new MoE layer designed to further enhance interpretability. 4.1. Preliminary: Sparse Mixture of Experts SMoE improves efficiency by activating only subset of computations per input. Unlike traditional models that use single MLP for each LLM layer, SMoE employs multiple parallel MLPs, referred to as experts. Each token is routed to subset of experts, and their predictions are combined using weights determined dynamically based on the input. Formally, SMoE comprises two key components: experts and gating network. Experts: Let Rd be the input vector of one token. SMoE layer consists of experts, each represented as fj(x) for {1, 2, . . . , }. Typically, each expert is small MLP fj(x; θj) = W(j) decz(j), z(j) = σ(W(j) encx) = σ(h(j)), dec RdD and W(j) where W(j) enc RDd are weight. h(j) and z(j) are pre-activation and post-activation vectors. Here, represents the hidden dimension of each expert. Gating Network: The gating network g(x; ϕ) generates set of weights = [ω1, ω2, . . . , ωM ] RM , with each ωj indicating the contribution of expert to the output. These weights are computed using learnable matrix Wg RM as follows: = g(x; ϕ) = Softmax(TopK(Wgx)), (1) Only top-k values are retained and normalized using softmax (Shazeer et al., 2017), while the rest are set to zero. The final output of the MoE model, ˆy, is weighted sum of the expert outputs ˆy = (cid:80)M j=1 ωjfj(x; θj). Since most ωj are zero, this model is referred to as Sparse MoE. 4.2. SMoE is Natural Fit for Interpretability SMoE naturally aligns with our identified interpretable architecture, as it is both wide and sparsely activated. To see this, consider mega-decoder by concatenating all expert decoder matrices Wdec = concat([W(1) dec ]) RdM D. We can also define new hidden code as = concat([ω1z(1), . . . , ωM z(M )]) RM D. Here, each ωjz(j) is the scaled activation from j-th expert. Notably, decoding through Wdec exactly the same as SMoE output (Liu et al., 2023c) dec , . . . , W(M ) ˆy = (cid:88) j=1 ωjfj(x; θj) = (cid:88) i=1 W(j) dec (cid:16) ωjz(j)(cid:17) = Wdecz, (2) Figure 4. Comparing BSP Coverage score v.s. L-0 norm of the hidden. of the hidden activations in the MLP. Specifically, we either use ReLU activation instead of GELU, or apply Top-k activation (Makhzani & Frey, 2013; Gao et al., 2024)1. We visualize the results in Figure 4. We find that, for various model sizes, imposing sparsity typically improves coverage scores (difference between the dashed blue and green line). Moreover, these gains become more pronounced in larger models. However, the most extreme sparsity does not always yield the best interpretability, and identifying the optimal sparsity level remains difficult. Analysis and Understanding. We hypothize wide and sparse neural networks are more interpretable as they minimize feature superposition (Elhage et al., 2022b). Width provides sufficient capacity for the model to assign distinct neurons to specific features. Sparse activations ensure that only small subset of relevent neurons is active for given input, which reduces interference between features. Although similar architectural properties have been explored in non-language tasks and non-transformer architecture (Jermyn et al., 2022; Elhage et al., 2022b), or SAE features (Gao et al., 2024), evaluating those properties on language model pretraining was previously unexplored. 4. Mixture of Experts for Intrinsic"
        },
        {
            "title": "Interpretability",
            "content": "Building on the observations in Section 3, we aim to design architectures that are both wide and sparsely activated, ensuring better interpretability. Sparse Mixture of Experts (SMoE) naturally aligns with these properties, making it an excellent candidate for such design. In this section, we first explain how SMoE works and demonstrate how it aligns with wide-and-sparse principles. Building on top 1We also experimented with ℓ1 regularization but found it ineffective for reducing the ℓ0 norm and challenging to tune, so we excluded it. 4 Mixture of Experts Made Intrinsically Interpretable In other words, SMoE acts like larger MLP whose hidden layer is each experts activations, scaled by its gating score. Because only top-k non-zero ωj are retrained, is structured sparse. When ωj = 0, all elements in ωjz(j) are zero. Consequently, SMoE is wide, as its hidden dimension is D, but also sparse, as activations are restricted to subset of experts. In this way, SMoE satisfies the criteria of wide and sparsely activated MLP that supports interpretability. SMoE is not perfect for interpretability. Despite its inherent sparsity and modularity, two key issues remain. First, activations in each expert are still dense, which can still lead to polysemantic features. Second, the gating function is trained purely for performance, so its values may not reflect interpretable expert properties. We address these issues in the following sections. 4.3. Designing SMoE for Greater Interpretability To address the issues discussed above, we redesign both the expert architecture and the routing function to enforce neuron-level sparsity within each expert. 4.3.1. RELU EXPERT FOR ACTIVATION SPARSITY To address the first challenge, we adopt the ReLU function as the non-linear activation σ() for each expert MLP. Empirically, we observe that experts trained with ReLU exhibit high degree of activation sparsity, which helps disentangle features while maintaining strong performance. While intrinsic activation sparsity has been studied in the context of efficiency (Zhang et al., 2024; Mirzadeh et al., 2023; Awasthi et al., 2024), its role in enhancing interpretability is less explored. 4.3.2. SPARSITY-AWARE ROUTING To tackle the second issue, we aim to route each input token to the expert fj, which produces sparsest activation (i.e., fewest non-zero entries). Formally, if z(j) = ReLU(h(j)), then the sparsity of z(j) can be discribed using its ℓ0-norm z(j)0 = I(h(j) 0), (cid:88) (3) where I() stands for the indicator function. The simplest approach to do gating is to evaluate z(j) for all experts and select the one with the fewest positive elements. However, this contradicts the SMoE principle of limiting computation to subset of experts. Instead, we use cheap proxy for z(j)0 based on probabilistic assumptions about the encoder weights W(j) enc. Approximate Sparsity via Gaussian Assumptions. For expert j, assume each column of the encoder weight matrix W(j) enc is drawn i.i.d. from the same Gaussian distribution, )2). Then each component {wm,i}D m=1 (µ(j) , (σ(j) h(j) of the pre-activation vector h(j) is sum of Gaussian random variables and thus also follows Gaussian distribution: h(j) (µ(j) , (σ(j) )2) = ( (cid:88) µ(j) xi, (cid:88) (σ(j) )2x2 ), (4) Thus, the probability that h(j) is positive is P (h(j) > 0) = 1 Φ( µ(j) σ(j) ) = Φ( µ(j) σ(j) ), (5) where Φ(h) is the CDF of the normal distribution. because each elements in h(j) follows the same distribution, the ℓ0-norm could be estimated as the expected value of z(j)0 (cid:88) E[I(h(j) 0)] = DΦ( µ(j) σ(j) ), (6) Hence, we pick expert(s) that minimize z(j)0, i.e., those with the lowest probability of being positive. Router Implementation. In practice, we estimate µ(j) σ(j) efficiently using column-wise statistics of W(j) For expert and enc and x. (cid:88) µ(j) = 1 W(j) enc [m, :]; σ(j) = m=1 = µ(j) µ(j) x; σ(j) (cid:88) m=1 1 = σ(j) (x2), (cid:16) W(j) enc [m, :] µ(j) (cid:17) , Using the CDF approximation Φ(h) 1 2)), we compute the gating weights via TopK+Softmax operation 2 (1 + erf(h/ = g(x; ϕ) = Softmax (cid:16) TopK(cid:0) erf( µ(j) 2σ(j) )(cid:1)(cid:17) , (7) Here erf() is the error function. The resulting gating scores select the experts and provide cheap estimate of each experts activation sparsity. Routing Regularizes Sparsity. Since we do not detach the gradient of W(j) enc, the gating function implicitly regularizes the experts to produce sparser activations. Specifically, when the model learns to favor an expert by increasing its gating score ωj, it simultaneously updates W(j) enc, to encourage activation z(j) to be sparser . Computational Complexity. Given input RN d, standard top-k gating has complexity O(N d). Naively computing all activations and their ℓ0-norm costs O(N Dd). In contrast, our Sparsity-Aware Routing only requires O(M Dd) for column-wise statistics plus and O(N d) for inner products, resulting in total complexity of O((N + D)M d). This design scales efficiently with large numbers of experts and high-dimensional inputs. 5 Mixture of Experts Made Intrinsically Interpretable Table 1. Comparison with baseline method by keeping model activated parameters the same. Model GELU (GPT-2) ReLU GEGLU SoLU Monet-HD Monet-VD PEER Switch MoE-X Val Loss Coverage Reconstruction 0.356 0.213 Activation Function 0.215 0.209 0.216 0.312 0.255 0.306 Mixture-of-Experts 0.312 0.283 0.323 0.424 0. 0.210 0.212 0.214 0.212 0.211 0.608 0.581 0.394 0.343 0.528 0.482 0.426 0.734 0.840 Figure 5. BSP Coverage and Reconstruction score of different model sizes. 5. Experiments In this section, we conduct experiments on the chess and language datasets to validate the design of MoE-X, focusing on both performance and interpretability. 5.1. Chess Play Experiments Experimental Setup. For chess experiments, we train models on lichess 6gb2 (Karvonen, 2024), 16 million games from the public Lichess chess games database. The input to the model is chess PGN string (1.e4 e5 2.Nf3 ...) of maximum length of 1023 characters, with each character representing an input token. The models vocabulary consists of the 32 characters necessary to construct chess PGN strings. We split the dataset into 99% of training corporse and validation on 1% of validation set, report validation loss to test performance. Additionally, we report the BSP Coverage and Reconstruction score defined in (Karvonen et al., 2024) to assess interpretability. We compared our proposed MoE-X against three families of models. The first is dense baseline model similar to GPT-2. The second includes models with activation functions designed for better interpretability, such as bilinear layers like GEGLU (Pearce et al., 2024; Shazeer, 2020) and SoLU (Elhage et al., 2022a). The third consists of MoE models, including fine-grained MoEs like Monet (Park et al., 2https://huggingface.co/datasets/adamkarvonen/chess games 2024) and PEER (He, 2024), as well as standard MoEs like the Switch Transformer (Fedus et al., 2022). For Switch Transformer and MoE-X, we use 8 experts, with 2 experts activated at time. For MoE models, we explain the scaled hidden representation defined in Section 4.2. This avoids using the raw activations from each expert. All models have 8 layers and are trained for 60k iterations with batch size of 100. We use the Adamw optimizer with an initial learning rate of 3e-4 and cosine scheduling to reduce the learning rate to 1e-4 in the end. We train MoE-X by upcycling the weights (Komatsuzaki et al., 2022) from dense model. Additionally, we applied load balance loss with value of λ = 0.001. All experiments were conducted on 4 NVIDIA A40 GPUs. More details are listed in Appendix. MoE Achieves Better Interpretability. We present the interpretability scores of different models in Table 1. To ensure fair comparison, we strictly match the number of activated parameters between dense models and MoE models. For dense models, we use an MLP hidden size of = 4096, while for MoE models, we activate 2 experts, each with 2048 hidden neurons. Several key observations emerge from the results. First, MoE models demonstrate superior interpretability. Swicth transformer readily improve the interpretability score, and our proposed MoE-X achieves the best Reconstruction Score of 0.84. Second, prior architecture designs claiming improved interpretability do not perform well in practice. For example, SoLUs scores is even lower than the GELU-based GPT-2 baseline. Similarly, recent MoE models claiming to improve monosemanticity, such as Monet (Park et al., 2024), do not do well. We hypothesize that this is due to the use of product key quantization (Lample et al., 2019), which relies on the Cartesian product. This method makes expert gating scores interdependent, preventing experts from functioning independentlya key requirement for interpretability. These findings call for thorough re-evaluation of this field, as many claims of improved interpretability lack strong empirical support. MoE-X Scales Interpretability Faster. We evaluate interpretability across different model sizes, comparing dense GPT-2, Switch Transformers, and MoE-X. To compare the size fairly, for dense models, we fix = 512 and vary α {4, 8, 16}. For MoEs, we set = 512 and α = 4 for each expert, while varying the number of activated experts {1, 2, 4}. As shown in Figure 5, interpretability scores improve significantly as model size increases. With the same number of activated parameters during inference, MoE-X consistently Mixture of Experts Made Intrinsically Interpretable Figure 6. t-SNE projections of encoder weights for original MoE layer, MoE with ReLU experts, and without full MoE-X layers, trained on Chess dataset. outperforms alternatives, particularly in the BSP Reconstruction Score. MoE-X beats SAE with Greater Faithfulness. We compare MoE-X with SAE trained on GPT-2-small post-res with SAE hidden size of 4096. As shown in Figure 5, MoE-X achieves better interpretability than SAE with the same total parameters (GPT-2 + SAE). Moreover, MoE-X is inherently more faithful due to its intrinsic interpretability. Unlike SAE, which relies on posthoc decomposition to approximate features, MoE-X directly learns interpretable features. As result, SAE always suffers some performance loss ( 96% validation loss), while MoEX achieves perfect fidelity (100% loss recovery). MoE-X Expert cluster features. To better understand the models, we visualize the encoder weights of different MoE models trained on chess data using t-SNE (Van der Maaten & Hinton, 2008) projections. We treat each row from W(j) enc is treated as data point, and apply t-SNE to project them onto 2D plot. As shown in Figure 6, our MoEX effectively clusters vectors for expert 0, 4, 6, 7, capturing topics related to interpretable factors. In contrast, vanilla MoE models, such as the Switch Transformer, use routing functions optimized solely for performance, which fail to form meaningful cluster of features. 5.2. Interpretability for Natural Language Experimental Setup: For natural language models, we pretrain on the 10BT subset of FineWeb (Penedo et al., 2024). We use batch size of 320, context length of 1024 tokens per sentence, and train all models for 100k gradient steps. We evaluate the models on OpenWebText (Gokaslan et al., 2019), LAMBADA (Paperno et al., 2016), WikiText103, and WikiText2 (Merity et al., 2016), and reported the perplexity (PPL) score to show the performance. In addition to performance evaluation, we measure the interpretability by running the auto-interpretability pipeline Figure 7. Activated tokens for experts in MoE-X small on RedPajama-v2 validation dataset. Their interpretations were identified using the auto-interpretation. and report the Detection Accuracy3 defined in (Paulo et al., 2024). To obtain this score, we collect the activations of the target MLP over 10M tokens from RedPajama-v2 (Weber et al., 2024). The activated contexts are then fed into an explainer LLM, which provides short interpretation for the corresponding neuron. scorer LLM is asked to do binary classification to determine whether whole sequence activated hidden neuron given an interpretation and test text. We report the accuracy of this classification. We use Llama 3.1b 70b instruct as both the scorer and the explainer model. More detail is in Appendix. We compare MoE-X with GPT-2 and Switch-Transformer. For GPT-2, we trained small (124M) and medium (354M) models. Similarly, for Switch-Transformer and MoE-X, we created small and medium configurations with 8 experts each. During inference, 2 experts are active, resulting in 180M and 555M active parameters for small and medium models. We evaluate interpretability at layer 8 for small models and layer 16 for medium models. GPT-2 with SAE is also evaluated at post-res of layer 8. Quantitative Experiments. Table 2 presents the language modeling performance for different models. We observe 3https://github.com/EleutherAI/sae-auto-interp 7 Mixture of Experts Made Intrinsically Interpretable Table 2. Language modeling performance for different architectures. For PPL, lower is better. Model GPT-2 Small GPT-2 Small SAE Switch-S (8124M) MoE-X-S (8124M) GPT-2 Medium Switch-M (8354M) MoE-X-M (8354M) OpenWeb (PPL) 22.83 31.60 18.36 19.42 17.19 15.43 14.78 LAMBADA (PPL) WikiText103 (PPL) WikiText2 (PPL) 49.89 55.33 45.22 43.80 37.87 35.41 35.01 32.71 38.21 27.63 28.11 24.31 20.82 21.34 44.36 49.16 38.90 42.58 35.70 34.71 35.16 Figure 8. Automated Interpretability Detection Results in 8th Layer Hidden Activation Quantiles 1000 Random Features with 95% Confidence Intervals. Not indicates non-activating text. Figure 9. Comparison between TopK gating and our Sparsity routing. Our score identifies more sparse set of experts. that MoE models outperform dense model like GPT-2, with Switch Transformer slightly ahead of MoE-X but at comparable level. Notably, GPT-2 performance drop significantly when running with SAE. This is because post-hoc explainations like SAE simply fail to capture all crucial features. It results in reduced performance and less faithful explanations. For interpretability, we report the Detection Score for 1,000 randomly selected features. Each feature is scored with 100 activating and 100 non-activating examples. The activating examples are chosen via stratified sampling such that there are always 10 examples from each of the 10 deciles of the activation distribution. Figure 8 illustrates the overall accuracy. GPT+SAE serves as strong baseline for interpretability, which MoE-X Small already matches. When the model size is increased to MoE-X Medium, interpretability improves further, surpassing SAE. Qualitative Experiments. We show some auto-interp results on MoE-X small and its top-activated context in Figure 7. More results are included in Appendix 5. MoE-X successfully identifies interpretable concepts. 5.3. Ablation Study and Analysis ReLU Expert. We verify the benefit of use ReLU experts by replacing it with default GELU function, and train on the chess dataset using 2-of-8 expert setup. Since ReLU zeros out negative values while GELU does not, this replaceTable 3. Ablation study of Routing and Expert Choice. ReLU Expert Sparsity Router Coverage Reconstruction 0.424 0.404 0.418 0.428 0.734 0.740 0.829 0.840 ment increases the average ℓ0 norm of hidden activations from 166 to 4091. Besides, as shown in Table 3 applying ReLU significantly improves the reconstruction score. Those experiments show that applying ReLU induces sparser and more interpretable features. Sparsity-Aware Routing. We evaluate our gating function from two perspectives: (1) whether it selects sparser experts, and (2) whether it improves interpretability. Figure 9 shows the ℓ0-norm of each experts activations across 5,000 sentences, alongside gating scores from both standard topk approach and our sparsity-based method. While standard gating often misestimates sparsity, our gating scores exhibit strong negative correlation (r < 0.95) with the actual expert sparsity and consistently selects sparser experts. As shown in Table 3, applying sparsity-aware gating on top of ReLU experts further boosts interpretability. 6. Conclusion This paper addresses the challenge of improving interpretability in LLMs by introducing MoE-X, mixture-of8 Mixture of Experts Made Intrinsically Interpretable experts architecture designed for intrinsic transparency. Our key finding is that sparsity and width are essential for interpretability. By structuring MoE as wide and sparse MLP layers, we show that it naturally enhances interpretability. To further improve this, we use ReLU-based experts and sparsity-aware routing to reduce polysemanticity and create sparser internal representations. Experiments on chess and language tasks demonstrate that MoE-X performs on par with dense Transformers while providing more interpretable outputs."
        },
        {
            "title": "Impact Statement",
            "content": "This paper introduces MoE-X, scalable and interpretable language model designed to promote trust and reliability in AI systems. By improving transparency with sparse activations and efficient routing, MoE-X helps make AI decisions easier to understand. It can be especially useful in fields like healthcare and education, where trust is critical. While there is some risk of misuse or bias, these can be addressed through careful and ethical use. Overall, this work aims to advance AI by providing more transparent and reliable approach to large-scale models."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich, B., Caruana, R., and Hinton, G. E. Neural additive models: Interpretable machine learning with neural nets. Advances in neural information processing systems, 34: 46994711, 2021. Awasthi, P., Dikkala, N., Kamath, P., and Meka, R. Learning neural networks with sparse activations. In The Thirty Seventh Annual Conference on Learning Theory, pp. 406 425. PMLR, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., HatfieldDodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html. Jones, A., Drain, D., Chen, A., Bai, Y., Ganguli, D., Lovitt, L., Hatfield-Dodds, Z., Kernion, J., Conerly, T., Kravec, S., Fort, S., Kadavath, S., Jacobson, J., TranJohnson, E., Kaplan, J., Clark, J., Brown, T., McCandlish, S., Amodei, D., and Olah, C. Softmax linear units. Transformer Circuits Thread, 2022a. https://transformercircuits.pub/2022/solu/index.html. Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M., and TransOlah, C. former Circuits Thread, 2022b. https://transformercircuits.pub/2022/toy model/index.html. Toy models of superposition. Elhage, N., Olsson, C., Nanda, N., and Others. mathematical transformer circuits. https://www.transformer-circuits.pub/ 2022/mech-interp-essay, 2022c. framework for Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Gokaslan, A., Cohen, V., Pavlick, E., and Tellex, S. Openhttp://Skylion007.github. webtext corpus. io/OpenWebTextCorpus, 2019. He, X. O. Mixture of million experts. arXiv preprint arXiv:2407.04153, 2024. He, Z., Ge, X., Tang, Q., Sun, T., Cheng, Q., and Qiu, X. Dictionary learning improves patch-free circuit discovery in mechanistic interpretability: case study on othellogpt. arXiv preprint arXiv:2402.12201, 2024. Hendrycks, D., Mazeika, M., and Woodside, T. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001, 2023. Huben, R., Cunningham, H., Smith, L. R., Ewart, A., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2023. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Elhage, N., Hume, T., Olsson, C., Nanda, N., Henighan, T., Johnston, S., ElShowk, S., Joseph, N., DasSarma, N., Mann, B., Hernandez, D., Askell, A., Ndousse, K., Jermyn, A. S., Schiefer, N., and Hubinger, E. Engineering monosemanticity in toy models. arXiv preprint arXiv:2211.09169, 2022. 9 Mixture of Experts Made Intrinsically Interpretable Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Karvonen, A. Emergent world models and latent variable estimation in chess-playing language models. arXiv preprint arXiv:2403.15498, 2024. Karvonen, A., Wright, B., Rager, C., Angell, R., Brinkmann, J., Smith, L. R., Verdun, C. M., Bau, D., and Marks, S. Measuring progress in dictionary learning for language model interpretability with board game models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=SCEdoGghcw. Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C. R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M., and Houlsby, N. Sparse upcycling: Training mixturearXiv preprint of-experts from dense checkpoints. arXiv:2212.05055, 2022. Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and Jegou, H. Large memory layers with product keys. Advances in Neural Information Processing Systems, 32, 2019. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lieberum, T., Rajamanoharan, S., Conmy, A., Smith, L., Sonnerat, N., Varma, V., Kramar, J., Dragan, A., Shah, R., and Nanda, N. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147, 2024. Liu, Z., Gan, E., and Tegmark, M. Seeing is believing: Brain-inspired modular training for mechanistic interpretability. Entropy, 26(1):41, 2023a. Liu, Z., Khona, M., Fiete, I. R., and Tegmark, M. Growing brains: Co-emergence of anatomical and functional modularity in recurrent neural networks. arXiv preprint arXiv:2310.07711, 2023b. Liu, Z. L., Dettmers, T., Lin, X. V., Stoyanov, V., and Li, X. Towards unified view of sparse feed-forward network in pretraining large language model. arXiv preprint arXiv:2305.13999, 2023c. Makhzani, A. and Frey, B. K-sparse autoencoders. arXiv preprint arXiv:1312.5663, 2013. Kramnik, V. Acquisition of chess knowledge in alphazero. Proceedings of the National Academy of Sciences, 119 (47):e2206625119, 2022. Menon, A., Shrivastava, M., Krueger, D., and Lubana, E. S. Analyzing (in) abilities of saes via formal languages. arXiv preprint arXiv:2410.11767, 2024. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016. Mirzadeh, I., Alizadeh, K., Mehta, S., Del Mundo, C. C., Tuzel, O., Samei, G., Rastegari, M., and Farajtabar, M. Relu strikes back: Exploiting activation sparsity in large language models. arXiv preprint arXiv:2310.04564, 2023. Ngo, R., Chan, L., and Mindermann, S. The alignment problem from deep learning perspective. arXiv preprint arXiv:2209.00626, 2022. Olah, C. Mechanistic interpretability, variables, and the importance of interpretable bases, 2022. URL https://www.transformer-circuits.pub/ 2022/mech-interp-essay. Accessed: 2025-0118. Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom in: An introduction to circuits. Distill, 5(3):e00024001, 2020. Oldfield, J., Georgopoulos, M., Chrysos, G. G., Tzelepis, C., Panagakis, Y., Nicolaou, M. A., Deng, J., and Patras, I. Multilinear mixture of experts: Scalable expert specialization through factorization. arXiv preprint arXiv:2402.12550, 2024. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction arXiv preprint requiring broad discourse context. arXiv:1606.06031, 2016. Park, J., Ahn, Y. J., Kim, K.-E., and Kang, J. Monet: Mixture of monosemantic experts for transformers. arXiv preprint arXiv:2412.04139, 2024. Paulo, G., Mallen, A., Juang, C., and Belrose, N. Automatically interpreting millions of features in large language models. arXiv preprint arXiv:2410.13928, 2024. Pearce, M. T., Dooms, T., Rigg, A., Oramas, J. M., and Sharkey, L. Bilinear mlps enable weight-based mechanistic interpretability. arXiv preprint arXiv:2410.08417, 2024. McGrath, T., Kapishnikov, A., Tomaˇsev, N., Pearce, A., Wattenberg, M., Hassabis, D., Kim, B., Paquet, U., and Penedo, G., Kydlıˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. The fineweb 10 Mixture of Experts Made Intrinsically Interpretable datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum? id=n6SCkn2QaG. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Sharkey, L. technical note on bilinear layers for interpretability. arXiv preprint arXiv:2305.03452, 2023. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Toshniwal, S., Wiseman, S., Livescu, K., and Gimpel, K. Chess as testbed for language model state tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 1138511393, 2022. Van der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. Waswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Weber, M., Fu, D., Anthony, Q., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., et al. Redpajama: an open dataset for training large language models. arXiv preprint arXiv:2411.12372, 2024. Zhang, Z., Song, Y., Yu, G., Han, X., Lin, Y., Xiao, C., Song, C., Liu, Z., Mi, Z., and Sun, M. Relu 2 wins: Discovering efficient activation functions for sparse llms. arXiv preprint arXiv:2402.03804, 2024. Zhong, Z., Liu, Z., Tegmark, M., and Andreas, J. The clock and the pizza: Two stories in mechanistic explanation of neural networks. Advances in Neural Information Processing Systems, 36, 2024. Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A. M., Le, Q. V., Laudon, J., et al. Mixture-ofexperts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. 11 Mixture of Experts Made Intrinsically Interpretable In the appendix, we provide additional details to complement our paper. Section explores how interpretability scores evolve during training and across different model layers. Section defines the metrics used to assess model interpretability. Section describes the auto-interpretability experiment setup and presents newly identified interpretable features in the MoE-X small model. Section provides full derivation of how Sparse MoE can be formulated as an MLP layer and our sparse-aware gating. Finally, Section details the model training configurations. A. Interpretability Dynamics We evaluate two types of interpretability dynamics in language model trained on chess. First, we study how interpretability evolves over the number of training steps. Second, we examine how interpretability varies across different layers of the language model. For our experiments, we use an 8-layer GPT-style model with configuration (α = 4, = 512). Figure 10. Dynamics of BSP Coverage Score and Validation Loss over Training Steps. Training Iterations & Interpretability. As shown in Figure 10, the interpretability coverage score generally increases as training progresses. However, the trend does not exactly mirror the validation loss. Even when the validation loss plateaus, the coverage score continues to increase, indicating ongoing improvements in interpretability. This observation motivates our decision to upcycle model weights from dense model, following (Komatsuzaki et al., 2022). We find that longer training leads to higher interpretability scores. However, in MoE models, training is inherently less efficient per expert. Given total of iterations, each expert in MoE model is only activated and trained for k/M iterations, where is the number of selected experts per step. As result, each expert is effectively under-trained compared to dense model, making direct interpretability comparisons unfair. To validate this and assess the impact of weight upcycling, we conduct an experiment comparing different training strategies. Specifically, we train MoE both from scratch and with upcycled dense weights while also continuing the training of dense model for the same number of iterations. The results, shown in Table 4, indicate that upcycling significantly improves interpretability. The MoE trained from scratch achieves higher coverage score than the dense model, but its reconstruction performance lags behind. In contrast, the upcycled MoE not only outperforms the dense models in interpretability but also shows the best reconstruction score, demonstrating the benefits of leveraging pre-trained dense weights. Method Dense Dense (Continued Training) MoE-X (Scratch) MoE-X (Up-cycle) Coverage Reconstruction 0.356 0.377 0.398 0.428 0.608 0.674 0.657 0.840 Table 4. Comparison of interpretability scores for different training methods. 12 Mixture of Experts Made Intrinsically Interpretable Figure 11. BSP Reconstruction Score at different language model layers. Layer number & Interpretability. In Figure 11, we show the BSP Reconstruction Score across different layers of the language model. The interpretability score increases initially, peaks at layer 6, and then decreases. Based on this observation, we evaluate layer 6 in our 8-layer transformer. Similarly, we select moderate layers (e.g., layer 8 for 12-layer model and layer 16 for 24-layer model) for other model depths in natural language experiments. B. Metrics Definitions B.1. Board State Properties in Chess We define board state property (BSP) as function : {game board} {0, 1}, which evaluates specific characteristics of board state. In this work, we focus on interpretable classes of BSPs that capture fundamental game properties. One such class, Gboard state, includes BSPs that determine whether specific piece is present at given board square. Chess use an 8 8 board. In chess, we consider the full board for all twelve distinct piece types (e.g., white king, white queen, ..., black king), resulting in total of 8 8 12 BSPs. B.2. Coverage The coverage metric evaluates how well the features learned by Sparse Autoencoder (SAE) align with given set of Board State Properties (BSPs). Let be collection of BSPs, and let {fi} denote the set of features learned by the SAE. For each feature fi, we define binary classifier ϕfi,t based on threshold [0, 1]: ϕfi,t(x) = I[fi(x) > max ], where: fi(x) is the activation of feature fi for input x, max = maxxD fi(x) is the maximum activation of fi over the dataset D, I[] is the indicator function, which outputs 1 if the condition is true and 0 otherwise. For given BSP G, the F1-score of ϕfi,t as classifier for is denoted by F1(ϕfi,t; g). The coverage of the SAE with respect to is then defined as: Cov({fi}, G) = 1 (cid:88) gG 13 max max fi F1(ϕfi,t; g). Mixture of Experts Made Intrinsically Interpretable In words, for each BSP g, we select the feature fi and threshold that maximize the F1-score for classifying g. The coverage score is the average of these maximal F1-scores across all BSPs in G. coverage score of 1 indicates that the SAE has at least one feature that perfectly classifies every BSP in G. B.3. Board Reconstruction The board reconstruction metric measures the ability of an SAE to recover the complete state of chessboard from its feature activations in human-interpretable way. Let be set of BSPs, and let {fi} denote the set of SAE features. For each feature fi, we identify the subset of BSPs for which ϕfi,t is high-precision classifier (precision 0.95) on training dataset Dtrain. For given activation x, the predicted state of BSP is determined by the rule: (cid:40) Pg({fi(x)}) = if ϕfi,t(x) = 1 for any fi that is high-precision for on Dtrain, 1, 0, otherwise. The full predicted board state is represented as ({fi(x)}) = {Pg({fi(x)})}gG, which contains predictions for all 64 squares of the chessboard. The quality of the reconstruction is evaluated using the F1-score of the predicted board state ({fi(x)}) compared to the true board state b, denoted as F1(P ({fi(x)}); b). The board reconstruction score is then computed as the average F1-score over all board states in test dataset Dtest: Rec({fi}, Dtest) = 1 Dtest (cid:88) xDtest F1(P ({fi(x)}); b(x)), where b(x) is the true board state corresponding to activation x. This metric reflects how well the SAEs feature activations can be combined to reconstruct the full board state, emphasizing interpretability and precision. C. Routing Regularizes Sparsity Our sparsity-aware gating significantly reduces expert sparsity. In 2-out-of-8 MoE setup with top-k gating and ReLU experts, the ℓ0 norm of the experts is approximately 313. With our sparsity-aware gating, this value decreases to around 166. This demonstrates that sparsity-aware gating greatly enforces sparsity. D. Auto-Interpretability We conduct an auto-interpretability experiment following the approach described in (Paulo et al., 2024). For MLP layer, We collected latent activations from the MLP over 10M token sample of RedPajama-v24(Weber et al., 2024). The activations are gathered from batches of 256 tokens, each starting with beginning-of-sentence (BOS) token. To interpret these activations, we use Llama 3.1 70B Instruct as the explainer model. It is presented with 20 activating examples, each consisting of 32 tokens, where the activating tokens can appear at any position. These examples are randomly selected from larger dataset to ensure diversity in activation patterns. For evaluation, we use the detection score, where scorer model identifies which sequences activate given latent based on an interpretation. In this setup, the model is shown five examples at time, each with an independent probability of activating the latent, regardless of the others. Each latent is evaluated using 100 activating and 100 non-activating examples. The activating examples are selected through stratified sampling, ensuring that 10 examples are drawn from each of the 10 deciles of the activation distribution. Example. Besides the sample showed in the main paper, we show more results in the Table 5 4https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2 14 Mixture of Experts Made Intrinsically Interpretable Auto-Interp Meaning Location Example"
        },
        {
            "title": "Time of day in expressions",
            "content": "Expert 2, #"
        },
        {
            "title": "Abbreviations with dots",
            "content": "Expert 5, #"
        },
        {
            "title": "Capitals at the start of acronyms",
            "content": "Expert 6, #"
        },
        {
            "title": "Ordinal numbers in sentences\nHyphenated compound words\nCurrency symbols preceding numbers\nParentheses around numbers or letters\nEllipsis usage\nMeasurements followed by units\nDates in numeric formats\nRepeated punctuation marks\nHashtags in text\nUppercase words for emphasis\nColon in timestamps\nContractions with apostrophes",
            "content": "Expert 3, #412 Expert 2, #187 Expert 1, #273 Expert 6, #91 Expert 0, #55 Expert 0, #384 Expert 7, #401 Expert 2, #1128 Expert 4, #340 Expert 4, #278 Expert 3, #521 Expert 6, #189 We went for walk in the evening. The meeting is scheduled for afternoon. She always exercises in the morning. She explained the concept using e.g. as an example. You must submit all forms by Friday, i.e., tomorrow. Common abbreviations include a.m. and p.m. for time. The NASA mission was successful. The company developed cutting-edge AI systems. Students use PDF documents for submissions. He finished in 1st place. This is well-being initiative. The total cost was $100. Refer to section (a) for details. He paused and said, ... Ill think about it. The box weighs 5 kg. The deadline is 2025-01-29. What is happening ??? Follow the trend at #trending. The sign read, STOP immediately! The train arrives at 12:30. cant do this alone. Table 5. Sampled Activated Tokens and Contexts for Neurons in MoE-X Small. The meanings are identified by the Auto-interp process. E. Derivation E.1. MoE Layer as Sparse MLP In this section, we demonstrate how Mixture-of-Experts (MoE) layer can be reformulated as large and sparse Multi-Layer Perceptron (MLP). The output of the MoE layer is expressed as weighted sum of the expert outputs: ˆy = = = = (cid:88) j=1 (cid:88) j=1 (cid:88) j=1 (cid:88) i=1 ωjfj(x; θj), (cid:16) ωj W(j) decσ(W(j) encx) (cid:16) W(j) dec ωjσ(W(j) encx) W(j) dec (cid:16) ωjz(j)(cid:17) (cid:17) (cid:17) (8) (9) (10) (11) where ωj is the gating weight for the j-th expert, and z(j) = σ(W(j) function σ in the j-th expert. Since ωj is scalar, it can be factored out before multiplication with W(j) dec. encx) is the hidden representation after the activation 15 To simplify this representation, we define mega-decoder by concatenating all expert decoder matrices: Mixture of Experts Made Intrinsically Interpretable Wdec = concat([W(1) dec , . . . , W(M ) dec ]) RM Dd Similarly, we concatenate the scaled hidden representations of all experts: = concat([ω1z(1), . . . , ωM z(M )]) RM D, With these definitions, the MoE output can be reformulated as: ˆy = Wdecz. This reformulation demonstrates that an MoE layer is equivalent to wide and sparse MLP, where sparsity is induced by the selective activation of only subset of experts for given input. Interestingly, similar derivation is mentioned in (Liu et al., 2023c). However, their work focuses on building efficient and sparse neural networks, while ours emphasizes interpretability. E.2. Sparsity-Aware Gating Suppose we have experts, each accosiated Wenc = w1,1 . . . wD,1 RDd and input = . . . w1,d . . . . . . . . . wD,d x1 . . . xd hidden activation is = Wencx = (cid:80) (cid:80) w1,ixi . . . wD,ixi = z1 . . . zD Rd. The (12) If we assume that each rows of Wenc is i.i.d from Gaussian distribution, {wj,i}D element of from mixture of gaussian distribution j=1 (µi, σ2 ). Then we can see each zj = (cid:88) wj,ixi (µz, σ2 ) = ( (cid:88) µixi, x2 σ2 ) (cid:88) (13) If we apply ReLU function on top of this hidden, the probability that zj is positive is (zj > 0). (zj > 0) directly corresponds to the sparsity of ReLU(zj). Given the Gaussian assumption, (zj > 0) = 1 Φ( µz σz ) = Φ( µz σz ) = 1 2π (cid:90) µz σz (cid:110) exp (cid:111) du u2 (14) Where Φ(z) = (Z z) is the CDF of the normal distribution. In practice, common closed-form approximation for the CDF Φ is Φ(z) (1 + erf(z/ 2)) (15) 1 2 The larger the sparsity, the less non-zero values, and the (zj > 0) gets smaller. Because we want to select the expert with the largest sparsity Therefore, we select the experts with the smallest (zj > 0) arg max (cid:104) (cid:105) Sparsity(Wenc) = arg min (cid:104) (zj > 0) (cid:105) In terms of the Gaussian CDF arg min Φ( µz σz ) = arg max (cid:16) 1 2 1 + erf( (cid:17) ) µz 2σz = arg max erf( µz 2σz ) We use this as the gating function for mixture-of-expert = g(x; ϕ) = Softmax(TopK(erf( µz 2σz ))) (16) (17) (18) Mixture of Experts Made Intrinsically Interpretable F. Training Details The training configuration and hyperparameters are presented in Table 6 and Table 7. Table 6. MoE & GPT-2 Training Configuration for Chess Dataset."
        },
        {
            "title": "Value",
            "content": "8 8 512 0.0 3e-4 3e-5 2000 600000 Adamw 100 1023 8 2 1.0 Table 7. MoE & GPT-2 Small Training Configuration for FineWeb Language Tasks. Names Small Medium Num layer Num head Num embd dropout Init learning rate Min lr Lr warmup iters Max iters optimizer batch size context len Num experts Num experts per Token grad clip 12 24 12 16 768 1024 0.0 0.0 3e-4 3e-4 3e-5 3e-5 5000 5000 100000 100000 Adamw Adamw 320 1024 8 2 1.0 320 1024 8 2 1.0 Load Balance Loss. To ensure balanced distribution of tokens across experts, we use an auxiliary loss borrow from (Fedus et al., 2022). This auxiliary loss is added to the total model loss during training. Given experts indexed by = 1 to and batch containing tokens, the auxiliary loss is defined as the scaled dot product between the token distribution vector and the router probability vector P: Lbalance = α (cid:88) i=1 fi Pi (19) where fi represents the fraction of tokens assigned to expert i: 17 Mixture of Experts Made Intrinsically Interpretable fi ="
        },
        {
            "title": "1\nT",
            "content": "(cid:88) xB I{arg max p(x) = i} and Pi denotes the fraction of the router probability allocated to expert i: Pi ="
        },
        {
            "title": "1\nT",
            "content": "(cid:88) xB pi(x). (20) (21) Since we aim for uniform token routing across all experts, both and should ideally have values close to 1/N . For all MoE model training in this paper, we set the load balancing weight to λ = 0.001."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Oxford"
    ]
}