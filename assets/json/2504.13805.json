{
    "paper_title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark",
    "authors": [
        "Guangyi Liu",
        "Pengxiang Zhao",
        "Liang Liu",
        "Zhiming Chen",
        "Yuxiang Chai",
        "Shuai Ren",
        "Hao Wang",
        "Shibo He",
        "Wenchao Meng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further develop LearnAct, a sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, a single demonstration improves model performance, increasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFT's task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstration-based learning as a promising direction for more adaptable, personalized, and deployable mobile GUI agents."
        },
        {
            "title": "Start",
            "content": "LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Pengxiang Zhao Zhejiang University Hangzhou, China Guangyi Liu Zhejiang University Hangzhou, China Liang Liu vivo AI Lab Hangzhou, China 5 2 0 2 8 1 ] . [ 1 5 0 8 3 1 . 4 0 5 2 : r Zhiming Chen vivo AI Lab Hangzhou, China Hao Wang vivo AI Lab ShenZhen, China Yuxiang Chai vivo AI Lab Hangzhou, China Shibo He Zhejiang University Hangzhou, China Shuai Ren vivo AI Lab ShenZhen, China Wenchao Meng(cid:66) Zhejiang University Hangzhou, China wmengzju@zju.edu.cn Figure 1: The LearnAct Framework and LearnGUI Benchmark focus on addressing the long-tail challenges in mobile GUI agent performance through demonstration-based learning. From rule-based automation to LLM-powered agents, mobile GUI automation has evolved significantly, yet still struggles with long-tail scenarios due to interface diversity. Our LearnAct framework introduces demonstrationbased learning to effectively handle these challenges, outperforming existing methods in both offline and online evaluations. ABSTRACT Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents. It comprises 2,252 offline tasks and 101 online tasks with high-quality Equal Contribution, Project Lead, (cid:66) Corresponding Author. human demonstrations. We further develop LearnAct, sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, single demonstration improves model performance, increasing Gemini-1.5-Pros accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFTs task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstrationbased learning as promising direction for more adaptable, personalized, and deployable mobile GUI agents. The project resources are available at https://lgy0404.github.io/LearnAct. Liu et al. to perform effectively. The prevailing approaches to building modern mobile GUI agents rely on either the inherent capabilities of general-purpose LLMs [18, 20, 28, 29, 34, 36, 37, 46] or fine-tuning with large volumes of data [11, 16, 41, 48]. However, these methods face fundamental limitations when confronted with diverse realworld usage scenarios. As of 2025, billions of users interact with 1.68 million applications on Google Play alone [17], each with unique task requirements and UI layouts [32, 43]. Pre-training or finetuning datasets cannot feasibly cover this immense variety, leading to poor performance in unseen scenarios and hindering the widespread adoption of mobile GUI agents [14], as illustrated in Figure 1 (left side). Traditional approaches simply cannot cover the entire spectrum of possible interactions and user-specific requirements across this heterogeneous landscape. To address these limitations, we propose novel paradigm that enhances mobile GUI agent capabilities through few-shot demonstration learning. Unlike traditional approaches that either lack flexibility or require massive datasets, our demonstrationbased approach achieves both robustness and personalization by learning from small number of user-provided examples. We recognize that mobile users have unique, repetitive tasks with inherent variabilitysuch as smart home control with dynamic configurations, health monitoring with personalized parameters, or enterprise software with company-specific layouts. These scenarios combine stable patterns with variable elements, creating \"personalization gap\" that pre-trained models cannot bridge. By leveraging user-specific demonstrations, our approach enables personalized assistants that learn both consistent patterns and adaptation strategies, acquiring task-specific knowledge impossible to cover in general training datasets. This personalization allows mobile GUI agents to overcome performance bottlenecks and provide truly helpful automation for the tasks users most want to delegate. To fill the gap in high-quality demonstration data, we introduce LearnGUI, the first dataset specifically designed to research and evaluate mobile GUI agents ability to learn from few-shot demonstrations. Built upon AMEX [5] and AndroidWorld [23], LearnGUI comprises 2,252 offline few-shot tasks and 101 online tasks with high-quality human demonstrations. This dataset enables systematic research into demonstration-based learning for mobile GUI agents. toy example for LearnGUI is shown in Figure 2. Furthermore, we present LearnAct, multi-agent framework that automatically understands human demonstrations, generates instructional knowledge, and uses this knowledge to assist mobile GUI agents in reasoning about unseen scenarios. LearnAct consists of three specialized agents: (1) DemoParser, knowledge generation agent that extracts usable knowledge from demonstration trajectories to form knowledge base; (2) KnowSeeker, knowledge retrieval agent that searches the knowledge base for demonstration knowledge relevant to the current task; and (3) ActExecutor, task execution agent that combines user instructions, real-time GUI environment, and retrieved demonstration knowledge to perform tasks effectively. Our experimental results decisively validate the effectiveness of demonstration-based learning for mobile GUI agents, as shown in Figure 1 (right side). In offline evaluations, single demonstration dramatically improves model performance across diverse scenarios, Figure 2: toy example for demonstration learning on mobile GUI Agent. We build benchmark named LearnGUI for demonstration learning on Mobile GUI Agent, which provides different few-shot task combinations and offers multi-dimensional metrics including task similarity, UI similarity, and action similarity between support tasks and query tasks."
        },
        {
            "title": "1 INTRODUCTION\nMobile device automation has evolved significantly over time, from\nsimple rule-based scripts to sophisticated AI-powered agents [17,\n32, 38, 43]. Traditional automation approaches like Robotic Process\nAutomation (RPA) [1] and rule-based shortcuts [10, 13] relied on\npredefined scripts to execute repetitive tasks, but they struggled\nwith dynamic interfaces, required frequent maintenance when apps\nupdated, and lacked understanding of complex user intentions.",
            "content": "More recently, mobile Graphical User Interface (GUI) agents have emerged as transformative technology with the potential to revolutionize how humans interact with mobile devices. These agents leverage Large Language Models (LLMs) to autonomously complete human tasks through environmental interaction [6, 18, 20, 28, 29, 33, 36, 37, 46]. They perceive phone states of mobile phone by observing screens (through screenshots or application UI trees) and generate actions (such as CLICK, TYPE, SWIPE, PRESS_BACK, PRESS_HOME, and PRESS_ENTER) that are executed via the phone user interface [17, 32, 38, 43]. By harnessing the powerful perception and reasoning capabilities of LLMs, mobile GUI agents have the potential to fundamentally change how people interact with their mobile devices, bringing to life the \"J.A.R.V.I.S.\" effect seen in science fiction. Despite these promising advances, mobile GUI agents continue to face significant challenges in real-world deployment scenarios. The immense diversity of mobile applications and user interfaces creates pervasive long-tail scenarios where current agents struggle 2 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Table 1: Comparison of different datasets and environments for benchmarking Mobile GUI agents. Column definitions: # Inst. (number of instructions), # Apps (number of applications), # Step (average steps per task), Env. (supports environment interactions), HL (has high-level instructions), LL (has low-level instructions), GT (provides ground truth trajectories), FS (supports few-shot learning). Dataset # Inst. # Apps # Step Env. HL LL GT FS PixelHelp [15] MoTIF [4] UIBert [3] UGIF [27] AITW [24] AITZ [45] AndroidControl [14] AMEX [5] MobileAgentBench [30] AppAgent [44] 187 276 16,660 523 30,378 2,504 15,283 2,946 100 50 LlamaTouch [47] AndroidWorld [23] AndroidLab [40] 496 116 138 LearnGUI (Ours) 2,353 4 125 - 12 357 70 833 110 10 57 20 9 73 4.2 4.5 1 6.3 6.5 7.5 4.8 12.8 - - 7.01 - 8.5 13.2 with the most striking results seen in Gemini-1.5-Pro [26], whose accuracy increases from 19.3% to 51.7% (a 198.9% relative improvement). Performance gains are particularly pronounced in complex applications, with accuracy in CityMapper increasing from 14.1% to 69.4% and in To-Do apps from 17.4% to 69.2%. For real-world online evaluations, our framework demonstrates exceptional effectiveness, with Qwen2-VL-7B [31] with LearnAct achieving significant performance gains, while UI-TARS-7B-SFT [22]s task success rate improves from 18.1% to 32.8% (+14.7%). These findings offer practical pathway to developing more adaptable and personalized mobile GUI agents. In summary, our contributions are as follows: We develop LearnGUI, the first dataset designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline and 101 online tasks with high-quality human demonstrations. We design and implement LearnAct, sophisticated multiagent framework that systematically extracts, retrieves, and leverages knowledge from human demonstrations. This framework includes three specialized components: DemoParser (knowledge extraction), KnowSeeker (knowledge retrieval), and ActExecutor (task execution). Our evaluations demonstrate unprecedented performance gains: single demonstration improves Gemini-1.5-Pro [26]s accuracy by 198.9% in offline tests, while enhancing UI-TARS7B-SFT [22]s online task success rate from 18.1% to 32.8%, advancing mobile GUI agents toward greater adaptability and practical deployability."
        },
        {
            "title": "2 RELATED WORK\nMobile GUI Datasets and Environments. The development of\nmobile GUI agents relies heavily on high-quality datasets for train-\ning and evaluation. Table 1 compares LearnGUI and existing mobile",
            "content": "3 GUI datasets and benchmarks. These resources can be broadly categorized into static datasets and dynamic benchmarking environments. Static datasets [35, 14, 15, 24, 27, 30, 45] typically provide natural language task descriptions, UI states (screenshots and/or application UI trees), and corresponding user actions (CLICK, SWIPE, TYPE, and other standardized interactions). These datasets vary in scale, ranging from hundreds to tens of thousands of instructions across different applications. Recent work like AppAgent [44] has explored demonstration-based learning but without ground truth annotations or systematic analysis, providing only 50 tasks across 10 applications with high-level instructions. Notably, the average task length varies significantly across datasets, with AMEX [5] featuring substantially longer sequences (12.8 steps on average) compared to AndroidControl (4.8 steps) and AITW [24] (6.5 steps). Benchmarking environments, on the other hand, typically select limited number of tasks and applications to provide dynamic testing environments [17]. These frameworks evaluate agent performance through metrics such as task completion rates, critical state achievements, and execution time. Examples include LlamaTouch [47], AndroidWorld [23], and AndroidLab [40], which offer interactive environments but lack few-shot demonstration capabilities. We present the first systematic study of demonstration-based learning for mobile GUI agents through LearnGUI, which distinguishes itself through three key innovations. First, it is designed to evaluate few-shot learning capabilities with comprehensive collection of 2,252 offline tasks and 101 online tasks. Built upon AMEX [5] and AndroidWorld [23], which feature longer, more complex tasks ideal for out-of-distribution and demonstration-based learning scenarios, LearnGUI provides unified framework for both offline and online evaluation. Second, while the original AMEX [5] dataset contains 2,946 independent tasks unsuitable for few-shot evaluation, we conducted detailed analyses to transform and enhance this resource. Specifically, we made three key modifications: (1) Action Space Standardization, refining the original action space by removing inconsistent TASK_IMPOSSIBLE actions, enhancing TASK_COMPLETE to support information retrieval tasks, and standardizing formats for consistency; (2) K-shot Task Combinations, constructing systematic task groupings by recovering application context, computing instruction similarity within applications, and creating k-shot combinations with similar tasks as support demonstrations; and (3) Similarity Measurement, computing UI and action similarity through descriptive representations, enabling analysis of how different similarity types affect learning efficacy. Third, regarding online evaluation, AndroidWorld [23] originally provides 116 dynamically constructed tasks without human demonstration trajectories. We collected 101 high-quality human demonstrations based on AndroidWorlds environment and dynamic instructions, forming LearnGUI-Online for evaluating the few-shot capabilities of mobile GUI agents in real-time scenarios. By addressing the limitations of existing datasets, LearnGUI enables systematic research into few-shot learning for mobile GUI agents with varying k-shot configurations and controlled similarity conditions between support and query tasks. Mobile GUI Agents. Mobile GUI agents are intelligent systems that leverage large language models to understand, plan, and execute tasks on mobile devices by integrating natural language processing, multimodal perception, and action execution capabilities [32, 38]. Recent developments in this field have explored various approaches to enhance agent performance and generalizability. One prominent category of work focuses on designing effective prompting strategies to guide pre-trained LLMs without additional training [8, 35, 42]. By crafting prompts that incorporate task descriptions, interface states, and action histories, researchers can direct model behavior toward specific automation goals [25, 28, 29, 37]. These approaches leverage the inherent capabilities of generalpurpose LLMs but often struggle with complex tasks. second category involves adapting LLMs specifically for mobile automation through fine-tuning techniques [7, 9, 11, 16, 19, 21, 41]. These methods train models on GUI-specific data to enhance their understanding of and interaction with graphical interfaces. While improving performance over pre-training approaches, these fine-tuned models require substantial training data and still face generalization challenges. Despite the progress made by both approaches, fundamental limitation persists: the inability to generalize effectively to out-of-distribution scenarios. These methods both struggle with unseen applications, novel UI layouts, or unexpected task variations. These limitations stem from the impossibility of covering all potential real-world scenarios during training, creating significant bottlenecks in mobile GUI agent development. To address these critical challenges, we introduce LearnAct, sophisticated multiagent framework that learns and reasons from screenshots without requiring UI tree information. The framework extracts, retrieves, and utilizes demonstration knowledge through three specialized components, enabling effective adaptation to new scenarios with minimal demonstrations."
        },
        {
            "title": "3 LEARNGUI DATASET\n3.1 Task Definition\nMobile GUI tasks require agents to interact with digital environ-\nments by executing actions to fulfill user instructions. These tasks\ncan be formally described as a Partially Observable Markov De-\ncision Process (POMDP), defined as M = (S, O, A, T , R), where\nS is the state space (current state of the mobile device), O is the\nobservation space (instructions, screenshots, UI trees, etc.), A is the\naction space (e.g., click, type, swipe), T : S × A → S is the state\ntransition function, and R : S × A → [0, 1] is the reward function.\nFor example, a user might request the agent to \"find the cheapest\nhotel in Paris for next weekend.\" The agent must perceive the cur-\nrent screen—either through an image or a UI tree—and execute a\nsequence of actions to complete the given task.",
            "content": "The key innovation in our approach is the integration of human demonstration knowledge into this POMDP framework. By incorporating demonstration knowledge into the decision process, we enhance the agents ability to handle out-of-distribution scenarios. This knowledge influences the agents policy 𝜋 : A, which maps observations and relevant demonstration knowledge to actions, providing valuable examples of successful interaction patterns. To study the impact of demonstration-based learning on mobile GUI agents, we need dataset that provides various k-shot demonstrations with controlled similarity relationships between support and query tasks. This allows us to systematically investigate Liu et al. how demonstration quantity and task similarity affect agent performance. While cross-application knowledge transfer remains an interesting research direction, we focus on within-application task learning, as this represents the most practical use case where users would provide demonstrations for applications they frequently use. Our dataset design specifically enables research on three key dimensions: (1) Unified comprehensive evaluation framework: LearnGUI provides standardized platform for studying few-shot demonstration learning in mobile GUI agents, featuring unified action space and evaluation protocols that reflect real-world use cases (2) K-shot demonstration learning: The dataset systematically explores how varying quantities of demonstrations (k=1, 2, or 3) affect agent performance, enabling research on the optimal number of examples needed (3) Multi-dimensional similarity analysis: LearnGUI enables investigation of how different types of similarity between demonstration and query tasks influence learning efficacy and generalization capabilities This comprehensive approach allows for nuanced analysis of how mobile GUI agents can leverage human demonstrations to improve task performance, especially in scenarios not covered by their training data."
        },
        {
            "title": "3.2 Data Collection\nThe LearnGUI dataset consists of two components: LearnGUI-\nOffline for systematic evaluation of few-shot learning capabilities\nacross varying similarity conditions, and LearnGUI-Online for\nreal-time assessment in an interactive environment. Both compo-\nnents share a unified action space to ensure consistent evaluation,\nas detailed in Table 2.",
            "content": "Table 2: LearnGUI Action Space Action CLICK[x, y] TYPE[text] Definition Click at coordinates (x, y). Type the specified text. SWIPE [direction] Swipe in the specified direction. PRESS_HOME PRESS_BACK PRESS_ENTER Go to the home screen. Go back to the previous app screen. Press the enter button. TASK_COMPLETE[answer] Mark the task as complete. Provide answer inside brackets if required. 3.2.1 LearnGUI-Offline. We built LearnGUI-Offline by restructuring and enhancing the AMEX dataset [5], which contains 2,946 independent mobile tasks. To transform this resource for few-shot learning evaluation, we made several key modifications: Action Space Standardization. We refined the original action space to better align with real-world scenarios. First, we removed 4 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Table 3: Statistics of LearnGUI dataset splits. Each split is analyzed across multiple dimensions: Tasks (number of tasks), Apps (number of applications covered), Step actions (total action steps), similarity metrics (Avg Ins/UI/ActSim), and distribution across four similarity profiles categorized by high (SH) and low (SL) UI and action similarity. Split K-shot Tasks Apps Step actions Avg InsSim Avg UISim Avg ActSim UISHActSH UISHActSL UISLActSH UISLActSL Offline-Train Offline-Train Offline-Train Offline-Test Offline-Test Offline-Test Online-Test 1-shot 2-shot 3-shot 1-shot 2-shot 3-shot 1-shot 2,001 2,001 2,001 251 251 251 101 44 44 44 9 9 9 26,184 26,184 26,184 3,469 3,469 3,469 1,423 0.845 0.818 0.798 0.798 0.767 0.745 - 0.901 0.898 0.895 0.868 0.855 0.847 - 0.858 0.845 0.836 0.867 0.853 0.847 - 364 216 152 37 15 10 - 400 360 346 49 42 36 - 403 358 310 56 55 49 - 834 1,067 1,193 109 139 156 - TASK_IMPOSSIBLE actions due to inconsistent labeling in the original dataset, which included errors such as tasks being incorrectly marked as impossible. Second, we enhanced TASK_COMPLETE to TASK_COMPLETE[answer] for information retrieval tasks. Many mobile tasks require returning specific information rather than just completion status. This aligns with both AMEX [5] and AndroidWorld [23] paradigms. K-shot Task Combinations. We constructed systematic k-shot task combinations through multi-step process. We began by recovering the application context for each task through instruction and screenshot analysis, as the original dataset lacked explicit app labels. Next, we computed instruction similarity between tasks within the same application using the all-MiniLM-L6-v2 model. Finally, we created k-shot combinations (k=1,2,3) for each query task by selecting the most similar tasks within the same application as support demonstrations, ensuring that the average similarity exceeded minimum threshold of 0.6. This process yielded 2,252 tasks with valid k-shot combinations. Similarity Measurement. To enable multi-dimensional similarity analysis, we computed metrics across three key dimensions. For Instruction Similarity, we utilized the scores calculated during the K-shot Task Combinations process. For UI Similarity, we merged the UI trees from all steps of each task and calculated similarity using TF-IDF vectorization and cosine similarity, capturing the visual and structural similarity of interfaces. For Action Similarity, following the DemoParser approach detailed in Section 4.1, we generated descriptive representations of each action and computed embedding-based cosine similarity between task pairs. 3.2.2 LearnGUI-Online. For evaluating mobile GUI agents in realtime interactive scenarios, we developed LearnGUI-Online based on the AndroidWorld environment [23]. While AndroidWorld provides 116 dynamically constructed task templates, it lacks human demonstration trajectories essential for few-shot learning evaluation. We identified 101 tasks suitable for human completion, excluding 15 tasks that proved challenging for human users. We then collected high-quality human demonstrations for these tasks. For tasks with dynamic elements, we generated specific instances and recorded corresponding demonstrations. The resulting LearnGUI-Online dataset provides realistic testbed for evaluating few-shot learning capabilities in mobile GUI agents under authentic conditions."
        },
        {
            "title": "3.4 Dataset Splits\nWe divided LearnGUI-Offline into training and testing splits to\nenable systematic evaluation of few-shot learning capabilities. Ta-\nble 3 presents the detailed statistics of these splits, including the\ndistribution of tasks across different similarity profiles.",
            "content": "The training set contains 2,001 tasks for each k-shot configuration (1, 2, and 3), spanning 44 applications with an average of 13.1 steps per task. The test set includes 251 tasks per k-shot configuration across 9 applications. Both splits maintain the same action space and similarity measurement methodology. Based on empirical analysis, we established threshold values of 0.9447 for UI similarity and 0.9015 for action similarity to classify tasks into high (SH) and low (SL) similarity categories, enabling systematic analysis of how different similarity types affect learning from demonstrations. As shown in Figure 3, we classify tasks into four categories based on UI and action similarity: UISHActSH: High UI similarity and high action similarity. For example, in smart home app, two tasks that both involve adjusting the brightness of different lights in the living room would navigate through similar UI screens. UISHActSL: High UI similarity but low action similarity. For instance, in smart home app, turning on all lights with single button press versus adjusting each lights color temperature. UISLActSH: Low UI similarity but high action similarity. For example, setting schedule for lights versus setting Liu et al. LearnAct is sophisticated multi-agent framework that automatically understands human demonstrations, generates instructional knowledge, and leverages this knowledge to assist mobile GUI agents in reasoning about unseen scenarios. The LearnAct framework consists of three specialized components, each addressing critical aspect of demonstration-based learning: (1) DemoParser (Section 4.1), knowledge generation agent that extracts usable knowledge from demonstration trajectories to form knowledge base; (2) KnowSeeker (Section 4.2), knowledge retrieval agent that searches the knowledge base for demonstration knowledge relevant to the current task; and (3) ActExecutor (Section 4.3), task execution agent that combines user instructions, real-time GUI environment, and retrieved demonstration knowledge to perform tasks effectively."
        },
        {
            "title": "4.1 DemoParser\nThe DemoParser transforms raw human demonstrations into struc-\ntured demonstration knowledge. It takes as input a raw action\nsequence (consisting of coordinates-based clicks, swipes, and text\ninputs) along with corresponding screenshots and task instructions.\nIt then utilizes a vision-language model to generate semantically\ndescriptive action descriptions that capture the essence of each\ndemonstration step (e.g., “On Search Page, click the search box,\nto enter keywords”). Building on these descriptions, it constructs\na structured knowledge base that records both the high-level ac-\ntion semantics and the contexts in which they occur, as shown in\nFigure 5.",
            "content": "Formally, DemoParser implements knowledge generation function 𝐺 : K, where represents the space of instructions, is the space of screenshot sequences, is the space of action sequences, and is the knowledge space. For each demonstration trajectory (𝑖, 𝑠, 𝑎) A, DemoParser generates knowledge entry 𝑘 that encapsulates the demonstration in semantically descriptive format, converting raw coordinate-based actions (e.g., CLICK[123,456]) into meaningful operation descriptions (e.g., \"click search box\"). The knowledge generation process is decomposed into sequence of description generation steps for each action in the demonstration trajectory. Let 𝑑 𝑗 represent the description for action 𝑎 𝑗 , which is generated using context-aware description function 𝛿 : 𝑗 V𝑗 H𝑗 1 D, where V𝑗 is the visual representation of action 𝑎 𝑗 execution and H𝑗 1 = {𝑑1, 𝑑2, . . . , 𝑑 𝑗 1} is the history of previous action descriptions. Algorithm 1 in Appendix B.3 outlines the knowledge generation process. For each demonstration, DemoParser preserves the original task instruction and action sequence while generating semantically descriptive action descriptions. These descriptions provide crucial context about the purpose and significance of each action in the demonstration, enabling more effective knowledge transfer to new scenarios. For intermediate actions, DemoParser analyzes visual representation of the action execution, showing before-action and afteraction screenshots with the action visualized (e.g., click locations highlighted). The framework combines this visual input with the task instruction, action history, and current action to generate description that follows standardized format: \"[On/In] [Screen Figure 3: Joint distribution of UI similarity and action similarity in LearnGUI-Offline. The scatter plot shows the relationship between UI and action similarity measures across task pairs. The quadrant divisions represent our categorization of tasks into four profiles: UISHActSH, UISHActSL, UISLActSH, and UISLActSL, enabling analysis of how different similarity combinations affect learning transfer. schedule for the thermostatdifferent UI screens but similar action patterns. UISLActSL: Low UI similarity and low action similarity. For instance, checking security camera footage versus creating scene that coordinates multiple devices. This categorization enables detailed analysis of how different types of similarity affect learning efficacy. For instance, we can investigate whether UI similarity or action similarity has greater impact on successful knowledge transfer from demonstrations. Additionally, the LearnGUI-Online test set contains 101 tasks across 20 applications. Unlike the offline dataset, these tasks are evaluated in real time through direct interaction with the mobile environment. The comprehensive structure of LearnGUI, with its carefully designed splits and similarity classifications, provides resource for studying how mobile GUI agents can learn from demonstrations under varying conditions of task similarity and demonstration quantity."
        },
        {
            "title": "4 METHOD: LEARNACT\nBuilding on the insights from our LearnGUI dataset, we introduce\nLearnAct, a novel framework designed to break through the lim-\nitations of traditional training approaches for mobile GUI agents.\nRather than pursuing universal generalization through extensive\ntraining data, LearnAct establishes demonstration-based learning\nas a paradigm for developing more adaptable, personalized, and\npractically deployable mobile GUI agents. As illustrated in Figure 4,",
            "content": "6 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Figure 4: Illustration of the overall framework of LearnAct. Architecture diagram showing the three main components (DemoParser, KnowSeeker, ActExecutor) and their interconnections within the LearnAct system, including data flow from human demonstrations to execution. Name], [Action Details], to [Purpose]\". For example: \"On Home Screen, tap Settings icon, to access device configuration.\" For terminal actions, DemoParser processes the final screenshot, task instruction, and complete action history to generate conclusion in the format: \"[On/In] [Screen], complete task, [Reason/Answer]\" distinctive feature of DemoParser is its memory mechanism, which captures critical information observed during task execution that may be necessary for future steps. The model identifies and annotates task-relevant information that is directly related to the users instruction, will likely be needed in subsequent steps, and has not been previously recorded. These memory annotations are included in the action descriptions when appropriate: \"[On/In] [Screen], [Action], to [Purpose]. [Memory: important information for future steps]\". For example, in shopping task, memory annotation might capture: \"[Memory: iPhone 13 Pro costs $999 with 128GB storage]\". The detailed prompt for this memory mechanism is provided in Appendix B.1. This memory mechanism is particularly valuable for complex tasks requiring information retention across multiple steps, such as comparing prices, remembering account details, or tracking status changes. By transforming raw demonstrations into structured, semantically descriptive knowledge with memory capabilities, DemoParser enables effective knowledge transfer from human demonstrations to automated task execution."
        },
        {
            "title": "4.2 KnowSeeker\nKnowSeeker is the retrieval component of the LearnAct framework\nthat identifies demonstration knowledge most relevant to the cur-\nrent task context. As depicted in Figure 6, this agent serves as the\nbridge between the knowledge base generated by DemoParser and\nthe execution environment of ActExecutor. While DemoParser fo-\ncuses on transforming demonstrations into structured knowledge,\nKnowSeeker specializes in efficiently accessing and selecting the\nmost applicable knowledge for a specific task, addressing the critical\nchallenge of knowledge relevance in few-shot learning scenarios.\nFormally, KnowSeeker implements a retrieval function 𝑅 : I ×\nK → K (𝑠 ) , where I is the instruction space, K is the knowledge\nbase, and K (𝑠 ) ⊂ K is a subset of knowledge entries determined to\nbe relevant for the given instruction. This retrieval process is crucial\nfor effective knowledge utilization, as it filters the potentially vast\nknowledge base to focus exclusively on demonstrations that offer\nvaluable insights for the current task.",
            "content": "The core of KnowSeekers retrieval mechanism relies on semantic similarity measurement between the current task instruction and the instructions associated with demonstrations in the knowledge base. This similarity-based retrieval can be formally defined as: Liu et al. Figure 5: Pipeline of DemoParser Agent. Input instructions and corresponding actions and screenshots; output low-level action descriptions and create knowledge database. This process transforms high-level user instructions into precise operation sequences while building reusable domain knowledge base to improve mobile interface interaction automation efficiency. similarity threshold, and 𝑡𝑜𝑝 𝑘 indicates selection of the 𝑘 most similar entries. To implement this similarity measurement efficiently, KnowSeeker employs two-phase approach: (1) Embedding Generation: Instructions are transformed into dense vector representations using pre-trained sentence transformer model. Specifically, we utilize the all-MiniLML6-v2 model, which offers an optimal balance between computational efficiency and semantic representational power. This model has been fine-tuned on diverse natural language understanding tasks, making it particularly well-suited for capturing the semantic essence of mobile GUI task instructions. (2) Similarity Computation: The cosine similarity between embedding vectors is calculated to quantify the semantic relationship between instructions. For instructions 𝑖 and 𝑖 𝑗 with corresponding embeddings 𝑒𝑖 and 𝑒 𝑗 , the similarity is computed as: 𝑠𝑖𝑚(𝑖, 𝑖 𝑗 ) = 𝑒𝑖 𝑒 𝑗 𝑒𝑖 𝑒 𝑗 (2) To optimize retrieval efficiency, KnowSeeker pre-computes embeddings for all instructions in the knowledge base during initialization. This approach transforms the potentially expensive operation of computing pairwise similarities during runtime into more manageable vector comparison task. The pre-computation process is described as: 8 Figure 6: Pipeline of KnowSeeker Agent. The KnowSeeker Agent converts demo trajectories from the knowledge base into vector database. When executing user tasks, KnowSeeker retrieves the top-k relevant demos from the vector database for subsequent use. This approach enables efficient retrieval of similar demonstrations to assist with new task execution. 𝑅(𝑖, 𝐾) = {𝑘 𝑗 𝐾 𝑠𝑖𝑚(𝑖, 𝑖 𝑗 ) 𝜏𝑠 } (1) where 𝑖 is the current instruction, 𝑖 𝑗 is the instruction associated with knowledge entry 𝑘 𝑗 , 𝑠𝑖𝑚(, ) is similarity function, 𝜏𝑠 is 𝑡𝑜𝑝 𝑘 𝑗=1 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Figure 7: Pipeline of ActExecutor Agent. The ActExecutor Agent executes the low-level action descriptions generated by the Action Planner Agent. It uses the KnowSeeker Agent to retrieve relevant demonstrations from the knowledge base and execute the actions in the demonstrations. This approach enables efficient execution of low-level actions to assist with new task execution. 𝐸 = {𝑒 𝑗 = 𝑓𝑒𝑚𝑏𝑒𝑑 (𝑖 𝑗 ) 𝑘 𝑗 𝐾 } (3) where 𝑓𝑒𝑚𝑏𝑒𝑑 is the embedding function implemented by the sentence transformer model. During task execution, when presented with new instruction 𝑖, KnowSeeker: 1. Computes the embedding 𝑒𝑖 = 𝑓𝑒𝑚𝑏𝑒𝑑 (𝑖) 2. Calculates similarities 𝑆 = {𝑠𝑖𝑚(𝑒𝑖, 𝑒 𝑗 ) 𝑒 𝑗 𝐸} 3. Selects the top-𝑘 knowledge entries based on similarity scores This approach ensures that knowledge retrieval scales efficiently with the size of the knowledge base, enabling rapid identification of relevant demonstrations even as the frameworks experiential knowledge grows over time. By systematically identifying the most relevant demonstration knowledge, KnowSeeker enables ActExecutor to perform tasks more effectively, particularly in unfamiliar scenarios."
        },
        {
            "title": "4.3 ActExecutor\nActExecutor is the execution component of the LearnAct framework\nthat translates retrieved demonstration knowledge into effective ac-\ntions in the target environment. As illustrated in Figure 7, this agent\nrepresents the culmination of the LearnAct pipeline, integrating\nuser instructions, real-time GUI observations, and demonstration\nknowledge to navigate even unfamiliar mobile applications suc-\ncessfully. While DemoParser creates structured knowledge and\nKnowSeeker retrieves relevant demonstrations, ActExecutor ap-\nplies this knowledge to solve practical tasks, addressing the critical\nchallenge of knowledge utilization in few-shot learning scenarios.\nActExecutor implements the POMDP framework introduced ear-\nlier, with the critical enhancement of incorporating demonstration\nknowledge into the decision-making process. The execution pro-\ncess can be formally described as a sequential decision-making loop\nthat iteratively selects actions 𝑎𝑡 ∈ A based on current observa-\ntions 𝑜𝑡 ∈ O and demonstration knowledge D, following policy\n𝜋 : O × D → A.",
            "content": "The ActExecutor policy 𝜋 is implemented through large visionlanguage model that processes carefully constructed prompt integrating all available information sources. This prompt-based policy can be expressed as: 𝜋 (𝑜𝑡 , D) = 𝑓𝐿𝐿𝑀 (𝑃 (𝑖, 𝑜𝑡 , ℎ𝑡 1, D)) (4) where 𝑖 is the user instruction, 𝑜𝑡 is the current observation (screenshot), ℎ𝑡 1 is the action history up to time 𝑡 1, is the retrieved demonstration knowledge, 𝑃 is prompt construction function, and 𝑓𝐿𝐿𝑀 is the LLM-based decision function. Algorithm 2 in Appendix B.3 outlines the execution process. For each task, ActExecutor processes the user instruction and screenshot observations through sequence of perception, decision, and action phases until the task is completed or maximum step limit is reached. The execution process integrates three key phases: (1) Perception Phase: ActExecutor perceives the current state of the mobile device through screenshot observations 𝑜𝑡 . These observations provide the visual context essential for understanding the available interaction options and current application state. (2) Decision Phase: The agent constructs comprehensive prompt that integrates the user instruction 𝑖, current observation 𝑜𝑡 , action history ℎ, and retrieved demonstrations D. This prompt is processed by large vision-language model using templates detailed in Appendix B.2, resulting in selected action from the predefined action space described in Table 2. (3) Action Phase: The selected action 𝑎𝑡 is executed in the mobile environment, generating state transition according to the transition function of the POMDP. Additionally, the agent generates description 𝑑𝑡 of the executed action using process similar to DemoParsers description generation, which serves as part of the action history for subsequent steps. The prompt construction function 𝑃 plays critical role in ActExecutors effectiveness. It integrates the agents role definition, demonstration examples, task and observation context, action history, and the action space definition into comprehensive prompt that guides the models decision-making. This approach enables ActExecutor to leverage demonstrations as exemplars that guide its decision-making process. When faced with novel UI state, the agent identifies analogous situations from demonstrations and adapts the demonstrated actions to the current context. This capability is particularly valuable for handling out-ofdistribution scenarios where the agent lacks direct experience. By closing the loop between demonstration knowledge and task execution, ActExecutor completes the LearnAct frameworks endto-end pipeline for demonstration-based learning. The combination of knowledge generation (DemoParser), knowledge retrieval (KnowSeeker), and knowledge-guided execution (ActExecutor) enables effective few-shot learning for mobile GUI agents, addressing the fundamental challenge of generalization to unseen scenarios with minimal examples. 10 Liu et al."
        },
        {
            "title": "5.1 Experiment Setup\nThe diverse similarity profiles in LearnGUI provide a unique oppor-\ntunity to evaluate mobile GUI agents’ capabilities. Our experiments\nhave two primary goals: (1) to evaluate the feasibility and effective-\nness of enhancing mobile agents through few-shot demonstrations\nas a means to overcome the limitations of traditional pre-training or\nfine-tuning approaches; and (2) to investigate how different factors\nsuch as demonstration quantity (k=1,2,3) and various similarity\naspects (instruction, UI, and action) influence the effectiveness of\ndemonstration-based learning.",
            "content": "Implementation Details. We conducted experiments with three foundation models: Gemini-1.5-Pro [26], UI-TARS-7B-SFT [22], and Qwen2-VL-7B [31]. For all models, we set the temperature to zero to obtain deterministic responses. For Qwen2-VL-7B [31] and UI-TARS-7B-SFT [22], we employed parameter-efficient fine-tuning using LoRA with rank 64, alpha 128, and dropout probability 0.1. We targeted all modules while freezing the vision encoder to ensure computational efficiency. Training used learning rate of 1e-5 with cosine scheduling, batch size of 1, gradient accumulation over 8 steps, warmup ratio of 0.001, and was conducted for 1 epoch. All fine-tuning experiments were conducted on 8 NVIDIA L40S GPUs. For offline experiments, Gemini-1.5-Pro [26] was evaluated directly on the LearnGUI-Offline test set without additional training. UI-TARS-7B-SFT [22] and Qwen2-VL-7B [31] were fine-tuned on the LearnGUI-Offline training set before evaluation. For online experiments, we deployed all models except Gemini-1.5-Pro [26] (which showed limited task completion capabilities in preliminary tests despite accuracy improvements) to the LearnGUI-Online environment, using 1-shot demonstration retrieval for all LearnActenhanced models. Baselines. To rigorously evaluate our approach, we compared LearnAct against several baselines. These include: (1) SPHINXGUI Agent, the original agent developed for the AMEX dataset [5], providing reference point for task execution on similar data; (2) Zero-shot inference versions of all models (Gemini-1.5-Pro [26], UI-TARS-7B-SFT [22], and Qwen2-VL-7B [31]) within the LearnAct framework but without demonstration knowledge, maintaining identical execution environments for fair comparison; and (3) For online evaluation, we additionally compared against GPT-4o, Gemini-Pro-1.5, Claude Computer-Use, and Aguvis to benchmark against current advanced systems. Evaluation Metrics. For offline evaluation, we adopted mainstream evaluation protocols widely used in recent mobile GUI agent research, such as UI-TARS [22] and OS-ATLAS [39]. Specifically, we LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Table 4: Performance comparison of mobile GUI agents on LearnGUI-Offline dataset (action match accuracy %). Results show absolute values and relative improvements [in brackets] compared to baselines. Performance is evaluated across different models and number of support examples (1/2/3-shot). Models Method Supports Average Gmail Booking Music SHEIN NBC CityMapper ToDo Signal Yelp SPHINX-GUI Agent[5] AMEX gemini-1.5-pro UI-TARS-7B-SFT Qwen2-VL-7B Baseline LearnAct Baseline LearnAct Baseline LearnAct 0-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 67.2 19.3 51.7 [+32.4] 55.6 [+36.3] 57.7 [+38.4] 77.5 82.8 [+5.3] 81.9 [+4.4] 82.1 [+4.6] 71.8 77.3 [+5.5] 78.5 [+6.7] 79.4 [+7.6] 45.9 20.1 55.5 57.5 58.4 68.1 79.9 80.1 79.9 60.8 75.0 75.0 75.0 64. 16.4 47.1 53.2 56.6 81.0 82.9 80.7 80.9 73.9 77.5 78.0 78.8 74.4 24.5 60.0 55.3 54.6 81.1 86.6 86.2 86. 76.0 77.8 77.8 78.6 71.8 70.3 10.2 35.7 39.6 43.9 72.9 75.7 76.1 75.7 65.5 69.8 73.3 72. 35.6 56.4 56.1 53.9 80.9 86.3 87.2 86.9 75.5 83.5 86.0 87.8 67.4 14.1 54.7 58.2 69.4 70.6 79.4 80.0 81. 62.9 72.9 73.5 77.1 79.3 64.9 17.4 60.6 68.1 69.2 66.0 84.0 83.7 85.8 78.7 78.0 81.9 82. 27.9 63.1 69.7 70.5 92.6 89.3 84.4 84.4 82.8 83.6 87.7 87.7 66.3 15.2 54.6 60.0 57.6 82.4 83.0 84.2 84. 69.1 78.8 77.6 80.6 measured step accuracy, which consists of two components: action type accuracy and action match accuracy. Action type accuracy measures the percentage of steps where the predicted action type (CLICK, TYPE, SWIPE, etc.) matches the ground truth. Action match accuracy measures the percentage of steps where both the action type and its parameters are correct, following standard evaluation criteria. For CLICK actions, coordinates are considered correct if they fall within 14% of the screen width from the ground truth. For TYPE actions, the content is correct if the F1 score between prediction and ground truth exceeds 0.5. For SWIPE actions, the direction must precisely match the ground truth. For other actions (e.g., PRESS_BACK), an exact match is required. For TASK_COMPLETE actions, we only verify the action type and ignore the answer field. For online evaluation, we measured the task success rate (SR), which represents the percentage of tasks completed successfully in the real-time interactive environment."
        },
        {
            "title": "5.2 Main Results\n5.2.1 Offline Agent Capability Evaluation. Table 4 presents the\nperformance comparison of different models on the LearnGUI-\nOffline dataset. The results demonstrate the substantial improve-\nments achieved by the LearnAct framework across all tested models.\nGemini-1.5-Pro [26] shows the most dramatic improvement, with\nperformance increasing from 19.3% to 51.7% (+32.4%) with just\na single demonstration, and further improving to 57.7% (+38.4%)\nwith three demonstrations. This represents a 198.9% relative im-\nprovement, highlighting the powerful potential of demonstration-\nbased learning even for advanced foundation models. UI-TARS-\n7B-SFT [22], despite already having strong zero-shot performance\n(77.5%), still achieves significant gains with LearnAct, reaching\n82.8% (+5.3%) with a single demonstration. This indicates that even\nmodels specifically fine-tuned for GUI tasks can benefit from demon-\nstration knowledge. Qwen2-VL-7B [31] demonstrates consistent\nimprovement from 71.8% to 77.3% (+5.5%) with one demonstration,\nand to 79.4% (+7.6%) with three demonstrations, confirming that",
            "content": "11 the benefits of LearnAct generalize across models with different architectures and capabilities. The results also reveal interesting patterns regarding the impact of demonstration quantity. For Gemini-1.5-Pro [26], performance scales monotonically with the number of demonstrations, suggesting that less specialized foundation models can benefit substantially from additional examples. In contrast, UI-TARS-7B-SFT [22] achieves its peak performance with just one demonstration, indicating that models already fine-tuned for GUI tasks may efficiently extract necessary information from minimal demonstrations. Application-specific results highlight LearnActs consistent improvement across diverse scenarios, with particularly notable gains in complex applications like CityMapper (from 14.1% to 69.4% for Gemini-1.5-Pro [26]) and To-Do apps (from 17.4% to 69.2%). This suggests that demonstration-based learning is especially valuable for navigating applications with complex interactions and nonstandard interfaces. To further understand the factors influencing LearnActs effectiveness, we analyzed performance across different similarity profiles, as shown in Table 5. Several important insights emerge: Gemini-1.5-Pro [26] shows substantial improvements across all similarity combinations, with the largest gains in action match accuracy (ranging from +29.3% to +39.6%). This indicates that demonstration knowledge significantly enhances the models ability to execute precise actions regardless of similarity conditions. UI-TARS-7BSFT [22] exhibits the most pronounced improvements in UISHActSH scenarios (+13.9% with 3-shot), suggesting that the model can extract maximum value from demonstrations when both UI and action patterns are similar to the target task. Qwen2-VL-7B [31] shows notably large improvements in action type accuracy for 2-shot settings (e.g., +67.4% for UISHActSH), potentially indicating threshold effect where multiple demonstrations trigger significant pattern recognition improvements. Interestingly, while UI similarity generally correlates with higher performance gains, we observe that action similarity also plays Table 5: Performance breakdown of LearnAct-Offline on different UI and action combinations. Performance metrics (type and match accuracy) across four similarity quadrants showing absolute values and relative improvements [in brackets] compared to baselines. Results are grouped by model and number of support examples (1/2/3-shot). Models Supports UISHActSH UISHActSL UISLActSH UISLActSL Liu et al. gemini-1.5-pro Qwen2-VL-7B UI-TARS-7B-SFT 1-shot 2-shot 3-shot 1-shot 2-shot 3-shot 1-shot 2-shot 3-shot type match type match type match type match 79.5 [+12.8] 50.2 [+35.6] 78.1 [+12.3] 47.8 [+33.2] 77.5 [+9.2] 52.3 [+30.5] 77.9 [+14.1] 44.2 [+29.3] 77.7 [+13.0] 53.9 [+37.3] 73.2 [+10.8] 49.9 [+34.7] 80.0 [+9.0] 56.5 [+34.8] 77.2 [+12.9] 48.9 [+34.4] 72.3 [+15.8] 53.5 [+39.6] 72.8 [+12.9] 49.5 [+34.6] 78.7 [+10.4] 60.0 [+38.4] 79.2 [+12.8] 51.6 [+36.3] 72.2 [+6.3] 86.0 [+5.3] 69.4 [+4.3] 85.0 [+67.4] 75.6 [+9.3] 84.0 [+67.2] 71.2 [+5.7] 86.9 [+73.3] 76.8 [+6.3] 84.0 [+68.5] 70.5 [+5.5] 72.8 [+6.6] 80.2 [+5.0] 85.6 [+1.9] 82.9 [+4.7] 70.2 [+5.7] 85.4 [+4.9] 69.6 [+5.5] 86.0 [+2.0] 70.3 [+7.9] 77.5 [+8.4] 85.6 [+3.4] 82.9 [+1.3] 76.2 [+5.4] 87.2 [+2.1] 77.8 [+6.6] 88.1 [+1.9] 85.5 [+2.1] 85.7 [+1.6] 76.7 [+8.3] 87.1 [+7.9] 78.2 [+13.9] 85.5 [+2.6] 75.3 [+6.4] 75.9 [+4.9] 75.4 [+4.9] 87.7 [+0.3] 87.3 [-0.4] 86.0 [-0.9] 80.1 [+5.9] 79.1 [+5.9] 78.9 [+6.8] 85.0 [-0.2] 84.9 [-0.8] 85.5 [-0.9] 75.0 [+2.8] 74.1 [+2.1] 75.2 [+2.7] Table 6: Performance comparison of different models on the LearnGUI-Online benchmark. Comparison of models with different inputs (Image, Image+AXTree) and parameters, measuring task success rate (LearnGUI-OnlineSR) with improvements shown in brackets for models with LearnAct enhancement. Input Models # Params LearnGUI-OnlineSR Image + AXTree GPT-4o[12] Image + AXTree Gemini-Pro-1.5[26] Image Image Image Image Image Image Claude Computer-Use[2] Aguvis[41] Qwen2-VL-7B + 0-shot Qwen2-VL-7B + LearnAct UI-TARS-7B-SFT + 0-shot UI-TARS-7B-SFT + LearnAct - - - 72B 7B 7B 7B 7B 34.5 22.8 27.9 26.1 9.9 21.1 [+11.2] 18.1 32.8 [+14.7] crucial role. For instance, Gemini-1.5-Pro [26] achieves its highest match accuracy in UISLActSH scenarios (+38.4% with 3-shot), suggesting that action similarity can sometimes compensate for UI differences. This finding highlights the importance of considering both UI and action similarity when designing demonstration-based learning approaches for mobile GUI agents. These results validate our hypothesized framework design, demonstrating that LearnAct successfully leverages demonstration similarity to enhance performance across varying conditions, with the most substantial benefits observed when demonstrations can provide both perceptual and procedural knowledge relevant to the target task. 5.2.2 Online Agent Capability Evaluation. While offline evaluations provide valuable insights into step-by-step execution capabilities, real-world deployment requires successful end-to-end task completion. Table 6 presents the results of our online evaluation on the LearnGUI-Online benchmark, which reveals several important findings. The LearnAct framework substantially improves performance for both evaluated models, with Qwen2-VL-7B [31] improving from 9.9% to 21.1% (+11.2%) and UI-TARS-7B-SFT [22] from 18.1% to 32.8% (+14.7%). These significant gains demonstrate 12 that the benefits of demonstration-based learning translate effectively to real-world interactive scenarios. Qwen2-VL-7B [31] with LearnAct achieves 21.1% success rate, showing meaningful improvements over its baseline performance. This suggests that the quality and relevance of demonstrations are highly effective for enhancing model capabilities. UI-TARS-7B-SFT [22] with LearnAct achieves 32.8% success rate, approaching the performance of GPT4o (34.5%) despite using much smaller model. This indicates that demonstration-based learning can help bridge the gap between smaller specialized models and large foundation models. Detailed visualizations of these performance comparisons are provided in Appendix C.1.To provide concrete examples of how LearnAct performs in real-world scenarios, we present three detailed case studies in Appendix C.2. The most striking finding is the effectiveness of our demonstrationbased learning approach. The LearnAct framework provides significant performance improvements through its demonstration mechanism, with gains of up to 14.7% in task success rate. This demonstrates the power of high-quality demonstrations for enhancing model performance, highlighting the importance of relevant examples over simply increasing model size. These results confirm that the LearnAct framework provides practical pathway to developing effective mobile GUI agents, making it particularly valuable for application-specific customization and personalization scenarios."
        },
        {
            "title": "5.3 Ablation Study\nTo understand the contribution of each component in the LearnAct\nframework, we conducted ablation experiments on the LearnGUI-\nOffline dataset using Gemini-1.5-Pro [26]. As shown in Table 7,\nwe systematically evaluated the impact of removing either the\nDemoParser or KnowSeeker component while keeping all other\nsettings constant.",
            "content": "The results reveal several important insights. Both components are essential, as removing either component leads to substantial performance degradation compared to the full LearnAct framework. The complete framework achieves 51.7% accuracy, while removing DemoParser reduces performance to 40.6% (-11.1%) and removing LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Table 7: Ablation study of LearnAct components. Performance comparison across four configurations: baseline (no components), DemoParser only, KnowSeeker only, and both components combined. Results are presented as overall average accuracy and per-application breakdown across nine applications. Ablation Setting DemoParser KnowSeeker Average Gmail Booking Music SHEIN NBC CityMapper ToDo Signal Yelp Baseline 19.3 40.6 41.6 51.7 20.1 47.7 46.9 55. 16.4 31.3 34.1 47.1 24.5 55.4 52.7 60.0 10.2 29.1 27.9 35.7 35.6 47.0 51.9 56.4 14.1 43.0 45.3 54.7 17.4 58.2 51.4 60. 27.9 48.8 61.1 63.1 15.2 50.7 51.8 54.6 KnowSeeker reduces it to 41.6% (-10.1%). Regarding DemoParsers contribution, comparing \"KnowSeeker only\" (40.6%) to the baseline (19.3%), we observe that even without action descriptions, relevant demonstrations improve performance by 21.3%. However, the addition of DemoParsers action descriptions further enhances performance by 11.1%, confirming the value of structured knowledge extraction. For KnowSeekers contribution, the \"DemoParser only\" configuration (41.6%) also substantially outperforms the baseline, indicating that detailed action descriptions are valuable even with randomly selected demonstrations. However, KnowSeekers retrieval of relevant demonstrations provides an additional 10.1% improvement, highlighting the importance of demonstration relevance. The performance variations across applications are particularly informative. For instance, in the Signal application, DemoParser appears more important (61.1% vs. 48.8% for KnowSeeker only), suggesting that detailed action descriptions are crucial for applications with complex interaction patterns. Conversely, for the ToDo application, KnowSeeker seems more valuable (58.2% vs. 51.4% for DemoParser only), indicating that demonstration relevance may be more critical for applications with varied task types. These findings validate our multi-agent framework design, confirming that both knowledge extraction (DemoParser) and relevant demonstration retrieval (KnowSeeker) play complementary and essential roles in enabling effective demonstration-based learning for mobile GUI agents."
        },
        {
            "title": "6 DISCUSSION AND FUTURE WORK\nOur experimental results demonstrate that demonstration-based\nlearning significantly enhances mobile GUI agents’ capabilities. The\nsubstantial performance improvements across all evaluated mod-\nels validate our core hypothesis that demonstration-based learn-\ning effectively addresses generalization challenges. Even advanced\nfoundation models like Gemini-1.5-Pro [26] show dramatic im-\nprovements (198.9% relative improvement). Our multi-dimensional\nsimilarity analysis reveals that both UI similarity and action simi-\nlarity influence learning efficacy, with action similarity sometimes\ncompensating for UI differences.",
            "content": "Data Collection and Dataset Expansion. While our approach shows promising results, several limitations and future directions warrant consideration. First, regarding data collection, our current dataset, while comprehensive, could benefit from greater diversity and representativeness. The LearnGUI dataset, comprising 2,252 offline tasks and 101 online tasks, represents significant step forward but remains limited in scale compared to the vast diversity of mobile applications and user interactions. Future work should expand the dataset to include broader range of applications, particularly those with complex interaction patterns and specialized domains. K-shot Learning Analysis. Second, our current investigation of k-shot learning is limited to k=1, 2, and 3 demonstrations. While these configurations provide valuable insights, more comprehensive analysis of how demonstration quantity affects performance would be beneficial. Future research could explore the relationship between the number of demonstrations and performance gains, potentially identifying optimal demonstration counts for different scenarios and model architectures. Enhanced Learning and Execution Strategies. Third, our learning and execution strategies could be enhanced to better leverage the relationship between support tasks and query tasks. While our current approach effectively retrieves relevant demonstrations, more sophisticated methods could be developed to extract and transfer knowledge more efficiently. For instance, techniques for abstracting common patterns across demonstrations, identifying critical decision points, and adapting demonstrated strategies to novel scenarios could further improve performance. Agent Self-Learning. promising direction for future research is to enable agents to learn from their own successful executions. Currently, our framework relies exclusively on human demonstrations, but agents could potentially learn from their own successful task completions. By incorporating these successful agent executions into the knowledge base, we could enable form of \"selflearning\" where agents continuously improve their capabilities through their own experiences. By addressing these limitations and pursuing these research directions, demonstration-based learning can evolve into robust paradigm for developing adaptable, personalized, and practically deployable mobile GUI agents that effectively address the diverse needs of real-world users. The insights gained from our multidimensional similarity analysis provide valuable guidance for future research in this domain, suggesting that both UI similarity and action similarity play crucial roles in successful knowledge transfer."
        },
        {
            "title": "7 CONCLUSION\nThis paper introduces a novel demonstration-based learning para-\ndigm that fundamentally addresses the generalization challenges",
            "content": "13 faced by mobile GUI agents. Rather than pursuing universal coverage through ever-larger datasets, our approach leverages human demonstrations to enhance agent performance in unseen scenarios. We developed LearnGUI, the first comprehensive dataset for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further designed LearnAct, sophisticated multi-agent framework with three specialized components: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results demonstrate remarkable performance gains, with single demonstration increasing Gemini1.5-Pro [26]s accuracy from 19.3% to 51.7% in offline tests and enhancing UI-TARS-7B-SFT [22]s online task success rate from 18.1% to 32.8%. These findings establish demonstration-based learning as promising direction for developing more adaptable, personalized, and practically deployable mobile GUI agents. REFERENCES [1] Simone Agostinelli, Andrea Marrella, and Massimo Mecella. 2019. Research challenges for intelligent robotic process automation. In Business Process Management Workshops: BPM 2019 International Workshops, Vienna, Austria, September 16, 2019, Revised Selected Papers 17. Springer, 1218. [2] Anthropic. 2024. Developing computer use model. https://www.anthropic. com/news/developing-computer-use [3] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. 2021. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731 (2021). [4] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan Plummer. 2021. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. arXiv preprint arXiv:2104.08560 (2021). [5] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. 2024. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490 (2024). [6] Yuxiang Chai, Hanhao Li, Jiayu Zhang, Liang Liu, Guozhi Wang, Shuai Ren, Siyuan Huang, and Hongsheng Li. 2025. A3: Android Agent Arena for Mobile GUI Agents. arXiv preprint arXiv:2501.01149 (2025). [7] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. 2024. GUICourse: From General Vision Language Models to Versatile GUI Agents. arXiv preprint arXiv:2406.11317 (2024). [8] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022). [9] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935 (2024). [10] Tiago Guerreiro, Ricardo Gamboa, and Joaquim Jorge. 2008. Mnemonical body shortcuts: improving mobile interaction. In Proceedings of the 15th European conference on Cognitive ergonomics: the ergonomics of cool interaction. 18. [11] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1428114290. [12] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [13] Courtney Kennedy and Stephen Everett. 2011. Use of cognitive shortcuts in landline and cell phone surveys. Public Opinion Quarterly 75, 2 (2011), 336348. [14] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the Effects of Data Scale on Computer Control Agents. arXiv preprint arXiv:2406.03679 (2024). [15] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020. Mapping natural language instructions to mobile UI action sequences. arXiv preprint arXiv:2005.03776 (2020). [16] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. 2024. Showui: One visionlanguage-action model for gui visual agent. arXiv preprint arXiv:2411.17465 (2024). 14 Liu et al. [17] William Liu, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Shuai Ren, Xiaoyu Liang, Linghao Li, Wenhao Wang, et al. 2025. Llm-powered gui agents in phone automation: Surveying progress and prospects. (2025). [18] Zhe Liu, Cheng Li, Chunyang Chen, Junjie Wang, Boyu Wu, Yawen Wang, Jun Hu, and Qing Wang. 2024. Vision-driven Automated Mobile GUI Testing via Multimodal Large Language Model. arXiv preprint arXiv:2407.03037 (2024). [19] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024. GUI Odyssey: Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices. arXiv preprint arXiv:2406.08451 (2024). [20] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. 2024. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203 (2024). [21] Pawel Pawlowski, Krystian Zawistowski, Wojciech Lapacz, Marcin Skorupa, Adam Wiacek, Sebastien Postansque, and Jakub Hoscilowicz. 2024. TinyClick: Single-Turn Agent for Empowering GUI Automation. arXiv preprint arXiv:2410.11871 (2024). [22] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. 2025. UI-TARS: Pioneering Automated GUI Interaction with Native Agents. arXiv preprint arXiv:2501.12326 (2025). [23] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. 2024. AndroidWorld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573 (2024). [24] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems 36 (2024). [25] Yunpeng Song, Yiheng Bian, Yongtao Tang, and Zhongmin Cai. 2023. Navigating Interfaces with AI for Enhanced User Interaction. arXiv preprint arXiv:2312.11190 (2023). [26] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). [27] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. 2022. Ugif: Ui grounded instruction following. arXiv preprint arXiv:2211.07615 (2022). [28] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024. Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration. arXiv preprint arXiv:2406.01014 (2024). [29] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158 (2024). [30] Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen, and Shoufa Chen. 2024. MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents. arXiv preprint arXiv:2406.08184 (2024). [31] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191 (2024). [32] Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. 2024. GUI Agents with Foundation Models: Comprehensive Survey. arXiv preprint arXiv:2411.04890 (2024). [33] Wenhao Wang, Zijie Yu, William Liu, Rui Ye, Tian Jin, Siheng Chen, and Yanfeng Wang. 2025. FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced Data from Diverse Users. arXiv preprint arXiv:2502.02982 (2025). [34] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. 2025. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks. arXiv preprint arXiv:2501.11733 (2025). [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [36] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024. Autodroid: Llm-powered task automation in android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking. 543557. [37] Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. 2023. Droidbot-gpt: Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061 (2023). [38] Biao Wu, Yanda Li, Meng Fang, Zirui Song, Zhiwei Zhang, Yunchao Wei, and Ling Chen. 2024. Foundations and recent trends in multimodal mobile agents: survey. arXiv preprint arXiv:2411.02006 (2024). [39] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. 2024. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark (2024). [40] Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. 2024. AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents. arXiv preprint arXiv:2410.24024 (2024). [41] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. 2024. Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction. arXiv preprint arXiv:2412.04454 (2024). [42] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems 36 (2024). [43] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. 2024. Large Language Model-Brained GUI Agents: Survey. arXiv preprint arXiv:2411.18279 (2024). [44] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 (2023). [45] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713 (2024). [46] Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, and Jianping Fan. 2024. MobileExperts: Dynamic Tool-Enabled Agent Team in Mobile Devices. arXiv preprint arXiv:2407.03913 (2024). [47] Li Zhang, Shihe Wang, Xianqing Jia, Zhihan Zheng, Yunhe Yan, Longxi Gao, Yuanchun Li, and Mengwei Xu. 2024. LlamaTouch: Faithful and Scalable Testbed for Mobile UI Automation Task Evaluation. arXiv preprint arXiv:2404.16054 (2024). [48] Zhuosheng Zhang and Aston Zhang. 2023. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436 (2023). 15 ADDITIONAL LEARNGUI STATISTICS Figure 8 illustrates the distribution of similarity scores across different dimensions in the LearnGUI-Offline dataset, enabling systematic analysis of how different types of similarity between demonstration and query tasks affect learning efficacy. Liu et al. Figure 8: Distribution of instruction, UI, and action similarity scores in LearnGUI-Offline. The histograms show the distribution of similarity scores across three dimensions: instruction similarity (top), UI similarity (middle), and action similarity (bottom). These distributions enable systematic analysis of how different types of similarity between demonstration and query tasks affect learning efficacy. LEARNACT FRAMEWORK DETAILS This section provides detailed descriptions of the components of our LearnAct framework, corresponding to the methods presented in Section 4 of the paper. B.1 DemoParser Prompts We provide all of our prompt templates used in DemoParser for generating semantically descriptive action descriptions from demonstration data. These carefully designed prompts guide the vision-language model to produce structured knowledge that captures the essence of human demonstrations, as shown in Figures 9 and 10. 16 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Prompt 1: Intermediate Action Description System Prompt: You are mobile UI interaction analyst. Follow these rules: 1. Analyze the split-screen image (Before-action left, After-action right) 2. For click actions, high-contrast red marker (white-bordered circle) shows the precise click location, with green square surrounding it and label at the top-right corner of the square indicating the click. 3. Output JSON with ONLY ONE action_description field in this exact format: \"[On/In] [Screen Name], [Action Details], to [Purpose]\" Action Types: - click [element] (e.g., Search button) - swipe [up/down/left/right] - type [text] in [field] - press [back/home/enter] Validation Rules: 1. Screen names should be 2-6 words 2. Keep purpose descriptions under 8 words 3. Never mention coordinates/IDs MEMORY RECORDING RULES: If the current screen contains information relevant to the users instruction that needs to be remembered for future steps, include Memory part in your action description. The format should be: \"[On/In] [Screen Name], [Action Details], to [Purpose]. [Memory: important information for future steps]\" Memory should ONLY be added when: 1. The information is relevant to completing the users instruction 2. The information will likely be needed in future steps 3. This specific information has NOT been recorded in previous action history entries Memory examples: 1. For travel planning task: On Travel Blog, click Bali Beach Guide, to read article. [Memory: Guide mentions Kuta Beach has surfing lessons for $25/hour] 2. For shopping task: In Product Details, click Add to Cart, to select item. [Memory: iPhone 13 Pro costs $999 with 128GB storage] 3. For note-taking task: On Weather App, swipe down forecast, to view weekend. [Memory: Saturday will be rainy with 80% precipitation] Avoid using Memory for: 1. Obvious UI changes that dont contain task-relevant information 2. Information already captured in previous action steps 3. Generic observations not specific to the users task objective Figure 9: Prompt template for intermediate action descriptions. The template guides DemoParser to generate standardized descriptions for intermediate actions, including detailed rules for memory annotations that capture important information observed during task execution. B.2 ActExecutor Prompts We provide the prompt templates used by ActExecutor to make decisions based on current observations, action history, and demonstration knowledge. These prompts guide the vision-language model to select appropriate actions for task execution, as shown in Figure 11. B.3 Algorithm Details We provide the detailed algorithms for the DemoParser and ActExecutor components of our LearnAct framework, which are the core computational processes enabling knowledge extraction and task execution. ADDITIONAL EXPERIMENTAL RESULTS AND ANALYSES This section provides additional experimental results and analyses that supplement the findings presented in Section 5 of the paper. C.1 Online Performance Comparisons Figures 12 and 13 provide detailed comparisons of model performance with and without LearnAct enhancement in online evaluation scenarios. C.2 Case Studies of LearnAct Online Experiments We present three detailed case studies from our online experiments to provide concrete examples of how LearnAct leverages demonstration knowledge to solve tasks in unseen mobile applications. These case studies highlight different scenarios where demonstration knowledge proves particularly beneficial for task execution. 17 Liu et al. Prompt 2: Terminal Action Description - Standard Completion System Prompt for standard completion: Determine the final task status. Output rules: 1. Use ONLY ONE action_description field 2. Format: \"[On/In] [Screen], complete task, [Reason]\" Validation Rules: - Reason should be less than 10 words - Screen name must match previous context Examples: 1. Basic completion: On Payment Screen, complete task, successfully submit order 2. Failure case: In Search Results, cannot complete task, no nearby Vivo mobile phone stores found Prompt 3: Terminal Action Description - With Answer System Prompt for completion with answer: Determine the final task status with the given answer. Output rules: 1. Use ONLY ONE action_description field 2. Format: \"[On/In] [Screen], complete task, the answer is [answer]\" Validation Rules: - Screen name must match previous context - Use the exact answer provided in the TASK_COMPLETE action Examples: 1. Answer is price: On Checkout Screen, complete task, the answer is \"$299.9\". 2. Answer is list: On Payment Options Screen, complete task, the answer is \"google pay, check out with affirm, add credit/debit card\". Figure 10: Prompt templates for terminal action descriptions. The templates provide specific formats for both standard task completion and information retrieval tasks, ensuring consistent output structure across different task types. Algorithm 1 DemoParser Knowledge Generation Process Require: Demonstration dataset 𝐷 = {(𝑖𝑘, 𝑠𝑘, 𝑎𝑘 )𝑁 𝑘=1 Ensure: Knowledge base 𝐾 with semantically descriptive action descriptions } where 𝑖𝑘 is instruction, 𝑠𝑘 is screenshot sequence, 𝑎𝑘 is action sequence 1: 𝐾 2: for each demonstration (𝑖, 𝑠, 𝑎) in 𝐷 do 3: 𝑑 for 𝑗 = 1 to 𝑎 do if 𝑗 < 𝑎 then Initialize empty knowledge base Initialize empty description sequence Intermediate action Create visualization of action 𝑎 𝑗 with before-after screenshots from 𝑠 𝑗 and 𝑠 𝑗+1 ℎ Previous action descriptions {𝑑1, 𝑑2, . . . , 𝑑 𝑗 1} 𝑑 𝑗 GenerateDescription(𝑖, 𝑎 𝑗 , 𝑣𝑖𝑠𝑢𝑎𝑙𝑖𝑧𝑎𝑡𝑖𝑜𝑛, ℎ) using prompt format detailed in Appendix B.1 𝑑 𝑗 follows format: \"[On/In] [Screen], [Action], to [Purpose]\" with optional memory else ℎ Complete action history {𝑑1, 𝑑2, . . . , 𝑑 𝑎 1} 𝑑 𝑎 GenerateFinalDescription(𝑖, 𝑠 𝑎 , ℎ, 𝑎 𝑎 ) using prompt detailed in Appendix B.1 𝑑 𝑎 follows format: \"[On/In] [Screen], complete task, [Reason/Answer]\" end if Add 𝑑 𝑗 to description sequence 𝑑 Terminal action 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Add (𝑖, 𝑎, 𝑑) to knowledge base 𝐾 17: 18: end for 19: return 𝐾 18 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Prompt 4: Task Execution Prompt Role Definition: You are smartphone assistant to help users complete tasks by interacting with apps. will give you screenshot of the current phone screen. Example Tasks: [Only when demonstrations are available] Example 1: [Demonstration instruction] Steps taken in this example: Step-1: [Action] [Action Description] Step-2: [Action] [Action Description] ... Background: This image is phone screenshot. Its width is [width] pixels and its height is [height] pixels. The users instruction is: [instruction] History operations: [Only when action history is available] Before reaching this page, some operations have been completed. You need to refer to the completed operations to decide the next operation. These operations are as follow: Step-1: [Action] [Action Description] Step-2: [Action] [Action Description] ... Response requirements: Now you need to combine all of the above to decide just one action on the current page. You must choose one of the actions below: \"SWIPE[UP]\": Swipe the screen up. \"SWIPE[DOWN]\": Swipe the screen down. \"SWIPE[LEFT]\": Swipe the screen left. \"SWIPE[RIGHT]\": Swipe the screen right. \"CLICK[x,y]\": Click the screen at the coordinates (x, y). is the pixel from left to right and is the pixel from top to bottom \"TYPE[text]\": Type the given text in the current input field. \"PRESS_BACK\": Press the back button. \"PRESS_HOME\": Press the home button. \"PRESS_ENTER\": Press the enter button. \"TASK_COMPLETE[answer]\": Mark the task as complete. If the instruction requires answering question, provide the answer inside the brackets. If no answer is needed, use empty brackets \"TASK_COMPLETE[]\". Response Example: Your output should be string and nothing else, containing only the action type you choose from the list above. For example: \"SWIPE[UP]\" \"CLICK[156,2067]\" \"TYPE[Rome]\" \"PRESS_BACK\" \"PRESS_HOME\" \"PRESS_ENTER\" \"TASK_COMPLETE[1h30m]\" \"TASK_COMPLETE[]\" Figure 11: Task execution prompt template. This comprehensive prompt directs ActExecutor to generate actions based on current observations, action history, and retrieved demonstrations, with explicit formatting requirements to ensure consistent action outputs. Algorithm 2 ActExecutor Task Execution Process Require: User instruction 𝑖, Knowledge base 𝐾, Maximum steps 𝑇 Ensure: Task execution trajectory 1: 𝑡 0 2: ℎ 3: KnowSeeker(𝑖, 𝐾) 4: while 𝑡 < 𝑇 and not IsTaskComplete do 5: 𝑜𝑡 GetObservation() 𝑃𝑡 ConstructPrompt(𝑖, 𝑜𝑡 , ℎ, D) 𝑎𝑡 𝑓𝐿𝐿𝑀 (𝑃𝑡 ) 𝑑𝑡 GenerateDescription(𝑖, 𝑎𝑡 , 𝑜𝑡 , ℎ) ℎ ℎ {(𝑎𝑡 , 𝑑𝑡 )} ExecuteAction(𝑎𝑡 ) 𝑡 𝑡 + 11: 12: end while 13: return {(𝑎0, 𝑑0), (𝑎1, 𝑑1), . . . , (𝑎𝑡 1, 𝑑𝑡 1)} 6: 7: 8: 9: 10: Initialize time step Initialize action history Retrieve relevant demonstrations Obtain current screenshot Construct decision prompt Generate action via LLM Generate action description Update action history Execute action in environment Increment time step 19 Liu et al. Figure 12: Detailed performance comparison of Qwen2-VL-7B with and without LearnAct on LearnGUI-Online. The figure shows the task success rates of Qwen2-VL-7B baseline versus Qwen2-VL-7B enhanced with LearnAct across different task dimensions in the LearnGUI-Online benchmark. Figure 13: Detailed performance comparison of UI-TARS-7B-SFT with and without LearnAct on LearnGUI-Online. The figure presents comprehensive breakdown of task success rates for UI-TARS-7B-SFT baseline versus UI-TARS-7B-SFT enhanced with LearnAct across multiple task dimensions in the LearnGUI-Online benchmark. 20 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Figure 14: UI-TARS-7B-SFT with LearnAct vs. Baseline in NotesRecipeIngredientCount Task. Task template: \"What quantity of {ingredient} do need for the recipe {title} in the Joplin app? Express your answer in the format <amount> <unit> without using abbreviations.\" 21 Liu et al. Figure 15: UI-TARS-7B-SFT with LearnAct vs. Baseline in SimpleCalendarDeleteOneEvent Task. Task template: \"In Simple Calendar Pro, delete the calendar event on {year}-{month}-{day} at {hour}h with the title {event_title}\" 22 LearnAct: Few-Shot Mobile GUI Agent with Unified Demonstration Benchmark Figure 16: Qwen2-VL-7B with LearnAct vs. Baseline in ExpenseDeleteMultiple Task. Task template: \"Delete the following expenses 23 from arduia pro expense: {expenses}.\""
        }
    ],
    "affiliations": [
        "Zhejiang University Hangzhou, China",
        "vivo AI Lab Hangzhou, China",
        "vivo AI Lab ShenZhen, China"
    ]
}