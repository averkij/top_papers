{
    "paper_title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
    "authors": [
        "Stephen E. Farr",
        "Stefan Doerr",
        "Antonio Mirarchi",
        "Francesc Sabanes Zariquiey",
        "Gianni De Fabritiis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] - c . s [ 1 1 8 5 0 0 . 1 0 6 2 : r AceFF: State-of-the-Art Machine Learning"
        },
        {
            "title": "Potential for Small Molecules",
            "content": "Stephen E. Farr, Stefan Doerr, Antonio Mirarchi, Francesc Sabanes Zariquiey, and Gianni De Fabritiis,, Acellera Labs, Dr Trueta 183, 08005, Barcelona, Spain Computational Science Laboratory, Universitat Pompeu Fabra, Barcelona Biomedical Research Park (PRBB), Dr. Aiguader 88, 08003, Barcelona, Spain Acellera, 38350 Fremont Blvd 203 Fremont CA, 94536 USA Institucio Catalana de Recerca Estudis Avancats (ICREA), Passeig Lluis Companys 23, 08010 Barcelona, Spain E-mail: g.defabritiis@gmail.com Abstract We introduce AceFF, pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as eﬃcient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains diﬃcult. AceFF addresses this via reﬁned TensorNet2 architecture trained on comprehensive dataset of drug-like compounds. This approach yields force ﬁeld that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF 1 establishes new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0."
        },
        {
            "title": "Introduction",
            "content": "Atomistic simulations are fundamental tools for understanding materials and molecular systems at the atomic level. The accuracy and utility of these simulations critically depend on the choice of force ﬁeld. Classical molecular mechanics (MM) force ﬁelds such as GAFF, 1,2 CGenFF, 3,4 OpenFF, 5,6 and AMBER 7 oﬀer high computational eﬃciency, but often fall short in predictive accuracy, particularly for diverse, drug-like molecules containing rare functional groups or where quantum eﬀects and polarization play signiﬁcant roles. Firstprinciples quantum mechanical (QM) methods, such as density functional theory (DFT), provide substantially higher accuracy but are prohibitively expensive for routine biomolecular simulations. Machine learning interatomic potentials (MLIPs) oﬀer promising alternative, achieving near-QM accuracy at fraction of the computational cost: an order of magnitude slower than MM methods, while being many orders of magnitude faster than QM approaches. 8 The current state-of-the-art MLIP architectures in terms of accuracy include equivariant message-passing graph neural network models such as MACE, 12 Nequip, 13 AIMNet2, 14 and TensorNet, 15 to name few. 16 One of the most used models to date is ANI-2x, 17 which employs less accurate invariant architecture but is one of the most computationally eﬃcient MLIPs available. newer and more accurate group of pre-trained models are the MACEOFF23 models with large, medium, and small variants available. These are signiﬁcantly more accurate than ANI-2x, with the downside of being substantially slower. 18 Other pretrained models include: AIMNet2 14 which supports charged molecules, AceFF-1.0 19 based on the TensorNet architecture 20 supporting charge and spins, the Egret models, which are based on the MACE architecture; 21 the recent OrbMol; 2224 and UMA-OMol25 models. 25 2 OrbMol and UMA are both trained on the recent OMol25 dataset 25 and cover most of the periodic table, while being accurate in standard benchmarks, they are slow relative to more specialized models. It is currently not feasible to perform production MD using full MLIP description of the entire molecular system. Instead, it is necessary to use hybrid MLIP/MM schemes, where only the ligand is modeled with the MLIP, and the remainder of the system uses classical MM force ﬁeld. It has been found that even with simple mechanical embedding approach, MLIP/MM can provide increased accuracy for RBFE calculations. 26,27 In our previous work, we were limited by the fact that ANI-2x supports only eight elements and only neutral molecules. This led us to create our own QM dataset and develop our ﬁrst MLIP, AceFF-1.0, 19 which we applied in large-scale RBFE benchmark, ﬁnding improved accuracy compared to traditional force ﬁelds and previous ANI-2x results. 27 AceFF-1.1 included larger dataset and was released on HuggingFace. 28 Both of these models are freely available under the Apache 2.0 license for academic and commercial use. In this paper, we report the latest update to our model family, AceFF-2, which includes both more training data and an improved model architecture, TensorNet2. Additionally, we describe the various benchmarks we use to quickly assess model accuracy, making comparisons in accuracy with the existing MLIPs ANI-2x, 17 AIMNet2, 14 OrbMol; 22,23 semi-empirical methods g-XTB, 29 GFN2-XTB; 30 and the MM forceﬁeld GAFF2. 1,2 In summary, AceFF-2 oﬀers accuracy higher than even slower models, reaching state-of-the-art on standard benchmarks, at speed. Because of this, we believe that it has immediate applications in drug discovery."
        },
        {
            "title": "2.1 TensorNet2 architecture",
            "content": "AceFF-1.x models were based on TensorNet 15 models with charge and spin encodings. 20 With AceFF-2, we introduce modiﬁed version of TensorNet with improved charge handling. The 3 approach we take closely follows that of AIMNet2: 14 we include additional scalar features to mimic partial charges, perform neutral charge equilibration on them, and use them in long-range Coulomb energy term. We name this version TensorNet2. An overview of the model architecture is shown in ﬁgure 1. Figure 1: The AceFF-2 architecture TensorNet2 builds upon TensorNet 15 with the new components colored in blue. The main change is that after the tensor embedding, and each tensor interaction block, set of partial charges and weights are computed and undergo neutral charge equilibration (NQE) procedure as done in AIMNet2. 14 The partial charges are then combined with edge features during the tensor interaction. The predicted charges are used to compute Coulomb energy ECoulomb alongside the standard short range node-wise MLIP energy term Ei. The dashed arrow represents where iteration occurs for more than one interaction layer. The pictured diagram corresponds to TensorNet2 1-layer. The grey dashed circles around the example molecule illustrate the short cutoﬀs used in the tensor embedding and interaction. The blue dashed circle represents the long range of the total charge (a global feature feature), NQE block, and the Coulomb interaction. From an input of atomic numbers and coordinates Zi,Ri, TensorNet learns set of 3x3 matrices (rank-2 tensors) for each atom (i) which are equivariant to rotation and translation. At each message-passing step, the (i) are updated based on the neighboring atoms, making use of the fact that (i) can be decomposed into scalar, vector, and symmetric traceless components, I, A, S, respectively. After several message passing updates, the squared Frobenius norms of the representations, which are invariant under O(3), are further processed by the neural network to predict energies, obtaining atomic forces via automatic diﬀerentiation. Moreover, distinctive feature of the TensorNet message-passing architec4 ture, speciﬁcally when interaction layers are active, is its natural transition from O(3) to SO(3) equivariance. This reduction in symmetry follows directly from how tensortensor interaction products are computed in the interaction layer, which mix even and odd parities and therefore break reﬂection equivariance while preserving full rotational equivariance. This behavior is especially valuable in drug discovery, where chirality is central: enantiomers are related by reﬂections but often diﬀer dramatically in binding aﬃnity and biological activity. 31 In the previous version of TensorNet, 20 total molecular charge was included in simple manner with no extra learned parameters. While this method eﬀectively resolved the degeneracy problem for molecules sharing the same coordinates and atomic numbers but diﬀering in Q, we observed limitations during extrapolation. Speciﬁcally, when tested on set of larger charged molecules, the performance was signiﬁcantly surpassed by the AIMNet2 architecture, 14 which explicitly models and learns partial charges and Coulomb interactions. We incorporate the AIMNet2-like neutral charge equilibration into TensorNet as follows, which is the main contribution to TensorNet2 new architecture. At each message passing layer, we introduce charge prediction neural network that accepts the node features (I i, Ai, Si) as input and outputs set of charge predictions (qi) and corresponding set of weights ( wi). It is important to note the inclusion of feature dimension (a hyperparameter) to increase expressivity. These outputs are not required to represent physical charges but rather scalar features that we equate to partial charge hypotheses (similar to the approach in 32). neutral charge equilibration step is then performed independently across each feature channel to ensure that the partial charges sum to the total molecular charge (Q). qi = qi wi wj X qj , ! (1) which is the same as in. 14 The qi from each node are then used in the next message passing 5 update by extending the scalar edge features such that Equation 11 in 15 becomes: I, i A, S = ϕ(rij) SiLU MLP eRBF(rij) qi qj , (2) (cid:16) (cid:0) (cid:1)(cid:17) where denotes tensor concatenation. The predicted qi from all layers are subsequently used to compute Coulomb interactions, which is performed independently across each feature channel. The weighted mean of the resulting Coulomb energy is then calculated, where the weights are assigned such that the output from the ﬁnal layer has the most signiﬁcant impact on the total energy. Utilizing the charge predictions from the earlier layers in the ﬁnal output calculation eﬀectively serves as regularization technique. This procedure constitutes type of self-consistent method and is fundamentally diﬀerent in style from alternative charge inclusion techniques such as the latent Ewald summation method, 33 which calculates an Ewald summation or Coulomb-like interaction using node features derived from just the ﬁnal node features. Another is the 4G-HDNNP approach, which employs neural networks to predict electronegativity and hardness. This prediction is followed by full charge equilibration scheme, which involves solving set of linear equations to ﬁnd the set of partial charges that minimizes the electrostatic energy subject to the total charge constraint, which scales poorly with system size. The TensorNet2 neutral charge equilibration approach adds only small overhead to training and inference speed. For example, 1-layer TensorNet model with an embedding dimension of 128 and 32 radial basis functions has 535,681 parameters, while 1-layer TensorNet2 model with the same embedding dimension, radial basis functions, and additionally charge dimension of 16 has 685,413 parameters. The inference speeds of each model for 1500 atom system (500 water molecules) are 35 steps/second and 32 steps/second, respectively. parameter increase of 28% and speed decrease of under 10%. As part of the TensorNet2 development, we optimized the tensor embedding and tensor interaction steps of both TensorNet and TensorNet2, resulting in speed increase of approxi6 mately 30% and reduction in memory usage of approximately 30%. The key change was to maintain I, A, and as scalar, vector, and tensor-shaped arrays rather than the 33 tensors used in the original code. We also re-implemented the custom CUDA neighbor list in Triton to ease development and deployment, also achieving minor speed increase. For example, for 1,000 atoms, the Triton neighbor list code has latency of 0.0136 ms, while the CUDA code has latency of 0.0194 ms. It should be noted that full TensorNet evaluation has latency of at least 10 ms for 1,000 atoms; therefore, the neighbor list speeds and diﬀerences between them are negligible (see Methods)."
        },
        {
            "title": "2.2 AceFF-2 trained model",
            "content": "We trained 2-layer version of TensorNet2 on an internal QM dataset. We used charge embedding dimension of 16, hidden dimension of 128, and 32 radial basis functions. The model was trained using the torchmd-train scripts that can be found in TorchMD-Net. 35 We refer to this model as AceFF-2. The model was trained on energy and force labels, without using any partial charge or dipole moment data. Optionally, one of the TensorNet2 charge channels could be trained on QM level speciﬁc type of partial charge, e.g. MBIS, to facilitate an approach such as. 36 Indeed, we expect the diﬀerent charge channels could be trained simultaneously on multiple QM-derived partial chargesfor example, the Omol25 dataset contains Mulliken and Lowdin charges. 25 For ablation purposes, we also trained 1-layer version with all other hyperparameters unchanged."
        },
        {
            "title": "2.3 Accuracy on torsion scans",
            "content": "The ﬁrst benchmark test we performed was Sellers et al torsion scan. 37 This benchmark data set contains torsion scans of 62 molecules done with diﬀerent levels of QM. We take the most accurate method from the set, CCSD(T)/CBS, and use this as the reference to compare against. For all molecules in the test, we performed relaxed torsion scans (as done in 10,38). For comparison we did the same procedure with our AceFF-1.0, ANI-2x 17 from Figure 2: Torsion scans. a. Sellers et al. torsion scan benchmark. 37 The orange lines are the median value. The methods are ordered from left to right by median MAE compared to the coupled cluster reference data. The y-axis has been intentionally capped at 2.0 kcal/mol to focus on the diﬀerences between the MLIP models. An image of one of the molecules with the torsion angle indicated is pictured. Values marked by * were taken from previous publication. 27 All other data points were recalculated. b. Behara et al torsion scan benchmark. 41 The orange lines are the median value. The methods are ordered from left to right by median MAE compared to the coupled cluster reference data. The scatter points are color-coded by the molecular charge. The y-axis has been intentionally capped at 4.0kcal/mol to focus on the diﬀerences between the MLIPs. Data points of methods marked with * were taken from the original data, 41 all other data points were recalculated. On both plots, guide line has been drawn at 1 kcal/mol. torchani, 39 AIMNet2, 14 and the recent OrbMol-conservative. 2224 We also did the scan with the semi-empirical method GFN2-XTB 40 and the molecular mechanics force-ﬁeld GAFF. 1,2 We excluded g-XTB because it does not have analytical gradients available yet. The MAE of each torsion scan relative to the reference scan is plotted in 2a as box-plots with all data points shown. We see that OrbMol is the most accurate model, with AceFF-2 close second. From left to right, the other MLIPs perform similarly until ANI-2X, which is signiﬁcantly worse. GFN2-XTB performs poorly compared to the MLIPs, and as expected, GAFF has very large errors. We also performed second torsion scan benchmark from Behara et al. 41 This torsion 8 scan contains charged molecules, speciﬁcally [0,-1,+1]. The reference torsion drives were done with MP2/heavy-aug-cc-pVTZ, and the reference single point energies were labeled with CCSD(T)/CBS. The benchmark diﬀers from the Sellers et al torsion scan in that we do not perform any geometry optimizations. This benchmark tests only the energy error of the MLIP and not the force calculations. We did the single point calculations for OrbMol, AceFF-2, AceFF-1.0, AIMNet2, GFN2-XTB, and GAFF. We took the single point results for MACE-OFF23 and the DFT method wB97M-V/dzvp from 41 for extra comparison. The MAE of each method is plotted in ﬁgure 2b. The trend between models is approximately the same as the Sellers et al torsion scan. As expected, the biggest outliers in AceFF-1.0 are the charged molecules, and similarly for MACE-OFF23 which was designed for only neutral molecules. The DFT method ωB97M-V/DZVP is less accurate than the MLIP OrbMol; however, OrbMol was trained on OMol25, which used ωB97M-V/TZVPD. The larger basis set oﬀers increased accuracy. AceFF and MACE-OFF23 are also trained on DFT data using the same functional and larger basis set, but they do not reach the same accuracy. This can be explained by the compounding eﬀects of smaller dataset (by factor of more than 10x) and smaller model architecture when compared to OrbMol."
        },
        {
            "title": "2.4 Accuracy on strained conformers: Wiggle150",
            "content": "The Wiggle150 benchmark 42 consists of 150 highly strained conformations of adenosine, benzylpenicillin, and efavirenz, with reference energies calculated using DLPNO-CCSD(T)/CBS level of theory. To evaluate performance, we used the MLIP model to predict the relative energies (relative to the minimized geometry) for all 150 conformations. The resulting mean absolute error (MAE) and mean absolute deviation (MAD) were then computed compared to the reference values. Table 1 summarizes the Wiggle150 results for ANI-2x, g-XTB, AceFF-1.0, AceFF-2, AIMNet2, and OrbMol. Data for ANI-2x, AIMNet2, and GFN2-XTB was taken from the original Wiggle150 publication, 42 while the remaining values were computed in-house using 9 Table 1: Performance of MLIPs, and one semi-empirical method, and one MM force-ﬁeld on the Wiggle150 benchmark 42 Method MAE (kcal/mol) RMSE (kcal/mol) GAFF GFN2-XTB ANI-2X g-XTB AceFF-1.0 AIMNet2 AceFF-2 OrbMol 22.87 14.6 4.41 3.85 2.73 2.39 1.76 0.89 30.86 15.2 5.41 4.76 3.32 3.13 2.34 1.22 their respective publicly available models. OrbMol exhibits the highest accuracy. Notably, all MLIPs (excluding ANI2x) outperform the semi-empirical g-XTB method, despite g-XTB being designed to achieve similar chemical accuracy level (ωB97M-V). Furthermore, AceFF2 shows signiﬁcant improvement over AceFF-1.0."
        },
        {
            "title": "2.5 Drug-like molecules: Schr¨odinger ligands",
            "content": "We created test set that evaluates how well the model generalizes to larger, previously unseen molecules. These ligands are all larger than 30 atoms, which is the maximum size included in the AceFF DFT training dataset. To create the Schrodinger ligand test set, we took the ligand structures from 43 available at https://github.com/schrodinger/public_ binding_free_energy_benchmark and labeled them with forces and energies computed at the AceFF level of theory (ωB97M-V/def2-TZVPPD). All ligands from the Merck, JACS, and charge-annil sets were included. We used the conformations directly from the provided SDF ﬁles and performed DFT calculations with the same settings used to create the AceFF DFT dataset. Any conformations that failed to converge were discarded, resulting in ﬁnal set of 650 conformers. We predicted the force and energy using AceFF-1.0, AceFF-2, AIMNet2, and OrbMol. For each method, we computed the force MAE per conformer and the energy absolute error. It is important to note that the energy is the total DFT energy; as 10 such, we only expect the AceFF models, which are trained on the same level of theory DFT, to have energy errors close to zero. The AIMNet2, and OrbMol models were trained on data generated by slightly diﬀerent functionals, basis sets, and software. However, the force errors should be comparable between the models, as this is an intrinsic physical property. Therefore, we only show energy errors for the AceFF models. The results are plotted in Figure 3a and 3b for AceFF-1.0 and AceFF-2, respectively, showing the force and energy errors. We have plotted guide lines at 0.05eV/A and 0.05eV, which indicate the approximate validation L1 force and energy loss that the models converge to. We see that, as expected, AceFF-1.0 performs very badly on charged molecules. AceFF-2 performs much better with the majority of force errors below 0.05eV/A. The energy errors are not below 0.05 eV; however, these molecules are mostly larger (range of 27 to 74 atoms) than any it was trained on. The force errors for all models are plotted in ﬁgure 3c. AIMNet2 has consistent performance between charged and neutral molecules, but is systematically above 0.05ev/A. OrbMol performs exceptionally well with force errors all below 0.03eV/A. We note that OrbMol was trained on the OMol25 dataset, which does contain molecules of this size (and may well contain these molecules in the training split). It is interesting to see that the most extreme outliers for AIMNet2 are neutral molecules."
        },
        {
            "title": "2.6 Potential energy smoothness",
            "content": "A smooth potential energy surface is crucial for stable molecular dynamics simulations. To assess model smoothness, we analyzed the potential energy scan as the C-C bond in an ethane molecule was compressed and extended. We compressed the bond to an unphysically small distance and extended it beyond the bond-breaking regime to examine how the MLIPs behave in these regions outside the training domain. We compared the MLIP results with those from DFT using ωB97m-V/tzvppd. The bond energy scans are plotted in Figure 4, where they have all been shifted such that the minimum energies are zero. 11 Figure 3: Schrodinger ligand test set evaluation. a. Energy error vs force error for AceFF-1.0, color-coded by ligand total charge. b. Energy error vs force error for AceFF-2, color-coded by ligand total charge. The axis range is the same in sub-plots and to aid comparison. The inset graph in sub-plot shows zoomed-in view of the data points. c. Force errors for all tested models. The orange line is the median value. The data points are color-coded by charge. 12 Figure 4: Potential energy of ethane C-C bond length scan for diﬀerent MLIPs compared to reference of DFT with ωB97m-V/tzvppd. All models exhibit similar behavior in the minima. At small distances, we observe the nuclear repulsion in the DFT curve, with the same feature visible in the OrbMol curve (notably, this model includes ZBL repulsive term). For the other MLIPs, the nuclear repulsion is modeled entirely by the neural network terms (and the data upon which they were trained). The energy barrier in ANI2x is less sharply deﬁned. In contrast, the AceFF-2 energy matches the DFT energy up to large energy of 40 eV. At the bond-stretching end of the curve, we observe that ANI2x signiﬁcantly overestimates the bond strength and OrbMol exhibits an unphysical step change between 5 and 6A, which is perhaps an artifact of the MLIP cutoﬀs. AIMNet2 and AceFF-2 have similar proﬁles to each other and to the DFT reference. 2."
        },
        {
            "title": "Inference speed scaling with molecular dynamics",
            "content": "We compared model speed by running molecular dynamics on water boxes of increasing size. When running MLIPs in this way, it is possible to turn on aggressive compilation settings because certain tensors will have ﬁxed shapes and/or values, e.g. the number of atoms will be constant, the number of edges can be set to constant, and the atomic numbers will always be the same. We used the Atomic Simulation Environment 44 (ASE) calculator implementations of each model. For OrbMol, compile=True is used; for AceFF models, we use torch.compile(mode=reduce-overhead), which enables CUDA graphs; for ANI-2x, we use the cuaev extension from the latest TorchANI2.0; 45 for the MACE-MPA-0 model, we used CuEquivarience plugin; and for AIMNet2, we used both the oﬃcial torch-scripted models and our modiﬁed version that is compatible with torch.compile and CUDA graphs. These are the fastest settings that the models currently have available. In this work, we are primarily concerned with simulating small drug-like molecules or small MLIP/MM region of protein ligand complex. No models are currently fast and accurate enough for full system bio-molecular simulation (> 10, 000 atoms) of any useful timescale. Therefore, we restricted our speed testing to limit of 3000 atoms (1000 water molecules). The time per step of all models as function of system size is shown in Figure 5. An important point to note is that only AceFF and AIMNet2-CUDA graphs utilize CUDA graphs, which are essential for low latency in small systems. 35 To make the plots clearer, we have plotted the speed curves in two regimes. In subplot Figure 5a, we see the small system behavior with below 300, and in subplot Figure 5b, we see the full range of up to 3000. There is clear latency limit for very small for all models except AceFF and AIMNet2-CUDA graphs the CUDA graph compatibility explained before is the primary reason they can reach near 1ms latency. OrbMol, ANI, and standard AIMNet2 are all limited to >10ms latency, while the MACE-MPA-A model has even larger latency limits. Once is large enough, the models scale as expected: Orb, AceFF-1.0, and MACE are linear; AceFF-2 and AIMNet2 are quadratic (due to their 2 Coulomb interaction implementations). ANI2X appears to have no clear speed decrease and is limited by latency for all we are looking at. This follows the speed scaling reported in TorchANI2.0, 45 where much larger system sizes are reached. We also tested the speed of batched molecular dynamics. To do this, we selected small 14 Figure 5: Inference speed of diﬀerent models vs number of atoms. a. Zoomed in on the x-axis to highlight speed for small number of atoms. b. Full range of tested atom sizes. Guidelines have been drawn which indicate where 1, 10, and 100 ns/day speeds are if simulation was run with 1fs timestep. Methods marked with an * have been run with CUDA graphs enabled. Aimnet2-CUDA graphs marked with runs out of memory with more than 1500 atoms. This is due to the 2 implementation of the CUDA graph-compatible code version. molecule (imatinib, 68 atoms) and generated conformers. We then ran Langevin dynamics, using the integrator from TorchMD, 46 in batched manner, such that all conformers were integrated simultaneously. TorchMD is molecular dynamics engine entirely written in PyTorch, which allows for easy integration and batching of molecular simulations using MLIPs and the Amber potential. This allowed the AceFF MLIP to be called once per timestep for the entire batch. The total time per step is plotted in ﬁgure 6a for increasing batch size. We compare against the serial OpenMM simulation speed, i.e., the time to sequentially run timestep for each conformer individually. We also compute the eﬀective total throughput in ns/day for the batched simulations, comparing against the OpenMM simulation throughput of running single simulation. These results are plotted in ﬁgure 6b. We see that for 100 conformers, the batched simulations are approximately 90% faster. The diﬀerence is not as dramatic as may be expected, this is because the OpenMM simulations with AceFF-2 can be run with CUDA graphs, which removes nearly all the latency limits 15 Figure 6: Batched molecular dynamics with AceFF-2. a. The total time per step for the corresponding number of conformers in batched mode (blue line) and in serial with OpenMM (black line). b. The eﬀective total throughput in ns/day for batched mode (red line) and using in serial using OpenMM (black line). that batching typically overcomes."
        },
        {
            "title": "2.8 MLIP/MM molecular dynamics stability and speed",
            "content": "A current use-case for MLIPs is in mixed MLIP/MM simulations where small region is modeled with the MLIP and the rest of the system with traditional MM forceﬁeld. 26,27,47 The simplest approach is the mechanical embedding approach, where just the ligand intramolecular forces are computed with the MLIP and the rest of the system uses the MM forceﬁeld. This accounts for the corrections due to ligand strain. To test that AceFF-2 is appropriate for this, we ran simulation of the ejm31 ligand bound to Tyk2, which is one of the systems studied in 27 and from , 43 the complex is shown in ﬁgure 7a. We used the OpenMM-ML package 48 to run the simulations with HMR of 4au, temperature of 300K, and Langevin thermostat. We ran the simulations for 100ns with timestep of 1fs and timestep of 2fs. The ligand RMSDs are plotted in 7b and the center of mass distance between the ligand and protein pocket are plotted in 7c. We observe that both 2fs and 1fs are stable. The simulation speeds on an RTX4090 are 36.7 ns/day and 16 Figure 7: Ligand RMSD of NVT molecular dynamics of protein-ligand complex using ML/MM. a. Visualization system, the MLIP region has been highlighted and water has been omitted for clarity. b. The RMSD of the ligand is plotted for the 1fs and 2fs timestep AceFF-2 simulations in blue and orange, respectively. c. The ligand-pocket center of mass distance is plotted for the 1fs and 2fs timestep AceFF-2 simulation in blue and orange, respectively. 72.6 ns/day, respectively. We also tested the speed of the same simulation using AceFF1.0, ANI2x, and MACE-MPA-0 using the implementations available in OpenMM-ML. The speeds for the tested models are shown in table 2 for simulations run with 1fs timestep. Table 2: Simulation speed of MLIP/MM using OpenMM-ML with 1fs timestep. AceFF models marked with * utilize CUDA graphs Speed on RTX4090 (ns/day) ANI-2x AceFF-1.0* AceFF-2* MACE-MPA-0 59.1 63.8 36.7 4.2 We see that the AceFF-1.0 speed of 63.8ns/day is slightly faster than the ANI-2x speed of 59.1 while it is signiﬁcantly faster than the MACE model. AceFF-2, which includes an additional message-passing layer, charge prediction heads, and explicit Coulomb terms, is less than factor 2 slower than AceFF-1.0. It is important to note that only the TensorNetbased AceFF models are compatible with CUDA graphs (and have CUDA graphs enabled in 17 this test), which we have previously found is essential for low-latency simulations. The code changes to make model CUDA graph compatible are non-trivial. Additionally, to interface seamlessly with OpenMM for MLIP/MM simulations, model must be implemented in the OpenMM-ML package. Currently, only ANI-2x and MACE implementations have been released. Since OpenMM-ML strictly requires TorchScript compatibility, we cannot test the OrbMol model without making non-trivial code changes."
        },
        {
            "title": "2.9 Batched conformer minimization",
            "content": "A common use case of molecular force ﬁelds is to energy minimize molecular conformations. To assess the ability of the MLIPs to minimize diverse array of molecules relevant for drug discovery, we tested them on the Platinum Diverse dataset, 49 which contains 2859 crystallographic structures of ligand conformations from protein ligand complexes. To perform the benchmark, we took the reference conformer from the dataset and minimized it with the MLIP. It is expected that the minimization should converge, and that the reference structure is close to local minima of the MLIP force ﬁeld, such that after minimization, the RMSD between the reference and the minimized structure should be small. We tested AceFF-2, AIMNet2, and OrbMol; and excluded ANI2x due to the large number of charged molecules present in the dataset and elements outside the ANI2x subset. For this test, we used the ASE LBFGS minimizer and the ASE calculator interfaces of the MLIP models. The number of failed minimizations and the values for the RMSD mean, median, and percentages under 1A, 2A, 3A, and 10A are shown in table 3. The histograms of the RMSDs are shown in ﬁgure 8. For consistent comparison, the mean, median, and standard deviation are taken over molecules where all methods succeeded (N=2850). For AceFF-2, we found that two molecules failed to minimize, resulting in unphysical exploded structures. These are molecules with index 1653 (name: GVF 2UVM A) and index 1816 (name: JD5 4NFK F). Molecule 1653 has net charge of -8 due to four -2 18 Table 3: Platinum Diverse geometry optimization benchmark results. The summary values describe the RMSDs between the initial reference structure and the geometry-optimized structure for the three tested MLIPs. The statistics are taken over the subset where all methods had no failures (N=2850). MLIP AceFF-2 AIMNet2 OrbMol mean 0.552 0.508 0.534 std median %<1A %<2A %<3A %<10A Number failed 0.446 0.426 0.432 0.419 0.388 0.400 86.29 89.75 86.92 98.43 98.60 98. 99.72 99.62 99.86 99.93 99.72 100.00 2 8 0 phosphate groups; similarly, molecule 1816 has net charge of -3 and two phosphate groups. Images of these molecules are shown in ﬁgure 8b and 8c. The AceFF dataset contains molecules with net charge up to -2 to 2, so these speciﬁc molecules are signiﬁcantly outside the domain of knowledge of the model, and it is not surprising that it fails to correctly model them. The next largest RMSD is 3.9 angstrom for molecule 1928, while not exploded, the conformation is in an elongated state compared to the reference and an atom has changed bond topology. This molecule has net charge of -4, so once again outside the training domain. An image of this molecule is shown in ﬁgure 8d. For AIMNet2, we found that 8 molecules exploded with RMSDs greater than 10. The molecule indexes are [259, 1160, 1468, 1648, 1816, 2189, 2328, 2803], all contain the same large net charge and phosphate groups that AceFF struggled with. OrbMol succeeds at minimizing all conformers; this can be attributed to its much larger and diverse training dataset (OMol25 is an order of magnitude larger than AceFF-2 and AIMNet2 datasets). AIMNet2 has the smallest mean and median, suggesting its local minima are the closest to the reference geometries. Looking at the distributions, we see no major diﬀerences. Two-sided Kolmogorov-Smirnov test (N=2850) between the three pairs is: AceFF-2 vs AIMNet2: statistic=0.0505, p-value=0.00138; AceFF-2 vs OrbMol: statistic=0.0347, p-value=0.0642; AIMNet2 vs OrbMol: statistic=0.0368, p-value=0.0418. Therefore, the AceFF-2 and OrbMol results are not signiﬁcantly diﬀerent, while the AIMNet2 results have maximal diﬀerence of 5%. Orb and AceFF-2 are trained on the same level of Figure 8: Platinum Diverse geometry optimization benchmark. a. The histogram of the RMSDs between the initial reference structure and the geometry-optimized structure for the three tested MLIPs for all conformers in the Platinum Diverse set. 49 b. Molecule index 1653 has net charge of -8 due to four phosphate groups. When minimized with AceFF-2 the phosphates are ejected. c. Molecule index 1816 has net charge of -3 and two phosphate groups, when minimized with AceFF-2 one of the phosphates is ejected. d. Molecule index 1928 has the largest RMSD (3.9 A) for AceFF-2, it has net charge of -4, upon minimization with AceFF-2 there is change in bond topology as highlighted by the red circles. DFT, so this is expected. An important point to note is that the reference geometries are from protein-ligand complexes; the in-vacuum local minima that the MLIPs are optimizing to will be diﬀerent from the bound minima. We also assessed the speed of optimizing multiple conformers. The beneﬁt of MLIPs is that batching calculations is trivial using the existing optimizers available in PyTorch. Helpfully, LBFGS, one of the standard methods for geometry optimization, is available. We recorded the time taken to minimize conformers of Imatinib, drug molecule with 68 atoms. 50 We varied from 1 to 100 and put all conformers into the same batch. For comparison, we timed ASE LBFGS minimization (for the same number of steps, in this case 400) for one conformer, and scaled this linearly by to get the ASE serial speed baseline. The measured timings are shown in Figure 9. The batched speed is signiﬁcantly faster for more than 10 conformers. In the supplementary information, we verify that LBFGS from PyTorch correctly minimizes atomic conﬁgurations in comparable way to ASEs LBFGS. 20 Figure 9: Comparison of batched conformer minimization speed with torch.optim.LBFGS and ASE serial minimization of the same number of conformers of an Imatinib molecule."
        },
        {
            "title": "3 Conclusion",
            "content": "We have presented AceFF-2, next-generation neural network potential tailored for the efﬁcient and accurate simulation of small drug-like molecules. Building upon robust DFT dataset, this model leverages the TensorNet2 architecture with charge embedding to deliver near-quantum mechanical accuracy while maintaining the speed required for routine molecular dynamics workﬂows. We evaluated AceFF-2 using comprehensive set of benchmarks, comparing it against other MLIPs and traditional methods. In the current MLIP landscape, ANI-2x 17 represents one extreme, oﬀering the fastest speed but with limited elemental coverage and restriction to neutral species. OrbMol 2224 represents the other extreme, oﬀering high accuracy and coverage of the full OMol25 dataset, spanning most of the periodic table and diverse charge states. AceFF-2 occupies strategic position between these extremes on the Pareto front; it creates balance similar in speed and applicability to AIMNet2, 14 yet oﬀers superior accuracy. The TensorNet2 architecture further extends the expressivity of TensorNet and remains one of the fastest equivariant MLIPs available. The models we have primarily compared 21 against are invariant (OrbMol, ANI2x, AIMNet2), these are design choices that may have be inﬂuenced by the typically higher speed of invariant models 23 when compared to equivariant ones. The only other equivariant model we test is MACE-MPA-0 which is the slowest. We have not investigated direct force prediction MLIPs (i.e., where forces are output in the forward pass and not by back-propagation) as the inherently non-conservative nature adds signiﬁcant complexity in using them for stable MD simulations. 51 AceFF-2 demonstrates substantial improvements over traditional molecular mechanics force ﬁelds and our models predecessors. Importantly, it supports all standard drug-relevant elements and exhibits strong transferability to larger, out-of-distribution molecules. The model produces stable molecular dynamics trajectories for both in-vacuum simulations and hybrid MLIP/MM protein-ligand systems. AceFF-2 is available to the community, supported by code and tutorials to enable rapid adoption within both the ASE and OpenMM-ML frameworks. We anticipate that AceFF-2 will facilitate more accurate and computationally tractable molecular simulations, particularly within drug discovery and chemical research, by providing reliable alternative to classical force ﬁelds and computationally expensive quantum calculations. Furthermore, the benchmarks established in this work provide the community with rigorous new protocol for testing MLIPs."
        },
        {
            "title": "4.1 Dataset",
            "content": "We built an in-house dataset of DFT data for drug-like small molecules, comprising approximately 2 million unique molecules from PubChem 52 with 12 million conformations. The elements included are H, B, C, N, O, F, Si, P, S, Cl, Br, and I. Molecule sizes are up to 30 atoms. Charge values include [-2,-1,0,1,2]. The DFT level of theory is ωB97M-V/def2-TZVPPD. 53 PySCF 54 and the plugin GPU4PySCF 55,56 was used. The dataset includes energy-minimized structures and high-energy geometries. One reason for creating this dataset was the inadequacy of the existing datasets at the time, notably, they did not contain the higher energy structures we believed were necessary for stable MLIPs. We note that OMol25 25 has recently been released, which contains 100M conformations, marking step change in the availability of molecular DFT data."
        },
        {
            "title": "4.2 Models and force ﬁelds",
            "content": "We used ANI2x 17 from TorchANI. 39 For all evaluations excluding the OpenMM-based ones, we used the latest TorchANI-2 45 (version 2.6) For the OpenMM-based simulation, we used an older version (2.2.4,) which is compatible with the NNPOps 57 (version 0.6) package. For OrbMol-conservative 2224 we used version 0.5.5 and all calculations were done via the ASE calculator interface. In the text, we refer to the model as OrbMol-conservative and OrbMol interchangeably. We only ever use the conservative model. For GFN2-XTB 30 we used version 22.1 from the xtb-python package and used the ASE calculator interface for all evaluations. For g-XTB 29 we used the preliminary version 1.1.0 https://github.com/ grimme-lab/g-xtb. We used GAFF 1,2 version 2.2 via the OpenMM force-ﬁelds package and did all evaluations with OpenMM 48 version 8.4. For MACE-MPA-0 we used mace-torch version 0.3.14 and the MPA-0 foundation model. 58 We used ASE 44 version 3.26. We used OpenMM-ML version 1.2, OpenMM-Torch version 1.5, and OpenMM version 8.4. Some models have speciﬁc limitations and were therefore excluded from certain tests. ANI2x supports only elements H, C, N, O, S, F, and Cl. g-XTB lacks analytical gradients, so we did not use it for tests requiring force calculations. OrbMol is not TorchScript compatible, so we could not use it with the OpenMM/OpenMM-Torch/OpenMM-ML interface. Additionally, the current OpenMM-ML 1.2 release implements only ANI2x and MACE models, so AIMNet2 could not be tested yet. Finally, the MACE-MPA-0 model is designed for materials rather than biomolecules; as its accuracy on these benchmarks would be insuﬃcient for meaningful comparison, we included it only in the speed benchmarks."
        },
        {
            "title": "4.3 Torsion scans",
            "content": "For the Sellers et al torsion scan, we performed relaxed torsion scans. We used the GeomeTRIC 59 package to perform geometry optimizations with the constrained torsion angles. We took the CCSD(T)/CBS conformation as the starting point for each optimization. To compare with the coupled cluster reference, we computed the relative energy of the torsion scan compared to the minimum energy along the scan. To report single number from each torsion scan, we computed the mean absolute error of the scan from the reference scan (MAE). For the Behara et al torsion scan we followed the protocol and scripts provided in: 41 We evaluated the single-point energy of each geometry to calculate the torsion scans. For each torsion scan, we then computed the MAE compared to the reference scan as the diﬀerence in the relative energies."
        },
        {
            "title": "4.4 Molecular dynamics",
            "content": "For the molecular dynamics in section 2.7, we used the ASE dynamics method. The water boxes were created with OpenMMs addSolvent functionality. The MD was run with an artiﬁcially small timestep and 100 steps. The timings were recorded every 10 steps and the ﬁrst values were omitted to ignore the overhead of any PyTorch compilation warmup. The simulations were run on an RTX4090. For the batched molecular dynamics, we used modiﬁed version of the Langevin Integrator from TorchMD. 46 For the MLIP/MM molecular dynamics in section 2.8 we used customized version of the OpenMM-ML package along with OpenMM-Torch version 1.5 and OpenMM version 8.4 on an RTX4090. The simulations were run for 100ns with snapshots saved every 10000 steps. The LangevinMiddle integrator was used with frequency of 1/ps, temperature of 300K, HMR of 4au, and timestep of either 1fs or 2fs. For ANI-2x we employed NNPOps. 57 The AceFF simulation was run using torchmd-net conﬁgured with max-num-neighbors=32, and CUDA graphs enabled. For MACE-MPA-0 we used the NNPOps neighbor list."
        },
        {
            "title": "4.5 Updated Triton neighbor list code",
            "content": "In Pelaez et al. 35 we introduced custom CUDA kernels for neighbor list construction. This faster code adds signiﬁcant complexity to the software build and deployment process, as diﬀerent versions need to be compiled and released for various PyTorch, Python, CUDA, and platform versions. To ease the development burden, we re-implemented the neighbor list code in Triton, which oﬀers the advantage of being Python-based code that is compiled at runtime by PyTorch. We retained only the brute and cell-list implementations; the brute-force approach is fastest for small systems, while the cell-list is fastest for large systems. We compared the speed of the original CUDA and new Triton versions of both, focusing on batch size of 1 with an increasing number of atoms. We found that the Triton version is slightly faster. current limitation of the Triton code is that it cannot yet be exported to TorchScript, which is the primary way to use MLIPs in OpenMM. Therefore, we also implemented fallback pure-Python neighbor list implementation. The time per step for an increasing number of atoms is plotted in Figure 10. We see that for fewer than 1,000 atoms, all methods exhibit under 1ms latency; for comparison, 1-layer TensorNet has latency of at least 20ms for 1,000 atoms. This is an order of magnitude faster. Eﬀectively, neighbor list construction is not bottleneck for the small MLIP systems we are interested in. For large systems, the Triton cell-list code oﬀers good performance (2ms latency for 100,000 atoms) and could easily be used in other models targeted at these larger system sizes."
        },
        {
            "title": "4.6 Software and data availability",
            "content": "TensorNet2 is available under the MIT license at https://github.com/torchmd/torchmd-net. We have provided set of example IPython notebooks (which can be run on Google Colab) and Python package containing code to enable using AceFF as an ASE (Atomic Simulation Environment 44) calculator and in OpenMM-ML 48 as an ML potential. The package and example notebooks can be found on the GitHub repository https://github. com/Acellera/aceff_examples. The AceFF models can be downloaded from Hugging25 Figure 10: Neighbor list speed comparison. The CUDA kernel neighbor list speeds are plotted in blue while the Triton neighbor list speeds are plotted in red. The brute methods are plotted with crosses and the cell-list methods are plotted with circles. The PyTorch baseline is plotted in black, which uses the pure Python fallback code (utilized when the model needs to be torch-scripted). guideline is plotted at 0.864ms, which corresponds to molecular dynamics speed of 100ns/day at 1fs. Face. 28 tutorial to use the models for ML/ML simulations can be found at https: //software.acellera.com/acemd/nnpmm.html. Our changes to AIMNet2 for CUDA-graph compatibility can be found in PR in the main repository https://github.com/isayevlab/aimnetcentral. The dataset for the Wiggle150 benchmark can be found at. 42 The dataset for the Sellers et al torsion scan can be found at. 37 The structures for the Schrodinger Ligand set can be found at 43 and https://github.com/schrodinger/public_binding_free_energy_ benchmark. The Behara et al torsion scan data can be found at. 41 The Platinum Diverse dataset can be found at."
        },
        {
            "title": "Author contributions",
            "content": "S.E.F. and G.D.F. wrote the manuscript. S.E.F. implemented the TensorNet2 architecture. S.E.F., S.D., and A.M. developed the AceFF models and datasets. S.E.F and F.S.Z per26 formed the model benchmarks. G.D.F directed the work."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors thank the volunteers of GPUGRID.net for donating computing time. This project has received funding from the Torres-Quevedo Program from the Spanish National Agency for Research (PTQ2023-012967/AEI/10.13039/501100011033). AM is ﬁnancially supported by Generalitat de Catalunyas Agency for Management of University and Research Grants (AGAUR) PhD grant 2025 FI-2-00278."
        },
        {
            "title": "Competing interests",
            "content": "The authors declare the following competing ﬁnancial interest(s): S.E.F, S.D, F.S.Z, and G.D.F. have potential conﬂict of interest due to direct interests in Acellera (See aﬃliations)."
        },
        {
            "title": "References",
            "content": "(1) Wang, J.; Wolf, R. M.; Caldwell, J. W.; Kollman, P. A.; Case, D. A. Development and testing of general amber force ﬁeld. Journal of computational chemistry 2004, 25, 11571174. (2) Wang, J.; Wang, W.; Kollman, P. A.; Case, D. A. Automatic atom type and bond type perception in molecular mechanical calculations. J. Mol. Graphics Modell. 2006, 25, 247260. (3) Vanommeslaeghe, K.; Hatcher, E.; Acharya, C.; Kundu, S.; Zhong, S.; Shim, J.; Darian, E.; Guvench, O.; Lopes, P.; Vorobyov, A., Igor MacKerell Jr CHARMM general force ﬁeld: force ﬁeld for drug-like molecules compatible with the CHARMM all-atom additive biological force ﬁelds. Journal of computational chemistry 2010, 31, 671690. 27 (4) Vanommeslaeghe, K.; Raman, E. P.; MacKerell Jr, A. D. Automation of the CHARMM General Force Field (CGenFF) II: assignment of bonded parameters and partial atomic charges. Journal of chemical information and modeling 2012, 52, 31553168. (5) Qiu, Y.; Smith, D. G.; Boothroyd, S.; Jang, H.; Hahn, D. F.; Wagner, J.; Bannan, C. C.; Gokey, T.; Lim, V. T.; Stern, C. D.; others Development and benchmarking of open force ﬁeld v1. 0.0the parsley small-molecule force ﬁeld. Journal of chemical theory and computation 2021, 17, 62626280. (6) Boothroyd, S.; Behara, P. K.; Madin, O. C.; Hahn, D. F.; Jang, H.; Gapsys, V.; Wagner, J. R.; Horton, J. T.; Dotson, D. L.; Thompson, M. W.; others Development and benchmarking of open force ﬁeld 2.0. 0: the Sage small molecule force ﬁeld. Journal of chemical theory and computation 2023, 19, 32513275. (7) Maier, J. A.; Martinez, C.; Kasavajhala, K.; Wickstrom, L.; Hauser, K. E.; Simmerling, C. ﬀ14SB: improving the accuracy of protein side chain and backbone parameters from ﬀ99SB. J. Chem. Theory Comput. 2015, 11, 36963713. (8) Behler, J.; Parrinello, M. Generalized Neural-Network Representation of HighDimensional Potential-Energy Surfaces. Physical Review Letters 2007, 98, 146401. (9) Bartok, A. P.; Payne, M. C.; Kondor, R.; Csanyi, G. Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons. Physical Review Letters 2010, 104, 136403. (10) Smith, J. S.; Isayev, O.; Roitberg, A. E. ANI-1: an extensible neural network potential with DFT accuracy at force ﬁeld computational cost. Chemical Science 2017, 8, 3192 3203. (11) Unke, O. T.; Chmiela, S.; Sauceda, H. E.; Gastegger, M.; Poltavsky, I.; Schutt, K. T.; Tkatchenko, A.; Muller, K.-R. Machine Learning Force Fields. Chemical Reviews 2021, 121, 1014210186. 28 (12) Batatia, I.; Kovacs, D. P.; Simm, G. N. C.; Ortner, C.; Csanyi, G. MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields. 2023; http://arxiv.org/abs/2206.07697, arXiv:2206.07697. (13) Batzner, S.; Musaelian, A.; Sun, L.; Geiger, M.; Mailoa, J. P.; Kornbluth, M.; Molinari, N.; Smidt, T. E.; Kozinsky, B. E(3)-equivariant graph neural networks for dataeﬃcient and accurate interatomic potentials. Nature Communications 2022, 13, 2453. (14) Anstine, D.; Zubatyuk, R.; Isayev, O. AIMNet2: neural network potential to meet your neutral, charged, organic, and elemental-organic needs. ChemrXiv preprint chemrxiv-2023-296ch-v3 2024, (15) Simeon, G.; De Fabritiis, G. TensorNet: Cartesian Tensor Representations for Eﬃcient Learning of Molecular Potentials. Advances in Neural Information Processing Systems. 2023; pp 3733437353. (16) Duval, A.; Mathis, S. V.; Joshi, C. K.; Schmidt, V.; Miret, S.; Malliaros, F. D.; Cohen, T.; Lio, P.; Bengio, Y.; Bronstein, M. Hitchhikers Guide to Geometric GNNs for 3D Atomic Systems. arXiv preprint arXiv:2312.07511 2023, (17) Devereux, C.; Smith, J. S.; Huddleston, K. K.; Barros, K.; Zubatyuk, R.; Isayev, O.; Roitberg, A. E. Extending the Applicability of the ANI Deep Learning Molecular Potential to Sulfur and Halogens. Journal of Chemical Theory and Computation 2020, 16, 41924202, PMID: 32543858. (18) Kovacs, D. P.; Moore, J. H.; Browning, N. J.; Batatia, I.; Horton, J. T.; Kapil, V.; Witt, W. C.; Magdau, I.-B.; Cole, D. J.; Csanyi, G. MACE-OFF23: Transferable machine learning force ﬁelds for organic molecules. arXiv preprint arXiv:2312.15211 2023, (19) Acellera AceFF-1.0 on HuggingFace. https://huggingface.co/Acellera/AceFF-1. 0, 2024; Accessed: 2025-02-24. 29 (20) Simeon, G.; Mirarchi, A.; Pelaez, R. P.; Galvelis, R.; De Fabritiis, G. Broadening the Scope of Neural Network Potentials through Direct Inclusion of Additional Molecular Attributes. Journal of Chemical Theory and Computation 2025, 21, 18311837. (21) Mann, E. L.; Wagen, C. C.; Vandezande, J. E.; Wagen, A. M.; Schneider, S. C. Egret-1: Pretrained Neural Network Potentials For Eﬃcient and Accurate Bioorganic Simulation. 2025; https://arxiv.org/abs/2504.20955. (22) Materials, O. OrbMol. https://huggingface.co/Orbital-Materials/OrbMol, 2025; Hugging Face repository. Accessed: 2025-11-17. (23) Rhodes, B.; Vandenhaute, S.; ˇSimkus, V.; Gin, J.; Godwin, J.; Duignan, T.; Neumann, M. Orb-v3: atomistic simulation at scale. 2025; https://arxiv.org/abs/2504. 06231. (24) Neumann, M.; Gin, J.; Rhodes, B.; Bennett, S.; Li, Z.; Choubisa, H.; Hussey, A.; Godwin, J. Orb: Fast, Scalable Neural Network Potential. 2024; http://arxiv. org/abs/2410.22570, arXiv:2410.22570. (25) Levine, D. S.; Shuaibi, M.; Spotte-Smith, E. W. C.; Taylor, M. G.; Hasyim, M. R.; Michel, K.; Batatia, I.; Csanyi, G.; Dzamba, M.; Eastman, P.; Frey, N. C.; Fu, X.; Gharakhanyan, V.; Krishnapriyan, A. S.; Rackers, J. A.; Raja, S.; Rizvi, A.; Rosen, A. S.; Ulissi, Z.; Vargas, S.; Zitnick, C. L.; Blau, S. M.; Wood, B. M. The Open Molecules 2025 (OMol25) Dataset, Evaluations, and Models. 2025; https: //arxiv.org/abs/2505.08762. (26) Sabanees Zariquiey, F.; Galvelis, R.; Gallicchio, E.; Chodera, J. D.; Markland, T. E.; De Fabritiis, G. Enhancing ProteinLigand Binding Aﬃnity Predictions Using Neural Network Potentials. Journal of Chemical Information and Modeling 2024, 64, 1481 1485. 30 (27) Sabanes Zariquiey, F.; Farr, S. E.; Doerr, S.; De Fabritiis, G. QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations Using Neural Network Potentials. Journal of Chemical Information and Modeling 2025, 65, 40814089. (28) Acellera AceForce-1.1 on HuggingFace. https://huggingface.co/Acellera/ AceFF-1.1, 2025. (29) Froitzheim, T.; Muller, M.; Hansen, A.; Grimme, S. g-xTB: General-Purpose Extended Tight-Binding Electronic Structure Method For the Elements to Lr (Z=1103). 2025; https://chemrxiv.org/engage/chemrxiv/article-details/ 685434533ba0887c335fc974. (30) Bannwarth, C.; Ehlert, S.; Grimme, S. GFN2-xTBAn Accurate and Broadly Parametrized Self-Consistent Tight-Binding Quantum Chemical Method with Multipole Electrostatics and Density-Dependent Dispersion Contributions. Journal of Chemical Theory and Computation 2019, 15, 16521671. (31) H. Brooks, W.; C. Guida, W.; G. Daniel, K. The signiﬁcance of chirality in drug design and development. Current topics in medicinal chemistry 2011, 11, 760770. (32) Ple, T.; Adjoua, O.; Benali, A.; Posenitskiy, E.; Villot, C.; Lagard`ere, L.; Piquemal, J.- P. Foundation Model for Accurate Atomistic Simulations in Drug Design. ChemRxiv 2025, (33) Cheng, B. Latent Ewald summation for machine learning of long-range interactions. npj Computational Materials 2025, 11, 80. (34) Ko, T. W.; Finkler, J. A.; Goedecker, S.; Behler, J. fourth-generation highdimensional neural network potential with accurate electrostatics including non-local charge transfer. Nature Communications 2021, 12, 398. 31 (35) Pelaez, R. P.; Simeon, G.; Galvelis, R.; Mirarchi, A.; Eastman, P.; Doerr, S.; Tholke, P.; Markland, T. E.; De Fabritiis, G. TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations. Journal of Chemical Theory and Computation 2024, (36) Semelak, J. A.; Pickering, I.; Huddleston, K.; Olmos, J.; Grassano, J. S.; Clemente, C. M.; Drusin, S. I.; Marti, M.; Gonzalez Lebrero, M. C.; Roitberg, A. E.; Estrin, D. A. Advancing Multiscale Molecular Modeling with Machine Learning-Derived Electrostatics. Journal of Chemical Theory and Computation 2025, 21, 51945207. (37) Sellers, B. D.; James, N. C.; Gobbi, A. comparison of quantum and molecular mechanical methods to estimate strain energy in druglike fragments. Journal of chemical information and modeling 2017, 57, 12651275. (38) Smith, J. S.; Roitberg, A. E.; Isayev, O. Transforming Computational Drug Discovery with Machine Learning and AI. ACS Medicinal Chemistry Letters 2018, 9, 10651069. (39) Gao, X.; Ramezanghorbani, F.; Isayev, O.; Smith, J. S.; Roitberg, A. E. TorchANI: free and open source PyTorch-based deep learning implementation of the ANI neural network potentials. Journal of chemical information and modeling 2020, 60, 34083415. (40) Bannwarth, C.; Ehlert, S.; Grimme, S. GFN2-xTBAn accurate and broadly parametrized self-consistent tight-binding quantum chemical method with multipole electrostatics and density-dependent dispersion contributions. Journal of chemical theory and computation 2019, 15, 16521671. (41) Behara, P. K.; Jang, H.; Horton, J. T.; Gokey, T.; Dotson, D. L.; Boothroyd, S.; Bayly, C. I.; Cole, D. J.; Wang, L.-P.; Mobley, D. L. Benchmarking Quantum Mechanical Levels of Theory for Valence Parametrization in Force Fields. The Journal of Physical Chemistry 2024, 128, 78887902, PMID: 39087913. (42) Brew, R.; Nelson, I.; Binayeva, M.; Nayak, A.; Simmons, W.; Gair, J.; Wagen, C. Wiggle150: Benchmarking Density Functionals And Neural Network Potentials 32 On Highly Strained Conformers. 2025; https://chemrxiv.org/engage/chemrxiv/ article-details/67f5035c6dde43c908ee4655. (43) Ross, G. A.; Lu, C.; Scarabelli, G.; Albanese, S. K.; Houang, E.; Abel, R.; Harder, E. D.; Wang, L. The maximal and current accuracy of rigorous protein-ligand binding free energy calculations. Communications Chemistry 2023, 6, 222. (44) Hjorth Larsen, A.; Jørgen Mortensen, J.; Blomqvist, J.; Castelli, I. E.; Christensen, R.; Du lak, M.; Friis, J.; Groves, M. N.; Hammer, B.; Hargus, C.; Hermes, E. D.; Jennings, P. C.; Bjerre Jensen, P.; Kermode, J.; Kitchin, J. R.; Leonhard Kolsbjerg, E.; Kubal, J.; Kaasbjerg, K.; Lysgaard, S.; Bergmann Maronsson, J.; Maxson, T.; Olsen, T.; Pastewka, L.; Peterson, A.; Rostgaard, C.; Schiøtz, J.; Schutt, O.; Strange, M.; Thygesen, K. S.; Vegge, T.; Vilhelmsen, L.; Walter, M.; Zeng, Z.; Jacobsen, K. W. The atomic simulation environmenta Python library for working with atoms. Journal of Physics: Condensed Matter 2017, 29, 273002. (45) Pickering, I.; Xue, J.; Huddleston, K.; Terrel, N.; Roitberg, A. E. TorchANI 2.0: An Extensible, High-Performance Library for the Design, Training, and Use of NN-IPs. Journal of Chemical Information and Modeling 2025, 65, 1165611671. (46) Doerr, S.; Majewski, M.; Perez, A.; Kramer, A.; Clementi, C.; Noe, F.; Giorgino, T.; De Fabritiis, G. TorchMD: deep learning framework for molecular simulations. Journal of chemical theory and computation 2021, 17, 23552363. (47) Rufa, D. A.; Macdonald, H. E. B.; Fass, J.; Wieder, M.; Grinaway, P. B.; Roitberg, A. E.; Isayev, O.; Chodera, J. D. Towards chemical accuracy for alchemical free energy calculations with hybrid physics-based machine learning / molecular mechanics potentials. bioRxiv 2020, (48) Eastman, P.; Galvelis, R.; Pelaez, R. P.; Abreu, C. R.; Farr, S. E.; Gallicchio, E.; Gorenko, A.; Henry, M. M.; Hu, F.; Huang, J.; others OpenMM 8: molecular dynamics simulation with machine learning potentials. The Journal of Physical Chemistry 2023, 128, 109116. (49) Friedrich, N.-O.; Meyder, A.; de Bruyn Kops, C.; Sommer, K.; Flachsenberg, F.; Rarey, M.; Kirchmair, J. High-Quality Dataset of Protein-Bound Ligand Conformations and Its Application to Benchmarking Conformer Ensemble Generators. Journal of Chemical Information and Modeling 2017, 57, 529539, PMID: 28206754. (50) National Center for Biotechnology Information PubChem Compound Summary for CID 5291, Imatinib. https://pubchem.ncbi.nlm.nih.gov/compound/Imatinib, 2025; Retrieved November 18, 2025. (51) Bigi, F.; Langer, M.; Ceriotti, M. The dark side of the forces: assessing non-conservative force models for atomistic machine learning. 2025; https://arxiv.org/abs/2412. 11569. (52) Kim, S.; Chen, J.; Cheng, T.; Gindulyte, A.; He, J.; He, S.; Li, Q.; Shoemaker, B.; Thiessen, P.; Yu, B.; Zaslavsky, L.; Zhang, J.; Bolton, E. PubChem 2025 update. Nucleic Acids Research 2025, 53, D1516D1525. (53) Mardirossian, N.; Head-Gordon, M. ω B97M-V: combinatorially optimized, rangeseparated hybrid, meta-GGA density functional with VV10 nonlocal correlation. The Journal of Chemical Physics 2016, 144, 214110. (54) Sun, Q.; Zhang, X.; Banerjee, S.; Bao, P.; Barbry, M.; Blunt, N. S.; Bogdanov, N. A.; Booth, G. H.; Chen, J.; Cui, Z.-H.; Eriksen, J. J.; Gao, Y.; Guo, S.; Hermann, J.; Hermes, M. R.; Koh, K.; Koval, P.; Lehtola, S.; Li, Z.; Liu, J.; Mardirossian, N.; McClain, J. D.; Motta, M.; Mussard, B.; Pham, H. Q.; Pulkin, A.; Purwanto, W.; Robinson, P. J.; Ronca, E.; Sayfutyarova, E. R.; Scheurer, M.; Schurkus, H. F.; Smith, J. E. T.; Sun, C.; Sun, S.-N.; Upadhyay, S.; Wagner, L. K.; Wang, X.; White, A.; Whitﬁeld, J. D.; Williamson, M. J.; Wouters, S.; Yang, J.; Yu, J. M.; Zhu, T.; Berkel34 bach, T. C.; Sharma, S.; Sokolov, A. Y.; Chan, G. K.-L. Recent developments in the PySCF program package. The Journal of Chemical Physics 2020, 153, 024109. (55) Li, R.; Sun, Q.; Zhang, X.; Chan, G. K.-L. Introducing GPU-acceleration into the Python-based Simulations of Chemistry Framework. 2024; https://arxiv.org/abs/ 2407.09700. (56) Wu, X.; Sun, Q.; Pu, Z.; Zheng, T.; Ma, W.; Yan, W.; Yu, X.; Wu, Z.; Huo, M.; Li, X.; Ren, W.; Gong, S.; Zhang, Y.; Gao, W. Enhancing GPU-acceleration in the Python-based Simulations of Chemistry Framework. 2024; https://arxiv.org/abs/ 2404.09452. (57) Galvelis, R.; Varela-Rial, A.; Doerr, S.; Fino, R.; Eastman, P.; Markland, T. E.; Chodera, J. D.; De Fabritiis, G. NNP/MM: Accelerating molecular dynamics simulations with machine learning potentials and molecular mechanics. Journal of chemical information and modeling 2023, 63, 57015708. (58) Batatia, I.; Benner, P.; Chiang, Y.; Elena, A. M.; Kovacs, D. P.; Riebesell, J.; Advincula, X. R.; Asta, M.; Baldwin, W. J.; Bernstein, N.; Bhowmik, A.; Blau, S. M.; Carare, V.; Darby, J. P.; De, S.; Pia, F. D.; Deringer, V. L.; Elijoˇsius, R.; ElMachachi, Z.; Fako, E.; Ferrari, A. C.; Genreith-Schriever, A.; George, J.; Goodall, R. E. A.; Grey, C. P.; Han, S.; Handley, W.; Heenen, H. H.; Hermansson, K.; Holm, C.; Jaafar, J.; Hofmann, S.; Jakob, K. S.; Jung, H.; Kapil, V.; Kaplan, A. D.; Karimitari, N.; Kroupa, N.; Kullgren, J.; Kuner, M. C.; Kuryla, D.; Liepuoniute, G.; Margraf, J. T.; Magdau, I.-B.; Michaelides, A.; Moore, J. H.; Naik, A. A.; Niblett, S. P.; Norwood, S. W.; ONeill, N.; Ortner, C.; Persson, K. A.; Reuter, K.; Rosen, A. S.; Schaaf, L. L.; Schran, C.; Sivonxay, E.; Stenczel, T. K.; Svahn, V.; Sutton, C.; van der Oord, C.; Varga-Umbrich, E.; Vegge, T.; Vondrak, M.; Wang, Y.; Witt, W. C.; Zills, F.; Csanyi, G. foundation model for atomistic materials chemistry. 2023, 35 (59) Wang, L.-P.; Song, C. Geometry optimization made simple with translation and rotation coordinates. The Journal of chemical physics 2016, 144 . 36 Supplementary Information for AceFF: State-of-the-Art Machine Learning Potential for"
        },
        {
            "title": "Small Molecules",
            "content": "Stephen E. Farr, Stefan Doerr, Antonio Mirarchi, Francesc Sabanes Zariquiey, and Gianni De Fabritiis,, Acellera Labs, Dr Trueta 183, 08005, Barcelona, Spain Computational Science Laboratory, Universitat Pompeu Fabra, Barcelona Biomedical Research Park (PRBB), Dr. Aiguader 88, 08003, Barcelona, Spain Acellera, 38350 Fremont Blvd 203 Fremont CA, 94536 USA Institucio Catalana de Recerca Estudis Avancats (ICREA), Passeig Lluis Companys 23, 08010 Barcelona, Spain E-mail: g.defabritiis@gmail.com"
        },
        {
            "title": "Software Optimizations of TensorNet",
            "content": "In addition to the architectural changes of TensorNet2, we implemented software optimizations that apply to both the base TensorNet and TensorNet2. The key change is that instead of using full 3 3 tensors for the I, A, and components, where possible we keep as scalar (shape [N, ]), as vector (shape [N, F, 3]), and remains tensor (shape [N, F, 3, 3]). The memory and speed improvements due to these changes are shown in ﬁgure S1 where the benchmarks have been run via OpenMM-Torch. For 300 atoms (100 water molecules) the times per step are 12.0ms and 7.93ms respectively; and the memory use is 3.2GB and 1 Figure S1: Memory diﬀerence of newer TensorNet code compared to the older code. Speed diﬀerence of newer TensorNet code compared to the older code. Both are for the same model (AceFF-1.0 / 1-layer TensorNet) run via OpenMM-torch with CUDAGraphs enabled. 2.4GB respectively. Overhead of TensorNet2 compared to TensorNet We compared the speed impact of the additional charge-prediction, neutral charge equilibration, and coulomb energy terms of TensorNet2 compared to the original TensorNet by running the inference benchmark for TensorNet-1Layer (AceFF-1.0) and TensorNet2-1Layer (the AceFF-2 1-Layer version used for the ablation study). The models have the same number of layers and hidden dimensions. The inference speeds are plotted in ﬁgure S2. We see that the overhead is minimal. In its current format the scaling of TensorNet2 is 2 due to an all-to-all coulomb term with charge features. For the system sizes we are interested in <1000 atoms this 2 implementation does not matter, the cost is completely hidden by the other more expensive TensorNet operations. If it became important for better scaling for very large systems it would be possible to modify the current implementation to use cutoﬀ (N scaling) or particle mesh ewald scheme (N log(N ) scaling). 2 Figure S2: Speed diﬀerence of TensorNet2 (with neutral charge equilibration and coulomb terms) compared to standard TensorNet. Both are 1-Layer models. LBFGS geometry optimization with torch.optim We compared the energy proﬁle of the LBFGS minimization with torch.optim.LBFGS 1 with the ASE 2 LBFGS optimizer. We generated one conformer of an Imbmatnib molecule 3 using RDKit ETKDG 4 with default settings. We then optimized the geometry with AceFF-2 forceﬁeld using torch.optim.LBFGS where max iter=20 for 20 steps (400 total steps) and with ASEs LBFGS optimizer for 400 steps. The ASE forcetol value was set to an arbitrarily low number such that it would not be reached and the full 400 steps would be performed. All other settings for both optimizers were left at the default values. The energy per step is plotted in ﬁgure S3. We see the convergence and values reached is very similar although not identical. The all atom (including Hs) RMSD between the ﬁnal structures is 0.27A. Ablation study: 1-layer model We trained second model with all hyper-parameters the same but with one TensorNet interaction layer instead of the two in AceFF-2. We evaluated its performance on some of the benchmarks. The results for Wiggle150 are shown in table S1. We see minor decrease 3 Figure S3: Energy proﬁles for geometry optimization using diﬀerent LBFGS implementations. in performance for the one layer model, it is still signiﬁcantly better than AceFF-1.0 which is also one layer model but without the neutral charge equilibration components. The results Table S1: Performance of AceFF-2 1-layer and AceFF-2 (which has 2 layers) on the Wiggle150 benchmark Model MAE (kcal/mol) RMSE (kcal/mol) AceFF-1.0 (1 layer) AceFF-2 (1 layer) AceFF-2 (2 layers) 2.73 1.88 1.76 3.32 2.39 2. for the Sellers et al torsion scan are shown in ﬁgure S4. We signiﬁcant drop in performance of the one layer model. The Schrodinger ligands benchmark results are shown in ﬁgure S5, we see signiﬁcant drop in accuracy particularly for the charged molecules. This follows the same observations in AIMNet2, which is that three neutral charge equilibration steps are needed for optimal accuracy. 6 AIMNet2 uses three NQE steps, TensorNet2 with two layers also uses three NQE steps as one occurs after the initial tensor-embedding layer and two more occur after the two interaction layers. 4 Figure S4: Sellers et al torsion scan benchmark 5 for AceFF-2, AceFF-1.0, and AceFF-2 (1-layer). Figure S5: Schrodinger ligands benchmark. AceFF-1.0. AceFF-2-1-layer. AceFF-2. Comparison of diﬀerent runtime methods There are diﬀerent ways to run inference with PyTorch model, the ones we are interested in are eager mode where the PyTorch model is used as-is with no export or compilation, torchscripted model where torch.jit.script is used to convert the model into scripted model (which can be loaded in Python on in C++ by libtorch), and compiled model which is compiled using torch.compile(). The compiled model can only current be used in Python. CUDAgraphs can be used with all three methods. The two diﬀerent programs we have used to run molecular dynamics are ASE and OpenMM. 7 ASE is in Python and uses the Python interface to models which means it can be run in eager mode and with torch compiled models. OpenMM is in C++ and can only run with the torchscript models. We compared the MD speed scaling, using the same water box method as in the main text, on RTX4090. The Figure S6: MD speed scaling for three diﬀerent runtime methods. OpenMMTorch which uses torchscript model with CUDAgraphs enabled. ASE using PyTorch in eager mode, and ASE using PyTorch with torch.compile(mode=reduce-overhead). results are plotted in S6. We see that the eager mode model hits the latency limit for small sytems. The torchscript and compiled model do not have the latency limit. The compiled model is approximately two time faster than the non-compiled model for large systems. The 6 torchcsript model fails with an out of memory error for 3000 atoms."
        },
        {
            "title": "References",
            "content": "(1) Ansel, J.; Yang, E.; He, H.; Gimelshein, N.; Jain, A.; Voznesensky, M.; Bao, B.; Bell, P.; Berard, D.; Burovski, E.; Chauhan, G.; Chourdia, A.; Constable, W.; Desmaison, A.; DeVito, Z.; Ellison, E.; Feng, W.; Gong, J.; Gschwind, M.; Hirsh, B.; Huang, S.; Kalambarkar, K.; Kirsch, L.; Lazos, M.; Lezcano, M.; Liang, Y.; Liang, J.; Lu, Y.; Luk, C.; Maher, B.; Pan, Y.; Puhrsch, C.; Reso, M.; Sarouﬁm, M.; Siraichi, M. Y.; Suk, H.; Suo, M.; Tillet, P.; Wang, E.; Wang, X.; Wen, W.; Zhang, S.; Zhao, X.; Zhou, K.; Zou, R.; Mathews, A.; Chanan, G.; Wu, P.; Chintala, S. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). 2024. (2) Hjorth Larsen, A.; Jørgen Mortensen, J.; Blomqvist, J.; Castelli, I. E.; Christensen, R.; Du lak, M.; Friis, J.; Groves, M. N.; Hammer, B.; Hargus, C.; Hermes, E. D.; Jennings, P. C.; Bjerre Jensen, P.; Kermode, J.; Kitchin, J. R.; Leonhard Kolsbjerg, E.; Kubal, J.; Kaasbjerg, K.; Lysgaard, S.; Bergmann Maronsson, J.; Maxson, T.; Olsen, T.; Pastewka, L.; Peterson, A.; Rostgaard, C.; Schiøtz, J.; Schutt, O.; Strange, M.; Thygesen, K. S.; Vegge, T.; Vilhelmsen, L.; Walter, M.; Zeng, Z.; Jacobsen, K. W. The atomic simulation environmenta Python library for working with atoms. Journal of Physics: Condensed Matter 2017, 29, 273002. (3) National Center for Biotechnology Information PubChem Compound Summary for CID 5291, Imatinib. https://pubchem.ncbi.nlm.nih.gov/compound/Imatinib, 2025; Retrieved November 18, 2025. (4) Riniker, S.; Landrum, G. A. Better Informed Distance Geometry: Using What We Know 7 To Improve Conformation Generation. Journal of Chemical Information and Modeling 2015, 55, 25622574, PMID: 26575315. (5) Sellers, B. D.; James, N. C.; Gobbi, A. comparison of quantum and molecular mechanical methods to estimate strain energy in druglike fragments. Journal of chemical information and modeling 2017, 57, 12651275. (6) Anstine, D.; Zubatyuk, R.; Isayev, O. AIMNet2: neural network potential to meet your neutral, charged, organic, and elemental-organic needs. ChemrXiv preprint chemrxiv2023-296ch-v3 2024, (7) Eastman, P.; Galvelis, R.; Pelaez, R. P.; Abreu, C. R.; Farr, S. E.; Gallicchio, E.; Gorenko, A.; Henry, M. M.; Hu, F.; Huang, J.; others OpenMM 8: molecular dynamics simulation with machine learning potentials. The Journal of Physical Chemistry 2023, 128, 109116."
        }
    ],
    "affiliations": [
        "Acellera",
        "Acellera Labs",
        "Computational Science Laboratory, Universitat Pompeu Fabra",
        "Institucio Catalana de Recerca Estudis Avancats (ICREA)"
    ]
}