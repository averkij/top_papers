{
    "paper_title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation",
    "authors": [
        "Qihui Zhang",
        "Munan Ning",
        "Zheyuan Liu",
        "Yanbo Wang",
        "Jiayi Ye",
        "Yue Huang",
        "Shuo Yang",
        "Xiao Chen",
        "Yibing Song",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 1 4 9 4 1 . 3 0 5 2 : r UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation Qihui Zhang1,3* Munan Ning1* Zheyuan Liu1,3 Yue Huang4 Shuo Yang1 Yanbo Wang1 Jiayi Ye1 Xiao Chen5 Yibing Song2, Li Yuan1 1School of Electrical and Computer Engineering, Peking University 2Hupan Lab 3DAMO Academy, Alibaba Group 4University of Notre Dame 5Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences. 1. Introduction Recently, Large Language Models (LLMs) have made significant strides in reasoning and application capabilities [7, 42, 59], enabling diverse applications such as text and code generation [1, 26, 51, 61]. Notably, due to the exceptional natural language understanding capabilities of LLMs, they are increasingly employed for automated eval- * Equal contribution Corresponding author {maskhui1003, munanning}@gmail.com yuanli-ece@pku.edu.cn Figure 1. Existing methods for evaluating MLLMs face various challenges. Our proposed UPME framework addresses these limitations by leveraging peer review mechanism, reducing annotation costs, and aligning closely with human judgment. uations, process known as LLM-as-a-Judge [71]. With the integration of vision encoders [46], Multimodal Large Language Models (MLLMs) [34, 42] can incorporate multiple modalities (e.g., text, image, and audio) [33, 72] and showcase remarkable performance in downstream applications, including visual conversation [14] and embodied intelligence [23]. Furthermore, they can act as agents to interact with GUI [16, 25]. Subsequently, the challenge of conducting objective and comprehensive evaluations of MLLMs while providing users with authoritative guidelines has emerged as key research focus. common approach involves constructing benchmarks based on Visual Question Answering (VQA), which assess the performance of MLLMs across various dimensions by setting image-based questions and answers [17, 38, 66, 69]. However, most of these works heavily depend on substantial human involvement in creating Q&A pairs for images [31, 56], leading to limited evaluation volume and making it insufficient for comprehensive assessment of visual content [12]. Another category of methods is MLLM-as-a-Judge [24, 27, 36, 71], which aims to reduce reliance on human annotations by allowing models to evaluate each other directly. However, these methods often still depend on human-designed questions and cannot entirely eliminate the need for human input. Additionally, significant challenge is the presence of verbosity and self-preference biases [15, 65], where MLLMs tend to favor longer responses or prefer their own outputs. These biases can lead mutual evaluation mechanisms to deviate from truly understanding visual content. To address these challenges, we propose UPME, the Unsupervised Peer review MLLM Evaluation framework, which is designed to evaluate MLLMs without requiring human QA annotations, as shown in Figure 1. During each iteration, UPME selects two candidate models and review model from the MLLM pool. The review model generates questions for given image and evaluates the responses provided by the candidate models. This evaluation is conducted using vision-language scoring system that assesses textual response correctness, visual understanding and reasoning, as well as image-text consistency through CLIP scores. Each model is initialized with confidence weight and alternates roles as candidate and review models. Their estimated scores are calculated based on weighted evaluations provided by the review models. The framework employs dynamic weight optimization, iteratively updating model scores and refining weights through consistency optimization. Ultimately, this process generates comprehensive and less biased estimated score list. We selected the MMStar [17] and ScienceQA [38] datasets as benchmarks because they encompass visual content reasoning and multimodal content across various disciplines. Experimental results show that under unsupervised settings, UPME achieved Pearson correlations of 0.944 and 0.814 with human evaluation results on these two datasets, while Spearman similarities reached 0.972 and 0.886, indicating high degree of similarity between our approach and human-annotated QA benchmarks. Further experiments demonstrated that UPME achieved higher consistency with human assessments compared to purely peer review-based methods, effectively reducing verbosity and self-preference issues in the MLLM-as-a-Judge framework. Overall, our contributions can be summarized as follows: We propose the first Unsupervised Peer review MLLM Evaluation (UPME) framework, addressing the challenge of reliance on human annotations in MLLM evaluation Our framework incorporates Vision-Language Scoring System that targets visual performance and image-text association, providing visually-focused evaluation. We conduct extensive experiments demonstrating that UPME achieves high consistency with human QA assessments and strong alignment with human preferences. 2. Related Work 2.1. MLLM Benchmark MLLM benchmarks rely on human-annotated QA pairs to measure the accuracy of MLLMs in answering questions. They focus on the different capabilities of MLLMs. ScienceQA [38] evaluate the multi-turn reasoning ability for broad range of scientific subjects by introducing Chain of Thought (CoT) [59]; MMVet [66] presents an LLMbased open-ended output evaluation method, enabling unified scoring across various question types and answer formats; MMMU [69], inspired by MMLU, primarily assesses model performance in complex reasoning tasks requiring college-level knowledge, covering six major disciplines and 30 fields with multimodal questions; MMStar [17] focuses on creating benchmark requiring visual content for reasoning, rigorously selecting visual-dependent samples and detecting data leakage to ensure genuine performance gains from multimodal training; RealWorldQA [62] is designed to assess the ability of MLLMs to understand real-world space. Although these benchmarks propose various evaluation methods, their evaluation scope is limited by the reliance on substantial human annotation. 2.2. MLLM-as-a-Judge To reduce the human annotation workload, LLM-as-aJudge [71] is proposed to enable models to perform mutual evaluation. This approach primarily involves using LLMs to assess responses in two ways: comparing pairs of answers to determine superiority [27, 35, 71], or directly scoring individual answers based on specific criteria [24, 35]. This strategy is extended to MLLM-as-a-Judge [15] to assess the performance of MLLMs across multiple evaluation tasks. MJ-Bench [18] evaluates the effectiveness of multimodal reward models in text-to-image generation. However, these methods face two main biases [48, 65]: the verbose bias and self-preference bias, which means MLLMs prefer the answers from themselves or with longer text length. Human preferences have also not been effectively incorporated into these automated evaluation systems [41]. 2.3. Human Preference-based Evaluation Predefined image and QA-pair benchmarks for MLLMs may encounter issues like data leakage and targeted optimization, which can prevent an accurate reflection of the true capabilities of these models [22, 63]. Therefore, evaluation methods based on human preferences offer better assessment of whether MLLMs meet human needs [40]. Chatbot Arena [19] provides public platform specifically for evaluating LLMs based on human preference. This platform uses pairwise comparison approach, where users inFigure 2. The UPME framework consists of three main components: (i) Peer Review Mechanism, where two candidate models and one review model are randomly selected from the MLLM pool. The review model generates questions based on selected image, and candidate models provide responses. (ii) Vision-Language Judgment Scoring System, which evaluates answers based on textual correctness, visual understanding and reasoning, and image-text correlation. (iii) Dynamic Weight Optimization, ensuring consistency between confidence weights and estimated scores through iterative optimization cycles. teract with two anonymous models and select the one that performs better, generating dynamic leaderboard via the Elo rating system. Although open-ended user evaluations can more realistically reflect LLM performance in realworld scenarios [58], the high economic cost of collecting large volumes of high-quality user feedback, combined with the long wait time for obtaining such feedback, limits the widespread adoption of this approach. 3. Methodology Assigning higher weights to better-performing models leads to significant improvement in the accuracy of the entire evaluation framework. This conclusion is supported by preliminary experiments detailed in subsection 8.1 and confirmed by recent research [20, 41, 53, 55]. Therefore, we focus on characterizing the relationship between model performance and its judging capability through consistency optimization. 3.1. Peer review formulation Problem definition. The UPME framework facilitates mutual evaluation among MLLMs via peer review mechanism, aiming to produce evaluation scores that closely reflect human preferences. Given an unsupervised image dataset = {Ii}n i=1 of images (with no human annotated questions and answers) and model pool = {Mj}m j=1 consisting of MLLMs (both open-source and closedsource), we can obtain an UPME-learned score list ˆG: ˆG := [ ˆGM1 , ˆGM2, ˆGM3 , . . . , ˆGMm ]. (1) On the other hand, given GM1 represents the score of model M1 on the human-annotated benchmark, we define the human-annotated score list as: := [GM1 , GM2, GM3 , . . . , GMm ]. (2) Our objective is to optimize the UPME-learned scores ˆG by maximizing their similarity to the human-annotated score list G, i.e., Sim(G, ˆG) 1, ensuring that UPMElearned scores align closely with human benchmark standards. Overview of UPME. Figure 2 shows an overview of our UPME framework. During each review iteration, we randomly select two different candidate models Mj and Mk from the pool for comparison, along with another model Mr to serve as the reviewer. Mr generates question Qr = Mr(Ii) for each image Ii and then prompts the candidate models Mj and Mk to provide responses Aj,r and Ak,r , respectively. The answers can be written as: Aj,r = Mj (Ii, Qr ) , Ak,r = Mk (Ii, Qr ) . (3) Based on the vision-language judgment scoring System SV (detailed in subsection 3.2), Mr then evaluates these responses, and compares the responses of Mj and Mk to produce review score Reviewj,k,r as: Reviewj,k,r = SV L(Aj,r , Ak,r , Qr , Ii Mr). (4) By assigning each model an initiated confidence weight w, we can calculate the learned score ˆGMj of each MLLM: (cid:88) (cid:88) (cid:88) Reviewj,k,r wr, (5) ˆGMj = k=j r=k,r=j then we can subsequently obtain the UPME-learned score list ˆG, as illustrated in Equation 1. 3.2. Vision-Language Judgment Scoring System In this subsection, we present our vision-language judgment scoring system, denoted as SVL in Equation 9. Our evaluation is structured around three core criteria: Response Correctness, Visual Understanding and Reasoning, and ImageText Correlation. The first two aspects are evaluated by employing Mr, which reviews responses based on prompts outlined in Section 10. The Image-Text Correlation is measured using CLIP scores. Response Correctness. The primary criterion is the correctness of the candidate models responses to questions posed by the review model. Existing studies indicate that paired comparative evaluation yields better accuracy than isolated assessments [36, 37], so we adopt pairwise scoring approach. Based on the review model Mrs evaluation Judger for candidates Mj and Mk, we calculate the models score for Response Accuracy SCorrect(j) as follows: SCorrect(j) = 1 0.5 0 if JudgeCorrect,j > JudgeCorrect,k if JudgeCorrect,j = JudgeCorrect,k if JudgeCorrect,j < JudgeCorrect,k, (6) where JudgeCorrect represents the review models assessment of response correctness. If JudgeCorrect,j > JudgeCorrect,k, it indicates that model Mj is judged superior in response accuracy to Mk, scoring 1 point; tie yields 0.5 points, and lower score results in 0 points. Visual Understanding and Reasoning. Inspired by the significant CoT ability demonstrated in the o1 model [44] through adversarial training, we leverage Mr to evaluate the interpretative and analytical capabilities of the model with respect to visual content. SVisual(j) is computed using the function Γ, which evaluates the response across four dimensions: caption, reasoning, grounding, and relationship, as follows: SVisual(j) = Γ(cap., rea., gro., obj. Aj, Mr), (7) where captioning evaluates the ability to generate precise descriptions, reasoning measures logical consistency, grounding assesses accurate object localization, and relationship captures the connections between subjects. Within this structure, the review model Mr provides combined score via Γ, yielding SVisual(j). SVisual(j) reflects models capability to go beyond basic VQA, integrating both description and reasoning to demonstrate deeper level of visual comprehension. Image-Text Correlation. To capture the alignment between image content and textual responses, we introduce the CLIP [47] score: SClip(j) = CLIP(Ii, Aj,r ), (8) where SClip(j) measures the correlation between image Ii and response Aj,r from model Mj. The details are described in subsection 9.2. Combining the above components, we compute the final vision-language judgment score as weighted sum of the three criteria: Response Accuracy, Visual Understanding and Reasoning, and Image-Text Correlation: SVL = γ1SCorrect + γ2SVisual + γ3SClip, (9) where γ1, γ2, and γ3 are weights reflecting the relative importance of each component in the overall evaluation. 3.3. Dynamic Weight Optimization In each iteration, we use Mean Squared Error (MSE) as the loss function to update the weights w, enhancing the consistency between the weights and the estimated scores ˆG: LMSE( ˆG, w) = 1 (cid:88) j=1 (cid:16) ˆGMj wMj (cid:17)2 . (10) The entire optimization process is dynamic. After each round of weight updates, the weights and scores continue to change. The loss function computes new scores based on the updated weights, which are then iteratively used to optimize the weights further, forming continuous iterative optimization loop, as detailed in Algorithm 1. 4. Experiments 4.1. Experimental Setup Datasets. To validate the effectiveness of our framework, we conduct experiments on two distinctly styled datasets: ScienceQA [38] and MMStar [17]. ScienceQA underscores the importance of CoT in scientific reasoning tasks, comprising approximately 21,208 multimodal multiple-choice questions covering various science topics. MMStar focuses on evaluating the multimodal capabilities of MLLMs, and consists of 1,500 meticulously curated samples including 6 core capabilities and 18 detailed axes. Each sample exhibits visual dependency and minimal data leakage. Models LLama-3.2-11b-V Claude-3-haiku Claude-3.5-sonnet Gemini-1.5-pro GPT-4o-mini GPT-4o Methods Peer Review Majority Vote [54] Rating Vote [2] PRD [32] UPME Pearson () 0.3140.0757 0.0950.1301 0.7800.0146 0.8640.0068 0.6680.0073 0.8780.0038 MMstar Spearman () 0.5500.0577 0.2250.1500 0.8250.0500 0.8500.0577 0.7250.1258 0.8750.0050 Per.Ent. () Pearson () 0.9830.2310 1.0990.0000 0.7520.2310 0.6370.0000 0.8680.2886 0.1590. 0.1600.0430 -0.1450.0308 0.4370.0452 0.41470.0464 0.33540.0635 0.6170.0071 ScienceQA Spearman () 0.2250.0957 -0.5250.2061 0.4500.1732 0.7250.2062 0.6000.0000 0.6250.1258 Per.Ent. () 1.0990.0000 1.0990.0000 0.7520.2310 0.4340.5352 1.0990.0000 0.6370.0000 0.7250.0044 0.7570.0013 0.7950.0013 0.8380.0027 0.9440. 0.7710.1616 0.7570.0857 0.7430.2309 0.8640.0317 0.9720.0330 1.0400.2830 1.2990.1733 0.6280.0755 0.4270.0087 0.1410.2812 0.4630.0193 0.5090.0181 0.6230.0084 0.6360.0042 0.8140.0024 0.6860.1777 0.5240.0660 0.6290.1895 0.6940.0734 0.8860.0286 1.0400.2830 1.0400.0000 0.9200.2387 0.7460.0016 0.4220.2812 Table 1. Main results of different models and methods on MMstar and ScienceQA datasets, evaluated by Pearson, Spearman, and Permutation Entropy metrics. Since the model cannot judge itself, scores from the other five models involved in the computations are used for evaluation. Optimal results are highlighted in bold and underlined. Standard deviation (std) is calculated based on four runs. Candidate MLLMs. Our MLLM pool includes 5 closedsource models and 1 open-source model. The open-source model is Llama-3.2-11b-vision-instruct [39], which adapts the Llama-3 architecture to the multimodal domain through visual instruction tuning [34]. The closed-source models consist of GPT-4o [45], GPT-4o-mini [43], Claude-3.5Sonnet [7], Claude-3-Haiku [8], and Gemini-1.5-Pro [57]. These commercial models employ enhanced human alignment strategies to optimize performance and user experience across various applications. Metrics. For all experiments, we employ three popular metrics to evaluate the setups mentioned above and our UPME method: the Pearson Correlation Coefficient [21] to evaluate the similarity of score lists, Spearmans Rank Correlation Coefficient [70] to measure the correlation of ranking lists, and Permutation Entropy [11] to assess the complexity and unpredictability of the scoring distributions. All experiments are performed for 4 runs with different seeds (seed = 1, 2, 3, 4 ) and record the average results. Comparison Methods. Since we are the first to investigate unsupervised peer review in MLLM evaluations, we choose the original peer review mechanism and two popular classical methods, Majority Vote [54] and Rating Vote [2], for our comparative analysis. We also adopt semi-supervised method, PRD [32], and conduct experiments using individual models as judges to further bolster our comparisons. 4.2. Experimental Results 4.2.1. Comparison Results The experimental results in Table 1 indicate that among the candidate MLLMs, GPT-4o exhibits the highest review capability on the MMstar dataset, achieving Pearson and Spearman correlation coefficients of 0.878 and 0.875, reFigure 3. Convergence experiments. spectively. In contrast, the open-source LLama-3.2 model demonstrates weaker performance. Although Claude-3.5Sonnet and Gemini-1.5-Pro claim to offer performance comparable to GPT-4o, they still show limitations when evaluating other models, falling noticeably short of humanlevel performance. The original peer review method yields Pearson and Spearman coefficients lower than that of GPT4o, indicating that the biases of weaker models influence the Peer Review approach. Our UPME method achieves Pearson correlation of 0.944 and Spearman correlation of 0.972 on the MMstar dataset, demonstrating high degree of similarity with human annotations and validating the effectiveness of our optimizations in visual QA comprehension and consistency evaluation. On the ScienceQA dataset, even the best-performing GPT-4o achieves Pearson and Spearman correlations of 0.617 and 0.625, reflecting the difficulty of the task. The original peer review method shows more modest results, with Pearson and Spearman of 0.463 and 0.686, respecMethod Peer Review Score Optimization Visual Alignment UPME MMstar ScienceQA Cor. Vis. Cli. Pearson Spearman Pearson Spearman 0.727 0.714 0.457 0.600 0.854 0.873 0.785 0.903 0.944 0.771 0.886 0.829 0.943 0.972 0. 0.701 0.548 0.775 0.814 0.657 0.771 0.771 0.886 0.886 Table 2. Ablation study with Pearson and Spearman similarity metrics for each dataset. Optimal results are highlighted in bold and underlined. Cor. stands for correctness, Vis. stands for visual understanding, and Cli. stands for Clip. tively. Following series of optimizations and alignments, UPME achieves Pearson and Spearman correlations of 0.814 and 0.886, showcasing its robustness and superior performance in complex datasets. While the performance on ScienceQA may not be as high, it is important to note that traditional datasets like ScienceQA might not fully align with the evaluation needs of current MLLMs because they may lack visual dependency or sufficient discrimination [17, 66]. UPME achieves higher Pearson and Spearman correlations, demonstrating its robustness and superior performance. Moreover, as shown in Figure 3, the loss consistently decreases, and the Pearson similarity metric increases over epochs in 64 different initial settings, which demonstrates the convergence of and ˆG. Under the current experimental setup (25 images and 1500 judgments), the framework reliably converges within the 30-epoch limit. 4.2.2. Ablation Study In the ablation study presented in Table 2, we evaluated the impact of progressively introducing different components on model performance. The Score Optimization phase showed consistent improvements in Pearson and Spearman correlation coefficients as modules were added. For instance, using only the correctness module resulted in Pearson scores of 0.854 for MMstar and 0.713 for ScienceQA. Introducing the visual understanding module alone yielded scores of 0.873 and 0.701, respectively. Adding the Clip component alone gave scores of 0.785 for MMstar and 0.548 for ScienceQA. Combining visual understanding and Clip modules further improved performance to 0.903 and 0.775. Ultimately, integrating all components within the UPME framework resulted in the highest performance, with Pearson and Spearman correlations of 0.944 and 0.972 on MMstar and 0.814 and 0.886 on ScienceQA. These results confirm that optimizing the scoring mechanism and visual alignment significantly boosts evaluation accuracy and robustness across different datasets. Figure 4. The performance of UPME in different sample size. 4.2.3. Impact of Sample Size In Figure 4, we analyze the impact of sample size on the alignment with human-annotated QA benchmarks, measured through Pearson and Spearman correlation coefficients, as well as Permutation Entropy. As the sample size increases from 5 to 25, there is clear improvement in both correlation metrics and decline in Permutation Entropy, indicating that the generated score lists and rank lists show greater alignment with human evaluations. Beyond sample size of 25, the metrics stabilize, showing minimal changes with further increases from 25 to 100 in sample size. This stabilization suggests that the consistency of the generated outputs relative to human judgments has reached satisfactory level, making 25 samples reliable point for evaluation. Therefore, we primarily selected sample size of 25 for our experiments, as it provides an effective balance between alignment accuracy and data efficiency. 5. Human Preference Alignment Analysis The aforementioned experiments primarily reflect the consistency between the UPME framework and the predefined QA-based benchmarks. To further explore the alignment between UPME and human preferences, as well as to examine the two types of biases inherent in MLLM-as-Judge approaches, we selected subset of responses for human evaluation and analysis of UPME scoring results. The detailed experimental design is provided in subsection 9.4. 5.1. Alignment with Human Preference To validate whether our proposed UPME method can align with human preferences without manual labeling, we conducted experiments on the MMstar and ScienceQA datasets. The experimental results on Table 3 indicate that the baseline method exhibited relatively low human agreement and model consistency rates, suggesting that the Peer Dataset MMstar ScienceQA Method Peer Review UPME Peer Review UPME Agreement (%) Consistency (%) 71.1 95.9 68.2 87.4 67.5 89.8 61.8 82.6 Table 3. Human Agreement (%) and Human Consistency (%) Rates on Various Datasets. Figure 5. Model accuracy comparison in peer review framework w/ and w/o UPME, where Peer Review Cor. represents the correctness of the original peer review, and UPME Cor. and UPME Vis. correspond to the two judgment dimensions of response correctness and visual understanding, introduced in subsection 3.2. Review mechanism under an unsupervised setting without weight optimization struggles to align with human preferences. In contrast, UPME demonstrated significant improvements by incorporating Correctness, Visual Understanding, and Clip Correlation. On the MMStar, UPME achieved an agreement rate of 95.9% and consistency rate of 89.8%, showing that the optimized scoring criteria significantly enhance the accuracy of evaluation outputs and alignment with human preferences. By capturing key multimodal understanding metrics without relying on manual labeling, UPME effectively achieved high consistency with human annotations, highlighting its substantial advantages in improving response consistency and accuracy under an unsupervised framework. 5.2. Performance w/ and w/o UPME To assess the improvement in model judgment accuracy within the Peer Review mechanism and the UPME framework, we conducted an experiment, the results are displayed in Figure 5. The calculation of accuracy is based on ACC = judgecorrect . Experimental results show that the judgeall UPME significantly improves the accuracy of models in (a) Verbosity Bias (b) Self Preference Bias Type Method Chi-Square p-value Phi Coefficient Verbosity Bias Peer review UPME Self Preference Peer review UPME 10.996 0.280 39.584 3.489 0.00091 0.59696 3.142e-10 0. 0.135 0.022 0.257 0.076 Figure 6. Heatmap and Table for Peer Review and UPME on Self Preference and Verbosity Bias. both correctness and visual understanding compared to the original correctness judgment. Specifically, the average accuracy in correctness evaluation increased from 61.56% in the baseline to 74.48% with UPME, an improvement of 12.92%. This indicates significant advantage of UPME in assessing the correctness of model responses. Furthermore, in the evaluation of visual understanding, UPME achieved an average accuracy of 73.93%, which is also notably higher than the baseline. This improvement is primarily attributed to UPMEs unsupervised evaluation framework, which employs multi-model peer review mechanism that combines correctness and visual understanding scores to achieve more efficient and accurate assessment. 5.3. Mitigating the Bias of MLLM-as-a-Judge Mitigation of Verbosity bias. As shown in Figure 6, the verbosity-preference bias is significantly alleviated by our method. The original Peer Review approach had p-value of 0.00091 (< 0.05), with Chi-square value of 10.996 and Phi coefficient of 0.135, suggesting significant prefIn conerence for verbose responses during evaluation. trast, the UPME framework yielded p-value of 0.59696 (> 0.05), with the Chi-square value reduced to 0.280 and the Phi coefficient decreased to 0.022, indicating that it does not exhibit significant bias toward selecting verbose responses. Additionally, the number of incorrect selections of verbose responses dropped from 233 to 103 in the UPME framework. Thus, the UPME approach effectively mitigates the bias toward verbose responses in models, enhancing evaluation diversity and alignment with human preferences in an unsupervised setting. Mitigation of Self-preference. Self-preference bias is In also significantly alleviated, as shown in Figure 6. the original Peer Review method, the Chi-square value was 39.584, the Phi coefficient was 0.257, and the p-value was Figure 7. Case study illustration of UPME. We provide the original human-designed questions and the UPME-generated questions, along with the answer analysis. The upper case presents the Disability of review model, where the review model can not answer the original question itself. The middle case demonstrates cases exhibiting verbosity bias. The bottom case shows self-preference bias. 3.142e-10, indicating highly significant bias. In contrast, the UPME framework reduced the Chi-square value to 3.489, the Phi coefficient to 0.076, and the p-value to 0.0618, approaching insignificance. The incorrect selections due to self-preference dropped from 226 to 129, further demonstrating our effectiveness in mitigating this bias. 5.4. Case Study We selected three cases to demonstrate how the UPME effectively addresses the core issues in traditional MLLM-asa-judge as shown in Figure 7. In the first case, the review model initially failed to identify the error in Candidate Bs response because it was unable to answer the question itself. After applying UPME, the review model was able to generate question within its capabilities and accurately determine that Candidate As response showed deeper understanding of the image, which aligned with human preferences, thus avoiding the misjudgment issue. In the second case, although Candidate Bs response was longer, it did not provide additional useful information. Traditional evaluations might wrongly consider the longer answer as more informative, but with UPME, the review model, through precise text evaluation, clearly identified Candidate As concise and clear answer as more valuable, thus avoiding verbosity bias. Finally, in the third case, UPME effectively avoided self-preference bias, enhancing fairness and objectivity. 6. Conclusion This study proposes an unsupervised peer review-based framework, UPME, effectively reducing the annotation workload in traditional human-designed QA benchmarks. Our vision-language scoring system emphasizes visionoriented judgment, addressing issues such as verbosity and self-preference biases inherent in MLLM-as-judge evaluation methods. Further experiments validate that our evaluation results achieve higher alignment with human preferences, providing promising and reliable approach."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by Alibaba DAMO Academy through the Alibaba Innovative Research Program."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Mohammad Allahbakhsh and Aleksandar Ignjatovic. Rating through voting: An iterative method for robust rating. arXiv preprint arXiv:1211.0390, 2012. 5 [3] FirstName Alpher. Frobnication. IEEE TPAMI, 12(1):234 778, 2002. [4] FirstName Alpher and FirstName Fotheringham-Smythe. Journal of Foo, 13(1):234778, Frobnication revisited. 2003. [5] FirstName Alpher and FirstName Gamow. Can computer frobnicate? In CVPR, pages 234778, 2005. [6] FirstName Alpher, FirstName Fotheringham-Smythe, and Journal FirstName Gamow. Can machine frobnicate? of Foo, 14(1):234778, 2004. [7] Anthropic. Claude 3.5: sonnet. https://www. anthropic . com / news / claude - 3 - 5 - sonnet, 2024. 1, 5 [8] Anthropic. https : / / www . anthropic.com/news/claude-3-haiku, 2024. 5, Claude 3 haiku. [9] Anthropic. Developing computer use model, 2024. [10] Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024. [11] Christoph Bandt and Bernd Pompe. Permutation entropy: natural complexity measure for time series. Physical review letters, 88(17):174102, 2002. 5 [12] Han Bao, Yue Huang, Yanbo Wang, Jiayi Ye, Xiangqi Wang, Xiuying Chen, Mohamed Elhoseiny, and Xiangliang Zhang. Autobench-v: Can large vision-language models benchmark themselves? arXiv preprint arXiv:2410.21259, 2024. 2 [13] Brett Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James Prather, and Eddie Antonio Santos. Programming is hard-or at least it used to be: Educational opportunities and challenges of ai code generation. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1, pages 500506, 2023. [14] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, et al. Low-code llm: Visual programming over llms. arXiv preprint arXiv:2304.08103, 2, 2023. [15] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788, 2024. 2 [16] Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: dataset for gui-oriented multimodal llm-based agents. arXiv preprint arXiv:2406.10819, 2024. 1 [17] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 1, 2, 4, 6 [18] Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Leria HUANG, et al. Mj-bench: Is your In ICML multimodal reward model really good judge? 2024 Workshop on Foundation Models in the Wild, 2024. 2 [19] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. 2 [20] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: peer review based large language model evaluator. arXiv preprint arXiv:2401.15641, 2024. 3 [21] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 14, 2009. [22] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Benchmark probing: Investigating data leakage in large language models. In NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly, 2023. 2 [23] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [24] Chujie Gao, Qihui Zhang, Dongping Chen, Yue Huang, Siyuan Wu, Zhengyan Fu, Yao Wan, Xiangliang Zhang, and Lichao Sun. The best of both worlds: Toward an honest and helpful large language model. arXiv preprint arXiv:2406.00380, 2024. 2 [25] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Yuxiao Dong, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. 1 [26] Zdenˇek Kasner and Ondˇrej Duˇsek. Beyond traditional benchmarks: Analyzing behaviors of open llms on data-totext generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1204512072, 2024. 1 [27] Zdenˇek Kasner and Ondˇrej Duˇsek. Beyond traditional benchmarks: Analyzing behaviors of open llms on data-totext generation, 2024. [28] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. [29] FirstName LastName. The frobnicatable foo filter, 2014. Face and Gesture submission ID 324. Supplied as supplemental material fg324.pdf. [30] FirstName LastName. Frobnication tutorial, 2014. Supplied as supplemental material tr.pdf. [31] Jian Li and Weiheng Lu. of multimodal arXiv:2408.08632, 2024. 1 large language models. survey on benchmarks arXiv preprint [32] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023. 5, 2 [33] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 5 [35] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023. 2 [36] Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950, 2024. 2, [37] Adian Liusie, Potsawee Manakul, and Mark JF Gales. Zeroshot nlg evaluation through pairware comparisons with llms. arXiv preprint arXiv:2307.07889, 2023. 4 [38] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 1, 2, 4 [39] Meta. Llama 3.2 11b-vision-instruct. https : / / huggingface . co / meta - llama / Llama - 3 . 2 - 11B-Vision-Instruct, 2024. 5, 1 [40] Birger Moell. Evaluating large language models with human feedback: Establishing swedish benchmark. arXiv preprint arXiv:2405.14006, 2024. [41] Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yu Wang, Ming Pang, and Li Yuan. Peerreview-in-llms: Automatic evaluation method for llms in open-environment. arXiv preprint arXiv:2402.01830, 2024. 2, 3 [42] OpenAI. Gpt-4 technical report. https://openai. com/research/gpt-4, 2023. 1 [43] OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence. https : / / openai . com / index / gpt - 4o - mini - advancing - cost - efficient - intelligence/, 2024. 5, 1 [44] OpenAI. Introducing openai o1: reasoning with model https://openai.com/index/introducing-openai-o1-preview/, 2024. 4 logical visual and Advancing insights. [45] OpenAI. Hello gpt-4o, 2024. 5, 1 [46] Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. Perceptiongpt: Effectively fusing visual perception In Proceedings of the IEEE/CVF Conference on into llm. Computer Vision and Pattern Recognition, pages 27124 27133, 2024. 1 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4, 3 [48] Vyas Raina, Adian Liusie, and Mark Gales. Is llm-as-ajudge robust? investigating universal adversarial attacks on zero-shot llm assessment. arXiv preprint arXiv:2402.14016, 2024. [49] Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakkar. Constructing domain-specific evaluation sets for llm-as-a-judge. arXiv preprint arXiv:2408.08808, 2024. [50] Refuel.ai. Llm labeling: Technical report, 2025. 2 [51] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 1 [52] Charles Spearman. The proof and measurement of association between two things. 1961. [53] Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, and Benjamin Roth. From calculation to adjudication: Examining llm judges on mathematical reasoning tasks. arXiv preprint arXiv:2409.04168, 2024. 3 [54] James Surowiecki. The wisdom of crowds/james surowiecki. NY.: Anchor, 2005. 5 [55] Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. 3 [56] Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, and Xiaokang Yang. Semantic equivalent adversarial data augmentation for visual question answering. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIX 16, pages 437453. Springer, 2020. 1 [57] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 5, 1 [58] Li Wang, Hui Zhang, and Rajesh Patel. user-centric benchmark for evaluating large language models. arXiv preprint arXiv:2404.13940, 2024. 3 [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1, [60] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [61] Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, et al. Unigen: unified framework for textual dataset generation using large language models. arXiv preprint arXiv:2406.18966, 2024. 1 [62] xAI. Realworldqa: benchmark for evaluating real-world spatial understanding in multimodal ai models, 2024. 2 [63] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024. 2 [64] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [65] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. 2 [66] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 1, 2, 6 [67] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. [68] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024. [69] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, et al. Mmmu: massive multi-discipline multimodal understanding and In Proceedings of reasoning benchmark for expert agi. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 1, 2 [70] Jerrold Zar. Spearman rank correlation. Encyclopedia of biostatistics, 7, 2005. 5 [71] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 1, 2 [72] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023. 1 UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Model Selection We select models as follows: GPT-4o [45] versatile multimodal model by OpenAI, handling text, image, and audio inputs. It excels in vision and language tasks with enhanced processing speed. Known for strong real-time performance in audio and vision, GPT-4o is ideal for variety of applications, including multilingual tasks. GPT-4o mini [43] smaller, cost-effective version of GPT-4o, optimized for handling text and images, with plans for audio support. It is designed for high-volume, real-time applications like chatbots and coding tasks, offering strong performance at lower cost. Gemini-1.5-Pro [57] Developed by Google DeepMind, this model uses Mixture-of-Experts architecture to optimize performance. It supports up to 1 million tokens and excels in translation, coding, and multimodal tasks. It is ideal for enterprise use due to its cost-efficiency and scalability. Claude-3.5-Sonnet [7] From Anthropic, this model is optimized for reasoning, coding, and multimodal tasks. It excels in complex problem-solving and visual understanding, making it useful for customer support and detailed code-generation tasks. Claude-3-Haiku [8] Developed by Anthropic, Claude 3.5 Haiku is high-speed language model optimized for rapid response and advanced reasoning. With 200K token context window and maximum output of 4,096 tokens, Its affordability and it efficiently handles large datasets. speed make it ideal for applications requiring quick, concise responses, such as interactive chatbots and real-time data analysis. Llama-3.2-11B-Vision-instruct [39] multimodal large language model from Meta with 11 billion parameters, designed to handle both text and image inputs. It excels in tasks such as image captioning, visual question answering, and interpreting complex visual data. This model is particularly effective for industries like healthcare and retail, where real-time visual and textual analysis is key. 8. Extended Experiment 8.1. Pre-experiment for Different Weights. We have conducted preliminary experiment to investigate the relationship between the confidence weights and scores of models, as substantiation of our methodology. Below is detailed description of the experiment and its findings, which align with the methodology discussed in the paper. Method MMstar ScienceQA Pearson Spearman Pearson Spearman Reverse Weight Uniform Weight Consistent Weight UPME 0.607 0.725 0.807 0.944 0.486 0.771 0.829 0. 0.334 0.463 0.760 0.814 0.257 0.686 0.771 0.886 Table 4. Performance comparison of Consistent, Uniform and Reverse weight. To begin, we designed toy experiment to examine the role of confidence weights w. Based on the scores on manually designed benchmarks, we can obtain model score ranking list, then designing weight configurations that are either consistent or reverse to this score ranking. Specifically, we constructed three weighting methods: Reverse Weight (w = [0, 0.1, . . . , 1]), Uniform Weight (w = [1, 1, . . . , 1]), and Consistent Weight (w = [1, 0.9, . . . , 0]). Using these manually constructed weight configurations, we calculated the response score Gj for each model based on the predefined Equation 6 in Section 3 and obtained the score list ˆG for all models. We then measured the alignment between the obtained ranking ˆG and the human-annotated score list using predefined metrics. The results as summarized in Table 4, demonstrate that the Consistent Weight configuration achieves the highest correlation values, while the Reverse Weight configuration consistently yields the poorest results. These findings validate the proposed consistency assumption: assigning higher weights to models with stronger capabilities leads to better alignment between the model score list and the humanannotated one. In essence, responses recognized more favorably by other reviewers (models) tend to originate from higher-level models. This reinforces the idea that high-capability MLLMs evaluate others responses more accurately and achieve higher scores. We formalize this observation as the consistency assumption, which states that: 1. High-level LLMs exhibit greater confidence and accuracy when evaluating responses compared to lower-level ones. 2. models ability and its associated score are generally consistent. Building on this preliminary finding, we devised an optimization framework aimed at maximizing the consistency between each models capability (w) and its response score (G), constrained by our proposed methodology. 8.2. More Datasets 8.4. Reliability of Judge Correctness We have experimented on MMVet and the results in Table 5 show that UPME maintains its superior performance. Models Peer Review Majority Vote Rating Vote PRD [32] UPME MMstar ScienceQA MMVet Pearson Spearman Pearson Spearman Pearson Spearman 0.725 0.757 0.795 0.838 0.944 0.771 0.757 0.743 0.864 0.972 0.463 0.509 0.623 0.692 0. 0.686 0.524 0.629 0.636 0.886 0.688 0.732 0.739 0.794 0.914 0.752 0.643 0.743 0.814 0.928 Table 5. Comparison with recent methods. 8.3. Hyperparameter The weights γ1, γ2, and γ3 in Equation 9 were initialized as 0.4, 0.4, and 0.2, respectively, reflecting balanced emphasis on response correctness and visual understanding while slightly de-emphasizing image-text correlation. This choice is intuited that correctness and reasoning typically have larger impact on multimodal evaluation tasks. γ1 0.4 0.3 0.5 0.3 γ2 0.4 0.3 0.3 0.5 γ3 0.2 0.4 0.2 0. Pearson Spearman 0.9415 0.9397 0.9306 0.9365 0.9441 0.8581 0.7174 0.8857 Advantages of UPMEs Question Generation: In the original peer review mechanism, the review model might encounter questions that it cannot answer accurately, leading to unreliable evaluations. In contrast, the UPME framework enables the review model to generate questions autonomously, ensuring that these questions fall within its capability. This significantly enhances the reliability of the review model in assessing the correctness of responses from the evaluated models. Empirical Evidence of Reliability: As shown in Table 7, UPME demonstrates substantial improvement in both accuracy and human agreement compared to the original peer review mechanism. Method Dataset Accuracy (%) Human (%) Alignment Peer Review UPME MMStar ScienceQA MMStar ScienceQA 64.2 60.3 87.8 79.6 71.1 68.2 95.9 87.4 Table 7. JudgeCorrect reliability. 9. More Information about UPME Table 6. hyperparameter in Scoring criteria. 9.1. The Cost of UPME To validate the optimality of this combination, we conducted experiments with different hyperparameter configurations. The results for four representative settings are summarized in Table Table 6. The proposed setting achieves the highest Pearson and Spearman correlations, indicating its effectiveness in aligning with human evaluations. Notably, our experiments also show that the framework is relatively insensitive to small variations in these weights within the range [0.2,0.5], demonstrating its robustness. Task-Specific Flexibility: The UPME framework supports task-specific flexibility. For instance, users may adjust γ3 to prioritize image-text correlation in tasks requiring strong alignment between modalities or increase γ2 for tasks demanding advanced reasoning capabilities, which allows the framework to cater to diverse evaluation needs. Future Directions: While manual tuning of hyperparameters has proven effective, we agree that automating this process would further enhance the frameworks generality and ease of use. We are actively exploring automated methods, such as validation-based optimization techniques or reinforcement learning approaches, to dynamically determine these weights based on task characteristics. UPME significantly reduces both time and financial costs: Time Costs: Creating VQAs manually requires deep understanding and may take several minutes to hours per task. AI tools significantly reduce this time, with labeling efficiency improved by up to 100 times [50]. UPME further accelerates evaluation, processing dozens of images per second. Financial Costs: Human annotations cost 15 per image depending on complexity, while UPME reduces this to approximately 1/7 of the manual cost. Baseline methods like PeerReview and Majority Vote require extensive human-labeled data, significantly increasing time and costs. UPME eliminates manual annotation, offering efficient evaluations. Method Human Annotation Majority Vote Rating Vote UPME Framework Time / img 310 min Finance / img $ 27 28 min 1.5 $ 15 $ 0.15 Table 8. Comparison of Time and Financial Costs 9.2. Image-Text Correlation To compute the Image-Text Correlation score SClip, we employ the CLIP model [47], which measures the cosine similarity between image embeddings and text embeddings. For textual responses Aj,r exceeding CLIPs maximum token input limit of 77 tokens, we implement segmentation strategy that ensures each segment contains no more than the limit, preserving the context across the text. Specifically, if the number of tokens in response exceeds 77, we calculate the starting indices for each segment by dividing the range from 0 to 77 into five equal intervals. These numbers serve as the starting points for each segment. Each segment then extends for 77 tokens from its starting index, ensuring full coverage of the original text with some overlap between consecutive segments. This overlapping is crucial as it helps preserve the continuity and context of the text, which might otherwise be lost if the segments were disjointed. The segments are then processed alongside the image through the CLIP model to generate embeddings, and cosine similarities between each text segment and the image are calculated. We derive the average of these similarity scores to evaluate the text-image alignment. notable feature of the segmentation strategy addresses potential verbosity bias by penalizing segments containing irrelevant or poorly aligned content. By computing the average cosine similarity across all segments, the approach inherently discounts segments that introduce irrelevant or poorly aligned content, reducing the score for long but less relevant responses. This mechanism effectively counteracts the verbosity bias of MLLM-as-a-judge. 9.3. Algorithm of UPME Algorithm 1 Algorithm of UPME Input: MLLM Pool M, Image pool I, Epochs Output: Model Scores ˆG, Weights 1: Initialize and for models in 2: // Dynamic Update ˆG and 3: for each iteration = 1 to do 4: , Aj,r Randomly select Mr, Mj, Mk , Ak,r Generate Qr , Ak,r Calculate SV L(Aj,r Update scores using EMA: G[Mj] (1 α)G[Mj] + αSV optimize weights(G) , Qr , IiMr) 5: 6: 7: 8: 9: 10: end for 9.4. Human Preference Alignment The annotation work for human preferences alignment was carried out by five human experts with professional English proficiency, taking them total of 170 hours. The labeling Figure 8. Screenshot of human annotation. screenshot is shown in Figure 8. The guidelines for human annotation are shown in Figure 9. Each data is associated with an image, review model, and two candidate models, and it requires the completion of the following two annotation tasks: 1) Without knowledge of the review models judgment, the annotator provides their own choice. 2) After being informed of the review models judgment, the annotator indicates whether they agree with the decision. These two tasks are assigned to different individuals for the same image, meaning that the same annotator does not perform both tasks for the same image. Each task is annotated by two annotators. When the results of the two annotators are consistent, the images human preference annotation is obtained. If the results are inconsistent, up to five annotators will vote, and majority vote determines the final annotation result for the data. Statistical analysis shows that such cases requiring voting account for only 2.17% of the final annotation results. The experimental results on Table 3 indicate that the baseline method exhibited relatively low human agreement and model consistency rates, suggesting that the Peer Review mechanism under an unsupervised setting without weight optimization struggles to align with human preferences. In contrast, UPME demonstrated significant improvements by incorporating metrics such as Correctness, Visual Understanding, and Clip Relevance. On the MMstar dataset, UPME achieved an agreement rate of 95.9% and consistency rate of 89.8%, showing that the optimized scoring criteria significantly enhance the accuracy of evaluation outputs and alignment with human preferences. By capturing key multimodal understanding metrics without relying on manual labeling, UPME effectively achieved high consistency with human annotations, highlighting its substantial advantages in improving response consistency and accuracy under an unsupervised framework. Human annotation guideline [Task Overview] You are human expert tasked with annotating the data assigned to you. You need to evaluate the responses of two candidate models to given image and question, make your judgment, and assess whether the review models judgment is correct. Each annotation involves assessing data instances that include an image, the responses from two candidate models, and judgment from the review model. For the given data, you will perform one of two tasks and focus your assessment on one of two aspects. Please be aware of which task the data belongs to and which aspect of the candidate models responses you are evaluating. [Two Annotation Tasks] ### Task 1: Independent Choice Without Review Model Judgment - You should independently evaluate and select your preferred response between the two candidate models based on their response to the image-related question. - No information about the review models judgment is provided during this step. ### Task 2: Agreement with Review Model Judgment - You are informed of the review models judgment and asked to decide whether you agree or disagree with it. Note: Tasks 1 and 2 must be conducted by different individuals for the same image to prevent cognitive bias. [Two Aspects to Evaluate] When evaluating the responses from the two candidate models, you need to focus on one of the following two aspects: ### 1) Correctness - Your evaluation should be strictly objective, focusing only on which response is correct. Please proceed as follows: - If only one model provides correct answer, identify the correct model. - If both answers are correct or both are incorrect, output to indicate tie. ### 2) Visual Understanding and Reasoning - Focus exclusively on the depth of visual understanding and the quality of reasoning in each response. Do not evaluate based on correctness. Here are the Evaluation Criteria: - Captioning: Evaluate the ability to generate precise descriptions of image elements. - Reasoning: Assess logical consistency and coherence in explanations and conclusions. - Grounding: Evaluate accurate object localization within the image. - Relationship: Assess the understanding of relationships and interactions between subjects in the image. Figure 9. Human annotation guideline. 10. Prompt Template Question generation prompt for review model You are review model tasked with evaluating the visual capabilities of two other models. Based on the provided image input, generate question that is directly related to the content of the image. Please respond with only the question and no additional content. Judge prompt for review model focusing on visual understanding and reasoning [System] You are review model tasked with evaluating responses from two assistants to question about an image. Each assistant has provided an answer based on their analysis of the image. Evaluation Criteria: - Captioning: Evaluate the ability to generate precise descriptions of image elements. - Reasoning: Assess logical consistency and coherence in explanations and conclusions. - Grounding: Evaluate accurate object localization within the image. - Relationship: Assess the understanding of relationships and interactions between subjects in the image. - Focus exclusively on the depth of visual understanding and the quality of reasoning (as described above) in each response. Do not evaluate based on correctness. Evaluation Format: - Compare the two responses impartially. Ignore the order of presentation and the length of the responses. Do not favor any specific assistant based on their name. - Conclude your evaluation by using the following format: - [[A]] if assistant As response demonstrates better visual understanding and reasoning, - [[B]] if assistant Bs response demonstrates better visual understanding and reasoning, - [[C]] if it is tie. [User Question] {question} [The Start of Assistant As Response] {Answer a} [The End of Assistant As Response] [The Start of Assistant Bs Response] {Answer b} [The End of Assistant As Response] [Task] Based on the image, question, and two responses provided, and following the criteria above, determine which assistant provided better answer focusing solely on visual understanding and reasoning. Use the specified format for your final verdict. Judge prompt for review model focusing on correctness [System] You are review model tasked with evaluating responses from two assistants to question about an image. Each assistant has provided an answer based on their interpretation of the image. Your evaluation is strictly objective, focusing only on which response is correct. Please proceed as follows: 1. Assess if Assistant As response is correct. 2. Assess if Assistant Bs response is correct. 3. Compare the correctness of both responses: - If only one assistant provides correct answer, output [[A]] if Assistant is correct, or [[B]] if Assistant is correct. - If both answers are correct or both are incorrect, output [[C]] to indicate tie. Avoid considering subjective factors such as response quality, detail, or reasoning process. Base your decision solely on the correctness of the answers. [User Question] {question} [The Start of Assistant As Response] {Answer a} [The End of Assistant As Response] [The Start of Assistant Bs Response] Answer [The End of Assistant Bs Response] [Task] Based solely on the correctness of the two responses, determine which assistant answered the question accurately. Use the specified format for your final verdict. Answer generation prompt for candidate model [System] Please act as an image-understanding expert to solve the problem based on the provided image. First, analyze the provided image in detail, focusing on its overall theme and key elements. Then, outline your reasoning process step by step, considering how each detail contributes to your understanding of the image. Finally, provide clear and accurate answer to the users question based on your analysis. Lets think step by step. [User Question] {question} Once youve completed your reasoning, pick one choice from the options. Output the final answer in the format: [[X]] where is the selected option."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "School of Electrical and Computer Engineering, Peking University",
        "Tsinghua University",
        "University of Notre Dame"
    ]
}