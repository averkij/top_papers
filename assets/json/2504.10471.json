{
    "paper_title": "MIEB: Massive Image Embedding Benchmark",
    "authors": [
        "Chenghao Xiao",
        "Isaac Chung",
        "Imene Kerboua",
        "Jamie Stirling",
        "Xin Zhang",
        "MÃ¡rton Kardos",
        "Roman Solomatin",
        "Noura Al Moubayed",
        "Kenneth Enevoldsen",
        "Niklas Muennighoff"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image representations are often evaluated through disjointed, task-specific protocols, leading to a fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given a piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 1 7 4 0 1 . 4 0 5 2 : r MIEB: Massive Image Embedding Benchmark Chenghao Xiao1, Isaac Chung2 Imene Kerboua3,4 Jamie Stirling1 Xin Zhang5 Marton Kardos6 Roman Solomatin7 Noura Al Moubayed1 Kenneth Enevoldsen6 Niklas Muennighoff 8,9 1Durham University, 2Zendesk, 3Esker, 4INSA Lyon, LIRIS, 5The Hong Kong Polytechnic University, 6Aarhus University, 7ITMO University, 8Contextual AI, 9Stanford University Correspondence: chenghao.xiao@durham.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Image representations are often evaluated through disjointed, task-specific protocols, leading to fragmented understanding of model capabilities. For instance, it is unclear whether an image embedding model adept at clustering images is equally good at retrieving relevant images given piece of text. We introduce the Massive Image Embedding Benchmark (MIEB) to evaluate the performance of image and image-text embedding models across the broadest spectrum to date. MIEB spans 38 languages across 130 individual tasks, which we group into 8 high-level categories. We benchmark 50 models across our benchmark, finding that no single method dominates across all task categories. We reveal hidden capabilities in advanced vision models such as their accurate visual representation of texts, and their yet limited capabilities in interleaved encodings and matching images and texts in the presence of confounders. We also show that the performance of vision encoders on MIEB correlates highly with their performance when used in multimodal large language models. Our code, dataset, and leaderboard are publicly available at https: //github.com/embeddingsbenchmark/mteb. 1. Introduction Image and text embeddings power wide range of use cases, from search engines to recommendation systems [32, 43, 115]. However, evaluation protocols for image and multimodal embedding models vary widely, ranging from image-text retrieval, zero-shot classification [84, 116], linear probing [80, 84], fine-tuning the models [12, 38], and using MLLM performance as proxies [95]. These divergent protocols reveal the lack of standardized criteria for assessing image representations. We introduce the Massive Image Embedding Benchmark (MIEB) to provide unified comprehensive evaluation protocol to spur the fields advancement toward universal image-text embedding models. We build on the standard for the evaluation of text embeddings, MTEB [73], extending its codebase and leaderboard for image and image-text embedding models. MIEB spans 130 tasks grouped into 8 task categories: Aligning with MTEB, we integrate Clustering, Classification, and Retrieval. Notably, we consider fine-grained aspects, such as interleaved retrieval, multilingual retrieval, instruction-aware retrieval. We additionally include Compositionality Evaluation and Vision Centric Question Answering, respectively assessing nuanced information encoded in embeddings and their capabilities in solving vision-centric QA tasks. We focus on tasks that require strong visual understanding of texts, for which we include Visual STS, the visual counterpart of semantic textual similarity in NLP, and Document Understanding, assessing the vision-only understanding of high-resolution documents with dense texts and complex layout, enabling evaluation that pushes forward the development of natural interleaved embeddings. Our analysis across task categories shows that the performance of current image embedding models is fragmented, with no method dominating all task categories. We further study the predictability of the performance of visual encoders as part of Multimodal Large Language Models (MLLMs), via large-scale correlation study. We find that the performance of vision encoders on MIEB strongly correlates with the performance of MLLMs that use the same vision encoder. For instance, the performance on our Visual STS tasks has over 99% correlation with the performance of Figure 1. Overview of MIEB task categories with examples. See Table 1 for details about capabilities measured and other information. an MLLM leveraging the same vision encoder on tasks like OCRBench and TextVQA. This provides practical way to select vision encoders for MLLMs based on MIEB results. 2. The MIEB Benchmark 2.1. Overview [98]) with fine-grained domains (e.g., Existing image benchmarks are often task-specific (e.g., retrieval landmarks [100], artworks [112]). MIEB provides unified framework to evaluate diverse abilities of embedding models. We categorize tasks based on combination of the evaluation protocol (e.g., Clustering) and the abilities assessed (e.g., Document Understanding) to better align with user interests. Figure 1 and Table 1 summarize MIEB task categories. Beyond traditional tasks like linear probing, zeroshot classification, and image-text retrieval, we emphasize under-explored capabilities in image-text embedding models via: 1) Visual representation of texts, covered by document understanding and visual STS; 2) Vision-centric abilities, including spatial and depth relationships; 3) Compositionality; 4) Interleaved embedding; 5) Multilinguality. In addition to MIEB (130 tasks), we introduce MIEBlite, lightweight version of MIEB with 51 tasks to support efficient evaluation, by selecting representative tasks from task performance clusters, detailed in 6.3. We refer to for all datasets, statistics, and evaluation metrics for MIEB and MIEB-lite, and 4 for implementation details. Here, we discuss task categories and capabilities assessed. Retrieval Retrieval evaluates if embeddings of two similar items (images or texts) have high similarity [20]. We focus on three retrieval aspects: 1) Modality: The combination of images and texts among queries and documents and whether they are interleaved; 2) Multilinguality: Whether tasks cover mulitple languages, including texts in images; 3) Instructions Some tasks may benefit from instructions on what to retrieve, e.g., in VQA tasks questions in the text serve as example-specific instructions. We use nDCG@10 as the primary metric [92, 98], and recall@1/map@5 for some tasks to align with prior work or adjust for difficulty. Document understanding There has been much interest in using image embeddings to understand entire documents with interleaved figures and tables [27]. To address these needs, we create separate document understanding category. It uses the same evaluation procedure as retrieval and nDCG@5 as the main metric. Linear probing For linear probing, linear model is trained on embedded images to predict associated class labels [4, 84]. Linear probing allows evaluating knowledge encoded in embeddings, even if they are not spatially consistent as would be needed for good clustering performance. We opt for few-shot linear probing [16, 73] with default of 16 shots per class on which we train logistic regression classifier with maximum of 100 iterations. This method is more efficient than probing on the entire dataset [13, 80, 84], making it suitable for large-scale benchmarks like ours. In 6.1, we ablate the performance Task category Example abilities assessed # Tasks # Languages Modalities Retrieval Document Understanding (Retrieval) Linear Probing (Classification) Clustering Zero-shot Classification Compositionality Evaluation (PairClassification) Vision-centric QA (Retrieval) Visual STS cross-modal/-lingual matching OCR abilities information encoded embedding space consistency cross-modal matching reasoning with confounders counting, object detection OCR abilities MIEB MIEB-lite all all 45 10 22 5 23 7 6 9 130 51 38 2 1 1 1 1 1 12 38 38 i-i; i-t; t-i; it-i; it-t; i-it; t-it; it-it; i-t t-i; i-t; it-t i-i; i-i i-i i-t; i-t i-t; t-i it-t; it-i i-i all all Table 1. An overview of MIEB tasks. In brackets behind task categories, we denote the task type implementation in the code, e.g., our document understanding tasks use our retrieval implementation. We denote the modalities involved in both sides of the evaluation (e.g., queries and documents in retrieval; images and labels in zero-shot classification) with i=image, t=text. trend of k-shot per class, showing that model ranking generally remains the same across different values of k. In text embeddings, this task is often called classification [73], so we adopt that term in our code."
        },
        {
            "title": "ImageNet",
            "content": "Zero-shot Classification While generally using the same tasks as linear probing (e.g., [21]), zeroshot Classification directly matches image embeddings to classes without training separate classifier. We follow common practice and turn class labels into text prompts (e.g., for our ImageNet task, text prompt could be photo of space shuttle). This task is related to retrieval, specifically, setting where we only care about the top-1 match. We measure accuracy following prior work [84]. Models trained with non-representation losses, such as autoregressive models, often lack good off-the-shelf zero-shot performance, but may still perform well in linear probing [85]. Compositionality Evaluation Vision-language compositionality assesses whether the composition of given set of elements aligns with an image and text, such as relationships between objects, attributes, and spatial configurations. Commonly, it involves distinguishing ground truth from hard negatives with perturbed inputs, e.g., word order shuffling in ARO benchmark [114]. In our code implementation, we also refer to it as ImageTextPairClassification, as images and texts come in small pairs. The main metric we use for this task category is accuracy. Vision-centric question answering Inspired by insights from MLLMs [95], we include vision centric question answering tasks, including object counting, spatial relationships, etc. We also include other challenging visual perception tasks, such as perceiving art styles. This task category can be seen as form of retrieval where the corpus is small set of query-specific options (see Figure 1), thus it uses our retrieval code implementation. Clustering We use k-means clustering (with set to the number of true labels) and Normalized Mutual Information (NMI) [19, 89] as the main metric to evaluate if image embeddings group meaningfully in the embedding space according to the labels. Visual STS Semantic textual similarity (STS) is an established task to evaluate text embeddings [3, 9]. It measures the similarity of text embeddings compared to human annotations via Spearman correlation. In MIEB, we conceptualize Visual STS [105] as an out-of-distribution task to assess how good vision encoders are at understanding relative semantics of texts. We implement it by rendering STS tasks into images to be embedded by models. We compute embedding similarity scores and compare with human annotations at the dataset level using Spearman correlation as the primary metric, following practices for STS evaluation [73]. Leveraging this novel protocol, we reveal optical character recognition (OCR) of models like CLIP, which have largely gone unnoticed. 2.2. Design Considerations zero-shot Generalization We evaluation emphasize where models are not fine-tuned for specific tasks; only their embeddings are used. special case is linear probing, where frozen embeddings are used to train linear model. However, as the embedded information is not modified, we still consider it zero-shot. Usability In line with MTEB [73], we prioritize: 1) Simplicity: New models can be added and benchmarked in less than 5 lines of code by using our existing implementations or defining new model wrapper that can produce image embeddings and text embeddings with the model checkpoint; 2) Extensibility: New dataset can be added via single file specifying the download location of dataset in the correct format, its name, and other metadata; 3) Reproducibility: The benchmark is fully reproducible by versioning at model and dataset level; 4) Diversity; MIEB covers 8 diverse task categories with many different individual tasks, assessing distinct abilities for comprehensive benchmarking and flexibility to explore specific capabilities. 3. Models We evaluate three main model categories on MIEB. Note that the categories may overlap. 3.1. Vision-only Models MOCO-v3 [13] builds upon MOCO-v1/2 with the ViT architecture and random patch projection technique to enhance training stability. DINO-v2 [80] scales selfsupervised learning to 142M images with similarity-based curation. Different from previous computer vision systems that are trained to predict fixed set of predetermined object categories (e.g., ImageNet models [51]), these models are also referred to as self-supervised models. 3.2. CLIP Models [84] CLIP (Contrastive Language-Image Pre-training) trains models simultaneously on text-image pairs. We evaluate many models across this line of research including CLIP, SigLIP [116], ALIGN [45], Jina-CLIP [52], DataComp-CLIP [31], Open-CLIP [16], and EvaCLIP [91]. These models are also sometimes referred to as language-supervised models [84, 95]. We also evaluate VISTA [118], which fuses ViT encoder [22] with pretrained language model followed by CLIP-style training. 3.3. MLLM-based models Embedding models increasingly leverage MLLMs. For open-source models, we benchmark E5-V [46] and VLM2Vec [47]. E5-V uses pre-trained MLLMs followed by text-only contrastive fine-tuning with prompts like summarize the above sentence with one word and last-token pooling [72, 76], showing surprising generalization to images and interleaved encodings. VLM2Vec trains MLLM backbones on paired image-text datasets. We also evaluate the Voyage API model [2]. Recent multi-modal API embedding models optimize not only for standard image search, but also for business search applications like figure and table understanding, making them strong candidates for tasks that require deep visual-text understanding in MIEB. 4. Implementation Details For interleaved inputs in retrieval and other task categories, we follow the original implementation of each model if it is capable of taking in mixed-modality inputs [118], e.g., MLLM-based embedding models [46, 47]. Else, we by default apply simple sum operation on text and image embeddings [98] to attain interleaved embeddings, e.g., for CLIP-style models [31, 84, 91, 116]. 5. Experimental Results Table 2 presents the overall results for the top 20 models on MIEB (130 tasks) and MIEB-lite (51 tasks). We find that there is no universal embedding model with the best performance on all task categories. MLLM-based models lead in overall performance on MIEB and MIEB-lite, most notably excelling in visual text understanding and multilingual tasks. However, they perform worse than CLIP-style models in linear probing and zero-shot classification, indicating loss of precision in image representations. MLLM-based models struggle particularly with fine-grained classification tasks, such as bird species identification (see B, Tables 16, 17). Conversely, CLIP-style models are strong in traditional tasks like linear probing, zero-shot classification, and retrieval. Scaling model size, batch size, and dataset quality improves performance in clustering, classification, and retrieval, but not universally. These models struggle on interleaved retrieval, visual text representations, and multilingual tasks unless specifically optimized (e.g., the multilingual variant of SigLIP). The strong performance of MLLM-based embedding models and insights from their training recipes highlight potential pathway for future universal embedding models. E5-V [46], LLaVA-based model [64], achieves state-ofthe-art open-source performance on document understanding, visual STS, multilingual retrieval, and compositionality, despite using small batch size of 768 for text-only lightweight contrastive finetuning. This suggests its generative pretraining already leads to strong multimodal representations. However, it performs poorly on linear probing and zero-shot classification. Focusing on such tasks in larger scale finetuning stage may lead to good universal performance."
        },
        {
            "title": "We analyze each category in the following sections and",
            "content": "refer to the Appendix for full results. 5.1. Retrieval Table 21 contains the full retrieval results. The best overall performance is achieved by CLIP-ViT-bigG-laion2B-39Bb160k [16] and siglip-so400m-patch14-384 [116]. We find that MLLM-based models with their natural interleaved encoding abilities excel on sub-categories like VQA retrieval (retrieving correct answers given questions and images). For some tasks vision-only models can achieve the best performance, e.g., Dino-v2 [80] on CUB200. 5.2. Clustering Table 9 contains the full clustering results. Similar to findings for Retrieval, MLLM-based models fall short on tasks Model Name () Voyage-multimodal-3 E5-V siglip-so400m-patch14-384 siglip-large-patch16-384 siglip-large-patch16-256 siglip-base-patch16-512 CLIP-ViT-bigG-14-laion2B siglip-base-patch16-384 EVA02-CLIP-bigE-14-plus CLIP-ViT-L-14-DataComp.XL siglip-base-patch16-256(m) CLIP-ViT-H-14-laion2B CLIP-ViT-g-14-laion2B EVA02-CLIP-bigE-14 siglip-base-patch16-256 siglip-base-patch16-224 CLIP-ViT-L-14-laion2B VLM2Vec-LoRA VLM2Vec-Full clip-vit-large-patch14 Model Name () Voyage-multimodal-3 siglip-so400m-patch14-384 siglip-large-patch16-384 E5-V siglip-large-patch16-256 CLIP-ViT-bigG-14-laion2B siglip-base-patch16-512 EVA02-CLIP-bigE-14-plus siglip-base-patch16-384 CLIP-ViT-L-14-DataComp.XL CLIP-ViT-H-14-laion2B EVA02-CLIP-bigE-14 siglip-base-patch16-256(m) CLIP-ViT-g-14-laion2B siglip-base-patch16-256 CLIP-ViT-L-14-laion2B clip-vit-large-patch14 siglip-base-patch16-224 CLIP-ViT-B-16-DataComp.XL VLM2Vec-LoRA Model Type MLLM MLLM Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. MLLM MLLM Enc. Model Type MLLM Enc. Enc. MLLM Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. Enc. MLLM Rtrv. Clus. (45) 38.8 34.0 40.8 39.9 38.8 38.1 41.5 37.7 40.1 38.1 35.6 39.7 39.8 39.0 36.6 36.3 38.0 27.7 27.6 33.7 (5) 82.4 70.0 82.1 79.9 82.1 74.7 85.6 76.3 92.4 86.4 74.6 83.9 82.7 89.4 75.2 74.5 83.5 72.6 70.7 76.4 Rtrv. Clus. (11) 33.2 32.4 31.9 26.9 31.0 34.2 30.8 35.2 30.6 31.0 32.8 34.3 28.2 33.5 29.5 31.1 26.7 29.3 28.3 21.0 (2) 76.6 75.9 75.2 51.7 76.5 80.8 69.7 87.3 72.2 80.4 79.3 86.7 68.2 76.8 69.6 76.4 71.3 68.4 73.6 66.3 MIEB Full (130 tasks) ZS. (23) 58.2 50.0 70.8 68.0 67.7 64.1 69.4 64.1 70.8 68.4 61.2 67.5 67.9 69.3 63.1 62.6 65.8 46.3 46.3 62.1 LP. (22) 71.3 74.5 84.6 83.7 82.5 80.9 83.6 80.6 86.0 82.0 78.9 82.5 82.8 84.5 79.7 79.3 81.2 62.0 62.0 80.1 Cmp. (7) 43.5 46.3 40.4 39.7 40.8 37.5 42.4 38.5 45.7 39.1 38.1 42.0 41.9 42.4 39.5 39.8 40.8 34.6 35.4 44.8 MIEB-lite (51 tasks) ZS. (7) 48.6 73.8 71.3 36.2 70.3 72.4 66.3 74.0 66.0 69.4 69.4 73.0 63.2 69.6 65.6 67.8 63.8 65.0 61.9 32.1 LP. (8) 69.3 78.8 77.7 70.6 76.3 77.8 74.6 80.0 74.4 75.3 76.8 78.3 73.4 77.3 73.6 75.9 74.5 73.5 73.2 64. Cmp. (6) 35.8 32.8 32.1 39.4 33.4 35.0 29.7 38.9 31.0 31.6 34.8 35.1 30.7 34.7 32.2 33.6 39.4 32.5 31.4 29.4 VC. (6) 48.6 51.9 46.3 45.4 44.9 53.2 43.2 52.8 39.4 52.3 51.3 45.8 44.2 43.6 52.2 51.1 45.9 62.0 62.1 44. VC. (5) 50.0 48.0 46.8 52.6 46.5 43.0 55.5 38.8 55.1 54.9 46.8 44.4 53.3 45.0 54.4 46.9 44.9 53.0 56.9 65.3 Doc. (10) 71.1 62.7 56.4 53.3 39.4 52.1 43.2 45.0 32.3 38.6 26.4 40.4 37.6 31.6 31.7 26.2 36.3 49.7 49.8 38. Doc. (6) 63.5 46.9 44.9 56.0 31.9 35.5 42.6 26.2 37.1 30.8 33.7 25.1 22.9 29.9 25.0 28.7 29.4 20.9 22.7 42.7 vSTS (en) (7) Rtrv. (m) (3 (55)) vSTS (x&m) (2 (19)) Mean Mean (m) (en) (130) (125) 81.8 79.3 68.0 69.5 67.4 67.7 70.9 67.0 72.0 69.9 65.5 65.5 69.1 68.8 66.2 64.3 65.8 72.6 72.6 64.5 58.9 66.6 40.2 51.1 49.8 43.2 28.0 42.5 27.8 23.8 59.2 25.5 25.9 25.5 41.3 41.2 23.0 34.9 35.0 20.2 70.4 46.3 41.4 39.8 38.1 38.1 34.5 37.5 28.2 35.8 40.3 33.9 31.7 28.3 34.4 33.5 26.0 42.2 42.2 35.1 62.0 58.6 61.2 59.9 57.9 58.5 60.0 57.8 59.8 59.4 53.9 58.4 58.3 58.6 55.5 54.3 57.2 53.4 53.3 55.4 62.5 58.2 57.1 57.0 55.2 54.9 54.2 54.2 53.5 53.4 53.1 52.7 52.4 52.2 52.0 50.9 50.6 50.5 50.4 49. vSTS (en) (2) Rtrv. (m) (2 (47)) vSTS (x&m) (2 (19)) Mean Mean (m) (en) (51) (47) 84.2 69.6 69.6 81.2 67.6 73.4 67.1 73.7 66.9 72.5 68.3 69.9 63.7 71.6 66.1 68.7 69.4 64.2 69.7 70.9 49.0 35.4 43.5 58.3 42.6 26.2 34.8 26.0 34.5 22.6 23.9 23.9 52.9 24.2 33.5 21.4 19.8 33.6 19.9 24. 70.4 41.4 39.8 46.3 38.1 34.5 38.1 28.2 37.5 35.8 33.9 28.3 40.3 31.7 34.4 26.0 35.1 33.5 28.5 42.2 57.7 57.3 56.2 51.8 54.2 56.5 54.5 56.8 54.1 55.7 55.2 55.9 50.4 54.8 52.0 53.6 52.4 50.8 52.2 49.1 58.1 53.5 53.3 51.9 51.4 51.3 50.9 50.8 50.5 50.4 50.0 49.9 49.7 49.4 48.4 47.6 47.4 47.4 46.6 46.0 Table 2. MIEB results broken down by task categories for the top 20 models. We provide averages of both English and multilingual tasks. Models are ranked by the Mean (m) column. Shortcuts are x=Crosslingual, m=Multilingual, en=English, and task categories from Figure 1. We refer to the leaderboard for the latest version: https://hf.co/spaces/mteb/leaderboard with fine-grained categories (e.g., dog breeds in ImageNetDog15 [21]), indicating their limitations in encoding nuanced image features. Figure 2 is UMAP visualization on ImageNet Dog15, where E5-V underperforms CLIP-style models, showing less separation between fine-grained labels. EVA-CLIP [91], DataComp-CLIP [31], and OpenCLIP checkpoints [16] dominate in most clustering tasks. Similar to patterns in classification shown in the next section, state-of-the-art MLLM-based models have poor performance distinguishing fine-grained classes. 5.3. Zero-shot Classification Similar to Retrieval and Clustering, Zero-shot Classification (Tables 18, 19) requires coherent image and text embedding subspaces, thus CLIP-style models still dominate. MLLMbased models like E5-V, Voyage, and VLM2Vec largely underperform in zero-shot classification tasks, most notably ones with fine-grained labels. While decoder-based generative models show inherent generalizability in embedding tasks [24, 46, 74, 90, 97, 106], it is likely still necessary to learn robust fine-grained nuances through contrasting multimodality finetuning paired with validated training recipes Model Name E5-V SigLIP VISTA (m3) VLM2Vec Open-CLIP EVA02-CLIP xFlickr&CO avg. var. XM3600 var. avg. WIT avg. avg. var. avg. var. 90.8 80.4 65.3 63.8 35.9 35.6 0.1 1.2 0.2 3.8 9.3 9.4 74.8 65.6 48.5 27.0 20.5 20.1 3.5 5.3 2.0 4.7 6.0 6.0 57.3 54.4 49.3 31.7 37.8 37.4 0.6 1.3 0.4 2.5 6.5 6. 74.3 66.8 54.4 40.8 31.4 31.0 1.4 2.6 0.9 3.6 7.3 7.2 Table 3. Performance of models on multilingual retrieval tasks across 38 languages. We compute the average performance across languages (avg) and the respective variance (var). We take the best variant from each top-6 model family. 12 13 15 16 17 STS* v-STS (ours) 80.0 73. 89.9 78.2 85.7 74.9 89.1 84.2 85.9 79.5 87.9 85.8 83.5 79. avg. 86.0 79.3 Table 4. E5-V performance on regular STS and our Visual STS. *: numbers from Jiang et al. [46]. Columns are STS12-17 and STS-b. tasks as images (2), if model is perfect in optical character recognition (OCR), its Visual STS performance would match its STS performance. Table 4 shows that this is almost the case, with some room left for improving the text recognition capabilities of E5-V. Tong et al. [95] show that textually-supervised models like CLIP are inherently good visual text readers, while purely visually-supervised models are not. Our results support this finding: EVA-CLIP, DataComp-CLIP (OpenCLIP variants trained on DataComp [31]), SigLIP, and CLIP achieve strong performance with EVA-CLIP-bigE-14-plus achieving an average English performance of 71.99 in Table 12, whereas Dino-v2 and Moco-v3 perform near random (Spearman correlation of 12.98 and 14.31). 5.7. Document Understanding As shown in 5.6, E5-V has strong OCR performance. This translates to strong performance on our Document Understanding tasks  (Table 15)  , where it is the best opensource model (avg. nDCG@5 of 62.69 on 10 Vidore tasks). Voyage-multimodal-3 has better performance but is closedsource. OpenCLIP [16] and DataComp-CLIP [31] variants provide insights into the positive impact of scaling model sizes and datasets to document understanding capabilities. The performance of OpenCLIP scales from 36.26 for its 430M parameter model (Vit-L) to 40.41 for its 990M parameter model (ViT-H); both having seen the same number of training examples. Data quality also matters with DataCompCLIP achieving 38.64 with ViT-L trained on only 13B seen examples, while the above OpenCLIP models use 32B examples. Figure 2. UMAP Visualization of ImageNet Dog15. Each class corresponds to one dog breed. CLIP clusters are more distinct. like large batch sizes and diverse datasets [16, 31, 84, 91]. 5.4. Linear Probing Average performance on linear probing is generally the highest among all our task categories, signaling that it is closer to saturation. However, with relatively low overall average scores on MIEB, there is still significant room to improve on the benchmark. In 6.1, we investigate label granularity and ablate the number of shots in linear probing, validating the robustness of our design choice of 16-shot for few-shot linear probing (2). 5.5. Multilingual Retrieval Our multilingual retrieval tasks span 38 languages with 55 subtasks [8, 93]. We present the full results in Table 11 and summarize the key findings here in Table 3. E5-V [46] achieves state-of-the-art performance on multilingual retrieval, highlighting the inherent strong multilingual abilities of LLaVA-Next [63], which E5-V initializes from. E5-V was fine-tuned contrastively using LoRA [41], which only lightly modifies the underlying models, thus leaving most knowledge (such as about different languages) intact. The multilingual version of SigLIP [116], siglipbase-patch16-256-multilingual, attains the second best performance. VISTA [118] models also perform strongly despite their relatively small sizes, showing notable consistency across languages. This cross-lingual robustness likely stems from its frozen backbone text model BGE-M3, which was trained to produce high-quality multilingual textual embeddings [11, 108]. Overall, these findings highlight that strong text encoder trained across various languages is critical to good multilingual performance. 5.6. Visual STS For Visual STS (Tables 12, 13, 14), E5-V [46] achieves the best performance. This is likely because it was trained on the allNLI collection (SNLI [7] + MNLI [101]), which is commonly used to train text representation models for STS tasks [85]. As our Visual STS simply renders existing STS Figure 3. Linear probing performance across different shots k. We select representative models from our vision-only and CLIP categories (3). See 6.1 for details on fine-grained and coarse-grained tasks. 5.8. Compositionality Evaluation Together with Retrieval, Compositionality Evaluation is Especially, where models have the lowest scores. WinoGround [94] is extremely challenging  (Table 20)  due to its image and textual confounders. We hypothesize that future models that better incorporate reasoning capabilities and test-time scaling techniques [35, 44, 68, 75, 109] may achieve better results on compositionality tasks. 5.9. Vision-centric QA BLIP models [58, 59] surprisingly contribute to two of the top 5 models in vision-centric QA  (Table 10)  despite their absence for other task categories. This highlights that including images in the contrastive finetuning stage can be beneficial, opposite to their exclusion in Jiang et al. [46]. 6. Discussions 6.1. K-shot Linear Probing We opt for k-shot linear probing instead of full-dataset linear probing as the default setting in MIEB (2) to make the evaluation cheaper given the large size of the benchmark. In Figure 3, we ablate this design by training kshot classifiers with in {8,16,32,64,128,256}. We find that different values of preserve the same model rank on both fine-grained classification (Birdsnap, Caltech101, CIFAR100, Country211, FGVCAircraft, Food101, Imagenet1k, OxfordFlowers, OxfordPets, RESISC45, StanfordCars, SUN397, UCF101) and coarse-grained classification (CIFAR10, DTD, EuroSAT, FER2013, GTSRB, MNIST, PatchCamelyon, STL10) tasks. As result, we choose modest 16-shot evaluation by default. Figure 4. Correlations between performance on generative MLLM benchmarks from Tong et al. [95] (y-axis) and our Visual STS (x-axis). High correlation means that our Visual STS tasks can predict generative performance. 6.2. On the predictability of MLLM performance MLLM evaluation has been proposed as robust method to assess visual representations [95], where the performance of an MLLM provides information about the strength of its visual encoder. However, this evaluation paradigm is much more computationally intensive than benchmarking only the vision encoder, given the large sizes of MLLMs and the large hyperparameter search space (data size, LLM choice, instruction-tuning details, etc.). Thus, it remains impractical as general benchmarking method. We explore the opposite: Can MLLM performance be predicted from the vision encoder [110]? To do so, we calculate correlations between vision encoder performance on MIEB tasks and their MLLM counterparts across 16 benchmarks using results from Tong et al. [95]. Figure 4 shows these correlations using our Visual STS protocol as an example [105]. Given the common need for visual text interpretation in MLLM tasks, vision encoders performance on Visual STS has strong correlation with the performance of their MLLM counterparts. The pattern is most pronounced for the 4 OCR and Chart tasks in [95], and least pronounced for CV-bench 3D, which relies little on visual text understanding. This highlights the utility of MIEB for selecting MLLM vision encoders. 6.3. MIEB-lite: lightweight Benchmark Computationally efficient benchmarks are more usable [25]. While MIEB avoids training MLLMs, evaluating 130 tasks remains resource-intensive. While more comprehensive coverage allows for more nuanced analysis, many tasks have high correlations (e.g., Visual STS in Figure 4). To enable lightweight evaluation, we build MIEB-lite by iteratively removing redundant tasks while preserving task category coverage and inter-task correlation. We first compute pairwise task correlations using model performance, then iteratively remove tasks with average correlations above 0.5 (11 tasks) and 0.45 (32 tasks). Key patterns emerged: 1) Established tasks (e.g., CLIP benchmark linear probing [84]) had high redundancy, possibly due to dataset exposure in pretraining; 2) Easy OCR tasks correlated unexpectedly with non-OCR tasks, though Visual STS and VIDORE remained distinct; 3) Novel tasks (e.g., ARO benchmark, M-BEIR protocols) had low correlations. To capture nuanced task relationships, we cluster tasks via UMAP+HDBSCAN [70, 71] using correlation vectors, yielding 17 interpretable clusters (e.g., fine-grained zeroshot, language-centric, easy OCR, VQA, low resolution tasks, etc). The outlier cluster (-1 label) spanned all categories, serving as foundation for balanced selection. MIEB-lite has 51 tasks by combining the above two approaches and excluding large-scale tasks (e.g., EDIS and GLD-v2 take 60-80 GPU hours for 7B models). MIEBlite reduces computation while maintaining category balModel Name E5-V CLIP (base-patch32) # Params Runtime (NVIDIA H100 GPU hours) (M) 8360 151 MIEB MIEB-lite Reduction % 264.0 16. 46.4 4.5 82.4% 72.9% Table 5. MIEB vs. MIEB-lite runtime comparison. ance and diagnostic power: 1) Table 5 compares model runtime on MIEB and MIEB-lite showing reduction of 82.4% for E5-V, an 8B model. 2) We find that the overall average performance of 38 models on MIEB and MIEB-lite has Spearman correlation of 0.992 and Pearson correlation of 0.986. See Tables 6, 7, and 8 for all MIEB-lite tasks. 7. Related Work Benchmarks Prior efforts toward universal image embedding benchmarks focus on narrow scopes. The CLIP Benchmark [84] evaluates semantic similarity via classification and retrieval, while UnED [113] and M-BEIR [98] expand retrieval evaluation to multi-domain and mixedmodality settings. However, three critical gaps persist: (1) Limited task diversity: Existing benchmarks overlook tasks like multi-modal composition [114], social media understanding [48], and multilingual evaluation [8], restricting cross-domain insights. (2) Neglect visual text tasks: While understanding text in images is key to many MLLM use cases [27], benchmarks for OCR [66] and visual document retrieval remain sparse. (3) Under-explored instruction tuning: Though instruction-tuned embeddings show promise for generalization [60, 117], their evaluation beyond retrieval is limited. MIEB addresses these gaps via unified protocols spanning 130 tasks, consolidating prior benchmarks into holistic framework. Protocol limitations Prior work relies heavily on linear probing and retrieval [38, 84], which struggle to assess generalization to complex tasks. While fine-tuning [12] adapts embeddings to specific tasks, it incurs high computational costs and risks overfitting. MIEB evaluates frozen embeddings through broader suite of protocols including retrieval, linear probing, zero-shot classification, and novel additions like pair-wise classification and clustering, providing more flexible and comprehensive assessment. 8. Conclusion We introduce the Massive Image Embedding Benchmark (MIEB), which consists of 8 task categories with 130 individual tasks covering 38 languages. We benchmark 50 models on MIEB, providing baselines and insights for future research. Our findings highlight the importance of evaluating vision embeddings beyond classification and retrieval, and their role in facilitating multimodal generative models."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Weijia Shi for feedback. We thank Contextual AI for supporting this benchmark. We thank all members of the MTEB community for their efforts in advancing the framework. We thank the creators of VLM2Vec for discussions."
        },
        {
            "title": "References",
            "content": "[1] Nomic embed vision: Expanding the nomic latent space. https://www.nomic.ai/blog/posts/nomicembed-vision. 31 all-in-one embedding model for [2] voyage-multimodal-3: https : interleaved text, //blog.voyageai.com/2024/11/12/voyagemultimodal-3/. 4, 31 images, and screenshots. [3] Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. *SEM 2013 shared task: SeIn Second Joint Conference on mantic textual similarity. Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 3243, Atlanta, Georgia, USA, 2013. Association for Computational Linguistics. 3 [4] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes, 2018. 2 [5] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014. 17 [6] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 17 [7] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632642, Lisbon, Portugal, 2015. Association for Computational Linguistics. 6 [8] Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, and Ivan Vulic. IGLUE: benchmark for transfer learning across In Proceedings of the modalities, tasks, and languages. 39th International Conference on Machine Learning, pages 23702392. PMLR, 2022. 6, 8, [9] Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 114, Vancouver, Canada, 2017. Association for Computational Linguistics. 3 [10] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1649516504, 2022. 16 [11] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multilingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. 6 [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. 1, 8 [13] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96409649, 2021. 2, 4, [14] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer visual information-seeking questions? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1494814968, 2023. 16 [15] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):18651883, 2017. 17 [16] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image In Proceedings of the IEEE/CVF Conference learning. on Computer Vision and Pattern Recognition, pages 2818 2829, 2023. 2, 4, 5, 6, 31 [17] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 17 [18] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 215 223, Fort Lauderdale, FL, USA, 2011. PMLR. 17 [19] Andre Collignon, Frederik Maes, Dominique Delaere, Dirk Vandermeulen, Paul Suetens, Guy Marchal, et al. Automated multi-modality image registration based on information theory. In Information processing in medical imaging, pages 263274. Citeseer, 1995. [20] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Wang. Image retrieval: Ideas, influences, and trends of the new age. ACM Computing Surveys (Csur), 40(2):160, 2008. 2 [21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 3, 5, 17 [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4 [23] Mathias Eitz, James Hays, and Marc Alexa. How do huACM Transactions on graphics mans sketch objects? (TOG), 31(4):110, 2012. 16 [24] Kenneth Enevoldsen, Marton Kardos, Niklas Muennighoff, and Kristoffer Nielbo. The scandinavian embedding benchmarks: Comprehensive assessment of multilingual and monolingual text embedding. Advances in Neural Information Processing Systems, 37:4033640358, 2024. 5 [25] Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Marton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Omer agatan, Jonathan RystrÃ¸m, Roman Solomatin, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, RafaÅ Poswiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bjorn Pluster, Jan Philipp Harries, LoÄ±c Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek ËSuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Gunther, Mengzhou Xia, Weijia Shi, Xing Han L`u, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, WenDing Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. Mmteb: Massive multilingual text embedding benchmark, 2025. [26] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) International Journal of Computer Vision, 88 challenge. (2):303338, 2010. 17 [27] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models, 2024. 2, 8, 16 [28] Li Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 Conference on Computer Vision and Pattern Recognition Workshop, pages 178178, 2004. 17 [29] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. Advances in Neural Information Processing Systems, 36, 2024. 16 [30] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 16, 18 [31] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024. 4, 5, 6, [32] Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. Learning image and user features for recommendation in social networks. In Proceedings of the IEEE international conference on computer vision, pages 42744282, 2015. 1 [33] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples, 2015. 17 [34] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 16 [35] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 7 [36] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from In Proceedings of the IEEE conference on blind people. computer vision and pattern recognition, pages 36083617, 2018. 16 [37] Xintong Han, Zuxuan Wu, Phoenix Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry Davis. Automatic spatially-aware fashion concept discovery. In Proceedings of the IEEE international conference on computer vision, pages 14631471, 2017. 16 [38] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual reparXiv preprint arXiv:1911.05722, resentation learning. 2019. 1, 8 [39] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. [40] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 18 [41] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6 [42] Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and MingWei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1206512075, 2023. 16 [43] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based In Proceedings of the 26th retrieval in facebook search. ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 25532561, 2020. 1 [44] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 7 [45] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904 4916. PMLR, 2021. 4, 31 [46] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, E5-v: Universal embeddings and Fuzhen Zhuang. with multimodal large language models. arXiv preprint arXiv:2407.12580, 2024. 4, 5, 6, 7, 31 [47] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training visionlanguage models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. 4, [48] Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. Mm-soc: Benchmarking multimodal large language models in social media platforms. In ACL, 2024. 8 [49] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional lanIn Proceedings guage and elementary visual reasoning. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 17 [50] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:26112624, 2020. 16 [51] Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better?, 2019. 4 [52] Andreas Koukounas, Georgios Mastrapas, Michael Gunther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Joan Fontanals Sturua, Mohammad Kalim Akram, MartÄ±nez, Saahil Ognawala, et al. Jina clip: Your arXiv preprint clip model is also your text retriever. arXiv:2405.20204, 2024. 4, [53] Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting large-scale dataset of fine-grained cars. 2013. 16, 17 [54] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 17 [55] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Image rearXiv preprint Goyal, Edoardo Ponti, and Siva Reddy. trieval arXiv:2203.15867, 2022. 18 from contextual descriptions. [56] Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015. 17 [57] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. 17 [58] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888 12900. PMLR, 2022. 7, 31 [59] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning. JMLR.org, 2023. 7, 31 [60] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. MMEMBED: UNIVERSAL MULTIMODAL RETRIEVAL In The Thirteenth InterWITH MULTIMODAL LLMS. national Conference on Learning Representations, 2025. 8 [61] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 16 [62] Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and challenges in news image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 67616771, 2021. [63] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 6 [64] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 4 [65] Siqi Liu, Weixi Feng, Tsu-Jui Fu, Wenhu Chen, and William Wang. Edis: Entity-driven image search over multimodal web content. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48774894, 2023. 16 [66] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: On the hidden mystery of ocr in large multimodal models, 2024. 8 [67] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21252134, 2021. 16 [68] Ximing Lu, Seungju Han, David Acuna, Hyunwoo Kim, Jaehun Jung, Shrimai Prabhumoye, Niklas Muennighoff, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, et al. Retro-search: Exploring untaken paths for deeper and efficient reasoning. arXiv preprint arXiv:2504.04383, 2025. 7 [69] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft, 2013. 17 [70] Leland McInnes, John Healy, Steve Astels, et al. hdbscan: Hierarchical density based clustering. J. Open Source Softw., 2(11):205, 2017. [71] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. 8 [72] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022. 4 [73] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, 2023. 1, 2, 3 [74] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. arXiv Generative representational preprint arXiv:2402.09906, 2024. 5 instruction tuning. [75] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 7 [76] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code arXiv preprint embeddings by contrastive pre-training. arXiv:2201.10005, 2022. 4 [77] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722729, 2008. [78] Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training reproducible long context text embedder, 2024. 31 [79] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 40044012, 2016. 16 [80] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pages 131, 2024. 1, 2, 4, 31 [81] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3498 3505, 2012. 17 [82] Jingtian Peng, Chang Xiao, and Yifan Li. Rp2k: largescale retail product dataset for fine-grained image classification. arXiv preprint arXiv:2006.12634, 2020. 16 [83] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and OndËrej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [84] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 1, 2, 3, 4, 6, 8, 17, 31 [85] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 39823992, 2019. 3, 6 [86] Chhavi Sharma, Deepesh Bhageria, William Scott, Srinivas Pykl, Amitava Das, Tanmoy Chakraborty, Viswanath Pulabaigari, and Bjorn Gamback. Semeval-2020 task 8: Memotion analysis-the visuo-lingual metaphor! In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 759773, 2020. 16 [87] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild, 2012. 17 [88] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: multi-class classification competition. In The 2011 International Joint Conference on Neural Networks, pages 14531460, 2011. 17 [89] Colin Studholme, Derek LG Hill, and David Hawkes. An overlap invariant entropy measure of 3d medical image alignment. Pattern recognition, 32(1):7186, 1999. 3 [90] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, et al. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883, 2024. 5 [91] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 4, 5, 6, [92] Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. Beir: heterogeneous benchmark for zero-shot evaluation of information retrieval In Thirty-fifth Conference on Neural Informamodels. tion Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 2 [93] Ashish Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715729, 2022. 6, 16 [94] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for In Proceedings of the visio-linguistic compositionality. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 52385248, 2022. 7, 18 [95] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 1, 3, 4, 6, 7, 8, 18 [96] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In Medical Image Computing and Computer Assisted Intervention MICCAI 2018, pages 210 218, Cham, 2018. Springer International Publishing. 17 [97] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective works best for zero-shot generalization? In International Conference on Machine Learning, pages 2296422984. PMLR, 2022. 5 [98] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. UniIR: Training and benchmarking universal multimodal information retrievers. arXiv preprint arXiv:2311.17136, 2023. 2, 4, [99] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. 16 [100] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2 - large-scale benchmark for instance-level recognition and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2, 16 [101] Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 11121122, New Orleans, Louisiana, 2018. Association for Computational Linguistics. 6 [102] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1130711317, 2021. 16 [103] Pengxiang Wu, Siman Wang, Kevin Dela Rosa, and Derek Hao Hu. Forb: flat object retrieval benchmark for universal image embedding, 2023. 16 [104] Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, and Chenghua Lin. Scimmir: Benchmarking scientific In Proceedings of the multi-modal information retrieval. 62nd Annual Meeting of the Association for Computational Linguistics (ACL), findings, 2024. 16, [105] Chenghao Xiao, Zhuoxu Huang, Danlu Chen, Thomas Hudson, Yizhi Li, Haoran Duan, Chenghua Lin, Jie Fu, Jungong Han, and Noura Al Moubayed. Pixel sentence representation learning. arXiv preprint arXiv:2402.08183, 2024. 3, 8, 18 [106] Chenghao Xiao, Thomas Hudson, and Noura Al Moubayed. Rar-b: Reasoning as retrieval benchmark. arXiv preprint arXiv:2404.06347, 2024. 5 [107] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 34853492, 2010. 17 [108] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597, 2023. 6 [109] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [110] Shijia Yang, Bohan Zhai, Quanzeng You, Jianbo Yuan, Hongxia Yang, and Chenfeng Xu. Law of vision representation in mllms. arXiv preprint arXiv:2408.16357, 2024. 8 [111] Peter Young, Alice Lai, Micah Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 16 [112] Nikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos Tolias. The Met dataset: Instance-level recognition for artIn Thirty-fifth Conference on Neural Informaworks. tion Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 2, 16 [113] Nikolaos-Antonios Ypsilantis, Kaifeng Chen, Bingyi Cao, Mario Lipovsk`y, Pelin Dogan-Schonberger, Grzegorz Makosa, Boris Bluntschli, Mojtaba Seyedhosseini, OndËrej Chum, and Andre Araujo. Towards universal image embeddings: large-scale dataset and challenge for generic image representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11290 11301, 2023. 8 [114] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? In International Conference on Learning Representations, 2023. 3, 8, 18 [115] Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. Learning unified embedding for visual search at pinterest. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, page 24122420, New York, NY, USA, 2019. Association for Computing Machinery. [116] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 1, 4, 6, 31 [117] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms, 2024. 8, 31 [118] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. Vista: Visualized text embedding for universal multi-modal retrieval. arXiv preprint arXiv:2406.04292, 2024. 4, 6, 31 A. Tasks overview C. Overall Results & First MIEB Leaderboard Based on the per-task category results, we provide an overall ranking in Table 22, aggregating all results. Note that we currently exclude all models that are not able to evaluate on all tasks in the overall table, including vision-only models like Dino-2 and Moco-v3 that are not able to test on image-text tasks, yielding 36 models in the first MIEB leaderboard. Note that for models that are not in the overall table, we refer readers to per task category tables for details. D. Models All models used in evaluations are listed in Table 23. This appendix provides detailed information on all tasks within MIEB, including size, language, metrics, and other relevant details. Note that we present the categories based on Abstask implementations here. We recommend refer to Table 1 for the taxonomy based on capabilities assessed. Table 6 shows all information related to retrieval tasks. Table 7 presents data related to clustering, standard image classification, zero-shot classification, and multi-label image classification tasks. Lastly, Table 8 covers information for visual STS, text-based multiple choice, and image-text pair classification tasks. B. Per Task Category Results B.1. Clustering Table 9 presents clustering results of clustering tasks. B.2. Vision-centric QA Table 10 presents results of all Vision-centric QA tasks. B.3. Multilingual Retrieval Table 11 presents all multilingual retrieval task results, which include 54 subtask results from the 3 multilingual retrieval tasks. B.4. Visual STS Table 12 presents English-only STS results across 7 STS tasks. Table 13 presents cross-lingual STS results across 11 language pairs. Table 14 presents multilingual STS results across 10 languages. B.5. Document Understanding Table 15 presents document understanding results. B.6. Linear Probe Table 16 and Table 17 respectively present linear probing results for coarse-grained and fine-grained classification tasks. B.7. Zeroshot Classification Table 18 and Table 19 respectively present zero-shot classification results for coarse-grained and fine-grained classification tasks. B.8. Compositionality Table 20 presents results of compositionality tasks. B.9. Retrieval Table 21 presents results of retrieval tasks. Type (# tasks) Task MIEB-lite # Queries # Documents # Qrels Avg. # Choices Any2AnyRetrieval BLINKIT2IRetrieval [30] BLINKIT2TRetrieval [30] CIRRIT2IRetrieval [67] CUB200I2IRetrieval [99] EDIST2ITRetrieval [65] Fashion200kI2TRetrieval [37] Fashion200kT2IRetrieval [37] FashionIQIT2IRetrieval [102] Flickr30kI2TRetrieval [111] Flickr30kT2IRetrieval [111] FORBI2IRetrieval [103] GLDv2I2IRetrieval [100] GLDv2I2TRetrieval [100] HatefulMemesI2TRetrieval [50] HatefulMemesT2IRetrieval [50] InfoSeekIT2ITRetrieval [14] InfoSeekIT2TRetrieval [14] MemotionT2IRetrieval [86] METI2IRetrieval [112] MSCOCOI2TRetrieval [61] MSCOCOT2IRetrieval [61] NIGHTSI2IRetrieval [29] OVENIT2ITRetrieval [42] OVENIT2TRetrieval [42] ROxfordEasyI2IRetrieval [83] ROxfordMediumI2IRetrieval [83] ROxfordHardI2IRetrieval [83] RP2kI2IRetrieval [82] RParisEasyI2IRetrieval [83] RParisMediumI2IRetrieval [83] RParisHardI2IRetrieval [83] SciMMIRI2TRetrieval [104] SciMMIRT2IRetrieval [104] SketchyI2IRetrieval [112] SOPI2IRetrieval [79] StanfordCarsI2IRetrieval [53] TUBerlinT2IRetrieval [23] VidoreArxivQARetrieval [27] VidoreDocVQARetrieval [27] VidoreInfoVQARetrieval [27] VidoreTabfquadRetrieval [27] VidoreTatdqaRetrieval [27] VidoreShiftProjectRetrieval [27] VidoreSyntheticDocQAAIRetrieval [27] VidoreSyntheticDocQAEnergyRetrieval [27] VidoreSyntheticDocQAGovernmentReportsRetrieval [27] VidoreSyntheticDocQAHealthcareIndustryRetrieval [27] VisualNewsI2TRetrieval [62] VisualNewsT2IRetrieval [62] VizWizIT2TRetrieval [36] VQA2IT2TRetrieval [34] WebQAT2ITRetrieval [10] WebQAT2TRetrieval [10] WITT2IRetrieval [8] XFlickr30kCoT2IRetrieval [8] XM3600T2IRetrieval [93] 285 1073 4170 5794 3241 4889 1719 6003 31014 31014 13250 1129 1972 829 829 17593 11323 700 87942 5000 24809 2120 14741 50004 70 70 70 39457 70 70 70 16263 16263 452886 120053 8041 250 500 500/451 500/494 280 1646 100 100 100 100 100 20000 19995 4319 214354 2511 2455 9790 570 26 21551 5794 1047067 61707 201824 74381 155070 155070 53984 761757 674 8045 8045 481782 611651 6988 260655 24809 5000 40038 335135 676667 4993 4993 4993 39457 6322 6322 6322 16263 16263 25000 120053 8041 20000 500 500 500 70 277 1000 968 977 972 965 537568 542246 2091 21597 403196 544457 8553 285 1073 4216 163756 8341 4889 4847 6014 155070 155070 13250 15138 1939 829 829 131376 73869 700 172713 24989 24989 2120 261258 492654 345657 345657 345657 4409419 435387 435387 435387 16263 16263 90577200 840927 325570 20000 500 500 500 280 1663 1000 1000 1000 1000 1000 20000 20000 4319 214354 3627 5002 16000 16000 16000 129600 259200 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Table 6. Datasets overview and metadata for Any2AnyRetrieval task. Supported LanguagesSupported Languages Queries per Language (multi) Metric - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 792, 806, 814, 541, 780, 854, 842, 889, 681, 869, 685 2000 each 3600 each Recall@1 Recall@1 NDCG@10 Recall@1 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 Recall@1 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 Recall@1 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 map@5 map@5 map@5 Recall@1 map@5 map@5 map@5 NDCG@10 NDCG@10 Recall@1 Recall@1 Recall@1 NDCG@10 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@5 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@ en en en - en en en en en en - - en en en en en en - en en en en en - - - - - - - en en en - - en en en en fr en fr en en en en en en en en en en ar, bg, da, el, et, id, ko, ja, tr, vi, en de, en, es, id, ja, ru, tr, zh ar, bn, cs, da, de, el, en, es, fa, fi, fil, fr, hi, hr, hu, id, it, he, ja, ko, mi, nl, no, pl, pt, quz, ro, ru, sv, sw, te, th, tr, uk, vi, zh Type Task MIEB-lite # Samples Train # Samples Test # Labels Metric ImageClassification ImageMultiLabelClassification* ImageClustering ZeroShotClassification Birdsnap [5] Caltech101 [28] CIFAR10 [54] CIFAR100 [54] Country211 [84] DTD [17] EuroSAT [39] FER2013 [33] FGVCAircraft [69] Food101Classification [6] GTSRB [88] Imagenet1k [21] MNIST [57] OxfordFlowersClassification [77] OxfordPets [81] PatchCamelyon [96] RESISC45 [15] StanfordCars [53] STL10 [18] SUN397 [107] UCF101 [87] VOC2007 [26] CIFAR10Clustering [54] CIFAR100Clustering [54] ImageNetDog15Clustering [21] ImageNet10Clustering [21] TinyImageNetClustering [56] BirdsnapZeroShot [5] Caltech101ZeroShot [28] CIFAR10ZeroShot [54] CIFAR100ZeroShot [54] CLEVRZeroShot [49] CLEVRCountZeroShot [49] Country211ZeroShot [84] DTDZeroShot [17] EuroSATZeroShot [39] FER2013ZeroShot [33] FGVCAircraftZeroShot [69] Food101ZeroShot [6] GTSRBZeroShot [88] Imagenet1kZeroShot [21] MNISTZeroShot [57] OxfordPetsZeroShot [81] PatchCamelyonZeroShot [96] RenderedSST2 [84] RESISC45ZeroShot [15] SciMMIR [104] StanfordCarsZeroShot [53] STL10ZeroShot [18] SUN397ZeroShot [107] UCF101ZeroShot [87] 2674 3060 50000 50000 28000 3760 16200 28709 - 75750 26640 45200 60000 7169 3680 262144 18900 8144 5000 76127 1786096 - - - - - - 2674 3060 50000 50000 51600 51600 28000 3760 16200 28709 - 75750 26640 45200 60000 3680 262144 6920 18900 498279 8144 5000 76127 1786096 NMI Accuracy 1851 6084 10000 10000 21100 1880 5400 7178 3333 25300 12630 37200 10000 1020 3669 32768 6300 8041 8000 21750 697222 500 101 10 100 211 47 10 7 - 101 43 744 10 102 37 2 45 196 10 397 101 4952 [1 5] Accuracy 10 10000 100 10000 15 1076 10 13000 200 10000 500 1851 101 6084 10 10000 100 10000 6 15000 15000 8 211 21100 47 1880 10 5400 7 7178 - 3333 101 25300 43 12630 744 37200 10 10000 37 3669 2 32768 1821 2 45 6300 5 16263 196 8041 10 8000 397 21750 101 697222 Accuracy Table 7. Datasets overview and metadata for ImageClassification, ImageMultiLabelClassification, ImageClustering and ZeroShotClassification tasks. * For ImageMultiLabelClassification, the number of labels per sample is between the given interval. Further, we again note that with the large scales of training set in classification datasets, we adopt the few-shot linear probe paradigm in the evaluation. Type Task MIEB-lite # Samples Test # Choices Any2AnyMultiChoice ImageTextPairClassification* VisualSTS CVBenchCount [95] CVBenchRelation [95] CVBenchDepth [95] CVBenchDistance [95] BLINKIT2IMultiChoice [30] BLINKIT2TMultiChoice [30] AROCocoOrder [114] AROFlickrOrder [114] AROVisualAttribution [114] AROVisualRelation [114] SugarCrepe [40] Winoground [94] ImageCoDe [55] STS12VisualSTS [105] STS13VisualSTS [105] STS14VisualSTS [105] STS15VisualSTS [105] STS16VisualSTS [105] STS17MultilingualVisualSTS [105] 788 650 600 600 402 1073 25010 5000 28748 23937 7511 400 25322 5342 1500 3750 3000 1186 5346 [4-6] 2 2 2 2 [2-4] 5 5 2 2 2 2 10 - - - - - - STSBenchmarkMultilingualVisualSTS [105] 86280 - Supported Languages # Samples per language Metric Accuracy Text Accuracy Accuracy Cosine Spearman - - - - - - - - - - - - - - - - - - 250 each, except ko-ko with 2.85k 8628 each en en en en en en - - - - - - - en en en en en ar-ar, en-ar, en-de, en-en, en-tr, es-en, es-es, fr-en, it-en, ko-ko, nl-en en, de, es, fr, it, nl, pl, pt,ru, zh Table 8. Datasets overview and metadata for Any2AnyMutipleChoice, ImageTextPairClassification and Visual STS tasks. * For ImageTextPairClassification, only 1 caption is correct over all the available ones for sample. model name CIFAR10 CIFAR100 ImageNet ImageNetDog15 TinyImageNet Avg. EVA02-CLIP-bigE-14-plus EVA02-CLIP-bigE-14 EVA02-CLIP-L-14 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-H-14-laion2B-s32B-b79K nomic-ai/nomic-embed-vision-v1.5 laion/CLIP-ViT-L-14-laion2B-s32B-b82K facebook/dinov2-large facebook/dinov2-base laion/CLIP-ViT-g-14-laion2B-s34B-b88K EVA02-CLIP-B-16 voyage-multimodal-3 google/siglip-large-patch16-256 google/siglip-so400m-patch14-384 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K facebook/dinov2-giant facebook/dinov2-small google/siglip-large-patch16-384 laion/CLIP-ViT-B-32-laion2B-s34B-b79K Salesforce/blip-itm-large-coco laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K Salesforce/blip-itm-large-flickr openai/clip-vit-large-patch14 google/siglip-base-patch16-384 BAAI/bge-visualized-base google/siglip-base-patch16-256 google/siglip-base-patch16-512 google/siglip-base-patch16-256-multilingual google/siglip-base-patch16-224 blip2-pretrain nyu-visionx/moco-v3-vit-b Salesforce/blip-image-captioning-large BAAI/bge-visualized-m3 TIGER-Lab/VLM2Vec-LoRA nyu-visionx/moco-v3-vit-l TIGER-Lab/VLM2Vec-Full Salesforce/blip-itm-base-coco royokong/e5-v jinaai/jina-clip-v1 openai/clip-vit-base-patch16 openai/clip-vit-base-patch32 blip2-finetune-coco Salesforce/blip-itm-base-flickr Salesforce/blip-image-captioning-base kakaobrain/align-base 98.65 90.30 97.83 93.65 87.66 88.10 87.39 93.62 79.90 82.62 87.63 89.23 86.22 83.61 83.79 89.93 76.84 79.25 81.61 83.76 84.95 79.81 87.94 80.87 71.62 82.16 76.82 73.95 75.94 76.11 96.67 74.69 77.64 81.41 72.89 71.65 69.43 70.83 82.58 74.12 69.25 73.85 90.37 63.94 64.18 54.13 89.51 89.03 86.14 84.26 79.97 78.69 81.16 77.43 79.93 77.20 78.13 83.51 75.15 76.23 76.67 78.21 75.77 72.62 74.43 70.92 72.62 74.85 70.67 64.54 67.78 77.80 67.58 66.56 67.89 67.01 81.46 63.99 68.45 73.89 60.56 60.60 60.72 60.44 70.43 64.84 59.35 58.07 75.81 58.89 53.81 50.68 99.09 94.32 94.37 93.39 98.75 93.93 95.80 93.74 92.23 93.93 94.38 89.22 97.58 97.87 98.19 93.42 91.84 91.23 93.28 95.22 98.29 91.54 94.34 94.00 97.63 98.33 92.57 93.32 92.63 92.61 97.77 90.30 93.27 97.74 97.03 86.41 92.64 93.19 93.85 96.69 92.58 93.14 93.12 92.46 90.94 84.21 Table 9. Clustering Results. 91.08 89.85 83.57 82.60 86.09 85.93 81.19 81.09 86.20 85.67 81.46 74.82 83.82 86.40 83.57 74.98 92.63 87.27 84.17 76.31 67.56 73.68 68.36 72.83 86.16 49.37 80.47 79.61 80.44 78.18 20.27 80.77 67.38 43.07 71.48 80.70 69.29 70.31 36.73 52.66 63.25 54.12 8.97 66.00 58.78 58.88 83.57 83.58 79.44 78.28 75.49 72.67 72.65 71.62 77.22 74.31 72.10 75.96 69.29 66.52 68.31 72.13 70.13 70.65 66.21 63.76 64.24 66.98 60.86 69.82 58.15 73.28 58.73 59.82 55.88 58.58 73.86 59.53 61.74 71.72 61.22 59.14 61.51 58.19 66.64 61.47 62.90 60.34 70.92 55.07 47.76 50.03 92.38 89.42 88.27 86.44 85.59 83.86 83.64 83.50 83.10 82.74 82.74 82.55 82.41 82.13 82.11 81.73 81.44 80.20 79.94 77.99 77.53 77.37 76.43 76.41 76.27 76.19 75.24 74.65 74.56 74.50 74.01 73.86 73.70 73.57 72.64 71.70 70.72 70.59 70.05 69.95 69.47 67.90 67.84 67.27 63.09 59.59 model name CVBenchCount CVBenchDepth CVBenchDistance CVBenchRelation BLINKIT2IMultiChoice BLINKIT2TMultiChoice Avg. TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K google/siglip-base-patch16-512 blip2-pretrain google/siglip-base-patch16-384 blip2-finetune-coco BAAI/bge-visualized-base Salesforce/blip-itm-base-flickr laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K google/siglip-base-patch16-256 royokong/e5-v google/siglip-base-patch16-256-multilingual Salesforce/blip-itm-large-coco google/siglip-base-patch16-224 Salesforce/blip-image-captioning-large voyage-multimodal-3 Salesforce/blip-itm-base-coco Salesforce/blip-itm-large-flickr openai/clip-vit-base-patch16 nomic-ai/nomic-embed-vision-v1.5 google/siglip-so400m-patch14-384 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K laion/CLIP-ViT-L-14-laion2B-s32B-b82K laion/CLIP-ViT-H-14-laion2B-s32B-b79K kakaobrain/align-base jinaai/jina-clip-v1 google/siglip-large-patch16-384 EVA02-CLIP-B-16 google/siglip-large-patch16-256 laion/CLIP-ViT-g-14-laion2B-s34B-b88K openai/clip-vit-large-patch14 BAAI/bge-visualized-m3 EVA02-CLIP-bigE-14 Salesforce/blip-image-captioning-base laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-B-32-laion2B-s34B-b79K openai/clip-vit-base-patch32 EVA02-CLIP-bigE-14-plus EVA02-CLIP-L62.18 62.56 61.93 55.20 46.95 53.43 44.54 50.25 60.66 43.27 54.44 39.21 34.64 45.30 43.91 14.72 26.40 26.65 25.25 20.81 21.83 21.70 23.86 8.25 19.80 47.59 14.85 8.76 36.80 8.88 10.15 2.66 7.61 30.46 10.15 4.19 0.38 6.60 10.15 1.02 62.17 62.50 52.50 53.67 57.67 52.17 59.67 49.00 44.67 55.83 52.00 48.50 54.00 50.00 51.50 63.33 53.17 45.17 46.83 51.67 45.33 48.33 49.17 49.17 48.67 43.17 49.33 54.67 53.33 56.17 47.00 52.67 45.33 48.83 51.50 47.17 50.00 45.33 43.83 49.50 58.00 58.17 46.00 42.83 50.17 42.17 52.33 56.33 50.33 46.50 40.67 43.83 49.00 49.67 42.67 59.67 47.50 45.50 52.00 46.17 50.33 40.00 43.67 47.50 40.17 50.83 47.00 45.83 53.00 46.17 41.33 46.83 49.33 48.17 55.33 42.17 40.83 46.00 40.50 53.50 71.69 71.08 49.23 51.38 47.69 51.69 48.77 48.15 53.08 55.54 51.08 59.69 53.85 48.77 51.54 46.92 53.54 52.92 53.23 49.85 48.62 53.38 47.38 55.08 50.92 47.08 50.77 50.92 49.54 48.15 50.15 50.92 50.62 49.85 52.62 48.15 49.69 48.46 47.38 45.69 Table 10. Vision-centric QA Results. 72.39 72.39 74.63 74.38 74.38 75.87 71.39 73.63 66.92 73.13 73.63 71.89 75.12 74.38 75.37 70.40 69.65 76.12 68.41 71.64 75.37 76.37 72.64 74.38 74.63 46.77 74.88 73.63 40.55 73.13 76.12 71.14 73.88 44.53 58.24 73.13 73.38 70.15 51.99 45. 46.28 45.40 41.74 41.74 41.99 41.49 39.60 37.20 38.46 39.72 41.24 48.30 40.86 38.46 41.36 39.61 41.11 37.20 36.32 41.36 38.84 37.70 39.60 40.73 40.60 38.71 35.44 38.34 38.84 36.70 40.23 40.35 36.32 39.60 32.83 44.14 43.51 39.85 42.75 41.24 62.12 62.02 54.34 53.20 53.14 52.80 52.72 52.43 52.35 52.33 52.18 51.90 51.25 51.10 51.06 49.11 48.56 47.26 47.01 46.92 46.72 46.25 46.05 45.85 45.80 45.69 45.38 45.36 45.34 44.87 44.16 44.10 43.85 43.57 43.44 43.16 42.97 42.73 39.43 39.37 model name XFde XFen XFes XFid XFja XFru XFtr XFzh XMar XMbn XMcs XMda XMde XMel XMen XMes XMfa XMfi XMfil XMfr XMhe XMhi XMhr XMhu XMid XMit XMja XMko royokong/e5-v google/siglip-base-patch16-256-multilingual voyage-multimodal-3 google/siglip-large-patch16-384 google/siglip-large-patch16-256 BAAI/bge-visualized-m3 google/siglip-base-patch16-512 google/siglip-base-patch16-384 google/siglip-base-patch16-224 google/siglip-base-patch16-256 google/siglip-so400m-patch14-384 TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA laion/CLIP-ViT-bigG-14-laion2B-39B-b160k EVA02-CLIP-bigE-14-plus laion/CLIP-ViT-g-14-laion2B-s34B-b88K EVA02-CLIP-bigE-14 laion/CLIP-ViT-H-14-laion2B-s32B-b79K kakaobrain/align-base laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K EVA02-CLIP-L-14 laion/CLIP-ViT-L-14-laion2B-s32B-b82K laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-32-laion2B-s34B-b79K EVA02-CLIP-B-16 openai/clip-vit-large-patch14 Salesforce/blip-itm-large-coco jinaai/jina-clip-v1 Salesforce/blip-itm-large-flickr Salesforce/blip-itm-base-coco openai/clip-vit-base-patch16 openai/clip-vit-base-patch32 nomic-ai/nomic-embed-vision-v1.5 blip2-pretrain Salesforce/blip-itm-base-flickr blip2-finetune-coco BAAI/bge-visualized-base 80.69 75.08 80.48 77.81 76.42 54.91 73.71 72.16 69.74 70.22 72.34 68.88 68.92 51.99 51.93 47.25 44.66 45.83 41.92 33.73 37.58 38.51 33.42 29.72 34.17 29.94 26.93 34.73 29.50 33.98 29.77 20.40 20.05 18.05 17.10 19.56 20.80 15.13 86.64 80.62 89.91 86.80 85.10 62.20 85.48 85.03 83.25 83.51 87.51 80.62 80.48 84.22 84.61 82.75 84.34 82.91 76.01 78.71 81.31 82.04 75.77 72.35 75.15 78.25 71.43 88.77 78.90 87.80 86.98 72.44 67.29 65.50 68.05 83.53 81.44 68.99 86.46 83.07 85.83 87.38 85.88 59.89 84.42 83.45 81.33 81.56 83.89 76.85 76.79 66.43 66.74 62.80 63.04 62.84 39.37 49.86 57.42 54.70 44.01 41.38 47.83 46.79 42.27 51.36 48.77 51.12 45.11 34.09 32.35 21.91 28.64 26.98 28.98 16.07 80.29 72.20 72.94 66.55 63.88 54.09 53.48 52.41 49.70 50.80 48.10 28.97 28.83 20.95 21.42 18.16 18.80 19.07 10.08 26.88 21.63 17.20 24.54 21.50 11.94 14.35 14.16 9.40 7.96 8.82 7.52 10.88 7.54 5.20 4.74 4.49 5.79 5. 80.68 44.93 79.25 28.71 25.51 49.88 25.71 24.34 23.09 21.88 6.93 48.30 48.12 5.75 5.79 6.03 4.72 4.65 8.24 3.11 6.52 4.31 4.09 3.02 3.87 4.76 7.14 2.07 2.69 1.97 1.93 3.83 4.60 2.13 0.28 0.68 0.44 2.85 87.57 83.99 85.23 78.83 77.66 57.49 70.61 69.53 66.51 67.11 43.82 52.62 52.76 8.36 7.74 3.68 3.37 3.22 13.57 1.30 1.22 3.08 1.42 1.47 2.23 0.99 1.10 0.71 1.30 0.65 0.70 0.76 0.72 0.73 0.47 0.50 0.42 0.98 78.56 70.66 48.56 64.72 62.41 50.96 51.24 49.21 47.58 47.19 45.05 25.10 25.03 10.36 9.19 8.87 7.64 8.61 6.11 9.21 6.26 7.49 9.93 8.85 6.01 5.45 4.47 6.01 5.53 5.83 5.32 3.58 4.05 3.68 4.04 3.59 4.61 3.82 84.46 63.87 86.47 40.02 37.78 58.77 34.90 33.62 31.08 31.52 9.76 60.02 60.06 5.47 4.95 5.24 3.76 4.18 1.83 5.61 2.19 2.90 5.84 5.64 2.50 1.38 1.97 1.51 2.86 1.28 2.02 1.24 1.45 1.81 0.60 0.98 0.56 2.41 68.64 61.58 49.83 51.10 50.51 38.72 37.42 36.73 36.55 35.83 14.76 20.37 20.46 0.71 0.66 0.71 0.71 0.76 1.53 0.48 0.60 0.61 0.52 0.55 0.63 0.53 0.64 0.29 0.63 0.34 0.35 0.49 0.52 0.40 0.32 0.25 0.23 0.29 58.92 33.28 18.55 2.16 1.97 28.33 0.24 0.22 0.16 0.15 0.17 0.47 0.48 0.11 0.12 0.12 0.13 0.14 0.18 0.14 0.15 0.13 0.10 0.12 0.13 0.12 0.12 0.12 0.10 0.13 0.19 0.11 0.14 0.20 0.14 0.14 0.12 0. 66.15 62.88 51.42 51.57 51.26 42.53 38.09 37.72 37.22 37.11 37.25 16.68 16.63 9.19 8.92 7.11 6.58 6.65 10.60 6.25 5.01 5.53 6.29 6.08 5.38 4.69 3.60 6.00 5.35 5.31 5.34 3.17 3.30 5.22 4.50 4.07 4.57 3.85 68.38 68.54 55.39 55.92 56.41 49.63 44.15 43.69 43.03 42.70 45.86 29.49 29.45 23.05 21.82 18.96 17.34 17.78 25.42 16.84 13.76 15.06 15.78 15.68 13.56 11.75 8.51 16.89 13.97 14.81 14.55 7.11 7.65 10.15 10.31 11.25 11.05 8.00 83.51 79.82 85.63 80.51 80.39 50.94 75.38 74.82 73.32 73.40 71.98 62.12 62.13 58.18 57.47 51.28 48.62 48.53 49.68 39.35 38.17 39.07 34.28 31.99 32.69 29.79 26.09 30.41 31.54 29.67 25.75 20.56 18.86 21.82 18.47 18.60 17.25 11.31 69.68 56.24 24.60 36.10 35.81 39.21 22.62 21.99 21.34 21.09 4.67 3.99 4.02 0.59 0.55 0.48 0.49 0.48 5.63 0.48 0.43 0.41 0.49 0.46 0.56 0.34 0.39 0.46 0.50 0.43 0.42 0.33 0.29 0.59 0.45 0.39 0.37 0.43 69.18 68.12 74.70 71.18 71.15 48.42 71.08 70.80 70.31 70.41 72.26 63.88 63.80 71.21 71.08 70.09 70.81 70.53 69.41 67.79 67.06 69.07 66.62 65.25 66.66 65.72 59.73 74.96 68.32 73.16 72.67 59.02 56.89 53.24 59.35 66.57 63.08 60.32 74.79 74.53 75.66 75.46 75.40 47.35 73.52 73.35 72.04 72.53 73.89 47.05 46.98 59.06 59.08 54.98 54.27 53.57 41.16 42.80 47.28 46.18 38.09 37.51 38.77 41.09 35.26 43.87 40.58 40.91 36.87 30.21 28.11 22.00 26.40 23.12 23.55 13. 73.14 67.73 36.97 33.74 32.93 44.37 13.73 13.56 13.79 13.19 5.25 1.69 1.68 0.32 0.30 0.30 0.30 0.29 0.92 0.26 0.25 0.28 0.29 0.27 0.25 0.28 0.28 0.18 0.26 0.15 0.13 0.21 0.16 0.19 0.16 0.14 0.16 0.14 67.60 59.52 22.21 29.05 28.84 47.20 17.69 17.50 17.46 17.23 17.81 9.94 9.92 5.92 5.58 5.08 4.14 4.44 6.97 4.76 4.83 4.16 4.29 4.22 4.18 3.87 3.21 4.79 4.31 4.30 4.06 2.94 2.94 4.00 3.64 3.82 3.44 3.07 54.28 32.80 17.98 20.53 19.78 27.05 14.74 14.72 14.55 14.42 14.14 9.29 9.33 9.33 9.13 8.71 8.60 8.62 8.56 8.91 10.62 7.82 8.65 8.32 7.60 9.64 7.68 8.60 8.27 8.09 6.97 7.41 7.30 6.60 6.72 5.80 7.05 6.08 model name XMmi XMnl XMno XMpl XMpt XMquz XMro XMru XMsv XMsw XMte XMth XMtr XMuk XMvi XMzh WIar WIbg WIda royokong/e5-v google/siglip-base-patch16-256-multilingual voyage-multimodal-3 google/siglip-large-patch16-384 google/siglip-large-patch16-256 BAAI/bge-visualized-m3 google/siglip-base-patch16-512 google/siglip-base-patch16-384 google/siglip-base-patch16-224 google/siglip-base-patch16-256 google/siglip-so400m-patch14-384 TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA laion/CLIP-ViT-bigG-14-laion2B-39B-b160k EVA02-CLIP-bigE-14-plus laion/CLIP-ViT-g-14-laion2B-s34B-b88K EVA02-CLIP-bigE-14 laion/CLIP-ViT-H-14-laion2B-s32B-b79K kakaobrain/align-base laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K EVA02-CLIP-L-14 laion/CLIP-ViT-L-14-laion2B-s32B-b82K laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-32-laion2B-s34B-b79K EVA02-CLIP-B-16 openai/clip-vit-large-patch14 Salesforce/blip-itm-large-coco jinaai/jina-clip-v1 Salesforce/blip-itm-large-flickr Salesforce/blip-itm-base-coco openai/clip-vit-base-patch16 openai/clip-vit-base-patch32 nomic-ai/nomic-embed-vision-v1.5 blip2-pretrain Salesforce/blip-itm-base-flickr blip2-finetune-coco BAAI/bge-visualized-base 10.44 0.71 0.66 0.80 0.73 2.02 0.85 0.86 0.81 0.73 0.58 0.88 0.87 0.75 0.77 0.67 0.61 0.72 0.75 0.52 0.63 0.81 0.68 0.55 0.74 0.54 0.56 0.67 0.60 0.50 0.54 0.57 0.55 0.56 0.45 0.51 0.56 0.43 70.01 68.33 68.39 68.36 67.94 45.58 62.37 61.92 60.70 60.60 64.46 38.17 38.27 44.91 44.47 39.83 36.78 36.74 39.45 32.16 30.08 30.52 25.91 26.81 25.91 22.76 20.37 26.12 22.19 23.58 23.15 15.78 15.47 13.99 16.49 16.32 16.66 10.69 67.07 65.69 58.60 57.00 57.61 49.48 46.44 45.97 45.29 45.09 47.47 29.12 29.20 23.46 21.94 18.85 16.40 16.82 22.39 15.68 13.29 13.50 15.20 14.84 13.36 11.34 8.36 16.95 13.90 14.72 14.08 6.94 7.17 9.43 9.56 10.89 10.37 7.36 73.08 72.29 66.65 65.66 65.40 48.86 55.46 54.62 54.19 53.65 48.86 23.21 23.20 13.52 13.12 9.98 8.98 9.19 14.98 8.87 6.75 7.76 8.53 8.41 6.44 5.32 5.05 7.20 6.32 6.43 5.73 3.97 3.76 6.36 5.45 4.47 5.23 4.01 76.06 74.59 76.17 75.28 74.38 46.98 72.45 72.16 70.19 70.64 70.07 50.21 50.10 51.09 50.12 45.06 42.48 43.04 35.58 35.73 35.87 35.06 32.28 31.44 29.73 28.20 25.77 29.55 27.72 27.80 24.97 21.23 19.77 18.00 18.77 15.03 17.55 11.31 8.00 5.90 4.96 5.41 5.68 4.25 5.70 5.59 5.50 5.71 4.27 2.73 2.74 4.09 4.13 3.70 3.61 3.50 3.69 3.75 3.60 3.39 3.24 3.09 3.02 3.22 2.73 3.12 3.16 2.80 2.67 2.61 2.63 2.67 2.61 2.23 2.71 1. 73.17 68.14 51.41 48.29 47.89 47.20 34.57 34.03 33.27 33.52 33.65 19.40 19.44 24.85 24.01 18.94 18.63 18.47 26.64 17.16 12.83 15.14 16.61 14.84 14.04 11.16 8.73 15.67 14.73 14.03 13.31 7.51 6.82 9.81 11.22 9.66 11.04 7.29 82.67 81.27 82.08 74.78 74.06 51.41 65.23 64.97 63.32 63.37 40.31 41.98 41.92 9.21 8.67 3.83 4.49 4.40 12.96 1.71 1.88 2.72 1.70 1.70 2.27 1.51 1.72 1.04 1.74 0.98 0.75 1.36 1.18 1.17 0.86 0.79 0.43 1.03 70.31 67.89 59.20 63.41 63.57 48.40 54.85 54.44 53.58 53.23 52.87 32.51 32.51 22.91 21.28 18.84 16.28 16.76 22.90 16.00 13.61 13.72 14.26 14.99 13.09 11.30 8.33 15.15 13.32 13.08 13.39 6.58 7.14 9.37 9.90 9.65 10.54 7.42 32.49 19.72 6.63 7.61 7.55 23.27 5.79 5.85 5.76 5.72 6.09 2.78 2.79 3.98 3.70 3.90 3.74 3.88 3.66 3.97 3.33 3.54 3.65 3.74 3.32 2.97 3.08 3.40 3.47 3.21 2.82 2.54 2.43 2.76 3.07 2.30 2.75 1.72 40.90 9.95 4.30 0.24 0.20 25.74 0.13 0.12 0.13 0.11 0.17 0.19 0.18 0.14 0.14 0.12 0.13 0.13 0.12 0.12 0.12 0.17 0.12 0.13 0.12 0.13 0.12 0.12 0.12 0.12 0.13 0.14 0.12 0.13 0.11 0.13 0.12 0.13 74.53 39.18 56.52 13.63 13.22 46.39 12.43 12.49 12.28 12.36 2.31 6.91 6.86 2.36 2.16 2.19 2.19 2.14 1.98 1.78 2.58 2.05 1.80 1.43 1.78 2.47 2.46 1.64 1.88 1.58 1.63 2.52 2.24 1.25 1.07 1.53 0.81 1. 68.70 64.99 38.61 53.14 52.07 40.31 40.17 39.90 39.10 39.06 32.80 10.63 10.64 7.52 7.16 6.20 5.96 6.15 6.84 8.84 5.39 5.21 7.18 6.72 4.28 4.42 4.12 4.14 3.96 3.82 3.68 3.18 3.10 3.87 3.22 2.81 3.01 2.74 77.28 69.72 64.50 52.27 52.30 48.97 38.13 37.41 37.33 37.66 19.41 21.89 21.93 3.26 2.99 1.71 1.81 1.85 5.77 0.82 0.95 1.15 0.83 0.82 1.09 0.76 0.84 0.70 0.89 0.56 0.49 0.69 0.62 0.66 0.47 0.44 0.38 0.43 77.67 68.59 72.31 40.38 38.60 44.09 22.99 23.10 22.19 21.48 9.46 4.98 4.96 3.65 3.52 2.91 3.09 3.10 7.05 4.04 2.40 2.95 3.94 4.45 2.56 2.10 1.86 2.13 2.38 1.86 1.67 1.70 1.79 1.26 2.12 1.04 1.84 1.21 79.48 63.08 83.16 34.23 34.51 46.58 29.12 28.77 28.23 28.41 8.84 39.55 39.53 5.09 4.56 3.99 3.63 3.58 2.42 4.78 1.85 2.50 4.18 3.95 2.25 1.19 1.42 1.03 1.92 1.06 1.01 1.02 1.05 1.67 0.44 0.71 0.41 1.84 53.58 45.16 53.33 34.10 31.60 42.36 22.32 21.53 21.26 20.32 23.76 20.61 20.51 6.77 7.04 6.90 7.03 6.87 5.33 5.51 6.73 5.72 5.18 5.09 5.36 6.22 7.59 4.92 5.36 4.78 4.23 6.00 6.70 4.62 4.59 3.57 3.74 3.69 43.80 41.76 44.84 32.52 31.65 38.86 22.21 22.72 21.12 21.76 25.43 12.01 11.78 10.29 10.00 8.24 8.22 8.09 8.14 5.34 5.61 6.55 4.60 4.30 5.56 4.94 5.86 3.51 4.71 3.23 3.13 5.27 4.82 4.63 2.95 2.42 2.85 3. 50.72 53.55 52.59 53.01 52.13 44.28 44.45 44.29 42.18 42.33 62.02 38.08 38.06 56.83 56.40 53.48 52.77 52.44 43.93 51.14 47.55 47.56 43.32 39.44 42.20 41.56 48.24 29.34 32.09 28.66 26.70 41.92 39.26 30.52 26.57 21.77 17.37 20.52 81.81 77.54 81.86 78.83 78.36 52.21 76.04 75.30 74.45 74.47 76.51 56.43 56.50 71.04 71.57 67.51 66.44 65.48 62.52 53.89 55.20 58.24 47.95 46.25 51.89 47.98 39.81 56.80 54.40 53.41 51.44 35.27 33.17 28.84 37.86 37.80 33.58 23.07 WIel 54.74 37.47 39.32 24.51 23.26 43.11 17.16 16.64 15.82 15.75 12.99 12.47 12.25 10.07 9.82 9.23 9.22 9.56 11.41 9.14 8.79 8.69 8.56 8.32 8.05 7.57 8.77 7.81 6.91 7.35 7.02 6.92 6.66 8.65 6.23 6.53 5.51 6.22 69.73 66.72 39.42 31.82 30.97 41.54 13.31 13.41 13.46 12.79 4.09 14.16 14.18 0.36 0.30 0.28 0.27 0.27 1.77 0.31 0.29 0.28 0.29 0.28 0.30 0.27 0.22 0.27 0.36 0.26 0.22 0.24 0.26 0.30 0.23 0.25 0.20 0.21 WIet 34.70 35.80 29.94 31.11 29.97 32.05 25.61 25.10 24.92 25.08 37.10 20.43 20.10 34.12 33.03 32.12 31.19 31.65 22.83 29.64 28.78 28.00 25.77 23.65 24.07 24.00 27.93 18.07 19.15 18.93 15.11 23.29 21.91 21.41 16.53 13.13 11.58 16.28 52.76 30.13 12.73 6.04 6.11 22.99 1.47 1.58 1.54 1.38 0.46 7.98 8.01 0.16 0.13 0.11 0.12 0.15 0.58 0.12 0.13 0.13 0.12 0.16 0.14 0.14 0.17 0.14 0.14 0.15 0.14 0.12 0.12 0.10 0.15 0.14 0.09 0.15 WIid 53.22 60.85 57.02 63.11 61.30 47.11 51.96 51.13 49.59 50.06 67.32 40.12 40.23 60.87 60.83 58.12 57.21 57.41 37.04 59.44 53.73 50.94 50.27 46.04 43.14 47.51 53.45 26.33 29.56 26.89 24.78 46.93 43.30 29.87 24.84 19.41 16.00 22.30 65.34 63.49 44.67 44.35 44.05 46.11 28.54 28.27 27.90 28.02 28.71 10.73 10.68 9.83 9.17 6.73 7.11 7.13 8.88 6.86 5.93 5.86 6.66 6.46 5.17 4.57 4.25 5.88 5.74 5.24 4.61 3.20 3.14 5.44 4.65 3.80 4.64 4.29 WIko 43.78 35.04 36.08 22.54 21.58 33.97 13.89 13.48 13.47 13.10 13.20 10.47 10.48 7.65 7.79 7.08 7.23 7.61 6.69 6.86 6.58 7.33 5.93 6.09 5.82 6.25 5.78 5.63 6.03 5.78 4.87 5.37 4.81 5.63 4.55 4.02 4.08 5.00 74.49 69.17 22.41 45.81 45.34 46.54 29.73 29.73 29.12 28.99 26.20 9.57 9.57 8.28 8.33 6.82 6.69 6.73 8.55 6.55 5.97 6.01 5.91 5.68 5.31 4.99 3.60 6.20 5.91 5.52 4.90 3.72 3.35 5.34 4.89 4.38 4.46 3.86 WIja 43.15 37.99 43.90 23.41 23.28 34.39 16.20 16.13 15.56 15.63 14.66 21.15 20.97 14.69 14.83 12.91 14.82 13.65 10.91 12.30 13.53 12.52 10.74 9.82 9.61 10.57 14.40 6.99 8.19 6.65 7.22 10.53 9.06 9.15 5.26 5.95 5.59 7.70 81.30 75.81 76.52 69.63 68.28 52.16 57.93 57.77 56.27 55.98 52.30 20.20 20.19 24.44 24.27 23.02 21.92 22.13 15.06 30.88 20.91 17.77 27.19 26.88 15.21 16.53 15.33 11.50 10.83 10.59 9.11 12.02 10.85 7.25 8.09 6.29 7.67 6.99 WItr 53.27 51.41 51.21 55.11 52.47 42.55 44.49 43.96 42.16 42.04 63.86 33.60 33.47 52.20 52.65 49.00 49.42 49.28 36.15 49.70 50.73 45.72 42.20 38.83 38.89 43.11 47.65 27.73 29.99 26.98 22.81 42.32 38.78 29.13 23.80 20.78 16.18 22.60 79.39 76.74 80.93 78.32 77.60 47.83 72.63 72.10 70.55 70.91 72.58 43.26 43.20 56.04 55.20 49.58 48.02 47.52 41.68 36.62 35.48 38.45 31.55 29.91 30.66 27.39 24.70 31.70 28.63 28.80 27.00 18.09 16.36 15.21 15.97 16.29 15.63 9.80 WIvi 57.85 59.63 58.15 56.87 54.99 49.28 44.39 43.49 40.81 41.37 51.21 30.15 30.33 47.92 47.08 45.52 44.84 44.77 36.46 44.64 39.10 40.69 36.42 32.04 32.82 32.38 34.26 20.70 25.92 21.71 19.56 31.11 31.37 22.98 21.33 14.54 14.37 17.40 80.47 57.93 77.71 25.88 25.35 48.20 21.57 21.53 21.30 20.21 5.22 44.76 44.68 8.86 8.74 7.06 7.04 6.99 11.35 4.21 7.11 5.36 4.66 3.54 4.58 5.32 6.41 1.60 3.17 1.46 1.38 4.84 4.20 1.91 0.23 0.70 0.26 1.82 WIen 60.74 67.86 64.45 74.79 73.26 51.77 71.10 69.76 68.40 68.31 80.31 58.65 58.70 79.50 80.59 78.98 79.42 78.73 70.98 76.47 78.60 76.13 70.50 67.16 71.54 72.12 78.47 61.62 62.20 61.76 57.75 72.90 71.82 64.44 62.25 52.31 50.48 52.94 75.97 63.80 48.62 40.91 39.66 43.14 28.86 29.07 28.23 27.41 7.49 17.76 17.78 0.31 0.29 0.31 0.31 0.38 1.33 0.47 0.27 0.28 0.50 0.35 0.27 0.27 0.34 0.23 0.29 0.25 0.23 0.21 0.22 0.24 0.21 0.18 0.21 0.39 Avg. 66.57 59.21 58.87 51.11 49.84 46.35 43.21 42.55 41.23 41.26 40.19 34.96 34.92 28.01 27.82 25.92 25.54 25.54 22.36 23.77 23.43 23.02 21.57 20.13 20.13 20.12 20.24 18.53 18.09 18.12 16.81 17.66 16.73 14.48 13.86 13.44 13.05 12.25 Table 11. Multilingual Retrieval Results. The average is the aggregated average of the 3 big tasks. model name STS12 STS13 STS14 STS15 STS16 STS STS-b mean voyage-multimodal-3 royokong/e5-v TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA EVA02-CLIP-bigE-14-plus laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K google/siglip-large-patch16-384 laion/CLIP-ViT-g-14-laion2B-s34B-b88K EVA02-CLIP-bigE-14 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K google/siglip-so400m-patch14-384 google/siglip-base-patch16-512 google/siglip-large-patch16-256 google/siglip-base-patch16-384 Salesforce/blip-itm-base-coco google/siglip-base-patch16-256 openai/clip-vit-base-patch16 laion/CLIP-ViT-L-14-laion2B-s32B-b82K laion/CLIP-ViT-H-14-laion2B-s32B-b79K google/siglip-base-patch16-256-multilingual openai/clip-vit-large-patch14 Salesforce/blip-itm-base-flickr google/siglip-base-patch16-224 BAAI/bge-visualized-m3 EVA02-CLIP-L-14 Salesforce/blip-itm-large-coco kakaobrain/align-base jinaai/jina-clip-v1 Salesforce/blip-image-captioning-large BAAI/bge-visualized-base Salesforce/blip-itm-large-flickr laion/CLIP-ViT-B-32-laion2B-s34B-b79K laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K Salesforce/blip-image-captioning-base openai/clip-vit-base-patch32 blip2-finetune-coco EVA02-CLIP-B-16 blip2-pretrain nomic-ai/nomic-embed-vision-v1.5 facebook/dinov2-base facebook/dinov2-giant nyu-visionx/moco-v3-vit-b nyu-visionx/moco-v3-vit-l facebook/dinov2-large facebook/dinov2-small 71.62 73.15 71.15 71.18 63.36 62.81 62.36 66.30 61.85 62.24 64.19 61.90 64.97 63.94 64.62 62.91 65.01 63.82 57.52 57.00 66.62 53.89 59.24 63.19 63.93 53.75 62.32 53.17 57.96 61.67 55.35 59.68 53.70 54.86 49.34 53.81 41.36 40.25 38.81 40.13 24.13 24.34 24.03 20.90 17.45 13.39 81.60 78.18 65.88 65.87 68.00 68.16 67.64 62.08 66.43 62.36 63.81 62.95 59.10 59.44 59.38 55.14 58.02 63.26 62.75 62.25 54.80 66.78 54.45 55.40 56.91 60.82 50.97 57.50 55.80 50.18 57.31 47.46 57.16 45.82 46.84 52.50 38.11 36.57 38.72 21.22 5.82 1.18 2.19 3.38 0.05 2.39 77.98 74.88 62.63 62.61 66.38 65.50 64.25 61.66 62.32 62.17 62.34 60.58 61.13 59.35 61.17 60.17 60.36 56.99 59.94 58.62 59.00 55.98 57.87 57.99 58.19 57.12 55.16 56.01 56.95 54.12 57.57 52.82 52.74 48.85 48.29 43.69 38.33 39.17 35.57 21.44 0.36 1.06 0.63 -1.13 -2.39 -1.9 86.85 84.22 76.00 75.92 79.45 78.67 77.36 77.11 76.73 77.41 75.48 76.17 75.08 75.74 74.34 72.83 74.25 73.32 74.55 74.40 72.65 72.03 71.10 73.07 70.94 71.53 70.15 69.13 70.52 70.03 68.27 68.29 66.64 64.02 60.57 59.56 54.36 51.18 51.81 27.21 13.75 13.65 11.48 12.99 12.28 12. 82.62 79.45 75.36 75.34 75.26 74.89 73.48 73.27 72.67 73.63 69.90 73.48 71.27 71.83 70.29 71.59 69.09 68.91 70.61 70.57 68.33 70.49 68.17 67.79 63.49 67.46 67.33 66.43 67.98 66.63 62.39 64.20 61.30 59.62 60.54 53.01 46.06 48.86 42.20 31.39 18.05 18.43 19.80 22.00 19.35 16.47 89.68 85.84 83.72 83.55 82.87 79.97 80.63 79.58 79.88 80.96 80.04 78.41 80.09 79.21 79.27 77.32 78.73 78.18 75.92 76.69 80.53 75.26 75.97 77.78 79.18 80.14 75.69 77.55 76.94 76.43 75.52 72.77 74.69 73.31 72.56 71.01 62.61 54.15 58.49 40.46 43.02 37.07 39.54 40.27 43.67 43.1 82.55 79.40 73.75 73.64 68.59 66.54 63.38 66.59 64.13 62.85 63.51 62.63 62.21 62.50 60.28 66.50 57.65 57.93 59.43 58.99 56.29 56.74 62.93 54.50 56.48 50.46 58.53 59.42 52.18 56.41 54.66 55.86 50.48 49.15 49.54 47.17 39.18 31.58 33.68 23.16 12.65 9.48 6.56 4.70 6.31 5.37 81.84 79.30 72.64 72.59 71.99 70.93 69.87 69.51 69.14 68.80 68.47 68.02 67.69 67.43 67.05 66.64 66.16 66.06 65.82 65.50 65.46 64.45 64.25 64.25 64.16 63.04 62.88 62.74 62.62 62.21 61.58 60.16 59.53 56.52 55.38 54.39 45.72 43.11 42.75 29.29 16.83 15.03 14.89 14.73 13.82 12.98 Table 12. Visual STS English Results. Note that for STS-17 and STS-b, we only average the English subset here. model name ko-ko ar-ar en-ar en-de en-tr es-en es-es fr-en it-en nl-en mean 62.80 voyage-multimodal-3 14.45 royokong/e5-v 13.65 google/siglip-so400m-patch14-384 11.07 openai/clip-vit-large-patch14 17.96 TIGER-Lab/VLM2Vec-Full 17.99 TIGER-Lab/VLM2Vec-LoRA 16.64 google/siglip-base-patch16-256-multilingual 15.67 google/siglip-large-patch16-384 23.37 google/siglip-base-patch16-512 23.08 google/siglip-base-patch16-384 google/siglip-large-patch16-256 16.00 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K 14.28 10.54 openai/clip-vit-base-patch16 19.39 laion/CLIP-ViT-H-14-laion2B-s32B-b79K 14.38 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k 19.71 Salesforce/blip-itm-large-coco 21.40 google/siglip-base-patch16-256 21.00 google/siglip-base-patch16-224 19.73 Salesforce/blip-itm-base-flickr 19.14 Salesforce/blip-image-captioning-large 22.40 Salesforce/blip-itm-base-coco 19.32 jinaai/jina-clip-v1 22.30 Salesforce/blip-itm-large-flickr 10.97 EVA02-CLIP-bigE-14 17.17 laion/CLIP-ViT-g-14-laion2B-s34B-b88K 17.69 kakaobrain/align-base 11.36 EVA02-CLIP-bigE-14-plus 14.31 facebook/dinov2-small 14.35 blip2-finetune-coco 14.19 nyu-visionx/moco-v3-vit-l 28.34 Salesforce/blip-image-captioning-base 14.77 EVA02-CLIP-L-14 21.28 facebook/dinov2-large nyu-visionx/moco-v3-vit-b 13.96 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K 19.21 16.25 laion/CLIP-ViT-B-32-laion2B-s34B-b79K 18.23 laion/CLIP-ViT-L-14-laion2B-s32B-b82K 18.10 openai/clip-vit-base-patch32 15.88 blip2-pretrain 14.76 BAAI/bge-visualized-m3 19.12 BAAI/bge-visualized-base 17.94 facebook/dinov2-base 12.60 facebook/dinov2-giant laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K 17.88 18.02 EVA02-CLIP-B-16 19.97 nomic-ai/nomic-embed-vision-v1.5 65.75 32.52 45.76 39.12 36.74 37.24 28.48 32.51 29.28 34.56 31.32 36.47 36.25 33.39 32.39 30.04 30.46 25.03 35.78 32.45 32.47 27.80 30.66 29.99 29.93 17.70 31.51 32.77 38.72 30.79 31.68 29.65 28.50 32.60 18.40 21.73 20.71 28.30 28.99 18.55 23.67 28.39 28.87 13.79 19.60 19. 34.40 11.28 11.22 18.95 7.10 6.54 1.67 14.53 16.98 17.04 16.79 12.75 13.13 19.49 12.21 17.25 12.67 14.36 9.69 11.21 0.66 7.55 8.47 13.49 14.27 21.55 10.71 12.52 7.17 6.57 -0.59 18.89 17.80 19.96 -1.69 4.20 4.66 8.25 11.13 -6.92 -1.90 18.25 7.33 1.29 3.58 -4.26 80.42 53.00 46.07 45.71 36.47 36.42 45.64 35.06 37.38 35.29 31.32 43.10 41.57 43.78 36.74 41.46 30.19 31.20 38.30 36.68 38.33 31.29 32.23 22.76 28.50 21.27 24.33 31.11 25.18 32.83 22.27 3.52 28.70 29.48 33.07 17.82 19.38 22.15 23.70 30.64 17.37 28.90 24.60 15.83 7.10 -7.82 44.98 23.88 30.62 39.70 16.96 16.69 20.73 30.00 25.21 22.75 18.98 19.70 35.42 16.68 14.99 21.80 19.81 24.80 9.73 16.77 15.47 2.29 6.44 6.39 -4.79 19.33 -10.05 25.47 7.01 26.85 5.81 16.61 27.26 20.01 6.57 17.37 0.88 17.97 1.60 6.53 3.89 14.41 18.36 -1.65 22.06 -14.73 74.72 51.92 40.08 36.76 46.72 47.32 47.14 39.64 34.41 32.58 36.14 37.37 24.63 27.99 30.44 29.98 28.50 29.32 22.46 23.02 20.01 29.59 27.43 29.03 34.19 28.31 20.18 11.11 20.48 12.51 16.40 12.23 12.43 15.71 16.93 25.07 19.49 12.15 21.98 8.45 2.68 8.91 11.10 17.27 4.52 -6.12 83.70 74.42 73.62 70.11 72.95 72.77 73.28 72.06 71.24 72.25 71.79 71.62 62.95 62.58 69.77 60.52 71.68 69.85 66.17 62.57 69.76 67.75 54.65 57.16 66.07 54.37 59.20 35.33 39.44 37.19 56.30 45.55 39.85 32.47 62.39 57.03 61.89 47.56 42.55 45.41 50.85 35.40 30.99 53.68 48.86 38.29 75.07 44.98 46.36 44.06 44.67 44.70 41.91 35.62 38.53 37.39 34.93 36.88 38.72 39.32 39.77 27.65 36.55 35.70 36.40 27.84 28.71 24.06 24.72 36.66 29.70 34.11 36.12 20.38 30.76 25.41 17.16 32.61 18.48 23.99 20.93 22.91 31.63 19.48 26.16 34.38 27.90 11.87 11.90 17.71 12.82 -4. 77.76 44.50 36.45 40.17 35.48 35.33 40.14 33.52 28.49 27.05 40.67 30.78 31.40 28.59 36.44 29.31 28.75 27.46 23.99 25.01 27.11 24.69 24.76 33.43 29.02 30.89 28.60 27.78 22.85 25.19 23.48 30.84 18.46 20.73 19.40 21.49 27.75 22.74 20.60 34.44 25.82 20.30 16.65 19.68 17.07 6.36 78.25 54.29 44.95 41.63 42.37 42.40 38.85 33.19 34.33 32.36 36.17 30.76 38.63 37.33 34.83 41.20 30.72 28.98 40.03 39.19 35.35 34.41 36.65 26.16 21.18 19.58 33.18 29.28 32.43 22.88 27.69 23.63 15.34 18.86 32.23 23.38 18.38 25.05 25.92 30.03 35.52 16.51 9.77 11.65 11.99 -2.8 67.79 40.52 38.88 38.73 35.74 35.74 35.45 34.18 33.92 33.43 33.41 33.37 33.32 32.85 32.20 31.89 31.07 30.77 30.23 29.39 29.03 26.88 26.83 26.60 26.52 26.48 24.52 24.01 23.84 23.44 22.85 22.83 22.81 22.78 22.74 22.72 22.30 22.18 21.85 21.63 20.49 20.09 17.22 16.71 16.56 4.34 Table 13. Visual STS cross-lingual Results. model name de es fr it nl pl pt ru zh mean voyage-multimodal-3 74.13 royokong/e5-v 58.29 TIGER-Lab/VLM2Vec-LoRA 52.69 TIGER-Lab/VLM2Vec-Full 52.65 Salesforce/blip-itm-base-coco 55.58 Salesforce/blip-itm-base-flickr 54.46 google/siglip-large-patch16-384 55.72 google/siglip-base-patch16-256-multilingual 48.11 google/siglip-so400m-patch14-384 50.73 google/siglip-large-patch16-256 53.23 google/siglip-base-patch16-512 45.18 jinaai/jina-clip-v1 47.25 45.57 google/siglip-base-patch16-384 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K 47.05 42.40 google/siglip-base-patch16-256 38.00 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k 48.01 laion/CLIP-ViT-g-14-laion2B-s34B-b88K 40.38 google/siglip-base-patch16-224 42.62 Salesforce/blip-itm-large-coco laion/CLIP-ViT-H-14-laion2B-s32B-b79K 41.31 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K 41.25 40.76 Salesforce/blip-itm-large-flickr 31.96 EVA02-CLIP-bigE-14-plus openai/clip-vit-large-patch14 37.50 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K 38.22 37.10 EVA02-CLIP-bigE-14 39.99 laion/CLIP-ViT-L-14-laion2B-s32B-b82K 41.43 laion/CLIP-ViT-B-32-laion2B-s34B-b79K 32.40 BAAI/bge-visualized-base 30.68 EVA02-CLIP-B-16 34.60 kakaobrain/align-base 32.72 openai/clip-vit-base-patch16 32.04 BAAI/bge-visualized-m3 29.92 nomic-ai/nomic-embed-vision-v1.5 22.00 EVA02-CLIP-L-14 24.59 blip2-finetune-coco 22.79 blip2-pretrain 29.41 openai/clip-vit-base-patch32 23.73 facebook/dinov2-base 21.10 facebook/dinov2-giant 20.60 facebook/dinov2-large 14.76 nyu-visionx/moco-v3-vit-l 12.98 nyu-visionx/moco-v3-vit-b 10.87 facebook/dinov2-small 75.99 64.24 60.83 60.78 53.93 50.91 56.23 53.45 56.23 52.39 49.45 47.42 48.15 45.13 44.36 43.63 41.47 41.80 36.03 39.11 31.76 33.39 37.53 44.18 28.92 35.37 31.22 26.40 28.99 27.02 25.79 30.81 22.26 23.12 28.24 20.31 19.74 23.43 16.42 16.14 13.14 9.48 9.99 11.58 74.43 61.79 58.64 58.68 59.40 56.04 54.78 51.69 54.72 50.98 53.46 53.08 51.66 50.76 46.72 52.36 45.03 45.75 43.42 48.44 45.92 41.04 46.88 47.53 38.00 41.49 40.69 35.96 37.14 36.05 38.57 39.06 36.13 23.35 33.36 27.19 31.75 26.85 19.46 22.06 20.47 15.35 15.34 12. 73.96 64.11 52.77 52.63 50.63 49.89 54.24 51.65 51.56 50.46 47.26 48.58 45.55 44.24 41.73 44.84 37.56 37.90 39.51 34.22 34.60 39.40 38.94 36.89 23.87 31.98 28.57 28.13 29.10 27.13 26.95 29.46 27.05 22.93 22.67 21.90 23.77 20.55 16.06 12.74 10.65 8.17 6.77 6.70 71.34 55.15 49.55 49.62 53.46 50.94 42.45 41.15 45.65 40.22 43.27 47.44 42.49 38.21 38.72 34.84 36.84 37.64 37.83 34.48 35.79 34.23 29.78 32.51 32.90 28.04 28.49 29.75 31.45 29.71 32.79 23.46 24.47 21.92 21.75 21.13 17.54 19.05 17.90 16.13 15.91 11.68 12.62 9.17 68.83 52.17 45.77 45.78 51.69 48.19 41.24 48.08 35.84 42.55 44.95 47.15 44.71 34.94 42.34 33.19 36.02 42.65 39.55 33.20 40.38 40.14 27.50 23.41 43.21 25.33 27.58 34.85 36.66 32.41 28.88 28.15 27.96 30.96 21.31 25.57 26.10 30.37 21.00 21.79 19.60 16.31 14.02 13.27 73.48 63.59 55.09 55.06 53.05 50.37 51.62 46.85 45.88 45.53 43.68 44.23 43.36 37.87 39.56 37.51 32.73 37.01 32.01 32.09 36.57 28.92 33.35 35.49 28.62 30.62 25.85 28.60 29.16 29.06 22.60 26.30 26.80 20.85 18.91 22.40 24.49 14.01 11.48 16.47 11.28 11.01 10.74 8.18 72.68 35.88 45.43 45.39 34.62 32.37 36.86 51.42 39.68 37.50 39.47 34.84 36.71 30.89 35.01 28.43 30.53 32.81 32.30 26.94 26.67 30.72 25.05 14.06 27.29 25.35 22.66 21.84 20.91 25.40 23.02 14.69 17.00 25.16 16.84 19.31 17.69 18.59 17.09 17.30 19.78 12.31 13.33 9.96 72.60 12.57 17.35 17.22 20.94 19.32 14.97 13.48 14.86 12.38 14.14 10.67 16.70 14.65 9.34 19.19 23.65 10.79 16.21 23.88 15.18 18.93 16.20 12.12 13.95 14.58 22.58 19.50 15.35 16.71 19.63 11.85 14.02 16.55 14.04 14.71 11.26 10.52 23.32 19.34 22.36 20.54 18.30 17. 73.05 51.98 48.68 48.65 48.14 45.83 45.35 45.10 43.91 42.80 42.32 42.30 41.66 38.19 37.80 36.89 36.87 36.30 35.50 34.85 34.24 34.17 31.91 31.52 30.55 29.98 29.74 29.61 29.02 28.24 28.09 26.28 25.30 23.86 22.12 21.90 21.68 21.42 18.50 18.12 17.09 13.29 12.68 11.18 Table 14. Visual STS multilingual Results. Model name ArxivQA DocVQA InfoVQA Shift Project Syn.Doc QAAI Syn.Doc Syn.Doc Syn.Dic Syn. Tatdqa Avg. QAEnergy QAGov. QAHealth. Tabfquad voyage-multimodal-3 royokong/e5-v google/siglip-so400m-patch14-384 google/siglip-large-patch16-384 google/siglip-base-patch16-512 TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA google/siglip-base-patch16-384 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-H-14-laion2B-s32B-b79K google/siglip-large-patch16-256 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K openai/clip-vit-large-patch14 laion/CLIP-ViT-g-14-laion2B-s34B-b88K laion/CLIP-ViT-L-14-laion2B-s32B-b82K EVA02-CLIP-bigE-14-plus google/siglip-base-patch16-256 EVA02-CLIP-bigE-14 kakaobrain/align-base laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K google/siglip-base-patch16-256-multilingual google/siglip-base-patch16-224 openai/clip-vit-base-patch16 EVA02-CLIP-L-14 Salesforce/blip-itm-large-flickr Salesforce/blip-itm-base-coco Salesforce/blip-itm-large-coco jinaai/jina-clip-v1 laion/CLIP-ViT-B-32-laion2B-s34B-b79K blip2-finetune-coco Salesforce/blip-itm-base-flickr openai/clip-vit-base-patch32 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K BAAI/bge-visualized-m3 blip2-pretrain nomic-ai/nomic-embed-vision-v1.5 BAAI/bge-visualized-base EVA02-CLIP-B-16 84.61 48.27 50.21 47.45 46.02 42.84 42.59 43.59 38.84 33.03 40.19 34.51 28.64 32.82 30.96 34.86 35.17 32.72 23.31 28.88 30.33 31.49 26.54 30.44 24.89 20.55 22.65 25.40 24.01 14.68 17.06 17.11 16.57 18.10 11.25 15.86 15.20 16.22 49.76 34.73 31.28 28.53 28.38 26.74 26.92 26.43 20.44 19.14 22.39 19.68 16.69 18.10 17.79 16.84 19.42 16.35 18.03 13.97 16.96 16.04 14.60 11.24 12.11 11.68 11.25 10.99 10.50 10.37 10.81 9.48 9.03 8.26 5.74 9.25 7.05 5.84 86.11 69.22 69.73 64.11 64.51 66.68 67.64 59.28 60.90 58.82 54.09 55.61 62.44 56.85 52.46 55.19 48.73 54.80 43.15 46.88 45.28 46.11 51.70 48.48 33.95 32.30 31.37 35.12 35.35 31.97 29.65 37.15 27.09 32.79 30.92 29.55 29.64 25.13 77.48 42.47 25.04 25.37 22.95 25.01 24.34 14.46 25.02 21.81 9.13 16.19 17.05 16.72 13.10 12.76 5.45 10.14 10.47 7.25 3.89 3.71 7.13 4.44 4.66 5.05 4.08 3.84 4.95 4.08 4.50 1.00 3.06 1.39 4.12 0.00 3.02 1.43 83.56 78.91 67.78 64.87 63.51 53.51 54.02 55.75 55.42 54.09 43.40 47.20 38.25 40.12 44.98 34.57 31.06 33.53 41.43 32.17 22.72 25.27 22.86 20.36 17.40 18.70 16.22 15.57 18.39 13.78 14.73 11.06 13.62 9.91 16.04 11.10 7.34 8. 79.42 78.11 73.52 67.34 66.79 63.49 63.35 57.47 59.95 60.23 50.79 58.93 61.62 60.07 57.08 44.99 41.28 48.50 49.76 38.53 29.93 35.53 32.43 29.87 23.16 18.68 19.03 19.34 22.52 18.23 15.23 18.31 15.67 8.91 15.05 10.94 11.05 9.58 83.92 82.16 75.35 74.52 72.79 64.03 64.06 58.54 62.27 52.92 55.45 50.28 52.84 52.21 49.29 43.14 40.07 41.32 42.07 31.05 28.73 32.35 39.84 18.37 16.14 24.74 19.85 21.83 14.50 17.47 15.23 9.14 9.08 8.41 10.66 8.90 6.91 5.26 82.39 82.31 83.10 74.67 79.70 70.73 70.62 67.67 57.86 55.50 56.03 58.04 60.23 52.09 53.15 42.47 49.94 42.98 47.46 35.83 37.17 37.01 37.54 33.68 27.18 25.58 24.45 20.84 18.60 23.95 20.40 13.14 12.51 12.58 14.42 15.79 9.39 10.35 56.36 81.37 60.29 61.09 50.71 63.54 61.93 47.61 35.02 33.11 49.81 30.70 30.95 32.51 29.13 30.36 37.00 28.80 29.00 26.60 44.16 29.04 17.61 20.64 21.87 19.42 24.59 20.14 16.09 17.60 19.33 14.09 14.92 21.61 12.53 15.20 11.83 10.88 27.63 29.32 27.52 25.26 25.30 21.45 21.61 19.19 16.21 15.41 12.38 15.27 11.00 14.80 14.68 7.52 8.50 7.09 9.69 9.07 4.38 5.08 4.71 3.28 3.33 3.42 3.50 3.34 3.66 3.80 2.71 1.86 2.76 1.81 2.31 2.61 1.94 1.30 71.13 62.69 56.38 53.32 52.06 49.80 49.71 45.00 43.19 40.41 39.37 38.64 37.97 37.63 36.26 32.27 31.66 31.62 31.44 27.02 26.35 26.16 25.50 22.08 18.47 18.01 17.70 17.64 16.86 15.59 14.96 13.23 12.43 12.38 12.30 11.92 10.34 9. Table 15. Document Understanding Results. model name CIFAR10 DTD EuroSAT FER2013 GTSRB MNIST PatchCamelyon STL10 VOC2007 mean EVA02-CLIP-bigE-14-plus google/siglip-so400m-patch14-384 google/siglip-large-patch16-384 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-g-14-laion2B-s34B-b88K EVA02-CLIP-bigE-14 google/siglip-large-patch16-256 laion/CLIP-ViT-H-14-laion2B-s32B-b79K laion/CLIP-ViT-L-14-laion2B-s32B-b82K royokong/e5-v laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K google/siglip-base-patch16-512 google/siglip-base-patch16-256 google/siglip-base-patch16-384 google/siglip-base-patch16-256-multilingual google/siglip-base-patch16-224 openai/clip-vit-large-patch14 blip2-finetune-coco laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-32-laion2B-s34B-b79K blip2-pretrain laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K voyage-multimodal-3 openai/clip-vit-base-patch16 facebook/dinov2-giant facebook/dinov2-base facebook/dinov2-large openai/clip-vit-base-patch32 facebook/dinov2-small Salesforce/blip-itm-large-coco TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA BAAI/bge-visualized-base Salesforce/blip-itm-base-coco kakaobrain/align-base Salesforce/blip-itm-large-flickr jinaai/jina-clip-v1 nyu-visionx/moco-v3-vit-l BAAI/bge-visualized-m3 EVA02-CLIP-L-14 nyu-visionx/moco-v3-vit-b Salesforce/blip-itm-base-flickr Salesforce/blip-image-captioning-large EVA02-CLIP-B-16 nomic-ai/nomic-embed-vision-v1.5 Salesforce/blip-image-captioning-base 99.50 96.92 96.74 98.42 97.88 99.47 96.72 97.64 97.15 94.14 98.55 92.66 93.34 92.91 92.89 92.60 96.15 97.70 96.84 94.05 98.62 95.86 95.54 91.60 98.53 96.45 97.95 89.85 92.95 95.74 87.93 87.94 97.75 87.66 81.23 94.34 90.62 90.13 96.27 98.99 89.36 83.85 94.58 98.12 97.25 81. 81.14 80.81 80.47 79.50 78.87 79.74 80.00 79.26 78.61 72.24 78.50 79.48 78.64 79.05 77.94 77.94 72.78 72.60 76.26 74.00 74.29 74.19 72.56 69.51 76.09 75.93 76.45 66.61 72.43 70.59 68.51 68.48 68.26 69.79 74.04 69.16 68.06 67.04 62.98 65.09 65.95 66.71 66.27 61.34 64.16 64.47 93.86 88.97 89.62 92.22 91.84 93.36 89.28 92.36 91.26 87.51 77.57 86.40 87.61 86.99 87.44 87.75 80.54 76.89 88.84 88.93 78.77 89.07 79.27 74.21 84.53 81.66 80.23 67.23 81.86 80.90 75.80 75.74 83.34 80.86 65.29 79.05 83.27 89.50 77.77 83.89 88.65 78.81 60.57 77.12 49.01 53.31 50.84 47.41 46.20 47.30 45.64 49.19 45.45 44.29 44.71 53.96 41.32 42.92 42.46 42.08 42.73 42.19 47.11 50.45 35.28 40.98 52.37 32.75 46.69 45.63 41.06 39.94 41.40 42.88 37.27 48.09 51.41 51.37 44.65 43.86 35.89 45.43 44.50 34.78 44.63 44.24 32.70 41.93 43.60 43.70 32.04 41.23 88.99 86.39 85.76 87.69 85.48 85.84 84.24 83.89 84.19 80.02 88.12 80.76 79.87 80.15 80.31 80.07 83.59 79.87 83.10 78.19 68.19 81.41 75.52 72.69 55.14 53.48 53.22 70.12 52.22 64.53 65.06 65.09 52.04 62.95 59.44 58.18 57.54 49.80 50.14 59.34 45.93 56.24 59.85 38.39 49.03 34.90 92.79 96.11 96.21 96.12 96.53 92.60 96.05 96.17 95.03 91.60 96.16 95.48 95.71 95.35 94.98 95.42 93.70 93.44 95.14 95.00 92.86 95.55 94.48 91.14 85.82 86.71 82.40 89.47 86.58 83.92 86.75 86.71 81.55 85.72 86.78 83.56 84.04 78.94 80.83 74.80 76.74 83.78 80.69 76.46 76.17 85.13 Table 16. Linear Probe for coarse-grained tasks. 76.48 75.41 77.31 71.25 74.08 72.82 75.48 73.02 72.32 72.39 73.53 73.52 73.10 73.57 73.91 73.07 74.74 71.68 70.21 69.68 73.17 69.92 67.79 70.60 74.17 72.73 74.37 70.96 74.55 67.69 68.21 68.16 65.19 64.62 68.30 66.09 62.97 73.08 68.29 69.04 72.99 63.98 66.72 65.57 65.69 63.43 99.76 99.51 99.33 99.53 99.39 99.73 99.21 99.39 99.18 98.81 99.44 98.72 98.35 98.63 98.24 98.33 99.39 99.38 98.57 97.77 98.60 97.67 98.78 98.65 97.84 97.72 97.93 97.73 97.36 98.85 97.60 97.60 99.19 97.87 95.95 98.37 97.06 95.39 98.84 99.39 95.14 96.89 98.23 99.00 98.59 94.20 91.68 92.40 92.23 91.81 91.99 85.98 92.12 92.11 91.94 96.11 91.68 92.63 92.29 92.54 91.88 92.02 90.93 94.41 90.74 90.28 90.62 89.63 78.51 90.46 85.44 85.94 85.51 90.07 86.94 69.32 71.00 70.97 68.89 66.72 91.81 64.03 66.89 72.80 67.35 51.74 71.02 60.62 45.21 45.06 60.33 34.29 86.11 84.88 84.87 84.87 84.63 84.30 84.28 84.24 83.82 82.98 82.76 82.51 82.37 82.36 82.26 82.15 82.10 81.82 81.66 80.99 80.83 80.67 78.79 78.28 77.62 76.73 76.61 76.10 75.80 75.52 74.70 74.67 73.43 73.34 73.19 73.13 72.77 72.39 71.90 71.83 70.94 70.31 68.41 67.20 65.81 61.37 model name Birdsnap Caltech101 CIFAR100 Country211 FGVCAircraft Food Imagenet1k OxfordFlowers OxfordPets RESISC45 StanfordCars SUN397 UCF101 mean EVA02-CLIP-bigE-14-plus EVA02-CLIP-bigE-14 google/siglip-so400m-patch14-384 google/siglip-large-patch16-384 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-g-14-laion2B-s34B-b88K laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K laion/CLIP-ViT-H-14-laion2B-s32B-b79K google/siglip-large-patch16-256 google/siglip-base-patch16-512 facebook/dinov2-giant laion/CLIP-ViT-L-14-laion2B-s32B-b82K google/siglip-base-patch16-384 facebook/dinov2-large openai/clip-vit-large-patch14 google/siglip-base-patch16-256 google/siglip-base-patch16-224 google/siglip-base-patch16-256-multilingual facebook/dinov2-base laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K openai/clip-vit-base-patch16 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K facebook/dinov2-small laion/CLIP-ViT-B-32-laion2B-s34B-b79K kakaobrain/align-base royokong/e5-v blip2-finetune-coco BAAI/bge-visualized-base openai/clip-vit-base-patch32 blip2-pretrain Salesforce/blip-itm-large-coco nomic-ai/nomic-embed-vision-v1.5 Salesforce/blip-itm-large-flickr jinaai/jina-clip-v1 voyage-multimodal-3 EVA02-CLIP-L-14 Salesforce/blip-image-captioning-large Salesforce/blip-itm-base-coco BAAI/bge-visualized-m3 Salesforce/blip-itm-base-flickr Salesforce/blip-image-captioning-base nyu-visionx/moco-v3-vit-l nyu-visionx/moco-v3-vit-b TIGER-Lab/VLM2Vec-LoRA TIGER-Lab/VLM2Vec-Full EVA02-CLIP-B-16 80.77 78.22 72.91 72.02 74.19 71.86 72.60 72.26 65.89 67.46 81.88 68.42 66.16 81.05 67.59 60.18 58.91 57.22 77.24 62.55 57.29 52.13 71.37 50.76 46.25 44.62 41.23 45.16 47.09 30.33 34.67 52.16 36.33 46.36 32.69 0.22 31.82 26.99 40.25 25.92 20.63 28.65 26.87 0.22 0.22 0.22 97.40 96.40 97.03 96.83 96.53 95.36 96.44 95.75 96.77 96.93 89.14 93.79 97.11 89.58 94.32 96.82 96.79 96.98 89.45 95.36 93.47 94.77 88.58 94.89 96.93 91.83 90.25 90.59 91.51 91.82 89.12 87.50 87.89 88.22 91.39 89.87 87.77 88.63 88.35 87.43 86.20 86.62 85.44 92.40 92.22 88. 94.15 93.76 84.59 83.49 88.03 86.62 88.85 86.29 83.48 74.47 89.63 84.73 74.81 88.83 79.99 75.67 74.10 74.68 84.49 83.40 71.24 80.84 77.02 77.17 58.83 71.90 82.26 82.96 67.41 87.51 76.45 84.12 73.50 70.19 78.13 87.17 72.95 59.69 78.14 55.88 52.57 69.95 70.18 60.20 60.19 82.82 31.64 28.84 32.47 23.00 28.57 26.10 24.59 25.11 19.40 18.05 13.53 22.02 17.38 12.73 26.97 14.98 14.58 15.11 10.73 16.98 18.69 13.17 8.10 13.41 15.16 10.64 8.72 10.12 14.73 10.75 9.08 11.73 10.12 9.59 8.90 18.12 8.44 8.74 10.37 8.13 9.53 7.03 6.56 8.35 8.35 0.47 78.19 74.37 78.49 75.44 70.02 65.99 64.15 65.09 71.15 70.08 70.37 60.45 69.00 65.14 56.19 66.13 66.07 59.93 62.74 53.93 46.01 49.27 58.67 47.12 40.19 37.74 35.21 35.91 38.13 34.27 22.37 54.01 21.78 32.35 22.52 54.65 20.62 24.01 38.87 20.10 17.31 18.62 18.90 22.06 22.02 39.64 94.93 94.70 95.47 95.00 92.52 91.13 92.73 91.04 93.64 92.67 88.10 89.61 92.13 87.79 91.99 90.25 89.88 89.97 84.44 88.07 86.46 81.51 77.68 78.78 82.74 85.31 84.13 84.12 79.45 88.24 81.76 86.86 81.95 79.50 87.38 90.21 78.36 76.31 80.37 74.50 68.59 54.39 50.54 74.68 74.71 80.76 82.40 81.47 82.16 81.20 78.23 77.32 77.29 76.54 79.34 77.54 78.70 74.67 76.77 78.62 75.10 74.73 74.19 73.95 75.92 70.75 67.53 64.94 69.40 64.13 67.18 66.57 65.51 64.86 61.16 58.72 67.91 0.10 66.97 60.57 58.51 0.10 64.42 60.60 0.10 59.25 50.03 64.27 62.39 0.10 0.10 0.10 99.55 99.47 99.53 99.57 99.33 99.18 99.10 98.98 99.47 99.10 99.71 98.82 99.02 99.61 98.92 99.00 98.57 99.33 99.57 98.92 97.27 97.75 99.51 96.84 96.24 94.16 94.06 95.25 94.82 96.61 96.49 98.88 96.24 93.06 92.65 98.02 94.20 87.55 94.41 85.88 85.35 89.92 88.80 84.29 84.20 95. 94.99 94.31 94.85 95.09 93.49 92.71 92.42 91.86 94.61 92.92 94.93 91.70 92.71 94.90 91.94 91.68 91.44 91.98 94.12 88.79 86.02 84.97 92.01 85.05 80.71 79.91 66.92 78.21 80.51 43.79 81.62 91.88 81.78 80.17 86.56 90.25 80.46 76.07 73.99 76.89 59.01 84.85 82.65 81.80 81.79 87.52 92.77 91.96 91.64 90.94 90.94 90.74 90.44 90.88 89.93 88.36 86.30 90.14 88.44 86.21 89.64 87.52 87.65 85.60 81.30 87.64 86.51 84.85 76.86 85.85 83.82 89.10 87.60 83.09 82.82 88.59 85.68 77.94 83.05 84.01 87.34 84.52 82.01 81.93 80.95 78.51 75.11 73.76 72.55 79.41 79.43 76.19 95.41 95.02 95.75 95.51 95.19 94.95 93.89 94.68 95.12 94.57 83.18 93.90 94.45 81.00 87.43 93.72 93.70 92.96 78.53 91.22 80.58 88.12 69.89 88.49 85.22 61.55 73.49 66.29 73.78 82.81 74.80 87.91 74.57 71.69 52.91 89.30 72.30 74.86 81.34 72.76 67.39 22.34 20.23 40.14 40.22 0.55 80.52 79.93 80.16 79.03 78.35 78.19 77.18 78.11 77.86 77.16 72.31 77.06 76.93 72.76 76.08 75.66 75.27 74.62 71.03 73.72 72.13 70.51 66.65 71.65 74.09 72.64 72.34 73.62 69.53 75.64 72.89 68.93 71.42 69.62 76.31 68.46 70.18 71.10 73.41 67.47 64.54 57.15 55.18 71.06 71.04 0.46 93.25 92.12 91.65 90.45 90.84 89.10 88.64 89.76 88.87 87.73 89.19 87.49 86.96 88.08 87.78 85.24 84.80 83.83 85.51 82.81 81.94 78.90 80.53 81.08 80.12 86.07 87.55 77.76 78.78 88.33 84.50 73.47 82.23 76.69 84.33 81.93 80.57 81.81 76.23 80.14 72.88 70.97 70.11 76.52 76.50 72.88 85.84 84.66 84.36 82.89 82.79 81.48 81.41 81.26 81.20 79.77 79.77 79.45 79.37 78.95 78.76 77.82 77.38 76.63 76.54 76.47 72.70 72.44 72.02 71.94 69.80 68.62 68.41 68.30 67.67 67.49 67.49 67.35 66.75 66.31 66.12 65.60 64.93 62.95 62.83 60.99 56.09 56.04 54.65 53.17 53.15 48. Table 17. Linear Probe for fine-grained tasks. model name CIFAR10 CLEVR CLEVRCount ZeroShot ZeroShot ZeroShot DTD EuroSAT FER2013 GTSRB MNIST ZeroShot ZeroShot ZeroShot ZeroShot ZeroShot PatchCamelyon RenderedSST2 ZeroShot STL10 ZeroShot mean google/siglip-so400m-patch14-384 voyage-multimodal-3 EVA02-CLIP-bigE-14-plus laion/CLIP-ViT-bigG-14-laion2B-39B-b160k google/siglip-large-patch16-256 laion/CLIP-ViT-g-14-laion2B-s34B-b88K google/siglip-large-patch16-384 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K EVA02-CLIP-bigE-14 laion/CLIP-ViT-H-14-laion2B-s32B-b79K laion/CLIP-ViT-L-14-laion2B-s32B-b82K EVA02-CLIP-L-14 google/siglip-base-patch16-384 google/siglip-base-patch16-512 google/siglip-base-patch16-256 google/siglip-base-patch16-224 royokong/e5-v google/siglip-base-patch16-256-multilingual laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K openai/clip-vit-large-patch14 laion/CLIP-ViT-B-32-laion2B-s34B-b79K Salesforce/blip-itm-large-coco laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K blip2-pretrain EVA02-CLIP-B-16 nomic-ai/nomic-embed-vision-v1.5 blip2-finetune-coco jinaai/jina-clip-v1 openai/clip-vit-base-patch16 TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA openai/clip-vit-base-patch32 Salesforce/blip-itm-large-flickr Salesforce/blip-itm-base-coco BAAI/bge-visualized-base BAAI/bge-visualized-m3 kakaobrain/align-base Salesforce/blip-itm-base-flickr 96.89 94.56 99.37 97.94 96.14 97.81 95.67 98.10 99.21 97.12 96.92 99.09 93.16 92.96 93.60 92.57 89.66 91.23 96.27 95.17 93.70 94.78 95.30 98.11 98.29 95.25 97.37 93.39 90.13 85.21 85.26 88.31 93.88 80.87 97.41 94.71 75.59 77. 22.43 13.91 19.97 20.05 21.47 18.59 20.73 25.19 16.18 16.85 16.09 20.17 22.37 22.21 23.39 24.00 15.80 20.36 23.57 16.08 18.85 19.08 20.64 24.71 15.69 15.81 25.03 15.62 15.83 19.26 19.25 16.34 17.32 19.93 15.79 21.03 23.52 12.21 40.57 45.65 29.72 29.71 40.65 37.43 32.99 35.57 16.24 26.57 31.31 31.47 22.02 24.09 22.69 23.66 20.72 25.40 32.53 19.43 15.29 29.03 12.94 18.26 21.09 23.39 15.96 22.35 21.21 31.59 31.69 23.20 13.94 21.46 16.06 14.01 19.47 14.08 66.91 59.04 63.99 64.20 64.73 66.17 63.78 64.47 63.30 62.50 58.88 62.77 65.27 65.53 65.37 63.51 54.52 61.06 55.37 52.82 54.36 54.63 54.41 46.28 50.48 47.77 44.84 52.87 42.87 47.23 46.91 41.91 52.98 51.49 42.55 33.40 57.71 47.23 58.69 52.35 71.30 66.56 53.80 63.09 55.11 70.70 74.76 72.30 65.35 67.20 39.96 38.37 44.19 41.09 50.48 33.33 52.06 60.85 48.70 49.20 55.31 65.69 58.46 42.37 52.31 47.37 48.59 24.80 24.93 49.74 43.50 40.39 51.37 41.11 36.87 34.43 54.21 51.35 54.51 57.13 56.59 55.66 58.28 40.97 54.42 50.82 56.27 49.44 51.23 51.41 53.01 52.42 58.44 51.18 30.66 47.52 47.21 47.35 27.90 51.10 48.31 26.99 51.88 47.21 43.55 32.22 31.72 43.52 47.99 40.05 35.59 37.77 37.81 29.84 64.46 55.11 68.24 61.15 61.30 49.78 63.71 56.92 65.31 57.49 58.27 56.77 51.87 51.17 50.78 51.98 46.17 53.06 53.34 48.71 45.74 34.82 48.89 27.05 42.35 44.26 40.33 38.76 41.05 41.51 41.93 34.42 33.25 35.09 32.34 31.08 26.84 32. 88.56 88.83 73.93 77.09 85.00 78.74 85.17 81.44 79.46 78.10 64.12 62.41 80.88 82.85 84.57 83.58 73.55 83.09 75.34 62.91 63.93 60.68 72.09 50.49 42.95 69.64 48.00 48.58 62.33 60.98 59.29 48.99 58.51 57.62 45.80 57.94 37.74 54.71 54.82 61.11 64.05 62.45 52.30 54.67 52.38 51.99 49.20 51.87 56.02 50.96 69.18 60.91 50.56 53.70 53.84 51.94 55.40 50.44 60.14 52.68 50.15 51.13 50.05 62.84 52.97 50.73 49.00 49.97 49.98 61.88 51.54 50.07 50.00 51.36 48.45 50.02 Table 18. Zero-shot Classification for coarse-grained tasks. 70.07 85.45 61.50 63.37 61.67 65.57 56.40 55.63 58.21 62.00 60.57 61.50 55.96 57.22 57.55 52.33 76.06 56.84 52.44 69.80 56.23 50.08 49.42 51.78 53.93 56.51 50.63 58.98 60.52 78.42 78.75 58.48 52.83 49.20 54.04 57.00 57.50 51.18 98.75 98.71 98.96 98.23 99.11 98.83 99.30 99.18 99.08 98.31 98.75 99.63 98.65 98.55 98.19 98.23 96.58 97.63 98.09 99.46 96.39 98.33 96.30 97.99 99.46 96.16 99.03 97.81 98.38 95.43 95.46 97.30 98.21 97.04 98.14 96.76 93.73 97.23 65.12 64.19 64.14 63.44 62.98 62.39 62.14 61.83 61.40 61.27 60.23 60.13 59.14 58.66 58.54 57.92 57.80 56.83 56.82 56.65 54.60 53.70 53.03 52.96 52.82 52.82 52.58 52.15 52.13 51.51 51.38 51.28 51.27 49.38 49.01 48.74 46.84 45. model name EVA02-CLIP-bigE-14-plus EVA02-CLIP-bigE-14 google/siglip-so400m-patch14-384 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K google/siglip-large-patch16-384 laion/CLIP-ViT-H-14-laion2B-s32B-b79K laion/CLIP-ViT-g-14-laion2B-s34B-b88K google/siglip-large-patch16-256 EVA02-CLIP-L-14 laion/CLIP-ViT-L-14-laion2B-s32B-b82K google/siglip-base-patch16-512 google/siglip-base-patch16-384 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K google/siglip-base-patch16-256 openai/clip-vit-large-patch14 google/siglip-base-patch16-224 EVA02-CLIP-B-16 google/siglip-base-patch16-256-multilingual laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-32-laion2B-s34B-b79K nomic-ai/nomic-embed-vision-v1.5 openai/clip-vit-base-patch16 openai/clip-vit-base-patch32 jinaai/jina-clip-v1 Salesforce/blip-itm-large-flickr kakaobrain/align-base Salesforce/blip-itm-large-coco voyage-multimodal-3 BAAI/bge-visualized-base Salesforce/blip-itm-base-coco blip2-pretrain BAAI/bge-visualized-m3 royokong/e5-v Salesforce/blip-itm-base-flickr TIGER-Lab/VLM2Vec-LoRA TIGER-Lab/VLM2Vec-Full blip2-finetune-coco Birdsnap Caltech101 CIFAR100 Country211 FGVCAircraft ZeroShot ZeroShot ZeroShot ZeroShot ZeroShot Food101 ZeroShot Imagenet1k OxfordPets RESISC45 ZeroShot ZeroShot ZeroShot StanfordCars ZeroShot UCF101 SUN397 ZeroShot ZeroShot 79.20 76.72 62.51 74.39 75.63 63.53 71.75 69.10 57.54 58.40 64.34 54.02 53.43 65.91 48.95 53.16 48.30 52.08 41.06 58.40 51.81 51.59 44.03 40.57 32.36 20.42 25.23 20.10 13.61 25.72 12.91 3.30 18.64 7.29 9.99 9.56 9.62 2.70 84.42 85.04 85.65 84.27 86.64 84.39 82.96 83.19 84.42 83.09 83.76 84.11 84.24 84.80 83.74 81.99 83.66 83.83 84.55 84.47 82.38 72.37 79.04 78.55 80.87 81.84 80.57 82.82 78.17 77.70 79.90 73.06 73.13 80.34 69.31 79.08 79.24 72.65 91.09 90.81 81.51 85.35 85.54 79.23 83.57 77.93 79.92 90.07 82.17 69.72 70.00 81.21 71.91 75.00 70.15 87.01 71.01 79.64 74.64 81.79 65.97 61.65 71.78 71.15 51.46 73.83 71.92 76.85 52.39 80.75 68.25 60.63 47.22 50.29 50.26 73.58 34.05 33.73 33.81 32.39 29.92 23.05 28.86 29.52 19.92 29.23 25.03 18.03 17.31 20.52 15.08 29.08 14.33 20.45 15.20 15.99 15.27 15.17 21.29 15.96 12.19 11.67 16.21 10.49 11.07 8.63 6.82 9.25 9.18 7.21 6.00 7.99 7.91 4.52 54.10 48.06 60.25 49.56 47.40 52.84 42.51 44.43 52.45 35.67 35.25 45.72 45.09 29.85 44.76 32.55 43.86 24.81 32.34 24.39 23.85 28.08 24.90 18.87 11.52 5.85 11.34 6.99 11.67 10.53 5.55 3.18 11.88 7.80 5.94 7.08 6.99 5.13 94.62 94.64 95.46 92.78 94.24 94.95 92.19 91.91 93.18 92.91 90.41 91.89 91.27 90.08 89.16 92.34 89.19 88.36 89.48 83.45 81.67 85.99 87.67 82.71 76.70 76.01 80.98 76.05 74.42 63.73 68.03 75.22 55.13 60.69 59.57 57.10 57.16 61. 79.13 79.93 80.89 77.65 77.41 79.93 76.11 76.44 78.77 78.06 73.23 78.13 77.33 72.23 75.64 71.21 75.12 73.19 74.36 67.87 65.15 69.82 63.99 58.84 58.70 60.95 62.70 60.94 60.91 50.35 53.47 39.04 38.48 48.66 48.28 50.38 50.41 44.20 95.80 95.69 96.54 95.28 94.96 96.78 94.55 93.95 96.16 93.95 93.21 94.96 95.01 92.80 94.25 93.40 94.17 92.26 93.79 90.43 90.62 91.09 89.04 87.49 80.84 77.92 84.30 76.64 70.51 56.23 69.09 23.68 49.74 32.60 62.20 48.02 48.24 29.93 72.46 73.51 69.57 69.76 71.22 68.65 70.65 71.21 67.75 69.30 71.00 62.97 62.83 65.08 61.19 67.67 62.27 63.11 58.48 60.90 63.32 57.62 60.54 53.40 55.63 57.14 48.89 57.03 61.73 59.24 50.40 55.57 52.06 57.95 45.46 52.71 52.59 52.87 94.17 93.98 94.63 94.02 92.81 94.19 93.02 93.76 93.61 90.03 92.12 92.51 92.38 88.30 91.01 76.57 90.83 78.82 89.13 85.64 85.59 87.44 63.54 58.61 68.09 71.16 72.95 69.71 49.40 43.10 66.29 62.89 56.35 26.08 54.12 22.37 22.26 29.71 74.09 76.04 75.26 73.54 73.89 73.22 74.65 73.28 72.87 73.98 73.57 71.29 70.74 70.57 70.02 64.40 69.85 70.32 69.46 66.97 68.23 64.42 60.56 61.06 65.44 70.40 70.48 67.14 68.64 62.01 64.76 64.46 56.30 64.79 61.11 60.51 60.37 60.32 69.61 71.12 76.85 68.62 63.21 69.76 67.05 70.00 68.75 69.18 67.20 64.68 64.14 54.78 61.10 66.66 61.90 64.66 62.87 49.73 53.95 50.51 62.19 58.15 50.67 51.54 45.45 47.49 61.47 44.84 44.23 50.51 43.85 60.48 44.83 53.92 53.85 46. Table 19. Zero-shot Classification for fine-grained tasks. Avg. 76.89 76.61 76.08 74.80 74.41 73.38 73.16 72.89 72.11 71.99 70.94 69.00 68.65 68.01 67.24 67.00 66.97 66.58 65.14 63.99 63.04 62.99 60.23 56.32 55.40 54.67 54.21 54.10 52.79 48.24 47.82 45.08 44.42 42.88 42.84 41.59 41.58 40.35 model name AROCocoOrde AROFlickrOrder AROVisualAttribution AROVisualRelation SugarCrepe Winoground ImageCoDE Avg. royokong/e5-v EVA02-CLIP-bigE-14-plus openai/clip-vit-base-patch16 jinaai/jina-clip-v1 openai/clip-vit-base-patch32 openai/clip-vit-large-patch14 EVA02-CLIP-B-16 EVA02-CLIP-L-14 voyage-multimodal-3 Salesforce/blip-itm-base-flickr laion/CLIP-ViT-bigG-14-laion2B-39B-b160k EVA02-CLIP-bigE-14 laion/CLIP-ViT-H-14-laion2B-s32B-b79K laion/CLIP-ViT-g-14-laion2B-s34B-b88K Salesforce/blip-itm-base-coco laion/CLIP-ViT-L-14-laion2B-s32B-b82K google/siglip-large-patch16-256 laion/CLIP-ViT-B-32-laion2B-s34B-b79K Salesforce/blip-itm-large-coco google/siglip-so400m-patch14-384 kakaobrain/align-base google/siglip-base-patch16-224 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K google/siglip-large-patch16-384 google/siglip-base-patch16-256 Salesforce/blip-itm-large-flickr laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K google/siglip-base-patch16-384 google/siglip-base-patch16-256-multilingual google/siglip-base-patch16-512 nomic-ai/nomic-embed-vision-v1.5 TIGER-Lab/VLM2Vec-Full TIGER-Lab/VLM2Vec-LoRA blip2-finetune-coco BAAI/bge-visualized-m3 BAAI/bge-visualized-base blip2-pretrain 41.30 45.05 48.18 52.83 46.37 45.66 40.54 39.41 40.90 29.56 33.20 33.23 33.01 32.92 21.63 31.07 32.63 33.28 18.27 30.08 25.26 31.67 33.17 29.82 29.97 16.59 28.70 30.16 27.34 27.60 22.99 29.59 22.27 20.33 6.09 22.87 16.44 4.21 36.12 52.82 56.12 49.02 56.60 54.90 51.74 47.00 37.18 32.16 38.84 39.06 39.42 37.44 26.02 38.70 37.76 40.46 20.56 34.92 36.08 38.40 38.10 33.34 38.62 15.04 34.56 35.92 34.18 36.00 32.14 37.20 22.20 20.36 6.38 8.74 6.16 5.12 74.54 60.52 61.84 61.70 61.40 61.63 61.79 61.98 65.88 76.30 61.11 62.01 62.19 62.89 76.91 58.94 57.97 58.05 76.52 59.67 66.95 54.04 57.35 57.80 54.10 75.34 59.18 56.18 54.38 53.85 54.13 55.10 62.39 61.41 68.32 58.43 53.89 67.21 59.20 51.40 53.64 51.36 51.76 53.27 53.92 53.31 51.94 53.72 51.80 49.40 50.17 50.90 52.06 50.97 45.82 50.39 52.28 46.56 51.09 46.24 49.52 45.25 45.84 53.44 47.73 46.44 45.99 45.19 46.09 46.36 55.72 55.02 51.55 45.89 46.56 49. 88.02 86.50 76.90 81.81 76.61 76.71 81.44 84.38 89.62 86.89 86.47 85.91 85.53 85.22 91.35 84.22 84.76 81.36 91.07 85.93 81.20 83.76 79.87 85.18 83.56 88.98 83.86 82.17 84.00 82.80 84.36 75.04 67.00 65.98 84.30 75.66 83.60 72.49 11.75 10.75 7.25 6.00 9.00 8.25 9.00 10.25 6.25 7.75 12.25 12.50 10.50 9.00 12.75 8.75 13.00 7.50 10.50 12.25 8.25 10.25 8.00 13.00 11.25 13.50 7.50 7.50 11.00 9.00 9.25 5.75 5.00 5.75 8.25 2.75 4.75 6.00 13.21 12.69 12.16 13.03 13.21 12.86 13.47 12.64 12.81 12.77 12.81 14.29 13.38 15.20 12.73 13.25 13.29 13.51 14.07 13.08 13.21 14.51 12.21 13.47 13.34 12.86 12.25 12.16 12.90 12.51 13.60 11.95 13.47 13.42 13.73 11.55 12.12 11.90 46.30 45.67 45.16 45.11 44.99 44.75 44.56 44.14 43.51 42.74 42.36 42.35 42.03 41.94 41.92 40.84 40.75 40.65 40.47 40.36 40.29 39.84 39.75 39.69 39.52 39.39 39.11 38.65 38.54 38.14 37.51 37.28 35.43 34.61 34.09 32.27 31.93 30.96 Table 20. Compositionality Evaluation Results. model name CIRRIT2I Fashion200kI2T Fashion200kT2I Flickr30kI2T Flickr30kT2I FORBI2I InfoSeekIT2IT InfoSeekIT2T METI2I MSCOCOI2T MSCOCOT2I NIGHTSI2I OVENIT2IT OVENIT2T EDIST2IT SketchyI2I SOPI2I TUBerlinT2I WebQAT2IT WebQAT2T VisualNewsI2T VisualNewsT2I RP2kI2I laion/CLIP-ViT-bigG-14-laion2B-39B-b160k google/siglip-so400m-patch14-384 EVA02-CLIP-bigE-14-plus google/siglip-large-patch16-384 laion/CLIP-ViT-g-14-laion2B-s34B-b88K laion/CLIP-ViT-H-14-laion2B-s32B-b79K EVA02-CLIP-bigE-14 voyage-multimodal-3 google/siglip-large-patch16-256 google/siglip-base-patch16-512 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K laion/CLIP-ViT-L-14-laion2B-s32B-b82K google/siglip-base-patch16-384 google/siglip-base-patch16-256 google/siglip-base-patch16-224 google/siglip-base-patch16-256-multilingual EVA02-CLIP-L-14 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K royokong/e5-v openai/clip-vit-large-patch14 laion/CLIP-ViT-B-32-laion2B-s34B-b79K Salesforce/blip-itm-large-coco jinaai/jina-clip-v1 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K Salesforce/blip-itm-large-flickr EVA02-CLIP-B-16 kakaobrain/align-base Salesforce/blip-itm-base-coco nomic-ai/nomic-embed-vision-v1.5 openai/clip-vit-base-patch16 BAAI/bge-visualized-m3 Salesforce/blip-itm-base-flickr BAAI/bge-visualized-base TIGER-Lab/VLM2Vec-LoRA TIGER-Lab/VLM2Vec-Full openai/clip-vit-base-patch32 blip2-pretrain blip2-finetune-coco 13.76 20.27 28.99 15.84 12.23 12.30 27.64 35.01 15.30 15.37 12.36 12.16 15.54 15.29 15.40 13.10 23.58 11.92 28.33 9.40 11.94 19.42 15.85 11.91 19.87 21.17 21.18 17.21 9.51 9.53 15.46 16.50 15.44 15.69 15.74 9.92 7.36 7.35 15.39 23.81 15.66 24.06 14.11 14.20 16.11 7.32 22.55 21.75 11.12 12.88 21.39 19.23 19.04 18.94 11.43 8.63 2.04 4.14 9.64 11.25 4.78 8.19 10.38 7.92 10.37 8.80 7.88 2.86 1.96 7.31 3.67 1.34 1.33 2.26 6.40 3.19 14.21 19.73 13.92 21.06 12.57 12.51 13.86 5.47 19.69 17.69 7.64 10.11 17.07 15.70 15.24 16.22 9.30 5.54 1.00 2.63 7.14 9.37 4.18 5.79 8.56 6.60 7.83 7.17 6.38 1.65 1.45 5.50 3.13 0.82 0.75 1.58 4.20 3.31 87.79 89.94 88.25 88.32 85.55 86.02 87.65 92.17 86.58 86.66 83.05 84.11 85.96 83.26 82.78 81.58 84.62 78.15 84.74 77.86 77.29 87.73 75.65 72.62 88.71 80.31 82.28 85.23 59.80 74.31 49.04 84.67 49.45 74.48 74.47 71.49 69.83 76.53 88.41 91.31 88.71 90.12 86.77 86.95 88.28 93.75 88.19 88.69 84.20 86.29 88.00 85.63 85.08 83.38 86.78 79.90 89.75 79.31 79.60 90.31 80.94 75.55 90.85 82.61 85.21 88.96 67.62 76.76 69.00 87.62 74.12 81.92 81.84 73.72 72.68 83.71 53.85 66.90 56.05 65.25 44.91 48.46 51.73 51.59 59.89 66.36 48.05 45.86 65.06 58.63 55.75 56.84 42.88 42.77 55.28 47.95 38.10 52.45 38.10 31.87 54.57 33.27 38.91 56.47 22.85 40.14 24.21 52.05 21.72 35.89 35.86 29.96 28.49 28. 26.28 4.38 0.31 9.37 24.53 23.63 0.21 33.95 10.00 5.73 16.89 18.21 5.92 6.33 5.96 6.80 0.05 9.99 8.04 10.29 10.64 11.40 14.22 7.66 9.65 0.00 0.06 8.20 21.98 5.24 9.19 5.26 14.58 5.30 5.39 3.57 10.92 6.07 5.74 0.24 0.12 1.09 8.74 5.74 0.16 22.65 1.09 0.37 1.52 2.74 0.38 0.48 0.37 0.45 0.05 0.81 6.42 0.14 0.96 3.40 3.46 0.44 2.80 0.00 0.01 1.19 3.02 0.15 6.58 1.02 11.03 3.38 3.43 0.16 2.41 2.41 43.82 42.69 44.70 42.84 43.35 43.43 44.48 31.97 41.80 42.09 42.69 41.57 41.80 41.16 40.78 39.63 39.48 41.22 35.15 37.34 39.56 39.77 38.77 39.97 39.38 37.53 36.63 40.58 29.98 35.69 32.98 40.39 34.06 35.59 35.40 35.84 39.63 38.51 60.18 63.53 61.18 62.35 58.83 59.13 60.68 62.16 60.78 59.79 55.56 56.11 59.28 57.29 57.19 54.73 56.08 50.97 55.19 48.84 50.21 68.27 48.28 47.22 61.67 51.77 53.26 65.62 37.11 45.80 25.11 54.58 31.43 50.61 50.57 44.40 48.73 55.70 65.60 68.94 66.60 68.47 64.26 64.36 65.83 70.36 67.36 65.60 60.69 61.72 65.22 63.62 63.81 60.43 60.74 56.58 68.14 52.66 56.23 73.85 58.37 53.49 69.77 57.29 58.34 71.64 46.67 50.33 39.62 61.69 50.04 59.50 59.48 47.65 48.63 63.20 26.02 23.63 26.46 24.86 26.45 26.04 25.93 24.86 23.98 25.59 25.68 25.69 25.52 25.66 25.75 24.65 23.06 25.66 22.38 21.89 25.79 25.30 23.87 25.71 24.04 23.22 22.57 24.97 23.09 21.73 23.56 24.28 22.23 25.67 25.22 23.05 20.23 18. 16.19 2.54 0.44 6.52 15.33 14.91 0.32 20.45 7.30 3.86 11.78 12.13 4.13 4.43 4.04 5.41 0.12 8.12 7.31 5.71 7.92 7.07 7.96 6.61 5.93 0.16 0.16 5.24 9.83 3.86 6.76 2.49 9.67 6.75 6.74 3.44 8.01 5.80 6.54 0.57 0.18 1.60 9.88 7.01 0.14 16.40 1.60 0.63 4.13 5.55 0.71 0.77 0.59 0.81 0.08 2.03 6.90 0.20 1.67 1.67 2.30 1.61 1.50 0.06 0.09 0.68 0.98 0.10 6.45 0.47 11.68 9.38 9.35 0.08 5.56 6.51 31.38 12.41 18.36 14.15 30.23 28.35 15.47 30.75 14.73 12.31 23.46 26.41 12.42 12.56 12.01 10.92 16.51 21.41 23.43 26.05 24.47 20.83 22.46 19.95 19.29 15.29 10.41 18.43 20.62 25.05 21.64 16.18 23.79 16.47 16.36 24.48 12.72 9.22 78.89 78.57 85.50 79.44 77.12 74.49 81.43 60.23 77.62 70.19 75.64 70.30 70.17 70.87 71.71 68.69 73.13 64.34 68.76 67.56 59.15 73.30 34.56 58.56 70.19 60.75 60.20 63.27 71.63 42.80 72.54 55.60 71.31 67.17 67.05 39.56 80.01 78.11 65.43 64.30 69.06 63.80 63.39 63.97 68.17 47.18 62.47 60.70 62.47 61.42 60.92 59.73 59.19 60.08 59.24 59.46 46.76 53.66 56.19 57.83 56.59 56.40 58.21 55.39 51.44 56.89 44.96 50.30 45.57 56.85 48.31 49.19 49.12 48.56 50.98 46.08 92.13 95.34 94.11 94.85 91.85 91.23 94.00 60.25 95.10 94.92 92.89 92.10 95.48 95.24 95.49 95.59 95.21 91.72 88.92 89.24 91.02 94.66 90.81 91.40 94.00 91.35 78.62 88.79 92.01 86.69 82.82 88.96 88.16 88.77 89.13 81.94 93.43 90. 46.10 26.17 47.13 30.46 42.41 42.94 42.34 65.64 32.61 31.43 40.71 39.68 31.20 31.50 30.84 29.42 38.03 35.59 49.63 34.30 37.59 45.99 51.61 37.22 42.99 36.21 35.51 43.28 52.67 27.64 59.38 42.00 59.80 20.91 20.80 23.94 21.12 19.14 36.34 20.04 39.18 21.48 34.34 33.99 35.73 62.62 23.58 22.04 32.44 31.88 22.05 23.54 22.19 21.09 31.20 29.19 60.47 23.98 31.12 30.75 62.41 29.46 30.72 26.90 26.15 27.32 64.90 21.22 68.58 28.53 68.91 35.50 35.56 21.06 19.00 22.39 44.50 36.50 46.36 28.69 41.01 40.16 44.51 12.47 26.05 22.73 34.67 35.34 22.57 20.02 19.54 18.09 38.69 26.02 8.40 35.24 28.02 17.03 12.77 20.76 16.15 29.28 24.77 12.33 11.59 28.64 4.89 9.65 3.52 11.62 11.43 24.84 6.71 4.13 44.65 36.46 45.25 27.90 41.44 40.64 43.79 16.91 25.36 21.64 33.90 35.88 21.29 18.81 18.41 16.31 37.38 24.82 12.97 37.39 26.59 16.28 13.95 19.21 16.15 27.55 25.43 12.66 12.50 29.68 8.93 8.94 8.41 12.18 12.00 25.48 10.57 7.66 model name GLDv2I2I CUB200I2I StanfordCarsI2I FashionIQIT2I GLDv2I2T HatefulMemesI2T HatefulMemesT2I MemotionI2T MemotionT2I SciMMIRI2T SciMMIRT2I VizWizIT2T VQA2IT2T ImageCoDeT2I BLINKIT2T BLINKIT2I ROxfordEasyI2I ROxfordMediumI2I ROxfordHardI2I RParisEasyI2I RParisMediumI2I RParisHardI2I laion/CLIP-ViT-bigG-14-laion2B-39B-b160k google/siglip-so400m-patch14-384 EVA02-CLIP-bigE-14-plus google/siglip-large-patch16-384 laion/CLIP-ViT-g-14-laion2B-s34B-b88K laion/CLIP-ViT-H-14-laion2B-s32B-b79K EVA02-CLIP-bigE-14 voyage-multimodal-3 google/siglip-large-patch16-256 google/siglip-base-patch16-512 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K laion/CLIP-ViT-L-14-laion2B-s32B-b82K google/siglip-base-patch16-384 google/siglip-base-patch16-256 google/siglip-base-patch16-224 google/siglip-base-patch16-256-multilingual EVA02-CLIP-L-14 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K royokong/e5-v openai/clip-vit-large-patch14 laion/CLIP-ViT-B-32-laion2B-s34B-b79K Salesforce/blip-itm-large-coco jinaai/jina-clip-v1 laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K Salesforce/blip-itm-large-flickr EVA02-CLIP-B-16 kakaobrain/align-base Salesforce/blip-itm-base-coco nomic-ai/nomic-embed-vision-v1.5 openai/clip-vit-base-patch16 BAAI/bge-visualized-m3 Salesforce/blip-itm-base-flickr BAAI/bge-visualized-base TIGER-Lab/VLM2Vec-LoRA TIGER-Lab/VLM2Vec-Full openai/clip-vit-base-patch32 blip2-pretrain blip2-finetune-coco 31.21 33.82 34.28 27.53 28.29 27.03 31.98 11.83 23.75 23.61 29.04 25.28 22.24 19.21 18.88 18.50 26.31 23.45 10.82 23.97 17.99 13.64 15.52 18.01 15.17 19.03 16.53 13.14 24.38 20.19 16.35 11.44 14.38 9.30 9.35 15.23 9.80 5.21 81.20 78.51 84.36 78.13 78.74 78.81 84.05 64.43 72.23 74.85 79.98 77.29 73.42 67.22 68.61 61.15 80.62 70.78 42.87 73.49 59.77 54.35 66.93 62.29 53.19 76.46 51.81 42.82 74.53 61.86 63.93 43.68 62.79 43.67 43.08 52.05 39.78 32. 90.78 93.22 91.06 93.36 89.52 89.68 90.66 46.78 92.87 91.97 88.73 90.01 92.00 90.64 90.67 89.29 88.67 86.07 42.57 80.66 81.18 66.63 64.86 82.13 66.36 82.18 78.04 70.23 89.01 71.66 82.32 69.47 65.04 43.29 43.10 62.43 68.57 49.20 5.16 11.06 10.88 7.64 3.92 4.19 9.92 15.46 6.75 6.86 4.39 4.13 6.49 5.51 6.12 5.53 7.56 3.44 5.76 3.11 3.05 6.65 4.51 2.83 6.59 6.03 6.73 5.38 2.02 2.59 5.31 5.01 4.52 2.12 2.21 2.21 1.63 1.86 80.05 81.79 80.73 67.12 75.92 75.03 78.27 46.66 64.81 58.31 75.65 70.26 58.45 56.23 56.02 57.96 72.88 70.21 28.55 73.51 61.27 33.34 47.10 65.62 37.18 62.52 51.22 25.70 57.14 64.55 22.99 24.81 28.24 18.50 18.40 59.09 31.29 11.67 66.32 84.16 65.29 83.25 64.79 64.23 65.37 55.51 82.53 83.65 63.64 63.62 83.12 81.65 80.72 79.84 59.68 62.19 62.50 56.96 57.54 35.66 50.31 55.31 36.14 46.96 54.68 35.74 37.48 51.57 37.31 35.86 26.17 20.60 20.87 43.22 42.93 25.71 67.44 84.39 66.54 84.70 66.00 65.31 66.69 71.98 83.83 83.90 64.65 64.78 83.77 82.23 81.98 80.90 62.11 61.87 69.39 61.85 58.53 46.20 54.03 57.24 42.35 48.48 57.25 46.16 44.65 55.78 50.75 39.83 34.61 43.59 43.83 46.49 44.71 28.54 90.76 97.21 90.26 96.20 88.72 88.76 90.15 87.06 96.05 96.61 88.82 87.35 96.48 96.25 95.96 94.79 82.35 85.03 80.31 76.09 84.13 64.11 79.01 81.17 63.40 70.26 85.09 60.31 51.20 72.61 58.41 51.62 47.27 20.02 19.82 65.15 56.77 41. 93.70 98.06 92.73 97.87 91.57 91.38 91.69 90.83 98.02 97.77 90.80 90.42 97.70 97.48 97.32 96.97 89.11 88.16 92.12 86.77 87.27 73.14 83.67 84.08 69.88 76.80 92.18 69.51 75.95 83.20 79.97 59.73 64.35 59.27 58.55 75.61 56.39 42.73 29.76 33.38 26.70 29.92 25.76 25.70 24.03 56.16 25.34 27.55 19.30 22.55 25.50 19.79 17.92 15.69 16.54 14.65 30.07 17.06 14.62 7.38 10.65 8.84 7.79 7.57 16.14 5.35 4.65 14.41 4.29 5.47 4.03 24.41 24.36 9.71 3.72 2.95 29.05 36.05 25.30 31.88 24.23 24.30 23.13 58.65 26.45 28.23 18.09 21.00 26.17 20.24 18.36 15.97 15.87 12.88 36.42 15.53 13.25 7.93 11.10 7.35 8.28 6.83 15.01 6.97 5.78 12.78 7.53 4.65 6.84 21.87 21.80 8.14 2.75 2.68 9.89 3.13 2.59 4.70 13.70 10.54 2.32 24.91 4.52 3.52 8.20 8.20 3.49 3.72 3.47 3.96 1.86 6.96 16.30 4.21 6.02 6.41 9.45 6.04 4.76 1.70 1.86 5.73 3.55 3.91 7.34 3.13 14.53 24.96 25.00 3.81 10.40 10.85 2.09 0.76 0.60 1.01 1.90 1.69 0.59 7.69 0.94 0.77 1.99 1.67 0.78 0.78 0.78 0.72 0.49 1.94 6.96 1.13 1.80 1.08 2.37 1.67 0.86 0.44 0.57 0.84 1.08 0.94 1.58 0.73 3.35 17.01 16.96 0.82 0.92 2.20 11.38 11.95 12.12 11.38 11.16 11.55 12.73 11.51 10.82 10.77 10.25 10.60 11.08 11.08 11.47 10.82 12.03 9.90 12.16 10.04 9.82 12.99 10.21 8.25 12.64 11.03 10.29 12.21 7.60 9.86 6.00 10.08 9.30 10.21 9.99 8.90 10.21 11. 39.55 36.82 0.75 33.58 38.56 37.81 0.25 37.31 35.57 38.31 36.32 38.56 35.82 34.58 34.08 37.31 0.25 34.08 31.34 35.82 32.84 42.54 33.33 30.35 35.82 0.25 0.00 40.30 35.57 34.58 32.59 32.09 33.83 32.34 32.09 31.59 37.06 36.32 29.76 20.18 28.12 19.80 21.69 21.06 21.82 33.80 19.42 13.74 29.26 24.09 12.74 12.74 13.87 18.92 27.87 16.90 36.07 9.33 7.44 24.34 25.35 17.91 22.19 15.51 23.33 22.57 21.18 6.81 19.80 9.08 16.02 39.85 39.98 6.68 19.55 26.23 30.75 30.75 34.96 34.52 28.52 32.93 32.67 16.47 34.44 31.92 33.10 30.79 29.73 31.66 31.53 28.83 27.44 33.57 13.22 21.29 28.26 20.44 22.62 28.99 17.91 24.82 22.52 22.20 27.73 19.19 24.27 17.02 19.88 15.00 15.00 21.55 12.40 11.43 17.01 18.11 18.64 18.33 15.09 18.69 17.95 9.26 16.84 17.13 17.99 15.92 16.29 17.39 17.09 15.55 15.57 17.09 7.04 14.04 14.18 11.29 11.71 16.27 9.84 13.16 11.04 11.98 16.28 10.57 11.77 9.20 8.53 9.61 9.53 11.21 7.11 6.80 10.54 13.12 12.02 12.82 9.13 13.08 14.03 6.66 8.95 10.36 9.79 10.14 9.17 9.78 8.53 7.75 9.29 10.05 3.02 10.94 7.38 7.42 6.68 11.04 5.49 4.91 3.13 7.33 10.52 7.28 6.55 5.53 3.79 5.90 5.81 4.54 3.33 4.90 11.51 10.94 11.54 10.19 10.82 10.86 11.67 10.27 10.01 10.21 10.50 10.77 9.51 9.96 10.00 9.48 11.25 10.63 9.65 10.17 9.59 9.93 11.13 9.53 10.20 11.21 9.05 11.05 10.60 9.53 9.61 10.43 10.43 9.58 9.68 9.22 9.30 9. 2.72 2.72 2.75 2.70 2.69 2.73 2.74 2.62 2.72 2.70 2.74 2.71 2.70 2.72 2.71 2.69 2.72 2.71 2.60 2.66 2.69 2.66 2.71 2.70 2.67 2.72 2.65 2.71 2.70 2.72 2.57 2.66 2.68 2.65 2.65 2.71 2.50 2.40 4.88 4.70 4.94 4.56 4.76 4.86 4.81 4.26 4.55 4.29 4.68 4.83 4.38 4.42 4.34 4.18 4.39 4.92 2.77 4.26 4.57 3.99 4.22 4.64 3.99 4.31 3.95 3.82 4.75 4.30 3.39 3.75 3.61 2.93 2.88 3.73 3.12 2.37 68.94 61.95 71.80 62.18 68.30 67.33 70.12 53.52 62.20 61.80 65.52 65.27 61.65 61.44 60.46 62.57 59.18 65.42 59.22 56.53 63.38 65.74 66.40 64.36 65.07 59.99 51.66 64.77 44.32 54.76 49.50 63.88 53.77 61.71 61.68 55.89 59.27 58.17 Avg. 41.50 40.78 40.12 39.91 39.85 39.73 39.02 38.84 38.83 38.11 38.11 37.96 37.66 36.59 36.27 35.61 35.59 34.84 34.04 33.67 33.27 32.80 32.35 32.18 31.85 31.06 30.98 30.93 30.45 30.13 28.44 28.22 28.05 27.70 27.64 27.49 26.69 24.52 Table 21. Retrieval Results. Model Name Clus. Compo. voyage-multimodal-3 google/siglip-so400m-patch14-384 google/siglip-large-patch16-384 royokong/e5-v google/siglip-large-patch16-256 google/siglip-base-patch16-512 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k google/siglip-base-patch16-384 laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K EVA02-CLIP-bigE-14-plus google/siglip-base-patch16-256-multilingual laion/CLIP-ViT-H-14-laion2B-s32B-b79K laion/CLIP-ViT-g-14-laion2B-s34B-b88K EVA02-CLIP-bigE-14 google/siglip-base-patch16-256 google/siglip-base-patch16-224 laion/CLIP-ViT-L-14-laion2B-s32B-b82K openai/clip-vit-large-patch14 laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K TIGER-Lab/VLM2Vec-LoRA TIGER-Lab/VLM2Vec-Full EVA02-CLIP-L-14 openai/clip-vit-base-patch16 Salesforce/blip-itm-large-coco laion/CLIP-ViT-B-32-laion2B-s34B-b79K jinaai/jina-clip-v1 Salesforce/blip-itm-base-coco laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K Salesforce/blip-itm-large-flickr kakaobrain/align-base Salesforce/blip-itm-base-flickr BAAI/bge-visualized-m3 BAAI/bge-visualized-base openai/clip-vit-base-patch32 EVA02-CLIP-B-16 blip2-pretrain blip2-finetune-coco nomic-ai/nomic-embed-vision-v1.5 (5) 82.41 82.11 79.94 70.05 82.13 74.65 85.59 76.27 86.44 92.38 74.56 83.86 82.74 89.42 75.24 74.50 83.50 76.41 81.73 72.64 70.72 88.27 69.47 77.53 77.99 69.95 70.59 77.37 76.43 59.59 67.27 73.57 76.19 67.90 82.55 74.01 67.84 83. (7) 43.51 40.36 39.69 46.30 40.75 37.51 42.36 38.54 39.11 45.67 38.14 42.03 41.94 42.35 39.52 39.84 40.84 44.75 38.65 34.61 35.43 44.14 45.16 40.47 40.65 45.11 41.92 39.75 39.39 40.29 42.74 32.27 31.93 44.99 44.56 30.96 34.09 37.28 Vis. STS (en) (7) 81.84 68.02 69.51 79.30 67.43 67.69 70.93 67.05 69.87 71.99 65.46 65.50 69.14 68.80 66.16 64.25 65.82 64.45 68.47 72.59 72.64 63.04 66.06 62.88 59.53 62.62 66.64 56.52 60.16 62.74 64.25 64.16 61.58 54.39 43.11 42.75 45.72 29.29 Doc. (10) 71.13 56.38 53.32 62.69 39.37 52.06 43.19 45.00 38.64 32.27 26.35 40.41 37.63 31.62 31.66 26.16 36.26 37.97 27.02 49.71 49.80 22.08 25.50 17.70 16.86 17.64 18.01 12.43 18.47 31.44 14.96 12.38 10.34 13.23 9.42 12.30 15.59 11.92 Cls. Coarse (8) 78.83 83.94 83.96 81.34 83.30 81.24 84.01 81.09 81.65 85.42 81.06 83.25 83.71 84.10 81.14 80.92 82.80 81.00 80.53 75.14 75.16 74.35 76.75 76.29 79.82 73.51 74.17 79.55 74.27 70.87 71.52 72.47 73.99 74.36 69.96 79.61 80.25 66.49 Cls. Fine (13) 66.12 84.36 82.89 68.62 81.20 79.77 82.79 79.37 81.41 85.84 76.63 81.26 81.48 84.66 77.82 77.38 79.45 78.76 76.47 53.17 53.15 65.60 72.70 67.49 71.94 66.31 62.95 72.44 66.75 69.80 60.99 62.83 68.30 67.67 48.16 67.49 68.41 67.35 ZS. Coarse (11) 64.19 65.12 62.14 57.80 62.98 58.66 63.44 59.14 61.83 64.14 56.83 61.27 62.39 61.40 58.54 57.92 60.23 56.65 56.82 51.38 51.51 60.13 52.13 53.70 54.60 52.15 49.38 53.03 51.27 46.84 45.56 48.74 49.01 51.28 52.82 52.96 52.58 52.82 ZS. Fine (12) 52.79 76.08 73.38 42.88 72.11 69.00 74.80 68.65 74.41 76.89 65.14 73.16 72.89 76.61 67.24 66.97 70.94 67.00 68.01 41.59 41.58 71.99 60.23 54.10 63.04 55.40 47.82 63.99 54.67 54.21 42.84 44.42 48.24 56.32 66.58 45.08 40.35 62.99 Vision Centric (6) 48.56 46.25 45.36 51.90 44.87 53.20 43.16 52.80 52.33 39.43 51.25 45.80 44.16 43.57 52.18 51.06 45.85 44.10 54.34 62.02 62.12 39.37 46.92 51.10 42.97 45.38 47.26 46.05 47.01 45.69 52.35 43.85 52.43 42.73 45.34 53.14 52.72 46.72 Retr. (41) 38.49 39.01 38.48 33.71 37.45 37.05 39.95 36.55 36.51 38.03 34.40 38.30 38.36 37.03 35.47 35.10 36.64 32.13 33.42 27.06 27.00 33.87 29.16 31.82 32.14 32.02 30.26 30.85 30.76 29.87 27.50 27.64 27.07 26.53 29.59 25.30 23.55 29.13 Multiling. Retrieval (3 (55)) Vis. STS (cross&multi) (2 (19)) Mean (Eng.) (125) Mean (Multiling.) (130) 58.87 40.19 51.11 66.57 49.84 43.21 28.01 42.55 23.77 27.82 59.21 25.54 25.92 25.54 41.26 41.23 23.02 20.24 21.57 34.92 34.96 23.43 17.66 18.53 20.13 18.09 16.81 20.13 18.12 22.36 13.44 46.35 12.25 16.73 20.12 13.86 13.05 14.48 70.42 41.39 39.76 46.25 38.11 38.12 34.54 37.54 35.78 28.21 40.27 33.85 31.70 28.29 34.44 33.54 26.02 35.12 28.49 42.21 42.19 22.48 29.80 33.69 26.16 34.59 38.59 23.63 30.50 27.29 38.03 23.46 24.75 21.80 22.4 21.77 22.87 14.10 62.79 64.16 62.87 59.46 61.16 61.08 63.02 60.45 62.22 63.21 56.98 61.48 61.45 61.95 58.49 57.41 60.23 58.32 58.55 53.99 53.91 56.28 54.41 53.31 53.95 52.01 50.90 53.20 51.92 51.13 49.00 48.23 49.91 49.94 49.21 48.36 48.11 48.76 63.10 60.27 59.96 58.95 58.29 57.68 57.73 57.05 56.81 57.34 55.77 56.19 56.01 56.11 55.05 54.07 54.28 53.22 52.96 51.42 51.35 50.73 49.29 48.77 48.82 47.73 47.03 47.98 47.32 46.75 45.12 46.01 44.67 44.83 44.55 43.27 43.08 43.02 Table 22. MIEB overall per-task category results, grouped by categories assessed. We provide averages of both English-only tasks and tasks of all languages, and the table is ranked by average on all tasks, including multilingual ones. Model Name Type Model Size Modalities Encoder kakaobrain/align-base [45] Encoder blip2-pretrain [59] Encoder blip2-finetune-coco [59] Encoder Salesforce/blip-vqa-base [58] Encoder Salesforce/blip-vqa-capfilt-large [58] Encoder Salesforce/blip-itm-base-coco [58] Encoder Salesforce/blip-itm-large-coco [58] Encoder Salesforce/blip-itm-base-flickr [58] Encoder Salesforce/blip-itm-large-flickr [58] Encoder openai/clip-vit-large-patch14 [84] Encoder openai/clip-vit-base-patch32 [84] Encoder openai/clip-vit-base-patch16 [84] Encoder facebook/dinov2-small [80] Encoder facebook/dinov2-base [80] Encoder facebook/dinov2-large [80] Encoder facebook/dinov2-giant [80] MLLM royokong/e5-v [46] Encoder QuanSun/EVA02-CLIP-B-16 [91] Encoder QuanSun/EVA02-CLIP-L-14 [91] Encoder QuanSun/EVA02-CLIP-bigE-14 [91] Encoder QuanSun/EVA02-CLIP-bigE-14-plus [91] Encoder jinaai/jina-clip-v1 [52] Encoder nyu-visionx/moco-v3-vit-b [13] Encoder nyu-visionx/moco-v3-vit-l [13] Encoder nomic-ai/nomic-embed-vision-v1.5 [1, 78] laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K [31] Encoder laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K [31] Encoder laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K [31] Encoder Encoder laion/CLIP-ViT-bigG-14-laion2B-39B-b160k [16] Encoder laion/CLIP-ViT-g-14-laion2B-s34B-b88K [16] Encoder laion/CLIP-ViT-H-14-laion2B-s32B-b79K [16] Encoder laion/CLIP-ViT-L-14-laion2B-s32B-b82K [16] Encoder laion/CLIP-ViT-B-32-laion2B-s34B-b79K [16] Encoder Alibaba-NLP/gme-Qwen2-VL-2B-Instruct [117] Encoder Alibaba-NLP/gme-Qwen2-VL-7B-Instruct [117] Encoder google/siglip-so400m-patch14-224 [116] Encoder google/siglip-so400m-patch14-384 [116] Encoder google/siglip-so400m-patch16-256-i18n [116] Encoder google/siglip-base-patch16-256-multilingual [116] Encoder google/siglip-base-patch16-256 [116] Encoder google/siglip-base-patch16-512 [116] Encoder google/siglip-base-patch16-384 [116] Encoder google/siglip-base-patch16-224 [116] Encoder google/siglip-large-patch16-256 [116] Encoder google/siglip-large-patch16-384 [116] Encoder BAAI/bge-visualized-base [118] Encoder BAAI/bge-visualized-m3 [118] MLLM TIGER-Lab/VLM2Vec-LoRA [47] MLLM TIGER-Lab/VLM2Vec-Full [47] MLLM voyageai/voyage-multimodal-3 [2] 176 1173 1173 247 247 247 470 247 470 428 151 151 22 86 304 1140 8360 149 428 4700 5000 223 86 304 92 428 151 150 2540 1367 986 428 151 2210 8290 877 878 1130 371 203 204 203 203 652 652 196 873 4150 4150 N/A image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image image image image image, text image, text image, text image, text image, text image, text image image image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text image, text Table 23. List of all models evaluated in MIEB. Model sizes are in millions of parameters. E. Task Category Examples E.1. Retrieval Figure 5 provides an example of retrieval task. Query An airport filled with planes sitting on tarmacs."
        },
        {
            "title": "Query",
            "content": "How many curtains are in the image? Choices 1, 3, 2, 0, 4 Answer 2 Figure 6. Vision-centric example from CVBench. E.4. Visual STS Figure 8 provides an example of Visual STS task. E.5. Document Understanding Given text query and corpus of image documents (documents can include figures, dense PDFs with texts, illustrations, etc.). We expect retrieval model to be able to return the most relevant document to the query based on their embeddings. See Figure 9 for an example. E.6. Zero-shot classification Figure 10 provides an example of zero-shot classification task. E.7. Linear Probing (Classification) Figure 11 provides an example of linear probing task. E.8. Clustering Figure 12 provides an example of clustering task."
        },
        {
            "title": "Answer",
            "content": "Figure 5. T2I Retrieval example from MSCOCOT2IRetrieval task. E.2. Vision-centric Tasks Figure 6 provides an example of vision-centric task. E.3. Compositionality Figure 7 provides an example of compositionality task."
        },
        {
            "title": "Rendered texts",
            "content": "Choices - table and chairs with wooden kitchen tools on top - kitchen and chairs with wooden table top on tools - and table with chairs on wooden kitchen tools top - table and chairs with wooden kitchen tools on top - top chairs with wooden table and kitchen tools on"
        },
        {
            "title": "Answer\na table and chairs with wooden kitchen\ntools on top",
            "content": "Figure 7. Compositionality example from ARO-COCO-order. Score 2.4 Figure 8. Visual STS example from rendered sts17 multilingual. Query What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?"
        },
        {
            "title": "Relevant document",
            "content": "Figure 9. Example of Document Understanding task from Vidore Benchmark, dataset SyntheticDOCQA healthcare industry BEIR."
        },
        {
            "title": "Image",
            "content": "Prompts photo of Coopers Hawk, type of bird. photo of Golden Eagle, type of bird. photo of Red-tailed Hawk, type of bird. photo of White-tailed Hawk, type of bird. photo of Rough-legged Hawk, type of bird. Answer photo of Golden Eagle, type of bird. Figure 10. Zero-shot Classification example from Birdsnap. Images of 3 different classes"
        },
        {
            "title": "Class Industrial Area",
            "content": "Figure 12. Clustering example from ImageNet-10. Figure 11. Linear Probing (Classification) example from RESISC45."
        }
    ],
    "affiliations": [
        "Aarhus University",
        "Contextual AI",
        "Durham University",
        "Esker",
        "INSA Lyon, LIRIS",
        "ITMO University",
        "Stanford University",
        "The Hong Kong Polytechnic University",
        "Zendesk"
    ]
}