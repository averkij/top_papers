{
    "paper_title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation",
    "authors": [
        "Tianwei Xiong",
        "Jun Hao Liew",
        "Zilong Huang",
        "Jiashi Feng",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to $\\bf{3 \\space billion}$ parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality."
        },
        {
            "title": "Start",
            "content": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation Tianwei Xiong1 Jun Hao Liew2 Zilong Huang2 Xihui Liu1 1The University of Hong Kong Jiashi Feng2 2ByteDance Seed Project page: https://silentview.github.io/GigaTok/ 5 2 0 2 1 1 ] . [ 1 6 3 7 8 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation qualitya challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers: (1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 billion parameters, GigaTok achieves stateof-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality. 1. Introduction Autoregressive (AR) language models (LM) have emerged as promising approach for visual generation [15, 50, 65, 68], driven by their proven scalability [2, 5, 14, 19, 37, 51, 52, 54, 55] and the potential for unified multimodal modeling [12, 45, 61]. The AR image generation framework consists of visual tokenizer and downstream AR generator. The tokenizer encodes images into discrete tokens, trained Figure 1. Reconstruction vs. generation dilemma: Naively scaling visual tokenizers achieves better reconstruction but degrades downstream autoregressive (AR) generation. In contrast, GigaTok achieves better performance for both reconstruction and generation as tokenizers scale up. with image reconstruction supervision, while the AR generator models the distribution of these discrete tokens through next-token prediction. The image tokenizer plays pivotal role in AR visual generation, providing compact and expressive latent space that enables effective generative modeling by downstream AR models. Despite its pivotal role, scaling of visual tokenizer is rarely explored in the literature. In fact, unlike the downstream AR models whose scalability has been widely validated [12, 30, 59, 61], scaling the visual tokenizer presents significant challenge. Specifically, there exists reconstruction vs. generation dilemma, where scaling tokenizer improves reconstruction fidelity but degrades downstream generation quality, as shown in Fig. 1. This dilemma is also observed in prior works [13, 21]. In this work, we seek to overcome this limitation and explore strategies for effectively scaling tokenizers to enhance both reconstruction and generation performance. Work partly done as an Intern at ByteDance. Correspondence Author. To investigate the root cause of this dilemma, we propose 1 Figure 2. The 2.9B GigaTok achieves SOTA autoregressive image generation with 1.4B AR model on ImageNet 256256 resolution. an AR probing scheme that trains lightweight downstream generative AR model to monitor the tokenizers training process. Surprisingly, we find that as tokenizers scale, the downstream AR model struggles more to learn the resulting token distribution, as evidenced by the increasing AR generation loss. This suggests that the larger tokenizers produce more complex token space, making it increasingly difficult for AR models to learn effectively. To address this challenge, we introduce pre-trained visual representation models (e.g. DINOv2 [43]) to regularize tokenizers. Specifically, we leverage semantic regularization loss during tokenizer training, encouraging high similarity between tokenizer features and the pre-trained model features. Such regularization helps constrain the latent space complexity, preventing the tokenizer from learning overly complicated latent token dependencies that hinder downstream AR generative modeling. Moreover, we design vector-quantized (VQ) tokenizer with hybrid CNNTransformer architecture as the backbone, suitable for both 1D and 2D tokenizers, and explore best practices for scaling tokenizers: (1) 1D tokenizers exhibit better scalability compared to 2D tokenizers; (2) Asymmetric model scaling, prioritizing decoder scaling over encoder scaling, proves effective; (3) Entropy loss [68] becomes crucial for convergence when training tokenizers with billion-level parameters. With our semantic regularization and three key scaling strategies, we effectively scale GigaTok to 3 billion parameters, overcoming the reconstruction vs. generation dilemma. We summarize our contributions as follows: We identify that the reconstruction vs. generation dilemma in tokenizer scaling stems from increased latent space complexity in larger tokenizers. To address this, we propose semantic regularization, effectively mitigating the dilemma and enabling tokenizer scaling. We explore best practices for scaling tokenizers, including 1D tokenizers with hybrid CNN-Transformer architecture, asymmetric encoder-decoder scaling, and entropy loss for billion-scale tokenizers. Our GigaTok is the first tokenizer scaled to 3B, achieving state-of-the-art reconstruction, downstream AR generation, and downstream AR representation on ImageNet. 2. Related Work Image tokenizers. Image tokenizers map image inputs into discrete or continuous tokens which can be modeled by downstream generative models. Continuous tokenizers are built upon Variational Autoencoders (VAE) [28, 29], and discrete tokenizers such as VQ-GAN [15] quantize visual features into discrete visual tokens during training. Vector Quantization (VQ) [15, 56, 65] is dominantly adopted for discretizing tokens, while other quantization methods [49, 68, 74, 75] focus on scaling codebook size for better tokenizers. However, how to properly scale up tokenizer models has rarely been studied in existing literature. concurrent work ViTok [75] attempts to scale continuous VAE-based tokenizers for downstream diffusion models, but ends up suggesting de-prioritizing tokenizer scaling due to its less predictable effect for downstream models. In contrast to previous work, we provide detailed analysis of the reconstruction vs. generation dilemma for scaling tokenizers and the solution to it. Autoregressive Visual Generation. Autoregressive visual generation models [15, 33, 38, 40, 49, 50, 56, 58, 59, 65] follow the next-token-prediction approach of LLMs, enabling them to leverage advancements in LLMs and simplifying the path to unified multi-modal generation. Other methods using discrete tokenizers incorporate visualspecific paradigms, such as mask image modeling [8, 60, 68, 69] and next-scale-prediction [36, 53], for improved performance. We focus on the autoregressive paradigm and reveal that scaling tokenizers helps AR models to be com2 parable to the best of those visual generation models. Semantic Guidance for Visual Models. Recent work has explored the use of guidance from visual foundation models [7, 23, 43, 46, 71] to enhance compressed visual latents with richer semantics [9, 10, 18, 36, 6264, 67, 72, 75, 76], as well as to improve the representations in visual generation models [70]. In contrast to these approaches, we analyze the dilemma in scaling tokenizers and emphasize the critical role of semantic regularization in their effective scaling. 3. Pilot Study We first introduce AR Probing as proxy to effectively monitor the tokenizers effectiveness for downstream generation (Sec 3.1), followed by pilot experiment that investigates the reconstruction vs. generation challenges when naively scaling visual tokenizers (Sec 3.2). 3.1. AR Probing for Tokenizer Evaluation In autoregressive visual generation, the training of the tokenizer and downstream AR model are performed in separate stages. In the first stage, visual tokenizer is trained to compress images into discrete tokens, optimized with reconstruction objective. In the second stage, the downstream generative model is trained based on the discrete tokens from the pre-trained tokenizer. However, tokenizer that performs well in terms of reconstruction fidelity in the first stage may not necessarily lead to better performance for downstream generative models. Thus, it is crucial to evaluate the effectiveness of the trained tokenizers for downstream generation alongside its reconstruction quality. Despite its importance, assessing how tokenizer influences downstream generation models can be computationally expensive. For example, sufficiently training 343M parameter downstream AR generator takes 170 hours on 64 V100 GPUs. To address this challenge, we introduce AR Probing, inspired by Linear Probing in representation learning literature [11, 23]. The key idea is to use the performance of small AR model as proxy to reflect the performance trends of large-scale AR models. Specifically, we use the tokenizer to train small Llamastyle model [50, 54] (111M parameters) for 50 epochs, and evaluate its gFID [24], validation loss, and linear probing accuracy [11, 23] for fair comparison between different tokenizers. Training the proposed AR Probing model for evaluating tokenizers is 10 more efficient than training the original 343M downstream AR model. Our experiments in Sec. 5.1  (Fig. 6)  demonstrate that the trends observed with AR Probing align with the performance of the large-scale AR models after sufficient training. gFID. The generation FID [24] of AR probing indicates the overall image generation performance of the two-stage framework. It reflects both the reconstruction fidelity of the Figure 3. Scaling trend for vanilla 1D tokenizers. As the model size increases, the reconstruction quality of vanilla tokenizers improves but the downstream AR Probing gFID consistently degrades. The increasing AR Probing validation loss indicates that scaling vanilla tokenizers results in more complex latent space, making it difficult for AR models to learn effectively. tokenizer and how well the downstream AR probing model can learn the dependency of the visual tokens (i.e., learnability of the token distribution). Validation loss. We use the validation loss of the AR probing model to measure the learnability of the latent tokens as disentangled factor. The validation loss is calculated as an average of the token-wise cross-entropy loss in the next-token-prediction paradigm on ImageNet [48] 50k validation set. With the same vocabulary size, the same number and structure of visual tokens, and the same AR probing model, larger validation loss indicates latent space that is more difficult for the AR model to learn. Therefore, we use validation loss to reflect the latent space complexity and learnability for AR models. Linear probing accuracy. Beyond visual generation quality, we also investigate whether scaling tokenizers will lead to better visual representations learned by the AR models, which may provide inspiration for future research in unified multimodal understanding and generation with AR models. To assess the representation quality of AR models, we adopt the standard practice [11, 23] of evaluating linear probing accuracy using features from the middle Transformer layer of the AR probing model. 3.2. Naively Scaling Tokenizers Does Not Work To study the challenges when naively scaling visual tokenizers, we train three vector-quantized tokenizers1 on ImageNet [48] at 256256 resolution with increasing model sizes. As shown in Fig. 3, as the tokenizer size increases, although the reconstruction quality (rFID) consistently improves, the AR generation performance (gFID) significantly degrades. This highlights the reconstruction vs. generation dilemma in tokenizer scaling. Moreover, we observe that the validation loss of AR Probing consistently increases as the tokenizers scale, indicating that larger tokenizers lead to complicated token dependencies that are more difficult for the AR model to learn. This observation motivates us to design the semantic regularization to constrain the latent space complexity of the tokenizer and therefore break the The tokenizer architectures are described in Sec. 4.1 3 reconstruction vs. generation dilemma in Sec. 4.2. 4. GigaTok In this section, we introduce the model structure and training strategies for our scalable visual tokenizer, GigaTok. In Sec. 4.1, we present tokenizer backbone supporting 1D and 2D token structures, and discuss the asymmetric scaling strategies for the encoder and decoder. In Sec. 4.2, we introduce semantic regularization, which breaks the reconstruction vs. generation dilemma by regularizing the complexity of the latent space with pre-trained visual representations. In Sec. 4.3, we show how entropy loss [68] facilitates the convergence of billion-scale tokenizers. 4.1. Architecture In current literature, the CNN [32] architectures are the dominant choices for image tokenizers [15, 40, 68, 75] due to their effectiveness in capturing fine-grained local details. Yet, Transformers are more scalable architectures with less inductive bias. Thus, we design vector quantized (VQ) tokenizer backbone with hybrid architecture that combines CNN [15, 32] and Transformer [6, 13, 57] for encoder and decoder  (Fig. 4)  . Specifically, our encoder consists of series of CNN blocks which progressively downsamples the input image by factor of p, followed by Transformer layers and vector quantizer to produce discrete latent codes. Similarly, our decoder consists of multiple Transformer layers, followed by CNN decoders which upsamples the features to obtain the reconstructed image2. Our tokenizer architecture can be adapted to both 1D and 2D tokenizers by utilizing different Transformer designs introduced in the next two paragraphs. 2D tokenizers with ViT. For 2D tokenizers, the Transformers in both tokenizer encoder and decoder are implemented by ViT [13] architecture. 2D structures of the latent features and tokens are preserved throughout the tokenizer. 1D tokenizers with Q-Former. For 1D tokenizers, we implement the Transformer modules in both encoder and decoder as Q-Formers [6, 34]. The Q-Former in the encoder employs 1D queries, transforming 2D input features into 1D latent tokens. The Q-Former in the decoder utilizes 2D queries to transform 1D latent tokens back to 2D features, which are then passed to the CNN decoder to reconstruct images. The 1D tokenizers remove the 2D inductive bias and demonstrate better scalability than 2D tokenizers in our experiments (Sec. 5.5). Asymmetric encoder-decoder scaling. Since the decoder faces the more challenging task of reconstructing images from lossy latent codes, we adopt an asymmetric design for more efficient parameter allocation. Specifically, we Throughout this work, we use downsample ratio = 16, codebook dimension = 8, and codebook size 16384 by default. Figure 4. GigaTok architecture and semantic regularization. Top: We use hybrid CNN-Transformer design for our visual tokenizer. The transformer layers are implemented with ViT for 2D tokenizer and Q-Former for 1D tokenizer. Bottom: We use frozen DINOv2 [43] image encoder for semantic regularization. scale both the encoder and decoder, while ensuring that the decoders are always larger than the encoders. In practice, we maintain the same and fixed size for the CNN encoder/decoder and only increase the depth and width of the Transformer modules for scaling. 4.2. Semantic Regularization In our pilot study (Sec. 3.2), the latent space complexity significantly increases as the tokenizer scales, which potentially leads to worse downstream AR generation for larger tokenizers. We hypothesize that larger tokenizers tend to capture excessive fine-grained low-level details for better reconstruction, resulting in overly complex latent token distributions, which makes it harder for AR models to learn the token dependencies effectively. To address this, we introduce semantic regularization to guide the tokenizer to encode more semantically consistent latent space, which is less complex and easier for downstream generative modeling. Specifically, we introduce simple semantic regularization term alongside the tokenizer training objective. The regularization aligns the intermediate features of the tokenizer decoder with the feature representations extracted from pre-trained frozen DINOv2 [43]. Mathematically, let dec,l be the output feature of the lth layer of the Transformer decoder, DINO be the semantic features of pretrained image encoder (here DINOv2B [43]). The semantic regularization can be represented as: Lreg ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) n=1 (cid:16) sim dec,l , ϕ(f DINO (cid:17) ) (1) where is the batch size, is the image index, sim(, ) is cosine similarity function, and ϕ() is an MLP that projects decoder feature dec,l to match the channel dimension of 4 Figure 5. Training curves for 2.9B XL-XXL tokenizers with and without entropy loss. 2.9B tokenizer does not converge without entropy loss. The entropy loss encourages high codebook usage and stabilizes training loss. DINO. When training VQ tokenizers, we add the semantic regularization to the original VQGAN [15, 50] objectives: Ltotal = Lvqgan + λLreg, (2) and we empirically set λ = 0.5 in this work. Here Lvqgan is combination of multiple losses , including Lrecon, the l2 reconstruction loss on image pixels, Lpercp, the perceptual loss [27, 73], LGAN, PatchGAN [26] adversarial loss, and LVQ [15, 65] the VQ codebook loss. 4.3. Entropy Loss for Billion-Level Tokenizers When training 2.9B tokenizer, we find that using the same training recipe as the 622M tokenizer leads to convergence failure for both perceptual loss and reconstruction loss, and consistently low codebook usage. We hypothesize that low codebook usage accounts for the convergence difficulty. To address this, we incorporate entropy penalty [66, 68] to encourage higher codebook utilization: Lentropy = Ez [H(ˆzz)] H(ˆz) (3) where H() denotes the Shannon entropy, RD is the input for quantizer to be quantized to ˆz = ci RD and ci is the i-th codebook vector. Ez [H(ˆzz)] penalizes the uncertainty in quantization to reduce quantization error, and H(ˆz) encourages the codebook vectors to be selected more uniformly across the entire codebook. The detailed derivation can be found in our supp. We find that the entropy penalty addresses the convergence difficulty of large tokenizers. As shown in Fig. 5, introducing entropy loss to the 2.9B tokenizer enables the codebook usage to quickly reach high level, and the loss converges properly3. 5. Experiments 5.1. Settings Type Enc./Dec. Params. Blocks Heads Dim. 1D Tok. 2D Tok. XL XXL 26M 115M 405M 948M 1870M 19M 86M 329M 6 12 24 36 48 6 12 24 8 12 16 20 24 8 12 16 512 768 1024 1280 512 768 1024 Table 1. Architectures of the transformer variants for tokenizer encoder/decoder parts in our experiments. We use QFormer [6, 34] for 1D tokenizers and ViT [13] for 2D tokenizers. Figure 6. Correlation between AR Probing Performance and Larger AR models. For 3 tokenizers: S-S, S-L, and B-L, we present that as the tokenizer improves, the performance improvements of AR Probing correlate to the performance improvements of larger AR models. Therefore, the AR Probing can effectively indicate how the tokenizer affects downstream larger AR models with limited computational costs. three perspectives: reconstruction, downstream AR generation, and downstream AR representation quality. We use rFID and LPIPS [73] to evaluate reconstruction fidelity, gFID to evaluate generation performance, and linear probing to evaluate the representation quality of the downstream AR model. Our downstream AR models are LlamaGen [50] with 1D absolute positional embedding. Our scaling experiments (Sec. 5.2) and ablation study (Sec. 5.3) use AR Probing (111M AR model described in Sec.3.1) validation loss, gFID, and linear probing to reflect the learnability of tokens, generation performance, and representation quality, respectively. While in the system-level comparison (Sec. 5.4), we train larger 1.4B AR models for comparison with previous work. More details are in the supplementary material. Effectiveness of AR Probing. As shown in Fig. 6, AR Probing performances including gFID and linear probing accuracy align with the larger LlamaGen-XL [50] model results. Therefore, we use AR Probing throughout the following experiments except for the system-level comparison. 5.2. Scaling with Semantic Regularization For scaling up visual tokenizers, we follow the architecture configurations for the Transformers in GigaTok tokenizers as summarized in Tab. 1. We evaluate the tokenizers from We take perceptual loss as an example, and reconstruction loss shows similar pattern We demonstrate that our proposed semantic regularization resolves the reconstruction vs. generation dilemma in scaling tokenizers. Model scaling with semantic regularization. Results are shown in Fig. 7. (1) Semantic regularization improves the 5 Figure 7. Scaling trends of tokenizers for reconstruction, downstream generation and representation quality with and without semantic regularization. By semantic regularization, GigaTok resolves the reconstruction vs. generation dilemma for tokenizer scaling in contrast to the vanilla version without semantic regularization. Moreover, GigaTok consistently improves the representation quality of downstream AR models by scaling up visual tokenizers. Note that in the last two figures, the red and blue curves correspond to different scales on the y-axis. Figure 8. Visualization of tokenizer features with and without semantic regularization. We compute PCA among the tokenizer features of group of images of the same golden retriever class and visualize the first 3 PCA components. We observe that the latent space of vanilla tokenizers shows inconsistent features both within single image or across multiple semantically similar images. In contrast, GigaTok encodes images with semantic consistency and thus reduces the latent space complexity for AR models. reconstruction fidelity, indicated by lower rFID. (2) More importantly, the AR Probing validation loss and gFID degrades for larger tokenizers without semantic regularization, showing the reconstruction vs. generation dilemma. The dilemma is addressed with semantic regularization, evidenced by the relatively constrained validation loss and consistently decreasing gFID. (3) The Linear Probing results show that semantic regularization helps AR models to learn better representations as the tokenizer model scales up. Visualization for the tokenizer feature space. We visualize the first 3 PCA components of the tokenizer features from the first Transformer decoder layer for group of images. As shown in Fig. 8, we find the vanilla tokenizer encodes latent space with limited semantic consistency, which potentially impairs its learnability for downstream AR models. In contrast, GigaTok presents semantically consistent patterns  (Fig. 8)  , indicating meaningful and consistent latent space. 5.3. Asymmetric 1D Tokenizer is More Scalable Enc./Dec. Size rFID LPIPS gFID Lin Acc. B-S S-B S-L B-L 0.98 0.94 0.83 0.81 0.221 0.214 0.206 0.206 6.56 5. 5.19 4.82 64.5 59.8 60.6 66.9 Table 2. The results for scaling encoder/decoder. Prioritizing the scaling of decoders benefits downstream generation more than scaling encoders (S-B v.s. B-S). But scaling encoders can still bring significant improvements (S-L v.s. B-L). Figure 9. Scalability comparison for 1D and 2D tokenizers. Using the same training setting, 1D tokenizers shows better reconstruction (rFID) and downstream representation quality (AR Probing: Lin Acc.). For downstream generation (gFID), 1D tokenizers present steeper improving trend than 2D tokenizers. tized when scaling up, we compare S-B4 and B-S tokenizers in Tab. 2, both trained under the same setting for 100 epochs. Our results show that scaling decoders, rather than encoders, leads to greater improvements in both reconstruction and downstream generation, suggesting that decoder scaling should be prioritized. Scaling tokenizer encoder is also important. While prioritizing the scaling of tokenizer decoders yields significant benefits, we also find that scaling tokenizer encoders can further enhance downstream models. In Tab. 2, we show Tokenizer decoder deserves more parameters. To determine whether the decoder or encoder should be prioriX-Y tokenizer denotes X-sized encoder and Y-sized decoder. For example, S-B indicates Small encoder-Base decoder structure 6 Tokenizer Tok. Type/Param. #Tokens rFID Generator Model/Param. Type gFID Acc."
        },
        {
            "title": "Continuous token modeling",
            "content": "VAE [47] SD-VAE [1] VA-VAE [64] VAE [35] VQGAN [8] TiTok-S [69] TiTok-L [69] B-AE-d32 [22] KL KL KL KL VQ VQ VQ LFQ VAR-Tok. [53] MSRQ ImageFolder [36]"
        },
        {
            "title": "MSRQ",
            "content": "VQ VQGAN [15] VQ ViT-VQGAN [65] RQ-VAE [33] RQ Open-MAGVIT2 [40] LFQ IBQ IBQ [49] LlamaGen-Tok. [50] VQ LlamaGen-Tok. [50] VQ GigaTok-B-L GigaTok-S-S GigaTok-S-B GigaTok-B-L"
        },
        {
            "title": "VQ\nVQ\nVQ",
            "content": "VQ GigaTok-XL-XXL VQ 55M 4096 0. 84M 1024 0.62 LDM-4 [47] DiT-XL/2 [44] SiT-XL/2 [42] SiT-XL/2 + REPA [70] LightningDiT [64] MAR-H [35] Diff. 400M Diff. 675M Diff. 675M Diff. 675M 675M Diff. 943M AR+Diff."
        },
        {
            "title": "Discrete token modeling",
            "content": "3.60 2.27 2.06 1.42 1.35 1.55 6.18 1.97 2.77 2.36 - - - - 74.6 - 60.0 - - - - 69.8 - - - - - - - - 40.5 - - 67.7 62.6 62.9 67.6 69.4 72.0 74.0 Mask. MaskGIT [8] 227M Mask. MaskGIT-UViT-L [4, 8] 287M Mask. 177M MaskGIT-ViT [8] AR+Diff 1.5B BiGR-XXL-d32 [22] 799M AR+Diff BiGR-XL-d32 [22] VAR-d24 [53] VAR-d30 [53] ImageFolder-VAR [36] 1.0B 2.0B 362M 1.4B Taming-Tran. [15] 1.7B VIM-Large [65] RQTran. [33] 3.8B Open-MAGVIT2-XL [40] 1.5B 2.1B IBQ-XXL [49] 343M LlamaGen-L [50] 1.4B LlamaGen-XXL [50] 1.4B LlamaGen-XXL [50] 111M LlamaGen-B (1d) [50] 111M LlamaGen-B (1d) [50] 111M LlamaGen-B (1d) [50] LlamaGen-B (1d) [50] 111M LlamaGen-XXL (1d) [50] 1.4B LlamaGen-B (1d) [50] 111M LlamaGen-XXL (1d) [50] 1.4B VAR 2.09 VAR 1.92 VAR 2.60 AR 15.78 AR 4.17 AR 7.55 AR 2.53 AR 2.05 AR 3.81 AR 3.09 AR 2.34 AR 3.33 AR 4.05 AR 3.83 AR 3.26 AR 2.03 AR 3.15 AR 1.98 70M 66M 66M 72M 641M 66M 109M 176M 23M 64M 66M 133M 128M 72M 72M 622M 136M 232M 622M 2.9B 256 256 256 128 32 256 286 256 1024 256 256 256 256 576 256 256 256 256 0.28 0.53 2.28 1.71 2.21 1.69 1.00 0.80 4.98 1.28 3.20 1.17 1.37 2.19 0.94 0.51 1.01 0.89 0.81 0. Table 3. System-level comparison for tokenizers and downstream generation models on ImageNet 256256. For gFID, we present the lowest value between w/ or w/o CFG scenarios. : Training set includes data besides ImageNet. : Using frozen DINO [7] for discriminator, which largely improves rFID. : Without classifier-free-guidance. : Data from BiGR [22]. that B-L tokenizer gains significant improvements compared to an S-L tokenizer. Therefore, we recommend scaling both encoders and decoders while maintaining larger decoder than the encoder for optimal performance. 1D tokenizers are more scalable than 2D tokenizers. We train S-S, S-B and B-L 1D/2D tokenizers with the same setting with semantic regularization. As shown in Fig. 9, 1D tokenizers consistently achieve better rFID and AR Probing linear probing accuracy than 2D tokenizers. For AR Probing gFID, the 1D tokenizers exhibit steeper scaling trend, eventually surpassing 2D tokenizers as the model scales. We attribute the superior scalability of 1D tokenizers to the reduced inductive bias. 5.4. System-level Comparison Experiment Settings. Using GigaTok for tokenization, we scale the training of LlamaGen [50] AR models on 256 256 ImageNet training set for 300 epochs to compare with other methods. We do not use AdaLN [44, 53] as it is specific for class-conditional generation. We provide the results of B-L tokenizer trained with DINO discriminator [36, 53] to fairly compare rFID. But in practice we find DINO discriminator provides limited improvement for LPIPS and may affect the training stability of billion-scale tokenizers. Therefore, we exclude it from our main design. Results. As shown in Tab. 3, our 2.9B GigaTok achieves state-of-the-art reconstruction performance (rIFD) among all discrete tokenizers. Furthermore, with our 2.9B tokenizer, the downstream 1.4B AR model achieves state7 DecoderAR Model Size L"
        },
        {
            "title": "XXL",
            "content": "Sem. Reg. λ rFID LPIPS gFID Lin Acc."
        },
        {
            "title": "B\nL\nXXL",
            "content": "3.7% 2.3% 1.3% 11.2% 7.0% 3.4% 32.4% 20.3% 9.9% 0.25 0.50 0.75 1.00 1.28 1.22 1.27 1.38 0.226 0.228 0.236 0.239 6.27 6.39 6.29 6.27 57.0 58.6 58.6 62. Table 4. Ratio of time consumptions for tokenizer decoding during image generation. When we use 2.9B XLXXL tokenizer for 1.4B LlamaGen-XXL AR model, the tokenizer decoding only takes 9.9% of the total inference time. of-the-art image generation performance (gFID) among LLM-style autoregressive next-token-prediction models. VAR [53] predicts images with next-scale prediction rather than next-token-prediction, which is less compatible with language models. Our model achieves comparable gFID to VAR [53] with simple LLM-style downstream AR generator without incorporating vision-specific designs like VAR. Moreover, this 1.4B AR model trained on the 2.9B tokenizer achieves state-of-the-art linear probing accuracy via visual generative pretraining5. This indicates that our GigaTok helps the downstream generation model to learn better representations. The high-quality representation learned from generative pre-training may also help unify generation and understanding for future native multimodal models. 5.5. Discussion and Ablation Study Align. Layer rFID LPIPS gFID Lin Acc. 2 3 4 1.06 1.01 1.07 0.224 0.223 0.223 6.26 6.10 6.07 63.4 61.9 58.6 Table 5. Layer for semantic regularization (S-S tokenizer). Smaller brings better downstream AR model representations but can sacrifice reconstruction and downstream generation quality. We choose l=3 by default for more balanced performance. Sem. Enc. rFID LPIPS gFID Lin Acc. CLIP-B [16, 46] SigLIP [71] DINOv2-B [43] 0.91 0.92 0.85 0.210 0.210 0. 6.35 6.20 5.55 61.4 56.7 64.4 Table 6. Ablation study for the choice of pretrained semantic encoders (S-B tokenizer). DINOv2-B delivers the best performance among all models. Discussion on generation costs. When generating an image, AR models take multiple passes to predict tokens, REPA [70] achieves better representation by directly distilling pretrained representations to the generation model, which is not fair comparison with ours as we do not leverage the supervision for AR training. Table 7. Ablation Study for the semantic regularization weight (S-S tokenizer). strong semantic regularization weight leads to worse reconstruction but better downstream representation. We choose λ = 0.5 by default for more balanced performance. while tokenizers only need one forward pass. Therefore, the time consumption for decoding tokens to images is relatively small compared to AR models. We record the ratio of time spent on tokenizer decoding for different tokenizer/AR models in Tab. 4. For 1.4B AR model, our largest 2.9B tokenizer takes only 10% of the total inference time. Searching the best layer for semantic regularization. We search l, the layers index in the Transformer decoder before intermediate features are extracted to calculate semantic regularization in Eq. 1. As shown in Tab. 5, varying presents trade-off between gFID and the Lin Acc. for AR Probing. Smaller means stricter regularization for the latent space so that the downstream generation models learn better representation. However, smaller also sacrifices generation quality. We choose = 3 for more balanced rFID, gFID, and linear probing accuracy for all tokenizers. Exploring pretrained semantic encoder choices. We compare CLIP-B (DFN) [16, 46], SigLIP-400M [71] and DINOv2-B [43] as the source of semantic regularization for S-B tokenizers. As shown in Tab. 6, utilizing DINOv2-B as the semantic encoder for regularization produces the best tokenizer for reconstruction, downstream class conditional generation and representation quality. Exploring weights for semantic regularization. We study the effects of different regularization weights λ (Eq. 2), from 0.25 to 1.00. As shown in Tab. 7, large λ (0.75, 1.00) will damage the reconstruction quality but benefits the linear probing accuracy, whereas smaller λ (0.25) results in suboptimal rFID and linear probing accuracy. We choose the more balanced λ = 0.5 as default for all tokenizers. 6. Conclusion In this work, we study and address the reconstruction vs. generation dilemma for scaling visual tokenizers. We identify that the dilemma stems from increasing latent space complexity in larger tokenizers. We propose semantic regularization to effectively regularize the tokenizer latent space by injecting pre-trained representations to align with tokenizer features in training. The semantic regularization, together with several key practices we explored, lead to the first 3B tokenizer, GigaTok, that achieves state-of-the-art reconstruction, downstream AR generation, and downstream AR representation quality. Please refer to discussions on limitations and future work in supplementary materials. 7. Acknowledgments The authors sincerely thank Qihang Yu and Liang-Chieh Chen for their valuable discussions during the development of GigaTok."
        },
        {
            "title": "References",
            "content": "[1] stabilityai/sd-vae-ft-ema. https://huggingface.co/ stabilityai/sd-vae-ft-ema, 2023. 7 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [3] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin ElNouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. arXiv preprint arXiv:2502.13967, 2025. 3 [4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 7 [5] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 4, 5, 1 [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 3, 7 [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 2, 7 [9] Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. arXiv preprint arXiv:2412.10958, 2024. 3 [10] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. arXiv preprint arXiv:2502.03444, 2025. 3 [11] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. [12] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1 [13] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 4, 5 [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 1, 2, 4, 5, 7 [16] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 8 [17] Christopher Fifty, Ronald Junkins, Dennis Duan, Aniketh Iger, Jerry Liu, Ehsan Amid, Sebastian Thrun, and Christopher Re. Restructuring vector quantization with the rotation trick. arXiv preprint arXiv:2410.06424, 2024. 2 [18] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. 3 [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [20] Alexander Hagele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024. 1 [21] Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. arXiv preprint arXiv:2501.09755, 2025. 1, 4 [22] Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, and Kwan-Yee Wong. Bigr: Harnessing binary latent codes for image generation and improved visual representation capabilities. arXiv preprint arXiv:2410.14672, 2024. 7 [23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 3 9 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [25] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small arXiv language models with scalable training strategies. preprint arXiv:2404.06395, 2024. 1 [26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. 5 [27] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694711. Springer, 2016. 5 [28] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 4 [29] Diederik Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4):307392, 2019. [30] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, MingHsuan Yang, Irfan Essa, Huisheng Wang, David Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation. In Proceedings of the 41st International Conference on Machine Learning, 2024. 1 [31] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 1 [32] Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995. 4 [33] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 2, 7 [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 4, 5, 1 [35] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 7 [36] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. 2, 3, 7, [37] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 1 [38] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 2 [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 2 [40] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 2, 4, 7 [41] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. 3 [42] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 7 [43] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3, 4, [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 7, 1 [45] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. 1 [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 3, 8 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 7, 1 [48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Imagenet large Aditya Khosla, Michael Bernstein, et al. scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 3 [49] Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. tokenizer for autoregressive image generation. arXiv preprint arXiv:2412.02692, 2024. 2, 7 Taming scalable visual [50] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. 2024. 1, 2, 3, 5, 7 [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [52] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [53] Keyu Tian, Yi Jiang, Zehuan Yuan, BINGYUE PENG, and Liwei Wang. Visual autoregressive modeling: Scalable imIn The Thirtyage generation via next-scale prediction. eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 7, 8, 1, 3 [54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3 [55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [56] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [57] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [58] Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. Larp: Tokenizing videos with learned autoregressive generative prior. In ICLR, 2025. 2 [59] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 2 [60] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: arXiv Embedding-free image generation via bit tokens. preprint arXiv:2409.16211, 2024. 2 [61] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 1 [62] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [63] Wanghan Xu, Xiaoyu Yue, Zidong Wang, Yao Teng, Wenlong Zhang, Xihui Liu, Luping Zhou, Wanli Ouyang, and Lei Bai. Exploring representation-aligned latent space for better generation. arXiv preprint arXiv:2502.00359, 2025. [64] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. 3, 7 [65] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 1, 2, 5, 7 [66] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 5, 1 [67] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, 36:5269252704, 2023. 3 [68] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 1, 2, 4, [69] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550, 2024. 2, 7, 3 [70] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 3, 7, 8, 2 [71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 3, 8 [72] Baoquan Zhang, Huaibin Wang, Chuyao Luo, Xutao Li, Guotao Liang, Yunming Ye, Xiaochen Qi, and Yao He. Codebook transfer with part-of-speech for vector-quantized image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77577766, 2024. 3 [73] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [74] Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image 11 and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. 2 [75] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024. 2, 3, 4 [76] Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, and Lidong Bing. Stabilize the latent space for image autoregressive modeling: unified perspective. arXiv preprint arXiv:2410.12490, 2024. 3 12 GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Limitations and Future Work This study primarily focuses on scaling tokenizers for classconditional image generation. While we have demonstrated the effectiveness of GigaTok for downstream classconditional generation, expanding the scope to include textconditional image generation or video generation remains an open avenue for future work. Additionally, unlike CNNbased 2D tokenizers, 1D Transformer-based tokenizers are not directly applicable to multiple resolutions without additional training adjustments. This challenge presents an important direction for further exploration. Besides scaling the model sizes of tokenizers, the effect of scaling training data, codebook dimension and codebook size for downstream autoregressive generation are left for future research. B. Configurations for AR models"
        },
        {
            "title": "Size",
            "content": "Params. Blocks Heads Dim."
        },
        {
            "title": "B\nL\nXL\nXXL",
            "content": "111M 343M 775M 1.4B 12 24 36 48 12 16 20 24 768 1024 1280 1536 Figure 10. The architecture of GigaTok with Q-Former. Table 8. Architectures of the LLamaGen models in our experiments. Figure 11. Initialization of 1D queries in Q-Former modules. AR model training. We scale up the training of downstream Llama-style [50, 54] AR models to compare generation performance with other models. For model training, we use WSD learning rate scheduler [20, 25] with 1104 base learning rate, 0.2 decay ratio and 1 epoch warm-up. We do not use AdaLN [44, 53] as it is specific for class-conditional generation. We use batch size of 256 for training the B, and XL models and 512 batch size for training the XXL model. Our AR models are trained for 300 epochs on the 256 256 ImageNet training set. CFG for gFID. Since gFID of GPT models can be largely affected by classifier free guidance (CFG) [47, 50] and often has an optimal CFG [50], for fair comparison, we search the optimal CFG using zero-order search with step of 0.25 and use the lowest gFID as the final value. For AR Probing, we use constant CFG scheduling for simplicity. For system-level comparison, we use step function for CFG scheduling inspired by [31]. Specifically, the AR models predict the first 18% tokens without CFG, i.e., CFG = 1 for better diversity, and use CFG for the remaining tokens for better visual quality. Interestingly, we find that the 1.4B LlamaGen model achieves the best gFID without CFG. C. Detailed GigaTok Implementation Please refer to Tab. 9 for training details. Q-Fomrer in GigaTok. GigaTok utilizes Q-Former [6, 34] to build 1D tokenizers, as shown in Fig. 10. For Q-Former encoder in GigaTok, we initialize the 1D queries initialized from the 2D input features of the CNN encoder using multi-level average pooling strategy, as shown in Fig. 11. Specifically, for the same 2D input features, we spatially divide them with different granularity at different levels, and perform average pooling for every divided region at each level. The pooled features are flattened and concatenated from level 0 to the last level. Therefore, 1D token sequence with 2L length can be initialized with levels from 2D input features. At the decoding stage, the 2D queries are all initialized from the first 1D latent feature. Entropy Loss for VQ Tokenizers. While entropy loss [66,"
        },
        {
            "title": "Configuration",
            "content": "Q-Former Encoder depth Q-Former Encoder heads Q-Former Encoder dim. Q-Former Decoder depth Q-Former Decoder heads. Q-Former Decoder dim. Params (M) S-S 6 8 512 6 8 512 136 S-B S-L B-L XL-XXL 6 8 512 12 12 768 232 6 8 512 24 16 1024 533 12 12 768 24 16 1024 622 36 20 1280 48 24 1536 2896 Codebook size Codebook dimension #Tokens Training epochs Batch size Alignment Layer Learning rate schedule Base learning rate Minimum learning rate LR warm-up iterations Optimizer Opt. momentum Entropy Loss weight 16384 8 256 100 128 200 128 200 256 200 256"
        },
        {
            "title": "3\nCosine Decay\n1 × 10−4\n1 × 10−5\n0\n0\nAdamW[39]\nβ1 = 0.9, β2 = 0.95",
            "content": "5000 0 0 5 103 0 0 0 Table 9. GigaTok configuration and default training details 68] is discussed for LFQ [68], its application to VQ tokenizers is less commonly explained. We provide detailed derivation of the entropy loss specifically for VQ tokenizers. Mathematically, for quantization process from continuous vector RD to quantized vector ˆz = ci RD where ci is the i-th codebook vector from codebook RN D, we assume this process is statistical and follows the following distribution: p(ˆz = ciz) softmax(l2(z, C))[i] (4) where l2(z, C) RN is the L2 distance between and all the codebook vectors. Then, minimization of the quantization error can be partially achieved by minimizing the expectation of entropy Ez [H(ˆzz)], which can be understood as maximizing the prediction confidence for p(ˆzz). To encourage higher codebook utilization, we aim to make the average appearance probability of codebook vectors more uniform. This is achieved by maximizing the entropy H(ˆz), Therefore, the optimization of the two entropy terms leads to the final entropy loss equation: Lentropy = Ez [H(ˆzz)] H(ˆz) (5) In practice, to calculate H(ˆz), we estimate p(ˆz = ci) by p(ˆz = ci) = Ez [p(ˆz = ciz)]. Note that entropy loss is not our contribution. We only provide detailed definition of entropy loss in VQ scenarios for better understanding. Additional implementation details. To stabilize the training of our tokenizer with hybrid architecture, we initially use shortcut feature reconstruction trick at the first 15k iterations of the tokenizer training. But we later found that this trick can be replaced with simple 1-epoch learning rate warmup combined with entropy loss [15, 68]. Specifically for this trick, we additionally give the output feature of the CNN encoder to the CNN decoder directly to be trained for reconstruction, and also align the output feature of the Transformer decoder to the output feature of the CNN encoder, besides the original training objectives. Note that this strategy is complex and can even hinder performance for XL-XXL tokenizers. We recommend using the learning rate warmup combined with entropy loss [15, 68] instead, for both XL-XXL tokenizer and the smaller ones. Additionally, we utilize the rotation trick [17] for all tokenizers, though we observe its effect on performance to be limited for our tokenizer. The implementation of the semantic regularization is partially inspired by REPA [70]. D. Full Evaluation Results and Analysis Here we present the full evaluation results for the tokenizers and downstream AR models, as summarized in Tab. 10. We observe that scaling up visual tokenizers consistently improves the reconstruction quality across multiple metrics. Interestingly, for the 1.4B AR model, the lowest gFID is obtained without applying any CFG. This phenomenon is 2 Tokenizer Param. rFID LPIPS PSNR SSIM AR Model Param. gFID Acc. IS Precision Recall LlamaGen-Tok. [50] 72M 2.19 - 20. 0.675 LlamaGen-B [50] 111M 5.46 - 193.61 GigaTok-S-S GigaTok-S-B 136M 1.01 232M 0.89 0.2226 20.74 0.2121 20.93 0.670 0.677 GigaTok-B-L 622M 0.81 0.2059 21. 0.685 GigaTok-B-L 622M 0.51 0.206 21.32 0. GigaTok-XL-XXL 2.9B 0.79 0.1947 21.65 0.699 LlamaGen-B (1d) [50] LlamaGen-B (1d) [50] LlamaGen-B (1d) [50] 111M 4.05 111M 3.83 111M 3.26 LlamaGen-XXL (1d) [50] 1.4B 2.03 111M 3.33 111M 3.15 LlamaGen-XXL (1d) [50] 1.4B 1.98 LlamaGen-B (1d) [50] LlamaGen-B (1d) [50] 62.6 240.61 62.9 233.31 67.6 221.02 69.4 238.52 67.7 265.43 72.0 224.28 74.0 256.76 0.83 0.81 0.83 0.81 0.80 0.80 0.82 0.81 0.45 0.51 0.51 0.56 0.63 0.56 0.55 0. Table 10. Full results for our tokenizers and AR models on ImageNet 256256. For gFID, we present the lowest value between w/ or w/o CFG scenarios. : Using frozen DINO [7] for discriminator, which largely improves rFID. : Without classifier-free-guidance. also observed in the concurrent work FlexTok [3], despite significant differences between GigaTok and FlexTok. We hypothesize that semantic regularization might be the primary contributing factor for this phenomenon. Discussion on Scaling and Enhancing the Discriminator. Recently, VAR [53], ImageFolder [36], and the concurrent work UniTok [41] have begun leveraging DINO-based discriminators [7, 43] to enhance tokenizer training, achieving impressive improvements in rFID scores. We have also experimented with the same DINO discriminator configuration as VAR. Our results indicate that although rFID scores improve, the downstream generation quality improvements are less significant, as detailed in Tab. 10. Furthermore, when applying the DINO discriminator to XL-XXL tokenizers, we observed that adversarial training frequently encounters instability. Specifically, strong discriminator quickly learns to distinguish reconstructed samples, diminishing the benefits of adversarial training and leading to blurry artifacts. We leave further exploration of discriminator scaling and enhancement strategies for future work. E. Training Tokenizers for More Iterations While we largely resolve the reconstruction vs. generation dilemma regarding tokenizer model scaling, this challenge persists for tokenizer training duration scaling. To illustrate this phenomenon, we train five S-S tokenizers ranging from 40 to 120 epochs using cosine learning rate scheduler, as detailed in Tab. 9. The results are presented in Fig. 12. When extending tokenizer training iterations, reconstruction quality consistently improves. However, downstream generation quality initially improves but subsequently degrades with further increases in tokenizer training duration. Additionally, the validation loss of AR probing continuously rises with longer tokenizer training, regardless of semantic regularization. This trend suggests an increasing complexity in the tokenizers latent space as the training duration extends. We hypothesize that data scaling may alleviate this issue, and leave it for future exploration. In practice, allocating computational resources toward model scaling rather than extended training duration may yield better tokenizer performance. F. Linear Probing Accuracy of Tokenizers We show that the linear probing accuracy of the tokenizer encoders may not necessarily indicate the performance of downstream AR models. We utilize the intermediate checkpoints during the training of B-L and XL-XXL tokenizers for evaluation. As shown in Fig. 13, the XL-XXL tokenizer encoder presents an overfitting trend in terms of tokenizer encoder linear probing accuracy. However, this overfitting trend is not reflected in AR Probing linear probing accuracy or gFID. Therefore, the linear probing accuracy of the tokenizer encoders may not be good indicator of downstream model performance. Similarly, concurrent work UniTok [41], also points out that the performance of the tokenizer encoder in terms of zero-shot ImageNet classification accuracy may not necessarily reflect the visual understanding ability of downstream LLMs trained on the tokenizer. The abnormality for large tokenizers reveals that the linear probing accuracy of the tokenizer is not necessarily good indicator for downstream generation models. Since we care more about the representation learning for downstream models than for the tokenizers, using AR Probing as direct evaluating method is better than indirect tokenizer linear probing accuracy. G. More Discussions About Related Work TiTok [69] explores the use of 1D Transformer-based tokenizers under high compression rate setting. TiTok seminally explores the model scaling of visual tokenizers and uses larger tokenizers for higher compression rate. However, the reconstruction vs. generation dilemma for scaling tokenizers is not solved in TiTok. As result, the best generation model in TiTok is still trained on its smallest tokenizer variant. 3 Figure 12. Training duration scaling trends of tokenizers for reconstruction, downstream generation and representation quality with and without semantic regularization. Note that in the last two figures, the red and blue curves correspond to different scales on the y-axis. ing to handle high-level semantic information and low-level visual details respectively. It reveals the importance of the learned representation of tokenizers. Figure 13. The linear probing accuracy of tokenizer encoders does not necessarily reflect downstream model performance. As the training proceeds, the XL-XXL tokenizer encoder presents an overfitting trend measured by linear probing accuracy, but downstream model performances consistently improve. ViTok [21] is concurrent work which has explored the effect of model scaling for VAE [28]. ViTok evaluates its VAE models in terms of both reconstruction and downstream diffusion generation performance. While having very different setting from our GigaTok, ViTok similarly finds that asymmetric design is better for VAEs. While ViTok suggests that small encoders are optimal, we point out that in our setting scaling encoders is also beneficial. Notably, the reconstruction vs. generation dilemma for scaling visual tokenizers is not solved in ViTok. We hypothesize that adding semantic regularization may similarly help solve the tokenizer scaling dilemma for VAEs, but leave it for future study. MAGVIT-v2 [68] introduces LFQ to enhance discrete tokenizers. It also introduces the entropy penalty for tokenizer training, which is shown to be important for training largeInstead of tokenizer model scale tokenizers in our work. scaling, MAGVIT-v2 focuses more on scaling the codebook size of tokenizers. While codebook dimension and codebook size are important bottlenecks for visual tokenizers, we point out that model size scaling is also an important aspect. ImageFolder [36] utilizes two branches for image encod-"
        }
    ],
    "affiliations": [
        "ByteDance",
        "The University of Hong Kong"
    ]
}