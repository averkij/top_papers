{
    "paper_title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment",
    "authors": [
        "Bryan Sangwoo Kim",
        "Jeongsol Kim",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: https://bryanswkim.github.io/chain-of-zoom/ ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 0 0 6 8 1 . 5 0 5 2 : r Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment Bryan Sangwoo Kim Jeongsol Kim Jong Chul Ye KAIST AI {bryanswkim, jeongsol, jong.ye}@kaist.ac.kr Figure 1: Extreme super-resolution of photorealistic images by CoZ with up to 64 magnification (top) and 256 magnification (bottom). Fine details such as textures on wall, wrinkles on flag, and leaf veins are clearly seen."
        },
        {
            "title": "Abstract",
            "content": "Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with critic VLM, aligning text guidance towards human preference. Experiments show that standard 4 diffusion SR model wrapped in CoZ attains beyond 256 enlargement with high perceptual quality and fidelity. Project Page: https://bryanswkim.github.io/chain-of-zoom/. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "The field of generative modeling has witnessed remarkable progress, enabling the synthesis of highly realistic data across various modalities, including images, text, and audio. key application benefiting from these advancements is single-image super-resolution (SISR), which aims to reconstruct highresolution (HR) details from low-resolution (LR) input image. Super-resolution is problem of core interest for effectively bridging the gap between low-cost imaging sensors and high-fidelity visual information; its usages range from enhancing consumer photographs and legacy media to improving critical details in medical imaging, satellite surveillance, and scientific visualization [2, 28, 30, 40, 43]. The standard approach to SISR is based on the posterior probability distribution: p(xH xL) (1) where the goal is to sample plausible HR image xH for given input LR image xL. However, the mapping from xL to xH is highly complex and fundamentally ill-posed: single LR image can correspond to multitude of plausible HR images. This makes directly modeling the distribution extremely challenging for large magnification factors, and early attempts relying on interpolation or regression often produced blurry results [10, 13, 19, 50]. Recent emergence of powerful generative models (e.g., diffusion-based models) has led to significant advancement in this task, providing strong generative priors over natural images that enable the synthesis of realistic textures and details consistent with the low-resolution input. Specifically, existing methods leveraging such generative priors largely fall into two categories. One line of work frames SR as an inverse problem, utilizing pre-trained generative model as prior during inference time to find realistic HR image consistent with the LR input [58, 20, 21]. While such inverse problem-solving methods benefit from being training-free, they typically require lengthy iterative optimization or sampling processes at inference time to enforce data consistency (i.e., ensuring the downsampled HR prediction matches the original LR input), making them computationally expensive. Another line of work aims to incorporate this data consistency directly into the models training objective, thereby enabling much faster inference [31, 42, 47, 48, 54, 55]. Modern state-of-the-art models within this category are capable of producing high-quality super-resolved images, even in single inference step [47, 55]. However, these fast, trained super-resolution models suffer from significant limitation: they are inherently upper-bounded by their training configuration and tend to collapse when presented with inputs requiring magnification beyond what they were trained on [22, 24, 57]. This failure occurs because the models internal representations and learned restoration functions are tightly coupled to the specific scale and degradation seen during training [34]. Applying it outside this domain violates its learned assumptions, leading to severe artifacts, blurry outputs, or complete failure to generate meaningful high-frequency details [11, 13, 19]. This lack of robustness severely restricts the practical applicability of these otherwise powerful models, demanding new models to be trained when the desired magnification factor exceeds what can be currently provided, which is highly inefficient. In this work, we therefore propose to solve fundamental question: How can we effectively utilize super-resolution models to explore much higher resolutions than they were originally trained for? Solving this question is critical in that it addresses the practical need for flexible and arbitrary-scale super-resolution, allowing users to magnify images to desired levels without being constrained by model training specifics. Furthermore, training models for extremely high magnification factors (e.g., 16x, 32x) directly is often computationally prohibitive due to memory and time constraints [45]. Enabling the extension of existing, well-trained models (e.g., 4x SR models) to higher factors offers significantly more resource-efficient pathway to achieving extreme resolutions. To address these fundamental challenges, we present Chain-of-Zoom (CoZ), novel framework for achieving extreme-resolution image generation beyond the training configurations of conventional super-resolution models. Specifically, we introduce intermediate scale-state modeling to bridge the gap between low-resolution (LR) input and high-resolution (HR) target image. These intermediate scale-states enable the decomposition of the conditional distribution in Eq. (1) into series of tractable components, forming the basis of scale-level autoregressive (AR) framework. Within this framework, models can progressively generate high-quality images at resolutions previously considered unattainable. In particular, building on the scale-level AR-2 model, we further propose multi-scale-aware prompt extraction technique. This approach leverages Vision-Language Models (VLMs) to extract descriptive text prompts by attending to multiple scale-states throughout the 2 Figure 2: (a) Conventional SR. When an SR backbone trained for fixed up-scale factor (e.g., 4) is pushed to much larger magnifications beyond its training regime, blur and artifacts are produced. (b) Chain-of-Zoom (ours). Starting from an LR input, pretrained VLM generates descriptive prompt, whichtogether with the imageis fed to the same SR backbone to yield the next HR scale-state. This prompt-and-upscale cycle is repeated, allowing single off-the-shelf model to climb to extreme resolutions (16256) while preserving sharp detail and semantic fidelity. zooming process, enabling semantically aligned and coherent super-resolution. This is from the observation that at extreme resolutions, conditioning provided by the original signal xL becomes insufficient, thus leading to unreasonable hallucinations by the SR model in cases. Furthermore, to obtain text prompts of even richer detail that aligns with human preference, we fine-tune the prompt-extraction VLM under novel RLHF pipeline leveraging GRPO [33]. core part of this pipeline is the utilization of critic VLM to score the outputs of the prompt extraction VLM, thus guiding it to produce prompts more aligned to human preference. Incorporated into the CoZ framework, our final VLM model successfully guides the super-resolution process towards reasonable high-quality results. In summary, our contributions are as follows: We present Chain-of-Zoom, scale-level autoregressive framework that decomposes superresolution into sequence of intermediate scale-states and multi-scale-aware prompts, enabling any existing SR model to reach much higher magnifications without retraining. We propose novel RL pipeline for tuning prompt-extraction VLMs with GRPO. This pipeline incorporates appropriate reward functions and critic reward model to endue multi-scale aware reasoning capabilities to the prompt-extraction VLM."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-Scale Image Generation and Super-Resolution. Unconditional multi-scale generators synthesize ever-larger images by passing coarse outputs through successive refinement stages. Cascaded Diffusion Models [16] pioneer this coarse-to-fine pipeline, while AnyresGAN [3], ScalespaceGAN [46], Generative Powers of Ten [44], ZoomLDM [52], and Make-a-Cheap-Scaling [15] share weights across latent zoom levels to reach megapixel resolutions. Because they are generation-based, these methods do not enforce consistency with given low-resolution input. For true SR, PULSE [26] searches GAN latent space, and Zoomed In, Diffused Out [27] alternates diffusion denoising with explicit up-sampling, but both do not explore extreme resolutions as in this work. Autoregressive Factorizations. Classic autoregressive models such as PixelCNN, PixelRNN [38, 39] and VAR [37] predict spatial tokens sequentially within fixed resolution. Pixel Recursive SR [9] extends this to super-resolution by autoregressing over pixels after each enlargementeffective for small factors but computationally prohibitive at extreme scales. The proposed CoZ instead autoregresses over scale-states: we factorize p(xH xL) into tractable sequence of intermediate zoom distributions, enabling arbitrarily high magnifications without retraining at every factor. 3 Diffusion-Based Super-Resolution. Diffusion models have become the de-facto approach for high-fidelity SISR. SR3 [31] first denoised noisy HR guesses into realistic outputs with diffusion models. StableSR [42] reuses diffusion prior for faster convergence, and prompt-aware variants (e.g., SeeSR [48], SUPIR [54]) add textual conditioning to bolster semantic faithfulness. OSEDiff [47] distills the multi-step chain into one-step denoising. Because of its accuracy and efficiency, we adopt OSEDiff as the backbone SR module in our CoZ demonstrations. However, CoZ is model-agnostic: the same scaling strategy can wrap any existing text-guided diffusion (or non-diffusion) SR network. RL for VisionLanguage Guidance. Reinforcement learning with human feedback (RLHF) is now widely used to align VLM behaviour with user preference. Early vision-grounded efforts such as LLaVA-RLHF [35] and LLaVACritic [49] employ reward models or critic networks to refine image-conditioned dialogue. Generalized Reward Policy Optimization (GRPO) was introduced by Shao et al. [33] as policy-space alternative to PPO [32]. GRPO has since been adopted in vision tasks outside SR: Seg-Zero [25] uses GRPO to train VLMs for open-set semantic segmentation, while MetaSpatial [29] applies it to 3-D spatial reasoning in virtual environments. Building on these precedents, we are the first to bring GRPO to prompt-extraction in super-resolution. Our pipeline fine-tunes prompt-extraction VLM with composite reward objective unexplored in prior SR work."
        },
        {
            "title": "3 Chain-of-Zoom",
            "content": "3.1 Intermediate Scale-State Modeling In the CoZ framework, we propose to bridge the gap between target HR image xH Rdn and an input LR image xL Rd0 by introducing intermediate scale-states xi Rdi. Suppose that an image generative process is modeled as sequence (x0, x1, ..., xn) where x0 := xL, xn := xH , and consecutive states have dimension ratio (i.e. di = sdi1) larger than 1. Under the Markov assumption, the joint distribution could be modeled as p(x0, x1, ..., xn) = p(x0) (cid:81)n i=1 p(xixi1). However, if the model follows Markov chain structure, relying solely on the transition probability p(xixi1) leads to loss of high-frequency details as increases (see Fig. 3). Inspired by recent work in inverse problems [7, 20] that demonstrate the effectiveness of text embeddings in reducing the solution space and improving super-resolution between consecutive scales, we therefore introduce latent variables ci through text embeddings. The text prompt extraction supplements information of the overall zoom process. Important, to reduce hallucinations caused by incorrect text guidance across scale, we find that multi-scale aware text extraction is necessary by feeding xi1 and the coarser state xi2 in prompt generation, leading to the conditional probability for the prompt: pϕ(ci xi1, xi2). (2) Therefore, instead of using Markov assumption, we propose AR-2 modeling of the image generative process with multi-scale-aware prompts as latent variables: p(x0, x1, ..., xn) = p(x0, x1) (cid:89) i=2 p(xixi1, xi2), p(xixi1, xi2) = (cid:90) p(xixi1, xi2, ci)p(cixi1, xi2)dci. (3) (4) Then, the joint distribution of the sequence (x0, c1, x1, ..., cn, xn) is expressed as follows: Proposition 1. Given sequence of scale-states xi that follows AR-2 structure and latent variables ci that satisfy Eq. (2), the joint distribution is expressed as p(x0, c1, x1, ..., cn, xn) = p(x0, x1) (cid:89) i= p(xixi1, xi2, ci)p(cixi1, xi2). (5) Now, our objective function is maximizing the likelihood of the entire joint distribution of xi and ci. Taking the logarithm of Eq. (5), we get the objective function to be maximized: = log p(x0) + (cid:88) log p(xixi1, xi2, ci) + (cid:88) log p(cixi1, xi2) (6) (cid:123)(cid:122) LSR We use parameterized models θ and ϕ to approximate the second and third terms in Eq. (6). (cid:123)(cid:122) LVLM (cid:125) (cid:125) i=1 (cid:124) i=1 (cid:124) 4 Figure 3: Significance of proposed multi-scale-aware prompts: (a) Null prompt: coarse structure is retained, but high-frequency details are smoothed out. (b) DAPE prompt: inserting text from degradation-aware prompt extractor (DAPE) helps, yet the images lack intricate detail at large magnifications. (c) VLM-generated prompts (ours): multi-scale prompts extracted by VLM steer the SR backbone to synthesize realistic textures and crisp details. 3.2 Training Objective The additive status of the components in Eq. (6) allows for the independent optimization of each parameterized model θ and ϕ. We perform this via next xi prediction and next ci prediction, respectively. Next xi prediction. The training objective LSR represents the likelihood of xi given previous scale-states xi1, xi2 and description ci for xi. Under the assumption that the distribution p(xixi1, xi2, ci) := (xi; fθ(xi1, xi2, ci), σ2I) is Gaussian, where the parameterized model fθ predicts the conditional mean of the distribution, the likelihood of xi is equivalent to 1 2σ2 xi fθ(xi1, xi2, ci)2 + log p(xixi1, xi2, ci) = (7) where = di 2 log(2πσ2). To reduce the computational complexity of training fθ, our key idea is that its dependency to xi2 is only through the multi-scale-aware prompt, i.e. ci = ci(xi1, xi2), leading to fθ(xi1, xi2, ci) = fθ(xi1, ci(xi1, xi2)). Maximizing the simplified likelihood thus reduces to minimizing the mean-squared error (MSE) between the predicted HR patch from xi1 and the ground truthprecisely the loss most SR backbones are already trained with. In this work, we perform experiments with backbone SR model trained via settings in Sec. 4.1, yet our framework is model-agnostic. Next ci prediction. Recall that the dependency to the xi2 in AR-2 model is through the multiscale aware prompt extraction, which supplements information of the overall zoom process and reduces hallucinations caused by incorrect text guidance. For single zoom step i, the prompt ci = (ci,1, , ci,Ti) is token sequence conditioned on the current and previous image, i.e. xi1, xi2. Modern VLMs model this distribution autoregressively: pϕ(ci xi1, xi2) = Ti(cid:89) t=1 pϕ(ci,t xi1, , xi2, ci,<t) (8) where ci,<t = (ci,1, , ci,t1). Maximizing the log-likelihood log pϕ(ci xi1, xi2) therefore amounts to minimizing the negative log-likelihood (cross-entropy) for each token: L(i) VLM = log pϕ(ci xi1, xi2) = Ti(cid:88) t=1 log pϕ(ci,t xi1, xi2, ci,<t). (9) 5 Figure 4: GRPO Training Framework. At every zoom step, multi-scale image crops are fed to the base VLM, which generates candidate prompts after perceiving input images. critic VLM scores the prompt for semantic quality, while phrase-exclusion and repetition penalties enforce conciseness and relevance. The weighted sum of these rewards forms the GRPO signal that iteratively fine-tunes the base VLM, steering it towards prompts that best guide extreme-scale super-resolution. Eq. (9) is exactly the standard next-token cross-entropy loss used to pre-train modern VLMs; hence our framework can employ any off-the-shelf VLM whose weights already maximize this objective. Inference. Given pre-trained parameterized models θ and ϕ, the sequence (x0, c1, x1, ..., cn, xn) can be generated recursively. Starting from the low-resolution image xL = x0, description for the next scale, c1 pϕ(c1 x0), is first sampled. Then, the next scale state is generated by sampling x1 pθ(x1 x0, c1). For subsequent steps, the description at scale is sampled as ci pϕ(ci xi1, xi2), followed by sampling the image at that scale as xi pθ(xi xi1, xi2, ci). This sequential sampling process generates specific, plausible high-resolution outputs xn without needing to model the full marginal distribution p(x0, ..., xn) explicitly. When using SR backbone models that require input and output dimensions to be identical (e.g., Stable-diffusion-based SR models [42, 47, 48, 54]), fixed-size window is cropped from the HR image and resized to the required dimension. Thus, super-resolution operates in local regions, and achieving outputs of entire images would require multiple runs of CoZ. 3.3 Training Multi-Scale-Aware Prompt Extraction using RL At extreme magnification factors, the visual evidence in the input image becomes extremely sparse, causing the SR backbone model to rely more heavily on text prompts. To curb the ensuing drift towards implausible high-frequency hallucinations, we fine-tune the prompt-extraction VLM so that its textual guidance aligns with human aesthetic and semantic preferences. Our fine-tuning pipeline  (Fig. 4)  adopts Generalized Reward Policy Optimization (GRPO). For each zoom step i, the VLM receives multi-scale image crops (xi2, xi1) and produces candidate prompt ci. The prompt is scored by set of task-specific reward functions, and the weighted sum R(ci) drives the GRPO update to align the VLM prompts with human preference. The overall reward R(ci) is weighted sum of three components, each targeting distinct failure mode observed during preliminary experiments: R(ci) = wcritic Rcritic + wphrase Rphrase + wrep Rrep (10) Critic Preference Reward (Rcritic). stronger visionlanguage critic VLM judges the candidate prompt in the context of the input multi-scale image crops and assigns raw score in [0, 100]. We linearly rescale this score to [0, 1] and treat it as proxy for human preference, thereby imbuing the prompt-extraction VLM with the critic VLMs higher-level semantic priors. Phrase-Exclusion Reward (Rphrase). Multi-image conditioning occasionally leads the promptextraction VLM to emit viewpoint markers such as first image or second image, which are meaningless to the downstream SR model. We therefore issue reward of 1 if none of predefined blacklist of such phrases appear, and 0 otherwise. Repetition Penalty (Rrep). Following Yeo et al. [53], we compute the fraction of repeated n-grams in the prompt and give negative reward (down to 1) for higher repetition ratio. 6 Figure 5: Qualitative Results. For each input image, super-resolution is performed on different magnifications with various methods: (a) Nearest neighbor interpolation; (b) One-step direct SR with the backbone SR model; (c-e) Variants of CoZ with different text prompts. The CoZ framework shows significantly better performance at large magnifications. Furthermore, with preference alignment with GRPO, our CoZ leveraging VLM prompts assists the SR model in generating realistic details without hallucinations."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings We adopt the setup of prior work [48, 47] and train OSEDiff [47] as the backbone SR model with the LSDIR [23] dataset and 10K images from FFHQ [17]. We use Stable Diffusion 3.0 [12] as the backbone diffusion model and adopt coarse-to-fine training strategy: first training on random degradation, and then training specifically for 4 magnifications. Text guidance is provided by Degradation-Aware Prompt Extractor (DAPE) [48] as the naive prompt extractor, while Qwen2.5-VL3B-Instruct [36] is used as the prompt-extraction VLM. RLHF training with GRPO is performed with InternVL2.5-8B [4] as the critic VLM. The same dataset used for training the backbone SR model is also used for GRPO training, and weights are given as: wcritic = 1.0, wphrase = 0.5, wrep = 0.5. Evaluation is performed on the training datasets of DIV2K [1] and DIV8K [14], consisting of 800 images and 1500 images, respectively. Each image is resized and center-cropped to resolution of 512 512 to be input to the SR model. For four recursions, the HR image of the previous zoom is center-cropped and resized by scale of 4 back to the resolution of 512 512. 4.2 Comparison Results We perform comparison across four recursions for various methods. Specifically, we compare between nearest neighbor interpolation, direct magnification via one-step SR, and three versions of the proposed CoZ leveraging different prompts (i.e., Null, DAPE, VLM). Qualitative Comparison. Qualitative results are depicted in Fig. 5. Nearest neighbor interpolation and one-step direct SR fall off at higher scales, while CoZ variants produce images of better quality. 7 Table 1: Quantitative comparison on no-reference metrics. Bold: best, Underline: second-best. Scale Method NIQE MUSIQ MANIQA CLIPIQA NIQE MUSIQ MANIQA CLIPIQA DIV2K DIV8K 4 16 64 NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) 256 NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) 12.1252 4.7320 4.7706 4.7312 4.6572 22.1215 7.2183 6.5016 6.5456 6.3957 27.4051 16.5915 8.3500 8.6598 8.2335 34.8461 16.1749 10.0456 10.4569 9.8260 39.96 67.00 66.99 67.01 67. 24.01 51.25 59.19 58.83 58.81 37.69 22.54 51.82 51.77 52.13 27.01 28.89 46.28 46.22 47.83 0.3396 0.6344 0.6309 0.6344 0.6360 0.3378 0.5406 0.5859 0.5946 0.5970 0.3803 0.3995 0.5627 0.5726 0. 0.4179 0.4470 0.5510 0.5564 0.5692 0.2630 0.7005 0.6977 0.7004 0.7017 0.2346 0.6080 0.6686 0.6609 0.6574 0.3690 0.4309 0.6305 0.6262 0.6315 0.5259 0.5196 0.5857 0.5889 0.5986 13.1984 4.8631 4.9011 4.8607 4. 22.2744 7.5855 6.7898 6.8607 6.6500 27.7533 16.5874 8.5694 8.7669 8.2992 37.2612 15.8667 10.0630 10.2788 9.6405 40.26 66.29 66.23 66.29 66.37 24.94 50.17 58.04 57.79 57.99 37.13 22.97 50.96 50.40 51. 26.98 28.90 46.56 45.81 47.25 0.3472 0.6359 0.6325 0.6359 0.6370 0.3465 0.5473 0.5881 0.5964 0.6006 0.3861 0.4069 0.5638 0.5714 0.5787 0.4184 0.4464 0.5479 0.5535 0.5646 0.2672 0.6946 0.6897 0.6946 0. 0.2585 0.6035 0.6618 0.6628 0.6615 0.3837 0.4451 0.6240 0.6274 0.6282 0.5299 0.5256 0.5899 0.5984 0.6041 Figure 6: Reward evaluation on validation set shows that values for Critic Reward, Phrase Exclusion Reward, Repetition Penalty, and Total Reward increase throughout the training process. Incorporating VLM prompts helps overcome the sparsity of the original input signal, leading to generation of more realistic images. Quantitative Comparison. Quantitative results are given in Tab. 1. Due to the non-availability of ground-truth images for 256 magnifications, we follow [26] and evaluate performance on no-reference perceptual metrics. Specifically, we use the metrics NIQE [56], MUSIQ [18], MANIQApipal [51], CLIPIQA [41] for thorough evaluation. At low scales (i.e., Scale 4), difference between methods is minimal, but at high scales (i.e., Scales 64, 256) the proposed framework shows consistently better performance. Furthermore, prompts by DAPE show comparable performance at low scales but fall off at higher scales, while VLM-generated prompts exhibit significantly better performance, supporting our claim that prompt-extraction by VLMs make up for the deficient visual conditioning provided by the initial image. 4.3 GRPO for VLM GRPO Training. The reward graphs for training the prompt-extraction VLM are shown in Fig. 6. Phrase exclusion reward and repetition penalty converge to 1.00 and 0.00, respectively, in the early stages of training, while the critic reward increases gradually throughout the training process. Preference Alignment. Using an off-the-shelf VLM for prompt-extraction can cause unwanted hallucinations to occur in the zoom process. An example case is shown in Fig. 7 (Top), where the off-the-shelf VLM generates improper prompts due to insufficient knowledge of the initial image at high magnifications. By inducing the VLM to generate multi-scale-aware prompts by conditioning on (xi1, xi2), we can produce more suitable prompts Fig. 7 (Middle). Finally, using the VLM fine-tuned with GRPO we can produce high-quality samples while reducing unwanted hallucinations as in Fig. 7 (Bottom). We further prove that the VLM after undergoing GRPO training is better 8 Figure 7: RLHF training with GRPO assists the prompt-extraction VLM in creating meaningful prompts for accurate guidance. (Top) Base VLM: generating prompts only from the LR input causes unwanted hallucinations as shown by the incorrect prompts; (Middle) Multi-scale image prompts are helpful at low scales (e.g., accurate prompt of \"dog, stick, water, ...\") but fail at high scales; (Bottom) VLM aligned with human preference guides samples with improved text guidance. aligned with human preference through user study. For this, we follow prior work [26], and perform MOS (mean-opinion-score) test on various samples. Results and details are included in the Appendix."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper tackles the long-standing scalability gap in single-image super-resolution: state-of-the-art models excel at their trained scale factors yet fail when asked to enlarge images far beyond that range. Specifically, we introduced Chain-of-Zoom (CoZ), scale-level autoregressive framework that transforms any existing SR backbone into an extreme-magnification engine by decomposing the LR to HR mapping into sequence of intermediate scale-states and multi-scale-aware prompts. CoZ is model-agnostic, requires no retraining of the base network, and thus offers cost-effective path up to extreme resolutions. In particular, to maintain semantic coherence as visual evidence thins out, we leverage multi-scale-aware prompt extractor driven by VLM fine-tuned through GRPObased RLHF pipeline. Overall, CoZ yields sharp, realistic results at extreme scales while keeping inference efficient. By decoupling super-resolution performance from fixed training magnifications and demonstrating the value of aligned textual guidance, our work opens new avenues for resourcefrugal image enhancement and lays foundation for future exploration of learned zoom policies, domain-specific reward functions, and adaptive backbone selection. Limitation and Potential Negative Impacts. While CoZ enables extreme super-resolution with high visual fidelity, it requires repeated application for extreme magnification, which may cause error accumulation over iterations. Moreover, high-fidelity generation from low-resolution inputs may raise concern regarding misinformation or unauthorized reconstruction of sensitive visual data."
        },
        {
            "title": "References",
            "content": "[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 126135, 2017. [2] Eric Betzig, George Patterson, Rachid Sougrat, Wolf Lindwasser, Scott Olenych, Juan Bonifacino, Michael Davidson, Jennifer Lippincott-Schwartz, and Harald Hess. Imaging intracellular fluorescent proteins at nanometer resolution. science, 313(5793):16421645, 2006. [3] Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, and Richard Zhang. Any-resolution training for high-resolution image synthesis. In European conference on computer vision, pages 170188. Springer, 2022. [4] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [5] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=nJJjv0JDJju. [6] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=OnD9zGAGT0k. [7] Hyungjin Chung, Jong Chul Ye, Peyman Milanfar, and Mauricio Delbracio. Prompt-tuning latent diffusion models for inverse problems. arXiv preprint arXiv:2310.01110, 2023. [8] Hyungjin Chung, Suhyeon Lee, and Jong Chul Ye. Decomposed diffusion sampler for accelerating large-scale inverse problems. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=DsEhqQtfAG. [9] Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In Proceedings of the IEEE international conference on computer vision, pages 54395448, 2017. [10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning deep convolutional network for image super-resolution. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13, pages 184199. Springer, 2014. [11] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295307, 2015. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [13] William Freeman, Thouis Jones, and Egon Pasztor. Example-based super-resolution. IEEE Computer graphics and Applications, 22(2):5665, 2002. [14] Shuhang Gu, Andreas Lugmayr, Martin Danelljan, Manuel Fritsche, Julien Lamour, and Radu Timofte. Div8k: Diverse 8k resolution image dataset. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 35123516. IEEE, 2019. [15] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. In European Conference on Computer Vision, pages 3955. Springer, 2024. [16] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23 (47):133, 2022. [17] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44014410, 2019. 10 [18] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. [19] Robert Keys. Cubic convolution interpolation for digital image processing. IEEE transactions on acoustics, speech, and signal processing, 29(6):11531160, 2003. [20] Jeongsol Kim, Geon Yeong Park, Hyungjin Chung, and Jong Chul Ye. Regularization by texts for latent diffusion inverse solvers. arXiv preprint arXiv:2311.15658, 2023. [21] Jeongsol Kim, Bryan Sangwoo Kim, and Jong Chul Ye. Flowdps: Flow-driven posterior sampling for inverse problems. arXiv preprint arXiv:2503.08136, 2025. [22] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image superresolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46814690, 2017. [23] Yawei Li, Kai Zhang, Jingyun Liang, Jiezhang Cao, Ce Liu, Rui Gong, Yulun Zhang, Hao Tang, Yun Liu, Denis Demandolx, et al. Lsdir: large scale dataset for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17751787, 2023. [24] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 136144, 2017. [25] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoningchain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. [26] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised In Proceedings of the ieee/cvf photo upsampling via latent space exploration of generative models. conference on computer vision and pattern recognition, pages 24372445, 2020. [27] Brian Moser, Stanislav Frolov, Tobias Nauen, Federico Raue, and Andreas Dengel. Zoomed in, diffused out: Towards local degradation-aware multi-diffusion for extreme image super-resolution. arXiv preprint arXiv:2411.12072, 2024. [28] Ozan Oktay, Wenjia Bai, Matthew Lee, Ricardo Guerrero, Konstantinos Kamnitsas, Jose Caballero, Antonio de Marvao, Stuart Cook, Declan ORegan, and Daniel Rueckert. Multi-input cardiac image In Medical Image Computing and Computersuper-resolution using convolutional neural networks. Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part III 19, pages 246254. Springer, 2016. [29] Zhenyu Pan and Han Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse. arXiv preprint arXiv:2503.18470, 2025. [30] Saiprasad Ravishankar and Yoram Bresler. MR image reconstruction from highly undersampled k-space data by dictionary learning. IEEE transactions on medical imaging, 30(5):10281041, 2010. [31] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. arXiv preprint arXiv:2104.07636, 2021. [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [34] Assaf Shocher, Nadav Cohen, and Michal Irani. zero-shot super-resolution using deep internal learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 31183126, 2018. [35] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [36] Qwen Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen2.5-vl/. 11 [37] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [38] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. [39] Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. [40] Lena Wagner, Lukas Liebel, and Marco Körner. Deep residual learning for single-image super-resolution of multi-spectral satellite imagery. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 4:189196, 2019. [41] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 25552563, 2023. [42] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision, 132(12): 59295949, 2024. [43] Peijuan Wang, Bulent Bayram, and Elif Sertel. comprehensive review on deep learning based remote sensing image super-resolution methods. Earth-Science Reviews, 232:104110, 2022. [44] Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steven Seitz, Ira Kemelmacher-Shlizerman, Ben In Mildenhall, Pratul Srinivasan, Dor Verbin, and Aleksander Holynski. Generative powers of ten. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71737182, 2024. [45] Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep learning for image super-resolution: survey. IEEE transactions on pattern analysis and machine intelligence, 43(10):33653387, 2020. [46] Krzysztof Wolski, Adarsh Djeacoumar, Alireza Javanmardi, Hans-Peter Seidel, Christian Theobalt, Guillaume Cordonnier, Karol Myszkowski, George Drettakis, Xingang Pan, and Thomas Leimkühler. Learning images across scales using adversarial training. ACM Transactions on Graphics, 43(4):131, 2024. [47] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. Advances in Neural Information Processing Systems, 37:9252992553, 2024. [48] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semantics-aware real-world image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2545625467, 2024. [49] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. [50] Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE transactions on image processing, 19(11):28612873, 2010. [51] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11911200, 2022. [52] Srikar Yellapragada, Alexandros Graikos, Kostas Triaridis, Prateek Prasanna, Rajarsi Gupta, Joel Saltz, and Dimitris Samaras. Zoomldm: Latent diffusion model for multi-scale image generation. arXiv preprint arXiv:2411.16969, 2024. [53] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-ofthought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. [54] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong. Scaling up to excellence: Practicing model scaling for photo-realistic image restoration in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2566925680, 2024. 12 [55] Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, and Xiaochun Cao. Degradation-guided one-step image super-resolution with diffusion priors. arXiv preprint arXiv:2409.17058, 2024. [56] Lin Zhang, Lei Zhang, and Alan Bovik. feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8):25792591, 2015. [57] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European conference on computer vision (ECCV), pages 286301, 2018."
        },
        {
            "title": "A Proofs",
            "content": "Proposition 1. Given sequence of scale-states xi that follows AR-2 structure and latent variables ci that satisfy Eq. (2), the joint distribution is expressed as p(x0, c1, x1, ..., cn, xn) = p(x0, x1) (cid:89) i=2 p(xixi1, xi2, ci)p(cixi1, xi2). (5) Proof. By substituting Eq. (4) to Eq. (3), we get (cid:89) (cid:20)(cid:90) p(x0, x1) p(xixi1, xi2, ci)p(cixi1, xi2)dci (cid:21) (cid:35) i=1 (cid:90) (cid:34) p(x0, x1) (cid:89) i= (cid:90) (cid:90) = = (cid:90) p(x0, c1, x1, ..., cn, xn)dc1 dcn p(xixi1, xi2, ci)p(cixi1, xi2) dc1 dcn = p(x0, ..., xn) where the first equality comes from Fubinis theorem."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Model Checkpoints We use the pretrained VLM models Qwen2.5-VL-3B-Instruct and InternVL2.5-8B, available at https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct and https://huggingface. co/OpenGVLab/InternVL2_5-8B, respectively. We also use the pretrained Stable Diffusion 3.0 model available at https://huggingface.co/stabilityai/stable-diffusion-3-medium. Evaluation is performed using the script for testing IQA (Image Quality Assessment) in https: //github.com/cswry/OSEDiff. B.2 User Prompts The user prompt used for the base VLM is as follows: The second image is zoom-in of the first image. Based on this knowledge, what is in the second image? Give me set of words. The user prompt used for the critic VLM is as follows: First Image: <image> Second Image: <image> The second image is zoom-in of the first image. Please rate the quality of the following description on how well it describes the second image. Output only single score between 0 and 100. Description: <Output of Base VLM> Rating (0-100): B.3 Other Settings The backbone SR model is trained based on the training scheme of OSEDiff [47], with Stable Diffusion 3.0 as the backbone diffusion model. We train using four NVIDIA GeForce RTX 3090 GPUs with the LSDIR [23] dataset and 10K images from FFHQ [17]. Coarse-to-fine training is used: random degradation (same setting as OSEDiff) for 25K iterations, then 4 specific upscaling for 20K iterations. Other settings (e.g., batch size, learning rate, etc.) follow the default settings of OSEDiff. 14 The VLM model is GRPO fine-tuned using four NVIDIA GeForce RTX 3090 GPUs with the LSDIR dataset, with train/validation split ratio of 0.01 (i.e., 849 images for validation). Specifically, the Qwen2.5-VL-3B-Instruct model is LoRA fine-tuned (Rank: 8, Alpha: 32, Dropout: 0.05), with two generations per prompt for 10K global steps. Reward graphs during training for the validation set are given in Fig. 6 of the main paper. Evaluation is performed with the code provided in [47], modified for no-reference metric evaluation. For occasional failure cases, worst values are given for each metric (100.0 for NIQE, 0.0 for others)."
        },
        {
            "title": "C Algorithms",
            "content": "The following algorithms are provided: Algorithm 1: the main algorithm for Chain-of-Zoom inference. Algorithm 2: the algorithm for GRPO-based human preference alignment training of VLMs. Algorithm 1 Chain-of-Zoom Inference Input: Low resolution image xL, Super-resolution model pθ, VLM pϕ, Number of recursions Output: High resolution image xn 1: x0 xL 2: for : 1 do if = 1 then 3: 4: 5: 6: 7: 8: 9: end for end if xi pθ(xixi1, ci) ci pϕ(cixi1, xi2) ci pϕ(cixi1) else Algorithm 2 GRPO-based RL Training of Prompt-Extraction VLM Input: Base (prompt-extraction) VLM pϕ with parameters ϕ, Critic VLM Vcritic, Phrase blacklist Bphrase for Rphrase, Number of training iterations Niter, Number of generations per prompt Ngen, Training dataset = {(x(j) j=1 of multi-scale image crop pairs k2, x(j) k1)}M Critic VLM scores prompt, range [0, 100] Rescale score to [0, 1] Output: Fine-tuned prompt-extraction VLM pϕ 1: for iteration : 1 Niter do 2: 3: for generation : 1 Ngen do Sample multi-scale image pair (xi2, xi1) from Generate candidate prompt c(g) scritic Vcritic(c(g) xi1, xi2) Rcritic Rescale(scritic, 0, 1) Rphrase 1 for all Bphrase do pϕ(xi1, xi2) if phrase is in c(g) Rphrase 0 break then end if 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for Rrep FractionOfRepeatedNgrams(c(g) R(c(g) ) wcriticRcritic + wphraseRphrase + wrepRrep ) Repetition Penalty, range [1, 0] Total weighted reward end for ˆA(g) R(c(g) Calculate LGRPO(ϕ) with estimated advantages ˆA(g) Update parameters ϕ of pϕ using GRPO policy update with LGRPO(ϕ) n=1 R(c(n) ) 1 Ngen (cid:80)Ngen ) i Group-based advantage estimation Detailed procedure in [33] 18: 19: 20: end for"
        },
        {
            "title": "D User Study",
            "content": "We further prove that GRPO fine-tuning of the VLM enhances human preference alignment by performing MOS (mean-opinion-score) test on various samples for 25 human participants. Specifically, we compare between three different VLM prompts: (i) prompts generated from only the LR input (i.e., pϕ(ci xi1)); (ii) prompts generated from multi-scale image prompts (i.e., pϕ(ci xi1, xi2)); and (iii) prompts generated after GRPO fine-tuning (i.e., pϕ(ci xi1, xi2) with RL-trained ϕ). Example questions are provided in Fig. 8. After being given set of instructions, each user was asked to evaluate five different sets of randomly mixed zoom sequences and five different sets of randomly mixed text generations. Users expressed their preference from Very Bad to Very Good, and the preferences were converted to score of 1 to 5. Resulting preference scores are shown in Fig. 9. We further conduct pair-wise t-test to confirm the statistical significance of the scores. Figure 8: Example questions used for the MOS test. (Left) Human-Preferred Image Generation. Users were first given the instruction: In this survey, several samples will be given where we zoom into the center of the image. For each sequence of zoom, please rate how preferable the zoom is. (i.e., If we zoom into this input image, will the images look like this sequence?) (Right) HumanPreferred Text Generation. Users were first given the instruction: In this section, several samples will be given where we try to explain the center of the image. For each image, please rate how preferable the explanation is. (i.e., Does the text explanation well explain what is in the white box?) Figure 9: (a) Mean opinion scores for image generation. (b) Mean opinion scores for text generation. The scores on each bar denote the means and the error bars represent standard deviation. Significance of scores are denoted as, *: < 0.05, **: < 0.01, ***: < 0.001. 16 Additional Results for Performing CoZ with Open-Source OSEDiff We further prove the applicability of our CoZ framework with the open-source OSEDiff [47] model (leveraging Stable Diffusion v2.1 backbone) available at https://github.com/cswry/OSEDiff. Quantitative comparison on DIV2K, DIV8K training datasets are provided in Tab. 2 and example qualitative results are provided in Fig. 10. Results show that CoZ is robust and shows good performance when utilizing OSEDiff that leverages the Stable Diffusion v2.1 model as its backbone. Table 2: Quantitative comparison using the open-source OSEDiff. Best, Second-Best. Scale Method NIQE MUSIQ MANIQA CLIPIQA NIQE MUSIQ MANIQA CLIPIQA DIV2K DIV8K 4 16 64 NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) 256 NN Interpolation Direct SR CoZ (Null) CoZ (DAPE) CoZ (VLM) 12.1252 4.7572 4.7295 4.7577 4.7241 22.1215 6.9951 6.5369 6.5628 6.5254 27.4051 15.6269 8.9369 8.8681 8.8259 34.8461 15.6688 11.0907 11.0014 10.8156 39.96 69.26 69.34 69.26 69. 24.01 51.88 61.86 61.47 62.05 37.69 21.56 54.46 53.50 54.84 27.01 26.37 47.14 45.81 48.22 0.3396 0.6366 0.6359 0.6366 0.6368 0.3378 0.5361 0.5776 0.5799 0.5801 0.3803 0.4255 0.5598 0.5622 0. 0.4179 0.4593 0.5441 0.5440 0.5495 0.2630 0.7266 0.7272 0.7265 0.7279 0.2346 0.6206 0.6988 0.6899 0.6958 0.3690 0.4943 0.6672 0.6553 0.6615 0.5259 0.5203 0.6223 0.6162 0.6257 13.1984 4.8659 4.8174 4.8662 4. 22.2744 7.4394 6.7363 6.7985 6.7348 27.7533 15.8252 8.9645 9.0221 8.8553 37.2612 15.9510 11.0661 10.9251 10.7086 40.26 68.16 68.11 68.16 68.31 24.94 51.65 60.76 60.58 61.11 37.13 22.02 53.48 52.76 53. 26.98 26.17 47.09 46.50 48.25 0.3472 0.6349 0.6332 0.6350 0.6346 0.3465 0.5472 0.5842 0.5888 0.5904 0.3861 0.4316 0.5643 0.5687 0.5716 0.4184 0.4574 0.5439 0.5475 0.5518 0.2672 0.7198 0.7184 0.7199 0. 0.2585 0.6300 0.6919 0.6926 0.6978 0.3837 0.5059 0.6655 0.6616 0.6677 0.5299 0.5231 0.6297 0.6345 0.6384 Figure 10: Qualitative results for performing CoZ with the open-source OSEDiff (leveraging Stable Diffusion v2.1 as the diffusion backbone). The GRPO fine-tuned VLM is used as the prompt extractor."
        },
        {
            "title": "F Additional Qualitative Results",
            "content": "Additional qualitative results of extreme super-resolution by CoZ are provided below. Figure 11: Extreme super-resolution of photorealistic images by CoZ up to 64 magnification. 18 Figure 12: Extreme super-resolution of photorealistic images by CoZ up to 256 magnification. 19 Figure 13: Extreme super-resolution of photorealistic images by CoZ up to 256 magnification. 20 Figure 14: Extreme super-resolution of photorealistic images by CoZ up to 256 magnification. 21 Figure 15: Extreme super-resolution of photorealistic images by CoZ up to 256 magnification. 22 Example Failure Modes of VLM before Fine-Tuning G.1 Repetition The second image is zoom-in of the first image. Based on this knowledge, what is in the second image? Give me set of words."
        },
        {
            "title": "RESPONSES",
            "content": "Qwen2.5-VL-3B-Instruct fur texture orange background animal fur close-up pattern texture orange fur texture orange fur background orange fur texture orange fur background orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur texture orange fur ... + GRPO Training fur G.2 Unwanted Phrases The second image is zoom-in of the first image. Based on this knowledge, what is in the second image? Give me set of words. RESPONSES Qwen2.5-VL-3B-Instruct The second image shows close-up view of surface with textured pattern. The texture appears to be combination of smooth and slightly raised areas, giving it somewhat wavy or ripple-like appearance. The color gradient ranges from lighter shade at the top to darker shade at the bottom, creating sense of depth and dimension. + GRPO Training feathers 23 G.3 Suboptimal Results The second image is zoom-in of the first image. Based on this knowledge, what is in the second image? Give me set of words. RESPONSES Qwen2.5-VL-3B-Instruct ant leg + GRPO Training crab claw"
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}