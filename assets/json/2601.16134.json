{
    "paper_title": "LLM Prompt Evaluation for Educational Applications",
    "authors": [
        "Langdon Holmes",
        "Adam Coscia",
        "Scott Crossley",
        "Joon Suh Choi",
        "Wesley Morris"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications."
        },
        {
            "title": "Start",
            "content": "LLM Prompt Evaluation for Educational Applications Langdon Holmes1,*, Adam Coscia2, Scott Crossley1, Joon Suh Choi1, and Wesley Morris1 1 Vanderbilt University, Nashville, Tennessee 2 Georgia Institute of Technology, Atlanta, Georgia Abstract As large language models (LLMs) become increasingly common in educational applications, there is growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology researchers can systematically evaluate and improve prompt designs, moving be-yond ad-hoc prompt engineering toward evidence-based prompt development for educational applications. Keywords Prompt Engineering, Large Language Models, Intelligent Tutoring Systems, Reading Comprehension 1. Introduction Large language models (LLMs) have become increasingly integral to educational platforms and applications. Even prior to the advent of large generative models such as the GPT (Generative Pretrained Transformers) series [5], LLMs demonstrated success in various educational contexts with applications in automated writing evaluation [2, 25, 30] and textual analysis [35, 46]. These early applications primarily focused on specific, constrained tasks that relied on carefully trained models for particular educational use cases. The advent of flexible and customizable LLMs, like GPTs and similar foundation models, has created an even greater demand for new and improved AI-powered applications in educational technology. Use cases now range from learner-facing chat bots that serve as study aids [3], sources of information on course content [41], and assistants for giving personalized feedback [22], to interactive systems for generating educational materials such as lesson plans and study guides [16]. For example, Taneja et al. developed Jill Watson, ChatGPT-powered chat bot that provides question-answering support in educational spaces such as online forums and classrooms [41]. In related work, Castleman and Turkcan created three intelligent tutors using GPT-4, each with different level of access to knowledge base [6]. Other studies have applied LLMs to assessment and feedback tasks. Mizumoto and Eguchi used ChatGPT to score essays written by second language learners [28]; Lin et al. com-pared multiple GPT models in the task of providing feedback on tutor responses [22]; and Liu et al. assessed GPT 3.5 for evaluating the helpfulness of peer EDM-AIED 2025: Workshop on Epistemics and Decision-Making in AI-Supported Education. Co-located with AIED 2025, July 26, 2025, Palermo, Italy. *Corresponding Author langdon.holmes@vanderbilt.edu (L. Holmes); acoscia6@gatech.edu (A. Coscia); scott.crossley@vanderbilt.edu (S. A. Crossley); choijoonsuh@gmail.com (J. S. Choi); wesley.g.morris@vanderbilt.edu (W. Morris) 0000-0003-4338-4609 (L. Holmes); 0000-0002-0429-9295 (A. Coscia); 0000-0002-5148-0273 (S. A. Crossley); 0000-00027732-0366 (J. S. Choi); 0000-0001-6316-6479 (W. Morris) 2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). review comments [23]. Finally, Singhal et al. used multiple GPT models to deidentify student writing and protect student privacy [38]. The above applications of general purpose LLMs to educational contexts make use of prompt to adapt the LLMs behavior. However, researchers rarely provide adequate information about how prompts were developed and evaluated. Reports vary from brief mention of the objectives that the prompt was designed to accommodate [28] to more detailed description of multiple prompting strategies and how these strategies were meant to address the specific needs of the learning context [6]. Many researchers also provide the full prompt text in their report [22, 38, 41]. How-ever, few if any researchers describe the formal process by which prompt performance was evaluated. The general approach to developing LLM-powered interventions often relies on ad-hoc methods and iterative refinement rather than systematic evaluation. While educational technology studies frequently cite prompt engineering as part of their methodology, they rarely report on specific prompt design choices or their effects on task performance. This makes it difficult to build upon previous work or to determine which prompting strategies are most effective for different educational contexts. This study presents systematic approach to prompt design and evaluation in the context of structured dialogue activity that takes place within an intelligent text webapp. Our approach addresses critical decision-making challenge in AI-supported education: how to optimize prompts for the generation of learning content that is both personalized and pedagogically aligned. The study demonstrates (1) how prompt patterns can be leveraged to create more effective educational interactions with LLMs and (2) how the performance of different prompts can be evaluated in replicable and resource efficient manner. 1.1. Prompt Design Patterns prompt is defined as an input to generative LLM that is used to guide its output [12, 36]. prompt generally comprises set of specific instructions pertaining to the target task, and some contextual information that can help the LLM better under-stand the task. prompt template is customizable, structured blueprint that can be used to generate prompts [37]. Prompt templates contain slots that can be filled in at the time of prompt generation. For example, in the template Generate {{question_type}}, the question type is variable that will be replaced by target string at the time of prompt generation. Prompt templates explicitly define which elements are static across prompts and which should be replaced by situation-specific inputs. Prompt design patterns provide reusable solutions to common problems encountered when engineering prompts for large language models (LLMs). Like software design patterns [10], prompt patterns codify proven approaches that can be adapted to different contexts while solving recurring challenges. White et al. present comprehensive catalog of prompt patterns, organizing them into categories including Input Semantics, Output Customization, Error Identification, Prompt Improvement, and Interaction patterns [43]. Within the White et al. catalog, several prompt patterns are particularly relevant for educational applications: For example, the Persona pattern can establish specific role or character for the LLM, such as tutor or coach, providing consistent framework for generating responses. For example, the prompt You are an expert mathematics tutor who specializes in explaining complex concepts using simple analogies. might be effective in educational contexts for maintaining an appropriate tone and expertise level for the LLM [23]. The Cognitive Verifier pattern structures the LLM's analysis process by breaking down complex questions into component parts before generating response. This pattern aligns with established pedagogical techniques for scaffolding learning [49]. For example, Before explaining the historical event, analyze (1) the key actors involved (2) the underlying societal conditions, and (3) the immediate trigger events, then use these elements to construct your explanation. The Context Manager pattern helps maintain focused educational dialogue by explicitly defining the scope of the conversation and relevant contextual elements. For example, \"When discussing this physics problem, consider only classical mechanics principles. Do not introduce quantum mechanics concepts unless explicitly requested.\" The Alternative Approaches pattern encourages the LLM to consider multiple ways to accomplish given task or solve problem and present these to the user. For example, \"Explain three different methods for solving quadratic equations.\" This pattern could help students to engage in comparative cognitive processes that might engage their critical faculties. These prompt patterns can be combined to create more sophisticated prompt templates. For example, the Persona pattern might be combined with Context Manager to create tutor that both maintains appropriate pedagogical stance and keeps the discussion focused on relevant topics. However, the effectiveness of such combinations in educational contexts requires systematic evaluation. 1.2. Evaluating LLM Outputs With the proliferation of LLMs and prompt engineering for education, principled evaluations of LLM outputs have become critical. Two primary approaches have emerged for assessing LLM performance: metric-based evaluation and comparative judgment methods. Metric-based evaluation offers granular approach to analysis, as demonstrated by Srivastava et al., who operationalized hundreds of dimensions for assessing LLM tasks and outcomes [40]. These dimensions span technical considerations like accuracy and latency, as well as human factors including agreeableness and cohesion. While this approach provides detailed lens for examining LLM outputs, it can struggle with the complexity of holistic assessment. An alternative evaluation strategy is comparative judgment method. The Chatbot Arena [48] exemplifies this approach by presenting users with outputs from different LLMs and asking them to select preferred response. This method constructs leaderboard based on user preferences, optimizing for holistic assessment rather than predefined criteria [19]. Such an approach proves particularly valuable when evaluating complex constructs like LLM output quality, which often resist decomposition into discrete rubric elements. Despite extensive work on LLM evaluation, principled prompt evaluation is under-studied. Conceptually, assessing LLM performance and evaluating prompts are equivalent, differing only in whether the prompt or the LLM is the primary object of manipulation. Researchers have recently called for more comprehensive approaches that simultaneously evaluate both LLMs and prompts, recognizing the sensitivity of LLM performance to prompting strategies [27]. In practice, prompt evaluation occurs most frequently through implicit, iterative prompt refinement [24]. The process often naturally occurs as humans encounter failure cases, such as needing to provide examples as hints to the model to handle edge-cases, form of in-context learning for LLMs [45]. For example, Denny et al. show how natural language prompt engineering is easily adopted by novice programmers to write better prompts for models that generate code, such as Copilot [8]. However, this type of iterative design does not provide the same quality of evidence as rigorous evaluation process. One problem is that humans can struggle to evaluate prompts at scale, often expecting LLMs to behave like humans and overgeneralizing results [47]. The current landscape of prompt evaluation research reveals critical need for more structured, systematic methods of assessment. Despite the sophisticated approaches to LLM output evaluation, prompt evaluation remains largely unexplored. As educational applications using LLMs proliferate, developing robust, scalable methods for prompt evaluation will be crucial to designing effective and personalized learning interventions. 1.3. Current work This study presents systematic, generalizable approach to prompt evaluation that can be adopted across diverse learning contexts. We demonstrate our methodology within specific reading comprehension dialogue system, but our primary contribution is the tournament-style evaluation framework itself. To illustrate this approach, we designed five prompt templates using established prompt patterns and adult learning theory. We then compared them against Baseline template through tournament-style evaluation with eight judges, assessing the quality of followup questions generated by an LLM using each template. RQ. What does tournament-style prompt evaluation process reveal about the performance of different prompting strategies in an educational dialogue? 1.4. Context To demonstrate our approach in real pedagogical context, we evaluate prompts designed for structured dialogue activity. The Strategic Thinking and Interactive Reading Support (STAIRS) dialogue occurs within an intelligent text framework supporting students reading comprehension and foundational reading skills. Intelligent texts created with this framework are interactive web apps that include text content augmented with suite of learning activities. One such activity is the end-of-page summary, which requires students to write summary of what they have read at the end of each page. These summaries are automatically evaluated across multiple dimensions using scoring pipeline that combines multiple natural language processing (NLP) approaches. The summary scoring system evaluates three key criteria: content, relevance, and language borrowing. Content scores assess whether the summary includes key ideas and details from the text. fine-tuned Longformer model [1] evaluates the content dimension, with preliminary assessments [30] indicating that the model explains 79% of the content variance. Relevance is measured through semantic similarity between the source text and summary using text embedding model [all-MiniLM-L6-v2, 33]. Language borrowing (also known as containment [4]) is analyzed using Spacy-based pipeline [15] that examines trigram overlap between the source text and summary [4]. When summary does not pass the content score threshold, STAIRS is activated to support the student's reading. The system first identifies an appropriate section of the text for the student to review. This decision is based on how long each section was visible on the students screen relative to the word count of the section, whether the student had already engaged in an activity related to the section, and the semantic similarity between each section and the students summary. The algorithm is intended to identify section of the text with which the student engaged less deeply. Using Llama 3 [9], STAIRS then generates question for the selected passage based on selfexplanation reading training [SERT, 26]. This framework is designed to support students' epistemic progressionthe gradual development and refinement of their knowledge structures and understanding [11]. Epistemic progression involves not just accumulating information but advancing through qualitatively different ways of knowing and reasoning about content. Through SERT questioning, learners are prompted to articulate connections between textual information and their existing knowledge frameworks, identify relationships between concepts, and engage in metacognitive reflection about their comprehension processes. The question is randomly selected from five possible SERT question types: logic, bridging, prediction, elaboration, or paraphrasing. After the student re-reads the section and responds to the SERT question, STAIRS generates follow-up question to deepen engagement with the text. After the learner responds to this followup question, they are directed to revise their summary. 2. Methods We developed LLM prompts by populating prompt template with set of input data. The input data is sourced from authentic user interactions with STAIRS in the intelligent text framework. This data was sourced from three separate intelligent text deployments, each of which was based on different textbook: 93 interactions are from Prolific crowd workers reading an economics text, 17 are from university students reading psychology text, and 9 are from college students reading programming text. 2.1. Prompt Templates Our study evaluates six prompt templates: one Baseline template that has been used in previous intelligent text deployments and five templates that were designed using the prompt patterns described by White et al. [43]. The templates were crafted to generate follow-up questions after learner's initial response to Self-Explanation Reading Training (SERT) question. Our approach to prompt design was grounded in principles of learning theory, with special emphasis on adult learners. Constructivist learning theory emphasizes that learners actively construct new knowledge by connecting it with their prior experiences [32], which aligns with research showing that adult learners have accumulated diverse experiences that make them more heterogeneous than adolescent learners [14, 39]. Similarly, Knowles' theory of andragogy [20] emphasizes that adult learners benefit from greater autonomy and agency in their learning process [34, 39]. Prompts were particularly influenced by theories of self-directed learning (SDL), which emphasize learner autonomy and helping students to develop better understanding of their learning preferences [7, 17, 39]. The implementation of SDL in the prompts was guided by research on scaffolded learning conversations [44] and Vygotsky's zone of proximal development [42], which emphasizes the role of guided support in helping learners achieve tasks just beyond their current capabilities. This perspective aligns with research on metacognitive strategy development in adult education [50] and suggests that effective educational dialogue should support learner autonomy while providing structured opportunities for reflection and strategy development [26]. As result, we designed the prompt templates in this study to emphasize learning and reading strategy development as well as scaffolding and meta-reflection. Excluding the Baseline prompt template (374 words), all prompt templates were of similar length (min =117 words, max = 192 words). Templates were designed to make use of prefix caching, which is an optimization strategy that can speed up response time by saving intermediate states of the LLM. The SERT question and learner response were different for every request to the LLM, so these elements were included at the end of the prompt using Llamas chat format. Static text and variable inputs that were repeated across multiple requests were included at the beginning of the prompt, which allowed our model serving software [vLLM, 21], to cache these prefixes. All templates used the same set of variable inputs: the title of the textbook, one-paragraph description of the textbook, the full-text of the passage being discussed (between one and three paragraphs), the initial SERT question, and the learners response to the initial SERT question. In total, six prompt templates were tested. The full prompt templates, as well as the prompt https://osf.io/4jus9/? are tournament view_only=1413f65f0adb465a8ac5afbbd3190ebb. annotation instructions, available at The Baseline prompt was adapted from other prompts used within the intelligent text framework. While it was not designed with specific prompt patterns in mind, analysis revealed several key pattern implementations. The Persona pattern was employed through the establishment of \"reading support agent\" role with clearly defined behavioral traits (\"factual and concise\") and explicit limitations (\"You do not complete assignments\"). The Context Manager pattern was implemented through multiple layers of control, including high-level principles based on Bloom's Taxonomy (\"encourage users to engage with the text at different cognitive levels\"), specific interaction guidelines (\"concise responses\", \"factual and clear\"), and explicit content boundaries through the text description and excerpt. The Cognitive Verifier pattern appeared in the prompt's approach to topic verification, instructing the LLM to \"redirect the user to the topic by asking question\" if their message strayed from the specified text. The prompt included repeated emphasis on key constraints (e.g., \"do not provide any form of summaries or overviews\"), which was an attempt to maintain consistent LLM behavior through repetition rather than more sophisticated pattern implementation. The Socratic Guide emphasized self-directed learning and critical thinking through Socratic questioning, aligning with Knowles' principles of adult learning [20]. This template employs the Persona pattern by establishing an \"expert adult education facilitator specializing in Socratic questioning\" role with clearly defined responsibilities. The Context Manager pattern is implemented through explicit scoping of the interaction around Socratic principles, including the directive to ask questions that \"encourage metacognitive reflection\" and to \"help [students] make connections between their prior knowledge and the text.\" The template maintains tight control over the dialogue structure by explicitly defining the sequence: \"You will ask the learner one selfexplanation reading training question, the learner will respond, then you will ask follow-up question.\" The Scaffolding Expert incorporated Vygotsky's Zone of Proximal Development [42], primarily utilizing the Cognitive Verifier pattern through structured three-step analysis process. This process analyzed three aspects: 1). The key concepts in the passage that the learner needs to understand, 2). The level of understanding demonstrated in the learners response, and 3). The potential gaps or misconceptions in the learners understanding. The Context Manager pattern was implemented through explicit scoping statements that direct the LLM to \"target the most critical gap or opportunity for deeper understanding\" and \"use scaffolding principles to build from their current understanding.\" The Connection Builder focused on constructivist learning principles, employed in the Context Manager pattern by explicitly defining the scope of connections the LLM should pursue. It did this by instructing the LLM to \"help the learner connect ideas within the text\" and \"encourage them to draw on their personal experience or prior knowledge.\" The Alternative Approaches pattern was employed through directions to explore different types of connections while maintaining focus on \"deepening their understanding through connecting different concepts or experiences.\" The template provided specific guidelines for handling off-topic responses: \"If the learner response does not address the initial SERT question, then you should ask question that will help steer the learning session back on-topic.\" The Strategic Reader Coach emphasized metacognitive strategies and self-directed learning. It utilized the Persona pattern by establishing \"reading strategy coach\" role with specific coaching objectives. The Context Manager pattern was implemented through explicit focus on strategic reading skills, directing the LLM to generate questions that \"prompt the user to reflect on their reading strategy\" and \"help them identify key relationships in the text.\" The template maintained pedagogical focus through specific directives to \"avoid suggesting specific interpretations\" while encouraging metacognitive engagement. The Comprehension Monitor incorporated self-regulated learning principles. It utilized the Cognitive Verifier pattern through structured analysis of \"the main ideas of the passage,\" \"the specific concepts addressed in the user's response,\" and \"areas where deeper processing might be beneficial.\" The Context Manager pattern was implemented through explicit instructions to \"help the user to evaluate their understanding\" and \"model strategy that the user can use to monitor their comprehension.\" The template provided specific guidance for non-responsive answers: \"If the user response does not address the initial SERT question, you may ask the user to try again.\" 2.2. Prompt Tournament The prompt tournament presented judges with pairs of follow-up questions to initial STAIRS responses and asked judges to choose their preferred output. This is an example of comparative judgment task, which asks judges to make preference decision between multiple options rather than assigning an absolute score to single sample at time [19]. Each follow-up question was generated using one of the six prompt templates described above. The tournament used the Glicko2 rating system [13] to rank prompts in terms of their probability of producing follow-up questions that would be preferred over those generated by other prompts. In addition to the overall prompt ranking, the Glicko algorithm also produces Bradley-Terry win probability for all pairwise comparisons between prompts, which is the estimated probability that one prompts output will be preferred over another. The tournament was conducted using Prodigy, commercial annotation software developed by Explosion AI [29]. The annotator interface included annotation instructions, the conversation dialogue from real user data, and two follow-up questions generated using different prompts. Additional metadata of the user interaction was also provided. Annotators were asked to select which follow-up question they prefer using radio buttons. skip button was available to annotators if they felt unable to make preference assessment. This happened, for example, if the two follow-up questions were equivalent, or if both follow-up questions were unacceptable. Eight judges participated in the tournament, contributing between 13-54 decisions (mean=30.42, SD=14.90) each. Judges were asked to complete 30 decisions over the course of two weeks. One judge was faculty member, four judges were doctoral students, and three judges were undergraduate students. All were members of the intelligent text development team and were therefore familiar with both the specific learning context and educational technology more broadly. In total, judges made 213 preference decisions. Rubric. To evaluate the quality of generated follow-up questions, we developed three-part rubric focusing on format, dialogue support, and appropriateness for adult learners. The rubric was developed through an iterative process involving the intelligent text development team, with the goal of identifying questions that would effectively support learner engagement and comprehension. The format criteria emphasized that responses should be presented as direct questions, allowing only brief supportive statements when contextually appropriate. This requirement ensures consistency in the interaction style and maintains focus on the dialogic nature of the reading support. Responses that include unnecessary preface text or procedural explanations (e.g., Here is follow-up question:) were considered less effective. The dialogue support criteria assessed how well the follow-up question built on both the initial SERT question and the learner's response. High-quality responses encourage discussion and deeper thinking about the text. Importantly, the rubric includes specific guidance for handling low-effort learner responses, preferring approaches that acknowledge lack of engagement and attempt to rebuild interest rather than simply repeating the initial question. The appropriateness criteria evaluate whether responses treat learners with appropriate respect and sophistication. These criteria favor questions that encourage connections between the text and learners' prior experiences, as well as questions that promote metacognitive reflection on reading strategies. This aligns with learning theories that emphasize self-directed learning and the importance of connecting new knowledge with existing experience during learning. Judges were trained on the rubric using example pairs of responses that highlighted key distinctions in question quality. When evaluating response pairs, judges were instructed to consider all three rubric components while selecting the more effective response. This holistic approach, rather than numerical scoring system, was chosen to better capture the nuanced differences in response quality that emerge from the interaction of multiple criteria. Rating System. As mentioned earlier, the tournament employed the Glicko2 rating system [13], which provides an approach to ranking competitors based on paired comparisons. The system maintains three parameters for each competitor: rating value (μ), rating deviation (RD) that indicates the reliability of the rating, and volatility score (σ) that measures the degree of expected fluctuation in the rating. Our prompt tournament was designed to maximize information gain by prioritizing comparisons between the most successful prompts at each time step. After each round of comparisons, the system identifies the two prompts with the highest ratings and schedules them for additional head-to-head evaluation. This adaptive sampling strategy helps to efficiently identify the strongest prompt by focusing evaluation effort on discriminating between the most promising candidates. Additionally, the system accounts for rating volatility through the RD parameter, allowing it to make more informed rating adjustments when judges show consistent agreement versus when their evaluations display more variability. This approach provides framework for identifying the most effective prompt while making efficient use of annotator labor and accounting for uncertainties in both prompt performance and judge reliability. 3. Results The prompt tournament results revealed clear differences in the effectiveness of different prompt templates for generating follow-up questions. We found that certain combinations of prompt patterns, particularly those emphasizing persona and context management, outperform others for generating follow-up questions. Our results further demonstrated that prompts grounded in learning theory produced more effective follow-up questions when combined with appropriate prompt engineering patterns. The Strategic Reading Coach template emerged as the strongest performer, with consistently high win probability estimates against all other templates. It demonstrated an estimated win probability of 81% against the second-best performing prompt, the Scaffolding Expert, and win probabilities of at least 90% compared to all other prompts. The estimated win probabilities and number of trials are reported in Table 1. The number of trials varies between matchups, from 0 to 91, because we designed the tournament as an adaptive comparative judgment task that prioritizes collecting information about the two most effective prompts at each time step. This was done to make the most effective use of annotator labor. Table 1 Results of Prompt Tournament Prompt Prompt Prob > Trials SRC SRC SRC SRC SRC SE SE SE SE Baseline Baseline Baseline CM CM SE Baseline CM SG CB Baseline CM SG CB CM SG CB SG CB 0.81 1.00 1.00 0.98 0.94 1. 1.00 0.96 0.91 1.00 0.85 0. 0.55 0.56 55 14 1 3 6 0 9 11 23 2 0 0 Note: SRC = Strategic Reading Coach, SE = Scaffolding Expert, CM = Comprehension Monitor, SG = Socratic Guide, CB = Connection Builder The Scaffolding Expert template emerged as the second-best performer overall, also showing strong performance across its matchups. Against the Baseline template, it achieved 100% win probability. The Scaffolding Expert template also performed strongly against the Socratic Guide and Connection Builder, with win probabilities of 96% and 91% respectively. The large margins of victory between templates suggest that the design differences had substantial impact on output quality. The Baseline template, despite not being explicitly designed with prompt patterns, performed relatively well, placing third overall. During the early phases of the tournament, the Baseline template was outperforming other prompts, which explains the high number of trials for the comparison between the Baseline and Comprehension Monitor templates. This observation indicates that some of the Baseline templates existing features, such as the emphasis on Bloom's Taxonomy and metacognition, align well with the pedagogical aims of the follow-up question. The results suggest that certain prompt patterns were particularly effective when combined. The Strategic Reading Coach's combination of the Persona Pattern and Context Manager Pattern produced consistently strong results, suggesting these patterns work well together for generating follow-up questions. Similarly, the Scaffolding Expert's use of the Cognitive Verifier Pattern and Reflection Pattern proved effective, indicating the value of structured analysis before question generation. In contrast, templates using the Context Manager Pattern (Comprehension Monitor) or Alternative Approaches Pattern (Connection Builder) performed less well, suggesting these patterns were less suited for generating follow-up questions in this specific context. 4. Discussion The goal of the research reported in this study is to showcase methods to optimize prompts for specific pedagogical objectives. As LLMs become increasingly integrated into educational technologies, structured approaches to prompt engineering and evaluation will be essential for creating effective, theoretically grounded learning experiences. The results of our prompt tournament provided clear and quantitative evidence about the most effective prompt template in specific educational application. Importantly, the prompt tournament methodology used in this study is highly adaptable to other educational contexts. Additionally, the approach helped us glean insights into the effectiveness of different prompt engineering approaches for educational dialogue. The Strategic Reading Coach (SRC) template emerged as the clear winner, demonstrating superior performance across all pairwise comparisons with win probabilities ranging from 81% to 100%. This strong performance can be attributed to several key factors revealed through our analysis. The SRC template's combination of the Persona and Context Manager patterns proved particularly effective for generating follow-up questions that maintain pedagogical focus while adapting to learner responses. The success of this combination suggests that establishing consistent tutoring persona while simultaneously refining questions based on learner input creates more effective dialogue structure than approaches that rely on single patterns. An unexpected finding was the relatively strong performance of the Baseline template, which placed third overall despite not being explicitly designed with modern prompt patterns. This suggests that some fundamental principles of educational dialogue, such as promoting metacognition and following Bloom's Taxonomy, remain effective regardless of how they are implemented in prompt engineering. However, the substantial performance gap between the Baseline and the top two templates indicates that thoughtful application of prompt patterns can significantly improve output quality. The tournament results also highlighted areas where several prompt engineering approaches fell short. The poor performance of the Connection Builder template, despite its grounding in constructivist learning theory, suggests that theoretical alignment alone does not guarantee effective prompt performance. This finding highlights the importance of empirical evaluation in prompt engineering. Our findings demonstrate the value of systematic prompt evaluation in educational applications, moving beyond the common practice of ad-hoc prompt engineering. The tournament methodology proved effective for identifying meaningful differences between prompt templates. The current study, however, is not without limitations. First, our evaluation focused solely on follow-up questions within specific reading comprehension context. While the tournament methodology can be adapted for other educational applications, the specific findings about prompt effectiveness might not generalize beyond this context. Second, our analysis used single LLM (Llama 3) due to deployment constraints. Different models might respond differently to the same prompt patterns, suggesting need for cross-model evaluation in future work. One advantage of the prompt tournament design is that different combinations of prompt and LLM can be evaluated simultaneously. This could prove important when deciding which LLM to use in given application, as the same prompt might perform better or worse when used with different LLM [27]. 5. Future Work This work represents an initial step toward more systematic design and evaluation of prompts for educational applications of large language models. While our tournament methodology focuses on comparing prompts for generating follow-up questions in reading comprehension, the approach could be adapted for evaluating prompts across range of educational tasks and contexts. Future research should explore how different combinations of prompt patterns and LLMs perform across various pedagogical goals. Additionally, future work might explore methodological improvements to the prompt design process. For example, interactive interfaces like PromptMaker [18] and ConstitutionMaker [31] have been developed to create structured prompt design process. The artifacts produced by this process could help to make prompt engineering practices more describable and reproducible in educational technology research."
        },
        {
            "title": "Acknowledgements",
            "content": "This material is based upon work supported by the National Science Foundation under Grant 2112532 Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "title": "Declaration on Generative AI",
            "content": "The author(s) have not employed any Generative AI tools."
        },
        {
            "title": "References",
            "content": "[1] Beltagy, I., Peters, M.E. and Cohan, A. 2020. Longformer: the long-document transformer. (2020). DOI:https://doi.org/10.48550/ARXIV.2004.05150. [2] Beseiso, M. and Alzahrani, S. 2020. An Empirical Analysis of BERT Embedding for Automated Essay Scoring. International Journal of Advanced Computer Science and Applications. 11, 10 (2020). DOI:https://doi.org/10.14569/IJACSA.2020.0111027. [3] Borchers, C., Yang, K., Lin, J., Rummel, N., Koedinger, K.R. and Aleven, V. 2024. Combining Dialog Acts and Skill Modeling: What Chat Interactions Enhance Learning Rates During AISupported Peer Tutoring? (2024), 117130. [4] Broder, A.Z. 1998. On the resemblance and containment of documents. Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171) (Salerno, Italy, 1998), 2129. [5] Brown, T.B. et al. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs]. (Jul. 2020). [6] Castleman, B. and Turkcan, M.K. 2024. Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (2024), 649657. [7] Coffield, F., Moseley, D., Hall, E. and Ecclestone, K. 2004. Learning styles and pedagogy in post-16 learning : systematic and critical review. Learning and Skills Research Centre. [8] Denny, P., Kumar, V. and Giacaman, N. 2023. Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language. Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1 (New York, NY, USA, Mar. 2023), 11361142. [9] Dubey, A. et al. 2024. The Llama 3 Herd of Models. arXiv. [10] Gamma, E., Helm, R., Johnson, R. and Vlissides, J. 1994. Design Patterns: Elements of Reusable Object-Oriented Software. Pearson Education. [11] Gardner, R. 2007. The Right connections: Acknowledging epistemic progression in talk. Language in Society. 36, 3 (Jul. 2007), 319341. DOI:https://doi.org/10.1017/S0047404507070169. [12] Giray, L. 2023. Prompt Engineering with ChatGPT: Guide for Academic Writers. Annals of Biomedical Engineering. 51, 12 (Dec. 2023), 26292633. DOI:https://doi.org/10.1007/s10439-02303272-4. [13] Glickman, M.E. and Hennessy, J. 2015. stochastic rank ordered logit model for rating multicompetitor games and sports. Journal of Quantitative Analysis in Sports. 11, 3 (Jan. 2015). DOI:https://doi.org/10.1515/jqas-2015-0012. [14] Hofmann, P. 2008. Learning to Learn: Key-Competence for All Adults?! Convergence. 41, (2008), 173181. [15] Honnibal, M., Montani, I., Van Landeghem, S. and Boyd, A. 2020. spaCy: Industrial-strength Natural Language Processing in Python. Explosion AI. [16] Hu, B., Zheng, L., Zhu, J., Ding, L., Wang, Y. and Gu, X. 2024. Teaching Plan Generation and Evaluation With GPT-4: Unleashing the Potential of LLM in Instructional Design. IEEE Transactions 14451459. Technologies. DOI:https://doi.org/10.1109/TLT.2024.3384765. Learning (2024), 17, on [17] Hultsch, D.F. 1974. Learning to Learn in Adulthood. Journal of Gerontology. 29, 3 (May 1974), 302309. DOI:https://doi.org/10.1093/geronj/29.3.302. [18] Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M. and Cai, C.J. 2022. PromptMaker: Prompt-based Prototyping with Large Language Models. Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New York, NY, USA, Apr. 2022), 18. [19] Jones, I. and Davies, B. 2024. Comparative judgement in education research. International Journal of Research & Method in Education. 47, 2 (Mar. 2024), 170181. DOI:https://doi.org/10.1080/1743727X.2023.2242273. [20] Knowles, M.S. 1978. Andragogy: Adult Learning Theory in Perspective. Community College Review. 5, 3 (Jan. 1978), 920. DOI:https://doi.org/10.1177/009155217800500302. [21] Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang, H. and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. arXiv. [22] Lin, J., Chen, E., Han, Z., Gurung, A., Thomas, D.R., Tan, W., Nguyen, N.D. and Koedinger, K.R. 2024. How Can Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses. (2024), 236250. [23] Liu, C., Cui, J., Shang, R., Jia, Q., Rashid, P. and Gehringer, E. 2024. Generative AI for Peer Assessment Helpfulness Evaluation. (2024), 412419. [24] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H. and Neubig, G. 2021. Pre-train, Prompt, and Predict: Systematic Survey of Prompting Methods in Natural Language Processing. arXiv. [25] Mayfield, E. and Black, A.W. 2020. Should You Fine-Tune BERT for Automated Essay Scoring? Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications (Seattle, WA, USA Online, 2020), 151162. [26] McNamara, D.S. 2004. SERT: Self-Explanation Reading Training. Discourse Processes. 38, 1 (Jul. 2004), 130. DOI:https://doi.org/10.1207/s15326950dp3801_1. [27] Mizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D. and Stanovsky, G. 2024. State of What Art? Call for Multi-Prompt LLM Evaluation. Transactions of the Association for Computational Linguistics. 12, (Aug. 2024), 933949. DOI:https://doi.org/10.1162/tacl_a_00681. [28] Mizumoto, A. and Eguchi, M. 2023. Exploring the potential of using an AI language model for automated essay scoring. Research Methods in Applied Linguistics. 2, 2 (Aug. 2023), 100050. DOI:https://doi.org/10.1016/j.rmal.2023.100050. [29] Montani, I. and Honnibal, M. Prodigy: modern and scriptable annotation tool for creating training data for machine learning models. Prodigy. Explosion. [30] Morris, W., Crossley, S., Holmes, L., Ou, C., McNamara, D. and Dascalu, M. 2023. Using Large Language Models to Provide Formative Feedback in Intelligent Textbooks: 24th International Conference on Artificial Intelligence in Education , AIED 2023. Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky - 24th International Conference, AIED 2023, Proceedings. (2023), 484489. DOI:https://doi.org/10.1007/978-3-03136336-8_75. [31] Petridis, S., Wedin, B.D., Wexler, J., Pushkarna, M., Donsbach, A., Goyal, N., Cai, C.J. and Terry, M. 2024. ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles. Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville SC USA, Mar. 2024), 853868. [32] Rannikmäe, M., Holbrook, J. and Soobard, R. 2020. Social ConstructivismJerome Bruner. Science Education in Theory and Practice. B. Akpan and T.J. Kennedy, eds. Springer International Publishing. 259275. [33] Reimers, N. and Gurevych, I. 2019. Sentence-BERT: sentence embeddings using Siamese BERTnetworks. arXiv. [34] Rogers, C.R. and Freiberg, H.J. 1994. Freedom to Learn. Pearson College Div. [35] Scarlatos, A., Brinton, C. and Lan, A. 2022. Process-BERT: Framework for Representation Learning on Educational Process Data. arXiv. [36] Schulhoff, S. et al. 2024. The Prompt Report: Systematic Survey of Prompting Techniques. arXiv. [37] Shin, T., Razeghi, Y., Logan Iv, R.L., Wallace, E. and Singh, S. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Online, 2020), 42224235. [38] Singhal, S., Zambrano, A.F., Pankiewicz, M., Liu, X., Porter, C. and Baker, R.S. 2024. DeIdentifying Student Personally Identifying Information with GPT-4. (2024), 559565. [39] Smith, R.M. 1982. Learning How to Learn: Applied Theory for Adults. Pearson Prentice Hall. [40] Srivastava, A. et al. 2023. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. arXiv. [41] Taneja, K., Maiti, P., Kakar, S., Guruprasad, P., Rao, S. and Goel, A.K. 2024. Jill Watson: Virtual Teaching Assistant Powered by ChatGPT. Artificial Intelligence in Education: 25th International Conference, AIED 2024, Recife, Brazil, July 812, 2024, Proceedings, Part (Berlin, Heidelberg, Jul. 2024), 324337. [42] Vygotsky, L.S. 1978. Mind in Society: Development of Higher Psychological Processes. Harvard University Press. [43] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J. and Schmidt, D.C. 2023. Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv. [44] Wood, D., Bruner, J.S. and Ross, G. 1976. The role of tutoring inproblem solving. Journal of Child Psychology and Psychiatry. 17, 2 (Apr. 1976), 89100. DOI:https://doi.org/10.1111/j.14697610.1976.tb00381.x. [45] Xie, S.M., Raghunathan, A., Liang, P. and Ma, T. 2021. An Explanation of In-context Learning as Implicit Bayesian Inference. (Oct. 2021). [46] Xu, Z. and Zhu, P. 2023. Using BERT-Based Textual Analysis to Design Smarter Classroom Mode for Computer Teaching in Higher Education Institutions. International Journal of Emerging Technologies 114127. DOI:https://doi.org/10.3991/ijet.v18i19.42483. in Learning (iJET). 19 (Oct. 2023), 18, [47] Zamfirescu-Pereira, J.D., Wong, R.Y., Hartmann, B. and Yang, Q. 2023. Why Johnny Cant Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (New York, NY, USA, Apr. 2023), 121. [48] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E. and Stoica, I. 2023. Judging LLM-as-a-judge with MT-bench and Chatbot Arena. Proceedings of the 37th International Conference on Neural Information Processing Systems (Red Hook, NY, USA, Dec. 2023), 4659546623. [49] Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. and Chi, E. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. arXiv. [50] Zimmerman, B.J. and Schunk, D.H. eds. 2013. Self-Regulated Learning and Academic Achievement: Theoretical Perspectives. Routledge."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Vanderbilt University"
    ]
}