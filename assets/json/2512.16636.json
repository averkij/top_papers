{
    "paper_title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "authors": [
        "Giorgos Petsangourakis",
        "Christos Sgouropoulos",
        "Bill Psomas",
        "Theodoros Giannakopoulos",
        "Giorgos Sfikas",
        "Ioannis Kakogeorgiou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 6 3 6 6 1 . 2 1 5 2 : r a"
        },
        {
            "title": "REGLUE Your Latents with\nGlobal and Local Semantics for Entangled Diffusion",
            "content": "Giorgos Petsangourakis1,2 Christos Sgouropoulos1 Bill Psomas3 Theodoros Giannakopoulos1 Giorgos Sfikas2 Ioannis Kakogeorgiou1 1IIT, National Centre for Scientific Research Demokritos 2University of West Attica 3VRG, FEE, Czech Technical University in Prague Figure 1. Overview of REGLUE, Representation Entanglement with GlobalLocal Unified Encoding. The encoder of vision foundation model (VFM) provides (i) local patch-level features and (ii) global image-level feature (i.e. [CLS]). lightweight semantic compressor (pre-trained offline) maps the patch features to compact spatial semantics. In parallel, frozen VAE encoder produces image latents. We concatenate the latents, compressed semantics, and the global token and feed them to SiT backbone [44], which jointly models all modalities with velocity objective. Diffusion noise injection is omitted from the illustration. At selected block, an MLP head applies an external alignment [73] to match hidden SiT features to clean VFM targets. At sampling, we decode the (jointly) generated VAE latents."
        },
        {
            "title": "Abstract",
            "content": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: highlevel semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multilayer spatial semantics available. We introduce REGLUE (Representation Entanglement with GlobalLocal Unified Encoding), unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) global (image-level) [CLS] token within single SiT backbone. lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our globallocallatent joint modeling framework. The code is available at https://github.com/giorgospets/reglue. 1. Introduction Latent Diffusion Models (LDMs) have become dominant choice for high-quality image synthesis, largely because they shift the generative paradigm to modeling compact VAE latent space [56]. However, training such models is challenging: under single denoising objective, the model must concurrently learn high-level semantics (what to generate, e.g. objects, layout, relations) and low-level visual details (how to generate it, e.g. fine-grained appearance) [71]. The reconstruction-style denoising loss provides semantic supervision only indirectly, so semantic structure emerges slowly, limiting image quality and convergence speed [73]. To address these challenges, recent works leverages semantic representations from strong, pretrained Vision Foundation Models (VFMs), accelerating convergence and improving image quality [33, 67, 73]. REPA [73] proposes representation alignment objective to distill VFM features as an external teacher to the diffusion model. REG [67] and ReDi [33] go step further: they jointly model the image latent and the semantic signal inside the diffusion process. Regarding this signal, REG uses single image-level (global) representation (i.e., [CLS]), while ReDi employs linearly PCA-projected patch-level (local) VFM features. Both REG and ReDi expose only narrow, low-capacity semantic slice of the VFM to the diffusion model, underutilizing the rich, nonlinear, multi-layer, and spatial semantics available. REGs [CLS] offers informative imagelevel guidance, but is inherently non-spatial; fine-grained semantics are recovered via an external alignment loss as in REPA [73], which distills spatial semantics and boosts generative performance. In contrast, ReDi explicitly models patch-level semantics but its linear PCA projection restricts the representations to low-dimensional linear subspace, limiting the richness and non-linear spatial information. We argue that critical design choice for effectively leveraging VFM features is to model spatial semantics within the diffusion model, while preserving their nonlinear, multi-layer information via compact learned representations produced by semantic compressor. Specifically, jointly modeling patch-level features with VAE latents provides spatial guidance critical for capturing fine-grained structure and yields larger gains than either external feature alignment alone (REPA) or jointly modeling single imagelevel token (REG). These signals remain complementary: an alignment loss and global [CLS] token can be added as orthogonal auxiliary signals, but the primary improvements stem from spatial joint modeling of compressed patch features within the diffusion model (see Table 1). In our work, we first train lightweight convolutional semantic compressor that maps nonlinear, multi-layer VFM features into compact, spatially structured, semanticspreserving representation. Then, we introduce unified diffusion modeling approach that jointly models: (i) these compact patch-level (local) semantic features, (ii) an imagelevel (global) representation, and (iii) the VAE image latents. During training, we also apply feature-alignment auxiliary loss that aligns diffusion internal features with VFM teacher representations, further improving image synthesis performance. The overview is shown in Figure 1. In summary, our contributions are: 1. We introduce REGLUE (Representation Entanglement with GlobalLocal Unified Encoding), unified diffusion framework that jointly models image-level (global) and patch-level (local) VFM semantics with VAE latents, significantly boosting generative performance. 2. We propose lightweight semantic compressor that aggregates multi-layer VFM features and maps them to compact, semantics-preserving space. This compact representation enables significant gains via patch-level joint modeling with VAE latents, strongly improving synthesized image quality. 3. We show that, in REGLUE, (a) patch-level (local) semantics, (b) image-level (global) semantics, and (c) REPAstyle representation alignment act synergistically, delivering substantial gains in image quality and training convergence, while keeping the diffusion models parameters and inference-time compute essentially unchanged. benchmark, SiT-XL/2+REGLUE reaches the 1M-step performance of ReDi and REG using less than 30% and 80% of their iterations, respectively. generation"
        },
        {
            "title": "ImageNet",
            "content": "256256 4. On 2. Related Work Latent-variable generative modeling. Latent-variable models like Variational Autoencoders and Diffusion Denoising Probabilistic models are core building blocks of modern generative pipelines [32, 56]. There are often underappreciated connections across these families [6, 51]: diffusion can be viewed as hierarchical VAE with fixed latent dimension and frozen encoder [43], and both are trained via variational approximations [5]. Normalizing flows [30, 76] have likewise been analyzed through their links to diffusion [1, 74, 78]. VAEs are also closely related to probabilistic PCA, replacing the linear projection with neural decoder [20, 64]. Analyzing models under unified framework has repeatedly yielded progress: Scalable Interpolant Transformers (SiT) [44], alongside DiT [50] and Lightning DiT [71], an indispensable component of stateof-the-art generative frameworks [33, 59, 67, 71, 73]. Our work espouses an analogous rationale, focusing on holistic, joint modeling of tokenizer (VAE latents) and VFM semantics within single framework. Representation alignment with VFM features. Latent diffusion typically uses VAE first stage to obtain compact image latents, which are optimized for reconstruction rather than semantics; this can slow semantic emergence and curb the ability to represent abstract concepts [2, 36]. To mitigate this, works enhance the first stage by aligning or reshaping the latent space: VA-VAE [71] adds VFM alignment loss, TexTok [75] injects text description embeddings, MAETok [9] argues for discriminative (nonvariational) latents, and FA-VAE [45] separates low/high 2 frequencies via wavelets. Complementarily, the second stage (the denoiser) can be aligned to VFMs: REPA [73] distills mid-block features to VFM targets and accelerates convergence; DDT [66] decouples encoder/decoder; REPAE [37] pursues end-to-end VAE+denoiser alignment; and SVG [59] focuses on few-step generation. Our approach is complementary: instead of exposing narrow semantic slice or relying solely on external representation alignment, we jointly model global and local VFM semantics with VAE latents via frozen, lightweight semantic compressor, entangling them within single diffusion backbone and thereby accelerating convergence and improving generative quality. Joint feature generative modeling. complementary line of work directly models foundation features as observed variables, conceptually related to multimodal diflearns joint spaces across modalities (e.g. fusion that textimagevideoaudio) rather than distilling from them. Examples include CoDi [62] for any-to-any generation across modalities, and video methods that entangle appearance with motion signals like VideoJam [8]. Closer to our setting, REG [67] proposes SiT-based model that incorporates compressed tokens with the addition of also modeling VFM [CLS] token; Representation Diffusion (ReDI) [33] takes this approach one step further and models spatially referenced high-level features on equal grounds to VAE latents by Diffusion Transformer. We argue that these prior works model VFM in suboptimal manner, either by discarding useful spatial cues [67] or working with constrained, linear projections of high-level semantics. Representation learning. Supervised convolutional networks [23] and, more recently, Vision Transformers (ViTs) [18] have established strong baselines for transferable visual features, but the field has largely shifted toward pretraining regimes that reduce or remove manual labels. Early self-supervised work relies on hand-crafted pretext tasks (e.g. patch permutation [47] or rotation prediction [21]), while modern approaches favor contrastive objectives [11, 48] and self-distillation [7, 22], yielding highly transferable features at scale. Transformer-era enables masked image modeling (MIM): from BEiT [4] and MAE [24] to hybrid variants like iBOT [81] and AttMask [27]. In parallel, visionlanguage pretraining learns joint embeddings from web-scale imagetext pairs [58]. CLIP [54] popularizes this paradigm by aligning images and captions with contrastive objective, yielding strong zero-shot recognition [16], retrieval [31, 53], and segmentation [61] performance. SigLIP [77] refines the recipe by replacing the softmax with independent sigmoid losses, and SigLIP-2 [65] further improves transfer via stronger data and training strategies. 3. Method 3.1. Preliminaries Scalable Interpolant Transformer (SiT). We adopt the SiT [44] framework, built on the stochasticinterpolant formulation [40, 44]. Given clean image and pretrained VAE encoder Ez that produces image latents RDzHzWz , we consider the continuous-time stochastic interpolant: zt = αtz + σtϵz, ϵz (0, I), [0, 1], (1) where α0 = σ1 = 1 and α1 = σ0 = 0, with αt decreasing and σt increasing in t. SiT adopts Transformer-based architecture with stacked blocks, parameterizes the velocity field vθ(zt, t) and is trained with the standard velocity objective: Ez,ϵz,t (cid:104) (cid:13) (cid:13)vθ(zt, t) αtz σtϵz 2(cid:105) (cid:13) (cid:13) . (2) Unless stated otherwise, we adopt the linear schedule αt = 1t and σt = t, which yields constant derivatives αt = 1 and σt = 1. Vision Foundation Model (VFM). We denote by Ev() pretrained VFM (e.g. DINOv2), which provides patchlevel semantic features (ℓ) RDf Hf Wf for each layer ℓ {1, 2, . . . , L}, and global image-level representation cls RDf . Hf Wf denotes the patch grid size and Df the feature dimensionality. In ViT-based encoders Hf , Wf , Df remain fixed across layers. 3.2. Globallocal representation entanglement Our aim is to jointly model (i) VAE latents, (ii) local semantics (patch-level VFM features), and (iii) global semantics (i.e. image-level [CLS] token) within single SiT model. We reuse the notation from Sec. 3.1. Given an image x, Ez(x) = RDzHzWz denotes VAE latents, cls RDf the global VFM token, and RDsHzWz patchlevel compressed VFM features derived from {f (ℓ) ℓ=1 and aligned to the VAE latents grid (cf. Sec. 3.3 for detailed description of the compressor). }L Forward process. We first adopt shared schedule (αt, σt) to inject noise for all three entangled modalities: zt = αtz + σtϵz, st = αts + σtϵs, clst = αtcls + σtϵcls, ϵz (0, I), ϵs (0, I), ϵcls (0, I), (3) with independent noise terms and [0, 1]. 3 Velocity objective. SiT parameterizes velocity field vθ(zt, st, clst, t) over the joint state (zt, st, clst) and is trained with the multimodal velocity loss: Lv = (cid:104) (cid:13) θ(zt, st, clst, t) αtz σtϵz (cid:13) 2 (cid:13) 2 (cid:13)vz (cid:13) (cid:13)vs θ(zt, st, clst, t) αts σtϵs (cid:13) (cid:13)vcls θ (zt, st, clst, t) αtcls σtϵcls (cid:13) 2 (cid:13) 2 + λs + λcls (4) (cid:105) , (cid:13) 2 (cid:13) 2 θ, vs θ and vcls θ where vz correspond to the predictions for the VAE latent, global VFM token, and patch-level VFM features velocity; λs and λcls are weighting coefficients. Tokenization and fusion. The SiT diffusion backbone operates on sequence of tokens with shared width D, so we aim to bring the different modalities to common dimensional space. Concretely, we first patchify1 zt and st, forming = patch(st) RN Ds (where =HzWz/p2 is the number of patches). We then project each modality to the model width with linear embedding layers: = patch(zt) RN Dz and zt = tWz, st = tWs, clst = clstWcls where Wz RDz D, Ws RDs D, and Wcls RDf are learned embedding matrices. To combine and jointly model zt (VAE latents) and st (local semantics), there are two straightforward options: (i) concatenate image latents and semantic features along the sequence dimension and pass 2N patch tokens through SiT, or (ii) merge channel-wise and keep single grid of tokens. We adopt (ii), avoiding the 2 longer sequence and quadratic self-attention overhead of (i). The global [CLS] is inherently single token; thus, we always keep it as separate token, adding negligible throughput overhead. Finally, the input sequence to the SiT Transformer is: h0 = (cid:104) clst (cid:124)(cid:123)(cid:122)(cid:125) 1D ; (cid:0)zt + st (cid:123)(cid:122) (cid:124) (cid:1) (cid:125) (cid:105) R(1+N )D, (5) where [ ; ] denotes concatenation along token dimension. R(1+N )D denote the hidPrediction heads. Let hK den sequence after the last (K-th) SiT block. We obtain the per-modality velocity predictions as: θ = unpatch(cid:0) hK vz θ = unpatch(cid:0) hK vs dec [1:N ] Wz [1:N ] Ws (cid:1), (cid:1), dec [0] Wcls dec. θ = hK vcls (6) 1Partitioning tensor RCHW into non-overlapping tiles (here p=2; denoted SiT/2), flattening each tile, and stacking them row-major into sequence of patch tokens, resulting in R(HW/p2)(Cp2). 4 dec RDDz , Ws dec RDDs are linear prewhere Wz diction heads, unpatch() reshapes the patch tokens back to spatial tensors of shape Dz Hz Wz and DsHz Wz, respectively; Wcls dec RDDf projects the global token. External representation alignment. At selected Transformer block {1, . . . , K}, we encourage SiT hidden tokens to stay close to clean, frozen VFM targets via lightweight projector ϕ : RD RDf and cosine loss, folt R(1+N )D be the hidden sequence lowing [73]. Let hk at block k. We form the target token sequence by concatenating the global token with patch-level VFM features: = (cid:2)cls ; (L) (cid:3) R(1+N )Df . (7) where (L) denotes flattened spatial dimension to tokens."
        },
        {
            "title": "The representation alignment loss is",
            "content": "LREPA = (cid:34)"
        },
        {
            "title": "1\nN + 1",
            "content": "N +1 (cid:88) n=1 (cid:16) sim , ϕ(cid:0)hk y[n] (cid:1)[n](cid:17) (cid:35) , (8) where we apply alignment on the [CLS] position and the semantic patch tokens and sim is cosine similarity. Total objective. We train with the multimodal velocity loss in Eq. (4) and the auxiliary alignment loss in Eq. (8): Ltotal = Lv + λrep LREPA. (9) Sampling. To generate new samples, we employ the reverse-time SDE EulerMaruyama [60] using vθ to obtain (z0, s0, cls0) from Gaussian noise, and reconstruct the image via the (frozen) VAE decoder. 3.3. lightweight spatial semantic compressor Our goal is to jointly model VAE latents with VFM semantics. Naıvely fusing patch-level VFM features with image latents substantially widens the dimensionality to Dz+Df with Df Dz, biasing SiT capacity toward the representation modality and hurting generative quality (a phenomenon also addressed in ReDi [33] via PCA). Convolutional autoencoder. Instead of linear subspace, we introduce nonlinear, lightweight semantic compressor Eψ that preserves spatial structure while re-balancing dimensionality. We instantiate Eψ as shallow convolutional autoencoder, pretrain it once to reconstruct VFM features and then keep it frozen. Our compressor finally projects compressed features of dimension Ds (cid:80) ℓ Dℓ. Multi-layer aggregation. Since leveraging representations across depths has shown benefits to many scene understanding tasks [10, 12, 39, 41, 55, 79], we typically aggregate the multi-layer patch-level VFM features by channelwise concatenation: = (cid:2)f (1) , (2) , . . . , (L) ] R(LDf )Hf Wf , (10) where [ , ] denotes concatenation along the channels. Training and inference. lightweight convolutional autoencoder (Eψ, Dψ) is trained (offline) to reconstruct f: (cid:104) min ψ Dψ(Eψ(f)) f2 2 (cid:105) , (11) We then freeze Eψ, produce compact spatial semantics Eψ(f) RDsHf Wf , and spatially resample them to the VAE latent grid (Hz Wz) resulting in RDsHzWz , typically using bilinear resampling. 4. Experiments 4.1. Setup Implementation details. We strictly follow the standard training protocols of SiT [44]. Our experiments are conducted on the ImageNet [15] dataset. Following the ADM preprocessing pipeline [17], all images are center-cropped and resized to 256 256 resolution. Each image is then encoded into latent representation R43232 using the pre-trained SD-VAE-FT-EMA [56]. Our main experiments are based on SiT-B/2 models, which use 22 patch size and are trained for 400K steps. To assess the impact of our approach at larger scales and longer training, we additionally train SiT-XL/2 models for 1M steps. We maintain batch size of 256 for all experiments. Unless stated otherwise, for semantic feature extraction, we employ DINOv2-B [14, 49], concatenating features from blocks 9-12 to obtain 3072-channel (768 4) feature map at 16 16 spatial resolution. Our lightweight convolutional autoencoder (Eψ, Dψ), with hidden layer of 256, is pretrained for 25 epochs using reconstruction loss (Eq. 11) to compress these maps. The encoder Eψ produces compact 16-channel, 16 16 latent representation. For external representation alignment we follow the formulation in Sec. 3. We apply the external alignment using the local and global representations derived from the VFMs last layer, with layer of the SiT backbone. We use = 4 for SiT-B/2 and = 8 for SiT-XL/2. Regarding the total objective coefficients we set λs = 1, λcls = 0.03, and λrep = 0.5. More implementation details regarding the semantic compressor and SiT are provided in Appendix Sec. B.1 and Sec. B.2. 5 Evaluation. In order to evaluate image generation quality, we report standard set of quantitative metrics. These include Frechet Inception Distance (FID) [25] for perceptual quality, sFID [46] for spatial coherence, Inception Score (IS) [57] for diversity, as well as Precision (Pre.) and Recall (Rec.) [34] to measure sample fidelity and distribution coverage, respectively. All metrics are computed using 50,000 generated samples, following the standard ADM evaluation suite [17]. For all experiments, we use EulerMaruyama SDE sampling with 250 steps. When using Classifier-Free Guidance (CFG) [26], we set CFG scale at = 2.8 and guidance interval to [0, 0.9], following [35]. 4.2. Analyzing VFM semantics for generation We leverage our framework to investigate how diffusion modeling in SiT-B/2 benefits from: (i) which semantics are modeled (global vs. local), (ii) how local semantics are compressed (linear vs. non-linear), and (iii) the degree to which an external representation-alignment objective contributes to generation quality. The corresponding design choices and their impact on FID are shown in Table 1. Local (patch-level) outperform global (image-level) semantics. We first focus on representations modeled directly by the diffusion model, without any external alignment. We find that patch-level semantics clearly outperform global-only signals. Relying solely on the global [CLS] token (setting (c)) attains 25.7 FID, whereas modeling patchlevel features (setting (d), ReDi, linear PCA) improves to 21.4 FID. Both settings substantially outperform the baseline SiT-B/2 backbone (33.0 FID, setting (a)), but the gap between (c) and (d) underscores that fine-grained spatial semantics are pivotal for improved generative modeling. Non-linear compression unlocks local guidance. Replacing the linear PCA used in ReDi with our lightweight non-linear semantic compressor boosts patch-level joint modeling: setting (i) reaches 14.3 FID without any alignment loss, an absolute 7.1 FID reduction over ReDi (setting (d)). Notably, this also surpasses the state-of-the-art REG baseline (setting (h), 15.2 FID), even though REG combines global modeling with external alignment. Moreover, enriching local guidance by aggregating multi-layer VFM patch-level features before compression (setting (l)) further reduces FID to 13.3, without using any global token or external alignment. The results indicate that rich spatial semantics are crucial signal, and non-linear compression is key to unlocking their full benefit. External alignment under local and global modeling. We analyze how REPA behaves when applied to local and/or global semantics. When the backbone does not Table 1. Impact of VFM semantics on SiT-B/2 for improved generation. Results at 400K training steps. (a) Baseline SiT-B/2 in Gray , (b) REPA in Purple, (d) ReDi in Light Green, (h) REG in Yellow, and (i-n) REGLUE (ours) in Light Cyan. denotes novel components proposed in our work. While the nonlinear patch-level semantics alone yield substantial gains, the other listed components provide additional improvements. Setting (n) uses stronger DINOv3-B VFM; for fairness and consistency with prior work, all other experiments adopt DINOv2-B as the default VFM. LOCAL (PATCH-LEVEL) GLOBAL (IMAGE-LEVEL)"
        },
        {
            "title": "EXTERNAL\nALIGNMENT",
            "content": "NON-LINEAR LINEAR MULTI-LAYER [CLS] PATCH-LEVEL [CLS] (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n)"
        },
        {
            "title": "FID",
            "content": "33.0 24.4 25.7 21.4 18.8 33.7 15.5 15.2 14.3 14.1 13.7 13.3 12.9 12.3 50K 100K 200K 400K Figure 2. REGLUE fast convergence. Qualitative evolution of SiT-B/2+REGLUE at 50K/100K/200K/400K training steps. All use identical noise, the same sampling schedule/step count, and no classifier-free guidance. REGLUE achieves high fidelity early. jointly model patch tokens, only aligning local VFM feathe original REPA tures already provides strong boost: configuration (setting (b)) improves the default SiT-B/2 baseline from 33.0 (a) to 24.4 FID. REPA also improves the patch-only setting of ReDi (d) to (e) (from 21.4 to 18.8 FID). similar pattern appears when starting from model that only jointly models the global [CLS] token: adding local-only external alignment (setting (g)) reduces FID from 25.7 (c) to 15.5, and including the global component in the alignment as well (setting (h)) further improves it to 15.2. This indicates that local patch alignment is the dominant 6 source of improvement, while global alignment provides smaller, complementary gain. In contrast, aligning only the global information without any local alignment (setting (f)) degrades performance from 25.7 (c) to 33.7 FID, suggesting that alignment on global features alone is unstable without spatial anchors. Finally, in our setting, adding REPA on top of non-linear patch-level modeling improves FID from 14.3 (i) to 14.1 (j), showing that once strong spatial semantics are jointly modeled, external alignment acts as mild but consistent performance complement. REGLUE: Joint local-global-latent modeling. Building on these observations, we progressively add the global token and alignment to compressed local modeling. Adding REPA-style alignment on top of (i) yields setting (j) with 14.1 FID (a modest but consistent gain), indicating that external supervision complements joint local modeling. Incorporating the global [CLS] and aligning both local and global signals (setting (k)) further improves to 13.7 FID. Finally, aggregating multi-layer patch features (from the last four VFM blocks) before compression (setting (m)) forms our final REGLUE unified setting, achieving 12.9 FID. To study the dependence of REGLUE on the underlying VFM, we also examine stronger VFM, DINOv3-B. As reported in setting (n), DINOv3-B yields the best result (FID 12.3), improving over DINOv2-B (FID 12.9) and indicating that REGLUE can effectively exploit more powerful semantic encoder. Nevertheless, to remain consistent with prior work Table 2. Conditional and unconditional generation. Comparison of SiT-B/2 with REPA, ReDi, REG, and REGLUE on ImageNet 256256 without classifier-free guidance (CFG). We report parameter count, iterations, and FID. Table 3. Conditional generation. Comparison of SiT-XL/2 with REPA, ReDi, REG, and REGLUE on ImageNet 256 256 without classifier-free guidance (CFG) under comparable settings. We report parameter count, iterations, and FID. MODEL #PARAMS ITER. FID (a) CONDITIONAL GENERATION SiT-B/2 + REPA + ReDi + REG + REGLUE (ours) + REGLUE (ours) 130M 400K 33.0 130M 400K 24.4 130M 400K 21.4 132M 400K 15.2 132M 300K 14.5 132M 400K 12.9 (b) UNCONDITIONAL GENERATION SiT-B/2 + ReDi + REG + REGLUE (ours) 130M 400K 59.8 130M 400K 43.6 132M 400K 29.7 132M 400K 28.7 and enable fair comparison, we adopt DINOv2-B as our default VFM in all main experiments. For more in-depth analysis of our compressor, see Sec. 3.3. 4.3. Enhancing diffusion models Accelerating convergence. Table 2(a) reports conditional ImageNet 256 256 results without classifier-free guidance (no CFG) with SiT-B/2 backbone. REGLUE reaches 14.5 FID at 300K steps surpassing REG (15.2 at 400K) with 25% fewer iterations, and further improves to 12.9 at 400K. At 400K, REGLUE reduces FID by 60.9% vs. vanilla SiT-B/2 (33.0), 47.1% vs. REPA (24.4), and 39.7% vs. ReDi (21.4). In Figure 2, we show visual examples demonstrating that REGLUE achieves high-fidelity generations early in training. Moving to larger backbone and more training steps, in Table 3 we show conditional ImageNet 256 256 results (no CFG) with SiT-XL/2 backbone. At 200K steps, REGLUE achieves 4.6 FID, outperforming REG (5.0) and substantially surpassing REPA (11.1) and ReDi (12.5). Notably, REGLUE reaches 2.7 FID at 700K, matching REGs 1M performance (2.7) with 30% fewer iterations. At 1M, REGLUE sets the best score (2.5) vs. REG (2.7), ReDi (5.1), and REPA (6.4). MODEL #PARAMS ITER. FID SiT-XL/2 675M 7M 8.3 + REPA + ReDi + REG + REGLUE (ours) + REPA + ReDi + REG + REGLUE (ours) 675M 200K 11.1 675M 200K 12.5 677M 200K 5.0 677M 200K 4.6 675M 400K 7.9 675M 400K 7.5 677M 400K 3.4 677M 400K 3.2 + ReDi + REGLUE (ours) 675M 700K 5.6 677M 700K 2.7 + REPA + ReDi + REG + REGLUE (ours) 675M 675M 677M 677M 1M 6.4 1M 5.1 1M 2.7 1M 2.5 Table 4. Comparison with state-of-the-art. Quantitative results on ImageNet 256 256 with classifier-free guidance (CFG). REPA, ReDi, REG and REGLUE employ an SiT-XL/2 model. MODEL EPOCHS FID SFID IS PRE. REC. Autoregressive Models VAR 350 MagViTv2 1080 MAR 800 Latent Diffusion Models LDM 200 U-ViT-H/2 240 DiT-XL/2 1400 MaskDiT 1600 SD-DiT 480 SiT-XL/2 1400 FasterDiT 400 MDT 1300 1.80 1.78 1.55 3.60 2.29 2.27 2.28 3.23 2.06 2.03 1. Leveraging VFMs Representations ReDi REPA REG 1.61 1.42 1.36 800 800 800 REG REGLUE (ours) REG REGLUE (ours) 80 160 160 1.86 1.61 1.59 1.53 - - - 365.4 0.83 319.4 0.83 303.7 0.81 - 5.68 4.60 5.67 - 4.50 4.63 4. 4.66 4.70 4.25 4.49 4.25 4.36 4.26 247.7 0.87 263.9 0.82 278.2 0.83 276.6 0.80 - - 270.3 0.82 264.0 0.81 283.0 0.81 295.1 0.78 305.7 0.80 299.4 0.77 321.4 0.76 313.1 0.78 304.6 0.77 320.4 0.78 0.57 0.57 0.62 0.48 0.57 0.57 0.61 - 0.59 0.60 0. 0.64 0.65 0.66 0.63 0.64 0.65 0.65 Unconditional generation. We evaluate our method in the unconditional setting and summarize the results in Table 2(b). The findings are consistent with the analysis in the previous section. Our REGLUE achieves 52%, 34.2%, and 3.4% improvements over SiT-B/2, ReDi, and REG, respectively, demonstrating the effectiveness of nonlinear compression and joint localglobal feature modeling. Remarkably, even in this more challenging unconditional setting, REGLUE (28.7 FID) substantially outperforms the conditional SiT-B/2 baseline (33.0 FID). State-of-the-art comparison. Table 4 reports quantitative results on ImageNet with classifier-free guidance. REGLUE improves over REG at matched epochs and closes the gap to longer-trained baselines. At 80 epochs, REGLUE lowers FID to 1.61 vs 1.86 for REG. At 160 epochs, it further improves to 1.53 vs 1.59. Although trained for 5 fewer epochs than the 800-epoch variants (REPA, ReDi, REG), the 160-epoch REGLUE remains competitive with models that leverage VFM representations and are Figure 3. Semantic compressor architecture and training. The representations from the last four layers of the vision foundation model (VFM) encoder are concatenated and passed to the compression model, which projects them into compact 16-channel semantic representation. In our default configuration (corresponding to the middle row of Table 5), the compressor maps the dense concatenated VFM features through an input layer Conv2D(3072, 256), middle ResidualBlock(256, 256), and an output layer Conv2D(256, 16), where 256 is the hidden dimensionality. The semantic de-compressor then reconstructs the compact semantics back to their original dimensionality. The model is trained using an MSE loss between the dense concatenated features and their reconstructed counterparts. trained for substantially longer (REPA, FID 1.42; REG, FID 1.36). Classifier-free guidance ablations are presented in Appendix Table 10. We provide qualitative results of generated images in Appendix Sec. E. 4.4. Semantic compressor impact As we highlight in Sec. 3.3, the channel dimensionality of VFM representations is substantially higher than that of image latents, which can lead to degraded performance when fused naıvely. To mitigate this, ReDi [33] employs linear PCA to project the representations into an low-dimensional latent space; however, as we show in Table 1, this design choice is suboptimal. In contrast, we show that our nonlinear CNN-based semantic compressor  (Fig. 3)  can substantially improve generation quality. In this section, we systematically examine its main design choices: the compression dimensionality  (Fig. 5)  , the compressor capacity (Tab. 5), the set of VFM layers used as input (Tab. 6), and quantify their effect on both sample quality and efficiency. We further measure how much semantic information is preserved under compression using downstream probing tasks  (Fig. 4)  . Semantic preservation under compression. Figure 4 evaluates how well compressed patch-level features retain VFM semantics via attentive probing accuracy on ImageNet [52] and how this relates to generative quality (FID). Our non-linear semantic compressor preserves semantics much better than linear PCA: with only 8 channels it ) ( F 22 20 18 16 12 PCA 8 channels Ours 8 channels Ours 16 channels 768 channels"
        },
        {
            "title": "40\nTop-1 accuracy (%)",
            "content": "60 80 Figure 4. Attentive probing accuracy vs. generation quality on ImageNet for different DINOv2 patch-level compression variants. Each point shows top-1 attentive probing accuracy [52] and FID of the corresponding SiT model, with bubble area proportional to the semantic feature dimensionality. Our non-linear semantic compressors (8 and 16 channels) achieve substantially better FID at higher probing accuracy than the PCA-compressed features of ReDi, while the vertical dashed line marks the accuracy of the full 768-channel DINOv2 representation. achieves substantially higher probing accuracy and lower FID than the PCA-compressed ReDi features, and increasing to 16 channels further improves both metrics, approaching the full 768-channel DINOv2 baseline. In contrast, the PCA-based compression in ReDi yields low probing accuracy and only modest FID gains, indicating that nonlinear, spatially structured compression is key to preserving semantic information while improving generation qual8 ) ( F 18 17 16 15 14"
        },
        {
            "title": "12\nCompression Channels",
            "content": "16 Table 6. The effect of multi-layer features. We compare the generation performance without using REPA loss across different sets of Dino-V2 layers.The results are reported at 400K training steps. In all runs VFM patch tokens are compressed to 16 channels and SiT-B/2 is used. The input layer of the compressor is changing accordingly, denoted by CNN In Dim column. 20 Figure 5. Performance vs. compression channels. Ablation of the final compression channels, in DINOv2 last layers representation, using SiT-B/2 trained for 400K steps without REPA loss. Table 5. Compression model size comparison. End-to-end evaluation of compression model hidden layer sizes using the last 4 DINOv2 layers and SiT-B/2 as the diffusion model, trained for 400K steps. Representations are compressed to 16 channels. REPA loss is not applied. Input and Output layers are Conv2D(in, out) layers. Each middle block is Residual Block which comprises two 3 3 convolutional layers (stride = 1, padding = 1) with Batch Normalization and ReLU activation. Samples in Throughput column are VFM local representations. INPUT MIDDLE OUTPUT #PARAMS THROUGHPUT (SAMPLES/SEC) LAYER BLOCK LAYER (M) (3072,128) (128,128) (128,16) (3072,256) (256,256) (256,16) (3072,512) (512,512) (512,16) 7.7 16.6 37.9 32,304 18,578 9, FID 14.2 13.3 13.3 ity. Additional semantic preservation analysis with semantic segmentation experiments on Cityscapes [13] are presented in Appendix Sec. A.1. Semantic compression dimensionality. In Figure 5, we examine how the number of compression channels affects generative performance. Aggressive compression (i.e., 4channels) removes too much information, leading to degraded FID. Performance improves as we increase the number of channels up to 16, but degrades again at 20 channels. This suggests an optimal intermediate subspace: the compressed features preserve essential information to guide generation, yet are compact enough to stay balanced with the 4-channel image latents and not dominate the models capacity. We therefore adopt 16 channels as the default in our main REGLUE configuration. Lightweight compressor design. Our aim is to keep the semantic compressor lightweight while preserving essential information. In Table 5, we present an ablation over different hidden-layer widths. Reducing the hidden dimensionality from 256 to 128 channels degrades FID, indicating that overly constrained bottlenecks limit the ability to preserve essential semantic information. Increasing the hidden size from 256 to 512 channels yields no further improvement in FID, while doubling the model size and significantly reducing throughput, making this configuration inefficient. Pushing the capacity further (e.g., 1024 hidden size) leads to unstable compressor training. Moreover, increasing the DINO LAYERS CNN IN DIM FID 12 3, 6, 9, 12 9, 10, 11, 12 768 3072 3072 14.3 16.9 13. model depth, via additional convolutional layers or residual blocks, exhibits the same instability. Overall, shallow compressor with 256 hidden size offers the best balance between stability, efficiency, and generative performance. Choosing VFM layers for compression. In Table 6, we study how the choice of VFM layers fed into the semantic compressor affects generation performance. Motivated by [49], we compare three configurations of DINOv2-B patch features: (i) using only the last layer (12), (ii) using four intermediate layers (3, 6, 9, 12), and (iii) using the last four layers (912). In all cases, the compressed features are mapped to 16 channels and fed into the SiT-B/2 diffusion model. Using only the final layer yields 14.3 FID, while including shallow intermediate layers (i.e., 3 & 6) degrades performance to 16.9 FID, indicating that earlylayer features do not provide useful semantic guidance to the generation. In contrast, aggregating the last four layers (912) leads to 13.3 FID, suggesting that jointly compressing semantically rich, deeper VFM features provides the most beneficial signal for our framework. For more analysis about our compressor, see Appendix. Ablations about other loss variants of the compressor can be found in Appendix Table 7 and more experimentations with different VFMs in Table 8. 5. Conclusion We introduced REGLUE, unified generative model for latent diffusion that enables efficient entanglement of reconstruction-optimized and semantics-optimized image representations. By jointly modeling VAE latents with VFM patch-level & global semantics, coupled with lightweight compression and aggregation components, we have shown that REGLUE improves generation FID and accelerate convergence on ImageNet baselines by significant margins. Acknowledgements We thank our colleague Theodoros Kouzelis for fruitful discussions. Bill was supported by the EU Horizon Europe programme MSCA PF RAVIOLI (No. 101205297). AWS resources were provided by the National Infrastructures for Research and Technology GRNET and funded by the EU Recovery and Resiliency Facility."
        },
        {
            "title": "References",
            "content": "[1] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. [5] Christopher Bishop. Pattern recognition and machine learning. Springer, 2006. [6] Sam Bond-Taylor, Adam Leach, Yang Long, and Chris Willcocks. Deep generative modelling: comparative review of VAEs, GANs, normalizing flows, energy-based and autoregressive models. IEEE transactions on pattern analysis and machine intelligence, 44(11):73277347, 2021. [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. [8] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. [9] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In ICML, 2025. [10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [12] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask In CVPR, transformer for universal image segmentation. 2022. [13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. [14] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. [17] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. [18] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [19] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. [20] Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. Factor analysis, probabilistic principal component analysis, variational inference, and variational autoencoder: Tutorial and survey. arXiv preprint arXiv:2101.00734, 2021. [21] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. [22] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. [25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [26] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [27] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and to hide from your students: Nikos Komodakis. What In European Attention-guided masked image modeling. Conference on Computer Vision, pages 300318. Springer, 2022. [28] Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. DINO-foresight: Looking into the future with DINO. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 10 [29] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. [30] Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. [31] Giorgos Kordopatis-Zilos, Vladan Stojnic, Anna Manko, Pavel Suma, Nikolaos-Antonios Ypsilantis, Nikos Efthymiadis, Zakaria Laskar, Jiri Matas, Ondrej Chum, and Giorgos Tolias. Ilias: Instance-level image retrieval at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1477714787, 2025. [32] Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. EQ-VAE: Equivariance regularized In latent space for improved generative image modeling. Forty-second International Conference on Machine Learning, 2025. [33] Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Boosting generative image modeling via joint image-feature synthesis. In NeurIPS, 2025. [34] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. [35] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. [36] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. [37] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. REPA-E: Unlocking VAE for end-to-end tuning with latent diffusion transformers. In ICCV, 2025. [38] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [39] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. [40] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [41] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. [42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. [43] Calvin Luo. Understanding diffusion models: unified perspective. arXiv preprint arXiv:2208.11970, 2022. [44] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, page 2340, 2024. [45] Tejaswini Medi, Hsien-Yi Wang, Arianna Rampini, and Margret Keuper. FAVAE-effective frequency aware latent tokenizer. In NeurIPS 2025 Workshop: Reliable ML from Unreliable Data, 2025. [46] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. [47] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of In Eurovisual representations by solving jigsaw puzzles. pean conference on computer vision, pages 6984. Springer, 2016. [48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [49] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. [50] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [51] Simon JD Prince. Understanding deep learning. MIT press, 2023. [52] Bill Psomas, Dionysis Christopoulos, Eirini Baltzi, Ioannis Kakogeorgiou, Tilemachos Aravanis, Nikos Komodakis, Konstantinos Karantzalos, Yannis Avrithis, and Giorgos Tolias. Attention, please! revisiting attentive probing for masked image modeling. arXiv preprint arXiv:2506.10178, 2025. [53] Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, and Giorgos Tolias. Instance-level composed image retrieval. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [55] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In CVPR, 2021. [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. [57] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [72] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024. [73] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [74] Mohsen Zand, Ali Etemad, and Michael Greenspan. Diffusion models with deterministic normalizing flow priors. Transactions of Machine Learning Research, 2024. [75] Kaiwen Zha, Lijun Yu, Alireza Fathi, David Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu. Languageguided image tokenization for generation. In CVPR, pages 1571315722, 2025. [76] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. In ICML, 2025. [77] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Sigmoid Loss for Language Image PreLucas Beyer. Training . In ICCV, pages 1194111952, 2023. [78] Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. In NeurIPS, pages 1628016291, 2021. [79] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. [80] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. [81] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. International Conference on Learning Representations (ICLR), 2022. [82] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer. In CVPR, pages 84358445, 2024. Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, pages 2527825294, 2022. [59] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. [60] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [61] Vladan Stojnic, Yannis Kalantidis, Jiˇrı Matas, and Giorgos Tolias. Lposs: Label propagation over patches and pixels for open-vocabulary semantic segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 97949803, 2025. [62] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36:1608316099, 2023. [63] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. [64] Michael Tipping and Christopher Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society Series B: Statistical Methodology, 61(3):611 622, 1999. [65] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understandarXiv preprint ing, arXiv:2502.14786, 2025. localization, and dense features. [66] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint DDT: Decoupled diffusion transformer. arXiv:2504.05741, 2025. [67] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. NeurIPS, 2025. [68] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. [69] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. [70] Jingfeng Yao, Cheng Wang, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. In NeurIPS, 2024. [71] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent 12 ) ( 20 18 16 14 12 PCA 8 channels Ours 8 channels Ours 16 channels 768 channels 60 65 mIoU 70 Figure 6. Semantic segmentation performance mIoU vs generation quality for different DINOv2 patch-level compression variants. Each point shows the segmentation mIoU on Cityscapes [13] using DPT [55] head on frozen features following implementation from [28, 68, 69] and the FID on ImageNet of the corresponding SiT model. Bubble area is proportional to feature dimensionality. Our non-linear semantic compressors (8 and 16 channels) achieve substantially better FID at higher mIoU than the PCA-compressed features of ReDi. The vertical dashed line indicates the mIoU of the full 768-channel DINOv2 representation. A. Additional Experimental Results A.1. Semantic preservation under compression feaTo assess how well our compressed patch-level tures preserve vision foundation model (VFM) semantics, we perform an additional experiment on semantic segmentation. Figure 6 shows semantic segmentation on Cityscapes [13] measured by mIoU (using DPT [55] head on frozen features) versus ImageNet FID for different DINOv2 patchlevel compression variants. At the same 8channel compression, our nonlinear compressor achieves 67.1 mIoU/14.3 FID, notably better than 59.1/21.4 of PCA (+8.0 mIoU/7.1 FID). Increasing to 16 channels further improves to 68.7 mIoU/13.3 FID. Despite having 96 or 48 less channels (respectively) than the original 768channel DINOv2 representation (vertical dashed line at 72.5 mIoU), our compressed variants effectively retain most of the semantics while substantially improving generative fidelity, indicating that the learned nonlinear compressor preserves semantic representations and is better fit than linear PCA in joint semantics-VAE latents modeling. A.2. Semantic compressor auxiliary objectives We further investigate how the compressors training objectives impact downstream generation. Starting from our MSE-only autoencoder, we (i) switch to variational formulation with KL term and (ii) add an adversarial (GAN) loss. In all experiments, we compress the last VFM layer to 16 channels and, during diffusion training, model only local semantics without external alignment (similar to setting (i) in Table 1). We follow [56] for the VAE/GAN setup: KL Semantic compressor loss variants. ImageNet Table 7. 256256 comparison without classifier-free guidance (CFG) using SiT-B/2. We compare the impact of different auxiliary objectives in our semantic compressor. We use DINOv2-B last layer representations compressed to 16 channels. For REGLUE, we follow setting (i) in Table 1 (main paper). TRAINING OBJECTIVES FID SFID PRECISION RECALL MSE MSE+KL MSE+GAN 14.3 17.2 14.4 6.7 7.1 6.9 0.62 0.63 0. 0.65 0.64 0.65 weight 106; lightweight two-layer discriminator applied from the start; all other hyperparameters identical to the MSE baseline. Table 7 shows that plain MSE yields the best performance (14.3 FID and 6.7 sFID). Adding KL noticeably degrades performance (17.2 FID, 7.1 sFID), while including the GAN term provides no gains and slightly worsens overall performance. Overall, exploring these wellestablished additions does not yield any further improvements in our setting. Table 8. REGLUE with different VFMs. ImageNet 256256 comparison without CFG using SiT-B/2. For REGLUE, we follow setting (m/n) in Table 1 (main paper). VFM FID SFID PRECISION RECALL DINOv2-B DINOv3-B CLIP-L 12.9 12.3 18.1 5.8 5.8 7.1 0.67 0.67 0. 0.63 0.63 0.62 A.3. Impact of VFM To evaluate REGLUE across different VFMs, we experiment with three encoders: DINOv2-B, DINOv3-B, and CLIPL. For each backbone, we concatenate the last four layers and adapt the compressors input projection to the corresponding embedding size (e.g., 4768=3072 for DINOv2B, 41024=4096 for CLIP-L). All compressors are trained for 25 epochs with target compression of 16 channels, and the downstream SiT-B/2 generator is trained for 400K steps in every setting for fair comparison. Table 8 reports FID, sFID, precision, and recall. DINOv3-B delivers the best generation quality (lowest FID), DINOv2-B is close second, while CLIP-L lags behind. As already discussed in the main paper, to remain consistent with prior work [33, 67, 73], we adopt DINOv2-B as our default VFM. A.4. Detailed benchmark We provide detailed evaluation of SiT-XL/2+REGLUE with more training iterations and additional metrics. Table 9 demonstrates the performance, reporting FID, sFID, inception score, precision, and recall. Notably, REGLUE reaches 7.8 FID at 100K steps, already surpassing the vanilla SiT-XL/2 baseline at 7M steps (8.3 FID). It continues to improve substantially, reaching 3.2 at 400K, 2.6 at 750K, and 2.5 at 1M steps. Table 9. Detailed evaluation for SiT-XL/2+REGLUE. ImageNet 256256 without CFG. MODEL #ITERS. FID SFID IS PREC. REC. SiT-XL/2 w/ REGLUE w/ REGLUE w/ REGLUE w/ REGLUE w/ REGLUE w/ REGLUE w/ REGLUE 7M 8.3 50K 20.0 100K 7.8 200K 4.6 400K 3.2 700K 2.7 750K 2.6 1M 2.5 6.3 131.7 6.3 64.7 4.7 116.0 4.4 148.0 4.3 171.6 4.2 185.0 4.1 185.6 4.1 188.6 0.68 0.65 0.64 0.74 0.75 0.76 0.76 0.76 0.67 0.58 0.57 0.63 0.63 0.65 0.65 0.65 Table 10. CFG ablations on SiT-XL/2+REGLUE (800K steps, ImageNet 256256). We vary the guidance interval [0, τ ] and scale w. VAE-only applies CFG only to the VAE latents; otherwise CFG is applied to both VAE latents and VFM representations. INTERVAL VAE-ONLY FID SFID IS PREC. REC. [0, 0.85] [0, 0.90] [0, 0.95] [0, 0.90] [0, 0.90] [0, 0.90] [0, 0.90] [0, 0.90] 2.8 2.8 2.8 2.7 2.8 2.9 2.8 2. False False False False False False False True 1.55 1.53 2.75 1.53 1.53 1.56 1.53 1. 4.30 4.26 4.20 4.3 4.3 4.4 4.3 4.5 278.20 320.43 395.91 315.02 320.43 323.68 320.43 235. 0.77 0.78 0.82 0.78 0.78 0.78 0.78 0.76 0.66 0.65 0.60 0.65 0.65 0.65 0.65 0. A.5. Classifier-free guidance We provide more evaluation results for classifier-free guidance scales and guidance intervals. We denote by the CFG scale applied to the VAE latents and the VFM representations, and use VAE-Only to refer to the setting where CFG is applied exclusively to the VAE latents. We also vary the guidance interval [0, τ ], following [35]. Table 10 presents ImageNet 256256 results for SiT-XL/2+REGLUE at 800K steps. A.6. Limited data In Figure 7, we evaluate data efficiency by training SiT-B/2 for 80 epochs on class-balanced ImageNet sets of 20%, 50%, and 100%. REGLUE consistently outperforms REG, with larger gains when data is scarce: -5.5 FID at 20% and -3.4 at 50%. This indicates that jointly modeling compact local and global VFM semantics improves robustness on data-limited regimes. B. Additional Experimental Setup B.1. Semantic compressor details Architecture settings. The compression model is lightweight convolutional autoencoder composed of the semantic compressor, which encodes the high-dimensional VFM features into compact representation and the semantic de-compressor, which symmetrically decodes them back to their original space. The detailed architecture is presented in Figure 3. The semantic encoder is composed of three main components: an input layer, middle block, and an output layer. The input layer is 33 convolutional layer (3072256), where 3072 corresponds to the number of input channels from the concatenated VFM features and 256 denotes the hidden size. The middle block is residual block (ConvBNReLUConvBN, 256 channels, identity skip) that preserves spatial shape. The output layer is convolutional layer (25616) that projects the representation to 16 compressed channels. symmetric semantic 5.5 40 30 20 ) ( F 3.4 2.3 10 20 50 ImageNet (%) 100 REG REGLUE (ours) Figure 7. Dataset pruning on ImageNet. FID on ImageNet 256256 for SiT-B/2 trained for 80 epochs on class-balanced subsets (20%, 50%, 100% of ImageNet). REGLUE consistently outperforms REG, with improvements of 5.5, 3.4, and 2.3 FID at 20%, 50%, and 100%, respectively. de-compressor mirrors this design (162563072). The model is fully convolutional, preserves the spatial resolution, and is trained with an MSE reconstruction loss; at inference we retain only the encoder to provide compact local semantics. Optimization settings. We train the semantic compressor for 25 epochs with an MSE reconstruction loss between the concatenated multi-layer VFM features and their decoded counterparts. We use Adam [29] with learning rate of 1 103, (β1, β2) = (0.9, 0.999), batch size 4096, and no weight decay. The learning rate decays with cosine schedule to final value of 8.5 104. Figure 8 plots the training curve: the loss decreases smoothly and plateaus by the final epoch, indicating stable convergence. The model is lightweight; full run finishes in under one hour on 8A100 GPUs. 14 ) ( g i 0.93 0.80 0.73 0. 1 5 10 15 Epochs 20 25 Figure 8. Compressor training. Training curve showing MSE loss over epochs. The compression model utilizes 4 last DINOv2B layers. The Input layer is 256 and the compression is to 16 channels. run finishes in less than one hour on 8A100 GPUs. B.2. SiT details Architecture settings. We adopt the official SiT configurations [44]. The base SiT-B/2 (132M params) uses 12 transformer blocks with embedding dimension 768 and 12 attention heads. The larger SiT-XL/2 (677M params) uses 28 blocks with embedding dimension 1152 and 16 heads. Table 11. SiT optimization settings."
        },
        {
            "title": "OPTIMIZATION",
            "content": "Batch size Optimizer Learning Rate (β1, β2)"
        },
        {
            "title": "INTERPOLANTS",
            "content": "αt σt Training objective Sampler Sampling steps 1 v-prediction Euler-Maruyama 250 Optimization settings. We use AdamW [29, 42] with constant learning rate of 1 104, (β1, β2) = (0.9, 0.999), and batch size of 256 for both SiT models. To speed up training, we use mixed-precision (fp16) with gradient clipping. We also pre-compute image latent using SD VAE [56]. The training objective is prediction, and we use the EulerMaruyama sampler with 250 steps, defining the interpolants as αt = 1 and σt = t. We provide the optimization details in Table 11. 15 C. Limitations and Future Work As shown in Table 2 and Table 3 in the main paper, REGLUE consistently improves sample quality under comparable training budgets and reaches or surpasses strong baselines in substantially fewer iterations. However, due to resource constraints, we restrict SiT-XL/2+REGLUE experiments to 1M iterations and do not explore full convergence at ultralong schedules (e.g., 4M iterations). On our available compute (8A100 GPUs), single 1Miteration SiT-XL/2 run requires roughly 7 days, making exploration of such long schedules impractical. Higherresolution ImageNet 512512 experiments are an interesting next step as well; in this work, we instead prioritize configurations that we consider also interesting and practical, such as limited-data regimes (see Figure 7). Beyond scaling, our results point to several promising extensions. First, swapping DINOv2 for DINOv3 yields further gains (Table 8 and Table 1 in the main paper), suggesting that stronger VFMs could enhance REGLUE. Second, while we currently include the raw global [CLS] token, learning compact global compressor (analogous to our spatial one) may better balance globallocal capacity within our joint modeling framework. D. Baseline Generative Models We briefly summarize the baselines used in our comparisons. Autoregressive baselines include VAR [63], which progressively predicts fine image details from coarse inputs across multiple scales; MagViTv2 [72], which removes lookup tables in quantization to support much larger token vocabularies; and MAR [38] which avoids vector quantization altogether in an autoregressive setup. in compact diffusion performs For latent diffusion models, we consider LDM [56], latent which space; DiT [50], transformer-based architecture; U-ViT-H/2 [3] ViT-based diffusion model with skip connections; MaskDiT [80], which adds maskreconstruction auxiliary task; MDT [19], which employs an asymmetric masked latent modeling scheme; SD-DiT [82], which augments MaskDiT with momentum-encoderbased discrimination loss; SiT [44], which recasts the DiT backbone within continuous-time interpolant framework; and FasterDiT [70], which accelerates training through velocity-supervised objectives. Finally, among methods that explicitly use visual representations, we include REPA [73], which aligns diffusionmodel features with local VFM features; ReDi [33] which linearly compress VFM features and directly model them in the diffusion model; and finally REG [67] which models the global VFM representation while also applying REPA-style alignment. E. Visualizations We present uncurated, class-conditional samples from SiT-XL/2+REGLUE trained for 1M steps at 256256 in Figure 9 and Figure 10. We use CFG with w=4.0. The grids illustrate both fine-grained detail (textures and object parts) and diversity within each class. Class label = Golden Retriever (207) Class label = Castle (483) Class label = Bald Eagle (22) Figure 9. Uncurated ImageNet 256 256 samples. Class-conditional generations from SiT-XL/2+REGLUE trained for 1M steps with CFG (w=4.0). Grids illustrate great fidelity and intra-class diversity. 17 Class label = Bee (309) Class label = Great Grey Owl (24) Class label = Cheeseburger (933) Figure 10. Uncurated ImageNet 256 256 samples. Class-conditional generations from SiT-XL/2+REGLUE trained for 1M steps with CFG (w=4.0). Grids illustrate great fidelity and intra-class diversity."
        }
    ],
    "affiliations": [
        "IIT, National Centre for Scientific Research Demokritos",
        "University of West Attica",
        "VRG, FEE, Czech Technical University in Prague"
    ]
}