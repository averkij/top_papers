{
    "paper_title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
    "authors": [
        "Shengpeng Ji",
        "Tianle Liang",
        "Yangzhuo Li",
        "Jialong Zuo",
        "Minghui Fang",
        "Jinzheng He",
        "Yifu Chen",
        "Zhengqing Liu",
        "Ziyue Jiang",
        "Xize Cheng",
        "Siqi Zheng",
        "Jin Xu",
        "Junyang Lin",
        "Zhou Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a margin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . e [ 1 8 5 5 9 0 . 5 0 5 2 : r WavReward: Spoken Dialogue Models With Generalist Reward Evaluators Shengpeng Ji Tianle Liang Yangzhuo Li Jialong Zuo Minghui Fang Jinzheng He Yifu Chen Zhengqing Liu Ziyue Jiang Xize Cheng Siqi Zheng Jin Xu Junyang Lin Zhou Zhao Zhejiang University & Alibaba Group"
        },
        {
            "title": "Abstract",
            "content": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1% to 91.5%. In subjective A/B testing, WavReward also leads by margin of 83%. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted."
        },
        {
            "title": "Introduction",
            "content": "Spoken dialogue models [1] represent one of the most direct methods of human-computer interaction, evolving from traditional voice assistants such as Alexa, Siri, and Google Assistant to the latest intelligent dialogue systems, such as GPT-4o-audio3. Early spoken dialogue models [2, 3] were typically comprised of automatic speech recognition (ASR) [4, 5], large language models (LLMs) [6, 7, 8], and text-to-speech (TTS) [9, 10, 11, 12, 13, 14, 15] components, which facilitated dialogue through text-based cascading process that bridged speech input and output. To reduce latency and mitigate the cumulative errors of cascading systems, understand and generate non-textual paralinguistic information (e.g., emotion and sound) for real-time interaction, end-to-end spoken dialogue models [16, 17, 18, 19, 20, 21] such as GPT-4o-audio and Moshi [22] have attracted Work done during internship at Alibaba Qwen Team. Corresponding author. 3https://openai.com/index/chatgpt-can-now-see-hear-and-speak/ Preprint. Under review. Figure 1: WavReward can be applied to evaluate various dialogue scenarios, including both explicit instruction and implicit dialogues. It directly accepts speech-to-speech dialogue as input, evaluating the conversational coherence at both the textual and acoustic levels, and providing the final score. considerable attention in both academic research and industry. By leveraging vast amounts of speech data [23, 24] for multi-stage training, these end-to-end spoken dialogue models [21, 22, 25] not only retain the intelligence quotient of text-based language models but also exhibit emotional quotient, they are capable of handling diverse speech-related dialogue scenarios, such as role-playing, emotional dialogue, paralinguistic understanding, and paralinguistic controllable generation. End-to-end spoken dialogue models [21, 22] have demonstrated remarkable conversational abilities, validating the potential of the speech modality in advancing toward Artificial General Intelligence. Thus, assessing the intelligence quotient and emotional quotient of end-to-end spoken dialogue models is key challenge. This evaluation task involves three main challenges: 1) the understanding and generation of substantial non-textual acoustic information (e.g., emotion, accent, pitch and sound) often present in dialogue scenarios, which is currently not well-supported by any dedicated evaluation datasets of dialogue benchmarks [26, 27, 28, 29]. 2) Dialogue is inherently multi-dimensional and multi-label. For example, responses from spoken dialogue models may vary in speech rate either faster or slower, without singular correct answer during casual conversations. 3) Non-textual information in dialogue is often implicit. For instance, when user return home late exhausted after work, an intelligent spoken dialogue model should be able to recognize the fatigue from users voice and respond with gentle, empathetic tone. Current benchmarks for evaluating spoken dialogue models, such as VoiceBench [26], AirBench [29], VoxDialogue [28] and SDEval [27] primarily focus on the accuracy of textual information in dialogue, similar to using models like ChatGPT to assess the coherence of conversational text. Evaluation of non-textual information is limited to fixed tasks, such as emotion classification, gender recognition, and audio event detection, which assess the models understanding of the acoustic information in the dialogue. To address the gap in evaluating end-to-end spoken dialogue models, we propose the WavReward model and the ChatReward-30K dataset. WavReward is novel framework where audio language 2 models [30, 25] (speech-to-text) can serve as evaluators for end-to-end spoken dialogue models [16, 22]. As shown in Figure 1, WavReward can directly assess the capabilities of spoken dialogue models in both textual and non-textual acoustic dimensions. We demonstrate that fine-tuning audio language models with multiple examples via reinforcement learning [31, 32, 33, 34] enables WavReward to provide reasonable scores across various scenarios. Furthermore, incorporating chainof-thought reasoning [35, 36, 37] into the evaluation process of audio language models significantly aids WavReward in generating more accurate scores. To augment the discriminative capability of WavReward across diverse dialogue contexts, WavReward includes the nonlinear reward mechanism and the positive-negative multi-sample sampling mechanism in the post-training reinforcement learning phase. Additionally, we construct the ChatReward-30K dataset to train WavReward and evaluate the performance of various evaluators [38, 30]. ChatReward-30K not only contains standard text-centric dialogue examples but also incorporates diverse acoustic information4 from end-to-end dialogues. Each speech-to-speech dialogue sample in ChatReward-30K includes multiple responses to the same query. To our knowledge, this is the first dataset capable of comprehensively evaluating both the acoustic capabilities and the implicit conversational abilities of end-to-end spoken dialogue systems. Compared to the original audio language models and the supervised finetuned evaluators, WavReward significantly outperforms these baselines in both in-domain and out-domain scenarios. Furthermore, in human subjective A/B tests, WavReward outperforms direct inference with Qwen2.5Omni [25] by the margin of 83%. In summary, our contributions are as follows: WavReward is the first reward model specifically designed for end-to-end spoken dialogue models. It accepts speech-to-speech dialogues as input and provides corresponding scores for wide range of dialogue scenarios. WavReward demonstrates that audio language models can serve as effective evaluators for spoken dialogue models. WavReward further enhances the evaluative capability through the reasoning-based assessment process, nonlinear reward feedback, and the positive-negative diverse sample sampling mechanism during the reinforcement learning post training. We introduce ChatReward-30K, the first dataset designed for training and evaluating audio feedback models. Compared to previous datasets, ChatReward-30K enables comprehensive evaluation of both the acoustic information and implicit dialogue capabilities."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Spoken Dialogue Models Spoken dialogue models refer to large language models [8, 7] capable of engaging in conversations through both speech input and speech output. Traditional spoken dialogue models, such as AudioGPT [2] and FunAudioLLM [3], typically employ three-stage cascading approach to facilitate dialogue. In this process, speech input is first transcribed into text using an automatic speech recognition model [4]. The transcribed text is then processed by text-based LLM such as ChatGPT, to generate textual response, which is subsequently converted back into speech using text to speech model [39, 40, 41]. However, these cascaded models often suffer from issues such as high latency, cumulative errors, and an inability to process non-textual acoustic information, which limits their effectiveness. Consequently, end-to-end spoken dialogue models [22, 42, 43] have garnered significant attention in recent months. These models eliminate the need for transcription into text and directly process speech using either semantic [4, 5, 39] or acoustic representations [44, 45, 46] for understanding and generation. For instance, LLaMA-Omni [17] utilizes Whisper encoder combined with an adapter to process speech, and generates corresponding Hubert tokens based on the LLM, which are then upsampled to produce speech. IntrinsicVoice [47] introduces GroupFormer to optimize the structure of Hubert token generation, while Mini-Omni1/2 [16, 18] employs delay-pattern approach [48] to directly generate the corresponding SNAC [49] acoustic tokens. Other similar end-to-end spoken dialogue models include SLAM-Omni [20], Freeze-Omni [19], VITA1.5 [50], OpenOmni [51]. Concurrently, numerous end-to-end spoken dialogue models such as GLM-4-Voice [52], Moshi [22], Qwen2.5-Omni [25], MinMo [21], and Kimi-Audio [53] have demonstrated significant intelligence quotient and emotional quotient emerging from large-scale speech training datasets. Although these spoken dialogue models exhibit strong conversational 4gender, age, language, accent, pitch, speed, volume, emotion and audio 3 performance, there remains substantial gap in the assessment of both intelligence quotient and emotional quotient. In this paper, we present the first reward model WavReward specifically designed for the evaluation of spoken dialogue models. 2.2 Benchmark for Spoken Dialogue Models Early benchmarks related to spoken language, such as AudioBench [54], SUPERB [55], and MMAU [56], primarily focus on evaluating fixed tasks such as emotion recognition, and are not well-suited for assessing models conversational abilities. With the rapid development of end-to-end spoken dialogue models [22, 21], numerous new benchmarks have emerged to evaluate these spoken dialogue models. AirBench [29] leverages ChatGPT to evaluate the differences between generated text of speech-to-text dialogue models [30] and ground truth text at the text level. SpokenWOZ [57] transcribes the audio of the conversation into text via ASR models, and then uses metrics like BLEU to assess the performance of text-based language models. VoiceBench [26] transcribes the dialogue audio from speech-to-speech dialogue models [22, 16] into text and utilizes ChatGPT to evaluate the models general knowledge and instruction-following ability. VoxDialogue [28] and SD-Eval [27] further focus on the ability of speech-to-text dialogue models [58, 30] to understand paralinguistic information, using BLEU and other text-based metrics in conjunction with ChatGPT to assess whether speech-to-text dialogue models [38] can generate different textual responses based on varying acoustic information from different users. However, the aforementioned benchmarks still rely on transcribing audio into text for evaluation and cannot directly assess the acoustic coherence in speech-to-speech dialogues. For example, when user returns home tired after long day, and the spoken dialogue model responds with cheerful tone mocking the user, \"mocking with cheerful tone\" cannot be directly evaluated by text-based models such as ChatGPT. WavReward is the first evaluation model that accepts speech input and can directly assess the acoustic dialogue between the user and the spoken dialogue model. It can handle diverse range of acoustic information, multi-label scenarios, and implicit dialogues scenarios, directly evaluating the realism of the acoustic interactions. In addition, ChatReward-30K is the first comprehensive dataset supporting the evaluation of paralinguistic understanding and generation, as well as implicit dialogue scenarios."
        },
        {
            "title": "3 Method",
            "content": "3.1 WavReward As shown in Figure 2, WavReward is an audio language model [25] that undergoes post-training fine-tuning through reinforcement learning [31, 32, 33, 59]. In contrast to text-based large language models (LLMs) such as ChatGPT, audio language models [30, 25] can directly accept speech-tospeech dialogue as input, enabling comprehensive evaluation of the coherence of both textual content and acoustic information in explicit and implicit dialogue scenarios. Similar to the conclusions drawn from reinforcement learning in text-based LLMs [33], we find that fine-tuning with small number of precise dialogue scoring samples via reinforcement learning significantly outperforms direct supervised fine-tuning. The relevant ablation results are presented in Table 2. In the reward models of text-based LLMs, the primary task is to assess whether the content of question-answer pairs is reasonable, typically by sampling and providing feedback based on single QA sample. However, in the speech dialogue, both the input and output contain abundant content and complex acoustic information. Single-sample QA feedback is insufficient for the reward model to effectively compare differences at various levels(content and acoustic). Therefore, we design multi-sample feedback mechanism in WavReward, as shown in the top-left corner of Figure 2. For each dialogue scenario, we construct multiple answer-score pairs {aj, sj} at different levels for the given question q. The first level s1 represents the content of answer that is deemed unreasonable and receives the lowest score. The second level s2 evaluates the acoustic mismatch (e.g., when the user requests the spoken dialogue model to introduce U.S. history in happy tone, but the model responds in an angry tone). Only when both the content and the acoustic information are correct will the dialogue receive the highest score s3. Therefore, the input and target for WavReward during the training process are as follows: = concat (q, aj) , = sj, 1 sj 5, {1, 2, 3} (1) 4 Figure 2: The overall structure of WavReward. WavReward directly accepts speech-to-speech dialogue audio for evaluation. The architecture is based on the audio language model and is trained using reinforcement learning on group samples. Additionally, WavReward incorporates the Chain-ofThought reasoning process (the center of the diagram), along with positive and negative multi-sample sampling in the top-left corner, and the nonlinear reward mechanism in the top-right corner. Upon receiving the speech input x, WavReward initializes two policy models Wθ and θ with θ are speech-to-text audio language models [25], where identical structures. Both Wθ and θ serves as the old policy model with frozen weights and the weights of the current training policy model Wθ remain updatable. Following the approach of DeepSeekMath [33], we employ the KullbackLeibler divergence loss to directly constrain the relationship between the reference policy model ref and the current training policy model Wθ during the early stages of training. Notably, the KL θ divergence loss LKL(Wθ, ref ) is not incorporated into the reward process of WavReward. The formulation is expressed as follows: θ LKL(Wθ, θ) = ref (oi,tx, tprompt, oi,<t) θ Wθ(oi,tx, tprompt, oi,<t) log ref (oi,tx, tprompt, oi,<t) θ Wθ(oi,tx, tprompt, oi,<t) 1 (2) where tprompt represents the text prompt for the policy model with specific examples provided in Appendix D, denotes the number of tokens, oi refers to the set of candidate outputs {o1, o2, . . . , oN } sampled by WavReward from the old policy model θ for each input x. It is important to note that each oi in WavReward is not solely score for evaluation. We further incorporate deep reasoning process by calculating the think format reward Rf (returning 5 or 0 based on compliance), which implicitly enables WavReward to analyze whether the responses ai of spoken dialogue models address the input question effectively from both content and acoustic perspectives, and subsequently assign final score p. WavReward computes the candidate rewards {r1, r2, . . . , rN } for candidate outputs by comparing the candidate scores {p1, p2, . . . , pN } with the ground truth score using the accuracy reward Ra. Considering the discrepancy between the acoustic and content information in speech dialogues (the challenge of accurately perceiving acoustic information and providing responses with appropriate acoustic features as compared to content accuracy), we design nonlinear accuracy reward Ra, as illustrated in the upper-right corner of Figure 2. When the difference between candidate score and ground score increases, the reward Ra decreases exponentially, encouraging WavReward to provide higher accuracy rewards to spoken dialogue models that exhibit both cognitive intelligence quotient and emotional quotient. The explicit 5 formulation of Ra is as follows: Ra(p, q) = (cid:40) 10 exp (cid:16) (pg)2 2σ2 (cid:17) 0 0 g 4 g > 4 (3) After obtaining candidate accuracy rewards {r1, r2, . . . , rN } through Ra, WavReward normalizes these accuracy rewards ri using the mean and standard deviation to derive the corresponding Ai: Ai = (cid:113) 1 (cid:80)N ri 1 i=1(ri 1 i=1 ri (cid:80)N (cid:80)N i=1 ri)2 (4) where Ai represents the advantage of the candidate output score pi relative to other sampled output. Following [60, 61, 62, 33], WavReward encourages the model to generate responses with higher advantages within the group by updating the policy model Wθ using the following objective JW avReward(θ), where ϵ and β are hyper-parameters: JW avReward(θ) = E[x (X), {oi}N 1 (cid:88) i=1 1 oi (cid:40) oi (cid:88) t=1 min i=1 θ (Ox)] (cid:20) Wθ(oi,tx, oi,<t) θ (oi,tx, oi,<t) Ai,t, clip βDKL[WθW ref θ ] (cid:41) 3.2 ChatReward-30K 3.2.1 The Overall of ChatReward-30K (cid:18) Wθ(oi,tx, oi,<t) θ (oi,tx, oi,<t) (cid:19) (cid:21) , 1 ϵ, 1 + ϵ Ai,t (5) Given the absence of end-to-end dialogue datasets incorporating scores, we have developed and made available dataset called ChatReward-30K, which contains spoken dialogue data across various scenarios along with corresponding scores. As shown in Table 1, ChatReward-30K demonstrates comprehensive coverage compared to existing evaluation datasets [28, 27] for spoken dialogue models in the following key areas. 1) Evaluation from both content and acoustic dimensions. Unlike previous datasets [26], ChatReward-30K evaluates dialogue performance from both content and acoustic perspectives, encompassing wide range of paralinguistic features, including gender, age, language, accent, pitch, speed, volume, energy, emotion and audio. 2) Inclusion of both understanding and generation. Previous datasets like Voxdialogue and SD-Eval primarily focus on the understanding component (speech-to-text) of spoken dialogue systems. In contrast, ChatReward-30K also evaluates the generation component, providing scenarios that assess how dialogue models generate speech in specific tones, such as speaking in the sad manner. 3) End-to-end implicit dialogue inclusion. To further assess the emotional intelligence of spoken dialogue models, ChatReward-30K includes implicit dialogues across variety of scenarios. For instance, it includes scenario where voice assistant offers gentle, empathetic comfort at slow speech rate when the user is crying due to criticism from their boss. 4) Inclusion of both positive and negative examples. To better train the WavReward model, as outlined in Equation 1, ChatReward-30K features different dialogue responses for the same user scenario, providing both positive and negative examples to facilitate more effective model training. 5) Human expert scoring. Each dialogue scenario in ChatReward-30K is accompanied by human expert ratings, ensuring that the scores reflect reasonable and well-founded assessments of dialogue quality. 3.2.2 Dataset Statistics ChatReward-30K consists of the total of 30000 samples, each dialogue sample represents the simulated user-chatbot interaction in the form of the speech-to-speech pair. Each dialogue is rated by human experts on scale from 1 to 5, with the duration of each dialogue audio ranging from 5 to 35 seconds. ChatReward-30K is primarily divided into four components. 15% of the ChatReward-30K focuses on the textual aspects of the conversation. Based on the coherence of the dialogue content, 6 Table 1: Comparison of different evaluation dataset/benchmark for spoken dialogue models. Dia. refers to spoken dialogue and pure question-answering evaluation is not categorized as the dialogue (chat) task. S2S. denotes evaluation of speech-to-speech models. Imp. indicates implicit dialogues. Neg. and Sco. represent whether all positive and negative samples in the evaluation data are scored. As there is currently no dedicated dataset for training reward models, all datasets in this area are empty. Acoustic Information covers aspects like age, accent (acc.), gender (gen.), language (lan.), emotion (emo.), volume (vol.), speech rate (spe.), pitch (pit.) and audio (aud.). Acoustic Paralinguistic Information emo. gen. spe. lan. pit. vol. aud. Dataset/Benchmark Dia. S2S. Neg. Imp. Sco. age. acc. SUPERB MMAU AudioBench AirBench SD-Eval VoiceBench VoxDialogue ChatReward-30K another 25% of ChatReward-30K addresses the explicit understanding of user paralinguistic features such as recognizing when child is interacting with the spoken dialogue model. The remaining 35% of ChatReward-30K pertains to the models generation ability of paralinguistic features such as adjusting the volume of the models voice upon user request. The final 25% is unique to ChatReward30K, representing implicit conversational scenarios such as the spoken dialogue models ability to automatically detect the users emotional state and respond appropriately. Detailed examples can be found in Appendix A. Following Equation 1, each dialogue sample contains both positive and negative responses for the same user input. In terms of content, the dialogues in ChatReward are more aligned with natural and daily conversations rather than explicit QA pairs. As shown in Figure 6 in Appendix E, the word cloud visualization of ChatReward-30K demonstrates prevalence of natural spoken words, such as \"cant\" which is representative of daily spoken interactions. Concerning the acoustic attributes of the dialogues, most attribute categories in ChatReward-30K exhibit relatively balanced distribution, as shown in Figure 7 in Appendix E. Given the subtle emotional cues that humans can perceive in dialogue models, ChatReward-30K assigns particular emphasis to emotional attributes. Detailed information on each acoustic category is provided in Appendix B. The ChatReward dataset is ultimately split into ChatReward-30K-train and ChatReward-30K-test sets with ratios of 85% and 15% respectively. 3.2.3 Dataset Construction Process Stage1: Dialogue Text Generation. We begin by utilizing the GPT-4 [6] large language model to generate the text portion of the ChatReward-30K dataset through prompt engineering [63]. To ensure the diversity of the dialogue content, we dynamically embed various topics, such as daily life, health management, education, entertainment, family relations, dietary culture, healthcare, shopping, internet usage, fitness, career development, and social interaction during the text generation process. To generate explicit instruction-based dialogue data, we instruct the language model to generate dialogues that contain various metalinguistic information. For implicit dialogue data, we require the language model to annotate the generated conversation texts with associated metalinguistic labels. In alignment with Equation 1, the ChatReward-30K dataset simulates dialogues between the same user and different model responses. This prompt template is as follows: ModelA text must be relevant, high-quality content matching the required < label_question > (where < label_question > represents the expected metalinguistic label of the question); ModelB text must be relevant, highquality content (similar quality to A), but using the < label_b > (where < label_b > represents deliberately incorrect metalinguistic label used in the B-type response); ModelC text must be irrelevant and incorrect content (ensure that is highly diverse and unique each time). We then assign the score of 1 to dialogues with incorrect content, the score of 3 to dialogues with incorrect metalinguistic labels, and the score of 5 to dialogues with both correct content and metalinguistic labels. We observed that the prompt template significantly influences the quality and diversity of the generated dialogue. Therefore, we had human experts continuously adjust the prompt templates 7 based on small-scale dataset before further scaling up. Prompt programming templates can be found in Appendix C. Stage2: Dialogue Speech Generation. In the generation process, we carefully tailor the most suitable SOTA TTS models for each attribute. We designed customized voice dialogue synthesis pipelines for each attribute to ensure the synthesized dialogue data accurately matches the corresponding attributes: 1) Accent, Pitch, and Emotion: we utilize GPT-4o-mini-TTS to generate conditionally based speech by adjusting stylistic instructions. This tool focuses on speech techniques such as tongue-twisting, pauses, breathing, and whispering to accurately produce accents and emotions. Based on ten built-in speaker timbre, the model is instructed to synthesize speech using the following command format: Repeat this sentence with the <emotion>/<accent>/<pitch> of <example>. 2) Age: we randomly selected 1000 speaker [64] samples from four age groups as reference voices. To minimize textual content discrepancies across different cloned voices, we selected cloned samples with different tones but identical dialogue content for four distinct age groups and used Step-Audio-TTS-3B [65] for voice cloning. 3) Speed, Volume, Gender, and Language: we use CosyVoice2 [40] to synthesize speech with specified voice characteristics. The volume and speech rate are adjusted using correlation coefficients to achieve the desired attributes. 4) Audio: we combine instruct speech clips with audio clips together. Specifically, we selected 39 categories from the AudioCaps [66] dataset (500 samples per category) that include various audio events. Given that current dialogue models generally do not support music-related conversations, while audio events such as coughing, laughter, and crying are integral components of everyday interactions, our acoustic information category includes audio but excludes music. After synthesizing all the speech segments, we concatenate the simulated user speech segments with the simulated model response speech segments, ensuring 1-second silence gap between them. Step3: Data Filtering and Scoring. We used the Whisper-Large-V3 [67] model to filter out all sentences with the WER greater than 5%. Given the large volume of emotional speech and the ambiguity in category boundaries, we utilized the Emotion2Vec [68] model to filter out audio with inaccurate emotional labels and removed synthetic speech with scores below 0.5. To further improve the quality of the ChatReward-30K dataset, we invited five human experts to manually verify and adjust the text, speech, and scoring results of the dataset."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment setup Datasets. Since there is currently no dataset available for training and evaluating evaluators for spoken dialogue models [16], we use ChatReward-30K-train as the training set for WavReward, with the ChatReward-30K-test subset reserved for testing. We evaluate the models using the ChatReward30K-test (4000 samples) across three aspects: content, explicit paralinguistic understanding and generation (with 9 distinct paralinguistic features), and implicit dialogue. Additionally, we record 120 real human-machine dialogues between users and LLaMA-Omni [17] (overall biased negative samples) and Kimi-Audio [53] (overall biased positive samples), named the RealDialogue to compare the performance of different evaluators in more realistic out-of-domain settings. In the RealDialogue dataset, we observed that certain dialogues have extended durations, and there are instances of poor audio quality, such as distorted electronic sounds. These factors present more rigorous challenge for evaluating the models performance in unseen, real-world scenarios. Baselines. Similar to using ChatGPT for assessing the coherence of text-based dialogues, we employ various audio language models [30] (speech-to-text) as baseline evaluators to score speech-to-speech dialogues. The specific audio language models include Qwen-Audio [58], SALMONN [38], Audio Flamingo2 [69], Qwen2-Audio [30], Qwen2.5-Omni [25] and GPT-4o-audio. Furthermore, we enhance two new versions by fine-tuning Qwen2.5-Omni using both full-parameter and LoRA [70] fine-tuning methods on the WavReward-30K-train dataset. We utilized the official modelscopes fine-tuning framework ms-swift [71] for this process. To ensure fair comparison, all supervised fine-tuned models were trained for the same steps. Training details. WavReward is trained using 8 NVIDIA H20 96GB GPUs, each running batch size of 1, with gradient accumulation performed every 2 steps. The model is trained for 3500 steps with learning rate of 1 106 and temperature of 1.0. The maximum number of cot tokens is set to 5120, and the weight coefficient for the KL loss is set to 0.01. The model architecture of WavReward is based on the sota open-source audio language model Qwen2.5-Omni-7B-Think [25] 8 with the identical parameters. All parameters of WavReward are updated during the training process. Metric. For evaluation on the ChatReward-30K-test and RealDialogue, we use accuracy to measure the difference between the predicted scores and the ground truth (GT) scores. On the RealDialoguetestset, we conduct subjective A/B testing via crowdsourcing, where 5 human experts are required to select the optimal score between the different scores given by WavReward and various baseline evaluators in the same real dialogue. 4.2 Main Results Table 2: The accuracy of scoring by WavReward and various baselines on the ChatReward-test and RealDialogue datasets is evaluated. Specifically, the ChatReward-test dataset is assessed across three main dimensions: content scoring, acoustic instruction dialogue scoring (which includes both understanding and generation), and implicit dialogue scoring. The acoustic information which are categorized as follows: age, accent (acc.), gender (gen.), language (lan.), emotion (emo.), volume (vol.), speech rate (spe.), pitch (pit.), and audio (aud.). Model Content age. acc. lan. Acoustic Instruction pit. emo. gen. spe. vol. aud. Implicit RealDialogue I. Baseline audio langauge models direct inference with prompt 24.5 23.0 34.8 25.4 57.7 50.0 Qwen2-Audio Qwen-Audio SALMONN Audio Flamingo2 GPT-4o-audio Qwen2.5-Omni 24.7 43.4 13.5 22.7 69.4 75.1 32.4 35.5 33.9 20.8 92.0 56.3 II. Baseline audio langauge models after supervised fine-tuning Qwen2.5-Omni w/ Full-param tuning Qwen2.5-Omni w/ Lora 37.4 63.8 69.6 43.6 61.4 81. 36.8 33.6 28.4 16.8 100 48.4 27.6 14.9 33.9 18.8 82.1 54.1 33.7 34.2 36.4 18.6 58.5 57.8 32.3 27.7 25.4 17.8 88.7 66.1 40.5 35.4 30.3 21.5 94.5 34.1 41.4 39.0 28.0 21.9 88.1 53. 50.8 32.2 51.3 20.6 83.3 64.9 64.2 100 67.1 82.1 66.5 49.1 59.8 74.1 57.0 83. 74.4 85.1 54.6 85.7 III. Different ablation versions of WavReward 84.2 85.3 88.6 WavReward w/o cot think WavReward w/o multi samples WavReward w/o nonlinear reward WavReward (ours) 90. 80.3 85.7 92.2 96.9 77.9 69.6 80.9 87.7 98.9 98.9 100 86.9 88.6 89.8 95.5 85.4 90.0 90.5 97.5 80.7 82.1 83.8 89. 86.0 88.6 87.3 91.1 90.2 85.4 92.7 97.6 88.6 90.2 85.7 97. 28.9 35.2 20.3 22.6 53.6 48.5 42.2 54.2 61.4 61.9 66.6 74.3 42.5 40.8 19.2 21.6 57.6 51.7 37.5 56. 59.1 72.5 70.8 80.8 We evaluated the generation and ground-truth score accuracy of WavReward and Baseline models on the ChatReward-30K-test as well as the real out-of-domain RealDialogue dataset. The Baseline is divided into two categories: one consists of direct inference from audio language models using text prompt templates consistent with WavReward, and the other is the evaluator fine-tuned using the ChatReward-30K-train. The specific experimental results are presented in Table 2. We can draw the following conclusions: 1) WavReward significantly outperforms the best audio language models GPT-4o-audio on all metrics. It achieved improvements of 21.4, 20.7, and 39.0 points in the content scoring, implicit dialogue scoring, and emotion-instructed dialogue scoring, respectively. Furthermore, it outperformed the direct inference model Qwen2.5-Omni by an average factor of two. This indicates that audio language models when optimized using reinforcement learning, can effectively serve as evaluators for spoken dialogue models. Moreover, RL significantly improves performance compared to direct inference. 2) We found that the RL-based WavReward surpassed the LoRA fine-tuned Qwen2.5-Omni. This may be due to the direct scoring approach of supervised fine-tuning, which is overly simplistic and struggles to capture the complex scoring logic needed for various scenarios. Additionally, LoRA-based methods outperformed full-parameter fine-tuning, likely because full-parameter fine-tuning lacks the KL loss constraint on the initial model, causing the model to deviate from its inherent capabilities. 3) We observed substantial performance gap between different audio language models during direct inference, highlighting the need for future work to develop and open-source more robust foundational audio language models. 4) We found that WavRewards accuracy in scoring accent-based sub-languages and implicit dialogues was lower than that for other scenarios. This may be attributed to the less accurate accent data in the ChatReward-30K dataset compared to other acoustic information, and the inherent difficulty in evaluating implicit dialogues. There remains room for improvement in determining what constitutes reasonable emotional response in spoken dialogue models. 5) On the RealDialogue dataset, WavReward achieved score accuracy of 80%, indicating that it exhibits strong robustness and can provide reliable evaluations in real-world, complex scenarios. 9 4.3 A/B Test on RealDialogue Given that evaluating the responses of end-to-end spoken dialogue models in implicit dialogue settings constitutes multi-dimensional, multi-label scenario, where there is no single ground truth label, and the responses of dialogue models should ideally align with human subjective preferences, we have incorporated subjective A/B testing approach. Specifically, five human experts were tasked with evaluating data from RealDialogue and determining which of two distinct discriminators provided the most reasonable assessment. To ensure the validity of the subjective criteria, experts were also asked to provide justification for their choices. We conducted pairwise comparisons between three baseline: Qwen2.5-Omni w/ direct inference, GPT-4o-audio w/ direct inference, and Qwen2.5-Omni w/ LoRA. The objective was to compare the different scoring outcomes of WavReward and the baseline model on the same test set from RealDialogue, with the results presented in Table 3. Our findings indicate that WavReward outperformed Qwen2.5-Omni w/ direct inference in the subjective A/B test by margin of 83%, and also achieved 77% success rate when compared to GPT-4o-audio w/ direct inference. These results suggest that WavRewards scoring is more closely aligned with human subjective preferences, demonstrating superior performance across wide range of real-world dialogue scenarios. Table 3: Detailed statistics of the corresponding subsets of each attribute in ChatReward-30K. Models WavReward Win WavReward Lose Qwen2.5-Omni w/ direct inference Qwen2.5-Omni w/ LoRA GPT-4o-audio w/ direct inference 83 79 77 17 21 4.4 Ablation experiments w/o cot think. We removed the chain-of-thought (CoT) reasoning from WavReward and referred to this version as WavReward w/o CoT think. Specifically, WavReward in this configuration directly generates scores without the additional CoT-based format reward (loss) and the corresponding tprompt was also modified as shown in Appendix D. All other training and model parameters remain unchanged. As shown in Table 2, we found that CoT reasoning improved accuracy by approximately 10% across all evaluation categories. In out-of-domain scenarios, the improvement was as high as 21.7%. This suggests that reasoning capabilities are beneficial for the evaluator model. w/o nolinear reward. We replaced the reward function in Equation 3 with classic linear 0/1 reward function [61, 62]. Specifically, the WavReward w/o nolinear reward version receives the reward of 5 when the generated score matches the ground truth score, and no reward is given when there is mismatch during training. All other training and model parameters are consistent with the previous configuration. By comparing the versions of WavReward and WavReward w/o nonlinear in Table 2, we observe that the non-linear reward function aids WavReward in learning the differences in various levels of information in speech. For instance, when there is large discrepancy between the GT score and the predicted score (e.g., high emotional intelligence response receives low score of 1 from WavReward), substantial penalty is applied which helps the model correct such errors. w/o multi samples. In classical reinforcement learning algorithms [32], single-sample sampling can be used to calculate rewards based on the difference between the GT score and the generated score. In the WavReward w/o multi-samples version, for each question only one randomly selected answer is used for evaluation. All other training and model parameters remain unchanged. We found that performance dropped when multi-sample evaluation was removed. This decline can be attributed to the loss of the ability to simulate range of reasonable and unreasonable responses to the same question, which assists WavReward in distinguishing between different scoring criteria and their variations."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present WavReward, the first evaluation framework capable of supporting speechto-speech input and providing comprehensive assessments of spoken dialogue models at both the text and acoustic levels. WavReward leverages reinforcement learning to turn audio language models 10 into evaluatorsn and incorporate the chain-of-thought reasoning process, nonlinear rewards, and both positive and negative sample feedback to enhance the validity of the evaluation. In variety of in-domain and out-of-domain explicit and implicit evaluation scenarios, WavReward significantly outperforms previous state-of-the-art evaluators. Furthermore, in human subjective A/B tests, it shows substantial lead with the 83% improvement. In the future, we aim to scale up audio language models (e.g., 7B-70B) to further enhance WavRewards capabilities."
        },
        {
            "title": "References",
            "content": "[1] Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, et al. Wavchat: survey of spoken dialogue models. arXiv preprint arXiv:2411.13577, 2024. [2] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech, music, sound, and talking head. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2380223804, 2024. [3] Tongyi SpeechTeam. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051, 2024. [4] Nan Cao, Yu-Ru Lin, Xiaohua Sun, David Lazer, Shixia Liu, and Huamin Qu. Whisper: Tracing the spatiotemporal process of information diffusion in real time. IEEE transactions on visualization and computer graphics, 18(12):26492658, 2012. [5] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. [6] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [8] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [9] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020. [10] Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, and Sangjin Kim. Vits2: Improving quality and efficiency of single-stage text-to-speech with adversarial learning and architecture design. arXiv preprint arXiv:2307.16430, 2023. [11] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, et al. Mega-tts: Zero-shot text-to-speech at scale with intrinsic inductive bias. arXiv preprint arXiv:2306.03509, 2023. [12] Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, Shengpeng Ji, Qian Yang, Chen Zhang, Pengfei Wei, Chunfeng Wang, et al. Mega-tts 2: Boosting prompting mechanisms for zero-shot speech synthesis. In The Twelfth International Conference on Learning Representations, 2024. [13] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116, 2023. [14] Shengpeng Ji, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan, Baoxing Huai, and Zhou Zhao. Textrolspeech: text style control speech corpus with codec language text-to-speech models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1030110305. IEEE, 2024. [15] Shengpeng Ji, Jialong Zuo, Minghui Fang, Siqi Zheng, Qian Chen, Wen Wang, Ziyue Jiang, Hai Huang, Xize Cheng, Rongjie Huang, et al. Controlspeech: Towards simultaneous zero-shot 11 speaker cloning and zero-shot language style control with decoupled codec. arXiv preprint arXiv:2406.01205, 2024. [16] Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. [17] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. arXiv preprint Llama-omni: Seamless speech interaction with large language models. arXiv:2409.06666, 2024. [18] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities, 2024. [19] Xiong Wang, Yangze Li, Chaoyou Fu, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm, 2024. [20] Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, Zhikang Niu, Yanqiao Zhu, Yifan Yang, Zhanxun Liu, et al. Slam-omni: Timbre-controllable voice interaction system with single-stage training. arXiv preprint arXiv:2412.15649, 2024. [21] Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, et al. Minmo: multimodal large language model for seamless voice interaction. arXiv preprint arXiv:2501.06282, 2025. [22] Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. [23] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, et al. Emilia: large-scale, extensive, multilingual, and diverse dataset for speech generation. arXiv preprint arXiv:2501.15907, 2025. [24] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, PierreEmmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 76697673. IEEE, 2020. [25] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [26] Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024. [27] Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, and Zhizheng Wu. Sd-eval: benchmark dataset for spoken dialogue understanding beyond words. arXiv preprint arXiv:2406.13340, 2024. [28] Xize Cheng, Ruofan Hu, Xiaoda Yang, Jingyu Lu, Dongjie Fu, Zehan Wang, Shengpeng Ji, Rongjie Huang, Boyang Zhang, Tao Jin, et al. Voxdialogue: Can spoken dialogue systems understand information beyond words? In The Thirteenth International Conference on Learning Representations, 2025. [29] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, et al. Air-bench: Benchmarking large audio-language models via generative comprehension. arXiv preprint arXiv:2402.07729, 2024. [30] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 12 [34] Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, and Jian Luan. Reinforcement learning outperforms supervised fine-tuning: case study on audio question answering. arXiv preprint arXiv:2503.11197, 2025. [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [36] Zhifei Xie, Mingbao Lin, Zihang Liu, Pengcheng Wu, Shuicheng Yan, and Chunyan Miao. Audio-reasoner: Improving reasoning capability in large audio language models. arXiv preprint arXiv:2503.02318, 2025. [37] Ziyang Ma, Zhuo Chen, Yuping Wang, Eng Siong Chng, and Xie Chen. Audio-cot: Exploring chain-of-thought reasoning in large audio language model. arXiv preprint arXiv:2501.07246, 2025. [38] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. [39] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [40] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. [41] Shengpeng Ji, Ziyue Jiang, Hanting Wang, Jialong Zuo, and Zhou Zhao. Mobilespeech: fast and high-fidelity framework for mobile zero-shot text-to-speech. arXiv preprint arXiv:2402.09378, 2024. [42] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. [43] Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, and Yang Feng. Llama-omni2: Llmbased real-time spoken chatbot with autoregressive streaming speech synthesis. arXiv preprint arXiv:2505.02625, 2025. [44] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. [45] Shengpeng Ji, Ziyue Jiang, Wen Wang, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Xize Cheng, Zehan Wang, Ruiqi Li, et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. [46] Shengpeng Ji, Minghui Fang, Ziyue Jiang, Rongjie Huang, Jialung Zuo, Shulei Wang, and Zhou Zhao. Language-codec: Reducing the gaps between discrete codec representation and speech language models. arXiv preprint arXiv:2402.12208, 2024. [47] Xin Zhang, Xiang Lyu, Zhihao Du, Qian Chen, Dong Zhang, Hangrui Hu, Chaohong Tan, Tianyu Zhao, Yuxuan Wang, Bin Zhang, et al. Intrinsicvoice: Empowering llms with intrinsic real-time voice interaction abilities. arXiv preprint arXiv:2410.08035, 2024. [48] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36:4770447720, 2023. [49] Hubert Siuzdak, Florian Grötschla, and Luca Lanzendörfer. Snac: Multi-scale neural audio codec. arXiv preprint arXiv:2410.14411, 2024. [50] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025. [51] Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, et al. Openomni: Large language models pivot zero-shot omnimodal alignment across language with real-time self-aware emotional speech synthesis. arXiv preprint arXiv:2501.04561, 2025. [52] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. [53] Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. [54] Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy Chen. Audiobench: universal benchmark for audio large language models. arXiv preprint arXiv:2406.16020, 2024. [55] Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Lin, Andy Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. Superb: Speech processing universal performance benchmark. arXiv preprint arXiv:2105.01051, 2021. [56] Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. Mmau: massive multi-task audio understanding and reasoning benchmark. arXiv preprint arXiv:2410.19168, 2024. [57] Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, and Yongbin Li. Spokenwoz: large-scale speech-text benchmark for spoken task-oriented dialogue agents. Advances in Neural Information Processing Systems, 36:3908839118, 2023. [58] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. [59] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [60] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [61] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [62] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [63] Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended abstracts of the 2021 CHI conference on human factors in computing systems, pages 17, 2021. [64] Khaled Hechmi, Trung Ngo Trong, Ville Hautamäki, and Tomi Kinnunen. Voxceleb enrichment for age and gender recognition. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 687693. IEEE, 2021. [65] Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-audio: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946, 2025. [66] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. [67] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya In International Sutskever. Robust speech recognition via large-scale weak supervision. conference on machine learning, pages 2849228518. PMLR, 2023. [68] Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen. emotion2vec: Self-supervised pre-training for speech emotion representation. arXiv preprint arXiv:2312.15185, 2023. 14 [69] Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio flamingo 2: An audio-language model with long-audio understanding and expert reasoning abilities. arXiv preprint arXiv:2503.03983, 2025. [70] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [71] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2973329735, 2025. 15 Examples in ChatReward-30K Various examples from the ChatReward-30K dataset are illustrated in Figure 3. Figure 3: The ChatReward-30K dataset encompasses wide range of both explicit and implicit dialogue scenarios, with responses evaluated by human experts based on model performance. Acoustic information in ChatReward-30K The specific categories, sample quantities, and durations of all acoustic information in ChatReward30K are detailed in Table 4. 16 Table 4: Detailed statistics of the corresponding subsets of each attribute in ChatReward-30K. Attributes Categories Gender Age Language Accent Emotion Pitch Speed Volume Audio Overall male, female children, elderly, middle-aged, adolescent chinese, english indian, canadian, british, singaporean, american, australian neutral, happy, sad, angry, surprised, disgusted, fearful low, high, normal slow, normal, fast low, normal, high laughing, crying, bee, bird, car, cat, chirping, clapping, coughing, dog, screaming duck, horse, ice, knocking, ocean, pig, police, sneezing, thunder, waterfall burbling Samples Duration 2177 2070 3583 9470 853 2303 2054 9.56Hours 8.36Hours 16.23Hours 5.70Hours 52.04Hours 3.65Hours 10.95Hours 7.53Hours 4081 15.38Hours 129.40Hours Prompt Programming Template for ChatReward-30K The sample prompt template for ChatReward-30K is illustrated in the Figure 4 below: Figure 4: The emotion prompt template for ChatReward-30K. Prompt Programming Template (Without COT) for WavReward The ablation text prompt template for WavReward w/o COT is shown in Figure 5. Statistics of ChatReward-30K tatistics of different acoustic attribute in ChatReward-30K is illustrated in the Figure 7. Word Cloud of ChatReward-30K is shown in Figure 6. 17 Figure 5: The ablation prompt template for WavReward. Figure 6: Word Cloud of ChatReward-30K. Figure 7: Statistics of different acoustic attribute in ChatReward-30K."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Zhejiang University"
    ]
}