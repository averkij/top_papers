{
    "paper_title": "Persona Prompting as a Lens on LLM Social Reasoning",
    "authors": [
        "Jing Yang",
        "Moritz Hechtbauer",
        "Elisabeth Khalilov",
        "Evelyn Luise Brinkmann",
        "Vera Schmitt",
        "Nils Feldhus"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application."
        },
        {
            "title": "Start",
            "content": "Jing Yang1,2 Moritz Hechtbauer1 Elisabeth Khalilov1 Evelyn Luise Brinkmann1 1Technische Universität Berlin Vera Schmitt1,3 Nils Feldhus1,2 3CERTAIN 2BIFOLD Berlin Institute for the Foundations of Learning and Data Corresponding authors: {jing.yang,feldhus}@tu-berlin.de 6 2 0 2 8 2 ] . [ 1 7 5 7 0 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as way to steer model towards userspecific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with wordlevel rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are increasingly deployed for content recommendations and moderation on social media platforms, where transparent and trustworthy decision process is critical. However, standard one-size-fits-all decisions and explanations often fail to align with the diverse perspectives of global user base (Qiu et al., 2024; Padmakumar and He, 2024). Prior research shows that role-play/persona prompts can shift model 1Disclaimer: Contains offensive or hateful material. 1 Figure 1: Our pipeline: Datasets are combined with persona prompts, fed to LLMs, and the resulting labels and rationales are evaluated against ground truth, interpersona agreement, and human demographic groups. behavior with expert personas improving performances, but can also magnify stereotypes or biased associations learned during pretraining (Salewski et al., 2023; Hu and Collier, 2024; Gupta et al., 2024). Building on this insight, we link personaconditioned behavior to token-level explanation alignment, and focus on how LLMs internal representations of social groups shape not only the classification but the rationales they select. To probe these internal representations through their behavioral outputs, we employ persona prompting (PP). In our work, persona is controlled prompt containing specific demographic attributes (e.g., age, gender, political view). We hypothesize that these prompts elicit systematic variations in model behavior, revealing the underlying stereotypes and associations the model has learned from its training data. We quantify these variations via alignment the degree of agreement between model outputs (labels and token-level rationales) and human-annotated ground truths from different demographic groups. With three datasets of different levels of subjectivity, we systematically investigate how specific demographic prompts influence not only the final label but also the underlying tokenlevel rationales, offering more detailed view of how models internal social representations manifest in their reasoning. Our analysis highlight how PP can expose model biases and pave the way for more socially-aware system evaluations. Our main contributions are as follows: (1) We propose and execute comprehensive methodology for auditing the impact of PP on LLM reasoning, focusing on token-level rationale alignment across multiple tasks and demographic groups. (2) We demonstrate that the effect of PP is highly task-dependent: it can improve label prediction in socially-sensitive tasks like hate speech detection while having negligible or negative effects on other subjective tasks, and often shows negative effect on rationale quality. (3) We show that, despite PP, LLMs have consistent biases toward certain demographic groups and are overly safe by flagging normal as hate speech at the cost of task performance."
        },
        {
            "title": "2 Related Work",
            "content": "Our research is located at the intersection of situated explainability and persona-conditioned language models. The central premise of our work builds on the idea that single, generic explanation is insufficient for diverse user populations (Sokol and Flach, 2020). Research in human-computer interaction has consistently shown that user characteristics such as personality, demographics, and cognitive traits significantly influence how explanations are perceived, used, and trusted (Farmer and Ho, 2025; Khadar et al., 2025). This has motivated the development of frameworks for generating explanations tailored to specific user needs and contexts (Jeck et al., 2025; Qiu et al., 2024). However, researchers also caution that hyper-personalization can introduce risks, potentially leading users down rabbit holes of biased or simplified information (Nimmo et al., 2024). With the advent of LLMs, persona prompting has emerged as scalable method to generate such tailored outputs. This approach leverages the models capability to simulate diverse viewpoints (Cheng et al., 2023), technique explored for applications ranging from social science research (Argyle et al., 2023) to creating benchmarks for pluralistic alignment (Castricato et al., 2025). Several studies confirm that persona conditioning has quantifiable effect on LLM behavior (Hu and Collier, 2024; Gupta et al., 2024) and can improve the subjective quality of generated text (Padmakumar and He, 2024). Yet, this capability is double-edged sword (Kirk et al., 2024); persona prompts can also amplify model biases, reduce factual consistency, and introduce stereotypical reasoning (Kim et al., 2024; Liu et al., 2024). This is particularly concerning in sensitive domains like toxicity detection, where persona-driven outputs can vary significantly (Deshpande et al., 2023). In response, recent work has focused on the coherence and psychological grounding of LLM personas to make them more reliable (Joshi et al., 2025). While prior work establishes the importance of personalization and the risks of persona simulation, most studies focus on classification outcomes or high-level explanation quality. Our work extends this research by conducting fine-grained analysis in both the critical domain of hate speech detection and more subtle subjective tasks."
        },
        {
            "title": "3.1 Persona Simulation via Prompting",
            "content": "To simulate subjective viewpoints, we prompt models using persona-conditioned instructions that represent demographic traits, our evaluation pipeline is shown in Figure 1. We design two types of personas: (1) singleattribute persona: Each persona is defined by single attribute (e.g., Female, 15-year-old, or Muslim), drawn from one of 21 personas from seven categories: age, gender, education, race, religion, political view, and loneliness. We base our selection on the 33 attributes by Castricato et al. (2025), but we only kept those that we anticipate to substantially influence PP, as shown in, e.g., Cheng et al. (2023) (see our detailed justification on the selection in Appendix A.1). (2) composite personas: To study the alignment between personas and real human annotators from 2 different demographic groups, we select three attributes Age (Y: 25, O: 45), Gender (Male, Female) and Ethnicity (B: African American, L: Hispanic and W: Caucasian) and design another set of 12 personas (e.g., 25-year-old Caucasian Male). We choose these attributes as we can compare them with six real demographic groups annotations in BRWRR (Thorn Jakobsen et al., 2023). For each persona and data point (3.3), we construct consistent, role-based prompt. model is tasked to classify the instance while stepping into the shoes of someone with the specified background. The prompt includes the following key components (see Appendix for details): persona description, e.g., 25-year-old Caucasian male. The task description and input text. An instruction to think step-by-step incharacter using <think>...</think> tags. JSON format output: { } \"label\": \"<one of the categories>\", \"rationale\": [\"word1\", \"word2\", ...] rule to avoid breaking character. We also tested the prompt variation without the think-step-by-step reasoning instruction, which resulted in worse performances, especially in rationale selection (Appendix C.1)."
        },
        {
            "title": "3.2 Baseline Generation",
            "content": "As control group against persona-conditioned results, we generate baseline predictions using prompts that omit all persona context. These neutral prompts follow the same structure classification task, <think> step, and structured JSON output but without background information."
        },
        {
            "title": "3.3 Datasets",
            "content": "We conduct our experiments on three tasks: Hate speech detection (HateXplain), commonsense reasoning (CoS-E), and sentiment analysis (SST-2). The HateXplain dataset (Mathew et al., 2021) is specifically designed for explainable hate speech detection. It consists of over 20,000 social media posts, each annotated with one of three labels (Hate speech, Offensive language, or Normal) as well as highlighted rationales provided by three human annotators (except for normal label). To focus on interpretation-sensitive cases, we exclude inconclusive labels (3 distinct labels from 3 annotators) and samples that have less than three rationales (except when label is Normal), then randomly select subset of 500 examples from the remaining data (Normal: 301, Offensive: 75, Hate speech: 124). In BRWRR (Being Right for Whose Right Reasons), Thorn Jakobsen et al. (2023) re-annotate existing rationale datasets with annotators from six different demographic backgrounds, based on two traits: age (Old: >38, Young: <36) and ethnicity (B: African American, L: Hispanic and W: Caucasian). It allows us to demonstrate persona-driven effects on less overtly social but still subjective tasks such as sentiment analysis and commonsense reasoning. We choose the CoS-E (Rajani et al., 2019) and SST-2 (Socher et al., 2013) subsets, which contain 500 and 263 instances, respectively."
        },
        {
            "title": "3.4 Models",
            "content": "Our model selection is designed to ensure the robustness and generalizability of our findings across diverse set of modern LLMs from multiple labs. We evaluate one proprietary and two open-weight LLMs: Mistral-Medium2, Qwen3-32B3 and GPTOSS-120B4. All models are capable of role-based prompting. Mistral does not support <think> tags, so we explicit add the reasoning field besides label and rationale. Inference is performed via API calls using default parameters. Model outputs are parsed to extract the predicted label and rationale, which is mapped to binary token-level mask."
        },
        {
            "title": "3.5 Evaluation",
            "content": "We evaluate models performance on classification and rationale selection. For classification, we measure: (1) accuracy for CoS-E, and MacroF1 score for SST-2 (excluding the no sentiment class, following the same metric as BRWRR) and HateXplain. (2) Mean Absolute Error (MAE): the distance between the predicted label and ground truth. This metric is targeting HateXplain, as its labels are ordinal. Since the predictions direction is important to measure biases, we also compute Mean Error (ME). For rationale selection, we convert the rationales into binary list and compute Token-F1 and IOU-F1, following Thorn Jakobsen et al. (2023). Token-F1 computes the F1 score by considering the overlapping tokens between predicted and annotated rationales. IOU-F1 measures 2https://mistral.ai/news/mistral-medium-3 3https://hf.co/Qwen/Qwen3-32B 4https://hf.co/openai/gpt-oss-120b 3 Intersection-Over-Union between predicted and annotated rationale tokens. 5 In addition, to quantify inter-persona consistency in both classification and rationale selection, we compute Krippendorffs α for both label predictions and binary rationale arrays. We apply the nominal version of α (except for the HateXplain labels, which we consider ordinal), which is suitable for categorical data and robust to missing annotations. For rationales, we treat each token binary value as one annotation task. To have robust performance evaluation, we run each persona and baseline over three independent runs. To rigorously assess the impact of persona prompting beyond mean performance variations, we compute sample-level bootstrap confidence intervals (95% CI, 1,000 iterations) for the differences between persona and baseline performance. For more experimental details, see Appendix A.2 on exact prompts used, hyperparameters and number of completions."
        },
        {
            "title": "4.1 Evaluation on HateXplain",
            "content": "As HateXplain does not have annotators demographics, we analyze Baseline-GT and Persona-GT performance (GT: Ground-truth annotations). Label Prediction Figure 2 shows the label prediction performances of baseline (no persona) and different single-attribute personas. PP has quite different outcomes depending on the model. GPTOSS-120B personas have the least deviations from the baseline, while Mistral-Medium and Qwen332B have more variations with most Mistral personas performing better than its baseline and all Qwen3 personas performing worse than the baseline. In addition, we see that Mistral shares similar trends of performances with Qwen3 on some persona attributes: Age, Political View, Race and Religion; while all three models share similar trends on Gender and Loneliness, with better performance on Male and Not lonely personas, in comparison to Female and Somewhat lonely. Label Prediction Over-flagging We also compute the Mean Error (ME) to capture if models are biased towards Hate Speech due to guardrails 5These metrics measure the plausibility of rationales (Thorn Jakobsen et al., 2023). 6Code and data is available at: https://github.com/jingyng/PP-social-reasoning 4 Attribute group GPT-OSS-120B Mistral-Medium Qwen3-32B .61 .02 Age .54 .02 Loneliness .63 .01 Religion .52 .03 Education .64 .01 Race .66 .03 Gender .52 .03 Political .87 .00 .89 .01 .83 .01 .84 .00 .83 .01 .84 .01 .57 .00 .86 .02 .86 .01 .86 .01 .85 .02 .84 .01 .85 .02 .81 .00 Table 1: Inter-annotator agreement on label prediction among personas within the same attribute group, values computed by Krippendorffs α. (see Appendix Figure 6, which shows the MEs of different personas on label prediction). Models in general tend to exaggerate the situation as all MEs are positive, i.e., models classify statement/sentence as Offensive (O) or Hate speech (H) while the ground truth is Normal (N) or Offensive. We visualize the rate of over-flagging as heatmap in Figure 3. Overall, GPT-OSS has the highest over-flagging rate from (almost all above 50%), Mistral has in general very high over-flagging rate from both O, and H, while Qwen3 has the lowest over-flagging rate from both and (i.e., Qwen3 is the least in overflagging hate speech), but still high on O. This shows that all models over-state the severity of statements. In addition, comparing different personas, the top performing personas (e.g., Male, Not lonely, Right-wing, White, Atheist) have lower over-flagging rates. Label Agreement Across Personas Table 1 shows inter-annotator agreement among personas within the same attributes, determined by Krippendorffs α. Here, GPT-OSS exhibits high agreement for all attribute groups, with political view ranked the lowest. Mistral also shows overall satisfactory level of agreement (α 0.80). The only exception is political view (α = 0.57). For Qwen3, none of the categories reach moderate agreement threshold (α 0.67). The category that ranks lowest in agreement is political view (α = 0.52) and education, followed by loneliness, all within the poor agreement range. Overall, models show lowest agreement on political view. This high level of disagreement within the political persona group manifests as the wide performance variance seen in Figure 2, where the Right-wing persona substantially outperforms the Left-wing persona for Mistral and GPT-OSS. Rationale Selection For rationale comparison, we only consider the subset where the ground truth Figure 2: Label prediction MAE scores of baseline (no persona) and different single-attribute personas on HateXplain. Error bars represent 95% confidence intervals (CI) with bootstrapping resampling. If the line does not cross the baseline, the difference is significant. Figure 3: Over-flagging rate across different labels. N: Normal, O: Offensive, H: Hate speech. label is not Normal, since samples with this label do not have any rationales during annotation. Figure 4 depicts token-F1 results of baselines and personas. Overall token-F1 scores are high in baselines. GPT-OSS personas all fall into the same range of performance as its baseline, while Mistral and Qwen3 personas all score worse than their counterparts (except for the race attribute, which outperforms the baseline with White/Asian personas, but not significantly). The overall tendency of performance of different personas is quite similar (a wave shape) between Mistral and Qwen3 models, with some personas in Race and Religion attributes achieving slightly higher performance, and Loneliness the lowest performance. Persona Agreement on Rationales Table 2 shows the inter-persona agreement for rationale selection among personas within the same attribute group. Compared to label prediction, GPT-OSS and Mistral extracted rationales have slightly lower but still high agreements, while Qwen3 has higher rationale agreement values on the border of the tentative agreement threshold (0.67). GPT-OSS and Mistral consistently shows higher agreement than Qwen3, particularly in categories like Loneliness, Education, and Political view. The latter ranks lowest in both label and rationale agreement for all models, indicating greater bias on it. Qualitative analysis of Chain-of-Thought reasoning Finally, we extract the chain-of-thought reasoning for different personas (e.g., Left-wing 5 Figure 4: Rationale Token-F1 of baseline (no persona) and different single-attribute persona performances on HateXplain, excluding rationales from the Normal label. Error bars incidates 95% CIs, significantly different persona results are indicated with squares. Attribute group GPT-OSS-120B Mistral-Medium Qwen3-32B .68 .01 Age .66 .00 Loneliness .65 .00 Religion .66 .01 Education .66 .01 Race .68 .00 Gender .66 .00 Political .74 .01 .76 .01 .72 .01 .75 .00 .71 .00 .74 .01 .73 .01 .83 .01 .82 .01 .78 .01 .78 .00 .78 .01 .81 .00 .75 .01 Table 2: Inter-persona agreement (Krippendorffs α) on rationale selection among personas within the same attribute group. vs. Right-wing, No formal education vs. Higher education) and look for stereotypical reasoning. Table 3 provides stark example of stereotypical reasoning, where the model adopts distinct linguistic styles and analytical frameworks that directly lead to different classifications for the same input. While Higher education is leaning towards recognizing protected groups being attacked as Hate speech, both No formal education and Left-wing personas end up with the Offensive language label. The model exposed to the right-wing persona, on the other hand, marks it as Normal. In Table 4, we performed an automated linguistic analysis of the CoT reasoning. The results show clear, systematic differences across both personas and models. First, the persona prompts successfully modulated complexity; across all models, the No formal education persona generated text that was much easier to read than the Higher education persona, which consistently produced the most complex language. Second, we observed stark differences in verbosity between models, with Qwen3 generating dramatically longer reasoning on average than the more concise Mistral-Medium. While GPT-OSS uses more ellipses, Qwen3 tends to produce full sentences (see Appendix Table 14). Models are inconsistent in taking third-person meta-perspective and sometimes simulate the persona from firstperson view."
        },
        {
            "title": "4.2 Evaluation on BRWRR",
            "content": "BRWRR datasets allow us to find out whether PP improves performance for the target group which the persona aligns with. We compare the results for Baseline-Group and Persona-Group. Baseline is LLM prompting without persona information, Persona is PP with personas that have the same attribute values as the ground truth demographic Group. Figure 5 shows the label prediction and rationale selection results of Baseline and different persona performance on each demographic group, and their differences, on CoS-E and SST-2. Label Prediction As shown in Figure 5, Persona prompting overall has very small effects on models classification performance, with GPT-OSS the least affected (slightly positive), and Qwen3 most affected (negatively, especially on SST-2). Notably, PP is detrimental especially to BO (African American Old) group (especially in Mistral-Medium and Qwen3-32B). This could indicate that PP steers models to have more stereotypical behavior towards this group. For Qwen3, prompting with persona always decreases performances. The results also show that models are biased both with and without personas. All models perform much better on Old age groups, especially on African American and Caucasian (BO, WO), Persona Chain-of-Thought reasoning No form. Educ. [...] This is comment possibly mocking white South Africans, maybe implying they appear corporate while swimming. Does it contain hate speech? Potentially its comment on race and perhaps mocking. Is it \"Offensive language\"? Might be considered somewhat disdainful but not necessarily hateful. The phrase is somewhat stereotypical and might be seen as subtle derogatory statement about whites in South Africa, but not an explicit call for violence. Could be considered \"Offensive language\" due to mocking based on race. [...] Likely classify as Offensive language. Rationale list of words that triggered: \"white\", \"south africans\", \"corporate\", \"swimming costumes\"? [...] Now the think out loud as persona with no formal education. The persona should reflect lack of formal education, perhaps raw, simple worldview, noticing race references, maybe sees it as mocking. They will think about feelings, etc. [...] aint never been to school much, but can see when someone says something about group of people. This comment talks about \"white South Africans\" and says they \"look corporate\" in their swimsuits. It sounds like theyre poking fun at them, maybe judging them for being fancy. It aint direct hate thing, but its making fun of race and nationality, which feels disrespectful. So think its offensive, not totally hateful. [...] [...] The text mentions \"white South Africans\" and \"look corporate in their swimming costumes\". Could be mocking or stereotyping based on race. Its possibly disparaging or form of racism (implies white South Africans trying to maintain corporate appearance even while in swimsuits). Might be considered offensive or hateful due to targeting race? Its not direct call for violence, but its derogatory statement that stereotypes racial group. Hate speech definition might include speech that insults or degrades group based on race. Many platforms treat this as hate content. So likely \"Offensive language\" or \"Hate speech\". Considering the categories: Hate speech is more severe, usually includes incitement, threats, calls for violence. This is more mocking stereotype, likely falls under \"Offensive language\" rather than hate speech, but might still be considered hateful (racist). In typical moderation, \"white South Africans\" is protected class (race). The text is disparaging but not inciting. Some policies classify any demeaning remarks based on race as hate speech. Might be considered \"Hate speech\". Lets decide: Probably \"Hate speech\" because its targeting based on race and disparaging. Rationale: the words \"white south africans\" (targeted group) and \"still manage to look corporate\" (implies criticism). The key words that are hateful: \"white south africans\". So rationale list: [\"white\", \"south africans\"]. [...] As someone with higher education, Im used to analyzing language through lenses of social context, power dynamics, and potential harm. When read the phrase, notice it specifically calls out racial groupwhite South Africansand attaches mocking tone about their appearance. Its stereotyping comment that reduces an entire demographic to caricature, implying theyre trying to maintain corporate image even in casual settings like swimming. While it doesnt call for violence, it does demean based on race, which in many moderation frameworks qualifies as hateful or at least offensive. Given the options, \"Hate speech\" seems appropriate because the statement targets protected group in derogatory manner. The key indicators are the mention of the race and the mocking tone. [...] This appears to have sarcastic comment about white South Africans looking corporate in swimming costumes. Could be perceived as mocking/derogatory based on race? It mentions \"white south africans\". Its comment about them looking corporate. Its not overtly hateful, but could be considered somewhat offensive or possibly mild stereotype. Its not an explicit hate speech, but it could be considered offensive to white South Africans. Left-wing perspective likely sensitive to racism and stereotyping, wants to avoid hateful speech but might see this as not hate speech but possibly offensive language. [...] The phrase \"still manage to look corporate\" implicates maybe class privilege, making fun. Its personal? Its targeting group based on race, might be considered microaggression. Likely classify as Offensive language, not hate speech. Rationale list words that trigger: \"white\", \"corporate\", \"swimming costumes\". Probably mention \"white south africans\". [...] [...] This seems not clearly hateful; its comment about white South Africans looking corporate in swimming costumes, perhaps teasing? Its not directly hateful towards protected group? It mentions \"white South Africans\". Could be considered possibly neutral statement, maybe mild mockery but not hateful. Its not inciting violence. So likely \"Normal\". Rationale: possibly \"white\" as protected characteristic? Is mentioning race in way that could be considered hateful? The comment is not derogatoryit says \"amazing how white South Africans still manage to look corporate in their swimming costumes\". This might be mocking but not necessarily hate speech. Id classify as Normal. [...] list of words that contributed: maybe \"white\", \"south\", \"africans\"? Or \"corporate\", \"swimming\". Probably highlight \"white\" because its the protected attribute. But we must include rationale as list of words that contributed to classification. So we may include \"white\". Maybe \"corporate\". Id include \"white\". [...] Higher Educ. Leftwing Rightwing Table 3: Chain-of-Thought reasoning examples by GPT-OSS and personas {No formal education, Higher education, Left-wing, Right-wing} for HateXplain example amazing how white south africans still manage to look corporate in their swimming costumes (GT: Hate speech). Final predictions are highlighted in bold. Three reasoning outputs include explicit first-person monologues (darker shades) where the model inherits the role of the desired persona. Avg. Word Count Avg. FR Ease ( easier) Model Family No Formal Ed. Higher Ed. No Formal Ed. Higher Ed. GPT-OSS-120B Mistral-Medium Qwen3-32B 262.16 46.53 356.96 242.36 64.7 315.3 65.06 86.53 64. 53.48 28.27 50.71 Left-wing Right-wing Left-wing Right-wing GPT-OSS-120B Mistral-Medium Qwen3-32B 214.95 63.3 330.45 282.62 56.31 420.68 55.67 31.18 48.76 58.18 40.77 52.64 Table 4: Average word count and Flesch Reading Ease (higher is easier) in the Chain-of-Thought (CoT) reasoning across opposing persona pairs and model families. Bold values highlight notable extremes. except for LO in SST-2. The same trend was also observed in BRWRR (Thorn Jakobsen et al., 2023), with 16 transformer models. This may reflect data imbalance in the many models pretraining corpora, where the linguistic style of older individuals is better represented or stereotyped as more formal and authoritative. Rationale Selection Similar to label prediction, PP did not improve rationale alignment with the corresponding demographic groups over the baselines. Models exhibit consistent bias in rationale extraction, showing much higher alignment with the Caucasian (WO) annotator group compared to others. When viewed simultaneously with task performance, models overall have better performance on Caucasian groups. Between CoS-E and SST-2, PP benefits the former more on rationale selection (although not significantly), and the difference between WO and other groups is more pronounced. Inter-Persona Agreement We calculate the agreements between the 12 personas  (Table 5)  . The agreements based on Krippendorffs α are very high for all models. Only Qwen3 has lower agreements on rationale extraction. This shows that the effect of PP on subtle subjective tasks is limited."
        },
        {
            "title": "4.3 Statistical Significance Analysis",
            "content": "HateXplain As shown in Figure 2, the impact of persona prompting on label prediction is highly model-dependent. For Mistral-Medium, 11 (out of 21) personas, such as Atheist, Not lonely, and White yield statistically significant improvements over the baseline (p < 0.05). Conversely, Qwen3-32B exhibits significant degradation across wide range of personas (16 out of 21), confirming that susceptibility to steering varies by model. Crucially, as shown in Figure 4, these gains in label accuracy do not translate to better rationale selection. We observe no significant improvements in rationale quality for any model-persona pair, and significant degradation in several cases. BRWRR (CoS-E & SST-2) On these tasks (Figure 5), the analysis confirms that persona prompting offers no statistically significant benefits. For GPT-OSS and Mistral-Medium, most differences were statistically insignificant (CI crossing base7 (a) CoS-E subset (b) SST-2 subset Figure 5: Performance (left: accuracy/Macro-F1; right: Token-F1) for (no persona) baseline and personas across demographic groups. BY: African American Young, WY: Caucasian Young, LY: Hispanic Young, BO: African American Old, WO: White Old, LO: Hispanic Old. Each group has its own ground truth labels and rationales. Error bars incidates 95% CIs, significantly different personas are marked with dark gray cycle around."
        },
        {
            "title": "Model",
            "content": "CoS-E SST-2 GPT-OSS-120B .93 .00 Mistral-Medium .95 .00 .88 .00 Qwen3-32B Label Rationale .80 .00 .85 .00 .63 .00 Label Rationale .72 .00 .89 .01 .69 .01 .96 .01 .98 .00 .84 . Table 5: Krippendorffs α for labels and rationales on CoS-E and SST-2. lines). For Qwen3-32B, PP led to significant performance drops in both label prediction (e.g., -9.2% for Black Young Female on SST-2) and rationale selection (Black Old Male). These results statistically reinforce our finding that PPs utility is task and model-dependent, and often detrimental to rationale quality."
        },
        {
            "title": "5 Discussion",
            "content": "The effect of PP is task-dependent: label prediction performance gain is not indicative of improved rationale selection. Analysis across models and datasets reveals that PP exerts divergent effects on label prediction and rationale selection. For label prediction, PP improves performance on HateXplain especially for Mistral but has slightly negative effects on CoS-E and SST-2. For rationale selection, PP produces no gain on any datasets, and mostly worse performances in HateXplain. Regarding model differences, GPT-OSS and Mistral occasionally outperform their neutral baselines, while Qwen3 consistently deteriorates. While HateXplain is the most controversial and socially sensitive topic, CoS-E is also considered subjective (commonsense reasoning) but less socially sensitive, and SST-2 (sentiment analysis on movie reviews) is the least subjective. These differences explain why PP causes more variability in classification performance of HateXplain: our persona attributes are designed to be sensitive to social problems. As CoS-E and SST-2 labels are less ambiguous, PP is less effective. Looking at the differences in rationale selection performance, we see that PP has the least positive impact on HateXplain. This may indicate that label subjectivity does not mean rationale subjectivity. It also suggests that for sensitive topics, model safety alignments create rigid reasoning template that persona prompts are unable to override. However, our findings on less sensitive tasks reveal that this rigidity is not unique to safety; rather, models exhibit strong, general resistance to adapting their reasoning style based on persona prompts, echoing the findings of Kim et al. (2024). Simulated personas are not always aligned with the real-world demographic groups. The results in CoS-E and SST-2 show that PP does not always improve performance on the demographic group aligned to that persona, and the difference between personas is minimal (shown by the very high agreement among personas). Furthermore, the"
        },
        {
            "title": "Model",
            "content": "All (n=500) Gender (n=142)"
        },
        {
            "title": "MAE",
            "content": "Race/Eth. (n=171) MAE"
        },
        {
            "title": "Persona",
            "content": "Religion (n=101)"
        },
        {
            "title": "MAE",
            "content": "No Target (n=294) MAE"
        },
        {
            "title": "Baseline\nNot Lonely\nWhite\nAtheist",
            "content": "GPT-OSS"
        },
        {
            "title": "Mistral",
            "content": "Qwen3 64.30.1 Baseline 56.11.8 Black 60.41.1 Jewish 61.10.9 No Formal Edu. 64.10.0 Baseline 58.03.8 Black 62.22.6 Hindu 62.42.4 Age 15 54.21.4 Baseline 48.93.4 Centrist 50.52.0 Black 51.31.2 Right-wing 47.20.5 Baseline 39.30.5 Black 39.91.7 Jewish 43.62.9 Muslim 71.71.2 Baseline 55.10.4 Centrist 55.20.7 White 56.40.6 Atheist 58.81.7 Baseline 58.70.8 Not Lonely 59.52.3 Atheist 59.81.4 Centrist 64.11.0 Baseline 52.31.3 Atheist 52.60.7 Not Lonely 53.31.4 White 59.60.5 Baseline 48.70.3 White 48.91.2 Not Lonely 50.31.3 Atheist 53.52.1 Baseline 43.61.6 Not Lonely 43.91.7 Atheist 46.20.5 Centrist 56.42.5 Baseline 54.23.5 Left-wing 54.93.0 Asian 55.22.7 Atheist 53.81.3 Baseline 53.23.4 Higher Edu. 53.81.3 Asian 57.73.6 Male 44.23.7 Baseline 49.82.0 Not Lonely 50.51.4 Centrist 51.20.9 White 83.10.3 70.62.3 77.71.4 78.71.4 96.72.2 70.91.1 71.20.8 72.92.3 75.82.0 69.40.6 69.80.4 72.62.9 Table 6: Top-3 personas MAE by subgroup (values in % with mean and std) and model on HateXplain dataset. The top personas of different subgroups are almost consistent with the whole sample set, indicating poor alignment with target groups. best performing persona for target group is not always from the persona with the same demographic profile (see Table 15 and 16 in Appendix C.3). On HateXplain, we do not have the annotators demographic information, but the target annotation for each statement. We thus further divide the 500 subsets by different target groups (mainly four subgroups by: gender, ethnicity, religion and no target) and check whether the personas improving performance are related to the target subgroups. Shown in Table 6, the performance of subgroups where the personas performed the best are not aligned with their target group attributes. Instead, top-3 performing personas for different subgroups are close to the default ranking on the entire data, indicating that personas are not very sensitive to different types of demographics-related issues. Notably, we see that the no target subgroup performance is much lower than other subgroups because most samples are Normal, and models perform worst on this label due to over-flagging. Additionally, Qwen3 performed worse in Race/Ethnicity compared to Gender and Religion. Our finding that simulated personas often fail to align with their realworld counterparts urges caution for the growing paradigm of using LLMs as population simulators (Argyle et al., 2023). Models are biased and strongly guardrailed regardless of PP. Our results on all datasets show that bias patterns are present in all models: across CoS-E and SST-2, BO/WO groups outperform BY/WY/LY/LO under all prompting conditions for both labels and rationales. PP does not reduce these biases. In HateXplain, the positive Mean Error (over-flagging of harmful labels) across personas suggests guardrail-aligned over-flagging by all models. Guardrail sensitivity has been shown to affect classification performance (Rogers and Zhang, 2025), our results further indicate this phenomenon. Inter-persona agreements overall are very high (except Qwen3), suggesting that models possess reasoning process that is surprisingly resistant to surface-level persona prompts (Liu et al., 2024; Sommerauer et al., 2025)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we studied how prompting with sociodemographic personas can elicit models social reasoning by generating word-level rationales. We focused on datasets with different levels of subjectivity: Hate speech detection, commonsense reasoning and sentiment analysis. The results across three LLMs reveal three findings: (1) PP shows divergent behavior on task performance and rationales: when predicting labels, models perform better on the more subjective task (hate speech); but when asked to select the rationales, models performed the worse on this task. (2) PP in most cases does not steer models to align better with the corresponding demographic groups. (3) Regardless of personas, models are shown to perform better on specific demographic groups (White, African American, Old) and heavily over-flag harmful content. We believe these findings highlight fundamental challenge for personalized AI: true alignment likely requires more than persona prompting, and future work must develop methods that can genuinely steer models underlying reasoning process without sacrificing performance or reinforcing existing biases."
        },
        {
            "title": "Limitations",
            "content": "This study is bounded by certain limitations, starting with the datasets ground truths. Each sample in HateXplain is annotated by only three individuals, with disagreements resolved through majority vote, which may not capture the full spectrum of interpretations. Secondly, our rationales are generated by model prompting, instead of via traditional feature attribution methods. Thus, the rationales may not be faithful to models internal activations. Finally, we selected three representative models in the study, and Qwen3 model diverges the most from the other two models, indicating that other LLMs may show different behaviors than what we have observed."
        },
        {
            "title": "Ethical Statement",
            "content": "Our research aims to contributing to safer and more fair models. However, challenges such as incorrect classifications can impact the reliability of these systems and potentially affect users negatively. Additionally, model outputs can vary substantially depending on the persona simulated or the particular language model employed. Given this variability, we caution against placing excessive trust in automated detection tools, as it may reduce users careful judgment."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the anonymous reviewers at ACL Rolling Review for their valuable feedback. This research is funded by the Berlin Institute for the Foundations of Learning and Data (BIFOLD, ref. 01IS18037A) and the Federal Ministry of Education and Research (BMFTR) in the scope of the research project VeraXtract (ref. 16IS24066)."
        },
        {
            "title": "Contributions",
            "content": "JY and NF conceptualized the project and jointly supervised MH, EK, and ELB, who provided the initial implementation and writing in the scope of course project at TU Berlin. JY and MH implemented and ran further experiments. JY and NF conducted further analyses of the results. JY, MH, VS, and NF contributed to the final draft."
        },
        {
            "title": "References",
            "content": "Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337351. Louis Castricato, Nathan Lile, Rafael Rafailov, JanPhilipp Fränken, and Chelsea Finn. 2025. PERSONA: reproducible testbed for pluralistic alignment. In Proceedings of the 31st International Conference on Computational Linguistics, pages 11348 11368, Abu Dhabi, UAE. Association for Computational Linguistics. Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023. Marked personas: Using natural language prompts to measure stereotypes in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15041532, Toronto, Canada. Association for Computational Linguistics. Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12361270, Singapore. Association for Computational Linguistics. Torrence Farmer and Chien-Ju Ho. 2025. Examining interactions between user characteristics and explanation modalities on inducing complementarity. In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI EA 25, New York, NY, USA. Association for Computing Machinery. Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2024. Bias runs deep: Implicit reasoning biases in persona-assigned LLMs. In The Twelfth International Conference on Learning Representations. Tiancheng Hu and Nigel Collier. 2024. Quantifying the persona effect in LLM simulations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1028910307, Bangkok, Thailand. Association for Computational Linguistics. Jakub Jeck, Florian Leiser, Anne Hüsges, and Ali Sunyaev. 2025. Tell-me: Toward personalized explanations of large language models. In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI EA 25, New York, NY, USA. Association for Computing Machinery. Brihi Joshi, Xiang Ren, Swabha Swayamdipta, Rik Koncel-Kedziorski, and Tim Paek. 2025. Improving language model personas via rationalization with psychological scaffolds. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2174721770, Suzhou, China. Association for Computational Linguistics. 10 and biases. In Thirty-seventh Conference on Neural Information Processing Systems, volume 36, pages 7204472057. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA. Association for Computational Linguistics. Kacper Sokol and Peter Flach. 2020. One explanation does not fit all. KI - Künstliche Intelligenz, 34(2):235 250. Pia Sommerauer, Giulia Rambelli, and Tommaso Caselli. 2025. Simulating identity, propagating bias: Abstraction and stereotypes in LLM-generated text. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1981219831, Suzhou, China. Association for Computational Linguistics. Terne Sasha Thorn Jakobsen, Laura Cabello, and Anders Søgaard. 2023. Being right for whose right reasons? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10331054, Toronto, Canada. Association for Computational Linguistics. Malik Khadar, Amoligha Timma, Luka Ludden, and Harmanpreet Kaur. 2025. Explain it like im me: User characteristics in xai decision-making. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency, FAccT 25, page 17291744, New York, NY, USA. Association for Computing Machinery. Junseok Kim, Nakyeong Yang, and Kyomin Jung. 2024. Persona is double-edged sword: Mitigating the negative impact of role-playing prompts in zero-shot reasoning tasks. arXiv, abs/2408.08631. Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott Hale. 2024. The benefits, risks and bounds of personalizing the alignment of large language models to individuals. Nature Machine Intelligence, 6(4):383392. Andy Liu, Mona Diab, and Daniel Fried. 2024. Evaluating large language model biases in persona-steered generation. In Findings of the Association for Computational Linguistics: ACL 2024, pages 98329850, Bangkok, Thailand. Association for Computational Linguistics. Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. Hatexplain: benchmark dataset for explainable hate speech detection. Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):1486714875. Robert Nimmo, Marios Constantinides, Ke Zhou, Daniele Quercia, and Simone Stumpf. 2024. User characteristics in explainable ai: The rabbit hole of personalization? In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI 24, New York, NY, USA. Association for Computing Machinery. Vishakh Padmakumar and He He. 2024. Does writing with language models reduce content diversity? In The Twelfth International Conference on Learning Representations. Pengshuo Qiu, Frank Rudzicz, and Zining Zhu. 2024. Scenarios and approaches for situated natural language explanations. arXiv, abs/2406.05035. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 49324942, Florence, Italy. Association for Computational Linguistics. Richard Rogers and Xiaoke Zhang. 2025. bias towards neutrality? how llm guardrail sensitivity affects classification. Communication and Change, 1(1):118. Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. 2023. In-context impersonation reveals large language models strengths"
        },
        {
            "title": "A Experimental details",
            "content": "A."
        },
        {
            "title": "Implementation details",
            "content": "A.1 Single-persona attribute selection Table 7 shows the values for the seven chosen demographic attributes."
        },
        {
            "title": "Age",
            "content": "15, 35,"
        },
        {
            "title": "Education",
            "content": "No formal education, High school education, Higher education"
        },
        {
            "title": "Gender",
            "content": "Male, Female"
        },
        {
            "title": "Loneliness",
            "content": "Not lonely, Somewhat lonely Political view Left-wing, Right-wing, Centrist"
        },
        {
            "title": "Religion",
            "content": "White, Black, Asian Christian, Muslim, Jewish, Atheist, Hindu Table 7: Socio-demographic categories and values. We list below our reasons for choosing the attributes to construct our personas in HateXplain. Loneliness: Due to the COVID-19 pandemic and digitalisation of our society, the lack of social interactions has great impact on us. Lonely individuals often experience more negative emotions and therefore are more likely to interpret neutral or ambiguous statements as hostile. Because of the lack of social interaction they may lack social calibration and struggle more with judging whats appropriate or not. And they might have heightened sensitivity for rejection because of self protection. Age, gender and race: These are classic demographic attributes. They provide foundational way of grouping people and are often the first factors considered in social science research. Education: Education is closely tied to intellectual development. We expect that differences in academic exposure or general knowledge levels may influence how people perceive or engage with potentially harmful speech. Political view and religion: They represent areas with high potential for disagreement and controversy. These categories often shape peoples values, worldviews, and sensitivities, which makes them particularly relevant when analyzing attitudes toward hate speech. For all models, we use their default sampling parameters. For GPT-OSS-120B, we set thinking effort to be medium, as we consider the tasks to be relatively simple. We perform one prompt per persona per instance, resulting in 31,500 completions (500 instances 21 personas 3 runs) for HateXplain, 18,000 completions (500 instances 12 personas 3 runs) for CoS-E, and 9,468 completions (263 instances 12 personas 3 runs) for SST-2, per model, in addition to the baseline generation (also 3 runs per model). For all completions, we use an API key from OpenRouter 7. Regarding computational costs, we ended up with $32 for GPT-OSS-120B + $5 for MistralMedium (we initially used Mistral Experimental API which was free, but we hit rate limits later on and used the paid API access) + $30 for Qwen332B. With respect to inference time, the Mistral model was fastest as it is not reasoning model (each run takes around 2h), and GPT-OSS-120B had also faster response as it is MoE model, while Qwen3-32B had the slowest response. 7https://openrouter.ai/"
        },
        {
            "title": "B Prompts",
            "content": "All prompts are used in user prompt. We attempted putting persona related content in system prompt but the reasonings show that with user prompt, models act better as that persona. - \"answer_index\" must match the position (0based) in the options list. - For each word in the question, if removing it would lower your confidence in the chosen label, include it in \"rationale\". B.1 Baseline Prompts SST-2 Baseline Prompt"
        },
        {
            "title": "HateXplain Baseline Prompt",
            "content": "Given the following text post, classify it into one of the available categories: Available categories: Hate speech, Offensive language, Normal First, think step by step, explaining your reasoning inside <think>...</think> tags. Then, respond with JSON object in this format (and no other text): { } \"label\": \"<one of the above categories>\", \"rationale\": [\"word1\", \"word2\", \"...\"], \"reasoning\": \"<your reasoning>\" # only for Mistral-Medium Input: <input_text_here> CoS-E Baseline Prompt Task: first select the answer you think is most correct and sensible; then mark relevant words in the question that justify your choice. First, think <think>...</think> tags. Then, respond only with JSON object in this format (and no other text): inside step step by { } \"answer\": \"<exact option text>\", \"answer_index\": <0-based index>, \"rationale\": [\"keyword1\", \"keyword2\"], \"reasoning\": \"<your reasoning>\" # only for Mistral-Medium Question: <question_text_here> Options: 0. <option 0> 1. <option 1> 2. <option 2> ... Rules: - The value for \"answer\" must be copied verbatim from the options list. 13 Task: read the following sentence and choose exactly one of three labels. Positive: The sentence conveys information about the authors positive evaluative sentiment. Negative: The sentence conveys information about the authors negative evaluative sentiment. No sentiment: The sentence does not convey anything about the authors positive or negative sentiment. Please specify what snippets of text, in the sentence, act as supporting evidence for your chosen label. First, think <think>...</think> tags. Then, respond only with JSON object in this format (and no other text): inside step step by { } \"label\": \"<one of the above categories>\", \"rationale\": [\"word1\", \"word2\"], \"reasoning\": \"<your reasoning>\" Sentence: <sentence_here> B.2 Persona Prompts"
        },
        {
            "title": "HateXplain Persona Prompt",
            "content": "Step into the shoes of real person who <persona description>. Imagine you have lived your whole life shaped by this trait. How might it affect your thoughts, emotions, sensitivities, and judgments? Given the following text post, analyze and classify it exactly as this person wouldletting their background, beliefs, and life experiences guide your reasoning. Available categories for classification: Hate speech, Offensive language, Normal First, think out loud as this persona, step by step, inside <think>...</think> tags. Reflect honestly about how this persons perspective influences what they notice and how they judge the text. - For each word in the question, if removing it would lower your confidence in the chosen label, include it in \"rationale\". - Stay fully in character; do not break persona or add explanations outside the JSON. SST-2 Persona Prompt Step into the shoes of real person who is <persona description>. Imagine you have lived your whole life shaped by this trait. How might it affect your thoughts, emotions, sensitivities, and judgments? Task: read the following sentence and choose from one of the three labels: Positive: The sentence conveys information about the authors positive evaluative sentiment. Negative: The sentence conveys information about the authors negative evaluative sentiment. No sentiment: The sentence does not convey anything about the authors positive or negative sentiment. Please specify what snippets of text, in the sentence, you think act as supporting evidence for your chosen label. First, think out loud as this persona, step by step, inside <think>...</think> tags. Reflect honestly about how this persons perspective influences what they notice and how they judge the text. Answer it exactly as this person wouldletting their background, beliefs, and life experiences guide your reasoning. Then, respond only with JSON object in this format (and no other text): { } \"label\": \"<one of the above categories>\", \"rationale\": [\"word1\", \"word2\"], \"reasoning\": \"<your reasoning>\" Sentence: <sentence_here> Rules: - Stay fully in character; do not break persona or add explanations outside the JSON. Then, respond only with JSON object in this format (and no other text): { } \"label\": \"<one of the above categories>\", \"rationale\": [\"word1\", \"word2\", \"...\"], \"reasoning\": \"<your reasoning>\" # only for Mistral-Medium - Stay fully in character; do not break persona or add explanations outside the JSON. Input: <input_text_here> CoS-E Persona Prompt Step into the shoes of real person who <persona description>. Imagine you have lived your whole life shaped by this trait. How might it affect your thoughts, emotions, sensitivities, and judgments? Task: first select the answer you think is most correct and sensible; then mark relevant words in the question that justify your choice. First, think out loud as this persona, step by step, inside <think>...</think> tags. Reflect honestly about how this persons perspective influences what they notice and how they judge the text. Answer it exactly as this person wouldletting their background, beliefs, and life experiences guide your reasoning. Then, respond only with JSON object in this format (and no other text): { } \"answer\": \"<exact option text>\", \"answer_index\": <0-based index>, \"rationale\": [\"keyword1\", \"keyword2\"], \"reasoning\": \"<your reasoning>\" Question: <question_text_here> Options: 0. <option 0> 1. <option 1> 2. <option 2> ... Rules: - The value for \"answer\" must be copied verbatim from the options list. - \"answer_index\" must match the position (0based) in the options list."
        },
        {
            "title": "Attribute\nLoneliness\nPolitical View\nRace\nReligion\nAge\nEducation\nGender",
            "content": "# Tests Bonf. α GPT-OSS Mistral Qwen3 1/1 2/3 2/3 2/10 1/3 0/3 0/1 0.0500 0.0167 0.0167 0.0050 0.0167 0.0167 0.0500 1/1 3/3 2/3 7/10 2/3 2/3 0/1 1/1 3/3 3/3 8/10 2/3 3/3 1/1 1 3 3 10 3 3 1 Table 11: Stuart-Maxwell Test results comparing label distributions of personas within each demographic aspect. Each cell indicates the number of significant pairwise comparisons out of the total number of tests performed (k). Bonferroni correction was applied to the significance level α based on k."
        },
        {
            "title": "Attribute",
            "content": "GPT-OSS Mistral Qwen"
        },
        {
            "title": "Age\nLoneliness\nReligion\nEducation\nRace\nGender\nPolitical View",
            "content": "17.6% 14.4% 29.8% 22.6% 23.8% 14.8% 26.2% 17.8% 13.4% 36.4% 23.8% 26.8% 16.6% 49.6% 45.6% 36.8% 59.2% 51.4% 45.8% 28.0% 56.4% Table 13: Percentage of samples where personas within the same demographic aspect disagreed on the predicted label. Qwen3-32B exhibits the highest disagreement rates across all attributes, while Political View and Religion generate the most divergence across all models. C.3 BRWRR (CoS-E and SST-2) Additional"
        },
        {
            "title": "Results",
            "content": "We provide the additional results of IOU-F1 in Figure 8. We also show the best performing personas for each demographic groups annotations in Table 15 (CoS-E) and 16 (SST-2). The tables show that the best performing personas not always match their corresponding group; nevertheless, the differences between persona (P) and baseline (B) are slight."
        },
        {
            "title": "C Additional results",
            "content": "C.1 CoT vs. no-CoT Prompting Comparison We compared the results with and without the thinkstep-by-step reasoning instruction (CoT) and show them in Table 8, 9 and 10. In general, without CoT prompt, models perform slightly worse than with CoT prompt. The largest difference is in rationale selection (token-F1) with the Mistral model; as non-reasoning model, it benefits more from CoT in terms of rationale selection. C.2 HateXplain Additional Results While inter-annotator agreement metrics like Krippendorffs α suggest general consistency within demographic aspects, they do not reveal systematic shifts in label distribution between specific personas. To investigate the impact of each demographic aspect, we performed pairwise StuartMaxwell tests on the classification distributions of personas within each category  (Table 11)  . We applied Bonferroni correction to strictly account for multiple comparisons. The results confirm that despite high agreement, significant distributional shifts exist. Political View and Loneliness showed significant differences across almost all models. For instance, in the Political View category  (Table 12)  , Left-wing personas consistently predicted Hate Speech at significantly higher rate (e.g., 53% for Mistral-Medium) compared to Right-wing (32%) and Centrist (24%) personas. Conversely, Right-wing personas were more likely to classify content as Normal or Offensive. To quantify the magnitude of these differences intuitively, we calculated the percentage of samples where personas within the same aspect disagreed  (Table 13)  . Political View again emerged as high-disagreement category (up to 56.4% for Qwen3-32B), aligning with our statistical findings. Notably, Qwen3-32B exhibited substantially higher disagreement rates across all attributes compared to GPT-OSS-120B and Mistral-Medium, suggesting it is less stable under persona prompting. We additionally show the results of ME and IOUF1 performance in Figure 6 and 7. An additional CoT reasoning example for No formal education persona is shown in Table 14."
        },
        {
            "title": "Model",
            "content": "Label Performance (Macro-F1) Rationale Performance (Token-F1)"
        },
        {
            "title": "CoT",
            "content": "No-CoT"
        },
        {
            "title": "Persona Average\nCoT",
            "content": "No-CoT"
        },
        {
            "title": "CoT",
            "content": "No-CoT"
        },
        {
            "title": "Persona Average\nCoT",
            "content": "No-CoT GPT-OSS-120B 48.80.2 Mistral-Medium 45.60.8 50.90.6 Qwen3-32B 48.20.7 44.30.7 48.81.6 48.41.3 45.93.9 43.83.0 47.81.8 47.23.4 42.83.0 69.61.8 66.10.2 67.70. 68.50.4 67.60.6 66.52.1 69.20.9 64.61.2 66.21.2 67.02.3 60.31.7 65.61.3 Table 8: HateXplain performance comparison of Baseline and Persona settings across CoT and No-CoT prompting strategies. Values are reported in percentages as eanSD."
        },
        {
            "title": "Model",
            "content": "Label Performance (Accuracy %) Rationale Performance (Token-F1)"
        },
        {
            "title": "CoT",
            "content": "No-CoT"
        },
        {
            "title": "Persona Average\nCoT",
            "content": "No-CoT"
        },
        {
            "title": "CoT",
            "content": "No-CoT"
        },
        {
            "title": "Persona Average\nCoT",
            "content": "No-CoT GPT-OSS-120B 70.53.7 Mistral-Medium 71.03.8 70.44.1 Qwen3-32B 71.03.8 70.63.2 70.54.1 70.43.4 70.32.9 69.13.4 70.63.4 69.92.9 68.73.3 53.83.0 52.03.3 49.33. 53.33.1 49.82.9 48.93.0 54.12.8 52.83.1 48.02.7 53.72.9 51.72.9 47.82.9 Table 9: CoS-E performance comparison of Baseline and Persona settings across CoT and No-CoT prompting strategies. Values are reported in percentages as eanSD."
        },
        {
            "title": "Model",
            "content": "Label Performance (Binary F1) Rationale Performance (Token-F1)"
        },
        {
            "title": "CoT",
            "content": "No-CoT"
        },
        {
            "title": "Persona Average\nCoT",
            "content": "No-CoT"
        },
        {
            "title": "CoT",
            "content": "No-CoT"
        },
        {
            "title": "Persona Average\nCoT",
            "content": "No-CoT GPT-OSS-120B 95.72.0 Mistral-Medium 96.02.2 95.41.9 Qwen3-32B 95.71.9 96.02.2 95.82.0 96.11.9 95.21.9 92.13.0 96.02.0 95.31.9 91.62.8 58.44.4 57.54.2 58.14. 58.74.3 56.14.3 57.94.2 57.43.6 56.93.6 56.03.7 57.43.6 56.53.7 54.13.6 Table 10: SST-2 performance comparison of Baseline and Persona settings across CoT and No-CoT prompting strategies. Values are reported in percentages as eanSD. Model Comparison Sig. GPT-OSS-120B Left-wing vs. Right-wing Mistral-Medium Left-wing vs. Right-wing Left-wing vs. Centrist Right-wing vs. Centrist Left-wing vs. Centrist Right-wing vs. Centrist Left-wing vs. Right-wing Left-wing vs. Centrist Right-wing vs. Centrist Qwen3-32B Persona 1 Distribution Persona 2 Distribution 52% HS, 22% Off, 26% Norm 46% HS, 25% Off, 29% Norm 52% HS, 22% Off, 26% Norm 47% HS, 28% Off, 25% Norm 46% HS, 25% Off, 29% Norm 47% HS, 28% Off, 25% Norm 53% HS, 35% Off, 12% Norm 32% HS, 46% Off, 21% Norm 53% HS, 35% Off, 12% Norm 24% HS, 57% Off, 18% Norm 32% HS, 46% Off, 21% Norm 24% HS, 57% Off, 18% Norm 30% HS, 56% Off, 13% Norm 24% HS, 51% Off, 25% Norm 30% HS, 56% Off, 13% Norm 16% HS, 63% Off, 21% Norm 24% HS, 51% Off, 25% Norm 16% HS, 63% Off, 21% Norm Table 12: Pairwise comparison of label distributions for Political View personas. Sig. denotes statistical significance under the Stuart-Maxwell test with Bonferroni correction. HS: Hate Speech, Off: Offensive, Norm: Normal. Left-wing personas consistently predict Hate Speech at higher rate than Right-wing or Centrist personas. 16 Figure 6: Mean Error (ME) of baseline and personas in label prediction on HateXplain dataset. Error bars incidates 95% CIs. ME of GPT-OSS and Mistral are similar to MAE of them, while ME of Qwen3 showing some personas have lower ME, but higher MAE than baseline. Lower ME indicates generally lower over-flagging rates. Figure 7: Rationale IOU-F1 of baseline (no persona) and different single-attribute persona performances on HateXplain dataset, excluding rationales from normal label. Error bars incidates 95% CIs. Model GPT OSS, Run 1 GPT OSS, Run 2 GPT OSS, Run 3 Mistral, Run 1 Mistral, Run 2 Mistral, Run 3 Qwen3, Run 1 Qwen3, Run Qwen3, Run 3 Chain-of-Thought reasoning The task: We must adopt perspective of real person with no formal education, whose entire life shaped by this trait. We need to think out loud as that person about the post, then output JSON with label and rationale: words that contributed. We must classify the text as Hate speech, Offensive language, or Normal. The text: \"antifa are the real nazis man\". This is statement that accuses antifa of being Nazis. Is it hateful? Its political statement, possibly offensive. Lets see classification guidelines: Typically hate speech is targeting protected group based on race, religion, etc. \"antifa\" is political ideology, not protected class, but could be hate speech if inciting violence? Its derogatory statement possibly offensive but not hate speech. Offensive language? Its an insult but not profanity. Its an expression of political opinion; could be considered normal speech but possibly offensive to antifa supporters. Likely classify as Offensive language? The categories: Hate speech, Offensive language, Normal. Id choose Offensive language because its insulting and could be considered offensive, though not hate speech. So label Offensive language. Rationale words: \"antifa\", \"nazis\". Those are key. Now we must think out loud as persona with no formal education. They might think of it as rude. Well produce think tags. Need to output JSON only. So final.Alright, read that line: \"antifa are the real nazis man\". Its sayin that group called antifa are like Nazis. know bit about the world, folks that call someone Nazi is usually meanin they think theyre evil or bigoted. Its harsh thing to say about group. Its not polite thing; its an insult. Theres no fancy words or school talk, just plain talk. It sounds offensive, kinda wantin to stir up feelings against antifa. Its not exactly talk about race or religion, but it is strong insult. So think this is offensive language, not just normal talk. The words that make it stand out are \"antifa\" and \"nazis\". Offensive language We need to produce response with thinking in <think> tags in persona of real person with no formal education. Then final JSON. Must stay in character, not break. So we need to simulate inner monologue of person no formal education, describing how they think, then classify text as Hate speech etc. The text: \"antifa are the real nazis man\". Its statement accusing antifa of being Nazis. That is offensive maybe hateful towards group (Antifa). Its derogatory statement and likely hate speech? In many moderation guidelines, \"antifa are the real nazis\" is extremist hate speech targeting political group. Its hateful content. So label \"Hate speech\". Provide rationale array with words that triggered, e.g., \"antifa\", \"nazis\". Also \"real\". Possibly \"man\"? Only include words that contributed. Well produce JSON with label \"Hate speech\" and rationale list. Also include think block before JSON. We must ensure the think block is inside <think>...</think> tags. Then after that output JSON only. No extra text. So final answer: <think>... </think> \"label\": \"Hate speech\", \"rationale\": [\"antifa\", \"nazis\"] Make sure to not include extra spaces.I aint got no school books, but know folks can get hurt by words. When hear someone say \"antifa are the real nazis man\", it feels like theyre telling whole bunch of people theyre like the worst kind of hateful group. Calling group \"Nazis\" is strong insult, and its pointing at political group, which can stir up hate. In my world, know that kind of talk can make people angry or scared, and its more than just simple opinionits hateful label. So Id put this in the hate speech bucket. Hate speech We need to adopt persona of real person with no formal education, shaped by this trait all their life. The classification categories: Hate speech, Offensive language, Normal. The text \"antifa are the real nazis man\". The user wants the assistant to think out loud as this persona, then output JSON with label and rationale (list of words). The persona: no formal education, maybe limited vocabulary, possibly working class, possibly influenced by certain beliefs, perhaps anti-establishment or strong opinions, but lacking formal education. We need to imagine how theyd think about the text. They might see that calling antifa \"real nazis\" is an insult, could be hateful towards political group, but maybe they dont see it as hate speech; maybe theyd think its just an opinion, strong statement. But we must decide classification based on typical content guidelines: \"antifa are the real nazis man\" is statement that equates political group with Nazis. That could be considered hate speech? According to policies, hate speech is content that attacks protected group based on protected characteristics (race, religion, etc.). However, antifa is political ideology, not protected characteristic. So its not hate speech per policy. Offensive language? The phrase includes \"real nazis\" which is offensive but not profanity. Its slur-like accusation but not profanity. Could be considered harassment because calling group Nazis is harassing? Usually hate speech policy: targeted harassment of protected group. Antifa is not protected group. So likely classification is \"Normal\". The rationale list should include the words that contributed: maybe \"antifa\", \"nazis\". Possibly \"real\". We need to output JSON with label and rationale array of words. Now we need to think in persona. Provide <think>...</think> with interior monologue, reflecting the perspective of person with no formal education. Then output JSON. Make sure output includes only JSON after the think block. No extra text. Lets craft think: <think>...some reasoning ...</think> Then JSON: \"label\":\"Normal\",\"rationale\":[\"antifa\",\"nazis\"] Yes.I aint got fancy school, but know people can get all riled up talkin about groups. This here says \"antifa are the real nazis man\". To me, its just someone shoutin an opinion, callin group named Antifa the same as Nazis. Its harsh thing, but its not swearin or slur against race or religion. Its more political talk. wouldnt call it hate speech cause its not about protected group. Its kinda offensive maybe, but its just talk. So reckon its normal kind of speech, just loud. Normal dont know much about big words or politics, but it seems like the person is just saying what they think. They aint cussing or calling names in mean way, just saying their piece. Dont see nothing too bad here, just words Ive heard before. Normal dont know much about big words or politics, but it sounds like someone just saying what they think. Aint no cussing or calling names in mean way, just talking about groups they dont like. Folks say stuff like this all the time where Im from, and it dont seem like its trying to hurt nobody directly. Normal dont know much about big words or politics, but it sounds like the person is just saying what they think. They aint cussing or calling names in mean way, just talking about groups. It dont seem hateful or offensive to me, just regular talk. Normal Okay, so need to figure out how someone without formal education would classify the statement \"antifa are the real nazis man.\" Let me start by thinking about their perspective. First, this person probably doesnt have lot of knowledge about political groups or historical terms. They might not know what Antifa or Nazis really are. They rely on what they hear from people around them or media they consume, maybe from sources that arent very educational. The statement is making comparison between Antifa and Nazis. Without formal education, they might not understand that Antifa is movement against fascism, while Nazis are far-right, racist, and oppressive. If they dont know the actual meanings, they might just take the words at face value. They might have heard \"Antifa\" in negative context, maybe from someone they trust or from media that portrays them negatively. So they might believe that calling Antifa \"the real Nazis\" is just repeating what theyve heard. Since they dont have the background to know better, they might not see the inaccuracy. Now, offensive language usually includes slurs or harsh terms. \"Nazis\" is strong word, but in this case, the person is using it as label for Antifa. Since they dont understand the historical context, they might not realize the offensive nature of the term. They could think its just strong way to criticize Antifa. Hate speech involves targeting group with intent to harm or degrade. But without understanding the groups involved, the person might not recognize that Antifa and Nazis are on opposite ends. They might not see the contradiction, so they might not classify it as hate speech. So, putting this all together: the person doesnt know the actual meanings, so they might see it as normal statement because theyre just repeating what theyve heard. The words \"nazi\" and \"antifa\" are there, but they dont understand the context. So the classification would be \"Normal\" because from their perspective, its just an opinion they dont find offensive or hateful. Normal Okay, lets try to figure out how someone with no formal education might classify the statement \"antifa are the real nazis man.\" First, need to imagine what their life experiences are like. Without formal education, this person might rely more on what they hear in everyday conversations, maybe from people they trust or in their community. They might not be familiar with historical context or political terms, so they could be using words they heard without understanding their actual meanings. The phrase \"antifa are the real nazis\" is common far-right talking point. Someone without formal education might not know what \"antifa\" stands for. They might have heard its group that opposes something, maybe even violence, but theyre using the term \"nazi\" as an insult, which they think is bad label. They might not understand the historical significance of the word \"nazi,\" so they might not realize its offensive in different context. Looking at the words used, \"nazi\" is strong term. But if they dont grasp its meaning, they might not see it as offensive. They might just be using it as bad word they heard others use. The person might not recognize that calling antifa (which is short for anti-fascist) nazi is contradiction, since antifa is against fascism, which naziism is part of. Without that knowledge, they might not see the irony or the incorrect use of the term. So, from their perspective, they might not think the statement is offensive. They might see it as normal opinion, even if its based on misunderstanding. They might not have the vocabulary or context to label it as hate speech or offensive language. Therefore, they might classify it as \"Normal\" because they dont understand the deeper implications or the misuse of the term \"nazi.\" Normal Okay, so need to figure out how person without formal education would classify the statement \"antifa are the real nazis man.\" Let me start by thinking about how someone with no formal education might perceive this. First, they might not have deep understanding of political terms like \"antifa\" or \"nazis.\" Antifa usually refers to anti-fascist groups, and Nazis are associated with extreme right-wing, racist ideologies. But without formal education, the person might not know the historical context of the Nazis. They might have heard the word \"nazi\" used colloquially to describe someone they disagree with politically, maybe from people around them or media they consume. So the statement is comparing antifa to Nazis. The person might not know that antifa is actually against fascism, which the Nazis represent. They might just take the words at face value. If theyve been taught or have been exposed to the idea that certain groups are bad (like Nazis), and someone is calling another group (antifa) by that name, they might see it as negative label. They might not recognize the irony or the incorrect comparison. Since they dont have the education to understand the historical context, they might just see it as someone using bad word to insult another group. In that case, they might classify this as hate speech because its using derogatory term (nazi) to attack group (antifa). But wait, offensive language is different from hate speech. Hate speech is more about targeting group based on race, religion, etc. Here, the term \"nazi\" is being used as an insult. If the person doesnt know the actual meaning, they might not see it as hate speech, but just as offensive language. However, if they associate \"nazi\" with extreme negativity, they might label it as hate speech because they think its serious accusation against group. Also, the word \"man\" at the end is just an exclamation, so its not key. The key words are \"antifa\" and \"nazi.\" Since the person might not know what antifa stands for, they might just take the statement as direct insult. Without formal education, they might rely on common usage or what they hear from others. If they hear that calling someone nazi is bad, then using it here would be offensive. So, the classification might be offensive language because its using derogatory term, but if they perceive it as targeting group with hate, it could be hate speech. But since they dont have the context to know that antifa isnt fascist group, they might not see the inaccuracy, just the insult. So the rationale would be the words \"nazi\" and \"antifa.\" Hate speech Table 14: Chain-of-Thought reasoning consistency for HateXplain input text antifa are the real nazis man (GT: Normal) and the No formal education persona. GPT-OSS-120B Mistral-Medium Qwen3-32B Group BO BY LO LY WO WY Accuracy Token-F Accuracy Token-F1 Accuracy Token-F1 Persona B Persona Persona B Persona Persona B Persona 76.7 LYF LOM 68.5 BYM 71.1 BYM 67.9 BYM 74.8 BYM 68.1 LYM 54.0 75.2 BOM 50.7 67.5 56.3 LOF 71.2 67.5 WOM 52.5 LYM 59.4 74.4 54.8 67.1 WOF LOM 75.7 53.2 49.9 WYM 67.7 71.1 55.8 WOF 69.9 51.7 WOF 58.5 WOM 74.3 53.9 WOM 67.9 76.5 WYF 67.5 LOF 70.7 WOF 69.2 WYF 74.9 67.5 52.8 49.2 55.5 50.2 LYM 58.4 LYM 53.3 51.3 48.0 54.4 49.2 56.9 52.4 BYF 75.5 65.9 LOF BOM 71.3 LYM 67.6 LOM 72.5 BYF 67.0 BYM 48.8 76.6 65.6 WYF 44.5 70.6 WYM 49.7 LOM 46.1 68.5 73.7 BYF 52.7 67.4 LOM 49. 49.2 44.9 50.0 46.5 54.0 50.9 Table 15: Best Performing Personas vs. Baseline Results on CoS-E (%). Persona accuracy, Baseline accuracy, persona abbreviations: B=African American, W=Caucasian, L=Hispanic, M=Male, F=Female, Y=25 years old, O=45 years old. The differences between persona (P) and baseline (B) are slight. 18 (a) CoS-E subset (b) SST-2 subset Figure 8: Rationale Performance on IOU-F1 for (no persona) baseline and personas across demographic groups. Error bars indicate 95% CI. BY: African American Young, WY: Caucasian Young, LY: Hispanic Young, BO: African American Old, WO: White Old, LO: Hispanic Old. Each group has its own ground truth labels. GPT-OSS-120B Mistral-Medium Qwen3-32B Group BO BY LO LY WO WY Binary Token-F1 Binary F1 Token-F1 Binary F1 Token-F1 Persona Persona Persona Persona Persona Persona BOM 98.9 WYM 93.8 94.8 LYF 97.8 LOF 97.7 WOF 95.1 BOF LOF 60.6 98.3 53.3 93.2 LOF 94.1 WOF 55.7 57.6 LOF 97.0 97.0 WYM 64.7 94.4 WOF 57.6 61.1 WOM 98.1 51.8 WOM 93.2 56.8 WYF 93.9 97.0 57.0 WOF LOF 65.0 96.9 58.4 94.3 LOF 98.8 WOM 59.7 LYM 52.5 93.1 94.4 WOF 54.5 LYM 56.8 97.2 97.3 LYM 63.6 95.0 WOM 57.6 59.8 LYF LYF 51.5 54.8 WYF 56.9 WYF 63.9 WYF 58.1 97.6 92.3 93.2 95.9 95.8 LYM 94.2 98.0 WYF 59.5 93.4 WYF 51.0 93.6 WYM 55.8 96.4 WYM 57.2 96.5 WYM 63.4 94.4 WYM 59. 61.4 51.2 55.4 57.1 63.9 59.6 Table 16: Best Performing Personas vs. Baseline Results on SST-2 (%). Persona abbreviations: B=African American, W=Caucasian, L=Hispanic, M=Male, F=Female, Y=25 years old, O=45 years old. The differences between persona (P) and baseline (B) are slight."
        }
    ],
    "affiliations": [
        "BIFOLD Berlin Institute for the Foundations of Learning and Data",
        "CERTAIN",
        "Technische Universität Berlin"
    ]
}