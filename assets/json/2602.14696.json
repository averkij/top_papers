{
    "paper_title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
    "authors": [
        "Nihal V. Nayak",
        "Paula Rodriguez-Diaz",
        "Neha Hulkund",
        "Sara Beery",
        "David Alvarez-Melis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection."
        },
        {
            "title": "Start",
            "content": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Nihal V. Nayak 1 Paula Rodriguez-Diaz 1 Neha Hulkund 2 Sara Beery 2 David Alvarez-Melis"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 6 1 ] . [ 1 6 9 6 4 1 . 2 0 6 2 : r Instruction fine-tuning of large language models (LLMs) often involves selecting subset of instruction training data from large candidate pool, using small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradientbased representations paired with greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/tar geted-instruction-selection. *Equal contribution 3Kempner Institute. <nnayak@seas.harvard.edu>. 2MIT 1Harvard University Correspondence to: Nihal V. Nayak Preprint. February 17, 2026. 1 Large language models (LLMs) have demonstrated remarkable ability to follow complex instructions through instruction fine-tuning on curated datasets of instructionresponse pairs (Hurst et al., 2024; Olmo et al., 2025; Agarwal et al., 2025). Constructing such datasets, however, typically requires careful experimentation and extensive ablation studies, making them both time-consuming and computationally expensive (Guha et al., 2025; Magnusson et al., 2025). This challenge is further exacerbated by the growing need to adapt LLMs to specialized downstream tasks under limited data or compute budgets (Thulke et al., 2024; Zhao et al., 2024b). As result, growing body of work studies how to automatically select subset of examples from large candidate pool that is most useful for given target taska problem commonly referred to as targeted instruction selection (Xia et al., 2024). Despite increasing interest, the literature on targeted instruction selection remains fragmented and difficult to interpret (Appendix A). Proposed methods vary widely in their formulations, often involve multiple design choices (e.g., representations, similarity metrics, selection algorithms), and lack key baselines, such as the zero-shot baseline. Empirical results are inconsistent, and it remains unclear which techniques actually drive performance, when, and why. In this work, we bring clarity to this space through disentangled framework that separates the two core ingredients of targeted instruction selection: (i) the representation used to encode the data and (ii) the algorithm used to select examples based on these representations. This disentangled view allows us to isolate the effects of each component and enables controlled comparisons across models, datasets, and budgets. We further show that, despite differing in algorithmic detail, many selection methods can be unified under the view of approximate distance minimization between the selected subset and the target task distributions. We support this perspective with new generalization bounds that characterize when distance-based selection helpsand when it doesnt. Our empirical findings are mixed but revealing. Gradientbased representations are the only ones whose similarity Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) (or distance) to the query set reliably predicts performance (Section 5.1, Section 5.2), but even these fail to improve over zero-shot inference in some regimes (Appendix M). While no single method dominates, greedy round-robin selection tends to perform best at small budgets, while optimal transport-based methods offer modest gains at larger ones (Section 5.2). Surprisingly, randomly sampled subsets often match or outperform many popular selection methods (TyDiQA and MMLU-Pro in Figure 5), especially as the budget increases, highlighting the brittleness of current practice. Our work makes the following contributions: We propose disentangled framework for targeted instruction selection that isolates the effects of (i) data representations and (ii) selection algorithms, enabling controlled comparisons. We conduct systematic empirical analyses across multiple target tasks and LLMs, showing that gradient-based data representations correlate most strongly with the loss and that greedy round-robin selection performs best at low budgets, while optimal transport-based methods perform best at high budgets. We develop unified theoretical perspective that interprets many selection algorithms as approximate distance minimization, and prove generalization bounds that explain both the benefits and limitations of this view. 2. Targeted Instruction Selection Let = {zi = (xi, yi)}N i=1 be large candidate pool and = {qj = (xj, yj)}M j=1 be the query set drawn from the distribution PT of the target task . We consider model fθ parameterized by θ. For sample = (x, y), we define the per-example loss as ℓ(θ; z) := ℓ(fθ(x), y). The goal of targeted instruction selection is to choose subset of examples of size (training budget) and train the model fθ using only examples in so as to minimize the expected loss on the target task : = arg min SD S=B EzPT [ℓ(θS ; z)] . Since we do not have access to the target tasks during training and selection, we use as proxy for selection and choose to solve the objective: ˆS = arg min SD S=B 1 M (cid:88) j=1 ℓ(θS ; qj) . Optimizing this objective is impractical and combinatorially expensive, which has motivated compute efficient instruction selection methods. 2 Figure 1. Disentangled view of targeted instruction selection. First, the query set (stars) and candidate pool (dots) are encoded as data representations. Then, for given budget, using the data representations for the query and candidates, we perform targeted selection (denoted by the dotted line) using selection algorithm such as greedy round-robin. 3. Disentangled View of Instruction"
        },
        {
            "title": "Selection",
            "content": "In this section, we present the disentangled view of targeted instruction selection. Then, we describe the commonly used data representations and selection algorithms. 3.1. Disentangling Data Representation and Selection Algorithm We adopt disentangled view of instruction selection that separates two key components: (i) data representation, and (ii) the selection algorithm (Figure 1). Data Representation: First, we encode instructionresponse pairs from the candidate pool and query sets into feature vectors (their representations). Ideally, these representations capture candidate-query distances that predict performance on the target task. Selection Algorithm: Next, the selection algorithm uses the data representations for the candidate pool and the query set to compute the similarity (or distances) between them and then select examples from the candidate pool based on the distances, where is the budget. With this view, we aim to isolate the effects of data representations and selection algorithms from prior work that reports the best results on targeted instruction selection (Liu et al., 2024b; Xia et al., 2024; Ivison et al., 2025). 3.2. Data Representation Here, we describe three data representation approaches for encoding the samples (More details in Appendix E). In this work, for fair comparison, all these data representations are used to compute cosine similarity matrix between the query and the candidate samples. RDS+. We call the data representation in Ivison et al. (2025) as RDS+ and discuss the selection algorithm (RR) in Section 3.3. RDS+ computes the representations for the samples in the query and candidate pool by taking position-weighted mean of the hidden states from the base Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) language model (Muennighoff, 2022). EMBED. Here, the query and candidate samples are passed through an off-the-shelf sentence encoder, such as GTR-T5 (Ni et al., 2022), to produce representations. Since these are typically much smaller, they significantly reduce FLOPs compared to RDS+. We follow the setup from Ivison et al. (2025) to produce these representations. LESS. Xia et al. (2024) propose LESS (Low-rank gradiEnt Similarity Search), an optimization-aware influence formulation that represents query and candidate samples as low-dimensional gradient features. LESS estimates influence using first-order approximation of training dynamics (Pruthi et al., 2020) and adapts it to Adam (Kingma & Ba, 2015). Concretely, it averages the cosine similarity between the query gradient and the candidates Adam update vector across multiple training checkpoints. To make this approach scalable, LESS computes LoRA gradients (Hu et al., 2022) and applies random projection to obtain compact vectors, resulting in an efficient and reusable gradient datastore (Johnson & Lindenstrauss, 1984; Park et al., 2023). LESS is part of broad literature on data attribution and influence estimation, which seeks to quantify the effect of individual training examples on model (Grosse et al., 2023; Kwon et al., 2024; Ruis et al., 2025; Chang et al., 2025; Wang et al., 2025). For simplicity, we use LESS representations to encode the candidate pool and query sets (Appendix for details). 3.3. Selection Algorithm We describe selection algorithms for targeted instruction selection (More details in Appendix F). Greedy Round-Robin (RR). We consider the greedy round-robin from Ivison et al. (2025). For each query sample, RR selects the sample from the candidate pool with the highest cosine similarity, adds the sample to the subset, and then removes it from the candidate pool. The algorithm repeats the round-robin process across all query samples until the data selection budget is exhausted. Doubly Greedy (DG). We consider the doubly greedy selection algorithm from Xia et al. (2024). Given similarity matrix, DG assigns each candidate sample an influence score equal to the maximum similarity it has with any query point. Then, DG selects the top-B influential candidates to create the subset. KNN-Uniform. KNN-Uniform is selection algorithm inspired by optimal transport (Liu et al., 2024b). KNNUniform proposes closed-form solution based on nearest neighbors to avoid explicitly solving the optimal transport problem. The algorithm first determines based on the trade-off between the alignment and diversity. After determining K, for each query, uniform probability mass is assigned to the nearest neighbors, and then top-B candidates with the highest mass (summed over queries) are selected. KNN-KDE. KNN-KDE builds on the closed-form solution of KNN-Uniform by adding an additional regularizer to reduce the effect of near duplicates in the candidate pool (Liu et al., 2024b). Instead of assigning uniform mass to nearly identical points, Liu et al. (2024b) proposes incorporating kernel density estimation (KDE) as regularizer. Then, for each query example, the nearest neighbors are assigned probability mass weighted by the inverse of their density estimates, and the top-B candidates are selected. Unbalanced OT (UOT). Here, we propose new selection algorithm based on unbalanced optimal transport (Chizat et al., 2018), which we refer to as UOT. Unlike KNN-Uniform, in UOT, we explicitly solve the optimization problem of transporting mass from the query set to the candidate pool, penalizing marginal deviations (Appendix C). This allows us to ignore outliers and less relevant samples for the target task. After solving for the transport plan, we sum the plan over the rows (queries) and choose the top-B candidates with the highest mass (Appendix F.2 for implementation). 4. Experimental Setup We closely follow the setup from Ivison et al. (2025) and Xia et al. (2024). Following Xia et al. (2024), we primarily use Llama-2-7B as the base model and train on the selected subset of training data. In Appendix M, we include experiments with additional models. Across all data representations, selection algorithms, and models, we use cosine similarity (or cosine distance) to measure the similarity between the query-candidate pairs. We experiment with the following target tasks: BBH, Codex, GSM8K, TyDiQA, and MMLU-Pro (Appendix B). We use the subsampled Tulu V2 dataset from Ivison et al. (2025) as the candidate pool. More training details and hyperparameters are included in Appendix G. We use the evaluation code from Ivison et al. (2025) and lm-eval-harness to evaluate the models and report the downstream metrics. We also report the average cross entropy loss over the response tokens on the query set (query loss). 5. Experiments and Analysis 5.1. Does Subset Distance to Query Predict Performance? In this experiment, we aim to understand the relationship between the subset distance to the query and performance. The goal is to determine which data representation creates 3 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 2. Query loss vs. subset-query distance quantile. We stratify candidates into 10 distance quantiles (1 = closest, 10 = farthest) using each representation, select 500 examples per quantile using the RR selection algorithm, and train the Llama 2 7B model. We report query-set cross-entropy loss and Spearman correlation per target task. LESS (RR) exhibits strong monotonic increase in loss with distance (high positive Spearman correlation), whereas RDS+ (RR) and EMBED (RR) show weak or inconsistent correlations. Figure 3. Downstream performance vs. subset-query distance quantile. Using the same quantile construction and training protocol as Figure 2, we evaluate downstream task performance across distance quantiles and report Spearman correlation per target task. LESS (RR) shows strong negative correlation across most target tasks, while RDS+ (RR) and EMBED (RR) exhibit weaker, less consistent trends. subsets whose distances predictably correlate with loss and downstream performance. If there is correlation, we can formulate optimization objectives to minimize the distance between the query set and the candidates, thereby achieving the highest downstream performance. In Section 6, we also theoretically motivate this view by showing that minimizing the subset-query distance translates into minimizing the loss. Setup. We consider all the data representation methods from Section 3.2. Given cosine similarity matrix, we run the greedy round-robin selection algorithm over the candidates to obtain an ordering, which we then use to stratify them into distance quantiles. The quantiles are indexed from 1 to 10, where 1 corresponds to the subset closest to the query and 10 to the farthest. Then, we select the top 500 training examples from each quantile and train Llama 2 7B on them. Results. Figure 2 shows that only quantiles created by LESS (RR) highly correlate with the loss on the query set. On the other hand, both RDS+ (RR) and EMBED (RR) show very low Spearman correlation Across all target tasks, models trained on subset in the first distance quantile using LESS (RR) achieve the lowest loss. In contrast, RDS+ and EMBED may not often show the lowest loss in their first quantile. To our surprise, we find that both methods can sometimes have the lowest loss in the last distance quantile (see BBH for RDS+ (RR) and MMLU-Pro for EMBED (RR)). Figure 3 shows that LESS (RR) shows strong negative Spearman correlation (lower is better) across four out of the five downstream tasks. However, we do find that lower loss in Figure 2 does not necessarily translate to the highest downstream performance in the first quantile. For instance, we see that EMBED (RR) performs better than LESS (RR) on four out of the five target tasks despite having much higher loss on the query set. We analyze the behavior of these representations by further subdividing the first quantile in Appendix H. Overall, LESS is the only representation that creates subsets whose distance correlates predictably with query loss and downstream performance. In Appendix M, across additional models, we find that LESS consistently shows strong correlation across target tasks, but the trends are less consistent on downstream performance with newer, over-trained models. 5.2. Effect of Data Representation across Subset Budgets Here, we select subsets with different data representations while keeping the selection algorithm fixed to study performance as the budget increases. Setup. We fixed the selection algorithm to greedy round-robin approach, since Ivison et al. (2025) showed strong performance across tasks. Given cosine similarity matrix from data representation method, we select samples from the candidate pool where {500, 1,000, 2,500, 5,000, 10,000}. Then, we train the base Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 4. Query loss vs. budget for different data representations (fixed selection algorithm). Using greedy round-robin selection and the query-candidate pool similarity, we select subsets of size {500, 1000, 2500, 5000, 10000}, train Llama 2 7B on them, and report average cross entropy loss averaged across three seeds and the standard error. Random averages over three uniformly sampled subsets from the candidate pool. LESS (RR) achieves the lowest loss across target tasks, while RDS+ (RR) and EMBED (RR) can underperform Random at larger budgets. Figure 5. Downstream performance vs. budget for different data representations (fixed selection algorithm). With the same greedy round-robin selection and budgets as Figure 4, we report the downstream performance for different data representations averaged across three random seeds and the standard error. LESS (RR) performs best on BBH, TyDiQA, and MMLU-Pro, whereas RDS+ (RR) performs the best on GSM8K and is competitive with Random on Codex. model, Llama 2 7B, on the selected samples and report the downstream performance. We also include Random baseline that uniformly samples candidates without replacement from the candidate pool. For all methods, we train three models with different seeds and report the average performance and standard error. For the Random baseline, we sample three times from the candidate pool, train one model on each, and report the average performance. Results. Figure 4 shows that LESS (RR) achieves the lowest loss on the query set across all the target tasks under different budget constraints. We observe that RDS+ (RR) and EMBED (RR) can have higher loss than the Random baseline. Finally, we see that the query loss can increase or may stabilize as we continue increasing the budget. Figure 5 shows that LESS (RR) outperforms the other methods by clear margin on three out of the five target tasks. Next, we observe that RDS+ (RR) achieves higher performance across all budget constraints on GSM8K, but Random is competitive baseline at larger budgets on Codex. Lastly, we note that Random is strong baseline but often underperforms targeted instruction selection methods at low budgets. Overall, these results show that, under fixed selection algorithm (greedy round-robin), gradient-based representations (LESS) perform better than model-based representations on majority of the target tasks. Across models and downstream target tasks that benefit from additional training (i.e., zero-shot performance is not saturated), we see that LESS often outperforms other baselines (Appendix M). However, key limitation of LESS representations is that they are computationally more expensive than model-based representations, as they require forward and backward pass over the entire candidate pool. For this reason, in Appendix J, we further investigate whether subsets created by smaller proxy LESS models offer cheaper alternative, and show that even models with 135M parameters can select examples for training larger models and dramatically reduce FLOPs. Finally, we find that subsets selected by model-based representations are competitive and often outperform Random under constrained budgets. 5.3. Effect of Selection Algorithms across Subset Budgets Here, we select subsets with different selection algorithms while keeping the data representation fixed to study the performance as the budget increases. Setup. The experimental setup closely follows Section 5.2. Building on our insights from Section 5.2, we fix LESS as our data representation. Given the LESS representations for the query set and candidate pool, we use Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 6. Query loss vs. budget for different selection algorithms (fixed data representation). Using LESS representations and the query-candidate pool cosine similarity (or distance), we select subsets of size {500, 1000, 2500, 5000, 10000} with each selection algorithm, train Llama 2 7B on them, and report average cross entropy loss on the query set averaged across three seeds and the standard error. Random averages over three uniformly sampled subsets from the candidate pool. UOT achieves the lowest loss on three of the five datasets and remains competitive on the others, while DG often underperforms, yielding the highest loss on three datasets. Figure 7. Downstream performance vs. budget for different selection algorithms (fixed data representation). With the same data representation and the budgets as Figure 6, we report downstream performance for different selection algorithms averaged across three seeds and the standard error. RR tends to perform best at smaller budgets, whereas UOT and KNN-KDE perform better at larger budgets; DG consistently underperforms across three of the five datasets. the selection algorithm to select samples for budgets {500, 1,000, 2,500, 5,000, 10,000}. We use the weighted cosine similarity matrix between the query and the candidate pool (Equation 2 in the Appendix) to select instructions using greedy round-robin (RR) and doubly greedy (DG). We convert this cosine similarity matrix into cosine distance matrix to select samples with KNN-Uniform, KNNKDE, and UOT (Appendix F.1 and F.2). Following Liu et al. (2024b), we also include results for KNN-Uniform and KNN-KDE using L2 distance in Appendix K. Results. Figure 6 shows that across the five tasks, UOT achieves the lowest loss on three of them, while remaining competitive on the others. On the other hand, we see that DG often has the highest loss across tasks at large budgets. Figure 7 shows the downstream performance after training Llama-2-7B on the selected subsets. We observe that LESS (RR), the greedy round-robin selection algorithm, performs well on low budgets. On the other hand, LESS (UOT) and LESS (KNN-KDE) perform better with higher budgets. Finally, we find that the doubly greedy (DG) algorithm consistently underperforms other selection algorithms, suggesting that it might be choosing samples closest to only subset of the queries. Overall, our results show that greedy round-robin (RR) selection works best under limited budget constraints, but these gains generally diminish as more examples are added. At larger budgets, optimal transportbased selection methods (UOT and KNN-KDE) provide greater benefits. In Appendix M, we find that the performance trends observed in this experiment generalize to other models. For example, LESS (RR) performs best on BBH with Llama 3.2 3B and Qwen3 4B Base. We also observe that LESS (UOT) performs best on MMLU-Pro at high budgets with Olmo 3 7B Base. These results suggest that selection algorithms exhibit similar performance trends across models for particular target tasks. 6. Unifying View: Instruction Selection as"
        },
        {
            "title": "Set Distance Minimization",
            "content": "While instruction selection algorithms vary in form, from greedy heuristics to transport-based selection, they often share deeper, unifying principle: selecting subset that is close to the query set in the representation space. In this section, we formalize and motivate this perspective. Concretely, we observe that broad class of existing methods (including those discussed in Section 3.3) can be viewed as approximately minimizing distributional distance between the selected subset and the query set Q: 6 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Greedy algorithms (e.g., round-robin, doubly greedy) aim to reduce the average or maximum similarity-based distance between and Q. Density-based methods (e.g., KNN-Uniform, KNN-KDE) select points from high-density regions near Q, implicitly matching the mass of the subset distribution to the query. Optimal transport approaches (e.g., UOT) directly solve for transport coupling minimizing cost between empirical distributions associated with and Q. Despite differing in implementation and exact objective, all these methods can be understood as minimizing distance function Dist(S, Q), where the choice of distance reflects the algorithms inductive biases. This unified view not only clarifies relationships between methods, but also explains their performance patterns. In the remainder of this section: We show that reducing the subset-query distance tightens generalization bound on downstream loss (Section 6.1). We show that the benefit of distance-aware selection over random sampling diminishes with budget, and characterize this tradeoff (Section 6.2). 6.1. Minimizing Subset-Query Distance Tightens Generalization Bound (cid:80) We establish theoretical bound formalizing the intuition that selecting subset whose empirical distribution is close to that of the query set leads to improved performance on the target task . We show that the empirical test lossLT (θS) := 1 zT ℓ(θS; z) for loss function ℓ(θS; z) and θS arg minθΘ LS (θ) an empirical risk minimizer trained on S, is upper-bounded by the 1-Wasserstein distance between the empirical distributions of the selected subset and the query set, denoted by W1( ˆPS , ˆPQ). Here, ˆPS and ˆPQ denote the empirical distributions associated with and Q, respectively. This result follows by viewing subset selection as two-stage domain adaptation problem: first from (source) to (target), and then from (source), to (target). Bounds of this form are well established in the domain adaptation literature, where optimal transport distances between source and target distributions are known to control transfer error (Redko et al., 2017; Courty et al., 2017). In our setting, this bound implies that algorithms which approximately minimize W1( ˆPS , ˆPQ) directly tighten theoretical upper bound on target task performance. Theorem 6.1. Let ℓ : Θ R+ be loss function, where Rd denotes the data space and Θ the parameter space. Assume that ℓ is symmetric, convex, bounded, satisfies the triangle inequality, and for = (x, y) admits the parametric form ℓ(θ; z) = fθ(x)q for some > 0. Let denote labeled candidate pool, and let be any subset of size := S. Let (the query set) and (the test set) be labeled datasets, and assume min(T , S). Then for any > and < 2, there exists constant N0, depending on d, such that for any δ > 0 and N0 max(δ(d+2), 1), with probability at least 1 2δ: LT (θS ) W1( ˆPS , ˆPQ) (cid:124) (cid:125) (cid:123)(cid:122) Subset and query dataset distance + W1( ˆPQ, ˆPT ) (cid:125) (cid:123)(cid:122) (cid:124) Query and test dataset distance + LS (θS ) (cid:124) (cid:123)(cid:122) (cid:125) training error +ζ (cid:114) 2 log (cid:17) (cid:16) 1 δ + λ (1) where W1 is the 1-Wasserstein distance, ζ is constant 2 , and λ is the minigiven by ζ := 1 mum combined error LS (θ ) + 2LQ(θ ) + LT (θ ) over datasets of size B. 2 + 2Q 1 2 + 1 Proof in Appendix L.2 Among the terms in equation 1, only W1( ˆPS , ˆPQ) is directly affected by the choice of (for fixed Q, ). Consequently, selecting to (approximately) minimize W1( ˆPS , ˆPQ) is principled objective: any reduction in this transport distance directly tightens the right-hand side, and hence the worst-case upper bound on LT (θS ). The second Wasserstein term, W1( ˆPQ, ˆPT ), quantifies mismatch between the query distribution and the downstream evaluation distribution. This term is independent of S, and therefore sets an irreducible error on how informative is about . the same time, the theorem does not imply that At minimizing W1( ˆPS , ˆPQ) alone guarantees strong target performance. The bound also includes the training error LS (θS ) and the term λ. In the instruction-selection setting, these terms capture two additional failure modes: (i) may be close to yet noisy or internally inconsistent, leading to large LS (θS ); and (ii) there may be no single hypothesis that performs well simultaneously on S, Q, and , leading to large λ. The quantity λ can be interpreted as an ideal joint error term: it is small when the candidate pool contains size-B subset whose ERM achieves low loss on S, Q, and simultaneously. Consequently, if lacks coverage of the skills required by and , or if the hypothesis class cannot realize predictor that works well across these datasets, then no selection rule based purely on distribution matching can guarantee strong downstream performance. The remaining terms account for finite-sample effects through ζ. 6.2. Diminishing Returns of Query-Aware Selection as Budget Increases Computing the exact subset that minimizes the distance to the query set is combinatorial problem, and even approximate distance-minimization methods can be computationally expensive. In contrast, uniformly sampling samples from the candidate pool is essentially cost-free and becomes increasingly competitive as the selection budget grows. For analysis, we consider the random baseline obtained by sampling points i.i.d. from the empirical pool distribution ˆPD (i.e., with replacement). 7 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Then, for fixed budget B, Theorem 6.2 formalizes the benefit of query-aware selection by upper bounding the improvement in test loss achieved by training on Wasserstein-optimal subseti.e., the subset closest to Qrelative to this random sample of the same size. Theorem 6.2. Let the candidate pool Rd lie in set of diameter and 3. Let rnd := (s1, . . . , sB) be tuple of elements sampled i.i.d. uniformly from D, and let be subset of size that minimizes the 1-Wasserstein distance to the query set Q, i.e., arg minSD;S=B W1( ˆPS , ˆPQ). Assume (i) LS (θ) is µ strongly convex in θ, (ii) LT (θ) is K-Lipschitz, and (iii) θℓ(θ; z) is Gθz-Lipschitz with respect to z. Then, there exists constant Cd > 0, depending only on the dimension d, such that, with probability at least 1 2δ: LT (θS rnd) LT (θS ) (cid:32) CdB1/d (cid:123)(cid:122) (cid:125) (cid:124) Curse of dimensionality (cid:114) + log(1/δ) 2B (cid:124) (cid:125) (cid:123)(cid:122) Concentration bound + W1( ˆPD, ˆPQ) (cid:124) (cid:125) (cid:123)(cid:122) Pool-query mismatch + W1( ˆPS (cid:123)(cid:122) Distance residual , ˆPQ) (cid:125) (cid:124) (cid:33) Proof in Appendix L.3. Theorem 6.2 shows that the potential improvement in test loss from training on query-aware subset decreases as the budget increases. In particular, when 3, the leading B-dependent term in the upper bound scales as CCd B1/d (the B1/2 concentration term is lower order). Thus, up to the additive shift induced by poolquery mismatch W1( ˆPD, ˆPQ) (and the residual W1( ˆPS , ˆPQ)), we can define critical subset size for tolerance ε: requiring CCd B1/d ε yields (CCd/ε)d. This highlights the curse of dimensionality: in high-dimensional embedding spaces, making random sampling competitive can require to grow on the order of εd, whereas query-aware selection can achieve smaller loss gap at substantially smaller budgets. Figure 8 shows the change in performance gap between LESS variants and random sampling as the budget increases on MMLU-Pro. As increases, the gap shrinks, and the performance approaches the random sampling, which is qualitatively aligned with Theorem 6.2. While B1/d reference rate shows slow worst-case decay in high dimensions, we see that LESS (KNN-Unif.), LESS (KNN-KDE), and LESS (UOT) exhibit slower or similar decay up to 2,500 samples, suggesting the bound is non-trivial in this regime. Finally, the LESS variants approach the random baseline at different rates, suggesting that the choice selection algorithm affects the constants and residual errors. Figure 8. Convergence of LESS variants toward random sampling as budget increases. We plot the loss gap between random sampling and the LESS variants as function of budget (logscale) on MMLU-Pro. The gray reference line indicates the B1/d decay predicted by Theorem 6.2. We report the average difference in loss across one seed of LESS and three randomly sampled multisets of size B, along with the standard error. For visual comparison of decay rates, we apply constant offset to each LESS curve so that all methods start at B1/d where B0 = 500 and = 8192. 0 7. Related Work Subset Selection. Subset selection (or coreset selection) is fundamental task in machine learning, where the goal is to choose subset of the data from large candidate pool for training and achieve downstream performance similar to or better than that of training on the entire candidate pool (Wei et al., 2015; Huang et al., 2019; Moser et al., 2025). Over the years, several techniques involving clustering (Har-Peled & Mazumdar, 2004; Chen et al., 2023), gradient matching (Killamsetty et al., 2021), kernel thinning (Dwivedi & Mackey, 2024; Carrell et al., 2025), and proxy models (Ye et al., 2025; Magnusson et al., 2025) have been proposed to efficiently select core training samples in variety of domains (Hulkund et al., 2025). In this work, we focus on selecting subset of instruction-response pairs from candidate pool for target task. Instruction Selection. Instruction selection is key ingredient training in todays large language model pipeline (Olmo et al., 2025). While curation via careful experimentation in the post-training pipeline has led to dramatic performance improvements (Longpre et al., 2023; Guha et al., 2025), there is growing interest in automatically curating instruction tuning datasets (Liu et al., 2024a; Yin & Rush, 2025). We focus on instruction selection for target tasks, selecting samples within given budget from candidate pool. Existing work on automatic instruction selection often relies on heuristics such as length (Zhao et al., 2024a), perplexity (Ankner et al., 2024), number of reasoning (Li et al., 2025), task similarity (Xia et al., 2024; Ivison et al., 2025; Nikdan et al., 2025), and more (Albalak et al., 2024). Our focus is to understand targeted instruction selection that uses similarity between the query and the candidate pool to select subsets. Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) 8. Conclusion We disentangle the key components of targeted instruction selection and provide new critical empirical and theoretical insights. Our experiments reveal that gradient-based data representations create subsets whose distances to the query strongly predict performance. Exploring computationally cheaper alternatives to produce such representations is promising research direction (Appendix J). Our theoretical insight, which unifies several existing selection algorithms as approximate distance minimizers, provides general framework for creating new selection algorithms. Overall, we present practical roadmap for designing improved targeted instruction selection frameworks."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Yonatan Belinkov, Bingbin Liu, Lyndon Lam, and the members of the ML Foundations group and the Kempner Institute for thoughtful feedback on the manuscript. Nihal V. Nayak, Paula Rodriguez-Diaz, and David Alvarez-Melis acknowledge support from the NSF AI-SDM Institute (Award No. 2229881), the Kempner Institute, the FAS Deans Competitive Fund for Promising Scholarship, and the Aramont Fellowship Fund. Neha Hulkund is supported by the NSF GRFP DGE-2146755. Sara Beery acknowledges support by the Schmidt Sciences AI2050 Program, NSF Awards No. 2330423 and 2441060, NSERC Award No. 585136, and the MIT-IBM Watson AI Lab."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work aimed at advancing targeted instruction selection in large language models (LLMs). While our work comprehensively evaluates numerous LLMs across many target tasks, we rely on existing pre-trained LLMs. Further fine-tuning these pre-trained LLMs might amplify any pre-existing biases or might even hurt performance on unrelated tasks. Before deploying any of these models, we recommend conducting careful evaluations on target tasks and additional safety checks."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. gpt-oss-120b & gpt-oss-20b model card. ArXiv preprint, abs/2508.10925, 2025. URL https://arxiv.org/ abs/2508.10925. Albalak, A., Elazar, Y., Xie, S. M., Longpre, S., Lambert, N., Wang, X., Muennighoff, N., Hou, B., Pan, L., Jeong, H., Raffel, C., Chang, S., Hashimoto, T., and Wang, W. Y. survey on data selection for language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/ forum?id=XfHWcNTSHp. Survey Certification, Featured Certification. Allal, L. B., Lozhkov, A., Bakouch, E., von Werra, L., and Wolf, T. Smollm - blazingly fast and remarkably powerful, 2024. Allal, L. B., Lozhkov, A., Bakouch, E., Blazquez, G. M., Penedo, G., Tunstall, L., Marafioti, A., Kydlıˇcek, H., Lajarın, A. P., Srivastav, V., et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Alvarez-Melis, D. and Fusi, N. Geometric dataset distances via optimal transport. Advances in Neural Information Processing Systems, 33:2142821439, 2020. Ankner, Z., Blakeney, C., Sreenivasan, K., Marion, M., Leavitt, M. L., and Paul, M. Perplexed by perplexity: Perplexity-based data pruning with small reference models. ArXiv preprint, abs/2405.20541, 2024. URL https://arxiv.org/abs/2405.20541. Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, pp. 214223. JMLR.org, 2017. Bakouch, E., Ben Allal, L., Lozhkov, A., Tazi, N., Tunstall, L., Patino, C. M., Beeching, E., Roucher, A., Reedi, A. J., Gallouedec, Q., Rasul, K., Habib, N., Fourrier, C., Kydlicek, H., Penedo, G., Larcher, H., Morlon, M., Srivastav, V., Lochner, J., Nguyen, X.-S., Raffel, C., von Werra, L., and Wolf, T. SmolLM3: smol, multilingual, longcontext reasoner. https://huggingface.co/ blog/smollm3, 2025. Benamou, J.-D. Numerical resolution of an unbalanced mass transport problem. ESAIM: Mathematical Modelling and Numerical Analysis, 37(5):851868, 2003. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., OBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Carrell, A. M., Gong, A., Shetty, A., Dwivedi, R., and In Forty-second InMackey, L. Low-rank thinning. ternational Conference on Machine Learning, 2025. URL https://openreview.net/forum?id= iAkg2nVmvN. Chang, T. A., Rajagopal, D., Bolukbasi, T., Dixon, L., and Tenney, I. Scalable influence and fact tracing for large 9 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) language model pretraining. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= gLa96FlWwn. Chen, H., Zhang, Y., Zhang, Q., Yang, H., Hu, X., Ma, X., Yanggong, Y., and Zhao, J. Maybe only 0.5% data is needed: preliminary exploration of low training data instruction tuning. ArXiv preprint, abs/2305.09246, 2023. URL https://arxiv.org/abs/2305.09246. Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. URL https://api.semanticscholar. org/CorpusID:235755472. Chizat, L., Peyre, G., Schmitzer, B., and Vialard, F.-X. Scaling algorithms for unbalanced optimal transport problems. Mathematics of computation, 87(314):25632609, 2018. Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, IEEE A. Optimal transport for domain adaptation. Transactions on Pattern Analysis and Machine Intelligence, 39:18531865, 2014. URL https://api. semanticscholar.org/CorpusID:13347901. Courty, N., Flamary, R., Habrard, A., and Rakotomamonjy, A. Joint distribution optimal transportation for domain adaptation. Advances in neural information processing systems, 30, 2017. Dwivedi, R. and Mackey, L. Kernel thinning. Journal of Machine Learning Research, 25(152):177, 2024. URL http : / / jmlr . org / papers / v25 / 21 - 1334 . html. Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):18, 2021. URL http: //jmlr.org/papers/v22/20-451.html. Flamary, R., Vincent-Cuaz, C., Courty, N., Gramfort, A., Kachaiev, O., Quang Tran, H., David, L., Bonet, C., Cassereau, N., Gnassounou, T., Tanguy, E., Delon, J., Collas, A., Mazelet, S., Chapel, L., Kerdoncuff, T., Yu, X., Feickert, M., Krzakala, P., Liu, T., and Fernandes Montesuma, E. Pot python optimal transport (version 0.9.5), 2024. URL https://github.com/PythonOT/ POT. Fournier, N. and Guillin, A. On the rate of convergence in wasserstein distance of the empirical measure. Probability theory and related fields, 162(3):707738, 2015. Gabriel, P. and Marco, C. Computational optimal transport with applications to data sciences. Foundations and Trends in Machine Learning, 11(5-6):355607, 02 2019. ISSN 1935-8237. doi: 10.1561/2200000073. URL https://doi.org/10.1561/2200000073. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., et al. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023. Guha, E., Marten, R., Keh, S., Raoof, N., Smyrnis, G., Bansal, H., Nezhurina, M., Mercat, J., Vu, T., Sprague, Z., et al. Openthoughts: Data recipes for reasoning models. ArXiv preprint, abs/2506.04178, 2025. URL https: //arxiv.org/abs/2506.04178. Har-Peled, S. and Mazumdar, S. On coresets for k-means and k-median clustering. In Proceedings of the thirtysixth annual ACM symposium on Theory of computing, pp. 291300, 2004. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= nZeVKeeFYf9. Huang, L., Jiang, S., and Vishnoi, N. Coresets for clustering with fairness constraints. Advances in neural information processing systems, 32, 2019. Hulkund, N., Maalouf, A., Cai, L., Yang, D., Wang, T.-H., ONeil, A., Haucke, T., Mukherjee, S., Ramaswamy, V., Shen, J. H., et al. Datasˆ 3: Dataset subset selection for specialization. ArXiv preprint, abs/2504.16277, 2025. URL https://arxiv.org/abs/2504.16277. 10 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. ArXiv preprint, abs/2410.21276, 2024. URL https://arxiv.org/ abs/2410.21276. Ivison, H., Wang, Y., Pyatkin, V., Lambert, N., Peters, M., Dasigi, P., Jang, J., Wadden, D., Smith, N. A., Beltagy, I., et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. ArXiv preprint, abs/2311.10702, 2023. URL https://arxiv.org/abs/2311.10702. Ivison, H., Zhang, M., Brahman, F., Koh, P. W., and Dasigi, P. Large-Scale Data Selection for Instruction Tuning. ArXiv preprint, abs/2503.01807, 2025. URL https: //arxiv.org/abs/2503.01807. Johnson, W. B. and Lindenstrauss, J. Extensions of Contempolipschitz mappings into hilbert space. rary mathematics, 26:189206, 1984. URL https: / / api . semanticscholar . org / CorpusID : 117819162. Khaddaj, A., Engstrom, L., and Madry, A. Small-tolarge generalization: Training data influences models In The Thirteenth Internaconsistently across scale. tional Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= 79ZkWgY2FI. et al. Naturalthoughts: Selecting and distilling reasoning traces for general reasoning tasks. ArXiv preprint, abs/2507.01921, 2025. URL https://arxiv.org/ abs/2507.01921. Liero, M., Mielke, A., and Savare, G. Optimal entropytransport problems and new hellingerkantorovich distance between positive measures. Inventiones mathematicae, 211(3):9691117, 2018. Liu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. URL https://openreview.net/ forum?id=BTKAeLqLMw. Liu, Z., Karbasi, A., and Rekatsinas, T. TSDS: data selection for task-specific model finetuning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024b. URL http : / / papers . nips . cc / paper files / paper / 2024 / hash / 13848b5893119ff772b69812c95914fa - Abstract-Conference.html. Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., De, A., and Iyer, R. K. GRAD-MATCH: gradient matching based data subset selection for efficient deep model training. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 54645474. PMLR, 2021. URL http://proceedings.mlr.press/v139/ killamsetty21a.html. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., and Roberts, A. The flan collection: Designing data and methods for effective instruction tuning. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 22631 22648. PMLR, 2023. URL https://proceedings. mlr.press/v202/longpre23a.html. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Magnusson, I., Tai, N., Bogin, B., Heineman, D., Hwang, J. D., Soldaini, L., Bhagia, A., Liu, J., Groeneveld, D., Tafjord, O., et al. Datadecide: How to predict best pretraining data with small experiments. ArXiv preprint, abs/2504.11393, 2025. URL https://arxiv.org/ abs/2504.11393. Kwon, Y., Wu, E., Wu, K., and Zou, J. Datainf: Efficiently estimating data influence in loRA-tuned LLMs and difIn The Twelfth International Conferfusion models. ence on Learning Representations, 2024. URL https: //openreview.net/forum?id=9m02ib92Wz. Li, Y., Emad, Y., Padthe, K., Lanchantin, J., Yuan, W., Nguyen, T., Weston, J., Li, S.-W., Wang, D., Kulikov, I., McDiarmid, C. On the method of bounded differences, pp. 148188. London Mathematical Society Lecture Note Series. Cambridge University Press, 1989. Moser, B. B., Shanbhag, A. S., Frolov, S., Raue, F., Folz, J., and Dengel, A. coreset selection of coreset selection literature: Introduction and recent advances. arXiv preprint arXiv:2505.17799, 2025. 11 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Muennighoff, N. Sgpt: Gpt sentence embeddings for semantic search. ArXiv preprint, abs/2202.08904, 2022. URL https://arxiv.org/abs/2202.08904. Ni, J., Qu, C., Lu, J., Dai, Z., Hernandez Abrego, G., Ma, J., Zhao, V., Luan, Y., Hall, K., Chang, M.-W., and Yang, Y. Large dual encoders are generalizable retrievers. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 98449855, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.669. URL https : / / aclanthology . org / 2022 . emnlp - main.669. Nikdan, M., Cohen-Addad, V., Alistarh, D., and Mirrokni, V. Efficient data selection at scale via influence distillation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=E6ZdfjtoiX. Olmo, T., Ettinger, A., Bertsch, A., Kuehl, B., Graham, D., Heineman, D., Groeneveld, D., Brahman, F., Timbers, F., Ivison, H., et al. Olmo 3. ArXiv preprint, abs/2512.13961, 2025. URL https://arxiv.org/ abs/2512.13961. Park, S. M., Georgiev, K., Ilyas, A., Leclerc, G., and Madry, A. TRAK: attributing model behavior at scale. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 2707427113. PMLR, 2023. URL https://proceedings.mlr.press/ v202/park23c.html. Pruthi, G., Liu, F., Kale, S., and Sundararajan, M. Estimating training data influence by tracing gradient descent. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, URL https : / / proceedings . virtual, 2020. neurips . cc / paper / 2020 / hash / e6385d39ec9394f2f3a354d9d2b88eec - Abstract.html. Redko, I., Habrard, A., and Sebban, M. Theoretical analysis of domain adaptation with optimal transport. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 737753. Springer, 2017. Ruis, L., Mozes, M., Bae, J., Kamalakara, S. R., Gnaneshwar, D., Locatelli, A., Kirk, R., Rocktaschel, T., Grefenstette, E., and Bartolo, M. Procedural knowledge in 12 pretraining drives reasoning in large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=1hQKHHUsMx. Schulman, J. and Lab, T. M. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20250929. https://thinkingmachines.ai/blog/lora/. Sejourne, T., Peyre, G., and Vialard, F.-X. Unfrom theory to numerbalanced optimal ics. ArXiv, abs/2211.08775, 2022. URL https: / / api . semanticscholar . org / CorpusID : 253553361. transport, Sinkhorn, R. relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876879, 1964. Solomon, J., Greenewald, K., and Nagaraja, H. $k$- variance: clustered notion of variance. SIAM Journal on Mathematics of Data Science, 4(3):957978, 2022. doi: 10.1137/20M1385895. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D., and Wei, J. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Rogers, A., BoydGraber, J., and Okazaki, N. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.824. URL https://aclanthology. org/2023.findings-acl.824. Thulke, D., Gao, Y., Pelser, P., Brune, R., Jalota, R., Fok, F., Ramos, M., Van Wyk, I., Nasir, A., Goldstein, H., et al. Climategpt: Towards ai synthesizing interdisciplinary research on climate change. ArXiv preprint, abs/2401.09646, 2024. URL https://arxiv.org/ abs/2401.09646. Villani, C. Optimal transport: old and new, volume 338. Springer, 2008. Wang, J., Lin, X., Qiao, R., Koh, P. W., Foo, C.-S., and Low, B. K. H. Nice data selection for instruction tuning in LLMs with non-differentiable evaluation metric. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=2wt8m5HUBs. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging In multi-task language understanding benchmark. Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers. nips . cc / paper files / paper / 2024 / hash / ad236edc564f3e3156e1b2feafb99a24 - AbstractDatasets and Benchmarks Track. html. Wei, K., Iyer, R., and Bilmes, J. Submodularity in data In International subset selection and active learning. conference on machine learning, pp. 19541963. PMLR, 2015. Xia, M., Malladi, S., Gururangan, S., Arora, S., and Chen, D. LESS: selecting influential data for targeted instruction tuning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=PG5fV50maR. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Ye, J., Liu, P., Sun, T., Zhan, J., Zhou, Y., and Qiu, X. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. In The Thirteenth International Conference on Learning Representations, 2025. Yin, J. and Rush, A. M. Compute-constrained data selection. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=4es2oO9tw1. Zhao, H., Andriushchenko, M., Croce, F., and Flammarion, N. Long is more for alignment: simple but In tough-to-beat baseline for instruction fine-tuning. Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https://openreview. net/forum?id=0AZAjkXhit. Zhao, T., Wang, S., Ouyang, C., Chen, M., Liu, C., Zhang, J., Yu, L., Wang, F., Xie, Y., Li, J., et al. Artificial intelligence for geoscience: Progress, challenges, and perspectives. The Innovation, 5(5), 2024b. 13 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Existing Work Data Representation Similarity Selection Algorithm Candidate Pool Budget Xia et al. (2024) Liu et al. (2024b) Ivison et al. (2025) LESS LESS RDS+ Cosine sim. L2 dist. Cosine sim. Doubly Greedy (DG) 13,533 KNN-KDE, KNN-Unif. Tulu V1 (Subset) Multiple 10,000 Round-Robin (RR) Tulu V1 (Subset) Tulu Table 1. Overview of the fragmented literature on targeted instruction selection. We summarize prior work and highlight differences in data representation, similarity between data representations, selection algorithm, candidate pool, and selection budget, which prevent us from systematically understanding which of these key factors contributes to performance. Note that Xia et al. (2024) uses 5% of the candidate pool as the budget, resulting in 13,533 samples whereas Liu et al. (2024b) uses 0.5%, 1.0%, and 5.0% of the candidate pool as the budget. A. Fragmented Literature on Targeted Instruction Selection Table 1 gives an overview of the fragmented literature on target instruction selection. We show that existing work, notably Xia et al. (2024), Liu et al. (2024b), and Ivison et al. (2025), widely vary in their data representations, similarity metrics, selection algorithm, candidate pool, and budget. While Xia et al. (2024) and Liu et al. (2024b) are similar in many ways, we observed that the selection algorithm in Liu et al. (2024b) makes additional changes to the data representation construction process and uses L2 distances between query and the candidates (Appendix K). These small but key differences prevent us from systematically comparing the selection algorithms, KNN-KDE and KNN-Uniform, introduced in Liu et al. (2024b). Finally, Ivison et al. (2025) uses Tulu V2 mixture (Ivison et al., 2023) as the candidate pool but also changes the data representation, similarity metric, and budget. For these reasons, we aim to bring clarity to this important but rather fragmented literature through systematic experiments. B. Target Tasks We closely follow the target tasks from Ivison et al. (2025). In addition, we include the MMLU-Pro dataset (Wang et al., 2024). Table 2 provides statistics of the target tasks used in this work. Below, we provide descriptions of the target tasks: BBH (BIG-Bench Hard). BBH is curated subset of tasks from BIG-Bench that are empirically difficult for language models without advanced reasoning strategies. The benchmark includes tasks such as logical deduction, causal reasoning, temporal reasoning, symbolic manipulation, and algorithmic problem solving. BBH is commonly used to assess emergent reasoning abilities and sensitivity to prompting strategies, particularly few-shot and chain-of-thought prompting. We use the few-shot examples across all subtasks (3 27 = 81 samples) as our query set. In the query set, all the few-shot examples are treated as individual examples. We follow the evaluation in (Suzgun et al., 2023) and evaluate the model with 3-shot chain-of-thought. We report the average exact match on the test set. Codex. Codex is an evaluation suite for coding-based models based on the correctness of completing coding tasks at scale, testing advanced reasoning ability over short and long-context coding tasks (Chen et al., 2021). We closely follow the custom query-set/test-set split from Ivison et al. (2025), which divides the existing dataset of 164 examples into query set of 16 samples and uses the remaining examples as the test set. For downstream evaluation on the test set, we report the pass@10 with sampling temperature of 0.8. GSM8K. GSM8K is dataset of grade-school-level math word problems that require multi-step arithmetic reasoning. Each problem is paired with detailed, step-by-step solution that explicitly outlines the reasoning process leading to the final answer. The dataset is widely used to evaluate numerical reasoning, chain-of-thought capabilities, and the ability of language models to perform symbolic manipulation and logical decomposition rather than surface-level pattern matching. We treat the 8-shot examples as individual samples and use them as our query set. We evaluate the test set with the 8 in-context samples using the chain-of-thought and report the exact match. TyDiQA. TyDiQA is multilingual question answering dataset built from Wikipedia that contains real information-seeking questions written by native speakers in 11 typologically diverse languages, avoiding translation artifacts. The dataset is widely used to evaluate multilingual and cross-lingual QA robustness, especially under linguistic diversity and distribution shift. We follow Ivison et al. (2025) and evaluate the models in 1-shot setting across 9 languages where the answer is provided in the passage. We use the 1-shot samples across the languages as the query set. 14 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Target Task Num. Query Set Samples Num. Test Set Samples Target Task Evaluation BBH (BIG-Bench Hard) Codex GSM8K TyDiQA MMLU Pro 81 16 8 9 70 6,511 148 1,319 5,077 12,032 3-Shot 0-Shot 8-Shot 1-Shot 0-Shot Table 2. Statistics of target tasks used in our experiments. The query set is used to select samples from the candidate pool to train the base model, and then evaluated on the test set from the target task. Each target task is evaluated using 0 to few-shot in-context examples, and we reuse the query set samples as these few-shot examples if mentioned in Appendix B. MMLU Pro. MMLU-Pro is designed to evaluate advanced reasoning across broad set of academic and professional domains, emphasizing multi-step reasoning and conceptual understanding, using carefully curated questions with reduced surface cues and strong controls against memorization or data leakage. It is commonly used to probe the upper limits of model reasoning performance in settings that more closely reflect expert-level problem solving. We treat the few-shot samples in the validation set as individual samples and use them as our query set. We perform zero-shot chain-of-thought evaluation on the test set and report exact match scores. In contrast to Ivison et al. (2025), we do not include MMLU and SQuAD in the main experiments, as these datasets are largely saturated, making it harder to see clear performance trends. We also exclude AlpacaEval from the comparison because it relies on the GPT API as an evaluator, and conducting large-scale study would be prohibitively expensive. C. Background on Optimal Transport Optimal Transport (OT) is principled approach for comparing probability distributions based on their underlying geometry, with strong theoretical guarantees (Villani, 2008). In machine learning, OT has been applied to variety of domains, including domain adaptation (Courty et al., 2014), generative modeling (Arjovsky et al., 2017), and distance metrics (Alvarez-Melis & Fusi, 2020). The OT problem considers complete metric space with probability measures µ, ν P(X ), which can be either discrete or continuous. The Kantorovich formulation of the transportation problem is defined as: OT(µ, ν) = min πΠ(µ,ν) (cid:90) c(x, y)dπ(x, y) where c(x, y) is cost function over R+ and the set of couplings Π(µ, ν) is defined as the joint probability distributions over the product space with marginals µ and ν such that Π(µ, ν) = {π P(X )P1#π = µ, P2#π = ν}. When has given distance metric dX , this is often treated as the cost function such that c(x, y) = dX (x, y)p for some 1. This is commonly defined as the pWasserstein distance where Wp(µ, ν) = OT(µ, ν) 1 . In the discrete optimal transport setting with finite samples, as considered in this work, let µ and ν be discrete probability measures defined as µ = (cid:80)n j=1 vjδyj , where {xi}n j=1 are points in metric + , and (cid:80) space (e.g., feature embeddings), with Rn vj = 1. This is now linear program and can be solved using classical solvers; however, its worst-case complexity is O(N 3), which is often prohibitive in large-scale settings. i=1 uiδxi and ν = (cid:80)m i=1 and {yj}m ui = (cid:80) +, Rm C.1. Entropy-Regularized Optimal Transport To improve computational efficiency, entropy regularization is commonly applied: OT(µ, ν) = min πΠ(µ,ν) (cid:90) c(x, y)dπ(x, y) + εH(πµ ν) where H(πµ ν) = (cid:82) log(dπ/dµdν) denotes the entropy of the transport plan. This objective is strongly convex and differentiable, and can be solved efficiently using the SinkhornKnopp algorithm for the discrete case (Sinkhorn, 1964). 15 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) C.2. Unbalanced Optimal Transport Classical optimal transport assumes that the two measures have equal total mass. In many applications, such as noisy data, out-of-distribution comparison, or dataset selection, this assumption is undesirable. Unbalanced optimal transport relaxes the hard marginal constraints by penalizing deviations from the prescribed marginals (Benamou, 2003; Chizat et al., 2018; Liero et al., 2018; Gabriel & Marco, 2019; Sejourne et al., 2022)."
        },
        {
            "title": "The unbalanced optimal transport problem is defined as",
            "content": "min πM+(X ) (cid:90) c(x, y) dπ(x, y) + λ1 D(P1#π µ) + λ2 D(P2#π ν), where P1#π and P2#π denote the first and second marginals of π, and D() is divergence between non-negative measures, commonly chosen as the KullbackLeibler divergence. Adding entropy regularization yields the unbalanced entropy-regularized OT objective: min πM+(X ) (cid:90) c(x, y) dπ(x, y) εH(π) + τ1 KL(P1#π µ) + τ2 KL(P2#π ν), where H(π) = (cid:82) log enforced exactly, recovering the balanced entropy-regularized OT formulation. (cid:17) (cid:16) dπ dx dy dπ denotes the entropy of the transport plan. As τ1, τ2 , the marginal constraints are D. Details on LESS We now describe how LESS computes an influence matrix, which we use as similarity matrix for our selection algorithms (see Appendix E.3 for implementation details). First, we perform LoRA warmup training of the model fθW on small randomly sampled warmup set for epochs, saving the model parameters {θ(t) t=1 along with the corresponding optimizer states at the end of each epoch. Because LLMs are typically trained with Adam, LESS estimates influence via first-order approximation of Adam training dynamics (Kingma & Ba, 2015), extending earlier first-order influence approximations developed for SGD (Pruthi et al., 2020). Concretely, LESS represents each example using low-dimensional projected features derived from (i) the query gradient and (ii) the candidate Adam update, where the Adam update is preconditioned gradient computed from Adams firstand second-moment estimates stored in the optimizer state. To make this scalable, we apply random projection to these high-dimensional vectors to obtain compact representations in lower-dimensional subspace. Finally, we compute influence for every query-candidate pair by aggregating cosine similarities between the projected query gradients and the projected candidate Adam updates across checkpoints, weighted by the average learning rate between checkpoints. }T More formally, for each checkpoint t, we first compute (i) the query gradient and (ii) the candidate Adam update vector: θℓ (cid:16) θ(t) ; qi (cid:17) RP , and (cid:16) Γ θ(t) ; zj (cid:17) = , , (cid:17) ˆm t+1 ˆv t+1 + ϵ (cid:16) θ(t) ; zj β1mt + (1 β1) gt 1 βt+1 1 β2vt + (1 β2) g2 1 βt+1 , . gt := θℓ ˆm t+1 = ˆv t+1 = Next, we apply random projection Π RP to obtain low-dimensional representations: (cid:16) ℓ θ(t) ; qi (cid:17) (cid:16) = Πθℓ θ(t) ; qi (cid:17) , (cid:16) Γ θ(t) ; zj (cid:17) = ΠΓ (cid:16) θ(t) ; zj (cid:17) . 16 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Then the influence of candidate sample zj on query sample qi is computed as the weighted cosine similarity aggregated across checkpoints: Inf Adam(qi, zj) = (cid:88) t=1 ηt cos (cid:16) ℓ (cid:16) θ(t) ; qi (cid:17) (cid:16) , Γ θ(t) ; zj (cid:17) (cid:17) . (2) Here, qi is the i-th query sample, zj is the j-th candidate sample, m(t) and v(t) are the first and second moments from the saved checkpoint optimizer states obtained during warmup training, β1, β2, and ϵ are Adam-specific hyperparameters, and Π is random projection matrix whose entries are drawn from Rademacher distribution (i.e., Πij U({1, 1})) (Johnson & Lindenstrauss, 1984; Park et al., 2023). ηt is the average learning rate between the (t1)-th and t-th checkpoint. We compute Inf Adam(qi, zj) for all query-candidate pairs to obtain an RM influence matrix, which we use as the similarity matrix for instruction selection. E. Implementation Details: Data Representation E.1. RDS+ We use the weighted mean for RDS+ from Ivison et al. (2025) and Muennighoff (2022). For an input sequence of length L, we compute its representation as = (cid:80)L i=1 wihi, where is the token index, hi is the i-th hidden state from the base pre-trained language model, and wi = j=1 is the positional weight. We truncate both queries and candidates to (cid:80)L maximum length of 2048 tokens. E.2. EMBED Following Ivison et al. (2025), we use GTR-T5 Base (Ni et al., 2022) to get data representations for the candidate pool and query set. For the examples in the query set, we also include an additional prefix Instruct: Given sample, find the passages closest to that sample. nQuery: {query}. E.3. LESS We closely follow the LESS implementation of Xia et al. (2024). For warmup training, we sample 10,000 instructionresponse pairs from the candidate pool and train the base model with LoRA (Hu et al., 2022) using cross-entropy loss. Following the original LESS setup for Llama 2 7B, we apply trainable LoRA parameters to all attention blocks during warmup. We follow the same LoRA setting for Llama 3.2 3B. For the remaining models, following recent recommendations for LoRA training (Schulman & Lab, 2025), we apply trainable LoRA parameters to both the attention and MLP blocks. We use the LoRA hyperparameters from Xia et al. (2024) (rank=128, α = 512, dropout=0.1), and all other hyperparameters are provided in Appendix G. We train for 4 epochs and save each checkpoint (total of 316 steps). Next, we compute gradients of the average loss over response tokens with respect to each LoRA checkpoint to produce the SGD update vectors for the query set and Adam update vectors for the candidate pool. We then apply random projection using the TRAK package (Park et al., 2023) to map these update vectors into 8192-dimensional feature vectors, resulting in four sets of data representations for both the candidate pool and the query set. For fair comparison with RDS+ and EMBED, we do not average representations when query samples come from the same subtask; instead, we treat them as individual samples. We also compute the average learning rate across training steps between each checkpoint, normalize these values by dividing each by their sum, and finally compute the weighted cosine similarity between candidate and query representations to measure candidate influence on each query sample (Equation 2). F. Implementation Details: Selection Algorithms F.1. KNN KDE and KNN Uniform We use the KNN-KDE implementation from TSDS (Liu et al., 2024b) in the released code. We reimplement KNN-Uniform from Algorithm 1 in their paper. We further simplify the implementation by using native PyTorch rather than the FAISS library to compute the distances between data representations. For fair comparison with other methods, we use cosine distance rather than L2 distance between the query and candidate data representations. In KNN-KDE with LESS, we compute the cosine distance between candidate examples by modifying Equation 2 as Cij = 1(cid:80)T )). t=1 ηt cos( Γ(zi, ˆθt ), Γ(zj, ˆθt 17 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) We closely followed the hyperparameters from Liu et al. (2024b) for KNN-KDE and KNN-Uniform (if the hyperparameter is used) as set = 5,000 (prefetching nearest neighbors), KKDE = 1,000 (KDE neighborhood size), σ = 0.75 (kernel bandwidth), and = 5.0. We set α = 0.01 so that at least 10,000 candidates would have transported mass greater than zero. F.2. Unbalanced OT (UOT) Algorithm 1 describes the UOT selection algorithm, new selection algorithm that explicitly solves the unbalanced optimal transport problem to obtain transport plan, then selects the candidate samples with the highest mass transported. We use the sinkhorn unbalanced implementation from the POT package (Flamary et al., 2021; 2024) to solve the optimization problem. We set ε = 0.01, τ1 = , and τ2 = 0.0001. In all our experiments, we compute the cosine distance between the query set and candidate pool and normalize between 0 and 1 by dividing by 2 as follows Cij = (cid:16) 1 (cid:80)T /2 . We use this normalized cosine distance matrix as our cost matrix. )) (cid:17) t=1 ηt cos( Γ(zi, ˆθt ), Γ(zj, ˆθt Algorithm 1 Unbalanced OT Selection i=1, distance matrix (or cost matrix) between the query set and the candidate Input: Candidate pool = {zi = (xi, yi)}N pool RM , entropy regularization term ε, marginal relaxation terms τ1 and τ2 and budget D. Output: Selected subset ˆS. Π UOT(C, ε, τ1, τ2) Π 1N π argsort(r) ˆS for = 1 to do ˆS ˆS {zπk } // solve UOT (Appendix C) and get the transport plan // sum columns (row-sums): rj = (cid:80)N i=1 Πj,i // indices sorted by descending mi // select the top-B candidates end for return ˆS G. Training Details and Hyperparameters We follow standard supervised fine-tuning pipeline and train the base model with cross entropy loss over the response tokens. During supervised fine-tuning, we set the maximum sequence length to 2048 tokens. For this reason, we remove instruction-response pairs if the response does not appear in 2048 tokens to avoid zero loss. The preprocessed candidate dataset contains about 198K examples. We apply the chat template to both the candidate and the query set during instruction selection, training, and evaluation, except during zero-shot evaluation of the pre-trained base language model. Hyperparameters Values Learning rate Learning rate scheduler Number of epochs Warmup ratio Optimizer Adam betas Adam epsilon Weight decay Max. gradient norm Max. sequence length Effective batch size Mixed precision 2e-5 linear 2 0.03 AdamW (0.9, 0.999) 1e-8 0.0 1.0 2048 128 bf16 Table 3. Hyperparameters used to train the base models on the selected instructions. Table 3 lists all the hyperparameters used to train the base models on the selected data. We closely follow the training setup from Ivison et al. (2025). We run all the training and evaluation experiments on single NVIDIA H100 GPU with 80GB of memory. 18 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 9. Query loss across subset-query distance sub-quantiles and Spearman correlation. We further stratify the first distance quantile from Section 5.1 into 10 sub-quantiles (1 = closest, 10 = farthest), select 500 examples per sub-quantile, and train the Llama 2 7B model. We report loss on the query set and Spearman correlation per dataset. LESS (RR) maintains strong monotonic increase in loss with distance (high Spearman correlation), whereas RDS+ (RR) and EMBED (RR) show weak or inconsistent correlations. Figure 10. Downstream performance across subset-query distance sub-quantiles and Spearman correlation. Using the same sub-quantile construction and training protocol as Figure 9, we evaluate downstream task performance across sub-quantiles and report Spearman correlation per dataset (more negative is better). LESS (RR) shows stronger negative correlation on average, but the performance differences are small, suggesting that many subsets within the closest quantile result in similar downstream performance. H. Fine-Grained Stratification of the Nearest Distance Quantile Building on the distance quantile experiment (Section 5.1), we now aim to determine whether data representations can differentiate between very similar subsets, i.e., subsets that are closer to each other. Since we know the first distance quantile contains the samples most similar to the query set, we further subdivide it into 10 distance quantiles, select the top-K samples from each, and train the base models. Setup. We closely follow the experiment setup from Section 5.1 for creating the distance quantiles, training, and evaluation. We further subdivide the closest distance quantile (first distance quantile) into 10 distance quantiles using the same procedure. Then, we select the top-500 samples from the distance quantiles and use them as training data to train Llama 2 7B. Results. Figure 9 shows that LESS (RR) shows high Spearman correlation across target tasks with the query loss. On the other hand, the RDS+ (RR) and EMBED (RR) do not show strong correlations across target tasks and often exhibit negative correlations with the distance sub-quantiles, suggesting that these data representations cannot differentiate between similar subsets. Figure 10 further shows that while LESS (RR), on average, shows stronger negative Spearman correlation on the downstream evaluation compared to the other baselines, the difference in downstream performance appears to be similar. This suggests that multiple subsets in the first distance quantile can achieve similar downstream performance. I. Differences between Selected Subsets Here, we understand the differences between the selected subsets created using different data representations from Section 5.2. We compute average token length and Jaccard index across the selected subsets for query sets when the budget is 10,000 samples. Figure 11 shows that subsets created with LESS (RR) contain shorter sequences than those produced by RDS+ (RR) and EMBED (RR) across all target tasks. We suspect this bias toward longer sequences may explain the competitive performance of RDS+ (RR) and EMBED (RR) as we increase the budgets across target tasks (Zhao et al., 2024a). Finally, Figure 12 shows that RDS+ (RR) and EMBED (RR) have higher Jaccard indices compared to LESS (RR), suggesting that model-based 19 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 11. Average token length of the query set and the selected subsets with different data representations. We find that LESS (RR) is biased towards shorter sequences, whereas RDS+ (RR) and EMBED (RR) select subsets with longer sequences. Figure 12. Jaccard index between selected subsets created using different data representations. We find that model-based representations (RDS+ and EMBED) have higher Jaccard index compared to LESS. embeddings share greater similarity in the examples they select. J. Cheaper Proxies for LESS LESS data representations are computationally expensive as they require forward and backward pass over all the candidate samples. Here, we revisit the experiment of computing the LESS data representations from smaller proxy model and then use them to select samples from the candidate pool to train the larger model (Appendix D.5 in Xia et al. (2024)). We broaden the set of proxy models, varying in size and include pre-trained LLMs trained with different token budgets to better understand how the performance of the larger base model relates to size and token budgets. Setup. We consider the following proxy models: Pythia 160M (Biderman et al., 2023), SmolLM 135M (Allal et al., 2024), SmolLM2 135M (Allal et al., 2025), and Llama 3.2 3B (Grattafiori et al., 2024). We follow the same procedure to obtain the data representations for all proxy models (Appendix E.3 and Appendix G) and select samples for different budgets using greedy round-robin approach. Then, we train Llama 2 7B on the selected instructions and report the downstream performance. We also include LESS without the proxy models, along with the Random baseline, to better contextualize the results. Results. Figure 13 shows that, at higher budgets, smaller proxy models outperform the baseline LESS (RR, Llama 2 7B) on three out of the five target tasks. While these results suggest that proxy models for instruction selection are viable, similar to Xia et al. (2024), not all the proxy models perform well. We find that Pythia-160M performs as poorly as Random on several target tasks. Next, we see that LESS (RR, SmolLM-135M) and LESS (RR, SmolLM2-135M) outperform or match Random across all target tasks, and sometimes even outperform LESS (RR, Llama 2 7B). key difference between the two is that SmolLM135M is pre-trained on 600B tokens, whereas SmolLM2-135M is pre-trained on 2 trillion tokens. We observe that on some target tasks, such as BBH and GSM8K, LESS (RR, SmolLM2-135M) achieves higher performance at low budgets, whereas on the rest, LESS (RR, SmolLM-135M) either matches or outperforms LESS (RR, SmolLM2-135M). These results show that the relationship between training tokens and proxy data representation for instruction selection remains unclear. Finally, we observe that LESS (RR, Llama 3.2 3B) outperforms the baseline LESS (RR, Llama 2 7B) on three out of five target tasks, suggesting trade-off between proxy and downstream model sizes in how well they approximate instruction selection of the larger models. Overall, these results suggest that using proxy models for instruction selection to train larger models can dramatically reduce 20 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 13. Cheaper proxies for LESS (fixed selection algorithm). With the same greedy round-robin selection procedure and the budgets from Section 5.2, we report downstream performance of Llama 2 7B when LESS representations are computed using proxy models, averaged across three seeds and the standard error. SmolLM-135M and SmolLM2-135M consistently match or outperform the Random baseline across target tasks, whereas Pythia-160M often matches Random on several tasks. Llama 3.2 3B matches or outperforms LESS computed with Llama 2 7B on multiple target tasks, suggesting trade-off between proxy and target model size when approximating instruction selection. cost, but that more thorough investigation is necessary to understand how to choose proxy models (Khaddaj et al., 2025). K. KNN-Uniform and KNN-KDE with L2 Distance We now select instructions with KNN-Uniform and KNN-KDE selection algorithms and use L2 distances instead of cosine distance to match the original implementation in Liu et al. (2024b). Setup. We use the Llama 2 7B LESS representations for both the query set and the candidate pool to compute distances. Instead of computing the weighted average between the query and the candidate representation (Equation 2), following Liu et al. (2024b), we scale the representation for the epoch checkpoint by the average learning rate for that epoch, concatenate all the scaled representations across epochs, and normalize them by the L2 norm. Then, we compute the L2 distance between the query and the candidate data representations. In KNN-KDE, we use the concatenated candidate data representations to compute L2 distances between them. We follow the same hyperparameters from Appendix to select the instructions for given budget. We report the downstream performance of these two selection methods across three seeds. We contextualize these results by comparing them with KNN-KDE and KNN-Uniform when implemented with cosine distance. Figure 14. KNN-Uniform and KNN-KDE with L2 distance (fixed data representation). With the same Llama 2 7B LESS representations and the budgets from Section 5.3, we report downstream performance for KNN-Uniform and KNN-KDE when distances are computed with L2 (following Liu et al. (2024b)), averaged across three seeds and the standard error. We compare against cosine distance variants and find similar performance trends across budgets. Results. Figure 14 shows that both the KNN-KDE and KNN-Uniform with L2 show similar performance trends when cosine distance is used to compute the distances, which further validates our conclusions regarding the choice of selection algorithms in Section 5.3. Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) L. Proofs L.1. Lemmas used for proofs of Theorem 6.1 and Theorem 6.2 (cid:80) Lemma L.1 (Domain adaptation bound; adapted from Theorem 2 in (Redko et al., 2017)). Let and be two samples of size NS and NT drawn i.i.d from probability measures PS and PT on Rd, respectively. Let ˆPS := 1 xS δxS NS and ˆPT := 1 xT δxT , where δx is the Dirac measure at x, be the associated empirical measures. Let ℓ be any loss NT function that is symmetric, convex, bounded, obeys the triangular inequality, and for = (x, y) has the parametric form fθ(x)q for some > 0. Then, for any > and < 2, there exists some constant N0 depending on such that for any δ > 0 and min(NS, NT ) N0 max(δ(d+2), 1) with probability at least 1 δ for all datasets the following holds: (cid:80) LT (θD) LS(θD) + W1( ˆPS, ˆPT ) + + λS,T (3) (cid:32) (cid:114) 2 log (cid:17) (cid:16) 1 δ"
        },
        {
            "title": "1\nNS",
            "content": "+"
        },
        {
            "title": "1\nNT",
            "content": "(cid:33) where λS,T := LT (θ) + LS(θ) for θ that minimizes the combined error LT (θ) + LS(θ) over all possible model parameters θ. Assumptions for Lemma L.2 and Theorem 6.2 A1. (Strong convexity) For any fixed dataset Z, the empirical risk LD(θ) is µstrongly convex in θ. A2. (Smooth data dependence) The gradient θℓ(z; θ) is Gθz-Lipschitz with respect to the data point z, i.e., θℓ(θ; z) θℓ(θ; z) Gθzz z, for all θ in relevant neighborhood. A3. (Lipschitzness at the ERM) For fixed dataset D, the empirical risk LD is K-Lipschitz with respect to the model parameter. That is, for any parameters θ and θ, LD(θ) LD(θ) Kθ θ. Lemma L.2 (Wasserstein stability bound). Let be target dataset. Assume assumptions A1A3 hold. Then, for any two datasets and D, LT (θD) LT (θD) W1( ˆPD, ˆPD) with := KGθz µ . Proof. Let θD = arg minθ LD(θ) and θD = arg minθ LD(θ). Using strong convexity once at LD (Assumption A1) µθD θD LD(θD) LD(θD) = LD(θD) where the last equality comes from LD(θD) = 0 due to optimality. Similarly, because LD(θD) = 0 , then For any fixed θ, Assumption A2 and KantorovichRubinstein duality (Arjovsky et al., 2017) gives µθD θD LD(θD) LD(θD). Combining (4) and (5) θLD(θ) θLD(θ) GθzW1( ˆPD, ˆPD). θD θD Gθz µ W1( ˆPD, ˆPD). Assumption A3 gives LT (θD) LT (θD) KθD θD, which combined with (6) gives LT (θD) LT (θD) KθD θD Gθz µ W1( ˆPD, ˆPD). (4) (5) (6) (7) Lemma L.3 (High-probability bound for the Wasserstein distance of an empirical measure). Let = {z1, . . . , zN } Rd lie in set of diameter and 3. Let rnd := (s1, . . . , sB) be tuple of elements sampled i.i.d. uniformly from D, and let ˆPD and ˆPS rnd be the empirical distributions on and rnd respectively. Then there exists constant Cd > 0 depending only on (with = 2 fixed) such that, for any δ (0, 1), with probability at least 1 δ over the draw of rnd: W1( ˆPS rnd , ˆPD) CdB1/d + (cid:114) log(1/δ) 2B 22 (8) Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Proof. The proof proceeds by decomposing W1( ˆPS rnd , ˆPDc) into its expectation and its concentration around the mean. First, we use Theorem 1 in Fournier & Guillin (2015) with = 1 and = 2, applied on bounded support: since diam(supp( ˆPD)) , we have M2( ˆPD)1/2 . This theorem yields E[W1( ˆPSrand , ˆPc)] C(1, d, 2) (B1/d + B1/2), and because 3 implies B1/2 B1/d, we can absorb the B1/2 term into the constant, defining Cd := 2C(1, d, 2). Then, for dimensions 3: E[W1( ˆPS rnd , ˆPD)] CdB1/d (9) where Cd is constant depending only on dimension d. Next, we view (S rnd) := W1( ˆPS rnd , ˆPD) as function of the random subset rnd. Since the data lies in bounded domain of diameter , changing single data point in rnd changes the probability mass at that location by 1/B, and moves it by distance of at most . Therefore, the function satisfies the bounded difference property with constant ci = /B. Then, by McDiarmids inequality (McDiarmid, 1989), for any ϵ > 0: (cid:16) (cid:17) W1( ˆPS rnd , ˆPD) E[W1( ˆPS rnd , ˆPD)] ϵ exp (cid:32) 2ϵ2 i=1(/B)2 (cid:80)B (cid:33) = exp (cid:19) (cid:18) 2Bϵ2 2 (10) Similar arguments have been used, for example, by Solomon et al. (2022), who apply McDiarmids inequality to the Wasserstein distance viewed as function of two random datasets, taking expectations over both. In contrast, in our setting, we fix and treat the Wasserstein distance as function of the random subset rnd, taking the expectation over single dataset. In this case, the bounded-differences condition required by McDiarmids inequality follows directly from the stronger assumption that the support of D, and hence the support of rnd D, is bounded with diameter at most . Setting the right hand side in (10) to δ and solving for ϵ, we get ϵ = (cid:113) log(1/δ) 2B . Thus, with probability at least 1 δ: W1( ˆPS rnd , ˆPD) E[W1( ˆPS rnd, ˆPD)] + (cid:114) log(1/δ) 2B Combining the expectation bound (9) with the concentration bound (11) yields the final result: W1( ˆPS rnd , ˆPD) CdB1/d + (cid:114) log(1/δ) 2B (11) (12) L.2. Theorem 6.1 Let ℓ : Θ R+ be any loss function that is symmetric, convex, bounded, satisfies the triangle inequality, and for = (x, y) admits the parametric form ℓ(θ; z) = fθ(x)q for some > 0. Let denote labeled candidate pool, and let be any subset of size := S. Let (the query set) and (the test set) be labeled datasets. Then for any < 2, with probability at least 1 2δ: LT (θS ) W1( ˆPS , ˆPQ) (cid:124) (cid:125) (cid:123)(cid:122) Subset and query dataset distance + W1( ˆPQ, ˆPT ) (cid:123)(cid:122) (cid:125) (cid:124) Query and test dataset distance + LS (θS ) (cid:124) (cid:123)(cid:122) (cid:125) training error +ζ (cid:114) log (cid:17) (cid:16) 1 δ + λ (13) where W1 is the 1-Wasserstein distance with respect to the underlying metric on the embedding space Z, ζ is constant 2 , and λ is the combined error of the dataset determined by the size of the datasets, with ζ = 1 of size that minimizes the combined error of LS (θ ) + 2LQ(θ ) + LT (θ ). 2 + 2Q 1 2 + 1 Proof. The proof proceeds by applying Lemma L.1 twice. We first apply it to LT (θS ), treating as the target dataset and as the source dataset, with the function obtained by training on S. We then apply it to LQ(θS ), now treating as the 23 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) target dataset and as the source dataset, again using the prediction function trained on S. LT (θS ) LQ(θS ) + W1( ˆPQ, ˆPT ) + (cid:114) 2 log (cid:32) (cid:17) (cid:16) 1 δ LS (θS ) + W1( ˆPS , ˆPQ) + W1( ˆPQ, ˆPT ) + 1 (cid:112)Q (cid:114) 2 (cid:33) + 1 (cid:112)T (cid:32) (cid:17) (cid:16) 1 δ log (cid:114) 2 + λT ,Q (14) 1 + 2 (cid:112)Q + 1 (cid:112)T (cid:33) + λT ,Q + λQ,S (15) = LS (θS ) + W1( ˆPS , ˆPQ) + W1( ˆPQ, ˆPT ) + ζ log (cid:17) (cid:16) 1 δ + λ (16) L.3. Theorem 6.2 Let the candidate pool Rd lie in set of diameter and 3. Let rnd := (s1, . . . , sB) be tuple of elements sampled i.i.d. uniformly from D, and let be subset of size that minimizes the 1-Wasserstein distance to the arg minSD;S=B W1( ˆPS , ˆPQ). Assume assumptions A1A3 hold. Then, there exists constant query set Q, i.e., Cd > 0, depending only on the dimension d, such that, with probability at least 1 2δ: LT (θS rnd) LT (θS ) (cid:32) CdB1/d (cid:123)(cid:122) (cid:125) (cid:124) Curse of dimensionality (cid:114) + log(1/δ) 2B (cid:125) (cid:123)(cid:122) (cid:124) Concentration bound + W1( ˆPD, ˆPQ) (cid:124) (cid:123)(cid:122) (cid:125) Pool-query mismatch + W1( ˆPS (cid:123)(cid:122) Distance residual , ˆPQ) (cid:125) (cid:124) Proof. LT (θS rnd ) LT (θS ) CW1( ˆPS rnd , ˆPS ) (cid:16) W1( ˆPS rnd , ˆPQ) + W1(PS (cid:16) C (cid:17) , ˆPQ) W1( ˆPS rnd , ˆPD) + W1( ˆPD, ˆPQ) + W1(PS (cid:17) , ˆPQ) (cid:16) CdB1/d + (cid:114) log(1/δ) 2B + W1( ˆPD, ˆPQ) + W1( ˆPS (cid:17) , ˆPQ) (cid:33) (17) (18) (19) (20) Line 17 follows from applying Lemma L.2, line 18 applied triangle inequality by introducing the distance to the empirical distribution of Q, line 19 again uses triangle inequality by introducing the distance to the empirical distribution of D, and 20 is result of applying Lemma L.3 on W1( ˆPS rnd , ˆPD). M. Ablations We run ablations for experiments in Section 5 across four models of different sizes: Llama 3.2 3B (Grattafiori et al., 2024), SmolLM3 3B (Bakouch et al., 2025), Qwen3 4B Base (Yang et al., 2025), and Olmo 3 7B (Olmo et al., 2025). Below, we summarize our key takeaways from the experiment across models (See Figures 15, 16, 17, and 18): Distance quantile experiment. Only LESS (RR) creates subsets whose quantile distances strongly correlate with the loss. However, with over-trained models such as SmolLM3 3B Base and Qwen3 4B Base, the performance trends for LESS (RR) on datasets such as BBH and Codex show less correlation with the distance quantiles. Effect of data representation across subset budgets. Although no single data representation with fixed selection algorithm selects subsets that always perform the best, LESS with greedy round robin performs the best across all models on most of the target tasks compared to other model-based representation baselines. But on some datasets and models (MMLU Pro with Qwen3 4B Base), we observe that the zero-shot baseline outperforms even instruction-tuned models, suggesting that either the candidate pool may not have sufficient training examples to further improve the performance, or due to an accidental leakage of the target task in the pre-training corpus (Olmo et al., 2025). Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Effect of selection algorithm across subset budgets. We observe that the performance trends for different selection algorithm generalizes to other models. For instance, LESS (RR) achieves the strongest results on BBH with Llama 3.2 3B and Qwen3 4B Base. We also find that, at higher budgets, often LESS (UOT) delivers the best MMLU-Pro performance with Olmo 3 7B Base. Overall, these findings indicate that, for certain target tasks, selection algorithms tend to exhibit consistent performance across different models. Figure 15. Ablation experiments with Llama 3.2 3B. 25 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 16. Ablation experiments with SmolLM3 3B. 26 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 17. Ablation experiments with Qwen3 4B. 27 Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesnt) Figure 18. Ablation experiments with Olmo 3 7B."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Kempner Institute",
        "MIT"
    ]
}