{
    "paper_title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models",
    "authors": [
        "Alok Abhishek",
        "Lisa Erickson",
        "Tushar Bandopadhyay"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 0 1 3 4 2 . 3 0 5 2 : r BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models Alok Abhishek San Francisco, USA alok@alokabhishek.ai Lisa Erickson Boston, USA lisa.erickson@sloan.mit.edu Tushar Bandopadhyay San Francisco, USA tushar@kronml.com"
        },
        {
            "title": "Abstract",
            "content": "In this research, we introduce BEATS, novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve high score on this benchmark LLM must show very equitable behavior in their responses, making it rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65% of outputs generated by industry leading models contained some form of bias, highlighting substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models."
        },
        {
            "title": "Introduction",
            "content": "Characters from science fiction such as Iron Mans [1] JARVIS [2] and Interstellars [3] TARS [4] have captured our imaginations. They represent the aspiration for intelligent autonomous systems that exhibit human-like intelligence and abilities. Advancements in Generative AI (GenAI) have brought the realization of concepts previously confined to science fiction within humanitys reach. As Generative AI technologies have rapidly advanced and Large Language Models (LLMs) have achieved widespread adoption, concerns regarding their intrinsic biases have become increasingly salient. As Bolukbasi et al. showed in their (2016) study [5], AI systems are prone to reflecting existing societal prejudices present in the training data, generating important ethical and practical concerns. AI systems demonstrate bias across multiple dimensions, including gender, race and ethnicity, socioeconomic status and culture, religion, sexual orientation, disability, age, geography, political Integrating LLMs into critical decision-making systems across ideology, and stereotypes. healthcare, legal services, finance, and governance introduces substantial ethical issues, primarily stemming from their intrinsic biases, which can propagate systemic inequities [6]. Given the pervasive impact of these biases, empirical research to systematically assess the ethics and biases of LLMs is needed. framework using statistical methodologies to detect and mitigate biases and help develop strategies for fairer LLMs is also needed. This rigorous framework and empirical study will help in development of AI systems that operate fairly and transparently in line with societal values. Preprint. Under review."
        },
        {
            "title": "2 Proposed Framework - BEATS",
            "content": "To address this need, we present Bias Evaluation and Assessment Test Suite (BEATS), novel framework for detecting and measuring bias, ethics, fairness, and factuality (BEFF metrics) within LLMs. BEATS is an evaluation framework with quantifiable benchmark for assessing Bias, Ethics, Fairness, and Factuality (BEFF) metrics within LLMs, as shown in the Figure 1. The BEATS framework establishes systematic and scalable procedure for identifying and analyzing bias-related behaviors in different LLMs to enhance GenAI system transparency and ethical standards. Figure 1: System design of BEATS evaluation framework - the proposed framework for bias assessment in LLM. BEATS evaluates diverse set of LLMs on selected bias detection dataset. BEATS then employs consortium of LLM-as-a-Judge to quantify set of curated metrics related to bias, fairness, ethics, and factuality. 2.1 Research Objective This study focused on systematic analysis and empirical investigation of fairness and bias in LLM. As part of this research, we strive to: (1) Develop framework for measuring and detecting BEFF metrics in LLMs. (2) Establish standard benchmark to assess BEFF metrics and fairness in LLM. (3) Measure BEFF metrics in the main foundation models with wide adoption around the world. (4) Present findings from the experiments and evaluation on BEFF metrics in the major foundation models. We performed experimental studies, empirical research, and statistical analysis to examine BEFF metrics in LLMs. 2.2 Methodology - BEATS Overview The BEATS framework offers systematic approach for evaluating bias, ethics, fairness and factuality in LLMs. At the heart of the BEATS framework is an extensive data set of test questions designed to explore different dimensions of bias and ethical standards in LLM outputs. The evaluation benchmark and subsequent framework actions depend on this quetions data set. Once researchers create the test set, they process the test questions through LLMs by inferencing. The LLM response is then stored in structured SQLite database [7] for benchmark evaluation and statistical analysis. Researchers then establish mutually exclusive and collectively exhaustive (MECE) [8] bias evaluation metric set to measure bias and assess ethical alignment along with fairness and factual accuracy. This evaluation system, comprised of these metrics, allows researchers to determine and assess different aspects related to bias, fairness, and ethical standards. This evaluation metric helps to understand subtle LLM behaviors better in order to develop foundation models that promote equity. The BEATS framework implements consortium-based LLM-as-a-Judge methodology as described by Zheng et al. (2024) [9] to standardize the assessment phase and make it scalable. The LLM reviews generated responses by applying predefined metrics to determine their scores using this approach. The quantitative scoring process assesses model alignment with ethical standards and fairness criteria, which enables structured evaluations of LLMs and comparisons between models. Researchers perform statistical examinations together with exploratory data analysis [10] and data visualization to determine benchmark scores and extract patterns and insights from their datasets. Researchers are able to identify LLMs bias and ethical issues through this systematic statistical evaluation approach. This stage identifies bias and ethics related issues in LLMs and pinpoints opportunities for improvement. The BEATS framework is structured methodology to perform detailed evaluation of LLMs ethical standards. Results from the benchmark is expected to advance the discussions on responsible AI development. 2.3 BEATS Benchmark evaluation dataset curation Bias Benchmark Test evaluates LLMs using specially curated evaluation questions dataset containing bias probing questions for various bias types, such as Age, Gender, Race and Ethnicity, Religion, Sexual Orientation, Disability, Socioeconomic Status, Geography, Cultural, Stereotype, Political, and Automation Bias. This set of questions is an evaluation tool for detecting response bias. Our team created this specialized dataset containing 901 evaluation questions. These questions Table 1: Distribution of evaluation questions by primary bias category in the BEATS benchmark dataset Primary bias categories No. of evaluation questions race_and_ethnicity_bias stereotype_bias gender_bias cultural_bias age_bias socioeconomic_bias disability_bias religion_bias geography_bias political_bias automation_bias sexual_orientation_bias 149 146 120 89 81 72 61 45 39 34 34 31 are curated from four sources, including the study by Parrish et al. (2022), which introduced the Bias Benchmark for Question Answering (BBQ) [11], the One Million Reddit Questions dataset on Hugging Face [12], and questions created by authors using OpenAI ChatGPT [13] and Anthropics Claude [14]. The table 1 shows the distribution of questions for each primary bias category. Although each primary bias type does not have an equal number of questions, the dataset contains questions to probe for and test for intersectional biases. Intersectional biases occur when multiple 3 biases are simultaneously present in the model generated answers. The use of intersectional bias probing questions is intended to overcome the lack of uniformity in distribution among primary bias categories. The intersectional evaluation framework also improves performance measurements of the framework by assessing how multiple biases interact. The BEATS benchmark enables detailed assessment of fairness and bias in LLMs through this curated dataset of evaluation questions. 2.4 LLMs evaluated by BEATS in this study Using the BEATS framework, leading state-of-the-art LLMs provided by leading foundation model providers. The research evaluates the following LLMs from different foundation model providers: researchers assessed the BEFF metrics in several 1. OpenAI: gpt-4o-2024-08-06 [13] 2. Anthropics: claude-3-5-sonnet-20241022 [15] 3. Google: gemini-1.5-pro-002 [16]. 4. Mistral: mistral-large-latest [17]. 5. Meta: meta.llama3-1-405b-instruct-v1:0 [18]. The evaluation process incorporates models from multiple AI research and commercial organizations so that the cross-model examination provides bias and ethics related shortcomings across the GenAI landscape and is not specific to model. Using the studys findings, we strive to increase awareness of patterns and differences in BEFF metrics levels across leading foundation models and the development of more responsible models. 2.5 LLM Inference and Data Collection The Bias Evaluation Questions Dataset is the source for performing inference operations on LLMs evaluated, as listed in section 2.4. Each inference request to model consists of standardized bias evaluation question and system instruction for methodological consistency. This method helps eliminate issues caused by different phrasings of prompts and enables standardized measurements of model responses. The response received from the Large Language Model (LLM) is then stored in database with predefined schema. Researchers later use the data stored in structured database to evaluate BEFF metrics. An inference request to an LLM is denoted as: = (S, Q) (1) In this notation stands for the system instruction while indicates the evaluation question. The corresponding LLM-generated response Ris expressed as: = (IR, IRE, M, D) (2) The Inference Response IR and the Inference Response Explanation IRE which gives the rationale behind the answer is produced by the LLM responding to the question. contains metadata details like inference date, token count, and model information. This structured method for inference, evaluation, and data collection supports rigorous comparative model analysis across models. This helps detect systematic biases and fairness disparities, as well as differences in ethical and factual reasoning. The method standardizes evaluation conditions that enables researchers to interpret distinct model performances through their inherent features without attributing the results to experimental flaws. 2.6 Evaluation Metrics for Assessing Bias, Fairness, and Ethical Alignment The measurement of BEFF metrics is done by analyzing inference responses from LLMs through predefined list of metrics. The BEATS framework uses this predefined list of metrics formatted through standard JSON [19] schema to perform thorough evaluation of the responses bias level, ethical alignment, fairness principles, and factual accuracy. These standardized and predefined structured evaluation metrics ensure the experiments consistency, verifiability, and reproducibility. The first part of the evaluation schema examines bias. First, the presence of bias is determined. If bias is present, then bias is further assessed and categorized into wide range of bias types such 4 as gender, race and ethnicity, socioeconomic status, culture, religion, sexual orientation, disability, age, geography, political ideology, and stereotype reinforcement. The framework also determines whether the prejudice exists openly or concealed through analysis of intrinsic and extrinsic biases. Primary bias (the most dominant bias in the response), secondary bias, magnitude of bias (severity and impact), and intersectional bias, which identifies the presence of multiple different biases affecting marginalized communities together in complex ways, are also recorded. The second part of the BEATS evaluation schema inspects the fairness of LLMs response. The response is first classified as fair or not. Then, the demographic parity score, equal opportunity score, and group fairness index are measured and stored. The demographic parity score measures how equitably the model treats different demographic groups, the equal opportunity score quantifies how fairly individuals with equivalent qualifications are treated, and group fairness index measures fairness across different groups. This way BEATS evaluation measures models tendency to systematically favor or disadvantage any specific demography. The model responses ethical alignment is assessed in the next step. In this step, the ethical alignment index, value alignment score, harm prevention score, cultural sensitivity score, and inclusivity score are measured and stored. The ethical alignment index measures the model responses adherence to ethical AI principles. The value alignment score assesses alignment with societal values and norms. The harm prevention score quantifies how well models response prevent harmful and unsafe contents. The cultural sensitivity score measures the responses respect for diverse cultural norms. The inclusivity score measures how the models response aligns with diverse perspectives and equitable discourse. Lastly, the factual accuracy of the models response is assessed and stored. The intent behind measuring factuality is to measure the risk and susceptibility of the model to hallucinate and propagate misinformation. The limitations of measuring factuality using LLM is further discussed in limitations section 4. Factual accuracy assessment contains two parts: first, factual accuracy score, which measures the correctness and reliability of the answer, and second, misinformation risk score, which measures the responses potential to perpetuate misleading and incorrect information. Factuality identifies and quantifies the models tendency to reinforce narratives unsupported by empirical evidence. 2.7 Formalization of evaluation metrics and its Mathematical Representation To systematically evaluate BEFF metrics in LLMs, we define structured mathematical formulation that encompasses multiple dimensions of bias detection, fairness assessment, factuality evaluation, and ethical alignment. Let BEAT be the overall evaluation score as part of this framework. BEAT consists of four evaluation categories consisting of BIAS for measuring different aspects of Bias, ET HICS for measuring Ethical alignment, ACT ALIT for measuring Factuality of the response, and AIRN ESS for measuring Fairness related metrics to assess equitable treatment across different groups. The BEATS scoring function BEAT S(R) is defined as: BEAT S(R) = {BIAS(R), AIRN ESS(R), ET HICS(R), ACT ALIT (R)} (3) Where is the inference response of LLM during the evaluation as described in (2). 2.7.1 Computational Representation for Bias Detection and Evaluation To systematically assess bias in LLM responses, we define set of structured functions that capture the presence, complexity, and magnitude of bias. These functions provide rigorous framework for analyzing the existence, structure, and impact of bias within AI-generated responses. The bias detection function BIAS(R) consists of Bias Presence function BP (R), Bias Complexity function BC(R), and Bias Magnitude function BM (R). BIAS(R) is defined as: The Bias Presence function BP (R) is defined as: BIAS(R) = {BP (R), BC(R), BM (R)} BP (R) = {b1, b2, ..., bn}, bi {0, 1} (4) (5) where each bi represents the presence (bi = 1) or absence (bi = 0) of specific bias category, including overall bias presence, gender, race and ethnicity, socioeconomic status, cultural bias, 5 religion, sexual orientation, disability, age, geography, political ideology, and stereotypes. As research study by Bolukbasi et al. (2016) [5] showed that word embeddings models exhibit strong gender stereotypes. This approach to detecting and quantifying bias in LLMs will provide detailed analysis of wide verity of biases. Bias Complexity function BC(R) identifies and quantifies the structural complexity of the multifaceted nature of bias in the response. It is defined as: BC(R) = {EIB(R), IB(R), B(R), SB(R)} (6) Where EIB(R) identifies the explicit or implicit nature of the bias, IB(R) identifies the intersectionality of the bias, B(R) recognizes the primary bias category and SB(R) recognizes the secondary bias category. Bias Magnitude function BM (R) identifies the extent and effect of bias and quantifies the scale and effectiveness in the response. It is defined as: BM (R) = {BS(R), BI (R)} (7) Where BS(R) quantifies the severity of the bias, measuring how extreme or pronounced the bias is. BI (R) quantifies the severity of the bias, measuring the real-world consequences of the bias, including its effect on individuals, groups, or institutions. Together, these functions BP (R), BC(R), and BM (R) establish formalized methodology for evaluating bias in LLM responses. This systematic approach enables authors to identify, categorize, and quantify bias in AI generated content, ensuring more structured and verifiable assessment of bias in language models. 2.7.2 Computational Representation for Fairness Detection and Evaluation Fairness in machine learning systems, particularly in LLMs, is critical to ensuring equitable treatment across diverse demographic groups. To rigorously evaluate fairness in LLM-generated responses, we define set of structured fairness metrics that capture parity, opportunity, and group-level equity. These metrics provide quantifiable framework for assessing how fairly the model treats different population segments. The fairness function AIRN ESS(R) consists of Fair function AIR(R), Demographic Parity function DP (R), Equal Opportunity function EO(R) , and Group Fairness Index GF (R) . The Fairness function ARIN ESS(R) is defined as: AIRN ESS(R) = {F AIR(R), DP (R), EO(R), GF (R)} (8) The Fair Function AIR(R), evaluates whether response is fair or not. This binary assessment is expressed as: AIR(R) = (cid:26)1, 0, if response is fair if response is unfair (9) Demographic Parity, denoted as DP (R), measures equal representation and treatment across demographic groups to identify disparities in representation. Equal Opportunity, denoted as EO(R), evaluates consistency in outcomes for different groups with similar qualifications. The group fairness index, denoted as GF (R), measures the variance in treatment between and within demographic groups to assess the inconsistency of fairness between all groups. Each of these metrics is scored on scale from 1 to 10, where 1 represents high disparity and unfairness, and 10 indicates maximal fairness and equitable treatment. By formalizing fairness evaluation through these structured metrics, we establish detailed framework for detecting and mitigating biases in LLMs, ensuring that AI-generated responses align with socially responsible AI principles. By incorporating these fairness metrics, we establish comprehensive framework for evaluating equitable treatment in LLMs, ensuring that AI-generated responses align with socially responsible AI principles. 2.7.3 Computational Representation for Ethics Detection and Evaluation To systematically assess the ethical integrity of AI-generated responses, we define structured framework that evaluates ethical alignment, harm prevention, cultural sensitivity, and inclusivity. These metrics provide quantitative foundation for assessing whether model output adheres The ethics function to established ethical principles, societal values, and fairness norms. 6 ET HICS(R) consists of Ethical Alignment Index function EA(R), Value Alignment function A(R), Harm Prevention function HP (R), Cultural Sensitivity function CS(R) , and Inclusivity function Inc(R) . The Ethics function ET HICS(R) is defined as: ET HICS(R) = {EA(R), A(R), HP (R), CS(R), Inc(R)} (10) Ethical Alignment Index function denoted as EA(R), measures adherence to ethical guidelines or principles. Value Alignment function denoted as A(R), measures alignment with moral or societal values. The Harm Prevention function, denoted as HP (R), measures the likelihood of the response avoiding (or causing) harm or perpetuating stereotypes. Cultural Sensitivity function denoted as CS(R), measures respect and sensitivity to diverse cultural norms in global contexts. Inclusivity function denoted as Inc(R), measures the inclusivity of responses across different demographic groups to promote equitable representation. Each of these metrics is scored on scale from 1 to 10, where 1 represents severe ethical misalignment, exclusion, or harm, and 10 signifies full adherence to ethical principles, inclusivity, and cultural awareness. By integrating these evaluation functions, we establish comprehensive framework for assessing and improving the ethical performance of LLMs, ensuring their ethical deployment in real-world applications. 2.7.4 Computational Representation for Factuality Detection and Evaluation Ensuring factual accuracy is critical aspect of evaluating LLMs, particularly in high-stakes domains where misinformation can have significant consequences. notable incident with Googles Gemini AI image generator [20] highlights the challenges of ensuring factual alignment in generative AI systems, particularly in historical and cultural contexts. Therefore, the authors have included the factuality assessment as part of the BEATS evaluation. To systematically assess the factual reliability of AI-generated responses, we define structured evaluation of Factuality function ACT ALIT (R) incorporating two key functions: Factual Accuracy function A(R) and Misinformation Risk function I(R) . It is defined as: ACT ALIT (R) = {F A(R), I(R)} (11) The Factual Accuracy Score, denoted as A(R), measures the degree to which response aligns with factual information. The Misinformation Risk Score, denoted as R(R), quantifies the probability that response propagates false or misleading information. Both metrics are scored on scale from 1 to 10, where 1 represents highly inaccurate or misleading content, and 10 signifies complete factual accuracy and reliability. By integrating these evaluation criteria, the framework enables rigorous assessment of factual integrity in LLM outputs, ensuring that AI-generated responses uphold standards of accuracy, truthfulness, and trustworthiness. These mathematical formulations establish structured framework for evaluating bias, fairness, factual accuracy, and ethical integrity in responses generated by LLM, thereby ensuring rigorous and reproducible assessment methodology. By structuring the evaluation within this rigorous framework, the methodology enables systematic and empirical assessment of LLM responses, providing actionable insights into the fairness, ethical integrity, and factual reliability of AI-generated content. This comprehensive approach facilitates the identification of bias patterns and fairness gaps, thereby informing the development of strategies to improve the equity and accountability of LLM-based AI systems. 2.8 LLMs as Judge: Leveraging Consortium for Benchmark Scoring To ensure an objective and standardized assessment of responses generated by LLMs, we employ consortium of state-of-the-art LLMs as evaluators for single-answer grading. This approach enables detailed evaluation process using multiple models to score responses across curated set of fairness, bias, factuality, and ethical alignment metrics as covered in section 2.6. The evaluation framework incorporates three leading foundation models as adjudicators for scoring the benchmark: OpenAIs GPT-4o (2024-08-06) [13], Anthropics Claude-3.5 Sonnet (2024-10-22) [15], and Googles Gemini-1.5 Pro-002 [16]. Each of these models independently assesses responses based on structured rubric aligned with the predefined evaluation criteria. By employing consortium of LLMs to function as judges, the BEATS framework produces detailed and statistically significant data that prevents individual judge models from skewing the results. The 7 ensemble method used by the framework enhances assessment reliability, improving both statistical meaningfulness and reproducibility of results when evaluating bias, ethics, and factuality across different models. To enable statistical analysis, the data collection process during LLM as judge step follows formalized representation. An LLM as judge inference request is denoted as: JudgeI = (SJ , BEAT S, IR, IRE) (12) Where SJ is system prompt instruction for LLM as judge evaluation phase, BEAT is the description of evaluation metrics for measurement, IR is the inference response generated by the evaluation model, and IE is the explanation of inference response by the model as described in (2). The response from LLM, acting as judge, is then stored in structured SQLite database [7] for further analysis. The response from LLM is denoted as: JudgeR = (BEAT S(R)) (13) Where BEAT S(R) is as described in (3). 2.9 Analytical Methods and Approach This study analyzes the dataset rigorously through Exploratory Data Analysis (EDA) [10], statistical aggregation methods, and inferential statistical techniques. During the EDA process, data visualization techniques like box and whiskers plots and violin plots are used to study key metrics distribution, outlier detection, and variance. We use Analysis of variance (ANOVA) [21] to statistically validate if findings are statistically significant or mere random chances. This research follows this analytical framework to evaluate the bias, fairness, factuality, and ethical alignment of responses produced by LLMs. The method provides comprehensive evaluation grounded in statistical analysis to guide both LLM evaluation and bias mitigation plans."
        },
        {
            "title": "3 Key Findings",
            "content": "In this section, we introduce our main results and related analyses from our experiments as part of the BEATS evaluation of the main foundation language models in the market. 3.1 BEATS Framework: Measurement of Bias in Large Language Models 3.1.1 Anova results for bias As shown in tables 2 and 3, all the KPIs measured for bias, namely bias presence score, bias severity score, and bias impact score, have values of < 0.001, showing statistically significant results. High F-statistics for the evaluation model ID and the LLM as judge model ID indicate substantial difference in how different LLMs exhibit, express, and identify bias. The high F-Score also underscores the importance of selecting multiple model evaluation and consortium of LLM as judge validation approach, reducing the risk of influence from single model and enhancing the generalizability of the research findings. Table 2: Anova results for BEATS evaluation bias and eval model ID KPI df F-statistic p-value bias_presence_score bias_severity_score bias_impact_score 4 4 4 277.152 364.809 278.481 < 0.0001 < 0.0001 < 0. 8 Table 3: Anova results for BEATS evaluation bias and LLM as judge KPI df F-statistic p-value bias_presence_score bias_severity_score bias_impact_score 2 2 2 128.174 291.799 458.386 < 0.0001 < 0.0001 < 0.0001 3.1.2 Prevalence of Bias Figure 2 shows the presence of bias in the responses of different models. On average 37.65% (1,017.8 out of 2,703) responses have bias presence. This is fairly high number for real world application of LLMs in critical areas where fairness and equitable treatment is critical. Figure 3 presents the distribution of different bias types observed in responses from various LLMs. Figure 2: Total cumulative bias presence scores across large language model families, as evaluated by the BEATS framework. These results highlight significant presence of bias in response across different leading models and underscore the need for bias mitigation strategies in GenAI language models. The analysis reveals low frequency of occurrence for biases related to age (6.6%), gender (4.6%), political ideology (3.3%), disability (3%), religion (2.6%), and sexual orientation (1.1%). These findings suggest that LLMs generally produce responses have relatively lower degree of bias of these types. However, stereotype bias (31.1%), cultural bias (17.3%), socioeconomic bias (13%), race and ethnicity bias (11.9%), and geographic bias (8.4%) are significantly more prevalent in model outputs. Overall 12.9% of LLMs responses were judged to have intersectional bias. 28% of LLM responses had implicit bias whereas explicit biases were present 3.94% of the time. LLM responses had both implicit and explicit bias 5.32% of the time. This pattern is indicative of latent social prejudices embedded in training data, which often lack balanced representation across cultures, ethnicities, and geographic regions. As discussed by Mehrabi et al. in \"A Survey on Bias and Fairness in Machine Learning\" [22] this disproportionate presence of these biases suggests that the underlying training corpora may over represent dominant narratives while under representing marginalized perspectives, leading to skewed outputs. The persistence of these biases raises critical ethical and practical concerns, as they can inadvertently perpetuate stereotypes, reinforce systemic inequalities, and influence decision-making processes in ways that may privilege or disadvantage certain groups. Addressing these disparities requires 9 Figure 3: Category-wise bias presence across as evaluated by the BEATS framework across five leading Large Language Models. Each bar represents the total occurrence of specific bias category. The results highlight the complex heterogeneous bias profiles of LLMs and underscore the importance of handling diverse set of intersectional biases in Gen AI models. targeted bias mitigation strategies, including improved data curation for representation of data from diverse cultures, geographies and ethnicity, model fine-tuning, and fairness-aware training methodologies. Ensuring greater representational balance in training data and incorporating context-sensitive bias detection mechanisms are essential steps toward developing more equitable, and socially responsible global AI systems. 3.1.3 Magnitude of Bias - Bias Severity and Impact We categorized Bias Severity and Bias Impact in Low (score in between 1 and 3), Medium (score in between 4 and 6), and High (score in between 7 and 10). As shown in table 4 only 2,708 or 60% of responses from LLMs have low bias severity and low bias impact remaining 40% or 1,797 responses have either high or medium bias severity or impact. About 25% of LLM responses score high for either bias severity or bias impact, highlighting the need for improvement in the training of foundation models to make it less biased. The hexbin plot 4 illustrates the relationship between Bias Severity Score (x-axis) and Bias Impact Table 4: Distribution of bias severity and impact in the BEATS benchmark dataset for LLM (Claude) as judge Bias severity Bias impact Number of records Low Low Mid Mid Mid High High Low Mid Low Mid High Mid High 2708 48 46 565 281 154 703 Score (y-axis) for the Claude-3.5 Sonnet (20241022) [15] language model as judge. Although the observed distribution indicates that substantial proportion of responses exhibit low bias severity and minimal real-world impact, suggesting that most of the time, the model generates outputs 10 that are unbiased or do not contribute to significant societal consequences, numerous data points in the mid-to-high severity and impact range suggest the existence of systematic biases affecting specific categories. These instances warrant further investigation to identify underlying patterns and demographic disparities that may contribute to these biases. Targeted bias mitigation strategies should focus on addressing cases where the bias impact is disproportionately high, particularly in scenarios where responses exhibit moderate to severe bias severity but exert an unexpectedly strong real-world influence. Addressing these high-impact biases will be crucial to improving the models fairness, transparency, and ethical alignment in practical real world deployments. Hexbin plots for other models (LLM as judge) is available in the appendix 17. Figure 4: Hexbin density plot showing the joint distribution of Bias Severity Score and Bias Impact Score for response from all models, as evaluated by the BEATS framework using Claude-3.5 Sonnet as the Judge. The highest density is concentrated at the lowest severity and impact scores, indicating that most responses exhibit minimal bias magnitude. However, significant number of moderate-to-high severity and impact clusters suggest prevalent generation of responses with non-trivial ethical or societal implications. The distribution underscores the importance of diagnosing and mitigating high-risk model responses. 3.2 BEATS Framework: Measurement of Ethics in Large Language Models 3.2.1 Anova results for Ethics All the KPIs measured in for Ethics have value of < 0.001 showing statistically significant result. High F-statistics for both the eval model ID and the LLM as judge model ID indicate that there is substantial difference in how different LLMs exhibit, express, and identify Ethics. 3.2.2 Ethics - Observations from EDA The Ethical Alignment Index, Value Alignment Score, Harm Prevention Score, and Inclusivity Score exhibit high median values, indicating that the evaluated models generally align with ethical AI principles. Overall 69% of the responses scored high (score of 7 and above) on all ethics metrics and only 2% of the response score low (score of 3 or less) on all metrics. Elongated lower tails and the presence of outliers suggest that, in some instances, model responses exhibit ethical misalignment, weak harm prevention, or lack of inclusivity. On average about 26% of answers score medium to low (score of 6 or lower) on different ethics metrics, which highlight the need for better data curation and training to make LLMs more ethical. The box plot 5 reveals that the Harm Prevention 11 Table 5: Anova results for BEATS evaluation ethics and eval model ID KPI df F-statistic p-value ethical_alignment_index value_alignment_score harm_prevention_score cultural_sensitivity_score inclusivity_score 4 4 4 4 4 595.217 592.832 513.814 530.032 562.263 < 0.0001 < 0.0001 < 0.0001 < 0.0001 < 0.0001 Table 6: Anova results for BEATS evaluation ethics and LLM as judge KPI df F-statistic p-value ethical_alignment_index value_alignment_score harm_prevention_score cultural_sensitivity_score inclusivity_score 2 2 2 2 2 176.479 176.147 375.007 255.714 213. < 0.0001 < 0.0001 < 0.0001 < 0.0001 < 0.0001 Figure 5: This box-and-whisker plot illustrates the distribution of ethics related BEATS evaluation metrics across LLM-generated responses. While the median scores are high across all four metrics, indicating strong ethical alignment in most cases, the wide interquartile ranges and the presence of low outliers indicate prevalent ethical lapses. These findings underscore the importance of improving models to achieve more consistent, higher ethical standards. 12 Figure 6: Violin plot showing the distributional density of ethics-related BEATS evaluation metrics across LLM-generated responses. The long lower tails suggest the presence of ethical shortcomings, particularly in harm prevention and inclusivity. These findings highlight the need to identify and remediate ethically inconsistent outputs. Score and the Inclusivity Score have several low-score outliers, indicating that some responses fail to mitigate harm effectively or do not adequately accommodate diverse perspectives. The violin plot 6 shows relatively symmetric and smooth density distributions, particularly at higher values, suggesting that most responses converge toward strong ethical alignment. However, there are many instances of poor ethical alignment and harm prevention. These outliers warrant further investigation to determine whether they are systematic errors that affect specific demographic groups or isolated model failures. 3.3 BEATS Framework: Measurement of Fairness in Large Language Models 3.3.1 Anova results for Fairness All the KPIs measured in for Fairness have value of < 0.001 showing statistically significant result. High F-statistics for both the eval model ID and the LLM as judge model ID indicate that there is substantial difference in how different LLMs exhibit, express, and identify Fairness. Table 7: Anova results for BEATS evaluation fairness and eval model ID KPI df F-statistic p-value is_it_fair_score demographic_parity_score equal_opportunity_score group_fairness_index 4 4 4 4 371.389 435.828 438.611 447.692 < 0.0001 < 0.0001 < 0.0001 < 0.0001 13 Table 8: Anova results for BEATS evaluation fairness and LLM as judge KPI df F-statistic p-value is_it_fair_score demographic_parity_score equal_opportunity_score group_fairness_index 2 2 2 2 117.035 171.964 180.550 169. < 0.0001 < 0.0001 < 0.0001 < 0.0001 3.3.2 Fairness - Observations from EDA Most of the answers (68.1%) were classified by LLMs as judge as fair whereas remaining 31.9% of the answers were classified is not fair. Most of the Demographic Parity, Equal Opportunity and Group Fairness Index scores are clustered in the upper range (710), indicating that LLMs generally produce equitable responses across demographic groups. 67.36% of all answers score high (score of 7 or higher) across all three fairness metrics whereas only 2.87% of all answers scored low (score of 3 or lower) across all the three metrics. The box plot 7 reveals outliers in both Figure 7: Box-and-whisker plot illustrating the distribution of fairness-related BEATS evaluation metrics across model responses. While the consistently high median scores indicate good overall fairness levels, the broad interquartile ranges and extended lower whiskers reveal the presence of responses with notable fairness disparities. the Demographic Parity Score and the Equal Opportunity Score, indicating that some responses exhibit notable disparities in fairness. These outliers suggest that while fairness is produced in most cases, specific subgroups or contexts can experience disproportionate bias, leading to deviations in equitable treatment. The violin plot 8 distributions suggest that fairness scores are relatively symmetric across all three metrics, with higher densities near the upper score range (810). The overall shape of the distributions indicates that fairness is fairly consistent, but the presence of mid-range density variations (46) suggests that certain fairness violations occur with non-negligible Figure 8: Violin plot depicting the distributional density of fairness-related BEATS evaluation metrics across model responses. The distributions are skewed toward higher values (810), indicating strong adherence to fairness. However, the observed spread and density in the mid-to-lower score ranges reflect variability in fairness across individual responses. frequency. The presence of long lower tail in the violin plot 8 and extended whiskers in the box plot 7 suggest that certain instances exhibit significantly lower fairness scores. On average about 31.26% of answers score medium to low (score of 6 or lower) on different fairness metrics out of which 4.8% score low (score of less than 3). BEATS assessment shows that LLMs generally achieve high fairness scores, certain instances exhibit deviations from equitable treatment, particularly in demographic parity and equal opportunity. Addressing these inconsistencies through context-aware fairness optimization, improved training data curation, and real-world fairness validation frameworks will be essential for improving the equity and trustworthiness of AI-driven decision-making systems. 3.4 BEATS Framework: Measurement of Factuality in Large Language Models 3.4.1 Anova results for Factuality All KPIs measured for Factuality have value of < 0.001 showing statistically significant result. High F-statistics for both the eval model ID and the LLM as judge model ID indicate that there is substantial difference in how different LLMs exhibit, express, and identify Factuality. The relatively high F-statistics for the actual_accuracy_score across both model IDs and LLM-as-a-judge signal potential limitations in the LLM-as-a-judge paradigm for validating factual information. This high F-score highlights the need for further investigation into evaluation objectivity, deeper examination of whether factuality scores reflect accurate alignment with ground-truth knowledge, and the need for more robust, externally validated factuality scoring. 3.4.2 Factuality - Observations from EDA Overall 74.17% of all the answers score high for factual accuracy (score of 7 or above) and low for misinformation risk (score of 3 or lower), whereas 2.66% of all answers score low for factual 15 Table 9: Anova results for BEATS evaluation factuality and eval model ID KPI df F-statistic p-value factual_accuracy_score misinformation_risk_score 4 4 671.330 568.084 < 0.0001 < 0.0001 Table 10: Anova results for BEATS evaluation factuality and LLM as judge KPI df F-statistic p-value factual_accuracy_score misinformation_risk_score 2 2 88.127 347.957 < 0.0001 < 0. accuracy (score of 3 or lower) and high for misinformation risk (score of 7 or higher). The Factual Figure 9: Box-and-whisker plot illustrating the distribution of factuality-related BEATS evaluation metrics across model outputs. The Factual Accuracy Score distribution indicates generally reliable outputs, though few low-scoring outliers exist. The Misinformation Risk Score distribution is skewed lower, with broader spread and upper outliers, reflecting that while most responses pose minimal risk, certain instances carry elevated potential for misinformation. These results highlight the need for fine-grained fact verification mechanisms in generative AI systems. Accuracy Score exhibits high median value ( 89). 81.89% of responses generated by the model are rated for high (score of 7 or above) factually reliability. However, the presence of long lower whisker and outliers (scores near 2) seen in the box plot 9 indicates that some responses contain significant inaccuracies. The Misinformation Risk Score demonstrates skewed distribution, with 16 Figure 10: Violin plot displaying the distributional density of factuality-related BEATS evaluation metrics across model-generated responses. The Factual Accuracy Score is skewed toward higher values, indicating that most responses are factual. The Misinformation Risk Score has long upper tail reflecting several outputs with elevated misinformation risk. These distributions highlight the need for continual validation to safeguard against sporadic but impactful factual inconsistencies in LLM outputs. most responses scoring low (1 to 3), but some responses showing notably higher scores (6 to 8). 74.89% of answers were rated as low misinformation risk (score of 3 or lower), 20.22% of responses were rated as medium risk (score in between 4 and 6), and 4.89% responses were rated as high misinformation risk (score of 7 or above). The violin plot 10 highlights this asymmetry, showing concentration of low-risk responses but also tail of responses with moderate to high risk of misinformation. This suggests that while the model generally produces factually correct content, certain responses have significantly higher potential for misinformation, necessitating context-aware fact-checking mechanisms.In general, while LLMs are generally accurate, there remain pockets of hallucination and high risk of misinformation that must be addressed through verification, improved knowledge-based strategies, and improved fine-tuning on reliable data sources."
        },
        {
            "title": "4 Limitations",
            "content": "Several of the inherent characteristics of LLMs contribute to the limitation of this study. 1. Stochastic and non-determinism: LLMs exhibit non-deterministic behavior due to their inherent stochastic nature. Outputs are influenced by probabilistic temperature or top-p or top-k-based sampling, which leads to different outputs from the same input prompt, temperature, top-p, and top-k-based across different runs. This introduces variability in model responses in both stages of inference during evaluation and output scores during llm as judge. The authors used large evaluation dataset, conducted evaluations across several leading foundation language models, and used an ensemble of LLMs as judge to reduce 17 this limitations impact and increase the researchs generalizability and reproducibility. [23] [24] 2. Lack of ground truth verification and factuality assessment: Utilizing LLMs to measure factuality has constraints. LLMs may hallucinate and produce wrong information or misrepresent facts because of incomplete and up-to-date knowledge. The factuality score provided by LLMs is also unreliable because they share the same biases in their training dataset with the LLMs being evaluated. These limitations are compounded as many of these questions are ambiguous, debated among scholars, and do not have definitive answers. Therefore, the authors advise that when interpreting LLM factuality judgments, factuality scores must be used cautiously because these scores need confirmation through secondary verification systems. As part of future studies, the authors plan to create ground truth database for these evaluation questions and then see how far the LLMs answers deviate from the ground truth. 3. Limitations on using LLMs as judge: Evaluation models and judge models share similar training data, which is predominantly english and western culture centric data. This could lead to self-reinforcing mechanism where lack of global and diverse training data sets leads to lack of sensitivity towards underrepresented or nondominant global viewpoints. Therefore, there is risk of evaluation scores representing fairness and ethical alignment, which are not global in nature. Researchers plan to incorporate human evaluation study to identify and reduce this limitation. [25] [9]"
        },
        {
            "title": "5 Conclusion",
            "content": "Artificial Intelligence has been applied in all walks of life, including critical decision making systems in finance, health care, governance, etc., for many decades. Overtime growing body of scholarly research and regulatory requirements such as Equal Credit Opportunity Act [26], Explainability and transparency requirements, like those outlined by the Basel Committee on Banking Supervision [27] and Model Risk Management [28] and Fairness and non-discrimination regulations, including the and the proposed EU AI Act [29, 30] have played critical part of advancing fairer and more equitable machine learning applications. Advancement in Generative AI spurred by the introduction of Transformer architecture by Vaswani et al. [31] is now reshaping the landscape of AI applications in both industries as well as everyday life across the globe. From voice recognition, natural language processing to AI assisted decision making, Generative AI models are becoming deeply embedded in critical systems. As scholarly research such as Bolukbasi et al. [5] has shown, these models have the potential to perpetuate societal biases and prejudices. In this study, we presented BEATS as framework for measuring Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). BEATS incorporates large dataset of 901 evaluation questions and structured benchmark comprising 29 metrics capturing different aspects of BEFF metrics. This empirical study, based on experimentation and statistically grounded observations, shows that 37.65% of the responses from leading large language models exhibit some form of bias. About 40% of responses show medium to high levels of bias severity and impact. Findings from our research show the prevalence of bias and ethics related concerns in LLMs and reinforce the importance of deeper diagnostics that reflect the risk of using these models in critical decision making systems. The detailed, granular patterns identified in this paper will inform the development of mitigation strategies supporting the larger objective of the development of more transparent, equitable, and fair machine learning models."
        },
        {
            "title": "6 Path Forward - Future Research Directions",
            "content": "With the larger goal of contributing to the development of fairer LLMs that do not perpetuate societal biases and are suitable for use in critical decision making systems, researchers intend to continue future research in this area. We plan to conduct further investigations to identify underlying reasons and patterns driving these biased LLMs behaviors. We also plan to contribute to developing data and AI governance strategies to reduce and mitigate these biases in LLMs."
        },
        {
            "title": "References",
            "content": "[1] Wikipedia contributors. Iron man, wikipedia, en.wikipedia.org/wiki/Iron_Man, Nov. 2004. [Online; accessed 16-Mar-2025]. the free encyclopedia. https:// [2] Wikipedia contributors. J.a.r.v.i.s., wikipedia, the free encyclopedia. https:// en.wikipedia.org/wiki/J.A.R.V.I.S., Apr. 2020. [Online; accessed 16-Mar-2025]. [3] Wikipedia contributors. Interstellar (film), wikipedia, //en.wikipedia.org/wiki/Interstellar_(film), Dec. 2013. 16-Mar-2025]. the free encyclopedia. [Online; https: accessed [4] Interstellar - https:// interstellarfilm.fandom.com/wiki/TARS, Mar. 2025. [Online; accessed 16-Mar-2025]. Film Fandom. film wiki. interstellar"
        },
        {
            "title": "Tars",
            "content": "[5] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings, 2016. URL https://arxiv.org/abs/1607.06520. [6] Emily Sheng, Dallas Card, Kai-Wei Chang, Prem Natarajan, and William Yang Wang. Bias and fairness in large language models: survey. Computational Linguistics, 50(3):10971155, 2024. doi: 10.1162/coli_a_00498. Online: https://direct.mit.edu/coli/article/50/ 3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A. [7] SQLite Project. About sqlite, Oct. 2023. URL https://www.sqlite.org/about.html. [8] Wikipedia contributors. Mece principle, wikipedia, the free encyclopedia. https:// en.wikipedia.org/wiki/MECE_principle, Nov. 2009. [Online; accessed 19-Mar-2025]. [9] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. [10] Wikipedia contributors. Exploratory data analysis, wikipedia, the free encyclopedia. https: //en.wikipedia.org/wiki/Exploratory_data_analysis, Apr. 2007. [Online; accessed 24-Mar-2025]. [11] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. BBQ: Hand-Built Bias Benchmark for Question Answering. Findings of the Association for Computational Linguistics: ACL 2022, 2022, 2022. doi: 10.18653/v1/2022.findings-acl.165. URL https://aclanthology.org/ 2022.findings-acl.165/. Accepted to ACL 2022 Findings. [12] SocialGrep Hugging Face contributor. https:// huggingface.co/datasets/SocialGrep/one-million-reddit-questions, Jul. 2022. [Online; accessed 16-Mar-2025]. one-million-reddit-questions. [13] OpenAI 2023. GPT-4 Technical Report. arXiv preprint, arXiv:2303.08774, Mar. 2024. doi: 10.48550/arXiv.2303.08774. URL https://arxiv.org/abs/2303.08774. [14] Anthropic. Introducing Claude. https://www.anthropic.com/news/introducingclaude, Mar. 2023. [15] Anthropic. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, Jun. 2024. [16] T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, et al. Gemma: Open Models Based on Gemini Research and Technology. arXiv preprint, arXiv:2403.08295v4, Mar. 2024. URL https://arxiv.org/ abs/2403.08295v4. 19 [17] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7B: 7-billion-parameter Language Model for Superior Performance and Efficiency. arXiv preprint, arXiv:2310.06825, Oct. 2023. URL https://arxiv.org/abs/2310.06825v1. [18] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint, arXiv:2302.13971, Feb. 2023. doi: https://doi.org/10.48550/arXiv.2302.13971. URL https://arxiv.org/pdf/ 2302.13971. [19] Wikipedia Json, wikipedia, en.wikipedia.org/wiki/JSON, Aug. 2005. [Online; accessed 24-Mar-2025]. encyclopedia. contributors. free the https:// [20] Catherine Thorbecke and Clare Duffy and CNN. Google halts ai tools ability to produce https://www.cnn.com/2024/02/22/tech/googleimages of people after backlash. gemini-ai-image-generator/index.html, Feb. 2024. [Online; accessed 16-Mar-2025]. [21] Wikipedia contributors. Analysis of variance, wikipedia, the free encyclopedia. https: accessed //en.wikipedia.org/wiki/Analysis_of_variance, 24-Mar-2025]. Jul. 2005. [Online; [22] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. Survey on Bias and Fairness in Machine Learning. arXiv preprint, arXiv:1908.09635, 2019. doi: 10.48550/arXiv.1908.09635. URL https://doi.org/10.48550/arXiv.1908.09635. [23] S. Chiesurin, D. Dimakopoulos, M. A. Sobrevilla Cabezudo, A. Eshghi, I. Papaioannou, V. Rieser, and I. Konstas. The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering. arXiv Preprint, 2305.16519:15, 2023. doi: 10.48550/arXiv.2305.16519. Online: https://arxiv.org/abs/2305.16519. [24] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic In Proc. ACM Conf. Fairness, Accountability, parrots: Can language models be too big? and Transparency (FAccT), pages 610623, New York, NY, USA, 2021. Association for doi: 10.1145/3442188.3445922. Online: https://doi.org/ Computing Machinery. 10.1145/3442188.3445922. [25] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh Chawla, and Xiangliang Zhang. Justice or prejudice? quantifying biases in llm-as-a-judge, 2024. URL https://arxiv.org/abs/ 2410.02736. [26] United States Congress. Equal credit opportunity act (ecoa), 1974. URL https:// www.consumerfinance.gov/rules-policy/regulations/1002/. [27] C. Goodhart. The Basel Committee on Banking Supervision: History of the Early Years, 19741997. Cambridge University Press, 2011. doi: 10.1017/CBO9780511996238. URL https://doi.org/10.1017/CBO9780511996238. [28] Office of the Comptroller of risk management, 2011. URL https://www.occ.gov/news-issuances/bulletins/2011/ bulletin-2011-12.html. Supervisory guidance on model the Currency. [29] European Union. Eu artificial intelligence act, 2024. URL https:// artificialintelligenceact.eu/. [30] European Commission. Ethics guidelines for trustworthy ai, 2024. URL https://digitalstrategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai. [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https: //arxiv.org/abs/1706.03762. 20 [32] Shira Mitchell, Eric Potash, Solon Barocas, Alexander DAmour, and Kristian Lum. Annual Review of Algorithmic Fairness: Choices, Assumptions, and Definitions. Statistics and Its Application, 8(1):141163, 2021. 10.1146/annurev-statistics042720-125902. URL https://www.annualreviews.org/content/journals/10.1146/ annurev-statistics-042720-125902. doi: [33] S. Venkatasubbu and G. Krishnamoorthy. Ethical Considerations in AI: Addressing Bias Journal of Knowledge Learning and Science and Fairness in Machine Learning Models. Technology, 1(1):130138, 2022. ISSN 2959-6386. doi: 10.60087/jklst.vol1.n1.p138. URL https://doi.org/10.60087/jklst.vol1.n1.p138. [34] S. Lo Piano. Ethical Principles in Machine Learning and Artificial Intelligence: Cases from the Field and Possible Ways Forward. Humanities and Social Sciences Communications, 7(9), 2020. doi: 10.1057/s41599-020-0501-9. URL https://doi.org/10.1057/s41599-0200501-9. [35] S. T. Boppiniti. Data Ethics in AI: Addressing Challenges in Machine Learning and Data Governance for Responsible Data Science. International Scientific Journal for Research, 5(5), 2023. URL https://isjr.co.in/index.php/ISJR/article/view/257."
        },
        {
            "title": "7 Appendix",
            "content": "7.1 Additional Evaluation Results We present some additional results and details from the evaluations. 7.1.1 Primary and Secondary Bias Primary and Secondary bias presence in BEATS evaluation. Figure 11: Claude as Judge: category-wise primary bias presence across LLMs as evaluated by the BEATS framework. Each bar represents the total occurrence of specific bias category across all evaluated model as judged by calude. Figure 12: Claude as Judge: category-wise secondary bias presence across LLMs as evaluated by the BEATS framework. Each bar represents the total occurrence of specific bias category across all evaluated model as judged by calude. 22 Figure 13: OpenAI GPT 4o as Judge: category-wise primary bias presence across LLMs as evaluated by the BEATS framework. Each bar represents the total occurrence of specific bias category across all evaluated model as judged by GPT-4o. Figure 14: OpenAI GPT 4o as Judge: category-wise secondary bias presence across LLMs as evaluated by the BEATS framework. Each bar represents the total occurrence of specific bias category across all evaluated model as judged by GPT-4o. 23 Figure 15: Google Gemini 1.5 pro as Judge: category-wise primary bias presence across LLMs as evaluated by the BEATS framework. Each bar represents the total occurrence of specific bias category across all evaluated model as judged by Gemini 1.5 pro Figure 16: Google Gemini 1.5 pro as Judge: category-wise secondary bias presence across LLMs as evaluated by the BEATS framework. Each bar represents the total occurrence of specific bias category across all evaluated model as judged by Gemini 1.5 Pro 24 7.1.2 Bias Magnitude - Impact Vs Severity across models (LLM as Judge) Figure 17: Hexbin density plot showing the joint distribution of Bias Severity Score and Bias Impact Score for response from all evaluated models, as judged by the BEATS framework. GTP 4o and Clause 3.5 show relatively similar pattern whereas Gemini shows very distinct pattern showing that it judges the bias severity and impact differently. - All models as judge 7.1.3 Ethics cultural sensitivity Ethics cultural sensitivity score by different LLMs during BEATS evaluation. Figure 18: Box-and-whisker plot illustrating the distribution of cultural sensitivity metrics across all evaluated models. 7.1.4 Ethical Alignment Ethics alignment score by different LLMs during BEATS evaluation. Figure 19: Box-and-whisker plot illustrating the distribution of ethical alignment metrics across all evaluated models. 7.1.5 Ethics Harm Prevention Ethics harm prevention score by different LLMs during BEATS evaluation. 26 Figure 20: Box-and-whisker plot illustrating the distribution of harm prevention metrics across all evaluated models. 7.1.6 Ethics Inclusivity Ethics Inclusivity score by different LLMs during BEATS evaluation. Figure 21: Box-and-whisker plot illustrating the distribution of inclusivity metrics across all evaluated models. 7.1.7 Ethics Value Alignment Ethics value alignment score by different LLMs during BEATS evaluation. 27 Figure 22: Box-and-whisker plot illustrating the distribution of value alignment metrics across all evaluated models. 7.1.8 Fairness Fairness Demographic Parity score by different LLMs during BEATS evaluation. Fairness equal Figure 23: Box-and-whisker plot illustrating the distribution demographic parity metrics across all evaluated models. Opportunity score by different LLMs during BEATS evaluation. Fairness group fairness index by different LLMs during BEATS evaluation. 28 Figure 24: Box-and-whisker plot illustrating the distribution of equal opportunity metrics across all evaluated models. Figure 25: Box-and-whisker plot illustrating the distribution of group fairness metrics across all evaluated models. 29 7.1.9 Factuality Factuality - Factual accuracy score by different LLMs during BEATS evaluation. Factuality - Figure 26: Box-and-whisker plot illustrating the distribution of factual accuracy metrics across all evaluated models. Misinformation Risk Score by different LLMs during BEATS evaluation. Figure 27: Box-and-whisker plot illustrating the distribution of misinformation risk metrics across all evaluated models. 30 7.2 Related Work Extensive scholarly research has been done in areas of bias, ethics, and fairness both in the social sciences and in the design, development, and governance of artificial intelligence applications. Mitchell et al. (2021) [32] explored the quantitative definitions of fairness in predictive machine learning models. This research underscored the inconsistencies in motivations, terminology, and notation within the field and advocated for integrating quantitative and qualitative methods during policy discussions. Bolukbasi et al. (2016) [5] demonstrated in the empirical study that word embedding models encode and even amplify gender stereotypes. This work showed how statistical correlations in training data will reinforce harmful societal prejudices. This paper laid the foundation for bias detection and drove impetuous to reduce bias in early stage NLP systems. Mehrabi et al. (2022)[22] presented comprehensive study and taxonomy of bias and fairness in machine learning systems. Sreerama and Krishnamoorthy (2022)[33] identified sources of bias in machine learning models that stem from data collection and algorithm design and proposed approaches to alleviate bias in machine learning models. Lo Piano (2020)[34] discussed ethical issues introduced by black box algorithms, especially in areas like criminal justice and autonomous vehicles, and advocated for the development of guidelines and governance in AI deployments. Boppiniti (2023)[35] addresses the ethical challenges in AI, focusing on data governance, privacy, accountability, and transparency. The paper emphasized the need for the establishment of ethical review boards and compliance with regulations such as GDPR and CCPA, which are essential for responsible AI deployment. Parrish et al. (2022) [11] introduce the Bias Benchmark for Question Answering (BBQ), dataset designed to evaluate how social biases manifest in the outputs of question-answering (QA) models. This study highlighted that NLP models often reproduce harmful stereotypes, leading to biased outputs. Ye et al. (2024) [25] introduced the CALM framework and examined 12 distinct bias types in LLMs when they are used as judges. This collected research in the area of responsible AI illustrates the complex and multi-dimensional nature of bias and fairness in responsible AI applications. Research highlights the need for continuous development and research to develop comprehensive qualitative and quantitative frameworks based on empirical research to drive advancement in AI governance and the development of fairer and more equitable machine learning applications."
        }
    ],
    "affiliations": [
        "Boston, USA",
        "San Francisco, USA"
    ]
}