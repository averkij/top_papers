{
    "paper_title": "Temporal Reasoning Transfer from Text to Video",
    "authors": [
        "Lei Li",
        "Yuanxin Liu",
        "Linli Yao",
        "Peiyuan Zhang",
        "Chenxin An",
        "Lean Wang",
        "Xu Sun",
        "Lingpeng Kong",
        "Qi Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains."
        },
        {
            "title": "Start",
            "content": "Lei Li1 Yuanxin Liu2 Linli Yao2 Lean Wang2 Xu Sun2 Lingpeng Kong1 Qi Liu1 1The University of Hong Kong 2Peking University nlp.lilei@gmail.com {liuyuanxin, linliyao}@stu.pku.edu.cn pez010@ucsd.edu {lean, xusun}@pku.edu.cn Peiyuan Zhang3 Chenxin An1 {lpk,liuqi}@cs.hku.hk cxan23@connect.hku.hk 3University of California, San Diego 4 2 0 O 8 ] . [ 1 6 6 1 6 0 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs temporal reasoning capability stems from the underlying LLMs inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7Bs temporal understanding, yielding 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVLChat-V1.5-20B and VILA1.5-40B. Further analysis reveals strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid development of large language models (LLMs) (OpenAI, 2024; Gemini Team, 2024) has sparked significant interest in video large language models (Video LLMs) (Zhang et al., 2023; Lin et al., 2023b) due to their impressive generation and reasoning capabilities. Current approaches typically use pre-trained vision encoders (Radford et al., 2021) combined with powerful LLMs (Touvron et al., 2023; Chiang et al., 2023; Yang et al., 2024) as the starting point for Video LLMs. These models employ various strategies to handle multiple video frames (Li et al., 2023b; Tan et al., 2024), and are then further trained on curated instruction-tuning datasets (Chen et al., 2024), demonstrating promising abilities in video comprehension tasks (Fu et al., 2024; Zhou et al., 2024). Despite the progress in video comprehension, Video LLMs often struggle with temporal reasoning, which is essential for truly interpreting video content (Li et al., 2023c; Tang et al., 2023b). Specifically, video temporal reasoning involves the ability to track changes over time, comprehend event sequences, and relate objects and actions to specific moments in video (Mangalam et al., 2023; Li et al., 2023d). As illustrated in Figure 1, two strong Video LLMs, LongVA-7B (Zhang et al., 2024a) and VILA8B (Lin et al., 2023b), both failed to answer basic questions about the chronological order of events Equal contribution. 1Project page: https://video-t3.github.io 1 Figure 1: Two popular Video LLMs struggle with basic temporal reasoning (left). We mitigate this issue via textual temporal transfer (middle), which demonstrates consistent improvement (right). in synthesized videos, whereas humans can predict correctly without difficulty. This significant gap between human performance and current Video LLMs in temporal reasoning tasks motivates us to explore the underlying reasons for this discrepancy. Previous research has largely attributed temporal reasoning deficiencies in Video LLMs to ineffective video encodings, leading to various temporal aggregation module developments (Ren et al., 2023; Jin et al., 2023; Tan et al., 2024). Our paper takes different approach by decomposing Video LLMs into two parts and asking fundamental question: What is the bottleneck of this limitation? Is it due to limitations in the vision encoder, or, surprisingly, shortcomings in the LLM itself? We conduct probing experiments using synthesized videos for basic temporal-related video question-answering (QA) tasks, allowing full control over temporal aspects. Our method involves: (i) Training small probe classifiers on video representations in the Video LLM embedding space to assess temporal information captured by visual encoders and aggregation modules. (ii) Transforming synthesized videos into textual descriptions using commercial visual language models (e.g., GPT-4V) to analyze how standalone LLMs process temporal information. By comparing the performance of these components with full Video LLMs, we precisely locate the bottleneck in temporal understanding. Our experiments reveal striking contrast in temporal reasoning capabilities between different components of Video LLMs. Probe classifiers trained on video embeddings achieve near-perfect accuracy (> 90% in most cases), indicating that these embeddings successfully capture sufficient temporal information. Even simple neural models like LSTM (Hochreiter & Schmidhuber, 1997) can accurately extract temporal relationships from these embeddings. Conversely, despite their significantly larger scale, LLMs struggle to process this temporal information effectively. They exhibit relatively low probing accuracy across various temporal aspects, highlighting their difficulty in handling temporal relationships. These findings provide compelling evidence that the LLM component, rather than the visual encoding, is the primary bottleneck in the temporal reasoning of current Video LLMs, motivating us to enhance LLMs ability to reason about temporal information. Building on our insights, we propose enhancing Video LLMs temporal understanding by focusing on the LLM component. Our approach leverages existing image-text datasets to generate diverse temporal reasoning tasks in pure text format, overcoming the scarcity of video samples with complex temporal scenarios (3). Remarkably, without using any video data, our text-only synthesized dataset enables LongVA-7B to outperform ShareGPT4Video-8B (trained on 28,000 video samples) on the TempCompass benchmark (Liu et al., 2024c). Moreover, as shown in the right of Figure 1, our enhanced model demonstrates competitive results on various video-understanding benchmarks. It improves temporal reasoning accuracy by 12.4 points on Video-MME (Fu et al., 2024) and increases average accuracy from 56.4 to 58.1 on MLVU (Zhou et al., 2024), surpassing larger models such as InternVL-Chat-V1.5-20B (Chen et al., 2023) and VILA-1.5-40B (Lin et al., 2023b). Furthermore, we observe high correlations between performance on our textual temporal validation set and benchmark results, e.g., Pearson = 0.89 and = 0.85 on TempCompass and MLVU, respectively. Analysis reveals the crucial role of self-attention modules in temporal reasoning transfer, and our method improves the utilization of more video frames. These findings underscore the effectiveness of our textual temporal reasoning transfer, offering valuable insights for future Video LLMs. 2 Figure 2: Example of videos and questions that focus on different temporal reasoning abilities."
        },
        {
            "title": "2 PINPOINTING VIDEO LLM TEMPORAL REASONING BOTTLENECK",
            "content": "In this section, we seek to examine and pinpoint the temporal reasoning bottleneck of Video LLMs. Video LLMs typically consist of two essential components for temporal understanding tasks: vision encoder and an LLM decoder, where the former extracts visual features from video frames and the latter is responsible for integrating this information with textual instructions to complete the end task. We aim to answer two questions: (1) Can existing Video LLMs understand the temporal information in videos? (2) If not, which componentthe vision encoder or the LLM decoderis the bottleneck? To address these questions, we design different tasks to test the full Video LLMs and these two components separately(2.1), incorporating various aspects of temporal understanding abilities (2.2), and discuss our findings (2.3). 2.1 TASK FORMULATION FOR DIFFERENT VIDEO LLM COMPONENTS Full Video LLM. We evaluate the full Video LLMs through multi-choice video question answering. Specifically, we uniformly sample eight frames from videos and present them to the model along with multiple-choice question. To encourage the Video LLM to directly output an option, we append the prompt Answer the option only. after the question. LLM Decoder. In terms of the LLM decoder, we replace the video frames with detailed frame captions generated by GPT-4o. In this way, we assess the ability of LLM decoder to understand temporal information in the textual context. The multi-choice questions are identical to those employed in testing the full Video LLM. To ensure that the temporal understanding questions can indeed be addressed using these frame captions, we carefully design the prompting strategy to incorporate all essential information in the captions (please refer to the Appendix A.2 for the details). Visual Features. Unlike the full Video LLM and LLM decoder, the quality of visual features cannot be directly tested via question answering. To determine whether the visual features contain sufficient information to differentiate between different temporal dynamics in videos (e.g., brightening versus darkening), we employ the classifier probe technique proposed by (Alain & Bengio, 2017). Assuming that there are categories of contradicting temporal dynamics, we train probe () that maps set of visual features Rndv to probability distribution Rc. In practice, () is single-layer LSTM model (Hochreiter & Schmidhuber, 1997) for capturing sequential correlation, and we employ the visual features that are down-sampled and projected into LLM embedding space. The probe is tested on the same set of videos as the full Video LLM but is trained on different set of videos. More details of the visual classifier probe can be found in Appendix A.3. 2.2 EVALUATION DATA COLLECTION Temporal Reasoning Abilities. Temporal reasoning encompasses several key aspects. Based on recent Video LLM benchmarks (Fu et al., 2024; Wu et al., 2024), we focus on four critical dimensions: (1) Order: comprehending the sequential arrangement of events; (2) Attribute: perceiving changes in environmental or object attributes over time; (3) Temporal Referring: formulating questions based on specific temporal positions within video; and (4) Temporal Grounding: identifying the temporal location of specific elements in video. While these aspects are limited approximation 3 Figure 3: Temporal probing results for LongVA (upper) and VILA (lower). The probe on visual representations achieve > 90 accuracy in most cases, while the LLM decoders still have large room for improvement even with textual inputs, leading to the poor temporal understanding ability of Video LLMs. Detailed results of the sub-categories are reported in Appendix A.4. of the full spectrum of temporal understanding, our study reveals that they are sufficient to expose significant deficiencies in current Video LLMs. Collecting Videos. Existing video benchmarks are unsuitable for our analysis due to two main limitations: (1) the lack of video clusters with contrasting temporal dynamics needed for visual feature probing, and (2) the inability to fully eliminate single-frame or language bias (Liu et al., 2024c). Consequently, we synthesize custom videos to isolate different aspects of temporal understanding. Figure 2 illustrates examples of our synthesized videos. For the Order aspect, we concatenate different source videos (e.g., person, cat, flower) temporally, creating various categories based on concatenation order. The Attribute aspect focuses on shape (flower blooming videos and their reversals) and brightness (gradually altering pixel values of static images). Temporal Referring and Temporal Grounding reuse videos from the Order aspect with three concatenated items. Appendix provides detailed information on the video creation process and data distribution. Constructing Questions and Answers. As depicted in Figure 2, we design multi-choice question templates for each temporal aspect. The questions remain consistent across videos within each aspect, while the correct answers vary based on the specific video content. This approach ensures controlled evaluation of temporal reasoning capabilities. 2.3 RESULTS We examine two advanced Video LLMs that utilize different LLM backbones on our synthesized probing datasets: LongVA-7B (Zhang et al., 2024a), which is based on Qwen2-7B (Yang et al., 2024), and VILA-8B (Lin et al., 2023b), which is built upon LLaMA3-8B (Dubey et al., 2024). The results are visualized in Figure 3, with detailed scores provided in Appendix A.4. Our analysis reveals several key findings: Performance of Video LLMs: Video LLMs demonstrate relatively poor performance, struggling to reach 70 accuracy across all temporal understanding tasks. This suggests potential limitation in their ability to process and reason about temporal information in video content. Efficacy of Visual Representations: Notably, small classifiers trained on the visual representations in the LLMs embedding space achieve near-perfect accuracy. This finding suggests that these visual representations encapsulate rich information, sufficient to distinguish between temporally contradicting videos, even with simple probe classifier. Consequently, we can conclude that the input processing is not the primary limitation in temporal reasoning tasks. 4 LLM Backbone Performance: To our surprise, moderate-sized LLM backbone decoders such as Qwen2-7B and LLaMa3-8B fail to answer considerable number of questions related to Referring, Grounding and Order aspects, which are intuitively very easy for SoTA LLMs given that the frame captions are provided in textual format. Worse still, these LLM decoders perform only at the level of random guessing when it comes to the Attribute aspect. These findings indicate that the LLM decoders face substantial challenges even in the context of textual temporal understanding, posing major limitation to the video temporal understanding capabilities of Video LLMs. In comparison, larger LLMs with over 70 billion parameters significantly outperform their smaller counterparts on these tasks, suggesting that (i) the generated frame captions are accurate and contain sufficient information for temporal reasoning; (ii) the textual temporal understanding ability emerges when the text decoder scale exceeds certain thresholds. We hypothesize that this discrepancy may be due to the sparse expression of temporal concepts in pre-trained corpora. Consequently, smaller-scale models might not have sufficient exposure to learn these temporal reasoning abilities effectively. This sparsity could explain why only very large language models (>70B parameters) demonstrate proficiency in temporal reasoning tasks. Our findings ultimately suggest that the temporal reasoning ability in current Video LLMs is primarily constrained by the LLM decoder rather than the quality of visual representations. This conclusion highlights critical area for improvement in the development of more effective Video LLMs, motivating us to enhance the temporal reasoning capabilities from the textual side."
        },
        {
            "title": "3 TEXTUAL TEMPORAL UNDERSTANDING TRANSFER",
            "content": "In this section, we explore strategies to mitigate the deficiency of Video LLMs in temporal reasoning. straightforward approach would be to create temporal-oriented instruction-tuning dataset from videos. However, existing approaches to video instruction-tuning data generation, whether through human annotation or automatic generation via rules/models, present significant challenges. Human annotation of video datasets is resource-intensive, demanding considerable time and financial investment. Synthetic video instruction tuning (Maaz et al., 2024; Zhang et al., 2024b), while more efficient and scalable, is constrained by the content of the videos themselves. For instance, if our goal is to enhance particular aspect of temporal reasoning, it is crucial to ensure that the videos contain information relevant to this aspect. This limitation makes it difficult to create instruction-tuning data that targets specific abilities. Motivated by (1) the limitations of video instruction-tuning and (2) our finding that the LLM backbone is the primary bottleneck of temporal understanding, we investigate the feasibility of enhancing video temporal reasoning through novel perspective: using synthesized textual temporal reasoning data. Our method uses sequences of image captions as proxies for video frames, allowing us to create temporal-oriented question-answering pairs without relying on actual video content. This text-only approach offers two key advantages: (i) Scalability: The abundance of available imagecaption pairs (Changpinyo et al., 2021; Liu et al., 2023) allows for easy expansion of the dataset. (ii) Flexibility: We can precisely control the targeted ability and sample complexity by adjusting the content and number of image captions. To address the four aspects of temporal understanding (Order, Attribute, Referring and Grounding), we design heuristics to construct textual contexts and generate question-answer pairs solely from caption sequences with examples visualized in Figure 4. The specific generation processes are as follows. Order. This aspect focuses on understanding the sequential order of image captions. To construct the textual context for question answering, we randomly sample 36 captions from caption pool. The questions are created using two methods: On the one hand, we provide GPT-4-turbo with examples and prompt it to generate order-related questions, which we refer to as Order-GPT. On the other hand, we employ predefined templates and heuristic rules to create questions. These template questions require rearranging shuffled sequences according to the textual context of image captions. These sequences comprise of (a) complete image captions, (b) phrases within captions or (c) phrases with prefix identifiers (e.g., (1)(2)(3)(4) as shown in the example in Figure 4). We denote the template-based questions as Order-Template (X), where {sentence, phrase, prefix}. Attribute. This aspect involves recognizing how specific attributes of objects or scenes change throughout the video. To achieve this, we first prompt GPT-4-turbo to generate new image captions by modifying particular attributes in the original captions. Specifically, we consider five types of 5 Figure 4: Temporal-oriented question-answering pairs with textual image captions as context. attributes: color, light condition, size & shape, posture, and emotion. Subsequently, we employ GPT-4-turbo again to generate questions that focus on the attribute changes between pairs of captions. Temporal Referring. This aspect requires understanding questions that refer to particular temporal locations (e.g., begin, middle and end). To increase the difficulty, we employ GPT-4-turbo to generate three similar image captions that only differ in certain aspect, such as object, action or attribute. These three captions are then placed respectively at the beginning, middle, and end of the textual context. For the questions, we first generate question for each caption without referring to temporal locations. Then, temporal references are added to the questions according to the specific temporal location of the corresponding caption. Temporal Grounding. This aspect is complementary angle to temporal referring, aiming to identify the temporal location of an element (a phrase describing an object, action or attribute) of interest. The textual context reuses the same image captions as in temporal referring. To formulate the questions, we first prompt GPT-4-turbo to generate declarative statement for each caption (e.g., the coins are black). These statements are then incorporated into the temporal grounding question templates, as illustrated in Figure 4. We enhance the basic textual context (i.e., captions relevant to the question) by incorporating distractor captions sampled from the caption pool and inserted between the original captions. This approach challenges the model to identify relevant temporal information amidst irrelevant context, enhances the robustness of its temporal understanding abilities, and presents more realistic scenario mimicking the complexity of real-world temporal reasoning tasks. To maintain question clarity, we ensure distracting captions do not share nouns with the original captions. Table 1 gives summary of our synthesized textual temporal QA datasets. For each task, we also create validation set consisting of 500 samples for later verification. Appendix provides detailed and formalized description of the data construction process, as well as the ablation study for distractor captions and comparison of temporal reasoning transfer via multiple images (Appendix B.4)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "This section details our experiments exploring the textual temporal understanding transfer. We begin by describing the experimental setup and training protocols (4.1). Next, we report the transfer results of textual temporal understanding and key findings (4.2) and provide analysis results (4.3). 6 Table 1: Summary of our textual temporal reasoning datasets where {phrase, prefix, sentence} for Order-Template (X). Detailed data statistics are provided in Appendices B.3 and C.1. Dataset #Relevant Captions #Distractor Captions Description Order-GPT (N ) Attribute (N ) Order-Template (X) Referring Grounding 24 2 36 3 3 100 50, {1, 2, 4, 8} Order-related questions generated by GPT-4. 100 50, {1, 2, 4, 8} Attribute-related questions. 20050 20050 20050 Order-related questions based on templates Temporal referring questions. Temporal grounding questions."
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Benchmarks. To establish connection between textual temporal reasoning ability and video comprehension, we adopt the fine-grained temporal understanding benchmark, TempCompass(Liu et al., 2024c) (Multiple-Choice subset), diagnosing multi-facet basic video temporal understanding abilities. For comprehensive assessment of general video understanding tasks, we additionally incorporate Multi-Task Long Video Understanding (MLVU) (Zhou et al., 2024) and Video-MME (Fu et al., 2024). For MLVU, we select the following tasks covering three aspects: holistic video understanding (Topic Reasoning (TR) and Anomaly Reasoning (AR)), single-detail understanding (Needle Question-Answering, Ego Reasoning (ER) and Plot Question Answering), and multi-detail understanding (Action order (AO) and Action Count (AC)). For Video-MME, which consists of 2700 video QA pairs spanning primary visual domains and three video duration types (Short, Medium, and Long), we focus on two temporal-oriented tasks (i.e., Temporal Perception and Temporal Reasoning) and overall performance. Compared Methods. We evaluate our temporal textual augmentation approach against two baselines: (i) LLaVA-Next: Continually fine-tuning on the original image-text instruction dataset used by LongVA.2 (ii) HotpotQA (Yang et al., 2018): Fine-tuning on multi-document QA dataset to assess enhancements in textual locating ability. We also report the performance of recent Video LLMs, including Video-ChatGPT (Maaz et al., 2024), Video-LLaVA-7B (Lin et al., 2023a), LLaVA-NextVideo-7B-DPO (Liu et al., 2024a), MA-LMM-7B (He et al., 2024), ShareGPT4Video-8B (Chen et al., 2024), LLama-3-VILA1.5-8B (Lin et al., 2023b), VILA1.5-40B (Lin et al., 2023b), and InternVLCHat-V1.5-20B (Chen et al., 2023). GPT-4o (OpenAI, 2024) is included to represent commercial model results. These models serve as reference points and we report the highest scores published to represent optimal performance. Training Details. We adopt LongVA (Zhang et al., 2024a) as our backbone model and continually fine-tune the official checkpoint. Despite being trained exclusively on text and image data, LongVA demonstrates exceptional zero-shot video understanding capabilities and can effectively handle long videos. We strictly adhere to the original LongVA implementation and follow the official fine-tuning protocol. This involves using Adam (Kingma & Ba, 2015) as the optimizer, with learning rates of 2e-6 for the visual encoder and 1e-5 for the rest part of the model. For the exploration of textual temporal reasoning transfer, we use 22k samples for fine-tuning across different augmentation datasets. We further scaled the dataset up to transfer to holistic video understanding benchmarks, where we find it necessary to incorporate the original instruction tuning data to maintain visual perception ability. According to our textual validation accuracy, we set the ratio of textual temporal QA and original data to 1:2 and the total samples to 200k. This mix applies to the Hotpot QA as well for fair comparison. Appendix provides the details of dataset composition and mixture configurations in our training. The model is trained on the corresponding dataset for one epoch, process that can be completed within 5 hours using 8 H100 GPUs. 4.2 RESULTS Our results aim to answer the following three research questions: (1) Can textual temporal reasoning ability be effectively transferred to video temporal reasoning? and (2) Do these two abilities correlate well? and (3) Does this transfer also reflect on holistic video understanding benchmarks? 2https://github.com/xiaoachen98/Open-LLaVA-NeXT 7 Table 2: Fine-grained video temporal understanding evaluation on TempCompass benchmark. Without training on any videos, our T3 helps LongVA-7B outperform ShareGPT4Video-8B trained on 28k video samples. Best results are shown in bold. Method Action Direction Speed Order Attribute Change Average Video-ChatGPT-7B (Maaz et al., 2024) Video-LLaVA-7B (Lin et al., 2023a) LLaVA-NeXT-Video-7B-DPO (Liu et al., 2024a) Llama-3-VILA1.5-8B (Lin et al., 2023b) ShareGPT4Video-8B (Chen et al., 2024) LongVA-7B (32 frm) + LLaVA-Next + Hotpot QA + Temporal Change (1x) + Temporal Change (2x) + Temporal Change (4x) + Temporal Change (8x) + Temporal Change (1x, 2x, 4x and 8x) + Order-Template + Temporal Referring + Temporal Grounding + Order-Template + Temporal Change (1x) + Temporal Grounding+ Temporal Change (1x) + Temporal Referring + Temporal Change (1x) + T3 (all tasks) GPT-4o 61.5 76.0 87.6 92.9 87.6 92.3 92.0 92.3 91.1 89.6 90.5 90.2 90.5 89.9 91.4 63.0 90.2 91.1 90.8 90.8 98.2 28.9 35.2 35.8 33.7 34. 37.3 36.4 37.0 37.3 39.1 43.0 42.4 38.2 35.8 33.7 31.6 40.6 38.8 38.2 39.7 52.8 29.0 35.7 41.3 44.2 47.5 42.0 43.2 39.4 39.8 40.4 39.4 39.8 39.8 46.1 46.1 29.3 42.9 41.6 42.3 41.6 52.1 36.1 37.8 39.7 50.0 62. 54.3 55.6 53.0 65.2 66.9 64.2 64.2 65.6 58.3 46.7 40.7 63.6 62.9 62.9 65.9 73.2 30.9 41.0 45.8 60.1 64.2 51.7 51.0 50.7 63.9 61.8 64.2 68.1 64.2 54.5 51.0 38.9 65.3 62.9 64.6 68.1 78.5 37.7 45.6 50.6 56.4 59. 55.9 56.0 54.9 59.5 59.6 60.4 61.0 59.7 57.2 54.2 41.0 60.6 59.6 59.8 61.2 71.0 Figure 5: Textual temporal reasoning accuracy correlates positively with video understanding results on three benchmarks. Textual temporal reasoning transfer: Table 2 presents the evaluation results of models trained on various compositions of our datasets. Continual training with the original LLaVA-Next dataset yields negligible improvements, while HotpotQA training surprisingly degrades performance, despite its similarity to temporal location tasks. These findings underscore the non-trivial nature of enhancing temporal understanding in Video LLMs. Our textual temporal transfer approach, by contrast, demonstrates significant improvements. Regarding the different tasks, we find that: (i) The Temporal Change (1x) subset which combines Order (1x) and Attribute (1x), enhances Order accuracy from 54.3 to 65.2 and Attribute accuracy from 51.7 to 63.9. Increasing the number of distractor captions (1x to 8x) leads to generally better performance. (ii) Order-Template and Temporal Referring excel in the Speed dimension, while Temporal Grounding negatively influences all aspects. (iii) Combining different aspects often leads to synergistic improvements. For example, supplementing Temporal Change (1x) with other tasks leads to better results than training with the set alone. This suggests complementary benefits across diverse temporal reasoning skills. Finally, despite without any video data, our T3 composition with all synthesized tasks, helps LongVA-7B achieve the best overall accuracy, even outperforming ShareGPT4Video-8B trained on 28,000 video samples annotated by GPT-4V. These results highlight the effectiveness of our approach in improving Video LLMs temporal reasoning across various dimensions. Correlation between textual and video temporal understanding: To assess transferability more intuitively, we calculate correlations between textual validation accuracy and benchmark scores of models trained on our textual samples. Figure 5 illustrates strong correlation with TempCompass overall accuracy (Pearson = 0.89, < 0.01), and MLVU accuracy (Pearson = 0.85, < 0.01). It also positively correlates with Video-MME performances (Pearson = 0.59, < 0.05). These findings validate the feasibility of enhancing video temporal understanding from the LLM side. Table 3: MLVU evaluation results. Our textual temporal reasoning transfer achieves the best overall performance, with significant gains in temporal-related aspects over the backbone model. TR: Topic Reasoning, AR: Anomaly Recognition, ER: Ego Reasoning, AO: Action Order, AC: Action Count. * denotes temporal-related dimensions. Best results are in bold. Model Video-ChatGPT-7B (Maaz et al., 2024) Video-LLaVA-7B (Lin et al., 2023a) MA-LMM-7B (He et al., 2024) Llama-3-VILA1.5-8B (Lin et al., 2023b) VILA1.5-40B (Lin et al., 2023b) InternVL-Chat-V1.5-20B (Chen et al., 2023) LongVA-7B (128 frm) + LLaVA-Next + Hotpot QA w/ LLaVA-Next + T3 w/ LLaVA-Next (Ours) GPT-4o AC 31.1 35.9 24.3 0.0 11.7 13.3 25.2 11.7 13.1 29.1 46.3 ER 42.0 45.2 38.9 24.7 35.8 24.5 48.6 20.5 27.6 48. 57.1 Needle QA AO Plot QA AR TR Macro Average 40.3 53.2 43.1 32.4 38.3 40. 70.4 36.6 40.6 72.1 64.8 25.1 20.1 25.1 6.6 34.3 14.3 41.7 17.8 19.7 54.4 56.7 29.9 48.4 35.8 20.0 62.0 42. 68.1 45.3 45.3 69.4 65.1 24.0 57.0 35.5 27.0 56.4 51.3 58.5 17.5 21.5 51.0 74.5 26.9 71.6 51.9 46.2 84.7 80. 82.2 72.7 72.4 81.4 87.4 31.3 47.3 36.4 22.4 46.2 37.9 56.4 31.7 34.3 58.1 64.6 Table 4: Video-MME evaluation results. Our method enhances LongVA-7Bs accuracy across various video durations, even outperforming VILA1.5-40B in temporal reasoning. Method Temporal Perception Temporal Reasoning Short Medium Long Overall Video-LLaVA-7B (Lin et al., 2023a) LLaVA-NeXT-Video-7B-DPO (Liu et al., 2024a) Llama-3-VILA1.5-8B (Lin et al., 2023b) VILA1.5-40B (Lin et al., 2023b) InternVL-Chat-V1.5-20B (Chen et al., 2023) LongVA-7B (128 frm) + LLaVA-Next + Hotpot QA w/ LLaVA-Next + T3 w/ LLaVA-Next (Ours) GPT-4o - 40.0 50.9 60.0 45.5 58.2 54.6 65.5 60.0 74.1 - 29.4 41.2 40.7 33.3 37.3 37.3 39.6 49.7 59. 45.3 48.9 56.1 72.0 60.2 61.1 61.2 60.2 63.3 80.0 38.0 42.0 42.1 61.2 46.4 50.4 50.6 50.9 54.8 70. 36.2 35.6 39.6 53.8 45.6 46.2 44.9 45.6 46.8 65.3 39.9 42.1 45.9 62.3 50.7 52.6 52.2 52.2 55.0 71. Holistic video understanding evaluation: Based on previous results, we adopt the T3 composition, which achieves the highest textual validation scores, to explore transfer effects on comprehensive video understanding benchmarks. Table 3 shows detailed task accuracy results on MLVU. Further training on the original image-text instruction dataset or Hotpot QA decreased overall performance, indicating that video temporal understanding cannot be enhanced through continued fine-tuning or long-context training alone. In contrast, mixing our T3 with LLaVA-Next improves performance across most tasks. Notably, the two highly temporal-oriented tasks, Action Count (AC) and Action Order (AO), see substantial average gain of 8.3 points. Specifically, AC scores increased by 3.9 points (25.2 29.1), while AO scores improved by 12.7 points (41.7 54.4). Table 4 demonstrates substantial improvement in Video-MMEs temporal reasoning subtask (37.3 49.7), even outperforming larger models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Besides, compared to fine-tuning with LLaVA-Next and Hotpot QA, our method yields the best overall performance at 55.0. These results validate that enhanced textual temporal reasoning in LLM backbones effectively transfers to holistic video understanding, confirming our approachs efficacy. 4.3 ANALYSIS Critical LLM components for temporal understanding: We evaluate the impact of selectively unfreezing LLM components across TempCompass, MLVU, and Video-MME datasets using the T3 /w LLaVA-Next dataset. Figure 6a illustrates that fully unfreezing the language model yields optimal performance. For TempCompass, the fully unfrozen model achieves 58.2% accuracy, outperforming self-attention-only (57.7%) and MLP-only (55.1%) configurations. Notably, unfreezing self-attention modules consistently surpasses unfreezing MLP layers, with MLVU showing substantial 4.2 percentage point difference (55.8% v.s. 51.6%) and 2.3 absolute accuracy gap on Video-MME (52.4% v.s. 50.1%). This suggests that self-attention modules play more crucial role in transferring temporal understanding capabilities. These findings align with the understanding that self-attention modules act as dynamic information aggregators (Wang et al., 2023), while MLP layers primarily serve as static knowledge banks (Geva et al., 2021). The superior performance of unfrozen attention 9 (a) Compared to MLP modules, self-attention modules are more important for temporal reasoning. (b) Our temporal-enhanced models leverage more input video frames better. Figure 6: Analysis experiment results. (Left) Effect of trainable modules in the LLM backbones. (Right) Accuracy gain when increasing input video frames from 32 to 128. modules implies that the ability to dynamically focus on and relate relevant temporal information is more critical for temporal reasoning tasks than accessing static knowledge. Enhanced LLMs utilization of increased video frames: We examine whether temporal-enhanced LLMs better leverage increased input video frames by comparing performance gains when increasing frames from 32 to 128, using models trained on Temporal Change sets. Figure 6b shows that our textual temporal reasoning-enhanced models benefit more from increased input frames compared to the baseline LongVA. On TempCompass, the Temporal Change (1x) model gains 1.7% accuracy with 128 frames, versus 0.6% for LongVA. Similarly, on MLVU, enhanced models achieve nearly double the gain of the baseline. These results indicate that our approach enables better utilization of additional input frames. We also observe that training on longer inputs with more distractor captions generally yields larger gains, though the optimal length varies between datasets."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Video Large Language Models (Video LLMs). Video LLMs, integrating LLMs and visual encoders, have shown promising results on diverse video tasks (Tang et al., 2023a). These models typically leverage open-source LLMs (Touvron et al., 2023; Yang et al., 2024) for generation and reasoning capabilities. Recent architectural explorations have focused on efficient video encoding, including Video Transformer and Q-Former (Li et al., 2023b; Ren et al., 2023), spatial and temporal QFormer (Zhang et al., 2023; Tan et al., 2024), and memory bank for long video frames (He et al., 2024). Other approaches like Video-LLaVA (Lin et al., 2023a), Chat-UniVi (Jin et al., 2023), VILA-series (Lin et al., 2023b), and LLaVA-series (Liu et al., 2024a; Zhang et al., 2024a) demonstrate effective transfer from image to video tasks with image-video unification. In contrast to these studies, our work focuses on probing and enhancing the temporal understanding ability of Video LLMs, identifying the LLMs poor grasp of temporal concepts as key bottleneck. Our textual-only temporal reasoning transfer effectively addresses this issue without using any image/video instruction tuning data. Temporal Understanding Benchmarks for Video LLMs. Video temporal understanding benchmarks have evolved rapidly to guide Video LLM development. Pilot studies incorporate existing tasks (Wang et al., 2019; Li et al., 2021) into comprehensive assessments (Li et al., 2023c; Ning et al., 2023; Li et al., 2023a). As Video-LLMs become stronger, benchmarks such as EgoSchema (Mangalam et al., 2023), Neptune (Nagrani et al., 2024), Video-MME (Fu et al., 2024) and MLVU (Zhou et al., 2024), focus on stress-testing models with diverse and challenging tasks of long videos. Specific benchmarks addressing temporal understanding (Li et al., 2023d; Liu et al., 2024b) have highlighted limitations in current Video LLMs. Our work not only diagnoses the temporal understanding bottleneck in Video LLMs but also demonstrates correlation between textual and fine-grained video temporal understanding. Importantly, our proposed methods show improvements on challenging benchmarks such as Video-MME and MLVU, advancing the field of video temporal comprehension."
        },
        {
            "title": "6 CONCLUSIONS AND LIMITATIONS",
            "content": "Our work investigates the temporal reasoning bottleneck of Video LLMs, identifying the language model backbone as the primary source. To address this, we develop textual temporal reasoning transfer framework that synthesizes QA pairs on various temporal concepts from image-text pairs. Experimental results validate the correlation between textual and visual temporal understanding, demonstrating the efficacy of our method on comprehensive video understanding benchmarks. By providing scalable and efficient solution for enhancing temporal reasoning capabilities, our work offers valuable insights for the future development of Video LLMs. Limitations: (i) Limited temporal concept scope: The covered four key dimensions are only an approximation of core abilities and may not encompass the full spectrum of temporal reasoning. (ii) Limited gain for stronger LLMs: Our synthesized dataset might be too straightforward for advanced language models, potentially limiting observable performance gains. Future work could address this by iteratively creating more complex datasets that better challenge and reflect real-world temporal reasoning."
        },
        {
            "title": "REFERENCES",
            "content": "Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. In ICLR (Workshop), 2017. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 35583568, 2021. doi: 10.1109/CVPR46437.2021.00356. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. ArXiv preprint, abs/2406.04325, 2024. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. ArXiv preprint, abs/2312.14238, 2023. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. ArXiv preprint, abs/2405.21075, 2024. 11 Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, 2021. doi: 10.18653/v1/2021.emnlp-main.446. Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1350413514, 2024. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 17351780, 1997. Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. ArXiv preprint, abs/2311.08046, 2023. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. ArXiv preprint, abs/2307.16125, 2023a. Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. ArXiv preprint, abs/2305.06355, 2023b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. ArXiv preprint, 2023c. Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: multi-task benchmark for video-and-language understanding evaluation. ArXiv preprint, 2021. Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of video-language models. ArXiv preprint, abs/2311.17404, 2023d. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. ArXiv preprint, abs/2311.10122, 2023a. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023b. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv: 2403.00476, 2024b. 12 Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. TempCompass: Do video LLMs really understand videos? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 87318772, 2024c. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, and Tobias Weyand. Neptune: The long orbit to benchmarking long video understanding. 2024. Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. ArXiv preprint, 2023. OpenAI. Gpt-4o system card, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 87488763, 2021. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. ArXiv preprint, abs/2312.02051, 2023. Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1358113591, 2024. Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. ArXiv preprint, abs/2312.17432, 2023a. Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. ArXiv preprint, abs/2312.17432, 2023b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 98409855, 2023. doi: 10.18653/v1/ 2023.emnlp-main.609. Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 45804590, 2019. doi: 10.1109/ICCV.2019.00468. 13 Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. ArXiv preprint, abs/2407.15754, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. ArXiv preprint, abs/2407.10671, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, 2018. doi: 10.18653/v1/D18-1259. Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. In Yansong Feng and Els Lefever (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 543553, 2023. doi: 10.18653/v1/2023.emnlp-demo.49. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. ArXiv preprint, abs/2406.16852, 2024a. Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimization of video large multimodal models from language model reward. ArXiv preprint, abs/2404.01258, 2024b. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. ArXiv preprint, abs/2406.04264, 2024."
        },
        {
            "title": "A MORE DETAILS OF VIDEO LLM TEMPORAL ANALYSIS",
            "content": "A.1 VIDEOS Section 2.2 describes our basic idea to create temporally contradicting videos for different temporal aspects. In this section, we provide more details of the source videos and the composition of synthesized videos. Table 6 summarizes the video composition for different temporal aspects. A.1.1 ORDER Source Videos. This aspect encompasses three categories of source videos: people, cats, and flowers. We obtained these videos from the ShutterStock3 platform. To create distinct training and test sets for the classifier probe, we collected videos of each category with both black and white backgrounds. The black background videos were used for training the probe, while the white background videos were reserved for evaluation. Composition of Synthesized Videos. The synthesized video for this aspect concatenate two or three source videos. As illustrated in Table 6, the Two Events videos comprise two categories, showcasing cats and people in reversing sequential orders. The Three Events videos expand to six categories, covering all possible permutations of cat, person, and flower sequences. A.1.2 ATTRIBUTE Source Videos. This aspect is further divided into Shape and Brightness. For Shape, we collected videos from ShutterStock that illustrate the process of flower blooming. Similar to the Order aspect, we gathered flowers with both black and white backgrounds to create separate training and testing sets for the classifier probe. For Brightness, we collected static images from the COCO dataset (Lin et al., 2014) and adjusted pixel values to synthesize videos with brightness variations. Car images were used to create training videos, while cat images were used for testing videos. Composition of Synthesized Videos. Videos of Shape consist of two categories: flower blooming and its reversed process, with the latter created by inverting the source videos. Brightness videos also comprise two categories: brightening and darkening. A.1.3 TEMPORAL REFERRING Source Videos. This aspect utilizes the Three Events videos previously collected for the Order aspect. Composition of Synthesized Videos. The Temporal Referring questions are categorized into three types, each referring to specific part of the video: the beginning, middle, and end. For instance, typical question might be, Which item is shown at the beginning/middle/end of the video? For each of these temporal reference types, the videos are further classified into three categories, yielding different possible answers: person, cat, or flower. A.1.4 TEMPORAL GROUNDING Source Videos. This aspect also employs the Three Events videos originally gathered for the Order aspect. Composition of Synthesized Videos. In this aspect, three question types are formulated, i.e., In which part of the video can we see person/cat/flower? For each question type, the videos are divided into three categories, resulting in different answers grounding to the videos different temporal locations: beginning, middle, or end. A.2 FRAME CAPTIONS To evaluate the textual temporal understanding capabilities of LLMs, we must ensure that frame captions contain adequate information about relevant temporal aspects. For example, in videos 3https://www.shutterstock.com depicting transition from cat to person, the initial frame captions should clearly identify the subject as cat, while later frame captions should describe the person. In our approach, we employ differential captioning strategy for aspects such as Order, Shape Attribute, Temporal Referring and Temporal Grounding, drawing inspiration from ShareGPT4Video (Chen et al., 2024). As outlined in Table 7,we begin by generating caption for the first frame. For subsequent frames, captions are created based on both the visual features of the current frame and the captions of preceding frame. Table 9, 10, 11 demonstrate that this method produces captions with sufficient and accurate per-frame information to comprehend these temporal aspects, thanks to the powerful visual perception ability of GPT-4o. In terms of the Brightness Attribute, we find that simple differential captioning cannot accurately reflect brightness changes across video frames due to the lack of consistent brightness standard. To address this issue, we developed chain-of-thought (CoT) captioning strategy. This approach first prompts GPT-4o to categorize frame brightness into four levels (very dark, slightly dark, normal, and bright) before generating the caption. As illustrated in Table 10, the resulting frame captions accurately reflect brightness changes throughout the video. A.3 CLASSIFIER PROBE TRAINING Model Architecture. Our classifier probe is built upon single-layer unidirectional LSTM model with hidden dimension of 128. This probe is trained on visual features that are down-sampled and projected into the LLM embedding space, i.e., the embedding of visual tokens for Video LLMs. Let Rndv represent these visual features, where = and l, h, w, dv correspond to the temporal, height, width and channel dimensions, respectively. For an 8-frame input video, the default setup of LongVA-7B is = 1152, dv = 3584, while for VILA-8B, it is = 1568, dv = 4096. We further down-sample to Rnd using bi-linear interpolation, where = 128 and = 1024. The LSTM probe is then trained on this down-sampled representation V. Data. Table 6 provides comprehensive breakdown of the training and test video compositions. For the Brightness Attribute, we generated training videos using car images, while the test videos are constructed from cat images. Regarding other temporal aspects, we created training videos with black background, contrasting with the white background used in the test videos. Training Hyper-parameters. Table 12 shows the training hyper-parameters for the LSTM classifier probe. For relatively easy temporal aspects, i.e., the Two Events Order, we train the probe for only 15 epoches. For Attribute and other aspects, we increase the training epoch to 80 and 120, respectively. A.4 RESULTS Figures 7 and 8 provide comprehensive breakdown of our temporal analysis results. The data reveals significant variations in model performance across different sub-categories within particular temporal aspects. In the Order aspect, we observe stark contrast in difficulty between two-event and three-event sequences. The task of discerning the order of three events proves substantially more challenging, with LongVA-7B and VILA-8B experiencing dramatic accuracy drops of 41.8 and 51.1 points, respectively. Regarding Temporal Referring, the accuracy is significantly lower when referring to the middle of the video. The deficiency of the LLM decoder in temporal understanding is also more pronounced in these challenging scenarios."
        },
        {
            "title": "B MORE DETAILS OF TEXTUAL TEMPORAL UNDERSTANDING DATA",
            "content": "B.1 CAPTION POOL The contextual information of our textual QA data are sourced from the detailed image captions from the LLaVA-ReCap-558K dataset 4. These captions are generated by the LLaVA-Next-34B model Liu et al., 2024a based on images from the BLIP558K dataset. We only retain the first sentence of the detailed image captions to form the caption pool Cpool. 4https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K 16 Table 5: Ablation study of the distractor captions and temporal reasoning transfer via images instead of captions. Method LongVA-7B (32 frm) + Temporal Change (w/o Distractor) + Temporal Change (1x) + Temporal Change (2x) + Temporal Change (4x) + Temporal Change (8x) + Image Order + Textual Order Action Direction Speed Order Attribute Change Avg. 92. 90.8 91.1 89.6 90.5 90.2 86.4 91.4 37.3 40.3 37.3 39.1 43.0 42.4 36.1 38.2 42. 41.0 39.8 40.4 39.4 39.8 40.4 41.6 54.3 61.6 65.2 66.9 64.2 64.2 55.3 63.6 51. 60.4 63.9 61.8 64.2 68.1 56.9 58.7 55.9 59.0 59.5 59.6 60.4 61.0 55.2 58.9 B.2 TEXTUAL TEMPORAL REASONING DATA CONSTRUCTION The construction process of our textual temporal reasoning datasets are illustrated in detail in Algorithm 1, 2, 3, 4, 5. The prompts used to generate question-answer pairs and captions are shown in Table 14, 15, 16, 17, 18. B.3 DATASET STATISTICS Table 13 presents detailed statistics of our textual temporal datasets and the two baseline datasets. The information includes the number of samples, the number of relevant and distractor caption, the number of input/output tokens, and the involved modalities. B.4 ABLATION STUDY OF TEXTUAL TEMPORAL REASONING TRANSFER Effect of distractor captions. We maintain the same setting as in our main paper and compare the results of the Temporal Change sets with and without distractor captions. As shown in the middle block of Table 5, incorporating distractor captions yields more pronounced improvement in the targeted Order and Attribute Change aspects. Moreover, increasing the number of distractor captions generally leads to better performance. These results validate our approach of inserting confusing captions to enhance textual temporal reasoning transfer. Transfer via images versus captions. To investigate this difference, we replace the caption with the original image in our Order set, keeping all other settings constant. As shown in the bottom block of Table 5, textual temporal reasoning outperforms the corresponding transfer set using images (Image Order). Notably, transfer via images only marginally increases the Order performance from 54.3 to 55.4, while text format transfer significantly boosts it to 63.6. This substantial gap corroborates our findings in the main paper that the temporal reasoning bottleneck lies on the LLMs side, and therefore, the textual format is more effective in enhancing temporal reasoning capability."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "C.1 TRAINING DATASETS Table 19 provides details of the training datasets used in our main paper. To ensure fair comparison across all compositions, we set the number of training samples to 22k, which is the maximum available in the smallest subsets (Referring and Grounding). For the transfer evaluation on MLVU and Video-MME, we combine the original LLaVA-Next image-text SFT dataset with our textually synthesized samples, as shown in Table 20. The mixing ratio and total sample size are determined based on the exploration results discussed in the following subsection. 17 Table 6: Details of the videos used for our temporal analytical study in Section 2. AB denotes synthesized video first showing and then B. (black) and (white) indicate videos with black and white background, respectively. (car) and (cat) indicate videos showing cars and cats, respectively. xN denotes the number of videos. Train Videos is only used when training the classifier probe. Dataset Order Two Events Three Events Attribute Shape Brightness # Classes Train Videos Test Videos 2 6 2 c1: personcat (black) x400 c2: catperson (black) x400 c1: personcat (white) x400 c2: catperson (white) x400 c1: personcatflower (black) x400 c2: personflowercat (black) x400 c3: catpersonflower (black) x400 c4: catflowerperson (black) x400 c5: flowerpersoncat (black) x400 c6: flowercatperson (black) x400 c1: personcatflower (white) x100 c2: personflowercat (white) x100 c3: catpersonflower (white) x100 c4: catflowerperson (white) x100 c5: flowerpersoncat (white) x100 c6: flowercatperson (white) x100 c1: blooming (black) x177 c2: unblooming (black) x177 c1: brightening (car) x955 c2: darkening (car) c1: blooming (white) x100 c2: unblooming (white) x100 c1: brightening (cat) x394 c2: darkening (cat) x394 Temporal Referring Begin Middle End 3 3 3 Temporal Grounding Person Cat Flower 3 3 3 c1: {personcatflower (black) x400, personflowercat (black) x400} c2: {catpersonflower (black) x400, catflowerperson (black) x400} c3: {flowerpersoncat (black) x400, flowercatperson (black) x400} c1: {personcatflower (white) x100, personflowercat (white) x100} c2: {catpersonflower (white) x100, catflowerperson (white) x100} c3: {flowerpersoncat (white) x100, flowercatperson (white) x100} c1: {catpersonflower (black) x400, flowerpersoncat (black) x400} c2: {personcatflower (black) x400, flowercatperson (black) x400} c3: {personflowercat (black) x400, catflowerperson (black) x400} c1: {catpersonflower (white) x100, flowerpersoncat (white) x100} c2: {personcatflower (white) x100, flowercatperson (white) x100} c3: {personflowercat (white) x100, catflowerperson (white) x100} c1: {catflowerperson (black) x400, flowercatperson (black) x400} c2: {personflowercat (black) x400, flowerpersoncat (black) x400} c3: {personcatflower (black) x400, catpersonflower (black) x400} c1: {catflowerperson (white) x100, flowercatperson (white) x100} c2: {personflowercat (white) x100, flowerpersoncat x100} c3: {personcatflower (white) x100, catpersonflower (white) x100} c1: {personcatflower (black) x400, personflowercat (black) x400} c2: {catpersonflower (black) x400, flowerpersoncat (black) x400} c3: {catflowerperson (black) x400, flowercatperson (black) x400} c1: {personcatflower (white) x100, personflowercat (white) x100} c2: {catpersonflower (white) x100, flowerpersoncat (white) x100} c3: {catflowerperson (white) x100, flowercatperson (white) x100} c1: {catpersonflower (black) x400, catflowerperson (black) x400} c2: {personcatflower (black) x400, flowercatperson (black) x400} c3: {personflowercat (black) x400, flowerpersoncat (black) x400} c1: {catpersonflower (white) x100, catflowerperson (white) x100} c2: {personcatflower (white) x100, flowercatperson (white) x100} c3: {personflowercat (white) x100, flowerpersoncat (white) x100} c1: {flowerpersoncat (black) x400, flowercatperson (black) x400} c2: {personflowercat (black) x400, catflowerperson (black) x400} c3: {personcatflower (black) x400, catpersonflower (black) x400} c1: {flowerpersoncat (white) x100, flowercatperson (white) x100} c2: {personflowercat (white) x100, catflowerperson (white) x100} c3: {personcatflower (white) x100, catpersonflower (white) x100} 18 Table 7: The prompt used to generate frame captions for videos of Order, Shape Attribute, Temporal Referring and Temporal Grounding. First Frame: You are an advanced AI visual assistant. You will be provided with the first frame extracted from video clip. Your task is to describe this frame in as much detail as possible, focusing on the following elements: 1. **Key Objects**: Identify and mention the main objects or subjects present in the frame. Be specific and provide relevant details about each object. 2. **Visual Attributes**: Describe the visual characteristics of the key objects, such as their color, size, shape, texture, or any other notable features. Pay special attention to the overall brightness or lighting conditions in the frame. 3. **Location**: Specify the location or setting of the frame, including the background, environment, or any identifiable landmarks or surroundings. 4. **Potential Action**: If applicable, describe any actions or activities that the key objects might be engaged in or are likely to perform based on their positioning, pose, or context within the frame. 5. **Movement**: If there is any visible or implied movement in the frame, describe the direction, trajectory, or nature of the movement for the relevant objects. Ensure your description accurately reflects only the contents of this frame. Do not reference any part of this prompt in your response. Begin with This frame. The caption should be written in present tense and should not exceed 2 sentences. Other Frames: You are an advanced AI visual assistant tasked with describing frames extracted from video clip. When provided with frame, describe it in detailed and accurate terms, focusing on the changes of following elements: 1. **Key Objects**: Identify and mention the main objects or subjects present in the frame. Be specific and provide relevant details about each object. 2. **Visual Attributes**: Describe the visual characteristics of the key objects, such as their color, size, shape, texture, or any other notable features. Pay special attention to the overall brightness or lighting conditions in the frame. 3. **Location**: Specify the location or setting of the frame, including the background, environment, or any identifiable landmarks or surroundings. 4. **Potential Action**: If applicable, describe any actions or activities that the key objects might be engaged in or are likely to perform based on their positioning, pose, or context within the frame. 5. **Movement**: If there is any visible or implied movement in the frame, describe the direction, trajectory, or nature of the movement for the relevant objects. While your primary focus should be the current frame, you can reference the provided caption of the preceding frame to describe any relevant relationships between the two frames. Ensure your description reflects only the contents of the current frame and do not include any elements of this prompt in your response. Begin with This frame. The caption should be written in present tense and should not exceed 2 sentences. The current frame is uploaded as an image. The caption of the preceding frame is provided below: [previous frame caption] C.2 DATASET MIXING EXPLORATIONS Our preliminary study revealed that training the model exclusively on large-scale textual temporal QA samples led to catastrophic forgetting of visual perception abilities, degrading it to text-only model. 19 Analyze the provided image and categorize its overall brightness level into one of the following categories: bright, normal, slightly dark, very dark. Based on this assessment, craft detailed description of the image, reflecting the main elements presented in the image. Here are two examples of classifed brightness and corresponding descriptions: {brightness: bright, description: person is sitting on bench with book in bright environment. } {brightness: slightly dark, description: The image shows an slightly dark room with red balloon. } Now please give your classification result and description, following the above JSON format: Table 8: The prompt used to generate frame captions for the videos of Brightness Attribute. We prompt GPT-4o to determine the level of brightness before generating the frame caption, which ensures information of brightness change is reflected in the frame captions (Table 10 shows an example). Table 9: Example of video focusing on the Order aspect and corresponding frame captions used to evaluate the LLM decoder. We show four frames here but in practice eight frames are used. The words in blue highlight the major subject depicted in each frame. Frame Captions: Frame 1: This frame shows fluffy white and gray kitten lying on its side against plain white background, playing with small purple toy. The kitten is holding the toy with its front paws and appears to be in playful mood, with some motion blur suggesting movement. Frame 3:This frame depicts the same fluffy white and gray kitten, now lying on its side on the plain white background. The kittens eyes are still closed, and its head is resting on the ground, continuing to appear relaxed or asleep. Frame 5: This frame shows woman with smooth blue skin and dark hair tied back, gazing directly at the camera. She is positioned against plain gray background with visible light reflecting from her skin, and she appears calm and serene based on her expression and posture. Frame 7: This frame shows smiling woman with smooth blue skin and dark, straight hair tied back, facing slightly away from the camera with her head turned toward the viewer. She is set against plain gray background, and her joyful expression and body posture convey sense of happiness and contentment. To mitigate this issue, we explore the integration of original instruction tuning datasets to enhance video understanding transfer performance. Specifically, we combine the original LLaVA-Next dataset with textual temporal QA pairs from our All Temporal subset, varying the mixing ratio from 1 to 20, while fixing total samples to 100k for fair comparison. We use textual validation accuracy as our selection criterion, as it correlates well with temporal understanding ability, as discussed in our main paper. Figure 9 (left) illustrates that performance on the textual component gradually decreased as more image-text instruction tuning samples are included. Based on these results, we selected ratio of 20 Frame Captions: Frame 1: The image shows very dark scene with black and white cat lying on top of laptop keyboard. The cats face and whiskers are slightly visible, but the overall image is hard to see due to the low brightness. Frame 3: The image shows black and white cat lying on top of laptop keyboard. The overall scene is very dark, making it difficult to see details clearly. Frame 5: The image shows black and white cat lying on top of laptop keyboard in slightly dark environment. Frame 7: The image shows black and white cat lying on top of laptop keyboard. The cat appears to be comfortably settled in the space between the laptop screen and keyboard. The overall brightness of the image is normal, allowing clear visibility of the cats fur and the laptops keys. Table 10: Example of video focusing on the Brightness Attribute aspect and corresponding frame captions used to evaluate the LLM decoder. The key information of brightness change is highlighted in blue. Figure 7: Detailed temporal analysis results of LongVA. 2, as it maintains relatively high textual validation accuracy ( 95%) while incorporating sufficient visual-text samples to preserve the models visual capabilities. We further explore the optimal sample size for the mixed dataset, with results shown in Figure 9 (right). As expected, textual accuracy continuously improved with larger training samples. Consequently, we set the total sample size to 200k, as this threshold first exhibits saturated textual validation accuracy while keeping the sample size manageable for efficient training. Frame Captions: Frame 1: This frame shows blue bud at the end of green cotton swab positioned vertically against plain white background. The cotton bud appears clean and unused, with no visible movement or action taking place. Frame 3: This frame shows the same blue bud at the end of green cotton swab, still positioned vertically against plain white background. The cotton on the bud appears slightly more compressed and misshapen compared to the preceding frame. Frame 5: This frame shows blue flower with five petals on green stem, set against white background. The flower appears to be blooming mid-stage with slightly spread petals, and the stem is straight and upright, positioned centrally in the frame. Frame 7: This frame features the same blue flower with six petals on green stem, now slightly to the right against white background. The petals are still widely spread, and the stem remains upright, suggesting no noticeable movement or change from the previous frame. Table 11: Example of video focusing on the Shape Attribute aspect and corresponding frame captions used to evaluate the LLM decoder. The key information of shape change is highlighted in blue. Table 12: Training hyper-parameters for the classifier probe. Temporal Aspect Learning Rate Batch Size #Epoch Optimizer Order (Two Events) Order (Three Events) Attribute Temporal Referring Temporal Grounding 5e-5 5e-5 5e-5 5e-5 5e-5 64 64 64 64 15 120 80 120 120 Adam Adam Adam Adam Adam Table 13: Dataset statistics. 1x8x indicates the length of the textual context, controlled by the number of distractor captions. #tokens are counted using LongVA-7B tokenizer. Dataset Order-GPT (1x) Order-GPT (2x) Order-GPT (4x) Order-GPT (8x) Attribute (1x) Attribute (2x) Attribute (4x) Attribute (8x) Order-Template (phrase) Order-Template (prefix) Order-Template (sentence) Referring Grounding Hotpot QA LLaVA-Next #Samples #Relevant Captions #Distractor Captions #Input Tokens #Output Tokens Modalities 10050 20050 40050 80050 10050 20050 40050 80050 20050 20050 20050 20050 20050 - - 1.8k 3.5k 6.9k 13.7k 1.8k 3.5k 6.9k 13.7k 3.6k 3.6k 3.6k 3.5k 3.5k 1.4k 36.5 13.6 13.6 13.5 13.6 8.4 8.4 8.3 8.3 17.1 15.8 77.1 4.5 8.4 4.0 57. Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text+Image 16k 15k 15k 15k 34k 15k 15k 15k 30k 30k 30k 22k 22k 90k 1M 24 24 24 24 2 2 2 2 36 36 36 3 3 - - 22 Figure 8: Detailed temporal analysis results of VILA. Algorithm 1: Textual temporal QA generation for the Order aspect using GPT-4-turbo. ExtCont(Cpool, C, n) inserts relevant captions in between distractor captions and ensures that every pair of relevant and distractor caption do not share common nouns. Input: Caption pool Cpool, number of relevant captions nrcap, number of distractor captions , 2 ndcap, large language model LLM(), function to extend context ExtCont(Cpool, C, n), QA generation prompt Output: SFT data sample {xin, xout} 1 # Sample relevant captions , ..., nrcap 2 Cr = {C 1 3 # Create extended context 4 Cext = ExtCont(Cpool, Cr, ndcap) 5 # Generate questions and answers 6 Q, = LLM(Cr, p) 7 # Concatenate context and question 8 xin = Cext Q, xout = } Cpool Figure 9: (Left) Exploration of mixing ratios between image-text instruction tuning and textual temporal QA. (Right) Dataset scaling analysis. 23 Algorithm 2: Textual temporal QA generation for the Order aspect using templates. Input: Caption pool Cpool, number of relevant captions nrcap, number of distractor captions ndcap, large language model LLM(), target to shuffle tg {sentence, phrase, pref ix}, function to extend context ExtCont(Cpool, C, n), function to extract noun phrase from sentence ExtrcPhrase(C), order-related question templates Qtp, prefix templates Ppref ix Output: SFT data sample {xin, xout} 1 # Sample relevant captions , ..., nrcap 2 Cr = {C 1 3 # Create extended context 4 Cext = ExtCont(Cpool, Cr, ndcap) 5 # Sample question from the templates, e.g., \"Reorder the } Cpool , 2 following captions according to the above video.\" 6 Qtp 7 if tg == sentence then 8 = Shuffle(Cr) # Add shuffled captions after the question = Cr 13 14 15 16 17 19 20 21 22 23 25 9 10 end 11 if tg == phrase or tg == pref ix then 12 # Extract noun phrases from the captions = {ExtrcPhrase(Cr)Cr Cr} # Shuffle the phrases Pshuf = Shuffle(P) if tg == pref ix then # Add prefix before the phrases, e.g., (1)(2)(3) Pshuf = {ppf pphappha Pshuf , ppf Ppref ix} = {ppf pphappha P, ppf Ppref ix} end # Add shuffled phrases after the question = Pshuf # Sample three other permutations for form the options = Sample(Permutations(P) P, 3) = Shuffle(o) = 26 27 end 28 # Concatenate context and question 29 xin = Cext Q, xout = Algorithm 3: Textual temporal QA generation for the Attribute aspect. Input: Caption pool Cpool, number of distractor captions ndcap, large language model LLM(), set of attributes = {color, light, shape, posture, emotion}, attribute modification prompt pa, QA generation prompt pqa, function to extend context ExtCont(Cpool, C, n) Output: SFT data samples {xin, xout} 1 # Sample caption from the caption pool 2 Cr Cpool 3 # Generate similar captions by modifying color, light condition, shape, posture, or emotion 4 Cattr = {Cattrattr a} = LLM(Cr, pa) 5 # Create extended context 6 Cext = {Cext attrattr a}, where Cext 7 # Generate questions and answers 8 Q, = LLM(Cattr, pqa), where = {Qattrattr a}, = {Aattrattr a} 9 # Concatenate context and question 10 xin = {Cext attr = ExtCont(Cpool, Cr Cattr, ndcap) attr Qattrattr a}, xout = 24 Algorithm 4: Textual temporal QA generation for the Temporal Referring aspect. Input: Caption pool Cpool, number of distractor captions ndcap, large language model LLM(), function to extend context ExtCont(Cpool, C, n), element types = {object, action, attribute}, temporal location reference = {at the begin, at the middle, at the end}, caption generation prompt pc, QA generation prompt pqa Output: SFT data sample {xin, xout} 1 # Sample caption from the caption pool 2 Cr Cpool 3 # Generate three similar captions for each element 4 = {Cee e} = LLM(Cr, pc), where Ce = 3 5 # Create extended context 6 Cext = {Cext e}, where Cext 7 # Generate questions and answers. = ExtCont(Cpool, Ce, ndcap) captions share the same question but have different answers. 8 Q, = LLM(C, pqa), where = {Qee e}, = {{AA Ae}e e}, Ae = 3 9 # Add temporal reference to the questions and concatenate the context 10 xin = {{Cext Qe tt t}e e}, xout = For each element, the three Algorithm 5: Textual temporal QA generation for the Temporal Grounding aspect. Input: Element types = {object, action, attribute}, similar captions = {Cee e}, extended context Cext = {Cext e}, questions for the captions = {Qee e}, answers to the questions = {{AA Ae}e e}, large language model LLM(), declarative statement generation prompt ps, grounding question templates Qtp = {Qtp }Qtp i=1 # Generate three declarative statements, corresponding to three answers in Ae Se = LLM(Qe, Ae, ps), where Se = 3 = Se Output: SFT data sample {xin, xout} 1 # Initialize statement set 2 = {} 3 for do 4 6 7 end 8 # Initialize SFT data sample set 9 xin, xout = {}, {} 10 for do 11 for Se do 5 12 14 end 15 16 end 25 # Sample grounding question template, e.g., \"In which part of the video can we observe [X]?\" and insert the statement in it. xin = xin {Cext xout = xout {A}, where {at the begin, at the middle, at the end} is determined by the location of corresponding caption in Cext Replace(Q, [X], S)}, where Qtp Table 14: The prompt used to generate textual temporal QA for the Order aspect. You will be presented with list of captions describing keyframes of video. Your task is to generate five multi-choice questions (and corresponding answers) based on the captions. The questions should focus on the sequential order of the keyframe contents. Make sure that the questions are related to the order of the keyframes and diverse. Here is an example of captions and the corresponding questions and answers: Captions: 1. The image shows person playing basketball. 2. The image shows dog running on the grass. 3. The image is about beautiful flower on the table. 4. The image illustrates bustling city street. Questions and Answers: { qas: [ { question: Sort the events from the video by their chronological order. (1) city street; (2) dog running on the grass; (3) person playing basketball; (4) flower on the table., options: [ (1)(2)(3)(4), (3)(2)(4)(1), (2)(1)(4)(3), (1)(4)(3)(2) ], answer: (3)(2)(4)(1) }, { question: Organize the listed events from the video according to their time sequence: (1) city street (2) dog running on the grass (3) person playing basketball (4) flower on the table, options: [ city street dog running on the grass person playing basketball flower on the table, person playing basketball dog running on the grass flower on the table city street, dog running on the grass city street flower on the table person playing basketball, city street flower on the table person playing basketball dog running on the grass ], answer: person playing basketball dog running on the grass flower on the table city street }, { question: What is the correct order that objects appear in the video?, options: [ dog, flower, person, street, person, dog, flower, street, street, flower, person, dog, flower, street, dog, person ], answer: person, dog, flower, street }, { question: In what sequence do the events occur in the video?, options: [ dog running and then person playing basketball, city street is shown and then flower is shown, flower appears followed by city street, city street appears followed by dog runnin ], answer: flower appears followed by city street } ] } Now please generate five question-answer pairs based on the following captions. Ensure your resonse follows the above JSON format and style of the example question-answer pairs. Captions: [image captions] Questions and Answers: 26 Table 15: The prompt used to generate similar captions by modifying attributes in the original caption. You will be presented with an original image caption. Your task is to enrich this caption with details regarding the attributes of objects or environment. The attributes could include but not limited to these aspects: light condition, color, size & shape, emotion and posture. You are required to create two distinct captions for each attribute. These two captions should contrast each other in terms of the corresponding attribute (e.g., black versus white for color, small versus big for size & shape). Here are some examples of original caption and enriched captions related to different aspects: Original Caption: The image shows person sitting on the chair. Enriched Captions: { captions: { light condition: [ The image shows person sitting on the chair with beam of light illuminating his face., The image shows person sitting on the chair. His appearance is barely recognizable in the dim environment. ], emotion: [ The image shows person, with big smile, sitting on the chair, The image shows an angry person sitting on the chair ], posture: [ The image shows person sitting relaxing on the chair., The image shows person standing straight in fromt of the chair. ], size & shape: [ The image shows person sitting on round, cylindrical chair., The image shows person sitting on square-shaped chair. ] } } Original Caption: The image shows an apple on the table. Enriched Captions: { captions: { color: [ The image shows red apple on the table., The image shows green apple on the table. ], size & shape: [ The image shows big ripe apple on the table., The image shows rotten apple on the table. ] } } Original Caption: The image illustrates an air balloon. Enriched Captions: { captions: { light condition: [ The image illustrates an air balloon in dark room., The image illustrates an air balloon in bright room., ], size & shape: [ The image illustrates deflated air balloon., The image illustrates an inflated air balloon. ], color: [ The image illustrates light blue air balloon., The image illustrates an air balloon in yellow color. ] } } Now please generate enriched captions based on the following original caption. Ensure your response follows the JSON format of the above examples. Original Caption: [original caption] Enriched Captions: Table 16: The prompt used to generate textual temporal QA for the Attribute aspect. You will be presented with several pairs of image captions. Each pair of captions depicts two keyframes in video. Your task is to generate multi-choice questions (and corresponding answers) for each pair of captions. The questions should focus on the change of attribute between the keyframe contents. Ensure that the questions are diverse and distinct from each other in wordings. Here are some examples of caption pairs and generated question-answer pairs: Caption Pair 1: 1. The image shows person sitting on the chair with beam of light illuminating his face. 2. The image shows person sitting on the chair. His appearance is barely recognizable in the dim environment. Caption Pair 2: 1. The image shows person, with big smile, sitting on the chair. 2. The image shows an angry person sitting on the chair. Questions and Answers: { qas: { caption pair 1: { question: How does the light condition change in the video?, options: [ remaining stable, turning darker, turning brighter ], answer: turning darker }, caption pair 2: { question: What change occurs to the person in the video?, options: [ changing from smiling to being angry, changing from being angry to smiling, changing from feeling shy to being angry, changing from feeling awkward to smiling ], answer: changing from smiling to being angry } } } Caption Pair 1: 1. The image illustrates an air balloon in dark room. 2. The image illustrates an air balloon in bright room. Caption Pair 2: 1. The image illustrates deflated air balloon. 2. The image illustrates an inflated air balloon. Caption Pair 3: 1. The image illustrates light blue air balloon. 2. The image illustrates an air balloon in yellow color. Questions and Answers: { qas: { caption pair 1: { question: What transformation is occurring in the brightness of the video?, options: [ increasing, staying the same, decreasing ], answer: increasing }, caption pair 2: { question: What is happening to the shape of the air balloon?, options: [ it is getting bigger, it is getting smaller, its size and shape remains consistent ], answer: it is getting bigger }, caption pair 3: { question: How can we describe the change happening to the air balloon?, options: [ its color changes from grey to yellow, its color changes from light blue to yellow, its color changes from yellow to light blue, its color changes from yellow to green ], answer: its color changes from light blue to yellow } } } Now please generate question-answer pairs based on the following caption pairs. Ensure your response follows the above JSON format and style of the example question-answer pairs. [caption pairs] Questions and Answers: Table 17: The prompt used to generate similar captions for the Temporal Referring aspect. You will be presented with an original image caption. Your task is to modify this caption by changing the original elements including object, action and attribute. Ensure that the modified element is distinct from the original ones. Here is an example of original caption and modified captions: Original Caption: The image shows person sitting on the chair. Modified Captions: { captions: { change object: [ The image shows cat sitting on the chair., The image shows dog sitting on the chair., The image shows cup placed on the chair. ], change action: [ The image shows person standing next to the chair., The image shows person sleeping on the chair., The image shows person dancing nearby the chair. ], change attribute: [ The image shows tall person sitting on the chair., The image shows short person sitting on the chair., The image shows strong person sitting on the chair. ] } } Now please generate modified captions based on the following original caption. Ensure your response follows the JSON format of the above example. Original Caption: [original caption] Table 18: The prompt used to generate declarative statements for the Temporal Grounding aspect. You will be given question paired with several answers. Your task is to reformulate each answer into simple declarative statement. ###Example1 Question: What is the color of the cat? Answer 1: white Answer 2: orange Answer 3: black Declarative Statement 1: the cat is white Declarative Statement 2: the cat is orange Declarative Statement 3: the cat is black ###Example2 Question: What is the person doing? Answer 1: running Answer 2: playing guitar Answer 3: swimming Declarative Statement 1: the person is running Declarative Statement 2: the person is playing guitar Declarative Statement 3: the person is swimming Now please reformulate the following question and answers according to the above examples. [question and answers] 29 Table 19: Datasets used in our textual temporal transfer explorations in Table 2. Dataset # samples Description Open-LLaVA-Next HotpotQA Temporal Change (w/o Distractor) Temporal Change (1x) Temporal Change (2x) Temporal Change (4x) Temporal Change (8x) Temporal Change (1-8x) Order-Template Temporal Referring Temporal Grounding Order-Template + Temporal Change (1x) Temporal Grounding + Temporal Change (1x) Temporal Referring + Temporal Change (1x) T3 22k 22k 22k 22k 22k 22k 22k 22k 22k 22k 22k 22k 22k 22k 22k LLaVA-Next image-text SFT dataset. Multiple-document question answering dataset. Balanced mixture of Order-GPT and Attribute without interesting distractor captions Balanced mixture of Order-GPT (1x) and Attribute (1x) Balanced mixture of Order-GPT (2x) and Attribute (2x) Balanced mixture of Order-GPT (4x) and Attribute (4x) Balanced mixture of Order-GPT (8x) and Attribute (8x) Balanced mixture of Temporal Change 1x, 2x, 4x and 8x Balanced Mixture of Order-Template (phrase), (prefix) and (sentence) Synthesized textual samples for enhancing Referring Synthesized textual samples for enhancing Grounding Balanced mixture of Order-Template and Temporal Change (1x) Balanced mixture of Temporal Grounding and Temporal Change (1x) Balanced Mixture of Temporal Referring and Temporal Change (1x) Balanced Mixture of all our synthesized subtasks. Table 20: Datasets used for transferring to holistic video understanding benchmarks in Table 3 and Table 4. Dataset # samples Description LLaVA-Next HotpotQA w/ LLaVA-Next T3 w/ LLaVA-Next (Ours) 200k 200k 200k LLaVA-Next image-text SFT dataset. Hotpot QA (66.7k) + + LLaVA-Next (13.3k) Temporal All (66.7k) + LLaVA-Next (13.3k)"
        }
    ],
    "affiliations": [
        "Peking University",
        "The University of Hong Kong",
        "University of California, San Diego"
    ]
}