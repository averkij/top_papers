{
    "paper_title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
    "authors": [
        "Baohao Liao",
        "Yuhui Xu",
        "Hanze Dong",
        "Junnan Li",
        "Christof Monz",
        "Silvio Savarese",
        "Doyen Sahoo",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios."
        },
        {
            "title": "Start",
            "content": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning Baohao Liao * 1 Yuhui Xu * 2 Hanze Dong * 2 Junnan Li 2 Christof Monz 1 Silvio Savarese 2 Doyen Sahoo 2 Caiming Xiong 2 5 2 0 2 1 3 ] . [ 1 4 2 3 9 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce Reward-Guided Speculative Decoding (RSD), novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines lightweight draft model with more powerful target model, incorporating controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4 fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. 1. Introduction Scaling laws are widely recognized by the machine learning community as foundational principle for the development of large language models (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022). They emphasize that increasing both model size and dataset scale leads to improved loss reduction and, consequently, enhanced generalization capabilities. When data and model size are scaled to extraordinary levels, performance can reach unprecedented heights. Large models demonstrate remarkable capabilities *Equal contribution 1Language Technology Lab, University of Amsterdam 2Salesforce AI Research. Correspondence to: Yuhui Xu <yuhui.xu@salesforce.com>. 1 across diverse tasks, showcasing robust generalization and advanced reasoning skills (Brown et al., 2020; Hurst et al., 2024; Anthropic, 2024; Team et al., 2024). These advancements result in high computational and economic costs. While training is resource-intensive, inference at scale is even costlier, requiring vast computational infrastructure and energy to serve billions of queries (Patterson et al., 2021). The exponential growth in inference costs makes it key challenge for large model deployment, highlighting the need for efficient techniques (Frantar et al., 2022; Lin et al., 2024; Xu et al., 2023; Sun et al., 2023; Zhang et al., 2023b; Li et al., 2024a; Xu et al., 2024; Liao & Monz, 2024; Liao et al., 2024a) to reduce energy use and ensure scalability. Specifically, Sequential token generation in large LLMs incurs significantly higher computational costs compared to smaller models. This increased latency can hinder their deployment in real-time or high-throughput applications. To address this issue, parallel decoding techniques, such as speculative decoding, have emerged as effective solutions (Leviathan et al., 2023). Speculative decoding operates by leveraging smaller, lightweight model to generate candidate outputs, which are then validated and refined by the larger model. This approach significantly reduces the number of decoding tokens required by the large model, thereby accelerating the overall process. The smaller model serves as guide, proposing sequences that the larger model can confirm or adjust, leading to faster inference times without compromising quality. Furthermore, speculative decoding ensures efficiency by maintaining high-quality outputs through careful calibration of the smaller model. By aligning the smaller models predictions with the larger models capabilities, this method minimizes discrepancies and enhances reliability during inference. Despite advancements in parallel decoding, speculative decoding remains underutilized for complex reasoning tasks, particularly multi-step generation. key limitation is the strict unbiasedness requirement, which ensures the final token distribution matches the large models but restricts flexibility in exploring diverse completions (Holtzman et al., 2020). While unbiasedness maintains theoretical fidelity, it often reduces efficiency, especially when the draft model diReward-Guided Speculative Decoding for Efficient LLM Reasoning Figure 1. Reward-Guided Speculative Decoding (RSD). This diagram illustrates how RSD improves upon standard speculative decoding (SD) by incorporating reward-guided selection. SD strictly enforces exact token matching between the draft and target model, leading to unnecessary computations when mismatched tokens are discarded. In contrast, RSD evaluates draft outputs based on reward signals and selectively refines them, reducing reliance on exact matching and improving efficiency. The process starts with small and fast draft model generating preliminary results, followed by larger and more reliable target model verifying and refining predictions. Darker background regions indicate higher computational costs, showing how SD wastes resources on rejected tokens, whereas RSD reduces unnecessary steps by accepting useful draft outputs even when they do not exactly match, balancing efficiency and accuracy. verges from the large model. High-quality tokens (e.g., those favored by process reward) may still be rejected if their probabilities under the large model are too low, leading to wasted computation and negating potential speedups. This dependence inflates overhead and limits speculative decodings benefits, particularly in long-trajectory reasoning tasks like math and coding. Allowing controlled bias, where the final distribution deviates slightly from the large model, can improve performance. If draft token is correct but does not match the large models distribution exactly, strict rejection is counterproductive. Reward-guided acceptance retains valuable partial solutions, reduces unnecessary queries, and can even surpass the large models performance. Thus, more adaptive approaches are needed to balance efficiency and robustness, ensuring broader real-world applicability. In this work, we introduce Reward-Guided Speculative Decoding (RSD), novel framework that balances efficiency and accuracy by integrating computationally lightweight draft evaluations with reward-driven refinements from more capable target model. Unlike traditional speculative decoding, which strictly enforces unbiasedness, RSD leverages reward signals to adaptively select high-value draft outputs rather than discarding mismatched tokens outright. The process begins with the draft model generating candidate steps, which are then evaluated using reward function. Steps with sufficiently high reward scores are accepted to continue the reasoning trajectory, while lowerscoring steps trigger speculative corrections using the target model. As illustrated in Fig. 1, this adaptive mechanism is robust against the distribution shifts issue between the draft and target models while optimizing resource allocation. By dynamically adjusting when to invoke the larger model, RSD significantly reduces unnecessary computations while maintaining or even surpassing the quality of traditional inference approaches. This approach is particularly wellsuited for long-horizon reasoning tasks, where balancing computational cost and accuracy is critical. Contributions. We propose Reward-Guided Speculative Decoding, novel approach to accelerate LLM inference, particularly for reasoning tasks. It introduces an adaptive decoding framework that dynamically mixes outputs from draft and target model, guided by reward function that evaluates output quality at each step. This enables efficient, high-quality reasoning by constructing flexible mixture distribution, PRSD, balancing efficiency and accuracy through reward-based weighting. RSD employs rejection sampling to selectively refine draft outputs, ensuring scalability. Theoretically, we derive optimal weighting strategies under computational constraints, maximizing efficiency without performance drop. Extensive experiments on GSM8K, MATH500, Olympiad Bench, GPQA, MMLU STEM, and GaoKao2023-En show that RSD not only improves the reasoning accuracy up to 3.5 than SD on average, but also significantly reduces the inference computation with up to 4.4 fewer FLOPs, compared to using the target model alone. 2. Reward-Guided Speculative Decoding Notations. Let all tokens be embedded in Euclidean space. The prompt is represented as Rld, and the response as RLd. The response can be further decomposed into sequence of steps [y1, , yn], which we denote as y1:n. Reward-Guided Speculative Decoding for Efficient LLM Reasoning For language models, we consider the iterative process of generating sequence of steps y1:n given an input and model (small/draft model) or (large/target model). At each step i, the context zi is constructed by combining the initial input with the sequence of previously generated outputs y1:i1, such that zi = [x, y1:i1]. Using this context, the next output yi is sampled from conditional distribution: yi Pm(yizi) or yi PM (yizi), where Pm corresponds to small draft model m; PM corresponds to large target model . Our goal is to optimize the expected reward at each step with computation constraints. For each step i, we define reward function r(yizi) = r(yix, y1:i1), which evaluates the quality of the generated step yi within the sequence y1:i given prompt x. higher reward r(yizi) indicates greater likelihood that the model output aligns with the desired response given the and y1:i1. We consider the case that the expected reward achieved by the large model at each step satisfies: EyiPM [r(yizi)] EyiPm [r(yizi)] , (1) i.e., the target model should outperform or at least match the draft model in terms of the expected reward at every step. Our analysis is based on the fact that the large models predictions yield higher quality outputs, leveraging its capacity for complex reasoning and contextual understanding. We define the distribution PRSD as dynamic mixture of Pm and PM , where the combination depends on the quality of the conditional output yizi. Specifically, we have: PRSD(yizi) = w(yizi)Pm(yizi) + v(yizi)PM (yizi), where w() and v() are weighting functions that dynamically adjust based on the quality of the output yizi. Unlike Pm, we assume PM is sufficiently robust and reliable (also costs more); therefore, we set v(yizi) = ν, where ν is constant. This ensures that PM always contributes to the mixture and is not rejected outright, as it acts as stable fallback for handling low-quality outputs. In our approach, w(yizi) is determined by reward function r, such that: w(yizi) = ωr(yizi) = ω(r(yizi)), where r(yizi) measures the quality or preference for the conditional output yizi. The function ω() maps r(yizi) to value in [0, 1], reflecting the confidence in Pm. For example, r(yizi) could depend on factors like the accuracy or relevance of yizi, and ω(r(yizi)) controls how much weight is assigned to Pm relative to PM . Because w(yizi)Pm(yizi) is an unnormalized distribution, the constant ν ensures proper normalization of the mixture."
        },
        {
            "title": "Probability Density",
            "content": "Pm PM w() wPm PRSD Figure 2. Illustration of Reward-Guided Speculative Decoding w(yizi)Pm dominates the mixture, and PRSD(yizi) primarily reflects the predictions of Pm. This shows high confidence in the smaller models ability to produce reliable results. Conversely, when r(yizi) is small, ω(r(yizi)) approaches 0. The mixture weight then shifts toward νPM , which allows the larger model PM to dominate. Since PM is assumed to be sufficiently robust and reliable, it compensates for the smaller model by effectively handling low-quality outputs, ensuring overall performance stability. Fig. 2 depicts the mixture distribution PRSD, which is influenced by the quality of the output yizi. The blue curve represents the distribution Pm, and the green dashed curve represents PM . The orange dotted line shows the weighting function w(yizi), which adjusts the mixture between these two distributions. In regions where yizi corresponds to high-quality outputs, the weighting function is elevated, placing more weight on Pm to prioritize efficiency. For low-quality outputs, decreases, shifting the emphasis toward PM and thereby penalizing low-quality samples. The red curve represents the resulting mixture distribution PRSD, which adapts based on the quality of the output, demonstrating more efficient sampling process for higher-quality outputs and more penalized approach for low-quality ones. Regarding the efficiency, large proportion of samples are drawn from Pm, with small part sourced from PM . Additionally, we maximize EyiPRSD r(yix, y1:i1) to ensure that the model performs effectively in reasoning tasks. 2.1. RSD Algorithm The Reward-Guided Speculative Decoding (RSD) algorithm operates by dynamically mixing the draft model and target model at each generation step, with the objective of balancing efficiency and quality. The algorithm leverages the reward function to guide this dynamic mixing, ensuring that necessary higher-quality steps are more likely to be generated by the target model , while the draft model is used for cost-effective generation when possible. Below, we describe the key components of the algorithm . At each decoding step i, the algorithm follows these steps: 1. Generate Draft Step: The draft model generates candidate ˆyi given the prompt and previous outputs. When r(yizi) is large, indicating highly preferred or high-quality output, ω(r(yizi)) approaches 1. In this case, 2. Compute Reward: The reward function r(yi zi), where zi = [x, y1:i1], evaluates the steps quality. 3 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Algorithm 1 RSD: Reward-Guided Speculative Decoding Input: Prompt x, draft model m, target model , process reward model r, acceptance criterion Aω, EOS token s, max length Assign y1:0 for = 1 to 1 do Generate draft step ˆyi m(x, y1:i1) Compute reward ri r(yix, y1:i1) if Aω(ri) then Accept the draft step yi ˆyi else Generate target step yi (x, y1:i1) end if if yi then break end if end for Output: Response y1:i 3. Apply Acceptance Criterion: The reward ri is assessed using Aω(ri). If accepted, ˆyi is used; otherwise, the target model generates new step. 4. Sample from Mixture Distribution: Accepted steps come from Pm, rejected ones from PM , dynamically balancing efficiency and accuracy. 5. Repeat Until Termination: Steps are generated until the EOS token appears or sequence reaches length . Computational Efficiency. The primary advantage of the RSD algorithm lies in its ability to combine the strengths of both the draft and target models while minimizing the computational cost. By using the draft model for most of the generation process, the algorithm reduces the computational burden compared to strategy that always uses the target model . The dynamic weighting function w(r) ensures that the draft model is used when the generated steps are of sufficient quality, thereby maximizing efficiency. Moreover, by employing rejection sampling, the algorithm only resorts to the more expensive target model when necessary, ensuring that high-reward steps are generated while keeping the overall cost low. This balance between cost and quality is particularly important in large-scale applications, where both efficiency and performance are critical. Formal Description of the Algorithm. The RSD algorithm is formally described in Algorithm 1, and the acceptance criterion is outlined in Algorithm 2. In the following, we present the theoretical basis for the mixture distribution used in the algorithm. We also provide the final distribution of the proposed algorithm in Proposition 2.1. That means the expected reward from PRSD lies within the reward bounds defined by ωrPm(yz) and PM (yz). Proposition 2.1. Given the Algorithm 1 and 2, for each step yz. Assume that ωr(yz) = ω(r(yz)), the PRSD(yz) = ωr(yz)Pm(yz) + νPM (yz), (2) Table 1. Variants of Weighting Function ω(r) and their definitions"
        },
        {
            "title": "Definition",
            "content": "ω(r) = ω(r) = 1(r δ) ω(r) = min(1, max(0, r)) ω(r) = max(0, 1+r ) 1 ω(r) = 1+eα(rδ) Constant (0, 1) 1 if δ, else 0 Clipping within [0,1] Sigmoidal transformation Logistic function Algorithm 2 Acceptance Criterion Aω Input: value R, weighting function ω : [0, 1] Compute weighting function ω(r) if ω(r) = 0 or 1 then Aω(r) = ω(r) else Sample (0, 1); Aω(r) = 1(ω(r) u) end if Output: Aω(r) where ωr(yz) is the weighting function that adjusts the relative contributions of the draft model Pm, and ν is the normalizing constant ν = 1 EPmωr. 2.2. Acceptance Criterion Aω and Weighting Function Proposition 2.2. Given the following assumptions: ω(r) is non-decreasing in r; EPM [r(yz)] EPm[r(yz)]; it follows that the expected value of r(yz) under the RSD induced distribution satisfies: EPRSD [r(yz)] EPm[r(yz)]. The weighting function ω() plays crucial role in adjusting the mixture distribution. Several variants of the weighting function are considered. Table 1 provides summary of different choices. Each of these variants provides new tradeoff between draft model and target model. Intuitively, binary function can maximize expected reward under strict sampling budget constraint, smooth weighting might in practice handle noisy reward model outputs more gracefully. 2.3. Optimal Weighting In the following Proposition, we demonstrate that the optimal weighting function for maximizing reward under constrained sampling budget is binary step function, which assigns weight of 1 only to high-reward outputs. Proposition 2.3. Given constrained sampling budget ν = 1 Eypm ωr(yz) γ, γ (0, 1), the optimal sampling strategy that maximize the reward is ωr(yz) = (cid:40) 1 0 if r(yz) δγ(z), if r(yz) < δγ(z), (3) Reward-Guided Speculative Decoding for Efficient LLM Reasoning Figure 3. Left: comparison of the reward scores for all questions generated by the draft model and the target model within the RSD framework. Middle: focused comparison of the reward scores for correctly answered questions generated by the draft model and the target model in the RSD framework. Right: The winning rate (in terms of reward) comparison between the draft model and the target model, highlighting the proportion of cases where each model outperforms the other. RSD is configured with Qwen2.5-Math-1.5B-Instruct as the draft model, Qwen2.5-Math-7B-Instruct as the target model, and Skywork-o1-Open-PRM-7B as the PRM. where δγ(z) is the largest possible threshold that makes the function satisfy the constraint. Note that when binary function is used, Algorithm 2 almost surely degenerates into deterministic procedure, producing definite acceptance or rejection outcome. In real implementation, δγ can be hyper-parameter δ. 2.4. Discussion Process Reward for Each Model. As stated in Eq. (1), we expect the target model to have higher reward. Fig. 3 confirms this on MATH500, showing that for correctly answered questions within RSD, PM consistently outperforms Pm in reward (Middle figure). Comparison with the reward of PM . Since PRSD is mixture of ωrPm and PM , its reward is weighted sum of their expected rewards. Thus, PRSD can exceed PM in expected reward when we use an aggressive that only assigns 1 to high-reward regions. In cases where ωrPm has higher expected reward than PM , PRSD also achieves higher reward. We also notice this phenomenon empirically. General Weighting Function and SD. Notably, rewardbased weighting functions ωr are not the only option. An alternative approach explicitly defines the weighting function in terms of the likelihood ratio: w(y z) = min 1, α (cid:18) PM (y z) Pm(y z) (cid:19) , where α > 0 is hyperparameter controlling how quickly the method transitions from the draft model to the target model. This formulation is algorithmically similar to speculative decoding, with α as tunable parameter and PM serving as the alternative distribution. In standard speculative decoding, one often uses PM Pm to propose alternative generations, which allows unbiased final distribution with α = 1. Intuitively, if token (or reasoning step) is significantly less likely under the large model than under the draft model (i.e., if PM is small), it may indiPm cate suspicious scenario that the draft model is unable to handle effectively. More generally, hybrid approaches can combine both the reward function and the ratio PM , Pm (cid:16) for example: w(yz) = min , or by normalizing these two quantities in differentiable manner. These ratio-based or hybrid weighting functions may help address situations where (i) the reward model is noisily correlated with true correctness or (ii) the small and large model distributions diverge substantially. 1, β r(yz) PM (yz) Pm(yz) (cid:17) Hence, the weighting function in Reward-Guided Speculative Decoding can be flexibly designed to incorporate process rewards, likelihood ratios, or any combination thereof, as long as it meets the non-decreasing requirement in (when appropriate) and remains bounded within [0, 1]. This flexibility allows RSD induced algorithm to be adapted to different practical constraints (e.g., distribution mismatch, reward model availability/accuracy) while still reaping the efficiency gains of speculative decoding. 3. Empirical Results Models. To assess the performance of RSD, we employ both general-purpose and math-focused LLMs as our target and draft models, specifically Qwen-2.5 (Yang et al., 2024a), Llama-3 (Dubey et al., 2024), and Qwen-2.5-Math (Yang et al., 2024b). Our system utilizes Skywork-o1-OpenPRM (o1 Team, 2024) as the Process Reward Model (PRM), as it was the most advanced open-source PRM available during our experiments (Zheng et al., 2024). The reward score ranges from 0 to 1, the higher the better. Datasets. We evaluate our method on diverse set including GSM8K (Cobbe et al., of reasoning tasks, 2021b), MATH500 (Hendrycks et al., 2021), MMLU STEM (Hendrycks et al., 2020), OlympiadBench (He et al., 5 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Table 2. Accuracy on reasoning benchmarks. In general, δ = 0.7 offers good trade-off between accuracy and efficiency for all tasks. δ is the optimized threshold for each task, as different tasks have different complexity of reasoning. Refer to B.1 for detailed results. Method Target Draft Model Model PRM Setting MATH500 GSM8K GaoKao 2023 En Olympiad Bench GPQA Diamond MMLU STEM Avg. Math Model, Target and Draft: Qwen2.5-Math-Instruct, PRM: Skyworko1-Open-PRM Single Model Majority Voting Best-of-N SD RSD RSD RSD RSD Single Model Majority Voting Best-of-N Majority Voting Best-of-N SD RSD RSD RSD RSD Single Model Majority Voting Best-of-N SD RSD RSD RSD RSD Single Model Majority Voting Best-of-N SD RSD RSD RSD RSD 7B - - 7B 7B 7B 7B 7B 72B - - - - 72B 72B 72B 72B 72B 7B - - 7B 7B 7B 7B 7B 8B - - 8B 8B 8B 8B 8B - 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B - 1.5B 1.5B 7B 7B 7B 7B 7B 7B 7B - - - maj@16 7B = 16 - δ = 0.7 δ = 0.7 δ δ - 1.5B 7B 1.5B 7B - - - - maj@64 7B = 64 maj@64 7B = 64 - δ = 0.7 δ = 0.7 δ δ - 1.5B 7B 1.5B 7B 83.2 79.0 82.2 83.4 82.6 84.6 82.6 84.6 85.6 80.2 82.4 88.0 86.2 84.8 86.4 88.0 86.6 88.0 95. 88.9 93.3 95.6 94.5 95.5 95.5 95.8 95.8 89.8 94.5 96.5 97.2 95.8 96.4 96.7 97.0 96.9 66.8 69.9 69.4 67.3 68.8 68.3 68.8 68.3 73. 71.2 68.6 73.8 71.4 71.7 73.0 74.0 73.2 74.0 41.2 45.5 44.9 40.6 39.6 42.1 40.6 43.6 48.4 45.9 44.3 47.6 44.4 47.6 49.8 49.9 49.8 49.9 General Model, Target and Draft: Qwen2.5-Instruct, PRM: Skyworko1-Open-PRM - 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B - - - maj@16 7B = 16 - δ = 0.7 δ = 0.7 δ δ - 1.5B 7B 1.5B 7B 77.4 66.4 73.4 77.8 73.6 75.0 74.8 75.8 92.0 82.1 89.7 91.8 90.8 93.3 92.3 93.3 64. 56.9 60.5 63.1 64.2 66.2 65.2 66.2 38.8 28.7 32.7 39.1 39.0 39.9 40.0 40.9 General Model, Target and Draft: Llama-3.1-Instruct, PRM: Skyworko1-Open-PRM - 1B 1B 1B 1B 1B 1B 1B - - - maj@16 7B = 16 - δ = 0.7 δ = 0.7 δ δ - 1.5B 7B 1.5B 7B 49. 38.0 52.6 47.0 50.0 50.4 50.0 50.6 83.9 60.2 74.8 83.4 83.9 85.4 84.1 85.5 41.3 32.2 45.7 42.1 41.8 41.8 43.4 42.6 14. 9.5 14.4 16.6 15.7 18.1 18.1 18.1 32.8 27.3 27.3 28.8 38.4 33.8 38.4 34.3 42.4 30.8 27.8 35.9 36.4 41.4 42.9 42.9 43.9 42.9 28. 27.3 23.7 26.3 31.3 20.2 31.3 29.8 20.2 19.7 14.1 19.2 20.2 19.7 20.2 19.7 71.8 65.9 71.4 72.0 71.4 72.3 71.7 72.6 85. 66.1 72.4 77.2 75.4 85.3 84.7 84.4 85.2 85.6 57.4 67.2 69.4 56.2 71.6 61.8 71.7 66.3 39.1 24.9 31.0 38.5 37.2 36.2 38.8 38.7 65. 62.3 (-3.0) 64.8 (-0.5) 64.6 (-0.7) 65.9 (+0.6) 66.1 (+0.8) 66.3 (+1.0) 66.5 (+1.2) 71.8 64.0 (-7.8) 65.0 (-6.8) 69.8 (-2.0) 68.5 (-3.3) 71.1 (-0.7) 72.2 (+0.4) 72.7 (+0.9) 72.6 (+0.8) 72.9 (+1.1) 59.9 54.8 (-5.1) 58.2 (-1.7) 59.1 (-0.8) 61.8 (+1.9) 59.4 (-0.5) 62.6 (+2.7) 62.1 (+2.2) 41. 30.8 (-10.6) 38.8 (-2.6) 41.1 (-0.3) 41.5 (+0.1) 41.9 (+0.5) 42.4 (+1.0) 42.5 (+1.1) 2024), GaoKao-2023-En (Liao et al., 2024b), GPQA (Rein et al., 2023), and Minerva Math (Lewkowycz et al., 2022). number of speculative tokens as 7, method designed to accelerate inference (Leviathan et al., 2023). Baselines. We consider three categories of baselines: (1) Target model only: This baseline uses the target model independently, requiring more cost than RSD. (2) Draft model with or without PRM: This category includes popular test-time scaling methods that achieve the best possible performance using the draft model. Specifically, we consider majority voting, Best-of-N (BoN) (Brown et al., 2024; Cobbe et al., 2021a) that selects the highest-scoring response (last step) among candidates based on PRM, beam search (Chen et al., 2024a) that leverages PRM to choose the optimal decoding path, process Best-of-N that samples candidate steps and selects the one with the highest reward. For majotity voting and Best-of-N , we prefer large number of samplings (more cost than the target model only) to show their converged performance. (3) Speculative decoding (SD): We also include speculative decoding with Default Setting. All experiments were conducted on NVIDIA A100 GPUs, using vLLM (Kwon et al., 2023) as the backend. We use temperature = 0.7 and top = 0.8 for majority voting, (process) Best-of-N and beam search, while setting temperature = 0 and top = 1 for the remaining methods. For process Best-of-N , beam search and RSD, we define generation ended with nn as reasoning step, and then apply PRM to rate this step. We employ the binary step function (the second option in Table 1) as the weighting function and set δ = 0.7. For brevity, RSD (7B/72B/7B) denotes RSD with 7B draft model, 72B target model and 7B PRM. SD (7B/72B) deotes SD with 7B draft model and 72B target model. BoN (1.5B/1.5B) denotes BoN with 1.5B base model and 1.5B PRM. Without explicitly mentioning, models chosen from the Qwen-2.5-Math-Instruct family are used. Reward-Guided Speculative Decoding for Efficient LLM Reasoning Table 3. Comparison with search-based methods. Beam Search and Process Best-of-N use 1.5B base model and 1.5B PRM. Method Setting MATH 500 GSM8K Minerva Math Single Model (1.5B) Process Best-of-N Process Best-of-N Beam Search Beam Search RSD (1.5B/7B/1.5B) - =8 =16 bs=4 bs=8 δ=0.7 73.8 75.8 76.0 78.2 78.2 82.6 85.0 87.8 87.9 88.9 88.4 94.5 29.0 32.7 31.2 33.5 32.4 34. 3.1. Reasoning Benchmarks We evaluate RSD across diverse set of reasoning benchmarks, as summarized in Table 2, and observe: (1) Test-time scaling methods like majority voting and Best-of-N , which rely on extensive sampling with draft model, consistently underperform single target model on average. This finding highlights the importance of larger model for reasoning tasks, as its performance cannot be easily matched by smaller model with increased computation. (2) While SD is theoretically unbiased, guaranteeing accuracy equal to the target model, it often underperforms in practice. This discrepancy, as also noted by Chen et al. (2023a), arises due to floating-point errors. Moreover, in cases where draft model outperforms the target model (e.g., Table B.1 and domain-specialized draft models), SDs strict unbiasedness leads to worse performance compared to the draft model. Thus, the decision to use SD must account for such scenarios. In contrast, RSD mitigates this concern by leveraging PRM, which evaluates the quality of reasoning steps from the draft model. (3) Among all evaluated methods, RSD consistently outperforms the single target model on average when using an optimized δ. Even with fixed δ = 0.7, RSD achieves better results in 7 out of 8 settings. Notably, on the challenging GPQA benchmark, RSD (1.5B/7B/1.5B) significantly surpasses the single target model (38.4 vs. 32.8), demonstrating the effectiveness of this efficient approach. Additionally, larger PRM (7B) slightly enhances performance compared to smaller PRM (1.5B), especially on complex datasets like GPQA and MATH500, where the increased reasoning capacity of larger PRM proves beneficial. Results with general models, such as Qwen2.5-Instruct and Llama-3.1-Instruct, align with those from math-specific models, validating RSDs robustness and generalizability. 3.2. Comparison with Search-Based Methods We also compare our method with beam search (Chen et al., 2024a) and process Best-of-N in Table 3. RSD significantly outperforms both search-based methods over all three benchmarks. These results highlight critical insight: for certain complex or hard reasoning steps, search-based methods struggle to find optimal solutions due to the combinatorial explosion of potential candidates, leading to suboptimal Figure 4. Flops vs. accuracy on MATH500. Figure 5. The impact of threshold δ with RSD (1.5B/7B/7B). performance. On the other hand, our approach leverages larger models capacity to generate plausible solutions more directly, bypassing the need for exhaustive search. By utilizing the PRM as feedback mechanism, RSD benefits from step-wise guidance, which helps mitigate the challenges of reasoning in high-complexity tasks. This suggests that, rather than relying on purely search-based strategy, incorporating larger models and targeted feedback mechanisms can lead to more efficient and effective reasoning, especially in cases where the search space is vast or the reasoning steps are particularly intricate. 3.3. Computation Analysis To evaluate the computational efficiency of our method, we compare RSD with speculative decoding and Best-of-N on MATH500. Following (Kang et al., 2024; Sardana et al., 2023), we adopt the standard approximation of FLOPs for transformers with parameters, i.e. 2N per inference token. Note that the inference cost for PRMs is also included in the calculations. As shown in Fig. 4, RSD (1.5B/7B/7B) outperforms both SD (1.5B/7B) and Target (7B), achieving an accuracy improvement of 1.2 and 1.4, respectively, while using fewer FLOPs. Moreover, RSD (7B/72B/7B) achieves notable accuracy of 88.0 on MATH500, compared to 85.6 for Target (72B), with nearly 4.4 fewer FLOPs. When compared to BoN (7B/7B,N =64), RSD (7B/72B/7B) delivers 1.8 points higher accuracy at significantly lower computational cost. These results clearly demonstrate both efficiency and effectiveness of RSD. 7 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Table 4. Accuracy of model merge. denotes that we merge these two models. Refer to B.3 for detailed setting and number. Method Target Draft PRM Setting Avg. Accuracy Single Model 7B - - - 65.3 SD RSD RSD RSD RSD 7B 7B 7B 7B 7B 1.5B 1.5B 1.5B 1.5B 1.5B - 1.5B 7B 1.5B 7B - δ = 0.7 δ = 0.7 δ = 0.7 δ = 0. 64.6 (-0.7) 65.9 (+0.6) 66.1 (+0.8) 65.0 (-0.3) 66.7 (+1.4) 4. Discussion PRM Overheads and Model Merge. In MATH500, the average number of reasoning steps per question is 18, suggesting that the PRM is invoked 18 times per question, akin to generating 18 tokens. Furthermore, even tiny PRM (1.5B) outperforms the single target model in RSD accuracy (see Table 2). Therefore, adding an additional PRM for RSD incurs minimal overhead compared to SD. Here, we further investigate the possibility of merging models to enhance the usability of RSD by reducing the number of served models. As shown in Table 4, merging models does not necessarily degrade performance and remains superior to SD. Interestingly, merging larger models even results in performance improvements, consistent with observations reported by Yadav et al. (2024). Combine RSD with SD. RSD is not inherently opposed to SD; in fact, they can be seamlessly combined to enhance efficiency. For instance, during rejected step, SD (draft+target) can be utilized to regenerate the step. This approach allows for further optimization of RSDs efficiency without incurring additional costs. Specialized PRM. In our experiments, we rely on an opensource general PRM. However, training or fine-tuning specialized PRM that is closely aligned with the draft model could further enhance performance. Such PRM would better recognize high-quality reasoning steps generated by the draft model, thereby increasing the acceptance rate. Future work could explore specialized PRM training or fine-tuning, leveraging targeted datasets and chain-of-thought annotations. This more tightly coupled feedback loop is especially promising in applications like mathematics, coding, or scientific discovery, where domain knowledge and consistent reasoning styles can amplify the benefits of reward-guided speculative decoding. 5. Related Work Speculative Decoding. Speculative decoding (Stern et al., 2018; Leviathan et al., 2023; Xia et al., 2024; Chen et al., 2023a; Zhang et al., 2023a; Sun et al., 2024a; Chen et al., 2023b; Li et al., 2024b) achieves lossless acceleration by employing draft model to predict subsequent tokens and Figure 6. Accuracy of weighting functions from Table 1 with RSD (1.5B/7B/7B). All settings share similar inference cost. 3.4. Ablation Studies Threshold δ. Fig. 5 illustrates the relationship between the threshold δ, accuracy, and the proportion of questions solved solely by the draft model within RSD. As δ increases, accuracy improves, peaking at δ = 0.7, before experiencing slight decline. Notably, the accuracy remains consistently higher than that of using only the target model (δ = 1.0) once δ surpasses 0.6. higher δ corresponds to stricter rejection of reasoning steps generated by the draft model. However, even at δ = 0.7, the draft model alone can still solve 48% of questions, as the rewards for its reasoning steps on these questions are sufficiently high. This eliminates the need for the target model to engage with these 48% of questions, distinguishing RSD from SD, which always involves both the draft and target models for every question. This adaptability makes RSD an effective method for automatic compute allocation, allocating less compute (draft model only) to simpler questions and more compute (both draft and target models) to more challenging ones. For more detailed discussion of the automatic compute allocation for questions in different levels, refer to B.2. Additionally, δ serves as critical role in balancing the computational efficiency of the draft model with the precision of the target model, ultimately optimizing overall performance. Weighting Function. In Table 1, we present several candidate weighting functions. Here we evaluate their performance under comparable inference costs, as shown in Fig. 6. First, all candidates outperform the draft model alone, underscoring the critical role of incorporating larger model within the reasoning loop. Second, the constant weighting function, which does not utilize rewards, performs the worst, emphasizing the significance of PRM feedback. Lastly, the binary step function achieves the best performance, even surpassing the single target model. Additionally, it introduces δ as hyperparameter, allowing for flexible control over inference costs to accommodate varying budget constraints. Thus, we use the binary step function in this paper, and leave the exploration of other rejection schemes to future. 8 Reward-Guided Speculative Decoding for Efficient LLM Reasoning verify them in parallel. Tree-based speculation (Miao et al., 2024; Fu et al., 2024; Sun et al., 2024b; Chen et al., 2024b) extends this approach by generating multiple candidates to increase the acceptance rate. Self-speculative decoding (Elhoushi et al., 2024; Zhang et al., 2023a) leverages parts of the large language model (LLM) parameters as the draft model while using the original base model as the verifier. Parallel decoding (Stern et al., 2018; Cai et al., 2024) further enhances efficiency by introducing draft models to streamline the process. Unlike previous speculative decoding methods, our approach utilizes process rewards to perform stepwise speculative reasoning. Reward Models on Reasoning. Reward models play crucial role in selecting correct reasoning trajectories during both training (Chen et al., 2024a; Wang et al., 2024) and inference (Brown et al., 2024). Outcome Reward Models (ORMs) (Yu et al., 2023; Dong et al., 2024) are trained exclusively on the models final output, whereas Process Reward Models (PRMs) (Lightman et al., 2023) rely on step-level annotations, providing dense and granular reward signals at each reasoning step. Scaling test-time compute (Snell et al., 2024; OpenAI, 2024) has gained significant traction with the advancement of reward models. Techniques like Bestof-N (Brown et al., 2024; Cobbe et al., 2021a; Dong et al., 2023) leverage ORMs to select the sample with the highest reward from candidates. Building on this, tree search methods have been introduced, enabling per-step predictions rather than relying solely on the final answer (Chen et al., 2024a; Qi et al., 2024; Yao et al., 2024). These methods are enhanced by process feedback, such as Process Reward Models (PRMs). We propose novel application of PRMs to accelerate reasoning during inference. 6. Conclusion We propose Reward-Guided Speculative Decoding (RSD), novel framework that enhances LLM inference efficiency, particularly for reasoning-intensive tasks. RSD dynamically combines lightweight draft model with more capable target model, using reward function to guide output selection at each step. This approach balances computational cost and quality by selectively refining outputs based on process rewards. RSD achieves significant efficiency gains over SD and BoN while maintaining accuracy benchmarks. Extensive evaluations across reasoning tasks highlight RSDs robustness, adaptability, and effectiveness, making it practical solution for LLM deployment."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning by proposing Reward-Guided Speculative Decoding (RSD), framework aimed at improving the efficiency and scalability of large language model inference. The potential societal implications of this work are primarily positive, as RSD facilitates more energy-efficient and cost-effective use of computational resources, contributing to the sustainability of deploying large-scale AI systems. However, as with any advancement in machine learning, there are ethical considerations to be acknowledged. The increased efficiency of language models could lead to wider accessibility and adoption, which, while beneficial in many respects, may also exacerbate risks such as misuse for generating misinformation or biases in outputs. We strongly encourage researchers and practitioners to apply RSD responsibly and consider incorporating safeguards to mitigate these risks. Overall, we believe the contributions of this work align with the broader goal of advancing AI technologies in an ethical and sustainable manner, with no immediate negative societal consequences requiring further discussion."
        },
        {
            "title": "References",
            "content": "Anthropic, A. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3, 2024. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023a. Chen, G., Liao, M., Li, C., and Fan, K. Alphamath almost zero: process supervision without process. arXiv preprint arXiv:2405.03553, 2024a. Chen, Z., Yang, X., Lin, J., Sun, C., Chang, K. C.-C., and 9 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Huang, J. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462, 2023b. Chen, Z., May, A., Svirschevski, R., Huang, Y., Ryabinin, M., Jia, Z., and Chen, B. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024b. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021a. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., Shum, K., and Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Elhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B., Wasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal, S., Roman, A., et al. Layer skip: Enabling early exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. Goddard, C., Siriwardhana, S., Ehghaghi, M., Meyers, L., Karpukhin, V., Benedict, B., McQuade, M., and Solawetz, J. Arcees mergekit: toolkit for merging large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, pp. 477485. Association for Computational Linguistics, 2024. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In International Conference on Learning Representations (ICLR), 2020. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kang, J., Li, X. Z., Chen, X., Kazemi, A., Sun, Q., Chen, B., Li, D., He, X., He, Q., Wen, F., et al. Mindstar: Enhancing math reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265, 2024. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference In Interfrom transformers via speculative decoding. national Conference on Machine Learning, pp. 19274 19286. PMLR, 2023. 10 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., GurAri, G., and Misra, V. Solving quantitative reasoning problems with language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024a. Li, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024b. Liao, B. and Monz, C. 3-in-1: 2d rotary adaptation for efficient finetuning, efficient batching and composability. CoRR, abs/2409.00119, 2024. doi: 10.48550/ARXIV. 2409.00119. Liao, B., Herold, C., Khadivi, S., and Monz, C. Apiq: Finetuning of 2-bit quantized large language model. In AlOnaizan, Y., Bansal, M., and Chen, Y. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 2099621020. Association for Computational Linguistics, 2024a. Liao, M., Luo, W., Li, C., Wu, J., and Fan, K. Mario: Math reasoning with code interpreter outputa reproducible pipeline. arXiv preprint arXiv:2401.08190, 2024b. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Zhang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pp. 932949, 2024. 11 o1 Team, S. https:// Skywork-o1 open series. huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Learning to reason with llms, OpenAI. URL learning-to-reason-with-llms/. 2024. https://openai.com/index/ Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.- M., Rothchild, D., So, D., Texier, M., and Dean, J. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021. Qi, Z., Ma, M., Xu, J., Zhang, L. L., Yang, F., and Yang, M. Mutual reasoning makes smaller llms stronger problemsolvers. arXiv preprint arXiv:2408.06195, 2024. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Sardana, N., Portes, J., Doubov, S., and Frankle, J. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448, 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024a. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., and Yu, F. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36, 2024b. Tang, Z., Zhang, X., Wang, B., and Wei, F. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., Reward-Guided Speculative Decoding for Efficient LLM Reasoning Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023b. Zheng, C., Zhang, Z., Zhang, B., Lin, R., Lu, K., Yu, B., Liu, D., Zhou, J., and Lin, J. Processbench: Identifying process errors in mathematical reasoning. CoRR, doi: 10.48550/ARXIV.2412. abs/2412.06559, 2024. 06559. et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 59986008, 2017. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024. Xia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T., Li, W., and Sui, Z. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024. Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X., and Tian, Q. Qa-lora: Quantizationaware low-rank adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023. Xu, Y., Jie, Z., Dong, H., Wang, L., Lu, X., Zhou, A., Saha, A., Xiong, C., and Sahoo, D. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018, 2024. Yadav, P., Vu, T., Lai, J., Chronopoulou, A., Faruqui, M., Bansal, M., and Munkhdalai, T. What matters for model merging at scale? CoRR, abs/2410.03617, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024b. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Yu, F., Gao, A., and Wang, B. Outcome-supervised verifiers for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023. Zhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G., and Mehrotra, S. Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168, 2023a. 12 Reward-Guided Speculative Decoding for Efficient LLM Reasoning"
        },
        {
            "title": "Code Availability",
            "content": "The code is available upon request and can be shared privately prior to its public release. The public release will take place after the completion of the necessary compliance procedures. A. Proof A.1. Proof of Proposition 2.1 Proof. For simplicity, we assume that = yz. The final distribution PRSD(y) combines contributions from two sampling paths: Accepted samples from Pm. Fallback samples from PM after rejection. According to Law of Total Probability, PRSD(y) = P(accept from Pm) (cid:125) (cid:124) (cid:123)(cid:122) Term 1 + P(reject Pm and draw from PM ) (cid:123)(cid:122) (cid:125) Term (cid:124) . For accepted samples from Pm, the acceptance probability for Pm is ω(r(y)): Term 1 = Pm(y) ω(r(y)). For samples from PM , the rejection probability from Pm is ν, where: After rejection, is drawn from PM :"
        },
        {
            "title": "By combining two terms",
            "content": "where ν + EyPm [ω(r(y))] = 1. ν = 1 EyPm [ω(r(y))] Term 2 = ν PM (y). PRSD(y) = Pm(y)ω(r(y)) + νPM (y), A.2. Proof of Proposition 2.2 Proof. We aim to prove that under the conditions: 1. ω(r) is non-decreasing in r, 2. EPM [r(yz)] EPm[r(yz)], the expectation of r(yz) under PRSD satisfies: EPRSD[r(yz)] EPm[r(yz)]. By definition: EPRSD [r(yz)] = (cid:90) (ω(r(yz))Pm(yz) + νPM (yz)) r(yz)dy where ν = 1 EPm[ω(r(yz))]. Substituting ν: EPRSD [r(yz)] = EPm [ω(r(yz))r(yz)] (cid:125) (cid:124) (cid:123)(cid:122) Term + (1 EPm[ω(r(yz))])EPM [r(yz)] (cid:123)(cid:122) (cid:125) Term 2 (cid:124) . 13 Reward-Guided Speculative Decoding for Efficient LLM Reasoning The condition of EPRSD [r(yz)] EPm[r(yz)] 0 can be rewritten as EPm [ω(r)r] + (1 EPm [ω(r)])EPM [r] EPm[r] 0. Note that: EPm [ω(r)r] = CovPm(ω(r), r) + EPm [ω(r)] EPm[r]. Substitute this into the inequality: CovPm (ω(r), r) + EPm [ω(r)]EPm[r] + (1 EPm[ω(r)])EPM [r] EPm [r] 0. Simplify the terms involving EPm[r]: CovPm(ω(r), r) + (1 EPm [ω(r)]) (EPM [r] EPm [r]) 0. 1. Covariance Term (CovPm(ω(r), r)): Since ω(r) is non-decreasing in r, higher values of correspond to higher values of ω(r). This implies CovPm(ω(r), r) 0. 2. Expectation Difference Term ((1 EPm [ω(r)]) (EPM [r] EPm[r])): 1 EPm [ω(r)] = ν 0 (since ν is normalizing constant for valid probability distribution). By the second condition, EPM [r] EPm [r] 0. Thus, this term is non-negative. Both terms in the inequality: CovPm(ω(r), r) 0 and (1 EPm[ω(r)]) (EPM [r] EPm[r]) 0 are non-negative under the given conditions. Therefore: EPRSD[r(yz)] EPm[r(yz)]. The conditions ω(r) is non-decreasing in and EPM [r(yz)] EPm [r(yz)] are sufficient to guarantee EPRSD [r(yz)] EPm[r(yz)]. A.3. Proof of Proposition 2.3 Proof. For simplicity, we assume that = yz. Our optimization problem is subject to the inequality constraint: L(ωr) = EyPm ωr(y)r(y) + EyPM νr(y) ν = 1 EyPm[ωr(y)] γ, 0 ω(y) 1. Equivalently, The Lagrangian1 is given by L(ωr) = EPm [ωr(y) r(y)] + (cid:0)1 EPM [ωr(y)](cid:1) EPM [r(y)] L(ωr, λ) = (cid:90) (cid:104) (cid:105) (Pm(y)r(y))ωr(y) + (PM (y)r(y))(1 Ey[ωr(y)]) (cid:20) dy + λ (1 γ) (cid:90) (cid:21) Pm(y)ωr(y)dy . The Lagrangian derivative yields: ωr(y) = Pm(y) [r(y) (λ + R)] , 1Here, the integral can also be defined for counting measure space, allowing it to be applied to the discrete case. 14 Reward-Guided Speculative Decoding for Efficient LLM Reasoning where = EyPM r(y). Setting this to zero gives the threshold rule: the optimal ω (y) is: ω (y) = (cid:40) 1, 0, if r(y) (λ + R) 0, if r(y) (λ + R) < 0. KKT Conditions. Primal feasibility: EyPm [ωr(y)] 1 γ. Dual feasibility: λ 0. Stationarity: The first-order condition holds. Complementary slackness: λ (cid:0)(1 γ) (cid:82) Pm(y)ωr(y)dy(cid:1) = 0. Conclusion. If EyPm[ω (y)] = 1 γ, then λ 0 and the solution is tight, as in the equality case. If EyPm[ω (y)] > 1 γ, then λ = 0, and the constraint is satisfied with slack. The solution is threshold-based function on r(y), where ω (y) = 1 for larger r(y) and ω (y) = 0 for smaller r(y), adjusted to ensure that the constraint EyPm [ω (y)] 1 γ is satisfied. Thus, The optimal sampling strategy involves using Pm (i.e., ωr(y) = 1) for the higher values of r(y) and using PM (i.e., ωr(y) = 0) for the lower values of r(y). The threshold on r(y) is selected such that the constraint EyPm [ωr(y)] 1 γ is met. Threshold-based strategy: ω (y) = 1{r(y)t}, where is chosen to satisfy the constraint EyPm[ωr(y)] 1 γ. B. Additional Empirical Results Table B.1. Accuracy on CN Middle School 24 (Yang et al., 2024b) and College Math (Tang et al., 2024). Due to SDs strict unbiasedness, it achieves worse accuracy than the draft model if the draft model outperforms the target model, while RSD doesnt have this issue."
        },
        {
            "title": "PRM Setting",
            "content": "CN Middle School 24 College Math"
        },
        {
            "title": "Single Model\nSingle Model\nSD\nRSD",
            "content": "- 7B 7B 7B 1.5B - 1.5B 1.5B - - - - - - 7B δ = 0.7 75.2 72.3 73.3 78.2 48.0 46.8 46.9 48. B.1. Tuning of δ RSD employs threshold, δ, to decide whether to accept reasoning step generated by the draft model. If the reward score of reasoning step exceeds δ, it is accepted. However, reasoning tasks vary in complexity, leading to diverse reward distributions. Using fixed δ may not yield optimal accuracy across different tasks. The results for varying δ values are presented in Table B.2. Overall, δ = 0.7 emerges as reliable choice across various settings. Slight adjustments within the range [0.6, 0.7, 0.8, 0.9] can further improve performance. 15 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Table B.2. Accuracy with different δs. Overall, δ = 0.7 works well for different models and tasks. However, since the complexity of different tasks varies, slight tuning of δ offers better accuracy. Method Target Draft Model Model PRM Setting MATH500 GSM8K GaoKao 2023 En Olympiad Bench GPQA Diamond MMLU STEM Avg. Math Model, Target and Draft: Qwen2.5-Math-Instruct, PRM: Skyworko1-Open-PRM RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD RSD 7B 7B 7B 7B 7B 7B 7B 7B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 72B 7B 7B 7B 7B 7B 7B 7B 7B 8B 8B 8B 8B 8B 8B 8B 8B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 7B 7B 7B 7B 7B 7B 7B 7B 1.5B 1.5B 1.5B 1.5B 7B 7B 7B 7B 1.5B 1.5B 1.5B 1.5B 7B 7B 7B 7B 1.5B 1.5B 1.5B 1.5B 7B 7B 7B 7B δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 δ = 0.6 δ = 0.7 δ = 0.8 δ = 0. δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 80.4 82.6 82.6 81.2 83.6 84.6 83.6 83.4 83.0 83.6 86.6 85.4 85.8 86.8 87.4 86.2 85.0 86.4 86.6 86. 88.0 88.0 87.4 86.0 93.3 94.5 95.3 95.5 95.4 95.5 95.8 95.7 93.5 94.7 95.6 95.6 96.0 96.3 96.3 96.0 97.0 96.4 96.6 95. 96.6 96.7 96.9 96.2 67.8 68.8 68.1 68.3 68.3 68.3 67.8 68.1 71.9 72.7 71.4 72.2 72.5 72.7 73.0 72.7 72.2 73.0 73.2 71. 72.5 74.0 73.5 73.5 40.6 39.6 39.4 39.4 43.6 42.1 40.9 40.1 48.4 47.7 48.0 47.7 49.0 49.0 48.3 48.9 48.4 49.8 48.7 48. 49.6 49.9 48.3 48.3 General Model, Target and Draft: Qwen2.5-Instruct, PRM: Skyworko1-Open-PRM 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 1.5B 7B 7B 7B 7B δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 72.8 73.6 74.8 74.6 75.4 75.0 74.2 75.8 89.7 90.8 91.6 92.3 92.6 93.3 92.4 92. 63.6 64.2 64.4 65.2 64.9 66.2 62.9 65.2 38.5 39.0 38.8 40.0 37.3 39.9 40.6 40.9 General Model, Target and Draft: Llama-3.1-Instruct, PRM: Skyworko1-Open-PRM 1B 1B 1B 1B 1B 1B 1B 1B 1.5B 1.5B 1.5B 1.5B 7B 7B 7B 7B δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 δ = 0.6 δ = 0.7 δ = 0.8 δ = 0.9 49.0 50.0 48.6 50. 50.4 50.4 50.6 50.0 82.6 83.9 84.1 84.0 85.5 85.4 84.2 83.5 40.8 41.8 41.8 43.4 42.6 41.8 41.3 42.3 18.1 15.7 16.3 15. 16.9 18.1 16.4 16.1 32.3 38.4 37.4 32.3 30.3 33.8 34.3 32.8 36.9 40.4 40.9 44.4 41.9 41.9 40.9 41.4 43.9 42.9 43.9 41. 40.4 42.9 41.9 42.4 22.2 31.3 23.7 28.3 29.8 20.2 21.7 26.8 19.2 20.2 18.7 17.7 18.2 19.7 18.2 18.2 69.1 71.4 71.7 71. 71.6 72.3 72.6 72.5 78.1 82.5 85.1 85.5 82.5 84.6 85.4 85.6 82.2 84.7 85.2 85.1 82.5 84.4 85.6 85.4 71.7 71.6 67.1 59. 66.3 61.8 58.2 56.1 34.5 37.2 38.8 38.6 34.9 36.2 37.9 38.7 63.9 65.9 65.8 64.7 65.5 66.1 65.8 65.4 68.6 70.3 71.3 71. 71.3 71.9 71.9 71.8 71.5 72.2 72.4 71.4 71.6 72.7 72.3 72.0 59.8 61.8 60.1 60.0 61.1 59.4 58.3 59.5 40.7 41.5 41.4 41. 41.4 41.9 41.4 41.5 B.2. Different Reasoning Complexity Thanks to the human annotated complexity levels in MATH500 (5 levels, the higher the harder), here we investigate how RSD works for questions in different complexity. As shown in Fig. B.1, the involvement of the target model (δ = 0) consistently improves the accuracy compared with the draft model only (δ = 0). The improvement varies for different levels, at most +4.7 for level 1, +5.6 for level 2, +6.7 for level 3, +16.4 for level 4 and +15.7 for level 5, showing the importance of the target model for harder questions. For the same δ, one can also observe that the proportion of questions solved by the draft model alone decreases with an increasing level. For example at δ = 0.7, draft model alone solves 84% questions in level 1, 67% questions in level 2, 58% questions in level 3, 44% questions in level 4 and 19% questions in level 5. It shows that harder questions need more involvement of the target model. In this way, RSD can be considered as method for automatic compute allocation, less compute for easy questions and more compute for hard questions, which is different from SD that always needs both target and draft models for every question. 16 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Figure B.1. The behaviour of RSD (1.5B/7B/7B) for different δs and questions in different complexity levels (the higher the level, the harder the question.). δ = 0 and δ = 1 denotes all questions are solved by the draft model alone and the target model only, respectively. Overall, the involvement of the target model improves the accuracy. The improvement is more obvious for harder question, +16 for level 4 and 5. In addition, with an increasing level, the questions solved by the draft model only decrease for the same δ, demonstrating harder questions need more involvement of the target model. Table B.3. Accuracy of model merge. denotes that we merge these two models."
        },
        {
            "title": "Setting",
            "content": "MATH500 GSM8K GaoKao 2023 En"
        },
        {
            "title": "MMLU\nSTEM",
            "content": "Avg. Math Model, Target and Draft: Qwen2.5-Math-Instruct, PRM: Skyworko1-Open-PRM Single Model 7B - - - SD RSD RSD RSD RSD 7B 7B 7B 7B 7B 1.5B 1.5B 1.5B 1.5B 1.5B - 1.5B 7B 1.5B 7B - δ = 0.7 δ = 0.7 δ = 0.7 δ = 0. 83.2 83.4 82.6 84.6 83.4 84.0 95.7 95.6 94.5 95.5 95.1 95.6 66.8 67.3 68.8 68.3 67.3 69. 41.2 40.6 39.6 42.1 39.3 43.0 32.8 28.8 38.4 33.8 33.3 35.4 71.8 72.0 71.4 72.3 71.6 72. 65.3 64.6 (-0.7) 65.9 (+0.6) 66.1 (+0.8) 65.0 (-0.3) 66.7 (+1.4) B.3. Model Merge To reduce the number of models required for facilitating RSDs usage, we consider merging either the target model with the PRM or the draft model with the PRM. Here, we focus on the simplest merging strategylinear mergingusing MergeKit (Goddard et al., 2024), leaving the exploration of more advanced merging methods for future work. The PRM and the policy model have different architectures. Specifically, the PRM includes projection layer atop the final transformer layer (Vaswani et al., 2017), which projects the hidden dimension to scalar output, whereas the policy model employs an lm head. We merge only the shared layers, retaining the PRMs projection layer and the policy models lm head. For interpolation weights in the linear merging process, we tested only [0.6, 0.4] and [0.5, 0.5], with the target or draft model receiving 0.6 and the PRM 0.4. The [0.6, 0.4] configuration performed slightly better. As shown in Table B.3, the results indicate the following: (1) Overall, the merged model outperforms SD; (2) Merging improves performance more substantially in larger models (+1.4 vs. +0.8). This observation aligns with the findings of Yadav et al. (2024). 17 Reward-Guided Speculative Decoding for Efficient LLM Reasoning Figure B.2. Left: comparison of the reward scores for all questions generated by the draft model and the target model within the RSD framework. Middle: focused comparison of the reward scores for correctly answered questions generated by the draft model and the target model in the RSD framework. Right: The winning rate comparison between the draft model and the target model, highlighting the proportion of cases where each model outperforms the other in the RSD framework. RSD is configured with Qwen2.5-Math-1.5B-Instruct as the draft model, Qwen2.5-Math-72B-Instruct as the target model, and Skywork-o1-Open-PRM-7B as the PRM."
        }
    ],
    "affiliations": [
        "Language Technology Lab, University of Amsterdam",
        "Salesforce AI Research"
    ]
}