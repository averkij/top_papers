{
    "paper_title": "ViDiC: Video Difference Captioning",
    "authors": [
        "Jiangtao Wu",
        "Shihao Li",
        "Zhaozhou Bian",
        "Jialu Chen",
        "Runzhe Wen",
        "An Ping",
        "Yiwen He",
        "Jiakai Wang",
        "Yuanxing Zhang",
        "Jiaheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence."
        },
        {
            "title": "Start",
            "content": "2025-12-05 ViDiC: Video Difference Captioning Jiangtao Wu1, Shihao Li1, Zhaozhou Bian1, Jialu Chen2, Runzhe Wen1, An Ping1, Yiwen He1, Jiakai Wang2, Yuanxing Zhang2,, Jiaheng Liu1, 1 NJU-LINK Team, Nanjing University jiangtaowu@smail.nju.edu.cn 2 Kling Team, Kuaishou Technology liujiaheng@nju.edu.cn Abstract Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changesa capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1Ka dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be challenging benchmark that lays solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence. ahttps://vidic-1k.github.io/ 5 2 0 2 4 ] . [ 2 5 0 4 3 0 . 2 1 5 2 : r Figure 1: Illustration of the seven categories of video-pair variations in our study: Subject, Style, Playback Technique, Camera, Position, Motion, and Background. Using Background Change as an exemplar, we showcase our Video Difference Captioning task, where model generates caption detailing similarities and differences. The captions accuracy is then assessed against fine-grained checklist. * Equal Contribution. Corresponding Author."
        },
        {
            "title": "1 Introduction",
            "content": "Understanding and describing differences between visual inputs is fundamental capability of human perception and cornerstone of visual reasoning. While recent progress in image difference captioning (IDC) (Jhamtani and Berg-Kirkpatrick, 2018; Park et al., 2019; Yao et al., 2022; Di et al., 2025; Liu et al., 2025; Anonymous, 2025) has enabled models to articulate semantic changes between pairs of static images, these methods remain inherently limited: they operate on snapshots, ignoring the temporal evolution and motion cues that define real-world visual experiences. In dynamic scenes, differences are not only found in static frames but also emerge over time emerging from variations in actions, events, camera movements, or stylistic transitions across time. To bridge this gap, as shown in Figure 1, we introduce Video Difference Captioning (ViDiC), new task that extends difference captioning into the video domain. Specifically, the ViDiC task requires models to generate natural language descriptions that accurately capture differences in both their static visual content and their temporal dynamics between two video clips while maintaining coherence and factual grounding. This formulation moves beyond traditional video similarity (Zeng et al., 2022; Liberatori et al., 2025) or video editing evaluation (Wu et al., 2023a; Argaw et al., 2022; Ju et al., 2025) tasks, focusing instead on edit understanding rather than edit execution. However, constructing such benchmark for the ViDiC task presents several challenges. First, video annotation is costly and ambiguous: differences may arise from subtle temporal cues or stylistic variations not easily expressible in simple labels. Second, existing video editing datasets emphasize task completion metrics (e.g., edit fidelity) (Chen et al., 2025a; Sun et al., 2024; Li et al., 2025a; Pan et al., 2025; Chen et al., 2025b), which fail to capture descriptive capabilities. Finally, scalable benchmarking requires standardized evaluation protocols to ensure consistency across diverse models and data sources. To address these issues, we introduce dual-checklist evaluation framework and high-quality video pair dataset called ViDiC-1K, which is designed explicitly for video difference captioning. Our dataset comprises 1,000 curated video pairs (both real and synthetic), annotated with over 4,000 fine-grained comparative questions spanning seven semantic dimensions: Subject, Style, Background, Camera Work, Motion, Position, and Playback Technique. Each pair is accompanied by both similarity and difference checklists, enabling detailed, interpretable assessment that transcends single-score metrics. Beyond data design, we propose an LLM-assisted evaluation protocol where large judge model (e.g., GPT-5-Mini) quantifies factual accuracy by comparing generated captions against human-verified ground truths. This scalable, model-agnostic evaluation paradigm ensures reliable comparison across systems without requiring direct visual access during judgment. In summary, our contributions are threefold: We introduce the Video Difference Captioning task by unifying descriptive, comparative, and temporal understanding, which generalizes image-level difference captioning into the temporal domain and establishes foundation for advancing multimodal models toward more robust and explainable video reasoning. To evaluate the capabilities of existing MLLMs, we first propose the ViDiC-1K benchmark, comprising 1,000 annotated video pairs with structured similaritydifference checklists across seven spatio-temporal dimensions, and introduce scalable evaluation framework leveraging LLM-as-a-judge for factual, interpretable, and reproducible benchmarking of MLLMs. Through extensive experiments on existing models, we demonstrate that ViDiC exposes crucial performance gaps in fine-grained temporal reasoning and edit interpretation, and reveals domainspecific weaknesses that remain unsolved even for leading MLLMs."
        },
        {
            "title": "2 Related Works",
            "content": "Visual Captioning. The current generation of large-scale multimodal models demonstrates remarkable proficiency, validated across diverse benchmarksfrom canonical captioning (MS-COCO (Lin et al., 2014), NoCaps (Agrawal et al., 2019)) and visual question answering (GQA (Hudson and Manning, 2019)) to fine-grained or knowledge-based tasks (OK-VQA (Marino et al., 2019), TextVQA (Singh et al., 2019)) and holistic reasoning suites (MMBench (Liu et al., 2024a), SEED-Bench (Li et al., 2024a)). However, these models remain constrained to single visual input, limiting their ability to perform comparative understanding (Peng et al., 2025), such as finding the differences between two videos or images. This gap highlights the urgent need for new evaluation frameworks designed specifically for these comparative reasoning tasks. Image Difference Captioning. Image Difference Captioning task focuses on describing semantic changes between two images. Early studies, such as Spot-the-Diff (Jhamtani and Berg-Kirkpatrick, 2018) intro2 duced datasets for learning to verbalize visual differences. Recent works attempt on scalable synthetic data generation and preference-based selection (Wu et al., 2025; Ju et al., 2025; Dunlap et al., 2024). Despite these advances, IDC methods rely on static image pairs and thus fail to capture temporal dynamics or motion consistency. In contrast, ViDiC-1K extends the task into the temporal domain by introducing benchmark for video difference captioning, where models must reason over both spatial and temporal variations between two video clips. Compared to IDC, this task requires understanding event evolution and motion patterns over time, providing more comprehensive evaluation framework for video understanding and video editing models. Video Editing Datasets. Constructing high-quality datasets for video editing is challenging due to the high annotation cost and the need to preserve spatio-temporal consistency. Early attempts produced task-specific suites and competitions to bootstrap progressfor example, the CVPR 2023 Text-Guided Video Editing (TGVE) competition (Wu et al., 2023b) and its dataset showcased practical evaluation protocols for text-driven edits (Wu et al., 2023a). Subsequent efforts, including Anatomy of Video Editing, expanded the scope to broader set of AI-assisted tasks such as reframing, rotoscoping, and color grading (Argaw et al., 2022). To address data scarcity at scale, recent work has adopted large-scale synthetic curation and unification across modalities: unified frameworks construct massive instructionedit pairs (images + videos) and release evaluation suites, demonstrating that scalable pipelines can produce hundreds of thousands of edit samples for joint training and evaluation (Ju et al., 2025). However, most of them emphasize direct edit fidelity rather than the models ability to describe and reason about editing differences. In contrast, ViDiC-1K shifts the focus from edit execution to edit understanding, systematically evaluating how well models capture and verbalize fine-grained semantic differences between video pairs."
        },
        {
            "title": "3 ViDiC-1K",
            "content": "To evaluate video comparison for editing, we introduce the ViDiC-1K benchmark, built on new framework derived from editing workflows. The framework organizes comparison criteria into seven super-categories (Figure 1) and uses dual-checklist design assessing both similarities and differences for each video pair. This enables granular evaluation that overcomes the limitations of single, coarsegrained similarity scores."
        },
        {
            "title": "3.1 Data Collection",
            "content": "3.1.1 Video Collection To establish benchmark with broad coverage, we constructed ViDiC-1K with 1,000 video pairs by aggregating data from existing public sources while also generating videos via our proprietary pipeline.The approximate proportions of specific data sources are shown in Figure 4f. Externally Sourced Video Collection: The external data was sourced from two primary channels : public academic datasets (VidDiffBench (Burgess et al., 2025) IF-Vidcap (Li et al., 2025b), VACE (Jiang et al., 2025) PKU-DyMVHumans (Zheng et al., 2024) ToCaDa (Malon et al., 2018) and VSC (Yokoo et al., 2024)) and web platforms (YouTube, and LMArena (Zheng et al., 2023)). For the IF-Vidcap dataset and YouTube subset, we employ temporal bisection strategy, selecting continuous long takes and dividing each into two consecutive segments of equal duration. To maintain high standard of data quality, all sourced videos were uniformly filtered to remove duplicates, videos with negligible motion, and those exhibiting excessively large inter-video differences. Controlled Synthetic Generation via Frame Splicing: To generate video pairs with subtle differences for fine-grained comparison, we designed novel, model-agnostic synthetic pipeline, illustrated in Figure 2. The core idea is to stack boundary frames, use video generation model(we use Veo3 (Wiedemer et al., 2025)) to synthesize composite video, and then split it. This approach grants us precise control over variations. CV and Rendering-Based Video Augmentation: As shown in Figure 3, we further augment our dataset using pipeline of computer vision and rendering tools for targeted modifications. This approach includes: (1) altering camera perspectives with ReCamMaster(Bai et al., 2025a); (2) modifying artistic styles via stylization tools (Ye et al., 2025); (3) adding or removing subjects using SAM-2 (Ravi et al., 2024) with inpainting; and (4) re-animating subject actions in rendering engines like Unreal Engine. Figure 2: Synthetic Generation via Frame Splicing. Figure 3: Edit via CV and Rendering tools. (a) Statistical distribution across various categories including Subject, Style, Background, Camera, Motion, Position, and Playback Technique. (c) Distribution of the number of checklist items per video pair. (b) Hierarchical classification of video content. (d) Distribution of video durations. (e) Distribution of video resolutions. (f) Distribution of video source datasets. Figure 4: An overview of the statistical analysis of our dataset across multiple dimensions. 3.1.2 Annotation Pipeline Our annotation pipeline employs two-stage process that combines automated generation with expert validation to ensure data quality. Stage 1: Automated Draft Generation. For each video pair sample, we first utilize Qwen3-VL-plus (Yang et al., 2025) to generate detailed analytical description, focusing on the key differences and similarities between the videos. Subsequently, we use Gemini-2.5-Pro(Team, 2025a) to systematically process this analysis, treating it as ground truth for the video comparison, to create draft checklist. Stage 2: Human Validation. team of six trained professional annotators meticulously refined the draft checklists. Each list was independently reviewed and corrected by two annotators based on unified criteria, targeting issues such as factual errors, logical contradictions, misclassifications, or excessive subjectivity. Any disagreements were resolved through consensus-driven discussion mediated by third senior annotator. This rigorous, multi-annotator protocol resulted in only 16.32% of the initial model-generated items being retained verbatim; the remainder were either substantially revised or discarded entirely. This process ensures that every item in the final checklists is factually accurate, consistent, and precisely aligned with human judgment. 4 Table 1: Comparison of benchmarks for image and video difference captioning and related tasks. We compare our ViDiC-1K with existing image difference captioning datasets, including Spot-the-Diff (Jhamtani and Berg-Kirkpatrick, 2018), CLEVR-Change (Park et al., 2019), OmniDiff (Liu et al., 2025), and ViDi (Anonymous, 2025), as well as the video action differencing benchmark VidDiffBench (Burgess et al., 2025). Syn. and Real denote synthetic and real-world data sources, respectively. The size denotes the number of samples in the test set. Reference-based evaluation is metric that measures model output quality by comparison with pre-defined reference answers (e.g., BLEU, CIDEr). Benchmark Source Task Category Count Size Evaluation Spot-the-Diff CLEVR-Change OmniDiff ViDi VidDiffBench Real Syn. Real and Syn. Real Real Image Difference Captioning Image Difference Captioning Image Difference Captioning Image Difference Captioning Video Action Differencing ViDiC-1K (Ours) Real and Syn. Video Difference Captioning 1 5 12 5 5 35 1,400 7,970 1,560 200 549 Reference-based Reference-based Reference-based Reference-based Checklist + LLM 1,000 Checklist + LLM"
        },
        {
            "title": "3.2 Dataset Statistics",
            "content": "3.2.1 Overall Statistics Our benchmark comprises 1,000 video pairs annotated with 4,107 comparative checklist items(1056 similarity, 3051 difference). Figure 4c illustrates the number of video pairs corresponding to each checklist length. The source videos are curated for diversity, with durations predominantly ranging from 2 to 12 seconds, reflecting the typical length observed in modern video editing.(Figure 4d), varied set of resolutions (Figure 4e), and broad spectrum of topics to ensure generalizability (Figure 4b). Our comparative checklist items are categorized according to multifaceted taxonomy, depicted in Figure 4a. This system encompasses seven key dimensions: 1) Subject, covering its type, count, and detailed attributes from appearance to pose; 2) Style, which utilizes constrained list of objective descriptors (e.g., Anime, Oil Painting); to enforce objectivity, videos with styles outside this predefined set were manually excluded; 3) Background, describing the scenes location, atmosphere, and lighting; 4) Camera Work, for analyzing cinematographic elements like camera movement and shot scale; 5) Subject Motion, detailing the dynamics of actions and interactions; 6) Positional Relationship, focusing on the spatial arrangement of subjects and objects; and 7) Playback Technique, which identifies simple editing effects like slow-motion or reverse. This structured approach ensures comprehensive and fine-grained comparative analysis. 3.2.2 Comparison with Other Benchmarks Current benchmarks for visual comparison suffer from critical fragmentation, focusing either on static images or on isolated tasks within the video domain. comprehensive overview in Table 1 highlights this gap. To bridge this divide, we introduce ViDiC-1K, the first unified benchmark that jointly evaluates both difference detection and similarity analysis in video across wide spectrum of comparative scenarios, from fine-grained subject and background alterations to sophisticated variations in camera work and playback techniques. This integrated and comprehensive approach enables more rigorous assessment of models spatio-temporal understanding, foundational capability for advancing intelligent video editing."
        },
        {
            "title": "3.3 Evaluation Methodology",
            "content": "3.3.1 Evaluation Framework Traditional metrics fall short of the evaluation for complex descriptive tasks, as they measure textual similarity rather than factual correctness. To overcome this, we propose framework to directly quantify factual accuracy using human-annotated checklist. This checklist is composed of set of binary (yes/no) questions, denoted as Q, derived from predefined evaluation dimensions. Each question has corresponding ground-truth answer, forming ground-truth answer set, AGT. During evaluation, the model under review, M, is prompted with given video pair and the evaluation dimensions to generate description, D. Subsequently, powerful and separate Judge model, (we use GPT-5-Mini), must answer the questions in based solely on the information present in D, without access to the videos. This process yields the Judges answer set, AJ . The factual accuracy of the description is then determined by the consistency between AJ and AGT, providing direct and reliable measure of the models ability to articulate facts. 5 3.3.2 Evaluation Metric We evaluate fine-grained video comparison using Accuracy over set of questions Q: Accuracy = 1 Q i=1 I(cid:0)AJ ,i = AGT,i (cid:1) (1) Here, I() is an indicator function that equals 1 if the models answer for question exactly matches the ground-truth answer, and 0 otherwise. Given the distinct design objectives of similarity and difference questions, we adopt tailored evaluation strategies for each type. Similarity Questions To penalize hallucination over omission (since enumerating all shared attributes in similar video pairs is impractical), questions about similarities are framed inversely (See Figure 1 for an example.). response is considered correct if it either confirms the similarity or omits the attribute, thereby only penalizing hallucinated differences. Difference Questions Conversely, to enforce descriptive accuracy, Difference questions are framed as verifiable propositions about specific differences (See Figure 1 for an example). The model must correctly affirm these true statements, with any failure to verify or omission of the specified details being penalized."
        },
        {
            "title": "4.1 Main Results",
            "content": "We evaluate 19 popular models including Gemini-2.5-Pro/Flash (Team, 2025a), GPT-5/4o (OpenAI, 2024), InternVL3.5 (Wang et al., 2025a), Qwen2.5-VL (Bai et al., 2025b), Qwen3-VL (Yang et al., 2025), Keye-VL-1.5 (Team, 2025b), Llama3 (Grattafiori et al., 2024), Mimo-VL-SFT (Xiaomi, 2025), Kimi-VL-A3B (Team et al., 2025a), GLM-4.1V (Team et al., 2025b), InternVideo2.5 (Wang et al., 2025b) and LLaVAv1.6-Vicuna (Liu et al., 2023). The main results are presented in Table 2, which lead to the following key observations: 1. Model Performance Gap.The scale of our dataset is sufficient to reveal clear performance hierarchy among models. While proprietary models still lead, open-source models like Qwen3-VL-32B are now outperforming some closed-source rivals, demonstrating rapid progress. Additionally, performance consistently scales with model size within given model family. 2. Semantic Dimension Variations. Models excel at Style recognition and perform reasonably on Subject, Motion, Position, and Background. However, Camera and Playback technique detection remain particularly weak, especially for open-source models, indicating critical limitations in temporal artifact identification. 3. Similarity and Difference Trade-off. High Similarity scores indicate low hallucination, but low Difference scores reveal weak fine-grained perception. GPT-4o achieves 81.12% on Similarity but only 39.14% on Difference, capturing coarse distinctions while missing subtle details. Balancing both remains critical challenge. 4. Thinking Mode Impact. Thinking mode improves Difference scores but degrades Similarity performance, revealing enhanced fine-grained perception at the cost of increased hallucinations on identical content. 5. Incompatibility with Dual-Video Inputs. Furthermore, predecessor models like LLaVA-v1.6Vicuna-7B exhibited pathological behaviors on dual-video inputs, such as generating repetitive, non-terminating text."
        },
        {
            "title": "4.2 Further Analysis",
            "content": "Judge Consistency Analysis. To select the most suitable LLM as an automated judge, we conducted humanmodel inter-rater reliability analysis. To balance efficiency and reliability, we randomly sampled 750 video pairs, which account for 75% of our dataset, to compute the consistency rate. For which subset of responses was randomly sampled from multiple models, including Gemini-2.5-pro (Team, 2025a), Qwen2.5-VL (Bai et al., 2025b), GPT-4o (OpenAI, 2024) and independently assessed by both human annotators (serving as the baseline) and three LLMs: GPT-5 Mini (OpenAI, 2025), DeepSeek-V3 (Liu et al., 2024b), and Qwen3-32B (Yang et al., 2025). The concordance rates, which quantify the alignment of each LLMs judgments with the human evaluation standards, are summarized in Table 3. The results indicate strong correlation, particularly for GPT-5 Mini, validating the potential of using LLMs for scalable and consistent evaluation. 6 Table 2: Results of different models on our benchmark across overall metrics and fine-grained categories. Diff., Sim., and Avg. stand for Difference, Similarity, and Average scores(all in %). Pos., Backgr., and Tech. denote Position, Background, and Playback Technique, respectively(all in %). Param. denotes the parameter scale. superscript lightbulb icon ((cid:17)) indicates \"thinking\" mode. Model Param. Overall Metrics Category Performance Avg. Diff. Sim. Subject Motion Pos. Backgr. Cam. Style Tech. Gemini-2.5-Pro GPT-5 Gemini-2.5-Flash Gemini-2.0-Flash GPT-4o Qwen3-VL Qwen3-VL Mimo-VL-SFT InternVL-3.5(cid:17) InternVL-3.5 Qwen2.5-VL-Instruct Qwen2.5-VL-Instruct InternVL-3.5(cid:17) InternVL-3.5 GLM-4.1V(cid:17) Qwen2.5-VL-Instruct Kimi-VL-A3B(cid:17) InternVideo2.5 Keye-VL-1.5 Llama-3.2 LLaVA-V1.6-Vicuna (cid:25) (cid:25) (cid:25) (cid:25) (cid:25) 32B 8B 7B 38B 38B 72B 32B 8B 8B 9B 7B 16B 7B 8B 11B 7B Closed-Source 66.72 63.73 75.33 62.94 57.32 79.17 58.87 52.11 78.37 53.71 50.26 63.66 49.95 39.14 81.12 67.71 61.52 59.63 58.90 46.79 Open-Source 61.38 58.54 71.50 53.23 50.44 63.20 52.59 46.51 70.17 52.44 46.25 70.30 50.49 40.09 80.46 49.71 42.56 70.30 47.83 43.42 60.53 45.01 41.18 56.07 43.67 35.68 66.70 40.95 33.99 61.08 39.39 35.22 51.42 35.16 28.68 53.88 32.70 23.14 60.32 32.45 25.53 57.13 61.01 5.23 19.43 20.07 5.11 8.96 64.60 58.66 54.39 52.66 48.35 48.07 49.72 46.79 43.21 42.60 39.82 37.48 32.72 32.86 14.48 7. 62.78 57.78 51.29 48.71 43.53 51.77 43.33 46.55 43.04 44.34 44.82 40.78 37.06 37.54 34.35 33.82 26.00 23.43 25.80 20.31 12.20 68.24 65.31 57.23 57.86 51.89 62.00 52.33 51.25 53.77 51.89 48.11 49.69 45.60 45.60 38.13 37.42 35.63 33.13 30.67 17.84 13.44 70.65 69.15 63.98 57.11 53.73 68.62 63.49 57.31 59.80 54.93 55.92 55.12 54.03 48.46 47.26 47.96 42.99 36.42 39.18 13.44 6. 59.97 75.79 74.32 57.39 77.60 54.66 52.82 81.58 55.41 47.30 55.79 18.92 49.18 77.89 27.03 52.66 74.86 47.83 40.92 66.28 11.59 48.37 67.71 25.33 47.80 72.63 20.27 49.18 76.32 14.86 46.42 68.95 22.97 38.39 68.42 20.27 35.76 61.58 17.57 39.02 68.42 14.86 33.83 64.58 14.67 30.74 58.95 14.86 22.56 70.31 14.67 28.70 66.15 14.67 24.69 60.00 8.70 29.56 40.00 11.70 6.67 6.25 10.02 Table 3: Concordance Rates of LLM Judges with Human Baseline. The table quantifies the agreement percentage between three evaluated LLMs and human annotators, showing the overall average alongside task-specific performance. LLMs Average Similarities Differences GPT-5-mini DeepSeek-V3 Qwen3-32B 95.22 89.37 87.23 95.90 90.84 88.98 94.97 88.84 86.60 To ensure the reliability of our evaluation and rule out stochastic fluctuations, we conducted five repeated judging rounds using GPT-5-mini on the captions generated by Gemini-2.5-Pro. The models accuracy remained stable between 66.20% and 66.79%, while its agreement with human judgments ranged from 94.38% to 95.12%. These results indicate high evaluation consistency and confirm the robustness of our judging framework. Effect of Different Video Parameters. We performed sensitivity analysis on Qwen2.5-VL-7B-Instruct and Keye-VL-1.5-8B to assess the impact of two key video parameters: temporal granularity and spatial fidelity. For temporal sampling, we fixed the resolution at 360p and varied the frame count across 8, 32, 64, 128. The results show non-monotonic trend in average accuracy, which peaked at 32 frames before declining. This suggests that excessively long sequences may saturate the models capacity to handle long-range temporal dependencies. Conversely, for spatial fidelity, we fixed the input at 1fps and varied the resolution across 224p, 360p, 480p, 720p. Here, performance improved monotonically with resolution, underscoring its crucial role in discerning subtle changes. As summarized in Figure 6a, these findings indicate that optimal performance is achieved by balancing moderate temporal context with high spatial fidelity. Effect of Input Video Noise To evaluate the robustness against common visual corruptions, we systematically apply blur, noise, and color saturation augmentations at three distinct intensity levels, visualized in Figure 6b. All experiments are conducted using the Qwen2.5-VL-7B-Instruct model. The 7 Figure 5: Detailed performance analysis. The top-left Overall chart summarizes model accuracy across seven high-level categories. The other seven charts offer fine-grained breakdown for each of these categories, detailing performance on specific sub-tasks. (a) Comparison of accuracy for two models: Kwai-Keye and Qwen across varying frame counts and resolutions. (b) Three video augmentations (blur, noise, color saturation) applied at light, medium, and heavy intensities, showing their progressive effects on frames. (c) Effect of augmentation: similarity (top) and difference (bottom) scores for blur, noise, and color saturation at original, light, medium, and heavy levels. Figure 6: Overview of video parameters and data augmentation effects. (a) Model accuracy with respect to frame counts and resolutions. (b) Examples of video augmentations at different intensity levels. (c) Quantitative impact of augmentation intensity on model performance metrics. empirical results, detailed in Figure 6c, reveal an intriguing trade-off: nearly all augmentations led to significant increase in the Similarity score while concurrently causing consistent decrease in the Difference score. We posit that this performance shift stems from reduction in the models fine-grained perceptual acuity; the introduced visual interference masks subtle, pixel-level details, thereby impairing its ability to discern minute distinctions and lowering the Difference score. Paradoxically, this same interference boosts the Similarity score by mitigating model hallucination. The augmentations act as regularizer, forcing the model to disregard spurious, low-level artifacts and focus on more robust, high-level structural features. Consequently, the model ceases to output unwarranted differentiations for fundamentally identical regions, leading to more accurate recognition of similarity. 8 Fine-Grained Category Accuracy Analysis. To uncover distinct model capabilities often masked by aggregate-level analysis, we conducted fine-grained evaluation of four representative multimodal models: GPT-4o, Gemini-2.5-pro, InternVL3.5-8B-nothinking, and Qwen2.5-VL-72B-Instruct. This diverse set, spanning proprietary and open-source multimodal models of varying scales, allows for multifaceted comparison as illustrated in Figure 5. Although the models generally exhibit strong capabilities in recognizing core attributes such as Type, Location, and Color, their performance on several fine-grained categories remains exceptionally challenging. These difficult areas reveal critical gaps in current model capabilities. Poor performance on tasks like OCR and video reversal techniques, combined with struggles in visual aspects such as expression and depth of field, not only differentiate models but also map out the precise frontiers where progress is most needed. By identifying these specific weaknesses, our analysis provides targeted guidance for future research priorities. Error Analysis. An analysis of our models failure cases reveals three recurring and distinct error patterns, as illustrated in Figure 7: (1) hallucinating non-existent differences on identical content; (2) generating self-contradictory responses, simultaneously claiming sameness while describing change; and most frequently, (3) incomplete or imprecise difference detection, where the model either fails to perceive salient change or describes it in vague, uninformative manner. These errors collectively highlight current limitations in the models fine-grained perception, logical consistency, and robust visual grounding, pointing to clear avenues for future improvement. Figure 7: We present two illustrative failure cases of the model. In each example, the left column displays two representative frames from the video pair. The top panel shows the generated caption, while the bottom panel identifies the errors and their underlying causes. For clarity, the model outputs have been simplified. As part of our error analysis, we focus on the models Thinking mode, highlighting instances of hallucinated differences. Activating Thinking mode reveals critical trade-off: it improves analytical depth at the cost of similarity accuracy, leading to over-discrimination. The model begins to invent distinctions where none exist, behavior best exemplified when analyzing two videos of the exact same beach. While the standard model correctly identifies the identical background, the Thinking model incorrectly flags them as different by focusing on fabricated detailsciting the presence of footprints and debris on one patch of sand versus smoother sand on the other. This demonstrates that its attempt at deeper reasoning can weaken perceptual grounding, causing it to misclassify even identical scenes."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces Video Difference Captioning (ViDiC), new task designed to address critical gap in the comparative spatio-temporal understanding of existing vision-language models. To this end, we present the ViDiC-1K dataset, spanning seven key dimensions such as motion, cinematography, and playback techniques. We further propose novel Dual-Checklist evaluation framework that leverages an LLM-as-a-Judge to directly assess the factual accuracy of model-generated descriptions. Extensive experiments on nineteen representative multimodal models reveal significant performance limitations in comparative captioning, particularly in perceiving cinematographic and playback techniques. By systematically exposing these weaknesses, ViDiC establishes foundational benchmark to drive the development of more robust and fine-grained video understanding in multimodal models."
        },
        {
            "title": "6 Limitations and Future Directions",
            "content": "While this work establishes foundational benchmark for the novel task of Video Difference Captioning (ViDiC), several limitations remain to be addressed in future research. First, regarding data scale, our current contribution focuses on constructing high-quality test set (ViDiC-1K) to ensure reliable evaluation. Although sufficient for benchmarking, this scale is not yet suitable for large-scale supervised fine-tuning. Second, regarding methodological scope, this paper prioritizes defining the task formulation and revealing the capabilities of off-the-shelf MLLMs via our dual-checklist framework. We have not yet proposed specialized model architecture or training objective tailored to address the specific deficiencies identified in our experiments. In future work, we aim to bridge these gaps by scaling up the dataset construction pipeline to generate large-scale instruction-tuning data, which aims to enhance MLLMs comparative spatio-temporal understanding and capture the fine-grained video differences highlighted in our analysis."
        },
        {
            "title": "7 Real-world Applications of ViDiC",
            "content": "ViDiC moves beyond simple similarity scoring to offer actionable, semantic insights into video discrepancies. By articulating what has changed, this capability empowers diverse domains: Content Integrity and Forensics: ViDiC facilitates copyright protection by detecting subtle obfuscations. Furthermore, it combats disinformation by explicitly identifying and describing manipulated regions, acting as an interpretable tool for fact-checking. Video Editing Verification: Serving as rigorous evaluator, ViDiC validates whether editing models adhere to user prompts. It ensures precise modifications without introducing unintended artifacts or background alterations by explicitly captioning discrepancies between source and edited footage. Automated Video Production: In collaborative workflows, the system streamlines post-production by generating automatic Change Logs. These summaries of editorial adjustments effectively bridge the communication gap between human editors and AI tools. Skill Assessment and Rehabilitation: For applications in sports training or physical therapy, ViDiC provides granular feedback on pose deviations and temporal misalignments by comparing user performance against standard reference videos. Scientific Monitoring: It automates change detection in longitudinal observation footage, converting visual shifts in dynamic environments into structured descriptive reports for efficient analysis. Intelligent Surveillance: Surpassing conventional motion sensing, semantic analysis distinguishes between environmental interference and relevant activity, ensuring that alerts are triggered only by specific objects or actionable security behaviors. References Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing images. (EMNLP), 2018. Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. arXiv:1901.02527 [cs.CV], 2019. URL https://doi.org/10.48550/arXiv.1901.02527. Linli Yao, Weiying Wang, and Qin Jin. Image difference captioning with pre-training and contrastive learning. arXiv preprint arXiv:2202.04298, 2022. URL https://arxiv.org/abs/2202.04298. Zonglin Di, Jing Shi, Yifei Fan, Hao Tan, Alexander Black, John Collomosse, and Yang Liu. Difftell: comprehensive dataset for image difference captioning, 2025. URL https://openreview.net/forum? id=86uYj8DcfK. Yuan Liu, Saihui Hou, Saijie Hou, Jiabao Du, Shibei Meng, and Yongzhen Huang. Omnidiff: comprehensive benchmark for fine-grained image difference captioning. arXiv:2503.11093 [cs.CV], 2025. URL https://doi.org/10.48550/arXiv.2503.11093. Anonymous. Vidi: benchmark for the identification and captioning of visual differences in image pairs. In Submitted to ACL Rolling Review - February 2025, 2025. URL https://openreview.net/forum? id=xpyqpk4bDc. under review. 10 Zhaoyang Zeng, Yongsheng Luo, Zhenhua Liu, Fengyun Rao, Dian Li, Weidong Guo, and Zhen Wen. Tencent-mvse: large-scale benchmark dataset for multi-modal video similarity evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31383147, 2022. Benedetta Liberatori, Alessandro Conti, Lorenzo Vaquero, Yiming Wang, Elisa Ricci, and Paolo Rota. Convis-bench: Estimating video similarity through semantic concepts. arXiv preprint arXiv:2509.19245, 2025. doi: 10.48550/arXiv.2509.19245. Accepted to NeurIPS 2025. Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003, 2023a. Dawit Mureja Argaw, Fabian Caba Heilbron, Joon-Young Lee, Markus Woodson, and In So Kweon. The anatomy of video editing: dataset and benchmark suite for ai-assisted video editing. In European Conference on Computer Vision, pages 201218. Springer, 2022. Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al. Editverse: Unifying image and video editing and generation with in-context learning. arXiv preprint arXiv:2509.20360, 2025. Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, and Shuicheng Yan. Ivebench: Modern benchmark suite for instruction-guided video editing assessment. arXiv preprint arXiv:2510.11647, 2025a. Shangkun Sun, Xiaoyu Liang, Songlin Fan, Wenxu Gao, and Wei Gao. Ve-bench: Subjective-aligned benchmark suite for text-driven video editing quality assessment. arXiv preprint arXiv:2408.11481, 2024. Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, and Jiaheng Liu. Omnivideobench: Towards audiovisual understanding evaluation for omni mllms, 2025a. URL https://arxiv.org/abs/2510.10689. Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, and Jiaheng Liu. Mt-video-bench: holistic video understanding benchmark for evaluating multimodal llms in multi-turn dialogues, 2025. URL https://arxiv.org/abs/2510.17722. Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, and Tieniu Tan. Vidcapbench: comprehensive benchmark of video captioning for controllable text-to-video generation, 2025b. URL https://arxiv.org/abs/2502.12782. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision ECCV 2014, pages 740755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 89478956, 2019. doi: 10.1109/ICCV.2019. 00904. Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 11 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024a. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024a. Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Gavin Chang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Steven Huang, Zhaoxiang Zhang, and Jiaheng Liu. Mvu-eval: Towards multi-video understanding evaluation for multimodal llms, 2025. URL https://arxiv.org/abs/2511.07250. Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, and Wenhu Chen. Editreward: humanaligned reward model for instruction-guided image editing. arXiv preprint arXiv:2509.26346, 2025. URL https://arxiv.org/abs/2509.26346. Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, and Serena Yeung-Levy. Describing differences in image sets with natural language, 2024. URL https://arxiv.org/abs/2312.02974. Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, and Forrest Iandola. Cvpr 2023 text guided video editing competition, 2023b. James Burgess, Xiaohan Wang, Yuhui Zhang, Anita Rau, Alejandro Lozano, Lisa Dunlap, Trevor Darrell, and Serena Yeung-Levy. Video action differencing. arXiv preprint arXiv:2503.07860, 2025. Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, and Jiaheng Liu. If-vidcap: Can video caption models follow instructions?, 2025b. URL https://arxiv.org/abs/2510. 18726. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1719117202, 2025. Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, and Ronggang Wang. Pku-dymvhumans: multi-view video benchmark for high-fidelity dynamic human modeling. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. T. Malon, G. Roman-Jimenez, P. Guyot, S. Chambon, V. Charvillat, A. Crouzil, A. Péninon, J. Pinquier, F. Sèdes, and C. Sénac. Toulouse campus surveillance dataset: scenarios, soundtracks, synchronized videos with overlapping and disjoint views, 2018. URL https://doi.org/10.5281/zenodo.3697806. Conference on Multimedia Systems (MMSys), Amsterdam. Shuhei Yokoo, Peifei Zhu, Junki Ishikawa, and Rintaro Hasegawa. Dataset: Vsc2022, 2024. URL https://doi.org/10.57702/2okbx305. Accessed: 2024-12-16. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llms is moving target: The trick of dynamic sourcing and llm-as-a-judge, 2023. URL https://arxiv.org/ abs/2306.05685. Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners, 2025. URL https://arxiv.org/abs/2509.20328. Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025a. Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26302640, 2025. 12 Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https: //arxiv.org/abs/2408.00714. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Gemini 2.5 Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 [cs.CL], 2025a. URL https://arxiv.org/abs/2507.06261. OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276 [cs.CL], 2024. URL https://arxiv.org/abs/ 2410.21276. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Internvl3. 5: Advancing open-source multimodal models in Jing, Shenglong Ye, Jie Shao, et al. versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Kwai Keye Team. Kwai keye-vl technical report, 2025b. URL https://arxiv.org/abs/2507.01949. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, 13 Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025a. URL https://arxiv.org/abs/2504.07491. GLM-V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale 14 Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2507.01006. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. OpenAI. Gpt-5 system card. 2025. URL https://openai.com/index/gpt-5-system-card/. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Arc-hunyuan-video-7b: Structured video comprehension of real-world shorts. arXiv preprint arXiv:2507.20939, 2025. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llavanext-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024b."
        },
        {
            "title": "A Dataset Samples",
            "content": "To better illustrate our dataset, this section presents two specific examples. Each example features key frames from pair of similar videos, along with corresponding checklist of questions. Example 1 Example 2 Similarity Question Class: Camera Category: Shot Scale Question: Is the cameras shot scale different in both videos? Correct Answer: no Class: Playback Technique Question: Is the playback technique different in both videos? Correct Answer: no Difference Question Class: Motion Category: Motion Type Question: Does the man in the Video touch his head with his hand, motion that is not seen in the Video A? Correct Answer: yes Class: Background Category: Lighting Question: Is the lighting in the Video white-toned, in contrast to the warm, golden-toned lighting in the Video A? Correct Answer: yes Similarity Question Class: Background Category: Key Background Object Question: Is the key background object different between the both videos? Correct Answer: no Class: Style Question: Is the style different between the Video and B? Correct Answer: no Difference Question Class: Subject Category: Gender Question: Is the foreground subject in the Video male, in contrast to the foreground subject in the Video who is female? Correct Answer: yes Class: Subject Category: Clothing and Accessories Question: Does the foreground subject in the Video wear blue longsleeved garment, while the one in the Video wears red sleeveless dress? Correct Answer: yes"
        },
        {
            "title": "B Experiment",
            "content": "B.1 Evaluation Settings We provide the detailed settings for all evaluated open-source models in Table 4, covering both the models discussed in the main text and those in the appendix. Most models are tested under default settings. Closed-source models are accessed via API calls, using the default configuration. B.2 Additional Experimental Results Due to space limitations in the main manuscript, we present the evaluation results of additional MLLMs in this section, providing broader perspective on the performance of current open-source 1 Table 4: Evaluation metrics for locally deployed open-source models. The \"Frame\" column represents the frame rate (float) or fixed frame number (integer). \"None\" in the table means disabled. \"Auto\" means determined by the models default configuration. superscript lightbulb icon ((cid:17)) indicates \"thinking\" mode. Models Params Resolution Frame Temperature Top_p Repetition Penalty InternVL-3.5(cid:17) InternVL-3.5 InternVL-3.5(cid:17) InternVL-3.5 Qwen2.5-VL-Instruct Qwen2.5-VL-Instruct Qwen2.5-VL-Instruct Qwen3-VL Qwen3-VL Keye-VL-1.5 Llama-3.2-Vision-Instruct LlaVA-V1.6-Vicuna Kimi-VL-A3B(cid:17) InternVideo2.5 GLM-4.1V(cid:17) Mimo-VL-SFT ARC-Hunyuan-Video VideoChat2 LLaVA-Video 38B 38B 8B 8B 72B 32B 7B 32B 8B 8B 11B 7B 16B 7B 9B 7B 7B 7B 7B 448 448 448 448 448 448 448 448 Auto Auto Auto Auto Auto Auto 448 448 448 448 Auto 448x448 Auto Auto Auto 224x224 Auto 1.0 1.0 1.0 1.0 2.0 2.0 2.0 2.0 2.0 2.0 1.0 32 6 32 1.0 32 1.0 16 32 0.6 0.1 0.6 0.1 0.1 0.1 0.1 0.7 0.7 0.1 0.1 None 0.2 None Auto 0.3 None Auto None 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.9 0.9 0.001 Auto None 0.9 None Auto 0.95 None Auto None 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 1.05 Auto Auto Auto Auto Auto Auto Auto Auto Auto models(VideoChat2(Li et al., 2023), ARC-Hunyuan-Video(Ge et al., 2025), LLaVA-Video(Li et al., 2024b)). These models were evaluated under the same rigorous protocol as those in the main paper. To establish human baseline, we also sampled 200 video pairs and invited independent evaluators unaffiliated with this project to answer them. The comprehensive results, including the human performance, are detailed in Table 5. Table 5: Evaluation results of additional models. Due to space constraints in the main paper, these results are reported here. Diff., Sim., and Avg. stand for Difference, Similarity, and Average scores (all in %). Pos., Backgr., and Tech. denote Position, Background, and Playback Technique, respectively (all in %). Param. denotes the parameter scale. superscript lightbulb icon ((cid:17)) indicates \"thinking\" mode. Model Param. Overall Metrics Category Performance Avg. Diff. Sim. Subject Motion Pos. Backgr. Cam. Style Tech. VideoChat2 ARC-Hunyuan-Video LLaVA-Video Human 7B 7B 7B - 27.85 5.21 93.28 76.33 4.46 22.94 21.67 12.19 49.05 20.11 16.27 20.75 29.53 23.43 14. 24.69 20.31 20.31 20.50 15.52 23.28 42.36 55.73 13.33 36.97 47.92 13.33 19.05 57.81 13.33 84.74 84.42 85.66 87.23 85. 85.53 82.17 85.64 75.61 89.47 B.3 Robustness Experiment Details Three distortion types (color saturation, Gaussian noise, Gaussian blur) are applied at Light, Medium, and Heavy intensities using OpenCV. Unlike random perturbation strategies, we apply deterministic parameters for each intensity level to ensure strict reproducibility. For Gaussian Noise, we inject additive noise sampled from normal distribution (0, σ2), with standard deviations σ set to 15, 30, and 45 for the Light, Medium, and Heavy levels, respectively. For Gaussian Blur, we apply smoothing filter with kernel dimensions of 5 5 (Light), 9 9 (Medium), and 15 15 (Heavy). For Color Saturation, we scale the S-channel in the HSV color space by multiplicative factors of 0.75 (Light), 0.50 (Medium), and 0.25 (Heavy). Crucially, to ensure that the observed similarity metrics reflect the models intrinsic robustness rather than artificial disparities, identical perturbation parameters are rigorously applied to both videos within comparison pair."
        },
        {
            "title": "C Construction of the Test Set",
            "content": "To establish robust benchmark, we implemented rigorous three-stage cleaning pipeline transforming 8,756 raw candidate pairs into 1,000 high-quality samples. First, an automated temporal filter removed outliers outside the 2s40s range, reducing the pool to 5,420 pairs. Second, annotators manually removed trivial (identical) or invalid (excessively disparate) pairs, narrowing the candidates down to 1,850. Finally, to guarantee the reliability of the benchmark, trained professionals performed comprehensive validation using the custom interface shown in Figure 8. In this stage, annotators simultaneously evaluated visual dynamics and annotation quality. Specifically, they discarded samples exhibiting static scenes or negligible motion, while concurrently verifying the rationality of the checklists and the correctness of the ground truth answers to ensure strict alignment with the video content. This process, conducted over one month, eliminated samples with either low-quality footage or ambiguous annotations to yield the final 1,000 pairs. The specific source distribution is detailed in Table 6. Figure 8: The annotation interface used for the final inspection. Annotators utilize this tool to simultaneously filter out unqualified videos and validate the rationality of the checklist and standard answers. Table 6: Source distribution of the final test set. After rigorous filtering and verification, the dataset retains diverse high-quality samples ranging from synthetic edits to real-world footage. Dataset Source Count Percentage CV and Rendered Synthesis LMArena IF-VidCap Splice-based Synthesis YouTube VidDiffBench PKU-DyMVHumans ToCaDa VSC VACE_Bench 340 157 131 115 86 50 49 28 27 17 34.00% 15.70% 13.10% 11.50% 8.60% 5.00% 4.90% 2.80% 2.70% 1.70% Total 1,000 100.00%"
        },
        {
            "title": "D Error Analysis",
            "content": "This section presents several representative error-analysis examples. For clarity of presentation, the displayed model outputs have been simplified. 3 Example 1 Example 2 Generated caption: The key distinctions lie in subject movement. In the Video A, the subject is seated with hands on lap and performs vertical head nod, while in the Video B, the subject stands with crossed arms and turns their head horizontally. Error Analysis: The description falsely claims the subject shifts from seated to standing and changes head movements. In reality, the central subjects actions are identical. The analysis fabricates these differences while ignoring the actual change: person walking dog in the background of the Video B. Generated caption: The video demonstrates how the same anime character appears in two completely different styles. The Video has soft, pastel-colored aesthetic, depicting an animated urban setting. Error Analysis: The core error is critical omission of information. The model only analyzes the style of the Video A, while completely failing to mention the Video Bs stylespecifically the \"cyberpunk\" style. This leaves the response half-finished, failing to provide the requested comparison or complete analysis. Example 3 Example 4 Generated caption: The Video shows the same room as the Video A: man in patterned short-sleeve shirt and shorts is seated at an electronic keyboard, with microphones and stands nearby. Windows line the back wall. Chairs and musical equipment are arranged around the room in the same overall layout. Generated caption: In both videos, the subject walks steadily along the identical walkway, maintaining consistent motion. The only variation lies in the framing and field of view. Due to the difference in perspective, the Video intentionally features fewer cars to create cleaner shot. Error Analysis: The analysis fails to account for the impact of camera perspective on object visibility. Although the actual scene layout remains unchanged, the Video reveals more chairs due to higher angle and shifted camera orientation, demonstrating limited understanding of how camera variations alter the quantity of visible objects in the frame. Error Analysis: The models response is incomplete because it fails to mention the two most important consequences of the perspective change. Specifically, it misses how the new angle changes the view of the walkway from horizontal to longitudinal, creating strong sense of depth, and how this in turn shifts the subjects perceived motion from moving sideways to moving directly toward the camera."
        },
        {
            "title": "E Prompts",
            "content": "E.1 Comparative Descriptions During the Automated Draft Generation stage, we used the Qwen3-VL-plus model to produce detailed comparative descriptions highlighting similarities and differences between video pairs. The prompt is as follows: 4 Video Analyst Prompt ROLE & TASK You are professional, detail-oriented video analyst. Your task is to perform comprehensive, objective, side-by-side comparison of two videos: Video and Video B. Your entire existence is governed by single principle: radical objectivity. You are machine for observing and reporting, not for interpreting. Before you write single word, you must ask yourself: \"Is this statement an undeniable, verifiable visual fact?\" If the answer is anything less than 100% certain \"yes\", you must discard the statement. This principle overrides all other instructions. CRITICAL INSTRUCTIONS You must strictly follow these rules for your entire response: 1. ENGLISH ONLY: Your entire response MUST be in English. 2. Strict Objectivity and Certainty: Your analysis must be based only on clear, verifiable visual evidence. No Subjective Language: Do not use interpretive words (e.g., \"beautiful,\" \"skillfully shot\"). No Ambiguity or Speculation: If an element is unclear, you MUST omit it entirely. No Fabrication: Do not add details that do not exist. 3. Diff SECTIONS ARE FOR DIFFERENCES ONLY: Under any Diff heading, you MUST only describe aspects that are different. 4. SKIP Diff IF IDENTICAL: If category is completely identical, describe the similarities under Same and then completely omit the Diff section. 5. NO TIMESTAMPS: Do not mention specific seconds or frame numbers. 6. Do not overuse the term dynamic. 7. Carefully verify which parts are truly the same before marking them as Same. OUTPUT FORMAT INSTRUCTIONS You MUST format your entire response in Markdown. Adhere strictly to the following structure: Main Categories: Use Level 2 Headings. Comparison Labels: Use bold for **Same** and **Diff**. Difference Sub-Categories: Under **Diff**, use bolded sub-headings. Details: Use nested bullet points for Video and Video B. ANALYSIS FRAMEWORK 1. Subject Analyze the core subjects present in the videos. Same State if subject categories, count, or core attributes are identical. Note if high-level categories are the same. (Only mention if different; otherwise, omit this section) Diff Subject Category: Differences in broad categories (Person, Animal, Object, Text). Video A: Video B: Subject Count: Differences in total count and count per category. Video A: Video B: Subject Attributes: Differences in appearance (age, gender, clothing) and state/pose. Video A: Video B: 2. Style Analyze the overall visual style using only the restricted list of descriptors: American comic style, Ukiyo-e, Anime, Pixel Art, Ghibli Style, Cyberpunk, Steampunk, Low Poly, Voxel Art, Minimalist, Flat Design, Retro, Oil Painting, Watercolor, Sketch, Graffiti, Ink Wash Painting, Black and White, Monochromatic, CG Rendering, realistic(un-stylized). 5 Same If the overall style is identical, describe it here. (Only mention if different; otherwise, omit this section) Diff Describe each videos style based on objective characteristics. Video A: Video B: 3. Scene & Background Analyze the environment and setting. Same If the scene (location, lighting) is identical, describe it here. (Only mention if different; otherwise, omit this section) Diff Location: indoor/outdoor, urban/natural. Lighting: daylight/artificial, time of day. Major Elements: key background objects. 4. Camera Work / Cinematography Analyze camera language and shooting techniques. Same If aspects of cinematography are identical, describe it here. (Only mention if different; otherwise, omit this section) Diff Shot Scale Sequence: long, medium, close-up. Camera Movement Sequence: pan, tilt, dolly, static. Angle Sequence: eye-level, high-angle, low-angle. Subjects Orientation to Camera: front, profile, back. 5. Subject Motion Analyze the dynamic performance and actions of the subjects. Same If core dynamic events and motion types are identical, describe them here. (Only mention if different; otherwise, omit this section) Diff Core Dynamic Event/Motion Type: running, jumping, expression changes. Interaction: Between subjects or with objects. Motion Details: Direction, speed, trajectory. 6. Positional Relationship Analyze the relative spatial relationship between elements. Same If relative positions are identical, describe it here. (Only mention if different; otherwise, omit this section) Diff Indicate if the scene is flipped or mirrored. Spatial Arrangement: Changes in relative positions between subjects and background elements. 7. Playback Technique Analyze special playback manipulations. Use only: \"slow-motion\", \"fast-forward\", \"reverse\", or \"no special playing techniques\". Same If identical, do not include the Same section at all. (Only mention if different; otherwise, omit this section) Diff Playback Manipulation: Video A: Video B: 6 E.2 Checklist In the checklist generation stage, we used the Gemini-2.5-Pro model. The prompt is as follows: Checklist Generation prompt ROLE & TASK You are highly precise Assessment Creator AI. Your task is to convert comparative analysis text into comprehension quiz. The quiz must rigorously test users understanding of specific and concrete differences between two items. Your entire existence is governed by single principle: radical objectivity. You are machine for observing and reporting, not for interpreting. Before you write single word, you must ask yourself: \"Is this statement an undeniable, verifiable visual fact?\" If the answer is anything less than 100% certain \"yes\", you must discard the statement. This principle overrides all other instructions. CRITICAL INSTRUCTIONS 1. Mandatory Comparative Framing Every single question MUST explicitly mention and compare both items (e.g., \"Video A\" and \"Video B\") within the question itself. Forbidden: Questions that only ask about one item are strictly prohibited. Correct Example: Is it true that in the Video and B, the person is assembling the same wooden chair? Forbidden Example: Is the person in the Video assembling wooden chair? 2. AVOID LOGICAL CONTRADICTIONS Absolute prohibition of contradictions is strictly enforced! All annotations must maintain complete logical consistency. Zero tolerance for conflicting information Immediate flagging of logical inconsistencies The following is contradiction that is not allowed: {\" class \": \" style \", \" question \": \" Is the visual style different in Video and ?\" , \" correct_answer \": \" no \"} {\" class \": \" style \", \" question \": \" Does the Video have black and white style , in contrast to the cyberpunk style of the Video ?\" , \" correct_answer \": \" yes \"} Explanation: These statements conflict. The first claims the style is the same, while the second confirms specific, significant difference. 3. Strict Adherence to Source Text: You MUST base every question and answer exclusively on the provided Input Text. 4. Question-Based Format: All items MUST be direct, closed-ended questions that can be definitively answered with \"Yes\" or \"No\". 5. Adaptive Question Quantity For brief diff description, generate 02 questions. For detailed diff description, generate 13 questions, each targeting different specific detail. 6. Every question generated MUST be followed by its correct standard answer: Yes or No. 7. Module Design Requirements Organize the quiz into two modules: Module 1 (Similarities) and Module 2 (Differences). The Similarities module must have fewer questions. Both modules must follow the Guidelines for asking questions. Module 1: Similarities (Same) Design Purpose: Module 1 performs reverse check. Questions are generated from similarities but are framed to ask Are [they] different?, so the correct answer is always \"no\". Critical Rule for Module 1: No examples allowed. Do not use phrases like In terms of..., involving..., or such as.... Questions must be abstract and must not reveal specific details. 7 Question Characteristics: Address categories, not specific attributes. Questions must be abstract. Prefer subcategories over major categories. Sparingly ask about playback techniques or perspective. Question Source & Answer Rule: Questions are derived from same information and phrased as query about difference. Therefore, the answer must always be \"no\". Module 2: Differences (Diff) Design Focus on specific, narrow points of primary difference. FORBIDDEN: Broad, general comparison questions. All Module 2 answers must be yes. The difficulty MUST be layered (include simple and hard questions). Principle of Specificity: Correct: Regarding assembly tools, does the Video feature manual screwdriver while the Video features power drill? Forbidden: Are the assembly methods different? (Too general). Core Principle: Avoid Subjectivity Questions must be based on observable facts. question is wrong if it is: Subjective: Uses words like \"better\" or \"clearer.\" Lacks Clear Standard: Uses comparisons like \"more\" or \"straighter.\" Focused on Trivialities: Asks about negligible difference. INPUT You will be given an Input Text in Markdown, divided into sections like ## Same and ## Diff. OUTPUT You MUST format your entire response as single JSON object. The keys and nesting format are non-negotiable. The value for the class key must be from the provided guidelines. { \" Similarities \": [ { } , { }, { } \" class \": \" subject \", \" question \": \" Is the subject category different between both videos ?\" , \" correct_answer \": \" no \", \" answer \": \"\" , \" explanation \": \"\" \" class \": \" style \", \" question \": \" Do both videos have different visual style ?\" , \" correct_answer \": \" no \", \" answer \": \"\" , \" explanation \": \"\" \" class \": \" camera \", \" question \": \" Are different cinematography techniques used in both videos ?\" , \" correct_answer \": \" no \", \" answer \": \"\" , \" explanation \": \"\" ], \" Differences \": [ { \" class \": \" subject \", \" question \": \" Does the Video contain two subjects while the Video contains only one ?\" , \" correct_answer \": \" yes \", \" answer \": \"\" \" class \": \" motion \", \" question \": \" In the Video , is the guitarist actively playing while in the Video he holds static pose ?\" , \" correct_answer \": \" yes \", \" answer \": \"\" \" class \": \" background \", \" question \": \" Does the Video show whiteboard while the Video shows nighttime window ?\" , \" correct_answer \": \" yes \", \" answer \": \"\" }, { }, { } ] } Guidelines for asking questions 1. Subject (\"class\": \"subject\") Can be asked from aspects like Type, Quantity, and Attributes (Appearance, Pose/State). Core Principles: Avoid subjective language. Dynamic actions do not belong here. Questions about similarities and differences must not contradict each other. 2. Style (\"class\": \"style\") Use descriptors from predefined list only (e.g., Anime, Cyberpunk, Oil Painting, Sketch, realistic). Core Principles: Strict objectivity. Focus on the one or two most significant features. In Module 1, similarity questions must never target specific style. 3. Background (\"class\": \"background\") Analyzes environment, setting, and key background elements (Location, Atmosphere, Lighting, Weather, Key Objects). Core Principles: Overlook minor details. Focus on major, unambiguous features. In Module 1, questions must not target specific background. 4. Camera Work (\"class\": \"camera\") Analyzes camera techniques (Perspective, Movement, Angle, Scale, Composition, Depth of Field, Orientation). Core Principles: Overlook minor details. Only ask about significant differences. Base descriptions on identifiable techniques, not artistic \"feeling.\" 5. Subject Motion (\"class\": \"motion\") Asks about motion and dynamics (Core events, event details, interactions). Core Principles: Overlook minor details. Describe physical movement, not emotional intent (e.g., \"raising fist,\" not \"threatening\"). 6. Positional Relationship (\"class\": \"position\") Asks about the relative position between key subjects or between subject and significant background element. Core Principles: Overlook minor details. Use clear, objective positional language relative to the viewers frame. 7. Playback Technique (\"class\": \"playback technique\") Concerns manipulation like \"slow-motion,\" \"fast-forward,\" and \"reverse.\" Core Principles: Use only the three standard terms. If both videos are at normal speed, avoid asking about this to prevent redundancy. 9 E.3 Evaluation The following prompt instructs the model under test to generate similarities and differences between given items. Prompt for the model under test to generate similarities and differences ROLE & TASK You are professional video analyst. Your task is to perform comprehensive, objective, side-by-side comparison of two videos: Video and Video B. Your entire existence is governed by single principle: radical objectivity. You are machine for observing and reporting, not for interpreting. Before you write single word, you must ask yourself: \"Is this statement an undeniable, verifiable visual fact?\" If the answer is anything less than 100% certain \"yes\", you must discard the statement. This principle overrides all other instructions. CRITICAL INSTRUCTIONS You must strictly follow these rules for your entire response: Core principle: Responses must strictly adhere to our analytical framework. It is CRITICAL. ENGLISH ONLY: Your entire response MUST be in English. Strict Objectivity and Certainty: Your analysis must be based only on clear, verifiable visual evidence. No Subjective Language: Do not use interpretive words (e.g., \"beautiful,\" \"sad,\" \"skillfully shot\"). No Speculation: If an element is unclear or ambiguous, you MUST omit it entirely. Do not guess or assume. No Fabrication: Do not add details that do not exist. NO TIMESTAMPS: Do not mention specific seconds or frame numbers. Use the MENTAL CHECKLIST: Before writing, mentally review all categories in the ANALYSIS FRAMEWORK. The answer should be concise and avoid excessive embellishment. The answer must not contain contradictions. Do not make your answers too brief. Do not include unnecessary or irrelevant details. OUTPUT FORMAT INSTRUCTIONS Your response must be organized by the following categories, but only include categories where relevant differences or similarities are noticeable. subject style background camera motion position playback technique For each category, specify Similarities and Differences. Example format Subject Similarities: Both videos contain one adult male human subject with short, dark hair. Differences: In the Video A, the subject wears eyeglasses and red, collared shirt. In the Video B, the subject does not wear eyeglasses and wears blue t-shirt. Style Similarities: Both videos are presented in realistic (un-stylized) manner. Differences: There are no discernible differences in style. Background Similarities: The subject in both videos is located indoors in room with white walls. Both scenes are brightly lit by an artificial light source originating from above. Differences: In the Video A, framed picture is visible on the wall. In the Video B, clock is visible in the same location. 10 Camera Similarities: Both videos are filmed at an eye-level angle. The shot scale is medium shot. Differences: The camera is static in the Video A. In the Video B, the camera performs slow zoom-in. Motion Similarities: In both videos, the subject begins by looking directly at the camera. Differences: In the Video A, the subject nods their head. In the Video B, the subject turns their head to their right. Position Similarities: In both videos, the subject is positioned in the center of the frame. Differences: In the Video A, the subject is seated. In the Video B, the subject is standing. Playback Technique Similarities: The main action in both videos is presented in forward motion. Differences: There is no difference. ANALYSIS FRAMEWORK Core principle: Responses must strictly adhere to our analytical framework. It is CRITICAL. 1. Subject Analysis aspects: Type: Person, animal, object, text, etc. Quantity: Total number of subjects. Attributes: Appearance: Age, gender, clothing, color, material, hairstyle, etc. Pose / State: Facial expression, body pose (standing, sitting), object state (open/closed), etc. 2. Style Use descriptors from the predefined list only: American comic style, Ukiyo-e, Anime, Pixel Art, Ghibli Style, Cyberpunk, Steampunk, Low Poly, Voxel Art, Minimalist, Flat Design, Retro, Oil Painting, Watercolor, Sketch, Graffiti, Ink Wash Painting, Black and White, Monochromatic, CG Rendering, realistic (un-stylized). 3. Background Analysis aspects: Location: office, street, park, indoor/outdoor. Atmosphere: festive, quiet, lively. Lighting: natural/artificial, time of day, brightness. Weather: sunny, cloudy, rainy, snowy. Key Objects: desks, chairs, buildings, trees. 4. Camera Work / Cinematography Analysis aspects: Perspective: First-person or third-person. Movement: Pan, Tilt, Zoom, Dolly, etc. Angle: High-angle, low-angle, eye-level. Scale: Close-up, medium shot, long shot. Composition: Position of subject in the frame. Depth of Field: shallow, deep, medium. Orientation: Front-facing, side-facing, back-facing. Shots: Single vs. Multiple shots. 5. Subject Motion Analysis aspects: Core Events: Primary movements, sequence of actions. Event Details: Direction, speed, trajectory of motion. Interactions: Between subjects or with objects. 6. Positional Relationship Analysis aspects: Relationship between primary subjects. Relationship between subject and significant background element. Indicate if the scene is flipped or mirrored. 11 7. Playback Technique Only use the following terms: \"slow-motion\", \"fast-forward\", \"reverse\", or state that there are \"no special playing techniques\". E."
        },
        {
            "title": "Judge",
            "content": "During the evaluation stage, we employed the gpt5-mini model to perform judgment tasks. The prompt is as follows: Judge Protocol My purpose is to act as an evaluation agent. Given \"Model Description\" and specific \"Question\", will determine whether the answer is \"yes\" or \"no\" and provide brief justification for my decision. My operational process is as follows: Input will receive [Model Description] and [Question]. Analysis will strictly base my answer on the information provided in the [Model Description]. Inference Rule For content not explicitly mentioned in the description, will apply the following logic: If the question pertains to potential similarity, may judge the answer as \"yes\" if the description does not contradict it. If the question pertains to potential difference, it will be confirmed as yes only if the difference is explicitly stated or directly inferable from the [Model Description]. In the absence of supporting information, the answer will be no. Output My entire response will be single JSON object in the following format: { } \" answer \": \" yes / no \", \" explanation \": \" reason \" am now ready to receive the [Model Description] and [Question] to perform this task."
        },
        {
            "title": "F Category Framework",
            "content": "The detailed category framework is presented in the following two tables. Table 7 lists the attributes regarding Subject, Style, and Background. Table 8 presents the attributes related to Motion, Position, Camera, and Playback Technique. 12 Table 7: Fine-grained subcategories within the three major classes: Subject, Style, and Background. Category Type Count Appearance Clothing Pose State Material Color OCR Subject Description Classifies the subject type, including persons, animals, plants, vehicles, buildings, and virtual characters. Quantifies the total number of subjects and specific counts per category. Describes physical attributes like age, gender, ethnicity, physique, facial features, hairstyle, and makeup. Details attire and accessories, such as hats, glasses, jewelry, and masks. Captures body posture (e.g., standing, sitting), hand gestures, and facial expressions. Describes the physical condition of inanimate objects (e.g., open/closed, broken). Identifies material composition and texture (e.g., metal, wood, fabric, plastic). Specifies the dominant colors of objects, clothing, or skin tones. Transcription of visible text, including signage, subtitles, logos, and documents. Category Description Style Restricted Style Descriptors To ensure objectivity, every video is strictly categorized into one of the following predefined styles: Traditional & Fine Art: Oil Painting, Watercolor, Sketch, Ink Wash Painting, Ukiyo-e, Graffiti. Digital & Graphical: CG Rendering, Pixel Art, Voxel Art, Low Poly, Minimalist, Flat Design. Pop Culture & Thematic: Anime, Ghibli Style, American comic style, Cyberpunk, Steampunk, Retro. Visual Tone: Black and White, Monochromatic, Realistic. Category Location Atmosphere Lighting Weather Key Background Objects Background Description Identifies the scene type, such as an office, street, park, indoor, or outdoor environment. Describes the mood or ambiance of the scene (e.g., festive, quiet, nervous). Details the light source, time of day, style, and brightness level. Indicates environmental conditions like sunny, rainy, snowy, or foggy. Lists prominent objects that form the main visual setting. Table 8: Fine-grained subcategories within the three major classes: Motion, Position, Camera, and Playback Technique. Category Type Interaction Direction Speed Amplitude Trajectory Motion Description Classifies the primary movement or temporal sequence of actions. Describes relations between subjects. Specifies spatial orientation (e.g., moving left, approaching, receding). Indicates velocity or rate of change (e.g., static, fast, accelerating). Describes the magnitude, range, or force of the motion. Defines the path of motion (e.g., linear, curved, circular). Category Description Position Subject-Object Relation Subject-Subject Relation Spatial Flipping Describes the spatial arrangement between the subject and key background elements. Defines the relative positions and distances between multiple subjects. Indicates geometric transformations like mirroring along horizontal or vertical axes. Category Scale Movement Orientation Angle Composition Depth of Field Perspective Shot Count Category Slow Fast Reverse Camera Description Determines the subjects apparent size in the frame (e.g., close-up, wide shot). Describes camera motion during recording (e.g., pan, tilt, zoom, dolly). Specifies the direction the subject is facing relative to the camera (e.g., front-facing, side profile). Refers to the vertical camera position relative to the subject (e.g., high angle, overhead). Describes the subjects placement within the frame (e.g., centered, left-aligned, off-center). Describes the clarity of the background relative to the subject (e.g., blurred background, sharp background). Indicates the point of view from which the scene is captured (e.g., first-person view, third-person view). Distinguishes between single continuous shot and multi-shot sequence. Playback Technique Description Plays video at reduced speed to emphasize details. Plays video at an increased speed to compress the timeline. Replays the video timeline backwards from end to start."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Nanjing University"
    ]
}