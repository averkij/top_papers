{
    "paper_title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark",
    "authors": [
        "Ziyu Guo",
        "Xinyan Chen",
        "Renrui Zhang",
        "Ruichuan An",
        "Yu Qi",
        "Dongzhi Jiang",
        "Xiangtai Li",
        "Manyuan Zhang",
        "Hongsheng Li",
        "Pheng-Ann Heng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 0 8 6 2 . 0 1 5 2 : r Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-COF Benchmark Ziyu Guo1, Xinyan Chen2, Renrui Zhang2, Ruichuan An3, Yu Qi4, Dongzhi Jiang2 Xiangtai Li3, Manyuan Zhang2, Hongsheng Li2, Pheng-Ann Heng1 CUHK 1IMIXR & 2MMLab 3Peking University 4Northeastern University Equal Contribution Project Lead Corresponding Author Project Page: https://video-cof.github.io"
        },
        {
            "title": "Abstract",
            "content": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation [70]. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3 [21]. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-COF, compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models."
        },
        {
            "title": "Introduction",
            "content": "Video models [21, 63, 55, 81, 11], including text-to-video and video-to-text generation models, have made rapid progress in recent years. Thanks to advances in diffusion [75, 7, 84] and autoregressive [36, 76, 16] architectures, current video models can produce high-fidelity videos maintaining consistent object relations and realistic motion dynamics across frames. This suggests that the models may have internalized substantial visual and structural knowledge about the world. Recent research from Google [70] further hints that, such models are evolving beyond pure content generation: Veo-3 [21] has been shown to perform dozens of distinct vision tasks across perception, modeling, manipulation, and reasoning, without any task-specific training. These emergent capabilities have led researchers to posit that video models could serve as unified, generalist vision models, much like large language models (LLMs) [1, 13, 3, 30] have become foundation models for natural language. Crucially, the sequential nature of video generation provides new perspective on how such models might reason. Each generated frame builds upon the last, creating temporal chain of information propagation. This has been dubbed Chain-of-Frame (CoF) reasoning [70], an analogy to the chainof-thought (CoT) process in LLMs [69, 35, 82, 23, 79] and their multi-modal variants (MLLMs) [12, 4, 40, 31, 10]. In essence, as video model generates sequence of frames, it can iteratively refine and update the scene, thereby working through problem step-by-step in time and space. This CoF concept suggests that, beyond surface-level pattern generation, general-purpose visual reasoning may emerge from video generative models. Figure 1: Overview of Our Study on the Reasoning Potential of Video Models. We investigate whether state-of-the-art video models exhibit emergent reasoning potentials beyond content synthesis. The analysis spans 12 reasoning dimensions under unified perspective, exploring whether large-scale video models can serve as zero-shot visual reasoners via CoF reasoning. However, it remains unclear to what extent current video models truly exhibit reasoning about the content they create. Strong generative performance does not automatically imply robust reasoning potential. Emerging evidence [22, 47, 5, 78] shows that model may produce coherent videos by learning surface-level patterns in the training data, rather than by internalizing general principles. For instance, video model can maintain object continuity yet fail to grasp physical plausibility across long sequence, or it may mimic observed visual sequences without understanding the underlying cause-and-effect relationships. This motivates our central question: Are video models, purely through large-scale visual learning, obtain the zero-shot reasoning potential? To this end, we present the first empirical study to systematically probe the CoF reasoning capabilities of modern video models, spanning 12 dimensions such as spatial, geometric, physical, temporal, and embodied logic, as detailed in 1. We carry out our analysis on Veo-3, which has been systematically examined as zero-shot learner in prior work [70]. Our preliminary observations suggest that current leading video models exhibit comparable reasoning patterns, making Veo-3 representative choice. Our analysis builds on reasoning scenarios distilled from diverse reasoning-oriented benchmarks [25, 67, 45, 71, 29, 34], as well as those we design ourselves, providing compact yet expressive foundation. The prompts for video models are meticulously crafted by transforming the underlying, textual reasoning process of problem-solving into clear, video-presentation format. Each case receives qualitative assessment across three performance levels, i.e., good, moderate, and bad, complemented by quantitative success rate to measure robustness. To standardize evaluation, we curate these tasks into the MME-COF benchmark, as illustrated in Figure 2 and Section 3.2. Leveraging this benchmark, we measure several state-of-the-art video models, i.e., Veo-3 [21], Sora-2 [56], Kling [38], and Seedance [19], to obtain directly comparable scores and qualitative behaviors across categories. Our investigation reveals that the models exhibit promising reasoning patterns in short-horizon spatial coherence, fine-grained grounding, and con2 (a) Evaluation Radar Map. (b) Word Cloud. Figure 2: Illustration of the MME-COF Benchmark. It showcases that different models specialize in distinct reasoning aspects, but most models exhibit limited reasoning capability across all tasks. sistent local dynamics; however, they struggle with complex reasoning conditions, particularly in long-horizon causal consistency, geometric constraint adherence, and abstract logic. Overall, current video models are not yet ready as standalone zero-shot reasoners. Still, they show encouraging signs of emergent reasoning, suggesting strong potential as complementary reasoning agents alongside specialized models. Our main contributions are summarized as follows: Comprehensive Empirical Study. We provide the first investigation of video models (Veo-3) to analyze their visual reasoning potential, detailing representative successes, characteristic errors, and the conditions under which CoF reasoning emerges, holds, or breaks. The MME-COF Benchmark. We curate MME-COF, compact benchmark providing standardized taxonomy and an evaluation protocol aligned with CoF reasoning, enabling consistent and category-wise assessment beyond surface-level visual fidelity. Insights and Directions. We summarize common success patterns (e.g., short-horizon coherence and stable spatial layout) and failure patterns (e.g., long-horizon degradation, violations of basic geometry/physics, and temporal logic), making clear when the behavior reflects genuine reasoning versus pattern replay."
        },
        {
            "title": "2 Deep-Dive Analysis on Veo-3",
            "content": "2.1 Overview To ensure rigorous empirical study, we detail our core methodology in this section, including the taxonomy of reasoning tasks, test case curation process, the standardized style for prompt design, and the analysis setup. Task Taxonomy. To capture different dimensions of reasoning, our study starts from dozens of reasoning-oriented tasks, which can be organized into the following 12 categories: 3 1) Visual Detail Reasoning 7) Rotation Reasoning 2) Visual Trace Reasoning 8) Table and Chart Reasoning 3) Real-world Spatial Reasoning 9) Object Counting Reasoning 4) 3D Geometry Reasoning 10) GUI Reasoning 5) 2D Geometry Reasoning 11) Embodied Reasoning 6) Physics-based Reasoning 12) Medical Reasoning Each category comprises several representative cases selected to test specific aspects of reasoning. Test Case Curation. We recruit five PhD-level experts with deep expertise in text-image reasoning, who are tasked with selecting representative cases from benchmarks [25, 67, 45, 52, 74] corresponding to each task category. For each reasoning case, the experts manually constructed text prompts that explicitly or unambiguously define the target reasoning objective, aiming to evaluate the potential of video models for multi-modal reasoning. Prompt Design Style. To ensure consistency and fairness, all prompts follow unified style emphasizing explicit visual constraints, controlled motion, and minimal linguistic ambiguity. Prompts are encouraged to be written in imperative form and designed to reduce variance from language interpretation, focusing the models behavior on the intended visual reasoning objective. The overall design principles are as follows: 1) Static camera and fixed viewpoint, unless motion is explicitly required by the task. 2) Stable spatial composition, consistent framing, and unchanging scene layout across frames. 3) Clear specification of allowed and disallowed changes (e.g., no zoom, no pan, no dolly) to constrain camera dynamics. 4) Explicit temporal phrasing to control the pace of motion, using cues such as instantly, smoothly, or step-by-step. 5) Avoidance of direct textual hints toward the answer; instructions are purely visual and task-oriented. 6) Inclusion of realistic phrasing and scene context to align with the models natural video priors while minimizing artifacts. The standardized prompt style ensures that differences in output primarily reflect the models internal reasoning potential rather than prompt variability. Analysis Setup. For every reasoning case, we construct text prompt that explicitly or implicitly specifies the target reasoning objective. Each prompt produces six video samples at resolution of 1280720, 24 FPS, and duration of 8 seconds. All experiments are conducted in unified zero-shot setup without fine-tuning, additional supervision, or auxiliary tools. We evaluate model outputs through qualitative judgments along three levels of performance, i.e., Good , Moderate , and Bad, based on the clarity, correctness, and temporal stability of the visual reasoning process. Detailed definitions and examples of these evaluation criteria are provided in the corresponding task subsections. Note that, since we observe that most video models struggle to follow the requirement of static shot reliably, we apply more permissive qualitative criteria for static-shot evaluations. We further define success rate to measure robustness across generations for each case, computed as the proportion of successful samples among the six generated. For cases categorized as Bad, the success rate is always 0. Non-zero success rates only appear in cases evaluated as Good or Moderate , indicating that Veo-3 exhibits some potential to perform as visual reasoner. higher success rate reflects more stable reasoning capability of the model. 4 Figure 3: Showcase of Visual Detail Reasoning by Veo-3. It illustrates Veo-3s ability to localize targets and maintain fine-grained visual attributes across frames, together with common failure modes when targets are small, occluded, or embedded in clutter. 2.2 Visual Detail Reasoning Task Description and Evaluated Aspects. In the visual detail reasoning category, the objective is to assess models ability to discern and maintain fine-grained visual attributes and spatial relations within generated video sequences. It covers attribute recognition, e.g., identifying color, texture or material of an object, and spatial relation identification, e.g., recognizing that one object is on the left of or behind another object. The model is evaluated on the capacity both to attend to the correct target region and to maintain visual consistency, across frames, of the attribute or relation in question. Definition of Good / Moderate / Bad. We define the three-level evaluation criteria as follows: Good: The reasoning video accurately centers on the correct target region, clearly resolves the relevant attribute, such as color, texture or position, and maintains sharp, stable and natural rendering throughout the sequence. There are no visible frame drops, artifacts or unintended motion. Moderate: The region of interest is approximately correct, and the attribute remains inferable, but the sequence suffers from minor blur, incomplete framing, slight instability mild unnatural motion, or sometimes deviates from the textual instruction and produces plausible but unaligned or self-directed visual interpretation, limiting confident interpretation. Bad: The target region is incorrect or ambiguous, the attribute cannot be reliably inferred, or the video exhibits severe artifacts: abrupt frame jumps, major jitter, unintended zoom or crop, extraneous objects interfering, or conspicuous quality degradation that obstructs the reasoning task altogether. Data Source. We sample data from the VBench [71], which provides comprehensive set of evaluation dimensions including spatial relationship and color/attribute consistency tasks. Example and Analysis. We illustrate typical behaviors of Veo-3 in visual detail reasoning through four representative cases in Figure 3. In case I, the model performs well in localizing the target: although it does not strictly execute the zoom in instruction, it instead achieves an equivalent visual outcome through semantically consistent motion with persons hand. This slight deviation suggests that the model may exhibit certain generation preferences in how it interprets and realizes spatial instructions, possibly reflecting stylistic tendencies learned from training data. In cases II and III, the model achieves better success rates when the targets are visually salient and contextually distinct. For the handbag and dog-motorcycle scenes, Veo-3 attends to the correct regions and maintains smooth temporal coherence. However, when the object (e.g., the motorcycle) is small or surrounded by distracting elements, the model occasionally fails to locate it accurately, indicating limited fine-grained spatial discrimination in cluttered scenes. In case IV, when the target object is tiny and visually indistinct, Veo-3 cannot identify it even with explicit positional hints, highlighting that the models perceptual grounding and reasoning weaken sharply when object size and salience are too low for reliable attention. Takeaway 1 Veo-3 performs well in fine-grained attribute and spatial reasoning for salient, well-grounded targets, but fails when objects are small, occluded, or cluttered. It sometimes exhibits stylistic generation biases that lead to plausible yet instruction-divergent outcomes. 2.3 Visual Trace Reasoning Task Description and Evaluated Aspects. The visual trace reasoning category evaluates models ability to represent and maintain causal continuity across sequential actions. Typical tasks include maze navigation, path following, and multi-step object manipulation, where the video must visually encode coherent sequence of intermediate decisions that lead to the correct goal. Performance is assessed based on two major aspects: (i) temporal coherence, which is the smoothness and logical 6 Figure 4: Showcase of Visual Trace Reasoning by Veo-3 (Part I). It shows short-horizon pathfollowing successes, object-grounding failures, and certain bias that causes step omissions/mistakes in multi-step traces. The ground-truth answers of cases II and III are intuitive and non-unique, which are omitted to highlight the key reasoning behaviors. 7 Figure 5: Showcase of Visual Trace Reasoning (Part II) by Veo-3. The examples highlight long-horizon planning breakdowns, inconsistent arrow/trajectory rendering, and failures to preserve comparative or sequential information across frames. progression between consecutive steps; and (ii) goal consistency, which means whether the full sequence visually completes the intended reasoning trajectory without deviation or contradiction. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: Good: Each movement step is depicted continuously and logically toward the correct goal. The motion is smooth, temporally consistent, and follows causal order with no skipping, stuttering, or direction reversal. Moderate: The overall trajectory roughly aligns with the intended sequence, but small discontinuities, timing irregularities, or partial missteps occur. The reasoning path remains interpretable, and the goal can still be inferred. Bad: Key steps are missing, reversed, or illogical. The sequence shows abrupt jumps, inconsistent object trajectories, or goal confusion, breaking the temporal and causal coherence of the reasoning process. Data Source. We select samples from MVoT [41], FrozenLake [8, 72], MiniBehavior [32], RBenchV [25], SpatialViz-Bench [66], and OmniSpatial [29], which provide controlled multi-step environments for evaluating temporal reasoning, sequential planning, and causal continuity in visual simulations. 8 Example and Analysis. In Figure 4 and Figure 5, we showcase six representative visual-trace examples. In case I, the model repeatedly fails to execute the exact step sequence and instead drifts toward visually salient central cell. However, case II is one of the few successes: the model can produce coherent step-by-step path in simple, low-branching settings, but this behavior is not robust across trials. Case III largely fails, where the model often does not ground the specified object (printer), sometimes hallucinating its appearance or placement rather than performing consistent pickup-and-place. Case IV shows near-uniform failure on long-horizon, highly branched navigation: outputs contain wrong turns, discontinuities, and no faithful global plan. Case reveals difficulty grounding abstract movement rules, producing inconsistent arrow trajectories. Case VI produces visually plausible motions along individual paths but fails to preserve or present the comparative information required for contrastive reasoning. Taken together, these examples indicate that the model can simulate locally coherent short traces but systematically fails at long-horizon planning, rule-grounded execution, and object-persistent manipulations. Takeaway 2 Veo-3 can produce locally coherent, short-horizon trace animations in simple, low-branching scenarios, but it does not reliably execute long-horizon plans or rule-grounded sequences. 2.4 Real-World Spatial Reasoning Task Description and Evaluated Aspects. This task investigates Veo-3 [21]s ability to perceive and maintain spatial relations within natural scenes, with focus on reasoning about viewpoint change, orientation consistency, and reference-frame alignment. We assess whether the model preserves stable global coordinate frame and coherent scene orientation under varying viewpoints, and whether objects retain correct relative positions and orientations with respect to each other across different views. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Good: Scene orientation, reference frame, and viewpoint are consistent and correctly represent spatial relations. The camera remains steady and the motion is natural. Moderate: Scene roughly matches the instruction but contains small perspective errors, unnatural transitions, or partial mirroring. Motion remains interpretable but not physically coherent. Bad: Reference frame or direction is wrong; viewpoint shifts abruptly or inconsistently. Video suffers from strong camera drift, disorienting motion, or spatial chaos. Data Source. To evaluate on orientation and layout reasoning, we specifically sample data from MMSI-Bench [74]. Also, the tasks of perspective taking and spatial interaction are selected from the OmniSpatial dataset [29]. Example and Analysis. As shown in Figure 6, Veo-3 can correctly handle basic spatial layouts in case I, but struggles with complex viewpoints or orientation changes in case II. The perspective transformations are sometimes inaccurate or even incorrect, suggesting that the model tends to prioritize visual plausibility over precise spatial reasoning, which hinders further reasoning in case IV. Moreover, case III demonstrates that Veo-3 has difficulty understanding depth, further limiting its spatial reasoning capability. Takeaway 3 While Veo-3 exhibits an emerging ability for simple real-world spatial reasoning, its capability remains insufficient for handling more complex spatial understanding tasks. 9 Figure 6: Showcase of Real-World Spatial Reasoning by Veo-3. Although Veo-3 can reason about simple spatial layouts, it still struggles to maintain consistency under complex perspective or orientation changes. 10 Figure 7: Showcase of 3D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows certain potential in basic 3D geometry reasoning, its performance remains unstable for complex geometry transformations. 11 Figure 8: Showcase of 3D Geometry Reasoning by Veo-3 (Part II). The model often generates misaligned or self-intersecting structures, compromising geometric consistency. 2.5 3D Geometry Reasoning Task Description and Evaluated Aspects. We also evaluate Veo-3s potential on 3D geometry reasoning tasks, such as geometric object motion and three-dimensional structural transformations like reconstructing cube net. The assessment focuses on three key dimensions: geometric accuracy, structural completeness throughout the transformation, and visual continuity across frames. Definition of Good / Moderate / Bad. We categorize the models performance into three levels: Good: Transformations like folding, rotation and assembly are geometrically correct, visually smooth, and continuous, maintaining structural integrity and realistic motion. No broken edges, jumps, or spatial artifacts. Moderate: Transformations are partially correct but show local misalignment, unrealistic deformation, or discontinuous motion; geometry is roughly interpretable but imperfect. Bad: Transformation fails. For example, wrong fold, structure collapse, or impossible geometry. Motion is erratic, discontinuous, or visually implausible, breaking the sense of physical realism. Data Source. To construct diverse and representative evaluation data, we adapt tasks from established geometric spatial reasoning datasets, including the 3D-Text-Instruct and Folding Nets subsets of the STARE benchmark [43], the BlockMoving subset from the SpatialViz-Bench [66], as well as VisuLogic [73] benchmark. Example and Analysis. We showcase the results of Veo-3 on 3D geometry reasoning tasks in Figure 7 and Figure 8. Veo-3 demonstrates degree of potential on 3D geometry reasoning, performing reasonably well on simple, single-step geometric transformations, as shown in case I. However, its performance degrades noticeably when facing multi-step or compositionally complex transformations in case II. As presented in cases III and V, the model frequently produces misaligned or self-intersecting structures, leading to loss of geometric consistency. Further observations in case IV, show that while the model can partially understand the geometric shape of individual objects, it lacks coherent understanding of coordinate systems and the spatial relationships among multiple objects. Takeaway 4 Veo-3 exhibits emerging reasoning potential on basic 3D transformations but breaks down on complex or multi-step geometry, often yielding misaligned or self-intersecting structures. Its Takeaway 5 3D geometric reasoning remains fragile, revealing substantial gaps in its ability to function as reliable 3D geometry reasoner. 2.6 2D Geometry Reasoning Task Description and Evaluated Aspects. To assess models competence in 2D geometric reasoning, we evaluate its zero-shot performance on planar geometric construction tasks. These tasks involve drawing geometric relations by connecting points, adding auxiliary lines, and moving geometric shapes. The evaluation focuses on whether the generated constructions or movements accurately reflect the described geometric relationships and adhere to the given instructions, while maintaining smooth, stable operations that ensure visual clarity and coherence throughout the process. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: Good: Constructions and movements are geometrically accurate and visually smooth. Endpoints, intersections, angles, and motion trajectories align correctly with the instructions. Both drawing and movement processes are stable, fluid, and natural, resembling human sketching or manipulation. Moderate: Constructions and movements roughly follow the intended geometry but exhibit minor inaccuracies in line placement, shape alignment, trajectory, or smoothness. Some local jitter or abrupt motion may appear, but the overall structure and motion remain interpretable. Bad: Constructions or movements deviate substantially from geometric correctness. Lines or shapes may be misplaced, disconnected, or moved in chaotic or discontinuous manner (e.g., jittering, overlapping, or distorted paths), leading to visual instability and loss of interpretability. Data Source. The evaluation data are drawn from multiple established sources, including the Geo170k dataset [18], the VarsityTutors subset of Math-PUMA [85] dataset, the line-connection subset of RBench-V [25], the MAVIS-Gen [80], Tangram Puzzle subsets of the STARE [43] benchmark, and data from VAT [46]. Example and Analysis. The representative examples of the 2D geometry reasoning task are presented in Figures 9 and 10. Veo-3 demonstrates foundational capability for simple geometric connection tasks, correctly identifying and linking elements in straightforward scenarios like in case III. However, this basic competence is inconsistent. The model often prioritizes producing visually symmetric or semantically meaningful patterns rather than strictly adhering to geometric instructions (cases and II). Furthermore, case II reveals instances where the model unintentionally modifies the original figures, indicating limited awareness of geometric constraints and poor spatial consistency. When tackling more complex connection tasks, the model frequently fails to interpret the intended drawing order or point indices, resulting in incorrect connection sequences, as demonstrated in cases V, VI, and VII. This is often coupled with an inability to control task termination, as the model tends to continue drawing beyond the required constructions. Finally, for tasks involving the movement of geometric shapes in cases IV and VIII, the model struggles to maintain geometric structural consistency throughout the motion. Takeaway Veo-3 shows initial 2D geometric reasoning ability but still falls short of consistent, constraintaware geometric understanding, remaining far from robust geometric reasoner. 13 Figure 9: Showcase of 2D Geometry Reasoning by Veo-3 (Part I). While Veo-3 shows potential in recognizing simple patterns, it lacks the robust constraint awareness essential for accurate geometric manipulation. 14 Figure 10: Showcase of 2D Geometry Reasoning by Veo-3 (Part II). Veo-3s reasoning abilities are further challenged by complex sequential instructions and the need to preserve structural integrity. Figure 11: Showcase of Physics-based Reasoning by Veo-3. The physics scenarios demonstrate locally plausible dynamics and reflections, alongside systematic quantitative and causal inconsistencies under frictional, force-driven, or constrained interactions. 16 2.7 Physics-based Reasoning Task Description and Evaluated Aspects. The physics-based reasoning category assesses models capacity to depict and reason about motion dynamics, physical causality, and rule-based interactions between objects. Tasks in this group involve gravity, collisions, reflection, momentum, or energy conservation, requiring the model to generate physically plausible and temporally coherent motion. Evaluation focuses on two complementary aspects: (i) physical plausibility, which means whether the simulated motion obeys common physical principles; and (ii) causal correctness, which is whether object interactions are consistent with the underlying cause-and-effect relationships described in the prompt. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: Good: The motion sequence adheres to physical laws such as gravity, momentum, and energy conservation. Object interactions are realistic and temporally smooth, and the visual outcome remains coherent and credible throughout. Moderate: The physical relations are approximately correct but include minor inconsistencies, such as irregular acceleration, timing mismatch, or slight violation of conservation. The overall motion remains interpretable and visually plausible. Bad: The motion is physically implausible or visually chaoticobjects float, stop abruptly, or behave contrary to basic causal principles. Severe artifacts or temporal discontinuities disrupt the perception of coherent physical process. Data Source. We draw samples from MMMU [77], ScienceQA [49], and related physical reasoning subsets of RBench-V [26] and SpatialViz-Bench [66], covering scenarios such as object collisions, pendulum motion, frictional sliding, and optical or magnetic interactions. Example and Analysis. Figure 11 presents four representative physics tasks and their outputs. Case shows that the model can produce visually coherent slide, but the behavior violates basic physical laws. Case II is the most reliable, where reflections and general trajectory shape are rendered plausibly and the task attains high success rate, although small angular or timing offsets are common. In case III, the model conveys attraction through motion, yet the depicted dynamics do not reliably track the intended force magnitudes or causal ordering. Finally, case IV exposes structural failures, incorrect meshing, inconsistent relative rotations, and nonphysical contact behavior occur frequently, so the mechanical constraints are not respected. Overall, the model can synthesize locally plausible dynamics and handle simple reflection rules, but it fails to maintain quantitative physical constraints and causal fidelity in frictional, force-driven, or mechanically constrained scenarios. Takeaway 7 Veo-3 often generates visually plausible short-term dynamics, but it systematically fails to preserve quantitative physical constraints (energy, momentum), causal ordering, and contact mechanics in frictional, force-driven, or mechanically constrained scenarios. Thus, its outputs are somewhat useful for qualitative illustration but are not reliable for quantitative physics inference or causal prediction. 2.8 Rotation Reasoning Task Description and Evaluated Aspects. The rotation reasoning task assesses the ability to reason about planar object rotation and maintain consistent spatial grounding under rotational transformations, thereby supporting subsequent reasoning processes. In each instance, the model is required to accurately rotate target objects within fixed 2D plane while preserving the overall scene structure and structural consistency, followed by performing reasoning tasks like grounding and OCR. The evaluation focuses on both the accuracy of the rotation in terms of angle and direction, and the precision of the resulting reasoning tasks. 17 Figure 12: Showcase of Rotation Reasoning by Veo-3. Veo-3 struggles in complex scenes. However, its foundational grasp of simple rotations signals its potential to support rotation-based reasoning tasks. 18 Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: Good: The rotation is accurate, complete, and strictly confined to the 2D plane, with no extraneous scene motion. The following reasoning tasks are completed correctly. Target objects remain precisely grounded after rotation. Moderate: The rotation is largely correct but may be incomplete or slightly off-angle, though still confined to the 2D plane. The following reasoning tasks are mostly completed. Minor temporal or visual inconsistencies may appear, but do not alter the core 2D structure or object grounding. Bad: The model fails to perform the correct rotation, extends the transformation into 3D space, or introduces substantial scene distortion. Cannot complete the following reasoning task. The original 2D structure is altered, leading to inaccurate grounding of the target objects. Data Source. To specifically assess the rotation reasoning task, we recruit some PhD-level experts with deep expertise in text-image reasoning to design the evaluation data manually, followed by the necessary review process, as mentioned in Section 3.2. Each question is designed following the principle that it must involve 2D rotation to reach the correct solution, ensuring the task genuinely probes rotational understanding rather than simple visual matching. Moreover, we sample data from the 2DRotation subset from the SpatialViz-Bench [66], and reformulate the question into instructions for the video models. Example and Analysis. The results are shown in Figure 12. In case I, we find that Veo-3 handles small-angle rotations and simple planar scenes reasonably well, demonstrating basic grasp of rotational motion. However, in more complex scenarios like cases II, III, and IV, the model often ignores the 2D rotation constraint and inadvertently alters the 3D structure, resulting in incorrect rotations and degraded spatial grounding. Such errors frequently propagate to downstream tasks, such as OCR in case III, or object localization in case II, due to inconsistencies in post-rotation alignment. These observations suggest that the reasoning behavior of Veo-3 remains more pattern-driven rather than principle-driven. However, as it demonstrates partial understanding of planar rotation, this can to some extent facilitate subsequent reasoning tasks. Takeaway Veo-3 exhibits only superficial understanding of rotation reasoning. While it can approximate small planar rotations, it fails to preserve geometric consistency under larger or compound transformations. 2.9 Table and Chart Reasoning Task Description and Evaluated Aspects. The table and chart reasoning task requires the model to identify and focus on the key elements within visualizations or tabular data. For evaluation, we further consider how effectively the model identifies the regions relevant to the query and whether it can transition smoothly and visually coherently to these areas, preserving clarity, continuity, and proper scaling. Definition of Good / Moderate / Bad. We rate the performance according to the following criteria: Good: Camera precisely focuses on the correct chart or table segment, smoothly highlighting or zooming into the queried data (e.g., correct year, category, or value). Motion is continuous, the chart and table remain clear, and no distortion or overexposure occurs. Moderate: Camera approximately focuses on the right region but partially misses boundaries, introduces slight blur, or transitions abruptly. Data can still be inferred. Bad: Video fails to locate the correct region or changes the chart or table geometry unnaturally. Motion jitter, scaling errors, or artifacts make data unreadable or misleading. 19 Figure 13: Showcase of Table and Chart Reasoning by Veo-3. Veo-3 demonstrates an initial ability to focus on relevant data regions but lacks the precision and consistency required for reliable visual analysis. 20 Data Source. We use samples from the ChartQA [52] dataset and TableVQA-Bench [34]. Example and Analysis. For charts, as presented in cases I, II and III in Figure 13, Veo-3 can often zoom into an approximately correct region but lacks the precision needed to accurately locate the queried data. For tables, as shown in case IV, Veo-3 fails to correctly identify the required element and tends to select entries randomly. The model also frequently adds, modifies, or distorts existing chart and table elements, resulting in visual inconsistencies that undermine the accuracy of chart interpretation. Takeaway 9 Veo-3 demonstrates emerging competence and potential in structured visual understanding, but still falls short of functioning as precise and reliable chart-table reasoner. 2.10 Object Counting Reasoning Task Description and Evaluated Aspects. In this category, we focus on the ability to accurately enumerate objects within 2D or 3D scene. In each instance, the model is required to identify, ground, and count target objects, typically by highlighting, drawing bounding boxes, applying numerical labels, or panning. The evaluation focuses on the accuracy of the count and the precision of the spatial grounding, performed within scene that remains static or experiences only minimal motion, ensuring the counting process is not influenced. Definition of Good / Moderate / Bad. Model outputs are categorized into three quality levels: Good: The model precisely highlights, draws bounding boxes around, or labels the objects with correct numbers, and performs smooth and controlled panning when necessary to cover all targets. Motion is continuous, and the scene remains static or experiences only slight changes that do not influence the counting process. Moderate: The model approximately highlights or draws bounding boxes around the objects, or performs panning with minor instability or incomplete coverage. Objects or the scene may move or change slightly, but this does not strongly affect the counting process. Bad: The model fails to correctly highlight, label, or draw bounding boxes around the objects, or pans erratically such that parts of the scene are missed or revisited unnecessarily. Objects or the scene move or change substantially, severely affecting the counting process. Data Source. The 2D object counting data are sampled from the counting subset of RBench-V [25]. The 3D object counting data are from the Super-CLEVER dataset [45] and VAT [46]. Example and Analysis. The results are shown in Figures 14 and 15. In the 2D counting tasks from cases to III, objects frequently move or change during the process, negatively impacting counting stability and accuracy. In the 3D counting tasks, Veo-3 successfully handles simple grounding and counting scenarios, as demonstrated in case V, but struggles with scenes involving complex materials or geometric variations in cases VI and VII, leading to inaccurate counts. Additionally, in the panning process of case VII, the camera fails to precisely move to the regions containing all target objects, further hindering the counting process. Takeaway 10 Veo-3 demonstrates basic counting capability but lacks the spatial control and robustness required for reliable object enumeration in dynamic or complex scenes. 21 Figure 14: Showcase of 2D Object Counting Reasoning by Veo-3. Veo-3s lack of spatial control often introduces object motion, undermining the stability and accuracy of the counting process. 2.11 GUI Reasoning Task Description and Evaluated Aspects. In the Graphical User Interface (GUI) reasoning task, we focus on the capability to understand and interact with graphical user interfaces across different operating systems, including Android, Linux, and Web environments. In each instance, the model is required to perform actions, such as clicking on specific UI elements. The evaluation focuses on the accuracy of the click and the temporal coherence of the interaction, ensuring the scene and irrelevant UI elements remain consistent. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Good: The click is precise, with no extraneous actions. No superfluous icons appear, and the original data and icons remain unchanged. Moderate: The click is precise but may be accompanied by minor extraneous actions. Superfluous icons might appear but do not obscure the click target, and original data or icons show only slight alterations. 22 Figure 15: Showcase of 3D Object Counting Reasoning by Veo-3. Veo-3s basic 3D counting abilities are challenged by complex materials, geometric variations, and imprecise camera control. 23 Figure 16: Showcase of GUI Reasoning by Veo-3. Veo-3s attempts at graphical interface interaction exhibit visual inconsistencies and logical inaccuracies, indicating only shallow grasp of underlying GUI logic. Note that the answer to each question is bounding box. For visual clarity, screenshots with the ground-truth bounding boxes are shown. Bad: The click is imprecise or erratic. Original data and icons are significantly altered, hindering judgment and assessment. Data Source. The Linux data are selected from the Common Linux Screenshot subset of ScreenSpotPro [42], while the Android and Web data are drawn from the OS Android and OS Web subsections of MMBench-GUI [67], respectively. Example and Analysis. Across the three cases in Figure 16, Veo-3 fails to accurately capture the correct click position and often exhibits inconsistencies between the click location and the resulting on-screen effect. In addition, it occasionally alters or generates new icons and text, which can interfere with judgment. In the Web system in case III, however, the model demonstrates partial GUI responsiveness and provides some degree of visual feedback. 24 Figure 17: Showcase of Embodied Reasoning by Veo-3. It illustrates plausible static affordance detection in simple settings, common workaround/hallucination behaviors for dynamic manipulations, and failures to reliably localize or preserve manipulation-relevant context. Green points in the answer image denote ground-truth points or trajectories. Takeaway 11 Veo-3 demonstrates limited awareness of GUI click actions, imitating interaction behaviors without fully grasping the underlying functional logic. 2.12 Embodied Reasoning Task Description and Evaluated Aspects. This category evaluates the models potential to perceive and reason about object affordances and manipulation dynamics. It involves recognizing both static and dynamic affordances, as well as identifying manipulation-relevant object and scene attributes. Evaluation focuses on two aspects: (i) the generation of stable and contextually relevant visual sequences, and (ii) the maintenance of reasoning fidelity without resorting to implausible planning shortcuts or hallucinated interactions. 25 Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Good: The sweep/framing covers all candidates fairly (equal or near-equal dwell), centers the manipulation-relevant geometry (e.g., handle + frame/gap, lid-body interface, hinge side) with crisp focus and stable scale; no cropping of key context; no content alterations. Moderate: The view roughly includes the right region(s) but with minor bias or coverage issues: slight off-center, brief under-exposure of one candidate, small motion jitter, or shallow context (still enough to infer). Bad: The camera misses or biases the evidence (e.g., lingers only on one point, crops away the hinge/rail, over-zooms non-relevant patch), introduces distortion/content edits, or produces footage from which fair decision cannot be made. Data Source. We select samples from Robobench [51] for the analysis. In addition to general understanding of static attributes, we also sample data to assess whether Veo-3 can perform direct reasoning on tasks involving the generation of static and dynamic affordances. Example and Analysis. As shown in Figure 17, Veo-3 demonstrates the ability to comprehend objects within real-world scenes. However, its capacity for assisting visual reasoning in embodied scenarios remains constrained by insufficient stability. As illustrated in case I, when provided with clearly defined object for manipulation, Veo-3 is capable of generating plausible manipulation affordances. When it comes to dynamic affordances, Veo-3 tends to employ workarounds to compensate for its planning deficiencies, as evidenced in case II, where it generated new cucumber instead of the intended object. With respect to static attributes, Veo-3 struggles to accurately differentiate visual prompts and misidentifies the position of containers. As shown in case III, the green box, intended to specify the location of the container, inadvertently led Veo-3 to produce hallucinations. Takeaway 12 Veo-3s capabilities are currently limited to basic object recognition rather than true embodied reasoning. It lacks the necessary planning and stability to reliably interpret and act upon dynamic or spatially constrained instructions, indicating its limitations in understanding and reasoning of real-world interactions. 2.13 Medical Reasoning Task Description and Evaluated Aspects. This category assesses the models ability to localize lesions or structures, identify relevant attributes (e.g., side, lobe), recognize pathological patterns (e.g., jump distribution), and make binary decisions (e.g., presence or absence). The evaluation focuses on both the correctness of object manipulation and the visual stability of the surrounding regions. Definition of Good / Moderate / Bad. We define the evaluation criteria in three levels: Good: The camera cleanly settles on the correct anatomical level/lesion, with clear margins and readable context; motion is reasonable; no geometric distortion or content alteration. Moderate: The view roughly covers the right area but is slightly off (partial coverage, mild blur, small framing mistakes). The general shape of the tissue or organ can still be observed. Bad: The video misses the target region or introduces distortions/crops that hide key cues. Tissues or organs begin to distort. Misleading results due to confusion of medical terminology. Data Source. We select samples representing different body parts from the ViTAR [9] dataset. Example and Analysis. We showcase the evaluation results in Figure 18. Veo-3 retains the ability to manipulate images when dealing with medical images. However, due to its lack of medical 26 Figure 18: Showcase of Medical Reasoning by Veo-3. As shown in cases and III, Veo-3 fails to maintain the shape of the rest of medical organization. Veo-3 also can not understand and precisely locate the mentioned medical terminology in the prompt, as demonstrated in case II. knowledge, Veo-3 struggles to accurately manipulate the correct objects when instructions include medical terminology. This phenomenon is evident across all cases. Furthermore, Veo-3 cannot model medical organs effectively. When performing operations such as zooming in, the medical images suffer from significant distortion, resulting in substantial loss of detail. Takeaway 13 Veo-3s failure to handle the reasoning in the medical domain, causing distortion even on simple zoom-ins, highlights its limited grasp of specialized, non-general knowledge. 27 Table 1: Key Statistics of MME-COF. Statistic Number Total entries Total categories Max prompt length Avg prompt length Max entries per category Avg entries per category 59 12 124 36.7 7 4. Figure 19: Category Distribution."
        },
        {
            "title": "3 MME-COF",
            "content": "3.1 Benchmark Overview To standardize the empirical study and systematically evaluate the reasoning potential of state-of-theart generative video models [21, 55, 56], we introduce MME-COF, which, to our knowledge, is the first benchmark specifically designed to reveal and quantify the reasoning potential of video models. 3.2 Benchmark Composition Data Curation and Distribution. Aligning with the task taxonomy in Section 2.1, the MME-COF benchmark is curated from the cases used in our empirical study. It comprises 59 curated entries and instruction prompts spanning 12 diverse reasoning categories. The key statistics of MME-COF and its overall composition are summarized in Table 1, Figure 2b and Figure 19. Review Process. Following the prompt design protocol in Section 2.1, all prompts undergo two-stage review process. In the cross-validation phase, each prompt was independently reviewed by another expert to ensure semantic clarity, alignment with the intended reasoning task, and the absence of linguistic bias. In the final adjudication phase, discrepancies were jointly discussed and resolved through consensus. This multi-step procedure ensured that every prompt was conceptually precise, visually grounded, and fully aligned with the evaluation objectives of MME-COF. 3.3 Evaluation Protocol Models and Generation Settings. We evaluate the leading video models in zero-shot setting, including Kling-v1 [38], Seedance-1.0-pro [19], Veo-3.0-preview [70], Veo-3.0-fast [70], Sora-2 [56], Sora-2-pro [56]. Each model generates six video samples per prompt, and final scores were computed as the mean across all samples. All videos are generated at 16:9 aspect ratio. We adopt the default 8-second duration for the Sora and Veo series, while retaining the default 5-second length for Kling and Seedance. Note that, since most video models apply automated safety filters and content moderation, which may block sensitive content, we exclude videos that are suppressed by such filters from our evaluation. Evaluation Metrics. We employ Gemini-2.5-Pro [12] as an automatic verifier to evaluate each generated video. Gemini is prompted with the following evaluation criteria and returns structured scores between 0 and 4, where higher values indicate better performance: 1) Instruction Alignment (0-4): Measures how well the video follows the described structure and sequence in the prompt. high score indicates that the visual steps faithfully reflect the textual instructions. 2) Temporal Consistency (0-4): Evaluates the smoothness and continuity between frames. Disjointed or abrupt transitions will lead to lower score. Table 2: Model-level Overall and Per-dimension Performance on MME-COF. Mean scores and standard deviations are reported on 04 scale, as graded by Gemini-2.5-Pro. Model Overall Instruction Alignment Temporal Consistency Visual Stability Content Fidelity Kling-v1 [38] Seedance-1.0-pro [19] Veo-3.0-fast [21] Veo-3.0-preview [21] Sora-2-pro [56] Sora-2 [56] 0.64 0.91 1.41 1.51 1.44 1.51 1.45 1.50 1.66 1.53 1.72 1.59 0.01 0.09 0.30 0.86 0.56 1.09 0.54 1.06 0.48 0.96 0.59 1.12 0.15 0.75 1.65 1.57 1.37 1.51 1.43 1.53 1.36 1.59 1.52 1.69 2.43 1.86 2.00 1.72 1.88 1.73 1.89 1.71 2.39 1.65 2.32 1. 0.21 0.79 1.13 1.65 1.10 1.52 1.12 1.49 1.64 1.72 1.62 1.75 Focus Relevance 0.43 1.07 1.98 1.75 2.27 1.69 2.26 1.73 2.44 1.73 2.52 1.71 Table 3: Per-category Scores on MME-COF. Mean scores and standard deviations are reported on 04 scale, as graded by Gemini-2.5-Pro. Category Kling-v1 [38] Seedance-1.0 Pro [19] Veo-3.0 Fast [21] Veo-3.0 Preview [21] Visual Detail Visual Trace Real-world Spatial 3D Geometry 2D Geometry Physics-based Rotation Table & Chart GUI Object Counting Embodied Medical 0.72 0.69 0.49 0.65 0.77 0.76 0.61 0.58 0.49 0.67 0.60 0.62 0.22 0.34 0.87 0.72 1.09 0.51 0.64 0.58 0.80 0.00 1.15 1.17 1.37 1.39 1.23 1.13 1.79 1.53 1.95 1.64 0.96 1.11 1.27 1.25 2.30 1.46 0.71 1.18 0.70 0.76 1.15 0.97 1.82 1.67 1.56 1. 1.10 1.24 1.43 1.26 2.07 1.54 1.71 1.54 1.18 1.15 1.44 1.39 1.83 1.44 0.82 1.30 1.11 1.09 2.03 1.42 1.33 1.57 0.27 0.39 1.59 1.68 1.48 1.24 2.10 1.46 1.54 1.43 1.27 1.20 1.44 1.35 1.60 1.29 0.96 1.44 1.18 0.89 1.84 1.42 1.18 1.46 0.30 0.58 Sora-2 [56] 1.14 1.32 1.51 1.37 1.84 1.43 1.37 1.49 1.77 1.45 2.13 1.32 1.62 1.37 1.84 1.61 1.88 1.64 2.06 1.48 1.30 1.51 2.08 1.56 Sora-2 Pro [56] 1.08 1.89 1.75 1.31 1.77 1.35 1.42 1.45 1.77 1.21 2.10 1.33 1.44 1.28 1.48 1.59 1.52 1.48 1.86 1.41 1.40 1.42 1.81 1. 3) Visual Stability (0-4): Assesses the stability of the video in terms of camera motion, object appearance, and scene composition. Shaky or glitchy outputs are penalized. 4) Content Fidelity (0-4): Determines how accurately the key elements described in the prompt are preserved. Hallucinated or missing objects/events will reduce the score. 5) Focus Relevance (0-4): Examines whether the videos visual attention remains focused on the correct objects or regions throughout. Irrelevant distractions or poorly framed targets are penalized. We adopt direct prompting strategy, instructing Gemini with the prompt, videos, and evaluation criteria to produce numerical scores in JSON format directly. 3.4 Quantitative Results and Analysis We report the quantitative scores of the five evaluated models across the five reasoning dimensions in Table 2, and provide detailed per-category results in Table 3 and Figure 2a. Overall, most models exhibit limited reasoning capability across all tasks in MME-COF, reflected by generally low scores. Among the five dimensions, Visual Stability achieves the highest average, indicating that current video models can generate smooth and coherent sequences. Yet, their behavior remains largely at the level of pattern replay rather than genuine reasoning. The Sora-2 series [56] shows relative advantages in physics-based, embodied, and medical reasoning, while the Veo-3.0 series [21] performs comparatively better in real-world spatial reasoning. Seedance-1.0-pro [19] demonstrates relative strength in rotation and 3D geometry reasoning. These trends suggest that different models specialize in distinct reasoning aspects. However, their mean scores remain below 2.0 out of 4, highlighting substantial room for improvement and pointing to opportunities for more targeted enhancement in future development."
        },
        {
            "title": "4 Related Work",
            "content": "Video Models. Video models have been progressively evolving both in the fields of video understanding and generation. For video understanding methods, earlier approaches, such as MViT [14], Video Swin Transformer [48], and VideoMAE [62], aim to learn robust representation that fosters downstream tasks. With the rise of LLMs, recent approaches encode videos as tokens and exploit the language backbone for captioning [61], event localization [59], and high-level reasoning [28, 83]. Video generation models have also attracted much attention. Closed system, including OpenAIs Sora [55, 56], Runways Gen-3 [58], Pika Labs [57], Luma AI [50], and Google DeepMinds Veo series [20, 21], have exhibited impressive results. However, they remain inaccessible due to their closed-source nature. Open-source alternatives have recently become available: Stable Video Diffusion [6] introduces efficient training strategies, Hunyan-Video [37] proposes systematic scaling, and Wan-2.1 [64] presents an efficient 3D VAE with expanded pipelines. Reasoning with Video. The advent of large reasoning models [24, 60, 27, 69], such as OpenAI o1 [54] and DeepSeek-R1 [23], has spurred the development of video reasoning benchmarks. Most current methods [15, 44, 53] employ MLLMs specialized in video reasoning understanding. For example, Video-R1 [15] specifically targets temporal reasoning capabilities by introducing temporal group relative policy optimization (GRPO) loss. VideoChat-R1 [44] focuses on spatio-temporal reasoning abilities by training with GRPO and rule-based rewards. two-stage training strategy, combining SFT and RL, is used by VideoRFT [65]. When trained on vast collections of images and videos, this strategy boosts the models ability to handle QA tasks, whether in general contexts or reasoning-focused ones. These methods primarily focus on enhancing specific types of question-answering or captioning tasks. Concurrently, [70] demonstrates the large potential of video generative models in video reasoning. These models have implicitly acquired world knowledge throughdemonstrates impressive performance on various tasks, includinging and reasoning capability. Yet, this direction has rarely been explored and only experimented with in zero-shot settings. Evaluation of Video Models as Zero Shot Learner. Recently, several works have been exploring the zero-shot capability of video generation models in various domains, including general-purpose vision understanding [70, 17], medical imaging [39], and world models [68]. [70] conducts experiments on Veo 3 with variety of vision tasks that have not been explicitly included during training. The video model showcases surprising performance on multiple tasks like object segmentation, image editing, and even maze solving. [39] later adopts similar paradigm to medical images understanding tasks and finds video generation models also show powerful capabilities, e.g., delineation of anatomical structures in CT scans, medical image segmentation, and even forecasting of future 3D CT phases. Besides, [68] shows that video generation models could also understand complex temporal causality and world knowledge in the real world, thereby serving as world model [2, 33]."
        },
        {
            "title": "5 Conclusions and Insights",
            "content": "Video models demonstrate an intuitive understanding of the simple visual world. Recent video models can generate high-fidelity videos with realistic motion dynamics, suggesting that they have internalized substantial visual and structural knowledge about the world. Through qualitative results from our empirical study and quantitative results from the MME-COF benchmark, our work confirms that these models do exhibit intuitive yet local reasoning potential. This emergent behavior, which aligns with the Chain-of-Frame (CoF) mechanism, is revealed across several common success patterns. (i) Fine-grained Grounding. Models demonstrate capability for fine-grained attribute and spatial grounding, especially when targets are visually distinct, as presented in visual detail reasoning tasks. (ii) Short-horizon Trace Consistency. In Visual Trace Reasoning tasks, models can maintain short-term consistency in visual traces. (iii) Emergent Tool-Use Simulation. An emergent ability to follow CoF instructions that mimic tool-use is presented, such as drawing lines in 2D geometry, highlighting targets in object counting, or controlling the camera in table and chart reasoning. (iv) Foundational Spatial and Geometric Grasp. This includes single-step 3D geometry transformations, understanding basic real-world spatial layouts, finding coherent sequential paths, and handling small-angle Rotations. (v) Preliminary Real-world Interaction. Models display preliminary comprehension of real-world interaction, generating coherent manipulation paths in embodied reasoning. 30 Complex visual reasoning reveals fundamental limitations. However, visual reasoning demands more than these foundational skills. It tests models ability to maintain long-horizon logical consistency, adhere to abstract constraints, and understand functional principles. In these complex areas, our study reveals fundamental limitations and several common failure patterns. (i) Causal and Physical Logic. This is evident in physics-based reasoning, where the model generates implausible motion that violates basic causal principles, and in visual trace reasoning, where the generated sequences break causal order with illogical steps. (ii) Long-horizon and Rule-grounded Reasoning. In visual trace reasoning, models fail to maintain state and adhere to task-specific rules over extended sequences. (iii) Geometric and Spatial Logic. Models fail at multi-step or complex transformations in 3D/2D geometry and real-world spatial tasks, often breaking constraints or prioritizing visual plausibility over correctness. (iv) Functional and Interaction Logic. They merely imitate GUI actions without grasping their purpose and lack the necessary planning and stability for reliable Embodied tasks, often resorting to workarounds. (v) Perceptual Precision and Specialized Knowledge. This weakness appears when models fail to identify small or indistinct targets in visual detail reasoning, distort data in table and chart tasks, and fail to process specialized medical imagery due to lack of domain understanding. Current video models are not yet ready as standalone zero-shot reasoners. Overall, our findings show that current video models are not yet reliable as standalone zero-shot reasoners. Strong generative performance does not automatically imply robust reasoning during inference. The models behavior appears to be driven more by learning surface-level patterns and correlations rather than by internalizing general principles. It excels at short-term coherence rather than long-horizon causality. This is evident when the model prioritizes visual plausibility over precise spatial reasoning, or favors visually symmetric patterns over strictly adhering to geometric instructions. This tendency to produce plausible but instructionally flawed outputs reveals reasoning process that is pattern-driven, not principle-driven, thereby undermining its ability to function as standalone zero-shot reasoner. The potential in advancing next-generation collaborative visual reasoning. Despite these limitations, the emergent behaviors observed in video models signal strong potential. The CoF concept suggests novel modality for reasoning through visual problems step by step. While these models are not yet robust standalone reasoners, their foundational capabilities demonstrate that they can be guided through carefully designed prompts. This suggests path where video models exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [5] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378, 2025. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent 31 diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [9] Kaitao Chen, Shaohao Rui, Yankai Jiang, Jiamin Wu, Qihao Zheng, Chunfeng Song, Xiaosong Wang, Mu Zhou, and Mianxin Liu. Think twice to see more: Iterative visual reasoning in medical vlms. arXiv preprint arXiv:2510.10052, 2025. [10] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought reasoning. arXiv preprint arXiv:2506.05331, 2025. [11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 68246835, 2021. [15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [16] Zhanzhou Feng, Qingpei Guo, Xinyu Xiao, Ruihan Xu, Ming Yang, and Shiliang Zhang. Unified video generation via next-set prediction in continuous domain. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1942719438, 2025. [17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CVPR 2025 Highlight, 2024. [18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [19] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [20] Google DeepMind. Veo 2, 12 2024. Accessed: 2024. [21] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025. [22] Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, and Ruihua Song. Etva: Evaluation of text-to-video alignment via fine-grained question generation and answering. arXiv preprint arXiv:2503.16867, 2025. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 32 [24] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [25] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: primary assessment for visual reasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025. [26] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, and Shi-Min Hu. Rbench-v: primary assessment for visual reasoning models with multi-modal outputs. 2025. [27] Ziyu Guo*, Renrui Zhang*, Chengzhuo Tong*, Zhizheng Zhao*, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. CVPR 2025, 2025. [28] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [29] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. [30] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix, and William El Sayed. Mistral 7b, 2023. [31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Martn-Martn. Mini-behavior: procedurally generated benchmark for long-horizon decisionmaking in embodied ai. arXiv preprint arXiv:2310.01824, 2023. [33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Huang Gao, and Jiashi Feng. How far is video generation from world model? physical law perspective. arXiv preprint arXiv:2406.16860, 2024. [34] Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. [35] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [36] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [38] Kuaishou Technology. Kling ai: Next-generation ai creative studio. https://klingai.com/, June 2024. [39] Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, and Xiaofeng Yang. Are video models emerging as zero-shot learners and reasoners in medical imaging? arXiv preprint arXiv:2510.10254, 2025. 33 [40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [41] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [42] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. [43] Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay Krishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations. arXiv preprint arXiv:2506.04633, 2025. [44] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [45] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1496314973, 2023. [46] Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, and Yang Liu. Visual abstract thinking empowers multimodal reasoning. arXiv preprint arXiv:2505.20164, 2025. [47] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. [48] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 32023211, 2022. [49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [50] LumaLabs. Dream machine, 06 2024. Accessed: 2024. [51] Yulin Luo, Chun-Kai Fan, Menghang Dong, Jiayu Shi, Mengdi Zhao, Bo-Wen Zhang, Cheng Chi, Jiaming Liu, Gaole Dai, Rongyu Zhang, Ruichuan An, Kun Wu, Zhengping Che, Shaoxuan Xie, Guocai Yao, Zhongxia Zhao, Pengwei Wang, Guang Liu, Zhongyuan Wang, Tiejun Huang, and Shanghang Zhang. Robobench: comprehensive evaluation benchmark for multimodal large language models as embodied brain, 2025. [52] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. [53] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. [54] OpenAI. Openai o1 system card. https://openai.com/index/ openai-o1-system-card/, December 2024. Accessed: 2024-12-05. [55] OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024. [56] OpenAI. Sora 2 system card. Technical report, OpenAI, September 2025. 34 [57] PikaLabs. Pika 1.5, 10 2024. Accessed: 2024. [58] Runway. Introducing gen-3 alpha: new frontier for video generation. https://runwayml. com/research/introducing-gen-3-alpha/, June 2024. [59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pages 247263, 2018. [60] Chengzhuo Tong*, Ziyu Guo*, Renrui Zhang*, Wenyu Shan*, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. [61] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-veval: versatile metric for evaluating image and video captions using gpt-4o. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 74197427, 2025. [62] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [63] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [64] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [65] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025. [66] Siting Wang, Minnan Pei, Luoyang Sun, Cheng Deng, Kun Shao, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: An mllm benchmark for spatial visualization. arXiv preprint arXiv:2507.07610, 2025. [67] Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025. [68] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and Lei Zhang. Videoverse: How far is your t2v generator from world model? arXiv preprint arXiv:2510.08398, 2025. [69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [70] Thaddus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [71] Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. [72] Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. [73] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. [74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [76] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. [77] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [78] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. [79] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024. [80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pages arXiv2407, 2024. [81] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. [82] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. [83] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84758489, 2025. [84] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [85] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2618326191, 2025."
        }
    ],
    "affiliations": [
        "CUHK",
        "IMIXR",
        "MMLab",
        "Northeastern University",
        "Peking University"
    ]
}