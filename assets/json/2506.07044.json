{
    "paper_title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning",
    "authors": [
        "LASA Team",
        "Weiwen Xu",
        "Hou Pong Chan",
        "Long Li",
        "Mahani Aljunied",
        "Ruifeng Yuan",
        "Jianyu Wang",
        "Chenghao Xiao",
        "Guizhen Chen",
        "Chaoqun Liu",
        "Zhaodonghui Li",
        "Yu Sun",
        "Junao Shen",
        "Chaojun Wang",
        "Jie Tan",
        "Deli Zhao",
        "Tingyang Xu",
        "Hao Zhang",
        "Yu Rong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ..."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 4 4 0 7 0 . 6 0 5 2 : r Lingshu: Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong DAMO Academy, Alibaba Group Equal Contribution, Project Lead Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements such as landscapes, household objects, and public events, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs faces the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As result, we build multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshus medical reasoning ability. Additionally, we develop MedEvalKit, unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks. Furthermore, we conduct five case studies closely aligned with real-world scenarios, demonstrating Lingshus potential for practical applications in medical contexts. Date: June 10, 2025 Correspondence: Hao Zhang and Yu Rong, {hz.hhea2e, royrong.ry}@alibaba-inc.com Homepage: https://huggingface.co/lingshu-medical-mllm"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly propelled progress in multimodal domains (OpenAI, 2024a; DeepMind, 2025a; Chen et al., 2025b; Kimi et al., 2025; Zhu et al., 2025), demonstrating capabilities that approach expert-level proficiency in areas like image captioning, visual question answering, and complex reasoning. Despite these successes, the performance of MLLMs in medical applications remains notably constrained (Yan et al., 2023; Lee et al., 2023; Nori et al., 2023). Biomedical image-text pairs differ fundamentally from typical web-based content, presenting unique challenges that general visual assistants are not well-equipped to handle (Li et al., 2025b). Lacking domain-specific medical visual knowledge, these models often respond to biomedical inquiries either with uncertainty or, worse, with incorrect information and hallucinations (Li et al., 2023a; Chen et al., 2024c). To address these gaps, recent studies have concentrated on adapting models specifically for medical contexts by integrating general-purpose LLMs or MLLMs with medical multimodal data to construct specialized Figure 1 Lingshu is trained on our meticulously curated datasets, encompassing rich and diverse open-sourced medical data across different modalities, general domain data, and high-quality synthetic medical data. Our results demonstrate that Lingshu achieves promising performance across various medical benchmarks and modalities. Furthermore, case studies in downstream tasks highlight its practical utility and significant potential in real-world healthcare settings. models (Wu et al., 2023; Hyland et al., 2024; Zhang et al., 2024a,b; Seyfioglu et al., 2025). These models demonstrate encouraging results in various tasks, including medical VQA, medical report generation, and diagnostic support, underscoring the considerable potential of MLLMs in healthcare scenarios. Pioneering models, such as LLaVA-Med (Li et al., 2023a), directly employ PubMed-derived datasets for vision-language alignment and supervised fine-tuning. However, their performance remains limited due to low data quality and inherent noise within PubMed. Subsequent research (Chen et al., 2024c; Li et al., 2025b; Lin et al., 2025) addresses the challenges with refined training recipes as well as larger and higher-quality medical multimodal datasets. Concurrently, advances in reasoning models, exemplified by OpenAIs o-series (OpenAI, 2024b, 2025b) and DeepSeek-R1 (DeepSeek-AI, 2025), set new benchmarks through advanced post-training techniques. Inspired by reinforcement learning with verifiable rewards (RLVR; Shao et al. (2024); Yu et al. (2025)), some studies (Lai et al., 2025; Pan et al., 2025) attempt to integrate this technique into the training of medical models to improve reasoning capabilities and enhance performance in clinical applications. Although significant progress has been achieved, existing medical MLLMs typically focus on collecting imagetext pairs by distilling knowledge from advanced models (OpenAI, 2024a,b, 2025a,b; DeepMind, 2025a,b). This solution is easy to implement and is scalable, but has notable limitations. First, medical tasks inherently demand extensive and diverse domain-specific knowledge. Nevertheless, the current distilled data are generated by prompting advanced models with medical imaging. These imaging prompts present challenges in extracting essential knowledge related to pharmacology, public health, and clinical contexts from the advanced models. Second, many distillation processes lack additional labels or supervision, relying exclusively on the generative capability of large models, thereby increasing hallucination risks. Third, the existing distilled data fall short in addressing complex medical cases that extend beyond basic image-text correlations and necessitate sequential decision-making. In this work, we address the challenges of medical data curation via (1) efficiently acquiring rich medical knowledge data by collecting not only multimodal medical data but also extensive medical texts and generaldomain multimodal/text data; and (2) synthesizing accurate medical data through robust pipeline for generating captions, QA pairs, and chain-of-thoughts (CoTs) reasoning samples. Leveraging this data, we develop Lingshu1, which is specifically tailored for the medical domain. Lingshu models undergo multi1Lingshu originates from chapter title in the Huangdi Neijing (https://en.wikipedia.org/wiki/Huangdi_Neijing), one 2 stage training paradigm to progressively infuse medical knowledge and enhance their problem-solving abilities. Besides, we investigate the role of RLVR in improving Lingshus multimodal medical reasoning and develop Lingshu-RL. In addition to the data and model contributions, we find that current models are often evaluated in isolated, non-standardized environments. The lack of accessible, unified evaluation frameworks further restricts the development of the medical field. Therefore, we introduce MedEvalKit, unified framework that consolidates major multimodal and textual medical benchmarks for efficient, standardized model assessment. Extensive experiments demonstrate that Lingshu consistently achieve state-of-the-art (SOTA) performance across most multimodal and textual medical VQA tasks, as well as in report generation, for both 7B and 32B configurations. Notably, in medical VQA tasks, Lingshu-32B outperforms the second-best model by an average of 7.2 accuracy points across seven tasks, even surpassing proprietary models like GPT-4.1 and Claude Sonnet 4. Through comprehensive ablation studies, we further confirm the importance of data quality and medical knowledge coverage on overall performance, validating the effectiveness of our data curation framework. Finally, our case study underscores the great potential of Lingshu across numerous medical applications, including medical report generation, medical support assistance, and clinical surgery assistance. To sum up, our contributions are fourfold: Data: We introduce novel data curation pipeline that efficiently gathers medical knowledge from various resources and synthesizes high-quality medical captions, QA pairs, and CoT reasoning samples. Training: We propose our medical-specific MLLMs, Lingshu in 7B and 32B parameter sizes. Lingshu models are trained with our tailored, multi-stage training paradigm that progressively infuses extensive medical knowledge into MLLMs and enhances problem-solving abilities. We also explore the role of RLVR in improving the multimodal medical reasoning and develop Lingshu-RL. Evaluation: We propose MedEvalKit, unified evaluation framework that consolidates major benchmarks for multimodal and textual medical tasks, streamlining model assessment and advancing standardized performance evaluation in the medical field. Performance: Through rigorous experimental validations, Lingshu demonstrates state-of-the-art performance across multiple multimodal and textual medical VQA tasks, as well as in report generation. Paper organization. We first describe our data curation process in 2, followed by the introduction of detailed training methodology (3). We then elaborate our unified medical evaluation framework, MedEvalKit in 4. The experimental results are reported in 5, followed by illustrative case studies in 6. Subsequently, we briefly review related literature on medical MLLMs in 7. Finally, we discuss limitations and outline directions for future research in 8."
        },
        {
            "title": "2 Data Curation",
            "content": "We address the intricate challenges of medical data curation via two primary strategies: (1) comprehensive collection of diverse medical knowledge sources, including medical multimodal datasets, extensive medical texts, and general-domain multimodal/textual data; and (2) synthesis of high-quality medical data through robust pipeline capable of generating detailed captions, OCR-based samples, QA pairs, and chain-of-thought reasoning examples. The overall medical data curation pipeline is illustrated in Figure 2. For data collection, we curate wide range of open-source datasets from the web, categorized as follows: (1) multimodal medical data for enhancing Lingshus comprehension of medical images and downstream task performance; (2) unimodal medical data, including standalone medical images and texts, to enrich domain knowledge and support synthetic data generation; (3) general-domain data to improve general visual and language understanding. All data undergo strict quality filtering prior to use. Each dataset is individually preprocessed and cleaned to ensure the quality and relevance. For medical data synthesis, we generate targeted synthetic data to strengthen specific model capabilities: (1) long-form captions to improve diagnostic detail extraction from images; (2) OCR-based instruction data to enhance text recognition in medical imagery; (3) VQA samples for tasks such as diagnosis, anatomical of the earliest classical texts of traditional Chinese medicine. 3 Figure 2 The overall data curation pipeline of medical multimodal and textual data. recognition, and modality classification; (4) distilled reasoning trajectories to promote advanced medical reasoning. All synthesized data are subject to rigorous quality control process to ensure accuracy and reliability."
        },
        {
            "title": "2.1 Data Collection",
            "content": "2.1.1 Medical Multimodal Data Collection We incorporate two primary categories of existing multimodal medical data into our training set: (1) medical caption data, which pairs one or more medical images with descriptive text to facilitate semantic alignment between visual and textual modalities; and (2) medical multimodal instruction data, which includes image-instruction pairs. The latter comprises two main types: medical VQA data, featuring open-ended or multiple-choice questions about medical images, and medical report data, consisting of radiology reports that detail clinical findings and impressions. These datasets support MLLM in learning broad range of medical multimodal tasks. complete list of the collected datasets is provided in Table 1. 2.1.2 Medical Unimodal Data Collection Medical text instruction data. Chen et al. (2023) has demonstrated that supervised fine-tuning (SFT) on medical instruction data is effective in injecting medical knowledge into LLMs. Building on this insight, we incorporate medical text instruction data into our training corpus to enhance the domain knowledge of Lingshu. The incorporated data spans four primary categories: (1) medical factoid QA, including both free-form and multiple-choice questions on medical knowledge; (2) distilled reasoning data, which supplements answers with step-by-step reasoning path annotated by large reasoning models; (3) patient-doctor dialogues, comprising single-turn exchanges with patient queries and corresponding doctor responses; and (4) general 4 Table 1 List of collected medical multimodal datasets. For MIMIC-CXR (Johnson et al., 2019) in medical captioning data, we only use its Findings part as the image captions. Collected Datasets Type of Data Medical Captioning Data LLaVA-Med Alignment (Li et al., 2023a), PubMedVision (Chen et al., 2024c), Quilt-LLaVAPretrain (Seyfioglu et al., 2025), PMC-OA (Lin et al., 2023), ROCO (Pelka et al., 2018), ROCOv2 (Rückert et al., 2024), MIMIC-CXR (Johnson et al., 2019), MedICaT (Subramanian et al., 2020), FairVLMed (Luo et al., 2024), and MedPix-2.0 (Siragusa et al., 2025) Medical Instruction Data PathVQA (He et al., 2020), PMC-VQA (Zhang et al., 2024b), SLAKE (Liu et al., 2021), QuiltLLaVA Instruct (Seyfioglu et al., 2025), VQA-Med-2019 (Ben Abacha et al., 2019), PubMedVision (Chen et al., 2024c), LLaVA-Med (Li et al., 2023a), LLaVA-Med-zh (BUAADreamer, 2024), VQA-RAD (Lau et al., 2018), MIMIC-Ext-MIMIC-CXR-VQA (Bae et al., 2024), CheXpert Plus (Chambon et al., 2024), MIMIC-CXR (Johnson et al., 2019), and IUXray (Demner-Fushman et al., 2016) Table 2 List of collected medical textual instruction datasets. Type of Data Medical Factoid QA Collected Datasets MedQA (Jin et al., 2021), MedQuAD (Ben Abacha and Demner-Fushman, 2019), ApolloCorpus (Wang et al., 2024b), and PMC-LLaMA (Wu et al., 2024) Distilled Textual Medical Reasoning Data MedThoughts-8K (hw hwei, 2025), MedReason (Wu et al., 2025), medical-o1-reasoningSFT (Chen et al., 2024b), medical-o1-verifiable-problem (Chen et al., 2024b), and Medical-R1-Distill-Data (Chen et al., 2024b) Patient-doctor Dialogues HealthCareMagic-100k (Li et al., 2023b), icliniq-10k (Li et al., 2023b), and HuatuoGPTII-GPT4-SFT-140K (Chen et al., 2023) General Medical Instruction Data AlpaCare-MedInstruct-52k (Zhang et al., 2025c) medical instruction data, covering diverse tasks such as medication summarization and medical text translation. complete list of the datasets is provided in Table 2. Medical imaging data. It comprises raw images paired with human-annotated metadata, including the depicted organ, diagnostic labels, and imaging modality. This metadata enables the synthesis of additional captioning and VQA samples to improve medical image understanding and downstream task performance. We primarily use these datasets for data synthesis, which is detailed in 2.2. Table 3 summarizes the collected datasets, which span wide range of medical modalities. 2.1.3 General Domain Data Collection To enhance the general visual understanding, text generation, and multimodal instruction-following capabilities of our medical MLLM, we incorporate diverse general-domain datasets. These include image captioning, textual, and multimodal instruction-following data, which collectively enhance the models generalization to wide range of vision-language tasks. Specifically, we utilize LLaVA-1.5 captions (Liu et al., 2024) and PixMo (Deitke et al., 2024) for image captioning; LLaVA-1.5 (textual instruction subset) and OpenHermes2.5 (Teknium, 2023) for textual instruction-following; and LLaVA-1.5 (multimodal instruction subset) along with ALLaVA (Chen et al., 2024a) for multimodal instruction-following. 2.1.4 Data Cleaning Process Data cleaning for medical multimodal data. Many open-source multimodal medical datasets are extracted automatically from scientific papers and thus often contain noise and redundancy. To ensure data quality, we implement three-stage cleaning pipeline. First, image filtering, where images with dimensions smaller than 64 pixels are removed to eliminate low-resolution or uninformative content. Second, image deduplication, where we apply perceptual hashing with strict Hamming distance threshold of zero to detect and remove exact duplicate images, retaining only one high-quality instance per set. To accelerate this process, we use chunk-based deduplication technique.2 Third, text filtering, where image caption samples with fewer than 10 2https://github.com/xuehuachunsheng/DupImageDetection 5 Table 3 List of collected medical imaging datasets of different medical modalities. Modality X-ray CT MRI Ultrasound Collected Datasets COVID19-Radiography (Chowdhury et al., 2020), NIH-Chest-X-Ray (Wang et al., 2017), CheXpert (Irvin et al., 2019), and Mendeley Digital Knee X-ray (Gornale and Patravali, 2020) KIPA22 (He et al., 2021), DeepLesion (Yan et al., 2017), and chest-ctscan (Yi, 2021) Brain-Tumor-MRI (Nickparvar, 2021), BraTS2024 (de Verdier et al., 2024), LLD-MMRI (Lou et al., 2025), and MAMA-MIA (Garrucho et al., 2025) AbdomenUS (Vitale et al., 2020), Breast Ultrasound Images (Al-Dhabyani et al., 2020), Annotated Ultrasound Liver images (Xu et al., 2022), and RadIMageNet (subset of ultrasound images) (Mei et al., 2022) Dermoscopy HAM10000 (Philipp et al., 2018), ISIC challenge 2020 (Kurtansky et al., 2024), and Fitzpatrick17k (Groh et al., 2021) Fundus BRESET (Nakayama et al., 2024), PAPILA (Kovalyk et al., 2022), and 2015&2019 Diabetic Retinopathy Detection (Lin, 2022) Histopathology CPD (Wagner et al., 2023), Breast-Cancer (Ding et al., 2023), PanNuke (Gamper et al., 2020), and EBHI-Seg (Shi et al., 2023) Microscopy Blood Cell Image Gan (2018), BioMediTech RPE (Nanni et al., 2016), and mhsma-dataset (Javadi and Mirroshandel, 2019) or more than 1024 tokens are excluded to filter out lengthy and potentially noisy examples. The full pipeline is applied to all medical image captioning datasets. For instruction data, we omit the text filtering step due to the validity of concise responses, e.g., yes/no, word phrases, or short options. Image deduplication in instruction datasets specifically targets PubMedVision (Chen et al., 2024c) and LLaVAMed (Li et al., 2023a), which exhibit high degree of redundancy. However, cross-dataset deduplication is avoided in this case, as individual images may be paired with diverse instructions. In contrast, image deduplication across all datasets is conducted for medical captioning data. Data cleaning for medical text data. To ensure the quality and compliance of collected medical text instruction data, we implement two key cleaning procedures. First, we apply model-based response cleaning to opensource patient-doctor dialogue datasets, such as HealthCareMagic-100k (Li et al., 2023b) and iCliniq-10k (Li et al., 2023b), which are typically scraped from online healthcare platforms. These responses often contain sensitive identity information or provide explicit diagnoses and prescriptions, raising privacy and legal concerns. Specifically, we employ LLaMA-3.1-70B (Dubey et al., 2024) to remove identity-related content and revise responses to avoid direct medical recommendations, as detailed in Appendix A.1. Second, to eliminate redundancy, we perform text deduplication across multiple instruction datasets using min-hash locality-sensitive hashing (LSH), retaining only the highest-quality version of each near-duplicate based on the reliability of its source."
        },
        {
            "title": "2.2 Medical Data Synthesis",
            "content": "This section details our methodologies for synthesizing four types of medical multimodal datalong-form captions, OCR-based instruction samples, VQA instances, and distilled reasoning examplesalongside the quality control strategy employed to ensure their accuracy and reliability. 2.2.1 Medical Long-form Caption Synthesis Given the brevity and limited descriptive depth of existing medical caption datasets, we aim to construct enriched captions that capture key visual features in medical images. To this end, we utilize data from medical image segmentation and classification tasks, which include human-annotated disease regions, diagnostic labels, and metadata such as patient gender and X-ray view position. This structured factual knowledge enables the synthesis of detailed and authentic medical captions. To ensure modality diversity and improve the models visual perception across imaging types, we source data from multiple modalities and balance their volume in the constructed caption dataset. 6 Specifically, we collect AbdomenUS (Vitale et al., 2020) for Ultrasound; PAD-UFES-20 (Pacheco et al., 2020) for Dermoscopy; CheXpert (Irvin et al., 2019), NIH-Chest-X-Ray (Rajpurkar et al., 2017), and Mendeley Digital Knee X-ray (Gornale and Patravali, 2020) for X-ray; KIPA22 (He et al., 2021) and DeepLesion (Yan et al., 2017) for CT; BraTS2024 (de Verdier et al., 2024), Brain-Tumor-MRI (Nickparvar, 2021), LLD-MMRI (Lou et al., 2025), and MAMA-MIA (Garrucho et al., 2025) for MRI; and CPD (Wagner et al., 2023), Breast-Cancer (Ding et al., 2023), PanNuke (Gamper et al., 2020), and EBHI-Seg (Shi et al., 2023) for Histopathology. For each image, we adopt GPT-4o (OpenAI, 2024a), whose prompt is shown in Appendix A.4, to generate detailed long-form caption that captures fine-grained visual features. The captioning process follows five-stage pipeline: (1) metadata preparation, (2) region-of-interest (RoI) identification, (3) annotation with factual knowledge, (4) annotation with doctor preference, and (5) summarization. This procedure yields approximately 100K high-quality medical image captions. Stage 1: metadata preparation. This stage leverages metadata to provide essential contextual information for given image, including modality, disease type, and, when applicable, camera view and observable patient characteristics. Following Xie et al. (2025), we begin by extracting metadata from each dataset, applying dataset-specific rules to convert this information into short captions. For example, in CPD (Wagner et al., 2023), we generate short captions using staining method and nuclei score metadata with the template: WSI image of carcinogenic DNA damage caused by ultraviolet (UV) radiation. The image is stained for {staining}. The relative amount of damaged nuclei (bounded in [0,1]) is {nuclei score}. To minimize hallucination risk, we exclude metadata not visually inferable from the image, e.g., patient age, based on expert consultation. All personal identifiers are removed to prevent racial or demographic biases. Additionally, we manually retrieve medical knowledge from the web to enhance the annotation process, particularly for models like GPT-4o that may lack domain-specific expertise. Thus, this stage involves metadata preparation from both dataset and external medical references. Stage 2: RoI identification. Building upon the textual data obtained in Stage 1, this stage incorporates complementary visual information. For image segmentation datasets, it provides disease or abnormality localization through RoIs, which are critical for reducing hallucinations and must be clearly marked in the images. RoIs are available as either segmentation masks or bounding boxes. Bounding boxes are directly overlaid on the original images, while segmentation masks are converted into bounding boxes by computing the minimal enclosing rectangles. For 3D datasets such as MAMA-MIA (Garrucho et al., 2025) and KIPA22 (He et al., 2021), we extract 2D slices and their corresponding masks along the z-axis, and apply the same bounding box rendering procedure to generate 2D images. Stage 3: annotation with factual knowledge. This stage focuses on synthesizing image descriptions by integrating textual and visual information from the previous two stages. For datasets with annotated RoIs, we input bounding-box-rendered images, short captions, and retrieved medical knowledge into GPT-4o to generate descriptive text. For datasets lacking RoI annotations, such as image classification tasks, GPT-4o is prompted with the original image and metadata from Stage 1. Since visual and textual inputs from the last two stages are both manually annotated, the resulting descriptions are considered highly reliable. Stage 4: annotation with doctor preference. Although the generated factual descriptions are highly reliable, they are often constrained by explicitly available metadata and may overlook critical visual details beyond RoIs. To mitigate this, we consult medical professionals to identify key factors they consider when interpreting medical images. Their insights are distilled into task-specific instructions. For instance, in MRI analysis, clinicians focus on sequence type, image orientation, anatomical structures, and visible abnormalities. These instructions are used to guide the annotation model, which, given the original image without bounding box overlays, generates alternative descriptions aligned with expert diagnostic perspectives. Stage 5: summarization. We use GPT-4o to combine the outputs from stages 3 and 4 to consolidate the final caption for each medical image. To avoid conflict, the description from stage 3 is prioritized due to its higher reliability. Figure 3 presents representative example of synthesized long-form caption for an MRI image. 7 Figure 3 An example of the synthesized medical long-form caption for the MRI image. Table 4 An example of the synthetic OCR-based instruction sample. Input In the following hybrid combinations, which one belongs to test cross ( ) A. EeFfGgEeFfGg B. EeFfGgeeFfGg C. eeffGgEeFfGg D. eeffggxEeFfGg"
        },
        {
            "title": "Explain the question in the photograph",
            "content": "Output Looking at the options: A. EeFfGgEeFfGg: It is cross between two heterozygotes. B. EeFfGgeeFfGg: It is cross between heterozygote and an individual that is homozygous recessive for one gene. C. eeffGgEeFfGg: It is cross between an individual that is homozygous recessive for two genes and heterozygote. D. eeffggEeFfGg: It is cross between an individual that is homozygous recessive for all three genes and heterozygote. The correct answer is 2.2.2 Medical OCR-based Instruction Data Synthesis OCR-based instruction datasets enhance the models ability to interpret textual information embedded within medical images, common scenario involving elements such as patient identifiers, age, and biological metrics. To construct these datasets, we collect biology and chemistry exam questions with ground-truth answers from online sources, as these subjects are closely related to the medical domain and offer rich resources. Each question is annotated with detailed reasoning steps using Gemini-2.0-Flash-Thinking (DeepMind, 2025a), and only samples with answers that exactly match the ground-truth are retained. The validated questions are then rendered as images to form multimodal instruction data, where the question image serves as input and the textual answer as output (see Table 4). This process yields total of 50K high-quality OCR-based samples. 2.2.3 Medical VQA Data Synthesis Medical diagnosis, anatomy identification, and modality classification are core competencies required for MLLMs to perform diverse medical multimodal tasks. To strengthen Lingshus capabilities in these areas, we synthesize additional medical VQA data using two complementary strategies: template-based method and self-instruct-based method. Template-based method. This method constructs VQA samples using human-annotated meta-information, e.g., abnormality labels, from public medical imaging datasets. Specifically, we extract datasets containing labels related to anatomical structures, abnormalities, or imaging modalities. For each label type, we manually design question templates. Ground-truth answers are derived from the original annotations, while distractor options are sampled from the label space of the respective dataset. This approach enables structured generation of high-quality VQA data. An example of this process is illustrated in Figure 4. Self-instruct-based method. To enhance question diversity and linguistic variation, we leverage GPT-4o to generate VQA samples from (image, caption) pairs sourced from medical captioning datasets. GPT4o is prompted using few-shot format, where seed examplesdrawn from open-source medical VQA 8 Figure 4 Illustration of the data synthesis process for synthetic medical VQA data. datasetsdemonstrate questions focused on diagnosis and anatomy. It then produces question, options, and the correct answer based on the provided caption. This method enables more varied and natural question formulations, as shown in Figure 4. The prompt design is described in Appendix A.2. 2.2.4 Medical Reasoning Data Distillation Most existing open-source and our synthesized medical instruction data, primarily composed of short-answer and multiple-choice questions, lack explicit annotations of the underlying reasoning process. To strengthen the models medical reasoning capabilities, we employ GPT-4o to generate CoT reasoning trajectories for subset of both multimodal and textual instruction data. Specifically, for each sample, we provide GPT-4o with the question, ground-truth answer, and, if applicable, answer options and the corresponding medical image. GPT-4o is instructed to produce step-by-step reasoning path without relying on or explicitly referencing the ground-truth answer. To ensure quality, we implement an LLM-based validation process wherein GPT-4o evaluates the consistency between the reasoning trace and the ground-truth answer. Samples deemed inconsistent are excluded from the final dataset. The prompts are detailed in Appendix A.3."
        },
        {
            "title": "3 Model Training",
            "content": "Lingshu is built upon the Qwen2.5-VL model architecture (Bai et al., 2025), which consists of three key components: large language model (LLM), vision encoder, and an MLP-based projector. We employ two parameter-scaled variants of Qwen2.5-VL, i.e., 7B-Instruct and 32B-Instruct, as the foundational models. The choice of instruct versions is motivated by two primary considerations. First, these models require significantly less alignment data during training, owing to their inherent image understanding and instruction-following capabilities. Second, they support the direct use of instruction-formatted data during the alignment phase. Consequently, Lingshu can adopt instruction-style data formatting from the initial training stage, which 9 Figure 5 The distribution of medical image modality in our training data (medical subset), where the category Pure Text indicates text-only medical data. promotes consistency across the multi-stage training pipeline, reduces the overhead of additional instruction alignment, simplifies the training process, and enhances overall training stability."
        },
        {
            "title": "3.1 Training Recipe",
            "content": "Building on prior advances in Multimodal Large Language Models (MLLMs; Bai et al. (2025); Zhu et al. (2025)), we develop multi-stage training framework to progressively adapt the backbone model to the medical domain. This framework follows shallow-to-deep progression and comprises four sequential stages: (1) Medical Shallow Alignment, (2) Medical Deep Alignment, (3) Medical Instruction Tuning, and (4) Medical-oriented Reinforcement Learning. The overall training pipeline is illustrated in Figure 6. Note that our Lingshu is trained after the first three stages. We further explore the effectiveness of RL in medical reasoning, leading to Lingshu-RL version. The initial two stages focus on constructing robust vision-language foundation tailored to medical scenarios. Medical Shallow Alignment fine-tunes the model using small set of medical image-text pairs, enabling it to encode medical images and generate corresponding descriptions accurately. This stage facilitates the models preliminary understanding of visual medical content. Medical Deep Alignment builds upon this by introducing larger, higher-quality, and semantically richer medical image-text pair datasets. This stage deepens the models domain knowledge and achieves finer-grained vision-language alignment. The subsequent stages aim to enhance the models utility in practical medical settings. Medical Instruction Tuning improves the models ability to comprehend and execute task-specific instructions across various medical use cases, thereby enhancing its generalizability to downstream tasks. Finally, Medical-oriented Reinforcement Learning incorporates the Reinforcement Learning with Verifiable Rewards (RLVR; Shao et al. (2024); Lai et al. (2025); Pan et al. (2025)) paradigm to strengthen the models medical reasoning, problem-solving capacity, and interpretability. 10 Figure 6 The training pipeline of Lingshu, which consists of four stages: medical shallow alignment, medical deep alignment, medical instruction tuning, and medical-oriented reinforcement learning. Note that our Lingshu is trained after the first three SFT stages. We additionally apply RL on top of it, leading to Lingshu-RL version. 3.1.1 Medical Shallow Alignment The goal of the medical shallow alignment stage is to establish effective alignment between diverse medical imaging modalities and their corresponding textual descriptions. This alignment enhances the models ability to comprehend and interpret medical images, thereby laying solid foundation for subsequent medical knowledge integration. During this phase, the LLM is kept frozen, while only the vision encoder and the projector are fine-tuned using coarsely annotated medical image-caption data. Freezing the LLM is deliberate choice, as the coarse caption data typically consists of short and informationsparse text. Exposing the LLM to such limited textual content could potentially impair its language generation capabilities. In contrast, training the vision encoder and projector enables the model to better map medical visual features into the LLMs representational space. The use of coarsely annotated data is motivated by its relatively low semantic complexity, which facilitates rapid learning of the general characteristics of medical imaging modalities. This not only aids in effectively adjusting the vision encoders outputs for better alignment with the LLM but also contributes to faster training and improved convergence stability. As shown in Table 5, the coarsely annotated medical image-caption data comprises two datasets with relatively short and concise captions, i.e., PMC-OA (Lin et al., 2023) and ROCO (Pelka et al., 2018). 3.1.2 Medical Deep Alignment The goal of the medical deep alignment stage is to comprehensively integrate medical knowledge into MLLM, enhancing its capabilities to understand diverse medical concepts and adapt to various clinical contexts. To achieve this, all model parameters, including those of the LLM, vision encoder, and projector, are unfrozen to allow end-to-end fine-tuning. This process is designed to improve the models ability in multimodal knowledge integration and its effectiveness in interpreting complex medical visual data. During this stage, training is conducted on substantially more diverse and enriched set of medical image-text pairs. Compared to the preceding shallow alignment phase, the dataset is significantly expanded in terms of modality diversity, linguistic complexity, and structural completeness. It includes images from less common medical imaging modalities, longer and more structured captions, as well as synthetic image-caption pairs generated from medical image classification and segmentation tasks. Moreover, high-quality general-domain multimodal image-text data is incorporated into the training process alongside medical data. This joint training not only helps preserve the models general multimodal capabilities but also introduces it to wider variety of visual formats, such as charts, tables, graphs, mathematics, and 11 Table 5 The overview of data mixture for the four training stages. Note all datasets listed below have been rigorously processed through our data pipeline (detailed in 2), resulting in cleaned and validated versions rather than raw data. Stage Medical Shallow Alignment PMC-OA and ROCO Medical Deep Alignment Training Data Composition 1. Medical Image Captions: ROCOv2, LLaVA-Med, Quilt-LLaVA, PubMedVision, MIMIC-CXR, FairVLMed, MedICaT, MedPix-2.0, and synthesized long-form medical caption data 2. General Image Captions: LLaVA-1.5 Caption and PixMo Medical Instruction Tuning Medical-oriented RL 1. Medical Multimodal: PathVQA, PMC-VQA, SLAKE, Quilt-LLaVA, VQA-Med-2019, MIMIC-Ext-MIMICCXR-VQA, PubMedVision, LLaVA-Med (en & zh), VQA-RAD, CheXpert Plus, MIMIC-CXR, ROCOv2, IU-Xray, and synthesized QA, OCR-based, and CoT data in the medical domain 2. General Multimodal: LLaVA-1.5 instruction (multimodal instruct subset only) and ALLaVA 3. Medical Text: MedQuAD, medical-o1-verifiable-problem, medical-o1-reasoning-SFT, ApolloCorpus, Medical-R1-Distill-Data, AlpaCare-MedInstruct-52k, HealthCareMagic-100k, icliniq10k, PMC-LLaMA, HuatuoGPT2-GPT4-SFT-140K, MedReason, MedThoughts-8K, MedQA, and synthesized medical QA data 4. General Text: LLaVA-1.5 instructions (textual instruct subset only) and OpenHermes-2.5 Data curated from MIMIC-Ext-MIMIC-CXR-VQA, PMC-VQA, Kvasir-VQA, PathVQA, SLAKE, VQA-Med-2019, VQA-RAD and synthesized QA datasets followed by rigorous post-processing Amount 927K 4.1M 7.1M 65K scientific illustrations. Exposure to these structured visual elements enhances the models ability to perform structured data interpretation and cross-modal reasoning. This is particularly beneficial in medical contexts where diagnostic reports often include laboratory tables, time-series physiological curves, and pathological mapsvisual formats that differ significantly from natural images in both distribution and semantics. By learning from these general-domain examples, the model acquires transferable skills in chart interpretation and symbolic reasoning, enabling more precise analysis of abnormal physiological metrics, lesion measurements, and their temporal dynamics in clinical scenarios. The overall medical and general image caption dataset usage is summarized in Table 5. 3.1.3 Medical Instruction Tuning In line with the prevailing practices in MLLMs (Li et al., 2023a; Chen et al., 2024c; Zhu et al., 2025; Chen et al., 2025c), the medical instruction-tuning stage is designed to refine MLLMs instruction following capability with broad spectrum of task directives, ensuring that its outputs are accurately aligned with user intentions. To accommodate wide breadth of clinical use cases, we unlock all parameters and perform large-scale, end-to-end optimization of the entire model. The training corpus transcends conventional instruction formats, such as image descriptions, question-answer pairs, and multiple-choice questions, by integrating an extensive suite of scenario-oriented queries collected and synthesized from multiple sources. These queries span diagnosis, clinical examination, medical knowledge retrieval, clinical report generation, and anatomical structure localization. This breadth markedly augments the models domain competency. In addition, we enrich the corpus with carefully vetted high-quality generaldomain data and medical text data, both of which have proven effective in enhancing performance on downstream medical tasks. Incorporating such data is capable of counterbalancing the inherent image-centric information bias of image-text pair data, expanding the models conceptual scope beyond visual content, and fostering more holistic grasp of medical knowledge. Moreover, our training data comprises advanced and complex data formats, such as multi-image reasoning tasks, multi-turn dialogues, queries that require detailed reasoning processes, that demand deeper analytical acuity. Collectively, this diversified and sophisticated training data equips the MLLM to navigate complex clinical scenarios with enhanced precision and analytical capabilities. 12 The datasets used in this stage are summarized in Table 5, which are categorized based on their domain (i.e., medical or general) and modality (i.e., multimodal or textual): In the medical multimodal subset, we initially employ multiple public medical instruction datasets, but found that they often emphasize specific image regions, which may introduce local view bias during training. Thus, we supplement the training with high-quality caption datasetsselected from ROCOv2 (Rückert et al., 2024), PubMedVision (Chen et al., 2024c), and MIMIC-CXR (Johnson et al., 2019)to promote holistic visual understanding. We further incorporate clinical report generation datasets, CheXpert Plus (Chambon et al., 2024) and IU-Xray (Demner-Fushman et al., 2016). Lastly, we synthesize additional medical data, as detailed in 2.2, including OCR-based, QA, and CoT data. For the general multimodal subset, we select the publicly available instruction-tuned version of LLaVA1.5 (Liu et al., 2024) and ALLaVA (Chen et al., 2024a). In the medical text subset, we curate data from diverse public sources and generate instruction data via an automated synthesis pipeline, followed by strict quality validation. We further incorporate open-source long-form medical reasoning datasets, primarily distilled from OpenAI-o1 (OpenAI, 2024b) and DeepSeek-R1 (DeepSeek-AI, 2025), which offer comprehensive analyses of medical problems and effectively enhance the models medical knowledge. For the general text subset, we leverage two public datasets, i.e., the text-only part of LLaVA-1.5 (Liu et al., 2024), and OpenHermes-2.5 (Teknium, 2023). 3.1.4 Medical-oriented Reinforcement Learning Recent advancements in reasoning models, exemplified by OpenAIs o-series (OpenAI, 2024b, 2025b) and DeepSeek-R1 (DeepSeek-AI, 2025), have set new benchmarks in complex tasks through advanced post-training strategies. Central to these improvements is reinforcement learning with verifiable rewards (RLVR), with Group Relative Policy Optimization (GRPO; Shao et al. (2024)) emerging as prominent method due to its efficiency and effectiveness. It has been increasingly adopted for training MLLMs to enhance reasoning capabilities (Meng et al., 2025; Wang et al., 2025a; Tan et al., 2025). Building on RLVRs success, some work applies it to medical domains for improving generalizability, interpretability, and reliability (Lai et al., 2025; Pan et al., 2025). Conventional SFT encounters key limitations: 1) over-reliance on answer supervision, which can cause overfitting and shortcut learning, particularly detrimental in medical scenarios; and 2) limited promotion of deliberate reasoning skills. RLVR addresses these issues by encouraging models to autonomously discover reasoning pathways through reward signals, rather than relying on answer memorization or teacherguided CoT imitation (Chu et al., 2025). Accordingly, we adopt GRPO to train Lingshu, leveraging carefully curated medical verifiable dataset. The medical verifiable dataset is derived from multiple medical sources  (Table 5)  and undergoes rigorous post-processing. Since most collected samples are in multiple-choice format, we reformulate those with word or phrase answers as open-ended questions to enhance difficulty while maintaining verifiability. This process also leads to data balance between MCQA and open-ended QA. Additionally, binary-answer questions (e.g., Yes/No) are downsampled to approximately 5% of the dataset to mitigate potential bias from their over-representation. Samples are then selected based on their modality and query format, aiming to ensure the data balance. total of 100K examples are curated to form the training set for the RL stage. For reward design, we follow common practices (DeepSeek-AI, 2025; Yeo et al., 2025) by using strict format reward and an accuracy reward, with respective weights of 0.5 and 1."
        },
        {
            "title": "3.2 Implementation Details",
            "content": "As previously outlined, Lingshu builds upon two variants of the Qwen2.5-VL-Instruct model, with parameter sizes of 7B and 32B, and is further optimized via continual training. The standard training process is organized into three primary stages, i.e., Medical Shallow Alignment, Medical Deep Alignment, and Medical Instruction Tuning. This leads to our Lingshu models. Across all stages, we adopt the AdamW optimizer with the cosine learning rate scheduler and warm-up step of 100. The maximum sequence length is set to 8,192 tokens, the per-device training batch size is configured as 1, and the gradient accumulation step is set to 8. 13 (a) Medical Multimodal Benchmark (b) Medical Text-only Benchmark (c) Report Gen. Benchmark Figure 7 An overview of the data distribution across benchmarks for different types of medical tasks. In the Medical Shallow Alignment stage, LLM remains frozen, only the vision encoder and the projection layer are fine-tuned for one epoch, with learning rates set to 2e-6 and 1e-5, respectively. During the Medical Deep Alignment and Medical Instruction Tuning stages, we unfreeze the LLM and fine-tune it with learning rate of 1e-5, while maintaining the same learning rates for the vision encoder and projector. These stages are trained for one and two epochs, respectively. To enhance training efficiency, we employ data packing during the Medical Instruction Tuning stage. Preliminary experiments suggest that data packing at this stage does not adversely affect model performance. However, similar attempts in the Medical Shallow and Deep Alignment stages have led to marked decline in model performance. This decline is likely attributable to the high prevalence of short textual samples within the medical image-caption dataset.3 When packed into longer sequences, these short entries contribute sparse gradients during normalization, undermining the training dynamics. Additionally, data packing substantially reduces the number of training steps, which may hinder sufficient convergence. Extending the number of epochs to compensate could result in overfitting. Consequently, we opt not to use data packing during the Medical Shallow and Deep Alignment stages. For the Medical-oriented RL stage, we initialize training from Lingshu checkpoint and apply GRPO for one epoch using the AdamW optimizer. The maximum sequence length is set to 4096,4 with rollout batch size of 512 and global batch size of 128. The learning rate is configured at 1e-6, the sampling temperature at 1.0, the KL divergence loss coefficient at 1e-3, and 16 responses are sampled for each prompt. The resultant model is named as Lingshu-RL."
        },
        {
            "title": "4 MedEvalKit: A Unified Medical Evaluation Framework",
            "content": "The rapid progress of medical MLLMs has yielded impressive performance across various tasks. However, the absence of unified evaluation framework has led to inconsistencies in performance comparison across models. Additionally, reproducing or deploying existing models in standardized environment often requires substantial time and computational resources. systematic and rigorous evaluation across diverse medical tasks is essential for accurately assessing model capabilities, identifying strengths and limitations, and providing actionable feedback for further development. Reproducible, quantitative evaluation is thus vital to advancing the optimization of medical MLLMs. An effective evaluation of medical MLLMs requires multiple-dimensional assessment, including medical knowledge, visual understanding, and reasoning ability. While existing frameworks, such as LMMs-Eval (Zhang et al., 2025a), Eval-Harness (Gao et al., 2024), and VLMEvalKit (Duan et al., 2024), can be applied to evaluate medical MLLMs, they are primarily designed for general multimodal tasks. As result, they exhibit limitations in terms of medical data coverage and model adaptation, making them difficult to apply directly. This not only reduces evaluation efficiency but may also introduce potential biases. To comprehensively and efficiently evaluate the performance of medical MLLMs, we develop systematic evaluation framework, termed MedEvalKit, which integrates mainstream medical benchmarks and task 3Although we synthesize certain amount of long-form captions, it constitutes only small proportion of the overall dataset. 4Although maximum sequence length of 8192 is used in previous stages, preliminary RL experiments show that most medical samples generate significantly shorter outputs. Thus, we reduce the maximum sequence length to 4096 for this stage. Figure 8 The prompt formats of different medical question types. types, supporting variety of question formats, including multiple-choice questions, closed-ended questions, open-ended questions, and medical report generation. It covers representative tasks in medical multimodal and textual understanding, offering broad applicability. MedEvalKit accommodates both multimodal and text-only inputs, enabling unified evaluation of both medical MLLMs and medical LLMs. To enhance standardization and reproducibility, we standardized data preprocessing formats and post-processing protocols. consistent model deployment and inference interface is provided, supporting rapid integration and one-click evaluation. For result assessment, MedEvalKit incorporates dual verification mechanism that combines rule-based evaluation with an LLM-as-a-Judge strategy, integrating both objective and subjective assessment to improve evaluation stability and reliability. Additionally, the framework supports inference acceleration via vLLM (Kwon et al., 2023), enabling high-throughput and parallel evaluation, with strong scalability and engineering usability."
        },
        {
            "title": "4.1 A Large-scale Medical Evaluation Benchmark",
            "content": "To enable more comprehensive evaluation of medical MLLMs, we curated collection of mainstream medical benchmark datasets covering multimodal question answering, text-only question answering, and report generation, comprising total of 152,066 evaluation samples with 121,622 distinct medical images spanning 16 benchmark datasets. The overview of the medical evaluation benchmark is depicted in Figure 7. More detailed information is presented in Table 25. Multimodal QA: We include the test sets of VQA-RAD (Lau et al., 2018), SLAKE (Liu et al., 2021), PathVQA (He et al., 2020), PMC-VQA (v2; Zhang et al. (2024b)), OmniMedVQA (Hu et al., 2024), MMMU (Yue et al., 2024), and MedXpertQA (Zuo et al., 2025). Specifically, we select the Health & Medical subset of MMMU, the multimodal portion of MedXpertQA, and the open-access part of OmniMedVQA. By incorporating these datasets, we encompass diverse range of medical imaging modalities, including X-ray, CT scans, MRI, PET, ultrasound, microscopy, pathology, OCT, dermoscopy, gastrointestinal (GI) examinations, endoscopy, fundus, and medical-related charts, tables, and figures. Text-only QA: We collect multiple medical text evaluation benchmarks, which comprise the test sets of MMLU (Hendrycks et al., 2021), PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022), MedQAUSMLE (Jin et al., 2021), MedBullets (Chen et al., 2025a), MedXpertQA (Zuo et al., 2025), and SuperGPQA (Team et al., 2025). Specifically, we select the text portion of MedXpertQA, the medicine-related questions of MMLU by following the split in Wang et al. (2024b), and the test set of SuperGPQA by the official split criteria. Report Generation: We utilize the official test splits of three established benchmarks: MIMIC-CXR (Johnson et al., 2019), IU-Xray (Demner-Fushman et al., 2016), and CheXpert Plus (Chambon et al., 2024). These datasets primarily consist of chest X-rays paired with corresponding radiology reports. We follow Zhang et al. (2024c) to filter out samples where both findings and impressions are empty. Moreover, we standardize the input format for all questions based on the question types, while adhering to the officially recommended chat template of the candidate medical MLLMs to be evaluated. The input prompt format of each question type is illustrated in Figure 8."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "As the evaluation set comprises multiple task types, each requiring its own evaluation criteria, we implement range of task-specific evaluation protocols accordingly. For the QA task, Accuracy is used as the primary evaluation metric. For multiple-choice questions, two-stage evaluation strategy is adopted: rule-based matching is first applied to assess alignment with answer options; if insufficient, we further employ the official MMMU codebase5 to compute similarity scores and select the option with the highest score as the final answer. For open-ended questions, we directly leverage GPT-4.1 (2025-04-14 version; OpenAI (2025a)) to assess consistency between model outputs and reference answers, with the prompting strategy detailed in Appendix B. For the report generation task, we develop multi-metric evaluation approach that integrates semantic and model-based metrics, specifically tailored to address the challenges posed by the lengthy and highly open-ended nature of the outputs. This design enables more comprehensive and granular assessment of model performance. For the semantic evaluation metrics, we choose Rouge-L (Lin, 2004) and CIDEr (Vedantam et al., 2015) to measure the quality of model-generated reports with respect to the reference answers. For the model-based evaluation metrics, we follow ReXrank (Zhang et al., 2024c) to utilize SembScore (Smit et al., 2020) and RaTEScore (Zhao et al., 2024b) to measure the candidate reports. We also include composite metric RadCliQ-v1 (Yu et al., 2023). The detailed descriptions of these metrics are presented in Appendix B."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "To comprehensively evaluate the performance of our model, Lingshu, on various medical benchmarks, we compare it against diverse set of baseline models, encompassing both proprietary models and open-source MLLMs from both general and medical domains. The open-source MLLMs are further categorized based on the parameter size. Specifically, our evaluation includes the following models: Proprietary Models: GPT-4.1 (2025-04-14 version; OpenAI (2025a)), Claude Sonnet 4 (Anthropic, 2025), and Gemini-2.5-Flash (Preview 05-20 version; DeepMind (2025a)), which are the most representative proprietary models available. Medical MLLMs: BiomedGPT (Zhang et al., 2024a), Med-R1 (Lai et al., 2025), MedVLM-R1 (Pan et al., 2025), MedGemma (Google, 2025), LLaVA-Med (Li et al., 2023a), HuatuoGPT-V (Chen et al., 2024c), BioMediX2 (Mullappilly et al., 2024), HealthGPT (Lin et al., 2025), and MedDr (He et al., 2024). General-purpose MLLMs: Qwen2.5-VL-Instruct (Bai et al., 2025), InternVL2.5 (Chen et al., 2025c), and InternVL3 (Zhu et al., 2025).6 To ensure fair and consistent comparison, all models are evaluated using our MedEvalKit (detailed in 4) within standardized evaluation environment."
        },
        {
            "title": "5.2 Performance Comparison on Medical Multimodal Benchmarks",
            "content": "Table 6 presents detailed comparison between Lingshu and diverse set of both proprietary and open-source MLLMs across seven medical multimodal benchmarks. Proprietary models such as GPT-4.1, Claude Sonnet 4, and Gemini-2.5-Flash, demonstrate consistently strong performance, with Gemini-2.5-Flash attaining the highest average score (65.1) among them. Among open-source models with fewer than 10B parameters, Lingshu-7B achieves the highest average score (61.8), exceeding the best-performing baseline in this category by substantial margin (+4.5). It ranks first on five out of seven benchmarksSLAKE (83.1), PathVQA (61.9), PMC-VQA (56.3), OmniMedVQA (82.9), and MedXpertQA (26.7)and secures second place on the remaining two, i.e., MMMU-Med and VQA-RAD. Notably, on PathVQA, Lingshu-7B outperforms the next best open-source model, MedGemma-4B-IT, by wide margin (61.9 versus 48.8). Furthermore, 5https://github.com/MMMU-Benchmark/MMMU 6Although categorized as general-purpose MLLMs, the InternVL series also incorporates massive medical multimodal data into its training corpus (Li et al., 2025b; Chen et al., 2025c; Zhu et al., 2025). 16 Table 6 Performance comparison of Lingshu with other MLLMs on medical multimodal benchmarks, where bold and underline scores indicate the best and the second-best method, respectively. Note that OMVQA and MedXQA indicate OmniMedVQA and MedXpertQA-Multimodal benchmarks, respectively. Med-R1 is trained on part of the OmniMedVQA test set, making its results on OmniMedVQA meaningless. BiomedGPT and MedDr do not support multi-image inputs, which is required in MedXpertQA-Multimodal. Models colored in light gray , gray and green denote the medical MLLMs, general-purpose MLLMs and our Lingshu, respectively. Models MMMU-Med VQA-RAD SLAKE PathVQA PMC-VQA OMVQA MedXQA Avg. GPT-4.1 Claude Sonnet 4 Gemini-2.5-Flash BiomedGPT Med-R1-2B MedVLM-R1-2B MedGemma-4B-IT LLaVA-Med-7B HuatuoGPT-V-7B BioMediX2-8B Qwen2.5VL-7B InternVL2.5-8B InternVL3-8B Lingshu-7B HealthGPT-14B HuatuoGPT-V-34B MedDr-40B InternVL3-14B Qwen2.5V-32B InternVL2.5-38B InternVL3-38B Lingshu-32B 75.2 74.6 76.9 24.9 34.8 35.2 43.7 29.3 47.3 39.8 50.6 53. 59.2 54.0 49.6 51.8 49.3 63.1 59.6 61.6 65.2 62.3 Proprietary Models 55.5 54.2 55.4 72.2 70.6 75.8 65.0 67.6 68. Open-source Models (<10B) 16.6 39.0 48.6 13.6 54.5 56.0 76.4 48.0 67.8 57.7 67.2 69.0 72.8 11.3 15.3 32.5 48.8 38.8 48.0 37.0 44.1 42.1 48.6 72.5 53.7 67.0 49.2 64.5 59.4 65.4 67.9 55.2 54.4 55.4 27.6 47.4 47.6 49.9 30.5 53.3 43.5 51.9 51.3 53. 75.5 65.5 71.0 27.9 - 77.7 69.8 44.3 74.2 63.3 63.6 81.3 79.1 83.1 61.9 56.3 82. Open-source Models (>10B) 65.0 61.4 65.2 66.3 71.8 61.4 65.4 56.7 44.4 53.5 48.0 41.9 46.9 51.0 66.1 69.5 66.4 72.8 71.2 70.3 72.7 76.5 89.2 65. 56.4 56.6 13.9 54.1 54.5 57.2 56.6 57.9 75.2 74.0 64.3 78.9 68.2 79.9 79.8 45.2 43.3 52.8 - 21.1 20.4 22.3 20.3 21.6 21.8 22.3 21.7 22.4 26. 24.7 22.1 - 23.1 25.2 24.4 25.2 63.4 61.5 65.1 - - 45.4 54.8 37.8 54.2 44.6 52.0 54.0 57.3 61.8 56.2 54.3 - 58.0 56.1 57.4 59.4 83. 30.9 66.6 Lingshu-7B achieves an average performance comparable to GPT-4.1 and Claude Sonnet 4, highlighting its competitiveness despite significantly smaller parameters. With increased model capacity, Lingshu-32B attains state-of-the-art average score of 66.6 across all benchmarks, outperforming both proprietary and open-source counterparts. These results validate the effectiveness of our approach and demonstrate its strong potential for broad application across diverse medical multimodal tasks."
        },
        {
            "title": "5.3 Performance Comparison on Medical Textual Benchmarks",
            "content": "Although primarily designed as multimodal models, Lingshu exhibit remarkable proficiency in handling medical text-based tasks, underscoring its generalization capabilities across data modalities. As reported in Table 7, Lingshu-7B achieves the highest average accuracy among all open-source models with fewer than 10B parameters. It leads on four key benchmarksPubMedQA (76.6), MedQA-USMLE (63.3), Medbullets (56.2), and MedXpertQA (16.5)and ranks close second on the remaining three benchmarks. For larger-scale models, Lingshu-32B delivers even more pronounced improvements. It outperforms the second-best open-source competitor, InternVL3-38B, by an average of 3.4 percentage points in accuracy and secures top performance on six of the seven evaluated benchmarks, which highlights the scalability of Lingshu. Nevertheless, performance gap persists between Lingshu and leading proprietary systems such as GPT-4.1, Claude Sonnet 4, and Gemini-2.5-Flash, particularly in tasks requiring deep clinical reasoning or access to extensive medical knowledge. Closing this gap is primary objective of our ongoing research, which aims to 17 Table 7 Performance of Lingshu and other MLLMs on medical text benchmarks. Note that MedQA, MedXQA, and SGPQA denote MedQA-USMLE, MedXpertQA-Text, and SuperGPQA-Medical benchmarks, respectively. Models colored in light gray , gray and green denote the medical MLLMs, general-purpose MLLMs and our Lingshu, respectively. Models MMLU-Med PubMedQA MedMCQA MedQA Medbullets MedXQA SGPQA Avg. GPT-4.1 Claude Sonnet 4 Gemini-2.5-Flash Med-R1-2B MedVLM-R1-2B MedGemma-4B-IT LLaVA-Med-7B HuatuoGPT-V-7B BioMediX2-8B Qwen2.5VL-7B InternVL2.5-8B InternVL3-8B Lingshu-7B HealthGPT-14B HuatuoGPT-V-34B MedDr-40B InternVL3-14B Qwen2.5VL-32B InternVL2.5-38B InternVL3-38B Lingshu-32B 89.6 91.3 84.2 51.5 51.8 66.7 50.6 69.3 68.6 73.4 74.2 77.5 74. 80.2 74.7 65.2 81.7 83.2 84.6 83.8 84.7 Proprietary Models 77.7 79.3 73.6 89.1 92.1 91.2 75.6 78.6 73.8 Open-source Models (<10B) 39.1 39.7 52.2 39.4 51.2 52.9 52.6 52. 39.9 42.3 56.2 42.0 52.9 58.9 57.3 53.7 62.1 66.2 66.4 72.2 26.4 72.8 75.2 76.4 76.4 75.4 76.6 63.3 57.7 55.9 Open-source Models (>10B) 63.4 54.7 38.4 62.0 63.0 65.9 64. 66.2 58.8 59.2 70.1 71.6 74.4 73.5 68.0 72.2 77.4 77.2 68.4 74.2 73.2 77.8 66.1 74.7 77.0 80.2 77. 33.6 33.8 45.6 34.4 40.9 45.9 42.1 42.4 48.5 56.2 39.8 42.7 44.3 49.5 54.2 55.0 54.6 65.4 30.9 33.6 35.6 11.2 11.8 12.8 9.9 10.1 13.4 12.8 11.6 13. 16.5 11.3 11.4 12.0 14.1 15.6 14.7 16.0 22.7 49.9 56.3 53.3 17.9 19.1 21.6 16.1 21.9 25.2 26.3 26.1 31.2 26. 25.7 26.5 24.0 37.9 37.6 39.9 42.5 41.1 70.0 73.1 69.9 37.0 37.8 46.8 31.3 45.6 48.6 48.7 48.1 52.2 52.8 50.7 48.7 45.8 56.1 56.2 58.4 58. 61.8 further enhance domain adaptation, instruction-following capabilities, and medical reasoning proficiency in future iterations of Lingshu."
        },
        {
            "title": "5.4 Performance Comparison on Report Generation Benchmarks",
            "content": "In addition to the conventional QA benchmarks, we further evaluate the models performance on practical task of high clinical relevance: automatic medical report generation. The results, summarized in Table 8, cover three widely adopted benchmarks: MIMIC-CXR, CheXpert Plus, and IU-Xray. case study about report generation can be found in 6.5. For models with fewer than 10B parameters, Lingshu-7B consistently surpasses all competing models on all three datasets, attaining either the best or second-best score for every evaluation metric. In the larger model size (>10B), performance differences among open-source models become more pronounced. Lingshu-32B delivers the strongest overall results, leading in almost all metrics. Notably, its IU-Xray score is nearly double those of the compared baselines. These quantitative gains are accompanied by two noteworthy qualitative observations: (1) On MIMIC-CXR and CheXpert Plus, larger models occasionally achieve lower performance compared to their smaller counterparts. We posit that this counterintuitive result may be because larger models produce more elaborate and stylistically varied descriptions, nuances that current automatic metrics fail to reward adequately. This finding highlights an urgent need for more sophisticated evaluation protocols for free-text medical report generation tasks. (2) Although the InternVL series achieves commendable scores on multimodal and textual QA benchmarks (see Tables 6 and 7), it underperforms markedly in report generation. This disparity reveals substantial disconnect between benchmark-centric evaluations and the demands of real-world medical applications, underscoring the importance of task-specific assessment when deploying medical AI systems. 18 Table 8 Comprehensive evaluation of medical report generation on MIMIC-CXR, CheXpert Plus, and IU-Xray. Models colored in light gray , gray and green denote the medical MLLMs, general-purpose MLLMs and our Lingshu, respectively. Metrics included are two semantic-based: ROUGE-L and CIDEr, two model-based: RaTE and SembScore (Semb), and one composite metric: RadCliQ-v1 (RadCliQ). As RadCliQ-v1 indicates better performance when the score is lower, we follow Zhang et al. (2024c) to take its reciprocal, i.e., RadCliQ1. All scores are scaled by factor of 100 to enhance clarity and comprehension. CheXpert Plus ROUGE-L CIDEr RaTE Semb RadCliQ1 ROUGE-L CIDEr RaTE Semb RadCliQ1 ROUGE-L CIDEr RaTE Semb RadCliQ1 MIMIC-CXR IU-Xray Models GPT-4.1 Claude Sonnet 4 Gemini-2.5-Flash Med-R1-2B MedVLM-R1-2B MedGemma-4B-IT LLaVA-Med-7B HuatuoGPT-V-7B BioMediX2-8B Qwen2.5VL-7B InternVL2.5-8B InternVL3-8B Lingshu-7B HealthGPT-14B HuatuoGPT-V-34B MedDr-40B InternVL3-14B Qwen2.5VL-32B InternVL2.5-38B InternVL3-38B Lingshu-32B 9.0 20.0 25.4 19.3 20.3 25.6 15.0 23.4 20.0 24.1 23.2 22. 30.8 21.4 23.5 15.7 22.0 15.7 22.7 22.8 Proprietary Models 82.8 56.6 80.7 51.3 45.6 50.3 23.9 19.7 29. 57.1 53.4 59.4 24.5 22.0 23.6 78.8 59.5 72.2 45.5 43.5 44.3 23.2 18.9 27.4 14.8 40.6 35.4 40.1 14.2 41.6 81.0 52.4 29.2 18.3 12.8 43.4 20.0 48.9 69.5 17.7 44.4 52.8 18.4 47.0 63.7 61.8 21.0 47.0 21.5 48.2 66.2 109.4 52.1 30. 64.7 68.5 62.3 63.7 50.2 61.4 64.6 48.4 48.5 45.2 48.6 47.5 47.5 47.9 16.5 23.0 12.2 17.4 17.1 18.2 18.1 Open-source Models (<10B) 42.4 48.3 62.9 52.9 48.2 53.0 55.1 56.2 55.1 69. 18.6 20.9 27.1 18.4 21.3 18.1 22.2 20.6 20.9 26.5 17.8 38.5 37.1 43.5 15.5 38.9 79.0 47.2 29.3 23.5 38.8 45.5 19.3 44.2 64.7 21.6 40.8 47.9 17.2 41.0 62.0 19.7 58.5 43.1 25.2 44.3 65.4 26.8 79.0 45.4 Open-source Models (>10B) 52.7 47.1 47.0 46.5 45.2 54.9 47.2 67. 20.6 22.5 24.1 20.4 15.2 21.6 20.5 25.3 22.7 44.4 66.2 62.8 22.1 42.9 66.1 44.7 24.2 20.7 44.1 60.2 18.5 43.4 54.8 20.3 42.6 60.6 62.7 20.2 43.8 75.9 43.4 24.2 45.5 43.3 44.0 37.6 40.9 46.6 44.0 39.4 43.3 43.1 42.7 43.7 47. 42.6 39.7 44.7 39.4 40.3 45.4 39.4 47.1 30.2 25.4 33.5 16.1 22.7 30.8 18.8 29.6 19.6 26.5 24.8 22.9 124.6 51.3 55.4 88.3 129.3 55.6 47.5 41.0 50. 41.4 38.3 61.1 46.1 103.6 57.0 68.2 40.9 104.3 52.9 40.1 58.8 48.4 78.1 51.1 75.4 51.2 76.2 12.5 22.7 46.8 16.0 40.7 11.6 36.3 36.7 31.3 80.3 72.1 91.6 43.6 54.3 86.7 58.1 63.6 53.8 66.1 67.0 59.9 41.2 180.7 57.6 48. 108.1 22.9 28.2 19.4 24.8 18.9 28.9 25.5 81.9 50.8 108.3 54.4 40.3 62.9 55.0 93.7 51.3 73.3 53.5 96.5 53.5 90.7 16.6 42.2 7.3 38.7 38.1 38.5 33.1 56.9 59.3 48.9 55.0 54.0 69.7 55.2 42. 189.2 63.5 54.6 130.4 28.8 96.4 50.8 30.1 Table 9 Performance comparison of Lingshu and its variant after medical-oriented RL, i.e., Lingshu-RL, on medical multimodal benchmarks. Note that OMVQA and MedXQA indicate OmniMedVQA and MedXpertQA-Multimodal benchmarks, respectively. Models Lingshu-7B Lingshu-RL-7B MMMU-Med VQA-RAD SLAKE PathVQA PMC-VQA OMVQA MedXQA Avg. 54.0 54.7 67.9 68.3 83.1 82. 61.9 61.1 56.3 57.0 82.9 81.3 26.7 26.7 61.8 61."
        },
        {
            "title": "5.5 Performance of Medical-oriented RL",
            "content": "We further conduct the initial exploration of RLVR to boost the medical multimodal reasoning capabilities of Lingshu. As reported in Table 9, while the RL-trained variant (Lingshu-RL-7B) achieves marginal improvements on MMMU-Med (+0.7%), PMC-VQA (+0.7%), and VQA-RAD (+0.4%), it shows performance degradation on SLAKE (0.6%), PathVQA (0.8%), and OMVQA (1.6%). The average performance across all datasets remains relatively stable, indicating that the current RLVR training has not achieved the same improvements as those observed in the math/code domains. We identify two potential causes: (1) Reward Design: Our implementation adopted the prevailing approach of accuracy-oriented, rule-based rewards. However, medical reasoning is knowledge-driven, and acceptable answers, especially for open-ended questions, often exhibit substantial linguistic variability. Simple heuristics reward, therefore, may provide an unreliable signal of correctness, which can misdirect optimization. (2) Data Quality: significant portion of medical multimodal problems do not necessitate complex reasoning processes, such as straightforward classification problems like identifying organ types. These data fail to effectively contribute to enhancing the models reasoning capabilities while introducing unnecessary noise to the training process. In this case, our current approach lacks criteria for selecting such reasoning-focused medical problems. 19 Table 10 Ablation study examining data composition during the medical instruction tuning stage. Data denotes the reduced amount of training data. The symbol indicates performance that is significantly lower compared to Lingshu, while denotes an even more substantial decrease in performance. Data MMMU-Med VQA-RAD SLAKE PathVQA PMC-VQA OMVQA MedXQA Models Qwen2.5VL-7B Lingshu-7B - Medical Multimodal - General Multimodal - General Text - Medical Text - - 50.6 54.0 64.5 67.9 67.2 83.1 44.1 61.9 2.7M 54.1 1.2M 51.4 1M 53.3 50.9 173K Overall Data Composition 66.5 66.7 65.2 66.7 69.0 83.6 83.6 82.4 - Medical Caption (stage 3) - Medical Caption (stage 1&2) - Synthetic Multimodal - Medical CoT-Text Effect of Selected High-value Data 66.3 2M 52.9 67.4 4.8M 53.8 67.4 1.1M 52.1 66.3 52.6 162K 82.7 83.3 81.6 83.6 51.9 56. 56.2 54.5 55.5 54.8 54.1 54.9 56.5 53.6 63.6 82.9 22.3 26.7 78.0 81.3 81.3 82.0 81.4 81.5 71.6 81.6 25.6 26.0 25.9 24.0 26.8 26.7 26.1 25.3 45.2 62.1 61.4 60.2 61.7 60.6 60.1 61."
        },
        {
            "title": "5.6 Ablation Study on Data Composition",
            "content": "To gain insights into the impact of different data types during the medical instruction tuning stage, we conducted an ablation study by selectively removing specific categories of training data, as shown in Table 10. Overall, all four categories of training data outlined in 3.1.3, namely, {Medical, General} {Multimodal, Text}, collectively contribute to Lingshus medical multimodal task-solving ability, even when some data sources are not explicitly tailored for medical domain. Notably, medical textual data emerges as the most critical component; the removal of merely 173K samples leads to substantial performance degradation across five of the seven evaluated tasks. Each data type contributes differently depending on the task. For instance, the exclusion of medical multimodal data (- Medical Multimodal ) results in pronounced performance declines on benchmarks such as SLAKE, PathVQA, and OmniMedVQA. This likely reflects the heavy reliance of these datasets on histopathology and X-ray imagery, which are well represented in the medical multimodal training corpus. In contrast, removing general multimodal data (- General Multimodal ) has more adverse effect on tasks like MMMU-Med and PMC-VQA. These tasks appear to require nuanced understanding of medical content presented in complex formats such as public health chartsknowledge that is underrepresented in current medical multimodal datasets. We hypothesize that such specialized knowledge can be effectively supplemented through the inclusion of general-domain multimodal and textual data, thereby compensating for existing limitations in domain-specific resources. These observations reinforce our hypothesis: acquiring comprehensive medical knowledge for multimodal problem solving should not rely exclusively on medical multimodal data. Rather, it necessitates the integration of medical text and general-domain data to develop more robust and generalizable model. Beyond the overall data composition, we examine the contributions of several high-value data components: Medical Caption (Stage 3): As mentioned in 3.1.3, we retain subset of high-quality medical caption data during Stage 3 to mitigate local view bias. Experimental results indicate substantial performance degradation on benchmarks such as MMMU-Med, VQA-RAD, PMC-VQA, and OmniMedVQA upon its removal. This underscores the continued importance of incorporating high-quality caption data in the final stages of instruction tuning. Medical Caption (Stage 1&2): To assess the role of early-stage training, we bypass Stages 1 and 2 and directly fine-tune the model using Stage 3 instruction data. This variant leads to moderate performance drops on PathVQA, PMC-VQA, and OmniMedVQA, suggesting that early-stage alignment remains critical step in the training pipeline. Synthetic Multimodal Medical data: Excluding all synthetic multimodal data, as defined in 2.2, yields notable declines in performance across MMMU-Med, SLAKE, PathVQA, and especially OmniMedVQA, 20 which includes broad spectrum of medical imaging modalities. These findings highlight the essential role of synthetic data in enriching modality diversity and compensating for gaps in existing datasets through the infusion of additional medical knowledge. Distilled Textual Medical Reasoning Data: Building on prior evidence of the significance of textual data, we further isolate the impact of distilled medical reasoning texts. Results reveal that removing only this subset produces performance drops on five out of seven tasks, nearly equivalent to removing all medical textual data. We attribute this effect to the dense medical knowledge encoded within chain-of-thought (CoT) reasoning traces. This finding suggests promising direction for future research: developing diverse, high-quality corpora of medical reasoning texts to systematically enhance medical expertise acquisition."
        },
        {
            "title": "5.7 Results per Modality on OmniMedVQA",
            "content": "The analysis in 5.2 demonstrates the effectiveness of Lingshu in diverse medical tasks. Here, we investigate how these MLLMs perform in terms of different medical modalities. Figure 9 benchmarks Lingshu-7B against the four strongest baselines based on their average performance in medical multimodal tasks, i.e., InternVL-3-8B, InternVL-2.5-8B, HuatuoGPT-V-7B, and MedGemma-4B-IT, on OmniMedVQA. Figure 9 Performance of different models across different medical modalities on OmniMedVQA. Across the eight medical imaging modalities evaluated, Lingshu either matches or exceeds the competitors on most fronts. It achieves leading performance in microscopy, MRI, dermoscopy and OCT modalities that demand precise recognition of micro-textural and meso-structural cues; this suggests that Lingshu is particularly effective at capturing both high-frequency detail and anatomical regularities. Lingshu also delivers promising performance on ultrasound and fundus photography, underscoring its robustness in domains characterized by speckle noise (ultrasound) and fine vascular patterns (fundus). In CT and X-ray, Lingshu trails the best baseline by modest margin. Overall, Lingshu establishes itself as modality-agnostic yet high-resolution specialist, outperforming larger competitors despite its comparatively compact parameter budget. The gaps identified in modalities like CT and X-ray offer targeted opportunities for improving Lingshu in the future."
        },
        {
            "title": "5.8 Analysis of Data Volume Scaling\nFigure 10 traces accuracy as the number of training samples is progressively expanded from 0% to 100% of its\nfull size. A common pattern emerges across all seven benchmarks: accuracy rises sharply with the first quarter\nof additional data, then continues to climb—albeit with diminishing marginal gains—until it converges near\nthe 75˘100% mark. The smoothed curves reveal a near-monotonic trajectory that is partially obscured by the\nvolatility of the raw scores, confirming that the overall trend is robust.",
            "content": "Two further observations merit emphasis. First, tasks that are intrinsically image-centric and less textintensive (e.g., SLAKE and PathVQA) saturate earlier than concept-heavy challenges such as MedXpertQA, which still registers steady improvements at the upper end of the data scale. Second, the aggregate plot mirrors the individual tasks, climbing from roughly 52% to 62% accuracy, thereby quantifying clear data scaling for training medical MLLMs. Taken together, these findings highlight the decisive role of large-scale data acquisition while also signaling an eventual plateau, after which further progress will likely depend on qualitative data enrichment or architectural innovation rather than sheer volume alone. 21 Figure 10 The impact on downstream task performance when increasing the scale of training data."
        },
        {
            "title": "6 Case Studies",
            "content": "This section presents illustrative case studies that showcase Lingshu in various clinical applications, such as medical diagnosis and report generation. All examples are produced with the Lingshu-32B model and, when feasible, compared with equivalently sized baseline models. The resulting comparisons elucidate the tangible advantages Lingshu can offer to both healthcare professionals and patients."
        },
        {
            "title": "6.1 Visual Question Answering across Different Medical Imaging Modalities",
            "content": "Figure 11 presents the medical VQA cases of Lingshu across eight distinct medical modalities. The results highlight Lingshu is capable of transparent its reasoning process to derive reliable decision. Reasoning trajectory. Lingshu traces diagnostic path that mirrors standard clinical workflows. In the chest-radiograph example, for instance, it inspects anatomical regions in the canonical order: first the lungs, then the heart, followed by the bony structures, and finally the diaphragm and costophrenic angles. This systematic progression ensures comprehensive coverage of relevant findings. Decision refinement. Rather than issuing final diagnosis immediately, the model incrementally constrains the differential. In the dermoscopy case, the initial hypothesis set included seborrheic keratosis, basalcell carcinoma, and dermatofibroma. Continued analysis reduced the options to the first and third, before converging on dermatofibroma after incorporating additional visual cues. This step-wise narrowing underscores the reliability and clinical plausibility of Lingshus decision-making process."
        },
        {
            "title": "6.2 Medical Diagnosis",
            "content": "Figure 12 compares the diagnostic reasoning of Lingshu with that of the second-best model, InternVL3, on medical diagnosis case. Guided by the dome-shaped retinal elevation observed in the image, Lingshu initially considers two differential diagnoses, macular edema and macular hole, and subsequently converges on macular edema after incorporating additional clinical cues such as subretinal fluid accumulation and cystoid spaces. InternVL3, by contrast, issues an incorrect verdict with unwarranted confidence, neither assessing alternative hypotheses nor integrating ancillary visual evidence. This discrepancy aligns with the broader modality analysis in 6.1, highlighting Lingshus more clinically grounded and cautious diagnostic strategy, resulting in more dependable outcomes. 22 Figure 11 Case studies on Lingshu in visual question answering across various medical imaging modalities."
        },
        {
            "title": "6.3 Medical Knowledge",
            "content": "Beyond its proficiency in clinical image interpretation, Lingshu exhibits sustained mastery of specialised biomedical knowledge. As depicted in Figure 13, Lingshu clearly interprets the intricate processes occurring within synapses. Notably, it precisely identifies the synapse type as noradrenergic, based on thorough understanding of neurotransmitter and receptor interactions. This is in stark contrast to the original Qwen2.5VL model, which, while adept at describing synaptic processes, fails to accurately classify them, crucial piece of information. Moreover, the Qwen2.5-VL produces great number of incoherent or irrelevant content, sign of unreliability in its understanding and application of medical knowledge."
        },
        {
            "title": "6.4 Public Health",
            "content": "As shown in Figure 14, public health case, Lingshu precisely extracts the relevant elements from the cohort-study table, identifying incidence rates of 58 per 100, 000 in the smoking (exposed) cohort and 5 per 100, 000 among non-smokers. It then applies the standard formula, subtracting the latter from the former to obtain an attributable risk of 53 per 100, 000. This example underscores Lingshus capacity to interpret tabular data accurately, implement appropriate public-health calculations, and articulate the population-level implications of those results."
        },
        {
            "title": "6.5 Report Generation",
            "content": "Figure 15 presents an X-ray report produced by Lingshu. It rigorously follows the conventional radiological structure, i.e., distinct Findings followed by an Impression. Within the Findings, it correctly delineates suspected abnormalities involving both the cardiac silhouette and pulmonary parenchyma. Furthermore, Lingshu exhibits clinical insight by appending actionable recommendations, such as obtaining follow-up radiograph after diuretic therapy. These results indicate that the model not only detects radiographic 23 Figure 12 The comparison between Lingshu and InternVL3 (the second-best MLLM in Table 6) on medical diagnosis. The green parts are some important reasoning during the generation of Lingshu. The red part indicates the thoughtless reasoning of InternVL3. Figure 13 Case study of Lingshu in explaining medical knowledge, where the red part is the incoherent or irrelevant content generated by Qwen2.5-VL. anomalies with precision but also furnishes contextually appropriate management advice, reflecting the holistic practices of experienced radiologists."
        },
        {
            "title": "6.6 Patient-Doctor Dialogue",
            "content": "Figure 16 showcases Lingshus text-understanding ability prowess within real-world consultation scenario. The model interprets the patients concerns, proposes plausible differential diagnoses, directs the individual to the relevant clinical specialty, and specifies suitable diagnostic tests. Such performance underscores Lingshus promise as an effective assistant for medical consultations and broader healthcare support."
        },
        {
            "title": "7 Related work",
            "content": "The rapid development of multimodal large language models (MLLMs) has achieved tremendous improvements across various domains, which has sparked widespread interest in their applications within the medical field and driven significant progress in this area, highlighting their enormous potential within healthcare scenarios (Tian et al., 2023; AlSaad et al., 2024). Early endeavors attempt to combine LLMs with specialized medical visual encoders through linear transformation layer to establish vision-language alignment for medical image understanding and analysis (Li et al., 2023a; Moor et al., 2023; Liu et al., 2023; Zhang et al., 2024a). Building upon this foundation, subsequent research has adopted similar architectural paradigms for developing medical MLLMs. These efforts incorporate various strategies, such as constructing more comprehensive training datasets (Ikezogwo et al., 2023; Chen et al., 2024c; Li et al., 2025b; Hamamci et al., 24 Figure 14 Case study of Lingshu in dealing with public health problem. Figure 15 Case study of Lingshu in generating medical report. 2025), designing sophisticated training recipes (Nath et al., 2025; Wang et al., 2024a), employing efficient fine-tuning techniques (Lin et al., 2025), integrating mixture-of-experts mechanisms (He et al., 2024), and leveraging reinforcement learning (Lai et al., 2025; Pan et al., 2025), to improve performance across diverse medical tasks. In parallel, widely used proprietary models have integrated medical knowledge into their platforms, such as Med-Gemini (DeepMind, 2024b) and Med-PaLM (Singhal et al., 2023a,b), and achieve strong performance on diverse medical tasks (Xie et al., 2024; Saab et al., 2024; Yang et al., 2024; Aydin and Karaarslan, 2025; Arora et al., 2025). Beyond general medical MLLMs, many studies also explore specialized MLLMs tailored to particular diseases or clinical contexts to support better targeted downstream applications, such as models developed specifically for pathology (Lu et al., 2024; Wang et al., 2024c; Zhao et al., 2024a; Seyfioglu et al., 2025), radiology (Hyland et al., 2024; Christensen et al., 2024; Shui et al., 2025; Zambrano Chaves et al., 2025; Pai et al., 2025; Tanno et al., 2025), ophthalmology (Deng et al., 2024), pan-cancer analysis (Keyl et al., 2025), specific cancer types (Sammut et al., 2022; Pai et al., 2024; Niu et al., 2025), etc. Furthermore, building on the capabilities of medical MLLMs, researchers have initiated the development of medical agents that extend model functionality through integration with external medical tools, as illustrated by systems like AgentClinic (Schmidgall et al., 2025), Agent Hospital (Li et al., 2025a), AIME (DeepMind, 2024a), and MedAgent-Pro (Wang et al., 2025b). In contrast, our work focuses on developing multimodal foundation model that accommodates diverse medical modalities and supports tasks such as question answering, diagnosis, report generation, and diverse clinical applications. Figure 16 Case study of Lingshu in real-life medical support."
        },
        {
            "title": "8.1 Conclusion",
            "content": "In this work, we introduce Lingshu, domain-specialized multimodal foundation model designed for medical AI, addressing key challenges in medical data construction, model training, and evaluation. By unifying large-scale, high-quality medical and general-domain corpora with comprehensive data curation pipeline, encompassing image captions, QA pairs, and CoT annotations, Lingshu acquires an expansive and diverse knowledge base. Our multi-stage training paradigm progressively enhances the models medical understanding and problem-solving skills. Besides, we also explore the potential of incorporating advanced reinforcement learning with verifiable rewards to further boost performance. Beyond model development, we emphasize the importance of evaluation standardization in the medical domain, and introduce MedEvalKit, unified framework that consolidates multiple multimodal and textual benchmarks while enforcing standardized evaluation. This toolkit enables fair, reproducible, and transparent comparisons across models. Extensive experiments confirm the models superiority. Across broad spectrum of medical VQA and report generation tasks, Lingshu consistently surpasses the open-source baselines and narrows the gaps to proprietary models. Overall, Lingshu and MedEvalKit provide high-performing model, robust evaluation toolkit, and empirically grounded guidelines on data curation, staged training, and evaluation. These contributions represent step forward in aligning MLLMs with real-world medical applications."
        },
        {
            "title": "8.2 Limitations",
            "content": "Despite Lingshus promising performance across various medical tasks, several limitations remain: Data Quality and Diversity: While extensive medical multimodal and textual data are collected and synthesized, yielding notable gains. Dataset quality and diversity remain limited. Open-source medical multimodal data often exhibit low annotation accuracy, poor image resolution, and uneven modality distribution. Additionally, in the absence of full expert medical supervision, many samples are generated using proprietary models, which may introduce severe hallucinations and factual errors. Despite rigorous quality control and manual verification, such issues persist and may hinder the models generalizability to downstream tasks. Model Performance and Generalization: Although Lingshu achieves promising results across several medical benchmarks, particularly in question answering and report generation, it still underperforms compared to state-of-the-art proprietary models. Moreover, its generalization to broader and more diverse medical tasks remains insufficiently explored. Enhancing its capabilities through the inclusion of additional downstream 26 scenarios and modality types is promising but entails substantial challenges in data integration, preprocessing, and model design. Training Paradigm and Reinforcement Learning: While we validate the effectiveness of our data strategy and training paradigm through ablation studies, the optimal data mixture and training configuration remain insufficiently explored. Our exploration of RLVR in medical settings is preliminary; although it offers moderate results in downstream tasks, the overall impact is limited, and deeper understanding of its effective application in medical contexts is still needed."
        },
        {
            "title": "8.3 Future Work",
            "content": "Building on Lingshu, we identify several directions for future work to address current limitations and further enhance the models capabilities: High-Quality Medical Data Construction: Given the scarcity of high-quality multimodal medical data, it is crucial to increase efforts in constructing and curating more diverse, enriched, and high-quality medical image-text datasets. key challenge is that medical data quality often requires expert evaluation, which is resource-intensive and infeasible to scale. To address this, it is essential to build robust data generation and quality control systems, including specialized assessment models and fine-grained data synthesis pipelines. Incorporating human-in-the-loop mechanisms for iterative refinement may further improve the efficiency and reliability of the data production process. Toward Comprehensive Medical Multimodal Benchmarking: Current medical multimodal benchmarks fall short of capturing real-world clinical complexity, limiting their ability to evaluate model utility in practical downstream tasks. The recent release of HealthBench (Arora et al., 2025) by OpenAIa realistic, open-ended evaluation framework for assessing LLMs across professional dimensionshighlights the need for similarly comprehensive and application-aligned benchmarks in the medical multimodal domain. Model Expansion: Exploring advanced model architectures tailored to medical multimodal scenarios is essential. Medical data encompasses not only general formats of image and video sequence but also 3D imaging (e.g., CT, MRI), ultra-high-resolution images (e.g., histopathology WSI), and molecular, protein, and genomic data. Although existing frameworks (DeepMind, 2024b) can be adapted to these modalities through optimized data pipelines, such adaptations may introduce information loss and limit the models ability to capture subtle variations in medical modalities. Future work will focus on extending Lingshu to natively support WSI, 3D imaging, and omics data, enabling seamless integration and comprehensive understanding across diverse medical modalities. Post-Training Techniques: Further exploration of post-training techniques is essential, particularly in adapting reinforcement learning for medical MLLMs to better align outputs with task-specific goals and contextual demands. Unlike the logic-driven nature of mathematical or programming reasoning, medical reasoning is predominantly knowledge-driven, relying on domain expertise and clinical experience. This distinction underscores the necessity for tailored reinforcement learning approaches in medical contexts, including the use of specialized reward functions, custom reward models, and process-level supervision. Evaluation: Currently, MedEvalKit primarily assesses MLLM capabilities within evaluation metrics from general domains. We have attempted to incorporate certain domain-specific metrics into evaluations for particular tasks such as report generation. In future work, it is crucial to introduce medical-specific evaluation measures, such as the Concordance Index (c-index), Clinical Efficacy Score, and Decision Curve Analysis, to assess model performance in real medical applications. Additionally, exploring methodologies for integrating human expert evaluation to enhance model credibility and safety indicates another promising direction for future investigation. Roadmap of Lingshu: This work marks our initial effort in the medical domain. Future developments will focus on enhancing Lingshu through improved and diversified medical datasets and benchmarks, exploring more effective post-training strategies tailored to medical tasks, scaling to larger model sizes, and optimizing performance for specific applications such as report generation, diagnosis, and staging prediction. We also plan to expand language support and develop efficient, task-oriented agent systems based on Lingshu to address the diverse needs of real-world clinical scenarios."
        },
        {
            "title": "References",
            "content": "Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. https://www.sciencedirect.com/science/article/pii/S2352340919312181. Rawan AlSaad, Alaa Abd-alrazaq, Sabri Boughorbel, Arfan Ahmed, Max-Antoine Renault, Rafat Damseh, and Javaid Sheikh. Multimodal large language models in health care: Applications, challenges, and future outlook. Med Internet Res, 26, Sep 2024. https://www.jmir.org/2024/1/e59505. Anthropic. Introducing claude 4, May 2025. https://www.anthropic.com/news/claude-4. Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, and Karan Singhal. Healthbench: Evaluating large language models towards improved human health. ArXiv, abs/2505.08775, 2025. https://arxiv. org/abs/2505.08775. Omer Aydin and Enis Karaarslan. Openai chatgpt interprets radiological images: Gpt-4 as medical doctor for fast check-up. ArXiv, abs/2501.06269, 2025. https://arxiv.org/abs/2501.06269. Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo Oh, Lei JI, Eric Chang, Tackeun Kim, et al. Mimic-ext-mimic-cxr-vqa: complex, diverse, and large-scale visual question answering dataset for chest x-ray images, 2024. https://physionet.org/content/mimic-ext-mimic-cxr-vqa/1.0.0/. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. https://arxiv.org/abs/2502.13923. Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. https://aclanthology.org/W05-0909/. Asma Ben Abacha and Dina Demner-Fushman. question-entailment approach to question answering. BMC Bioinformatics, 20(511), 2019. https://doi.org/10.1186/s12859-019-3119-4. Asma Ben Abacha, Sadid Hasan, Vivek Datla, Dina Demner-Fushman, and Henning Müller. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes, 2019. https://ceur-ws.org/Vol-2380/paper_272.pdf. BUAADreamer. llava-med-zh-instruct-60k dataset, 2024. https://huggingface.co/datasets/BUAADreamer/ llava-med-zh-instruct-60k. Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack, Shih-Cheng Huang, Zhihong Chen, Maya Varma, Steven QH Truong, Chu The Chuong, and Curtis P. Langlotz. Chexpert plus: Augmenting large chest x-ray dataset with text radiology reports, patient demographics and additional image formats. ArXiv, abs/2405.19538, 2024. https://arxiv.org/abs/2405.19538. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models. ArXiv, abs/2402.11684, 2024a. https://arxiv.org/abs/2402.11684. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering and explaining challenging medical questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35633599, Albuquerque, New Mexico, April 2025a. Association for Computational Linguistics. https: //aclanthology.org/2025.naacl-long.182/. Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, and Benyou Wang. Huatuogpt-ii, one-stage training for medical adaption of llms. ArXiv, abs/2311.09774, 2023. https://arxiv.org/abs/2311.09774. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. ArXiv, abs/2412.18925, 2024b. https: //arxiv.org/abs/2412.18925. 28 Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, and Benyou Wang. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. ArXiv, abs/2406.19280, 2024c. https://arxiv.org/abs/ 2406.19280. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. ArXiv, abs/2501.17811, 2025b. https://arxiv.org/abs/2501.17811. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. ArXiv, abs/2412.05271, 2025c. https://arxiv.org/abs/2412.05271. Muhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid Mazhar, Muhammad Abdul Kadir, Zaid Bin Mahbub, Khandakar Reajul Islam, Muhammad Salman Khan, Atif Iqbal, Nasser Al Emadi, Mamun Bin Ibne Reaz, and Mohammad Tariqul Islam. Can ai help in screening viral and covid-19 pneumonia? IEEE Access, 8:132665132676, 2020. https://ieeexplore.ieee.org/document/9144185. Matthew Christensen, Milos Vukadinovic, Neal Yuan, and David Ouyang. Visionlanguage foundation model https://doi.org/10.1038/ for echocardiogram interpretation. Nature Medicine, 30:14811488, May 2024. s41591-024-02959-y. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In The Second Conference on Parsimony and Learning (Recent Spotlight Track), 2025. https://openreview.net/forum?id=d3E3LWmTar. Maria Correia de Verdier, Rachit Saluja, Louis Gagnon, Dominic LaBella, Ujjwall Baid, Nourel Hoda Tahon, Martha Foltyn-Dumitru, Jikai Zhang, Maram Alafif, Saif Baig, et al. The 2024 brain tumor segmentation (brats) challenge: glioma segmentation on post-treatment mri. ArXiv, abs/2405.18368, 2024. https://arxiv.org/abs/2405.18368. DeepMind. Google cal amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/. conversations, reasoning research system 2024a. Amie: and Jan ai for medihttps://research.google/blog/ diagnostic Google DeepMind. Advancing medical ai with med-gemini, May 2024b. https://research.google/blog/ advancing-medical-ai-with-med-gemini/. Google DeepMind. Gemini: family of highly capable multimodal models. ArXiv, abs/2312.11805, 2025a. https: //arxiv.org/abs/2312.11805. Google DeepMind. Gemini 2.5: Our most intelligent ai model, Mar 2025b. https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv, abs/2501.12948, 2025. https://arxiv.org/abs/2501.12948. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. ArXiv, abs/2409.17146, 2024. https://arxiv.org/abs/2409.17146. Dina Demner-Fushman, Marc Kohli, Marc Rosenman, Sonya Shooshan, Laritza Rodriguez, Sameer Antani, George Thoma, and Clement McDonald. Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304310, 2016. https://doi.org/10. 1093/jamia/ocv080. 29 Zhuo Deng, Weihao Gao, Chucheng Chen, Zhiyuan Niu, Zheng Gong, Ruiheng Zhang, Zhenjie Cao, Fang Li, Zhaoyi Ma, Wenbin Wei, and Lan Ma. Ophglm: An ophthalmology large language-and-vision assistant. Artif. Intell. Med., 157(C), November 2024. https://doi.org/10.1016/j.artmed.2024.103001. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. https: //aclanthology.org/N19-1423/. Kexin Ding, Mu Zhou, He Wang, Olivier Gevaert, Dimitris Metaxas, and Shaoting Zhang. large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer. Scientific Data, 10(1):231, 2023. https://doi.org/10.1038/s41597-023-02125-y. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, MM 24, page 1119811201, New York, NY, USA, 2024. Association for Computing Machinery. https://doi.org/10.1145/3664647.3685520. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. https://arxiv.org/abs/2407.21783. Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benes, Simon Graham, Mostafa Jahanifar, Syed Ali Khurram, Ayesha Azam, Katherine Hewitt, and Nasir Rajpoot. Pannuke dataset extension, insights and baselines. ArXiv, abs/2003.10778, 2020. https://arxiv.org/abs/2003.10778. Sheng Gan. Bccd dataset, 2018. https://github.com/Shenggan/BCCD_Dataset. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. https://zenodo.org/records/12608602. Lidia Garrucho, Kaisar Kushibar, Claire-Anne Reidel, Smriti Joshi, Richard Osuala, Apostolia Tsirikoglou, Maciej Bobowicz, Javier del Riego, Alessandro Catanese, Katarzyna Gwoździewicz, Maria-Laura Cosaka, Pasant AboElhoda, Sara Tantawy, Shorouq Sakrana, Norhan Shawky-Abdelfatah, Amr Muhammad Abdo Salem, Androniki Kozana, Eugen Divjak, Gordana Ivanac, Katerina Nikiforaki, Michail Klontzas, Rosa García-Dosdá, Meltem Gulsun-Akpinar, Oğuz Lafcı, Ritse Mann, Carlos Martín-Isla, Fred Prior, Kostas Marias, Martijn Starmans, Fredrik Strand, Oliver Díaz, Laura Igual, and Karim Lekadir. large-scale multicenter breast cancer dce-mri benchmark dataset with expert segmentations. Scientific Data, 12(1):453, 2025. https://doi.org/10. 1038/s41597-025-04707-4. Google. Medgemma: gemma 3 variant optimized for medical text and image comprehension, May 2025. https: //deepmind.google/models/gemma/medgemma/. Shivanand Gornale and Pooja Patravali. Digital knee x-ray images, 2020. https://doi.org/10.17632/t9ndx37v5h.1. Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri. Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1820 1828, 2021. https://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Groh_Evaluating_Deep_Neural_ Networks_Trained_on_Clinical_Images_in_Dermatology_CVPRW_2021_paper.pdf. Ibrahim Ethem Hamamci, Sezgin Er, Chenyu Wang, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Doga, Omer Faruk Durugol, Weicheng Dai, Murong Xu, Muhammed Furkan Dasdelen, Bastian Wittmann, Tamaz Amiranashvili, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Christian Bluethgen, Kayhan Batmanghelich, Mehmet Kemal Ozdemir, and Bjoern Menze. Developing generalist foundation models from multimodal dataset for 3d computed tomography. ArXiv, abs/2403.17834, 2025. https://arxiv.org/abs/2403.17834. Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, Ronald Cheong Kin Chan, Varut Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, and Hao Chen. Gsco: Towards generalizable ai in medicine via generalist-specialist collaboration. ArXiv, abs/2404.15127, 2024. https://arxiv.org/abs/2404.15127. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. ArXiv, abs/2003.10286, 2020. https://arxiv.org/abs/2003.10286. Yuting He, Guanyu Yang, Jian Yang, Rongjun Ge, Youyong Kong, Xiaomei Zhu, Shaobo Zhang, Pengfei Shao, Huazhong Shu, Jean-Louis Dillenseger, Jean-Louis Coatrieux, and Shuo Li. Meta grayscale adaptive network for 3d integrated renal structures segmentation. Medical Image Analysis, 71:102055, 2021. https://www.sciencedirect. com/science/article/pii/S1361841521001018. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. https://openreview.net/forum?id=d7KBjmI3GmQ. Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. OmnimedIn Proceedings of vqa: new large-scale comprehensive evaluation benchmark for medical the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2217022183, June 2024. https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_OmniMedVQA_A_New_Large-Scale_ Comprehensive_Evaluation_Benchmark_for_Medical_LVLM_CVPR_2024_paper.pdf. lvlm. hw hwei. Medthoughts-8k dataset, 2025. https://huggingface.co/datasets/hw-hwei/MedThoughts-8K. Stephanie L. Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Mercy Ranjit, Anton Schwaighofer, Fernando Pérez-García, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, Noel Codella, Matthew P. Lungren, Maria Teodora Wetscherek, Ozan Oktay, and Javier Alvarez-Valle. Maira-1: specialised large multimodal model for radiology report generation. ArXiv, abs/2311.13668, 2024. https://arxiv.org/abs/2311.13668. Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. https://openreview.net/forum?id=OL2JQoO0kq. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590597, 2019. https://doi.org/10.1609/aaai.v33i01.3301590. Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven Truong, Du Nguyen Duong Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew Lungren, Andrew Ng, Curtis Langlotz, Pranav Rajpurkar, and Pranav Rajpurkar. Radgraph: Extracting clinical entities and relations from radiology reports. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/ file/c8ffe9a587b126f152ed3d89a146b445-Paper-round1.pdf. Soroush Javadi and Seyed Abolghasem Mirroshandel. novel deep learning method for automatic assessment of human sperm images. Computers in Biology and Medicine, 109:182194, 2019. https://www.sciencedirect.com/ science/article/abs/pii/S0010482519301386. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14), 2021. https://www.mdpi.com/2076-3417/11/14/6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. ArXiv, abs/1909.06146, 2019. https://arxiv.org/abs/1909.06146. 31 Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Roger G. Mark, and Steven Horng. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):110, 2019. https://doi.org/10.1038/s41597-019-0322-0. Julius Keyl, Philipp Keyl, Grégoire Montavon, René Hosch, Alexander Brehmer, Liliana Mochmann, Philipp Jurmeister, Gabriel Dernbach, Moon Kim, Sven Koitka, Sebastian Bauer, Nikolaos Bechrakis, Michael Forsting, Dagmar FührerSakel, Martin Glas, Viktor Grünwald, Boris Hadaschik, Johannes Haubold, Ken Herrmann, Stefan Kasper, Rainer Kimmig, Stephan Lang, Tienush Rassaf, Alexander Roesch, Dirk Schadendorf, Jens T. Siveke, Martin Stuschke, Ulrich Sure, Matthias Totzeck, Anja Welt, Marcel Wiesweg, Hideo A. Baba, Felix Nensa, Jan Egger, Klaus-Robert Müller, Martin Schuler, Frederick Klauschen, and Jens Kleesiek. Decoding pan-cancer treatment outcomes using multimodal real-world data and explainable artificial intelligence. Nature Cancer, 6:307322, February 2025. https://doi.org/10.1038/s43018-024-00891-1. Kimi, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, and Zongyu Lin. Kimi-vl technical report. ArXiv, abs/2504.07491, 2025. https://arxiv.org/abs/2504.07491. Oleksandr Kovalyk, Juan Morales-Sánchez, Rafael Verdú-Monedero, Inmaculada Sellés-Navarro, Ana Palazón-Cabanes, and José-Luis Sancho-Gómez. Papila: Dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment. Scientific Data, 9(1):291, 2022. https://doi.org/10.1038/s41597-022-01388-1. Nicholas Kurtansky, Veronica Rotemberg, Maura Gillis, Kivanc Kose, Walter Reade, and Ashley Chow. Isic 2024 - skin cancer detection with 3d-tbp, 2024. https://kaggle.com/competitions/isic-2024-challenge. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, page 611626, New York, NY, USA, 2023. Association for Computing Machinery. https://doi.org/10.1145/3600006.3613165. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. ArXiv, abs/2503.13939, 2025. https://arxiv.org/abs/2503.13939. Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. https://doi.org/10.1038/sdata. 2018.251. Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine, 388(13):12331239, 2023. https://www.nejm.org/doi/full/10.1056/NEJMsr2214184. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. LLaVA-med: Training large language-and-vision assistant for biomedicine in one day. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023a. https://openreview.net/forum?id=GSuP99u2kR. Junkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng Zhang, Xinhui Kang, Siyu Wang, Peng Li, Ya-Qin Zhang, Weizhi Ma, and Yang Liu. Agent hospital: simulacrum of hospital with evolvable medical agents. ArXiv, abs/2405.02957, 2025a. https://arxiv.org/abs/2405.02957. Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, Yanjun Li, Pengcheng Chen, Xiaowei Hu, Zhongying Deng, Yuanfeng Ji, Jin Ye, Yu Qiao, and Junjun He. Gmai-vl & gmai-vl-5.5m: large vision-language model and comprehensive multimodal dataset towards general medical ai. ArXiv, abs/2411.14522, 2025b. https://arxiv.org/abs/2411.14522. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023b. https://doi.org/10.7759/cureus.40895. 32 Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. https://aclanthology.org/ W04-1013/. Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, and Beng Chin Ooi. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. ArXiv, abs/2502.09838, 2025. https://arxiv.org/abs/2502.09838. Wei-Ming Lin. Resized 2015-2019 diabetic retinopathy detection, 2022. https://www.kaggle.com/datasets/c7934597/ resized-2015-2019-diabetic-retinopathy-detection. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In Medical Image Computing and Computer Assisted Intervention MICCAI 2023, pages 525536, Cham, 2023. Springer Nature Switzerland. https://doi. org/10.1007/978-3-031-43993-3_51. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 16501654, 2021. https://ieeexplore.ieee.org/document/9434010. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26296 26306, June 2024. https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_ Visual_Instruction_Tuning_CVPR_2024_paper.pdf. Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, and Yining Hua. Qilin-med-vl: Towards chinese large vision-language model for general healthcare. ArXiv, abs/2310.17956, 2023. https://arxiv.org/abs/2310.17956. Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqin Zhang, and Yizhou Yu. Sdr-former: siamese dual-resolution transformer for liver lesion classification using 3d multi-phase imaging. Neural Networks, 185:107228, 2025. https://www.sciencedirect.com/science/article/pii/S0893608025001078. Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Kenji Ikamura, Georg Gerber, Ivy Liang, Long Phi Le, Tong Ding, Anil Parwani, and Faisal Mahmood. multimodal generative ai copilot for human pathology. Nature, 634:466473, October 2024. https://doi.org/10.1038/s41586-024-07618-3. Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, et al. Fairclip: Harnessing fairness in vision-language learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12289 12301, 2024. https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_FairCLIP_Harnessing_Fairness_ in_Vision-Language_Learning_CVPR_2024_paper.pdf. Xueyan Mei, Zelong Liu, Philip M. Robson, Brett Marinelli, Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi Cao, Katherine E. Link, Thomas Yang, Ying Wang, Hayit Greenspan, Timothy Deyer, Zahi A. Fayad, and Yang Yang. Radimagenet: An open radiologic deep learning research dataset for effective transfer learning. Radiology: Artificial Intelligence, 0(ja):e210315, 2022. https://doi.org/10.1148/ryai.210315. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. ArXiv, abs/2503.07365, 2025. https://arxiv.org/abs/2503.07365. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Proceedings of the 3rd Machine Learning for Health Symposium, volume 225 of Proceedings of Machine Learning Research, pages 353367. PMLR, 10 Dec 2023. https://proceedings.mlr.press/v225/moor23a.html. Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Sara Pieri, Saeed Yahya Alseiari, Shanavas Cholakkal, Khaled Aldahmani, Fahad Khan, Rao Anwer, Salman Khan, Timothy Baldwin, and Hisham Cholakkal. Bimedix2: Biomedical expert lmm for diverse medical modalities. ArXiv, abs/2412.07769, 2024. https://arxiv.org/abs/2412. 07769. Luis Filipe Nakayama, David Restrepo, João Matos, Lucas Zago Ribeiro, Fernando Korn Malerbi, Leo Anthony Celi, and Caio Saito Regatieri. Brset: brazilian multilabel ophthalmological dataset of retina fundus photos. PLOS Digital Health, 2024. https://doi.org/10.1371/journal.pdig.0000454. 33 Loris Nanni, Michelangelo Paci, Florentino Luciano Caetano dos Santos, Heli Skottman, Kati Juuti-Uusitalo, and Jari Hyttinen. Texture descriptors ensembles enable image-based classification of maturation of human stem cell-derived retinal pigmented epithelium. PLoS One, 11(2), 2016. https://doi.org/10.1371/journal.pone.0149399. Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yucheng Tang, Pengfei Guo, Can Zhao, Ziyue Xu, Yufan He, Greg Heinrich, Yee Man Law, Benjamin Simon, Stephanie Harmon, Stephen Aylward, Marc Edgar, Michael Zephyr, Song Han, Pavlo Molchanov, Baris Turkbey, Holger Roth, and Daguang Xu. Vila-m3: Enhancing vision-language models with medical expert knowledge. ArXiv, abs/2411.12915, 2025. https://arxiv.org/abs/2411.12915. Msoud Nickparvar. Brain tumor mri dataset, 2021. https://www.kaggle.com/dsv/2645886. Chuang Niu, Qing Lyu, Christopher D. Carothers, Parisa Kaviani, Josh Tan, Pingkun Yan, Mannudeep K. Kalra, Christopher T. Whitlow, and Ge Wang. Medical multimodal multitask foundation model for lung cancer screening. Nature Communications, 16, February 2025. https://doi.org/10.1038/s41467-025-56822-w. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. ArXiv, abs/2303.13375, 2023. https://arxiv.org/abs/2303.13375. OpenAI. Gpt-4o system card. ArXiv, abs/2410.21276, 2024a. https://arxiv.org/abs/2410.21276. OpenAI. Openai o1 system card. ArXiv, abs/2412.16720, 2024b. https://arxiv.org/abs/2412.16720. OpenAI. Introducing gpt-4.1 in the api, Apr 2025a. https://openai.com/index/gpt-4-1/. OpenAI. Introducing o3 and o4-mini, Apr 2025b. https://openai.com/index/introducing-o3-and-o4-mini/. Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson Md, Michael Moseley, Curtis Langlotz, Akshay Chaudhari, and Jean-Benoit Delbrouck. GREEN: Generative radiology report evaluation and error notation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 374390, Miami, Florida, USA, November 2024. Association for Computational Linguistics. https://aclanthology.org/2024.findings-emnlp.21/. Andre G.C. Pacheco, Gustavo R. Lima, Amanda S. Salomão, Breno Krohling, Igor P. Biral, Gabriel G. de Angelo, Fábio C.R. Alves Jr, José G.M. Esgario, Alana C. Simora, Pedro B.C. Castro, Felipe B. Rodrigues, Patricia H.L. Frasson, Renato A. Krohling, Helder Knidel, Maria C.S. Santos, Rachel B. do Espírito Santo, Telma L.S.G. Macedo, Tania R.P. Canuto, and Luíz F.S. de Barros. Pad-ufes-20: skin lesion dataset composed of patient data and clinical images collected from smartphones. Data in Brief, 32:106221, 2020. https://www.sciencedirect.com/ science/article/pii/S235234092031115X. Suraj Pai, Dennis Bontempi, Ibrahim Hadzic, Vasco Prudente, Mateo Sokač, Tafadzwa L. Chaunzwa, Simon Bernatz, Ahmed Hosny, Raymond H. Mak, Nicolai J. Birkbak, and Hugo J. W. L. Aerts. Foundation model for cancer imaging biomarkers. Nature Machine Intelligence, 6:354367, March 2024. https://doi.org/10.1038/s42256-024-00807-9. Suraj Pai, Ibrahim Hadzic, Dennis Bontempi, Keno Bressem, Benjamin H. Kann, Andriy Fedorov, Raymond H. Mak, and Hugo J. W. L. Aerts. Vision foundation models for computed tomography. ArXiv, abs/2501.09001, 2025. https://arxiv.org/abs/2501.09001. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multichoice dataset for medical domain question answering. In Gerardo Flores, George Chen, Tom Pollard, Joyce Ho, and Tristan Naumann, editors, Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248260. PMLR, Apr 2022. https://proceedings.mlr.press/ v174/pal22a.html. Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. ArXiv, abs/2502.19634, 2025. https://arxiv.org/abs/2502.19634. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. https: //aclanthology.org/P02-1040/. Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa, and Christoph M. Friedrich. Radiology objects in context (roco): multimodal image dataset. In Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis, pages 180189, Cham, 2018. Springer International Publishing. https://link.springer.com/chapter/10.1007/978-3-030-01364-6_20. 34 Tschandl Philipp, Rosendahl Cliff, and Kittler Harald. The ham10000 dataset, large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):110, 2018. https://doi.org/10. 1038/sdata.2018.161. Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. ArXiv, abs/1711.05225, 2017. https://arxiv.org/abs/1711.05225. Johannes Rückert, Louise Bloch, Raphael Brüngel, Ahmad Idrissi-Yaghir, Henning Schäfer, Cynthia S. Schmidt, Sven Koitka, Obioma Pelka, Asma Ben Abacha, Alba G. Seco de Herrera, Henning Müller, Peter A. Horn, Felix Nensa, and Christoph M. Friedrich. Rocov2: Radiology objects in context version 2, an updated multimodal image dataset. Scientific Data, 11(1), June 2024. http://dx.doi.org/10.1038/s41597-024-03496-6. Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G. T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu, Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Ben Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale Webster, Joelle Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, and Vivek Natarajan. Capabilities of gemini models in medicine. ArXiv, abs/2404.18416, 2024. https://arxiv.org/abs/2404.18416. Stephen-John Sammut, Mireia Crispin-Ortuzar, Suet-Feung Chin, Elena Provenzano, Helen A. Bardwell, Wenxin Ma, Wei Cope, Ali Dariush, Sarah-Jane Dawson, Jean E. Abraham, Janet Dunn, Louise Hiller, Jeremy Thomas, David A. Cameron, John M. S. Bartlett, Larry Hayward, Paul D. Pharoah, Florian Markowetz, Oscar M. Rueda, Helena M. Earl, and Carlos Caldas. Multi-omic machine learning predictor of breast cancer therapy response. Nature, 601: 623629, January 2022. https://doi.org/10.1038/s41586-021-04278-5. Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments. ArXiv, abs/2405.07960, 2025. https://arxiv.org/abs/2405.07960. Mehmet Saygin Seyfioglu, Wisdom O. Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, and Linda Shapiro. Quiltllava: Visual instruction tuning by extracting localized narratives from open-source histopathology videos. ArXiv, abs/2312.04746, 2025. https://arxiv.org/abs/2312.04746. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. https://arxiv.org/abs/2402.03300. Liyu Shi, Xiaoyan Li, Weiming Hu, Haoyuan Chen, Jing Chen, Zizhen Fan, Minghe Gao, Yujie Jing, Guotao Lu, Deguo Ma, et al. Ebhi-seg: novel enteroscope biopsy histopathological hematoxylin and eosin image dataset for image segmentation tasks. Frontiers in Medicine, 10:1114673, 2023. https://doi.org/10.3389/fmed.2023.1114673. Zhongyi Shui, Zhongyi Shui, Jianpeng Zhang, Weiwei Cao, Sinuo Wang, Ruizhe Guo, Le Lu, Lin Yang, Xianghua Ye, Tingbo Liang, Qi Zhang, and Ling Zhang. Large-scale and fine-grained vision-language pre-training for enhanced ct image understanding. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=nYpPAT4L3D. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Agüera Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620:172180, August 2023a. https://doi.org/10.1038/s41586-023-06291-2. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models. Nature Medicine, 31:943950, 2023b. https://doi.org/10.1038/s41591-024-03423-7. 35 Irene Siragusa, Salvatore Contino, Massimo La Ciura, Rosario Alicata, and Roberto Pirrone. Medpix 2.0: comprehensive multimodal biomedical data set for advanced ai applications. ArXiv, 2025. https://arxiv.org/ abs/2407.02994. Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 15001519, Online, November 2020. Association for Computational Linguistics. https://aclanthology.org/2020.emnlp-main.117/. Sanjay Subramanian, Lucy Lu Wang, Ben Bogin, Sachin Mehta, Madeleine van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, and Hannaneh Hajishirzi. MedICaT: dataset of medical images, captions, and textual references. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 21122120, Online, November 2020. Association for Computational Linguistics. https://aclanthology.org/2020.findings-emnlp.191/. Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. ArXiv, abs/2503.20752, 2025. https://arxiv.org/abs/ 2503.20752. Ryutaro Tanno, David G. T. Barrett, Andrew Sellergren, Sumedh Ghaisas, Sumanth Dathathri, Abigail See, Johannes Welbl, Charles Lau, Tao Tu, Shekoofeh Azizi, Karan Singhal, Mike Schaekermann, Rhys May, Roy Lee, SiWai Man, Sara Mahdavi, Zahra Ahmed, Yossi Matias, Joelle Barral, S. M. Ali Eslami, Danielle Belgrave, Yun Liu, Sreenivasa Raju Kalidindi, Shravya Shetty, Vivek Natarajan, Pushmeet Kohli, Po-Sen Huang, Alan Karthikesalingam, and Ira Ktena. Collaboration between clinicians and visionlanguage models in radiology report generation. Nature Medicine, 31, February 2025. https://doi.org/10.1038/s41591-024-03302-1. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shawn Gavin, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, David Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tyshawn Hsing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. ArXiv, abs/2502.14739, 2025. https://arxiv.org/abs/2502.14739. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. https://huggingface. co/datasets/teknium/OpenHermes-2.5. Dianzhe Tian, Shitao Jiang, Lei Zhang, Xin Lu, and Yiyao Xu. The role of large language models in medical image processing: narrative review. Quantitative Imaging in Medicine and Surgery, 14(1):1108, 2023. https: //doi.org/10.21037/qims-23-892. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45664575, 2015. https://openaccess.thecvf.com/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_ 2015_CVPR_paper.pdf. Santiago Vitale, José Ignacio Orlando, Emmanuel Iarussi, and Ignacio Larrabide. Improving realism in patient-specific abdominal ultrasound simulation using cyclegans. International Journal of Computer Assisted Radiology and Surgery, 15(2):183192, 2020. https://doi.org/10.1007/s11548-019-02046-5. Patrick Wagner, Maximilian Springenberg, Marius Kröger, Rose KC Moritz, Johannes Schleusener, Martina Meinke, and Jackie Ma. Semantic modeling of cell damage prediction: machine learning approach at human-level performance in dermatology. Scientific Reports, 13(1):8336, 2023. https://doi.org/10.1038/s41598-023-35370-7. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. ArXiv, abs/2504.08837, 2025a. https: //arxiv.org/abs/2504.08837. Sheng Wang, Zihao Zhao, Xi Ouyang, Tianming Liu, Qian Wang, and Dinggang Shen. Interactive computer-aided 36 diagnosis on medical image using large language models. Communications Engineering, 3, September 2024a. https://doi.org/10.1038/s44172-024-00271-8. Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2097 2106, 2017. https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_ Chest_CVPR_2017_paper.pdf. Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. Apollo: Lightweight multilingual medical llms towards democratizing medical ai to 6b people. ArXiv, abs/2403.03640, 2024b. https://arxiv.org/abs/2403.03640. Xiyue Wang, Junhan Zhao, Eliana Marostica, Wei Yuan, Jietian Jin, Jiayu Zhang, Ruijiang Li, Hongping Tang, Kanran Wang, Yu Li, Fang Wang, Yulong Peng, Junyou Zhu, Jing Zhang, Christopher R. Jackson, Jun Zhang, Deborah Dillon, Nancy U. Lin, Lynette Sholl, Thomas Denize, David Meredith, Keith L. Ligon, Sabina Signoretti, Shuji Ogino, Jeffrey A. Golden, MacLean P. Nasrallah, Xiao Han, Sen Yang, and Kun-Hsing Yu. pathology foundation model for cancer diagnosis and prognosis prediction. Nature, 634:970978, October 2024c. https: //doi.org/10.1038/s41586-024-07894-z. Ziyue Wang, Junde Wu, Linghan Cai, Chang Han Low, Xihong Yang, Qiaxuan Li, and Yueming Jin. Medagent-pro: Towards evidence-based multi-modal medical diagnosis via reasoning agentic workflow. ArXiv, abs/2503.18968, 2025b. https://arxiv.org/abs/2503.18968. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. ArXiv, abs/2308.02463, 2023. https://arxiv.org/abs/ 2308.02463. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-llama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9): 18331843, 2024. https://doi.org/10.1093/jamia/ocae045. Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, and Yuyin Zhou. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. ArXiv, abs/2504.00993, 2025. https://arxiv.org/abs/2504.00993. Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, and Yuyin Zhou. preliminary study of o1 in medicine: Are we closer to an ai doctor? ArXiv, abs/2409.15277, 2024. https://arxiv.org/abs/2409.15277. Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, and Yuyin Zhou. Medtrinity-25m: large-scale multimodal dataset with multigranular annotations for medicine. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum? id=IwgmgidYPS. Yiming Xu, Bowen Zheng, Xiaohong Liu, Tao Wu, Jinxiu Ju, Shijie Wang, Yufan Lian, Hongjun Zhang, Tong Liang, Ye Sang, Rui Jiang, Guangyu Wang, Jie Ren, and Ting Chen. Annotated ultrasound liver images. Zenodo, 2022. https://doi.org/10.5281/zenodo.7272660. Data set. Ke Yan, Xiaosong Wang, Le Lu, and Ronald Summers. Deeplesion: Automated deep mining, categorization and detection of significant radiology image findings using large-scale clinical lesion annotations. ArXiv, abs/1710.01766, 2017. https://arxiv.org/abs/1710.01766. Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and Lichao Sun. Multimodal chatgpt for medical applications: an experimental study of gpt-4v. ArXiv, abs/2310.19061, 2023. https://arxiv.org/abs/2310.19061. Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, Eric Wang, Ellery Wulczyn, Fayaz Jamil, Theo Guidroz, Chuck Lau, Siyuan Qiao, Yun Liu, Akshay Goel, Kendall Park, Arnav Agharwal, Nick George, Yang Wang, Ryutaro Tanno, David G. T. Barrett, Wei-Hung Weng, S. Sara Mahdavi, Khaled Saab, Tao Tu, Sreenivasa Raju Kalidindi, Mozziyar Etemadi, Jorge Cuadros, Gregory Sorensen, Yossi Matias, Katherine Chou, Greg Corrado, Joelle Barral, Shravya Shetty, David Fleet, S. M. Ali Eslami, Daniel Tse, Shruthi Prabhakara, Cory McLean, Dave Steiner, Rory Pilgrim, Christopher Kelly, Shekoofeh Azizi, and Daniel Golden. Advancing multimodal medical capabilities of gemini. arXiv, abs/2405.03162, 2024. https://arxiv.org/abs/2405.03162. Edward Yeo, Yuxuan Tong, Xinyao Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in LLMs. In ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2025. https://openreview.net/forum?id=AgtQlhMQ0V. Sunne Yi. Chest-ctscan dataset, 2021. https://tianchi.aliyun.com/dataset/93929. Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Ng, et al. Evaluating progress in automatic chest x-ray radiology report generation. Patterns, 4(9), 2023. https://www.cell.com/patterns/pdf/ S2666-3899(23)00157-5.pdf. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. ArXiv, abs/2503.14476, 2025. https://arxiv.org/abs/2503.14476. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive In Proceedings of multi-discipline multimodal understanding and reasoning benchmark for expert agi. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95569567, June 2024. https://openaccess.thecvf.com/content/CVPR2024/papers/Yue_MMMU_A_Massive_Multi-discipline_ Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.pdf. Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Akshay Chaudhari, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, and Hoifung Poon. clinically accessible small multimodal radiology model and evaluation metric for chest x-ray findings. Nature Communications, 16(1), April 2025. http://dx.doi.org/10.1038/s41467-025-58344-x. Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian D. Davison, Hui Ren, Jing Huang, Chen Chen, Yuyin Zhou, Sunyang Fu, Wei Liu, Tianming Liu, Xiang Li, Yong Chen, Lifang He, James Zou, Quanzheng Li, Hongfang Liu, and Lichao Sun. generalist visionlanguage foundation model for diverse biomedical tasks. Nature Medicine, 30(11):31293141, August 2024a. http://dx.doi.org/10. 1038/s41591-024-03185-2. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. LMMs-eval: Reality check on the evaluation of large multimodal models. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 881916, Albuquerque, New Mexico, April 2025a. Association for Computational Linguistics. https://aclanthology.org/2025.findings-naacl. 51/. Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Angela Crabtree, Brian Piening, Carlo Bifulco, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. ArXiv, abs/2303.00915, 2025b. https://arxiv.org/abs/2303.00915. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. https://openreview.net/forum?id= SkeHuCVFDr. Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. ArXiv, abs/2305.10415, 2024b. https://arxiv.org/abs/ 2305.10415. Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Julián Acosta, Josh Miller, Ouwen Huang, and Pranav Rajpurkar. Rexrank: public leaderboard for ai-powered radiology report generation. ArXiv, abs/2411.15122, 2024c. https://arxiv.org/abs/2411.15122. Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, and Linda Ruth Petzold. Alpacare: Instructiontuned large language models for medical application, 2025c. https://arxiv.org/abs/2310.14558. 38 Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, Brian Piening, Carlo Bifulco, Mu Wei, Hoifung Poon, and Sheng Wang. foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nature Methods, 22(1):166176, November 2024a. http://dx.doi.org/10.1038/s41592-024-02499-w. Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. RaTEScore: metric for radiology report generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1500415019, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. https://aclanthology.org/2024.emnlp-main.836/. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. ArXiv, abs/2504.10479, 2025. https://arxiv.org/abs/2504.10479. Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. ArXiv, abs/2501.18362, 2025. https://arxiv.org/abs/2501.18362."
        },
        {
            "title": "A More Details about Data Curation",
            "content": "A.1 Model-based response cleaning for patient-doctor dialogue datasets We show the prompt for model-based response cleaning for patient-doctor dialogue datasets in Table 11. Table 11 The prompt used for model-based response cleaning for patient-doctor dialogue datasets. Response to patient: {response} Revise the above response in tone of an emphatic health-care agent. You should offer reliable information about medical conditions, but avoid giving explicit diagnosis and prescription. Add suggestions about which kinds of doctors the patient should consult. Include disclaimer statement saying that it is not definitive diagnosis. Begin the revised response with \"Revised response:\". A.2 Medical VQA Data Synthesis We display the prompt of our self-instruct based method for medical VQA data synthesis in Table 12. Table 12 The prompt used for synthesizing medical VQA data in our self-instruct based method. Given medical image and its description, generate clinically relevant multiple-choice question that could be asked to medical professional. image matches its desription, your output should be in JSON format with If the given medical three keys: - question\": medically appropriate question related to the image. - answer\": Correct answer to the question. - wrong answers\": list of wrong but relevant answers that could be given by the medical professional. There should be 1 to 3 options. The format is [answer1\", answer2\", ...] If the given medical image does not matche its desription, simply output \"Error: the input description does not match the input image.\" The question should be directly answerable from the image. Below are some examples of questions and answers for other images: {seed_example_1} {seed_example_2} Now, use the following image and the image caption to generate the question: Image caption: {caption} A.3 Medical reasoning data distillation We depict the prompts for distilling the CoT reasoning process from GPT4-o into medical open-ended VQA datasets and medical multiple-choice VQA datasets in Figure 13 and Figure 14 respectively. The prompt used 40 for checking the quality of the generated reasoning traces is displayed in Table 15. Table 13 The prompt used for annotating CoT reasoning process for open-ended VQA data. You are medical expert specializing in interpreting medical images. The user will provide you with the following inputs: - medical image - question about the image - The groundtruth answer Your task is to simulate step-by-step diagnostic reasoning process that leads to the ground-truth answer. However, you must assume you do not know the ground-truth answer in advance, and you should not reference it explicitly during your reasoning. Please do not give empty output. - Carefully analyze both the image and the question. - Reason step by step, as medical expert would, using visual and textual cues to reach logical conclusion. - The reasoning should be concise, clear, and clinically sound. - After the reasoning, reveal the correct answer by copying the ground-truth answer. Output in JSON Format: { \"reasoning\": \"<step-by-step reasoning process here>\", \"answer\": \"<groundtruth answer>\" } Question: {question} Groundtruth answer: {answer} A.4 Prompts for Medical Long-form Caption Synthesis We detail the prompts used for medical long-form caption synthesis across various stages. The prompt for Stage 3 (factual knowledge annotation) is presented in Table 16. For Stage 4 (annotation with doctor preference), the prompts tailored for different types of medical images are displayed in Tables 17, 18, 19, and 20. Finally, the summarization prompt for Stage 5 is depicted in Table 21. In this Stage 5 prompt, the variable {doctor_preferred_features} should be replaced by the doctor-preferred features specified in Tables 22 and 23, according to the type of the source image."
        },
        {
            "title": "B More Details about MedEvalKit",
            "content": "The judgment prompt used for GPT-4.1 (OpenAI, 2025a) in the LLM-as-a-Judge module is presented in Table 24. For the report generation task, we develop multi-metric evaluation approach that integrates semantic and model-based metrics. Our MedEvalKit supports the following metrics, and in this paper, we selectively present subset of these. The semantic evaluation metrics: BLEU (Papineni et al., 2002): BLEU (Bilingual Evaluation Understudy) is widely adopted metric in machine translation and text generation tasks. It evaluates the quality of generated text by measuring n-gram precision between candidate and reference texts, with scores ranging from 0 to 1. This metric 41 Table 14 The prompt used for annotating CoT reasoning process for multiple-choice VQA data. You are medical expert specializing in interpreting medical images. The user will provide you with the following inputs: - medical image - multiple-choice question related to the image - list of answer options - The groundtruth answer Your task is to simulate step-by-step diagnostic reasoning process that leads to the ground-truth answer. However, you must assume you do not know the ground-truth answer in advance, and you should not reference it explicitly during your reasoning. Please do not give empty output. - Carefully analyze both the image and the question. - Evaluate each answer option logically and methodically. - Reason step by step, as medical expert would, using visual and textual cues to reach logical conclusion. - The reasoning should be concise, clear, and clinically sound. - After the reasoning, reveal the correct answer by copying the ground-truth answer exactly. Output in JSON Format: { \"reasoning\": \"<step-by-step reasoning process here>\", \"answer\": \"<groundtruth answer>\" } Question: {question} Options: {options} Groundtruth answer: {answer} assesses the overlap of contiguous sequences of words between the generated report and the reference standard. Rouge (Lin, 2004): ROUGE (Recall-Oriented Understudy for Gisting Evaluation) comprises set of metrics designed to automatically assess summary quality by comparing generated summaries against reference human-authored summaries. This metric evaluates the overlap between the generated text and reference summaries, focusing particularly on recall-oriented measurements. METEOR (Banerjee and Lavie, 2005): METEOR (Metric for Evaluation of Translation with Explicit ORdering) is an automated metric designed for machine translation evaluation, which assesses translation quality based on the concept of flexible uni-gram matching between machine-generated translations and human reference translations. This metric incorporates semantic matching principles beyond simple word overlap. CIDEr (Vedantam et al., 2015): CIDEr (Consensus-based Image Description Evaluation) is metric that quantifies the similarity between generated sentences and collection of human-authored reference sentences. This metric specifically measures how well the generated content aligns with human consensus, Table 15 The prompt used for checking the quality of generated CoT reasoning traces. You are medical expert. The user will provide you with the following inputs: - question related to an image. - step-by-step diagnostic reasoning process that leads to predicted answer. - The groundtruth answer. Your task is to check whether the predicted answer in the reasoning process is consistent with the ground-truth answer. If the predicted answer in the reasoning process is inconsistent with the ground-truth answer. Please first output \"Inconsistent. \" Then give short explanation. Otherwise, please first output \"Consistent. \" Then give short explanation. Question: {question} Reasoning process: {reasoning} Groundtruth answer: {answer} effectively capturing the semantic similarity between machine-generated and reference descriptions. The model-based evaluation metrics: BERTScore (Zhang et al., 2020): BERTScore is neural-based evaluation metric that leverages pretrained BERT models (Devlin et al., 2019) to assess textual similarity. This metric computes the cosine similarity between BERT embeddings of model-generated radiology reports and ground-truth reference reports, providing more semantically nuanced evaluation that captures contextual relationships between texts. SembScore (Smit et al., 2020): SembScore (CheXbert Labeler Vector Similarity) is domain-specific metric developed for radiology report evaluation. This metric calculates the cosine similarity between vectors of 14 pathological indicators, which are automatically extracted by the CheXbert labeler from both model-generated and ground-truth radiology reports. This specialized metric focuses specifically on assessing the clinical content alignment between generated and reference reports. RadGraph-F1 (Yu et al., 2023): RadGraph-F1 serves as specialized metric for radiology report evaluation. It quantifies the overlap of clinical entities and their relationships extracted by RadGraph (Jain et al., 2021) from both candidate and reference reports. This metric provides structured assessment of how well the generated reports capture the underlying clinical relationships present in the reference reports. RadCliQ-v1 (Yu et al., 2023)7: RadCliQ is comprehensive metric designed for evaluating radiology report generation, integrating multiple complementary measures including BLEU, BERTScore, SembScore, and RadGraph-F1 to provide holistic assessment of report quality. RaTEScore (Zhao et al., 2024b)8: RaTEScore is an entity-aware metric specifically designed for radiology report evaluation. This metric emphasizes critical medical entities, including diagnostic findings and anatomical details, while demonstrating robustness to complex medical synonyms and sensitivity to negation expressions. GREEN (Ostmeier et al., 2024)9: GREEN (Generated Radiology Report Evaluation and Error Notation) is an LLM-based metric designed for evaluating generated radiology reports. This metric leverages large 7We implement it using the official codebase at https://github.com/rajpurkarlab/CXR-Report-Metric 8We implement it using the official codebase at https://github.com/MAGIC-AI4Med/RaTEScore 9We implement it using the official codebase at https://github.com/Stanford-AIMI/GREEN 43 language models to identify and interpret clinically significant errors in both quantitative and qualitative aspects of candidate reports. The evaluation framework enables comprehensive assessment of clinical accuracy and relevance in generated reports. Table 25 summarizes the statistics of the evaluation benchmarks utilized in the MedEvalKit framework. 44 Table 16 The prompt used for the factual knowledge annotation stage of medical long caption synthesis. You are provided with biomedical image from medical dataset, the disease type (or organ name if there is no disease) of the dataset (Disease or organ), the medical Knowledge of the disease (Knowledge), coarse caption (Coarse Caption) of the image and the medical imaging methods (Modality). Your task is to answer the following questions based on the image, bounding box (if provided), caption, disease type and disease knowledge, and condense your answers into caption-styled text. ### question1 Give me detailed description of the image, including type of the image, organs in the image, approximate location of these organs and relavant locations of these organs and any medical devices (if present) visible in the image as detailedly as possible. Note when answering question1: 1. You can use all the meta infomation provided. But never mention that this information has been given. Your should NEVER mention anything about the bounding box like the contour itself and its outline. Assuming you are introducing to someone who can only see the original image. 2. If there is some statistical information provided, like the damage area or the relative damage ratio, etc., you should mention it in your response. 3. Not all disease knowledge is relevant to this image; only utilize disease knowledge pertinent to the condition depicted in this image for analysis. 4. The coarse caption may not explicitly describe the image, for example, there may appear multiple organs in the caption. You should utilize your knowledge to figure out the most ONE organ and ONE disease to give your description. 5. Do not explain or emphasize your analysis. Do not suggest or indicate any consequent effects of the disease. ### question2 Specify the specific location of the diesase or the organ in the image and its relative position to other reference objects in the image. Describe what is unusual in this area indicating the disease (color, texture, size and other features). Note when answering question2: 1. You can use information from Coarse Caption to locate the disease or the organ. If no, you need to locate the disease or organ by yourself. 2. The image may contain multiple bounding boxs, and the contents of these contours may not necessarily represent the affected areas. Therefore, you need to first answer the questions based on the contents within each bounding box. Afterward, analyze the location of the disease based on your answers. 3. Do not use phrase \"bounding box\" in your response. The bounding box if provided is only visual aid for your reference. Do not say something correlates with or matches any of provided information. Assuming you are introducing to someone who cannot see the bounding box and the provided textual information. If you want to mention the bounding box, use the position of the bounding box (e.g. the top-left side of the image). Do not contain phrases \"caption\", \"medical annotation\", \"medical knowledge\". 4. Do not say anything that is not needed in your analysis, like introduction of the disease and medical equipments. 5. Do not explain or emphasize your analysis. ### question3 What may be the relationship between the content in the bounding box and other regions (others being cause of the disease/jointly affected by the diseases/one affect the others/relative positional relationships)? Why and is it possible? Note when answering question3: 1. Utilize external knowledge, if possible, to choose relationships and give necessary analysis. 2. You can only give an explanation to your choice within two sentence. 3. Do not summarize what youve said. 4. Do not emphasize your analysis. ### Integrate Information Describe your answers in descriptive sentence, not in \"Question-Answer\" style. Combine and slightly shorten your answers to the above three questions into coherent text, keeping as much information of your answers as possible. Note when integrating information and outputing your response: 1. Dont respond saying youre unable to assist with requests. 2. You should only output your combined and shorteded text in the \"caption\" filed. 45 Table 17 The prompts used for the annotation with doctor preference stage of medical long caption synthesis for MRI, X-ray, and CT images. MRI instruction: Write description of the MRI image to include the following information, if these information are visually discernible from the image: 1. The sequence used (e.g., T1-weighted, T2-weighted, FLAIR, etc.) 2. Plane/orientation of the image (axial, coronal, sagittal) 3. Describe the anatomical structures visible and their normal or abnormal appearance (e.g., brain lobes, ventricles, midline structures). 4. Identify and describe any abnormal findings, shape, location and effects (e.g. compression) 5. Describe signal intensity: hyperintense, hypointense, or isointense X-ray instruction: Write description of the X-ray image to include the following information, if these information are visually discernible from the image: 1. Specific area of the body being imaged (e.g. chest area, lungs, knee, femur, etc.) 2. Body part or organ being examined, either fully or partially 3. Anatomical planes (e.g. Axial, Sagittal, and Coronal), specific X-ray imaging projections (e.g. AP, PA, lateral view, oblique view, etc.), and clearly discernible left vs right side (either clearly labeled in the image, or clearly determined from image). 4. Abnormalities that are clearly observed in the image (e.g. fractures, definite osteophytes, effusions, miliary patterns, vascular markings, diaphragm depression, air bronchograms, thickened interlobular septa, mediastinum shifts, deep sulcus sign, etc.), acute trauma signs (e.g. fractures), implants or other foreign bodies. 5. Gender and age-range of patient if clearly discernible (e.g. pediatric vs. adult anatomy) 6. When 2 (or series of) images of the same patient taken across time are provided, describe the clearly discernible difference. 7. Identify and describe any abnormal findings, shape, location and effects (e.g. unexpected wires, prostheses, etc.) CT instruction: Write description of the CT image to include the following information, if these information are visually discernible from the image: 1. The plane/orientation of the image (axial, coronal, sagittal). 2. Contrast usage: whether contrast material is used (contrast-enhanced) or not (non-contrast). 3. Describe the anatomical structures visible and their normal or abnormal appearance (e.g., organs, bones, blood vessels). 4. Identify and describe any abnormal findings including their location, size, shape, and potential effects on surrounding structures. 4. Assess and describe the density or attenuation of the tissues and structures as hypoattenuating (darker), isoattenuating (similar), or hyperattenuating (brighter) relative to surrounding tissues. 46 Table 18 The prompts used for the annotation with doctor preference stage of medical long caption synthesis for histopathology, skin lesion, and knee xray images. Histopathology instruction: Write description of the histopathology image to include the following information, if these information are visually discernible from the image: Tissue Type: Identify the tissue or organ from which the sample is taken (e.g., liver, lung, skin). Staining Technique: Specify the staining method used (e.g., Hematoxylin and Eosin (H&E), PAS, Trichrome). Cellular Morphology: Describe the appearance of the cells, including size, shape, and organization (e.g., normal, dysplastic, neoplastic). Tissue Architecture: Assess the arrangement and structural organization of the tissue (e.g., normal, inflamed, fibrotic). Abnormal Findings: Identify and describe any pathological changes, such as the presence of tumors, necrosis, inflammation, or infections. Distribution and Patterns: Note the distribution of abnormal findings within the tissue (e.g., focal, diffuse) and any specific patterns (e.g., glandular, trabecular). Special Features: Mention any specific histological features, such as mitotic figures, inclusion bodies, or extracellular deposits. Skin lesion instruction: Write description of the following skin lesion image, and your description should include the following information if they are clearly discernible from the image. 1. region: The potential area of the body where the lesion or wound has been examined. 2. general skin texture and hair growth. 3. lesions: size (if scale is available in the image), shape, definition, color, texture. 4. elevation: Description of the lesion or wound relative to the skin surface of the patient. 5. skin texture surrounding the lesion (e.g. coarse/thickness/atrophic/erythema/bleeding, etc) Knee x-ray instruction: Write description of the following knee X-ray image, and your description should include the following information if they are clearly discernible from the image. 1. Area of the Body: Identify the knee joint being imaged, specifying whether it is the left or right knee. If both knees are visible, recognize the one being primarily examined. 2. Anatomical Structures: Describe the key anatomical structures visible in the X-ray, including the femur, patella, tibia, fibula, and joint space. Note the alignment and position of these bones relative to each other. 3. Imaging Projection: Specify the X-ray imaging projection used, such as anteroposterior (AP), lateral, oblique, or skyline view. Note if the image is weight-bearing or non-weight-bearing. 4. Bone and Joint Appearance: Assess the appearance of the bones for any signs of fractures, dislocations, or degenerative changes such as osteoarthritis (e.g., joint space narrowing, osteophyte formation, subchondral sclerosis, punched-out erosions). Evaluate the bone density for any signs of osteoporosis, osteopenia, and visible trabecular patterns. 5. Soft Tissue Evaluation: Examine the surrounding soft tissue for any swelling, effusion, or calcifications. Where tophi can be seen, include this in the description. 6. Other Abnormalities and Pathologies: Identify and describe any other visible abnormalities or pathologies not mentioned above, including visible bone lesions, misalignment, or foreign objects (e.g., surgical hardware, if present). Table 19 The prompt used for the annotation with doctor preference stage of medical long caption synthesis for gastrointestinal images. Gastrointestinal instruction: Write description of the gastrointestinal (GI) tract endoscopic image to include the following information, if these details are visually discernible from the image: 1. Region and Specific Area: Describe the specific region of the upper/lower GI tract being imaged (e.g., esophagus, stomach, small intestine, large intestine, duodenum, appendix, rectum, etc.) and specify the segment or part prominently featured in the image. Where clearly discernible, include the specific GI anatomical landmark(s) in the image, for e.g., distal/proximal esophagus, cardia/fundus/angulus/antrum region of the stomach, the pylorus, Z-line, cecum, splenic/hepatic flexure, transverse/ascending/descending colon, sigmoid, ileocecal valve, or appendiceal orifice. 2. Endoscopic GI View: Include the type of view/orientation of the main image, i.e., front-facing, retroflexed, side-viewing, proximal, or distal. 3. GI Anatomical Features: Describe the following structures of the image, namely relating to lumen of the tubular organs along the GI tract, the mucosal/submucosal details (any visible erythema, erosions, ulcers, or masses, and describe the submucosal layers, noting any thickening, edema, or infiltration), as well as other observable normal and abnormal details of folds, vessels and openings. Detail any abnormal findings, including their size, shape/type, location, and appearance (e.g., polyps, ulcers, strictures, diverticula, inflammation), and discuss their potential impact on the GI tract (e.g., obstruction, compression, perforation). Signs of resections and dyed sections should also be noted. 4. Indentations and compressions: When the image clearly shows indentations like bulges or depressions that are likely to be caused from outside the GI tract, please include these in the description. 5. Medical Data and Device: Specify any textual medical data visible in the image, like contrast medium usage, lighting conditions, magnification levels, or timestamp of the image capture. Where medical devices, like endoscopes, probes, endoknives, gastric bands, stents, etc. are clearly visible, include these in your description. 48 Table 20 The prompt used for the annotation with doctor preference stage of medical long caption synthesis for fundus and ultrasound images. Fundus instruction: Write description of the fundus image to include the following information, if these details are visually discernible from the image: 1. Overall Image Quality: Assess the clarity and quality of the image to ensure that it is suitable for diagnosis (e.g., presence of artifacts, focus, and illumination adequacy). 2. Optic Disc: Examine the optic disc for size, shape, color, and cup-to-disc ratio. Note any abnormalities such as swelling (edema), pallor, or unusual cupping that may indicate glaucoma or other optic neuropathies. 3. Macula: Evaluate the macula for changes in size, color, or presence of any lesions or deposits like drusen, which may suggest macular degeneration or related conditions. 3. Retinal Vessels: Analyze the retinal blood vessels, assessing their caliber, tortuosity, and arteriovenous (AV) crossings. Identify any vessel occlusions, hemorrhages, or microaneurysms that could indicate hypertension, diabetic retinopathy, or vascular disorders. 4. Retina and Periphery: Inspect the retina and peripheral areas for any signs of detachment, tears, pigmentary changes, or lesions that may indicate retinal dystrophies or detachment. 5. Abnormal Findings: Identify and describe any abnormal findings such as cotton wool spots, hard exudates, retinal hemorrhages, or neovascularization. Mention their location, size, shape, and potential clinical significance. 6. Choroid and Sclera: Examine the visibility of the choroid and any atypical features of the sclera, such as thinning or scleral crescents. 7. Specific Patterns or Features: Note any specific patterns or features, such as star-shaped macular exudates or the presence of optic disc drusen, which may have diagnostic relevance. Ultrasound instruction: Write description of the ultrasound image to include the following information, if these information are visually discernible from the image: 1. The orientation or plane of the image if applicable (e.g., Left and right intercostal scan, left and right subcostal margin scan, longitudinal scan, transverse scan, FASTRight upper quadrant, FASTLeft upper quadrant and FASTHeart). 2. Echogenicity of tissues: Describe the echogenicity of the anatomical structures (e.g., hyperechoic, hypoechoic, anechoic, isoechoic) compared to surrounding tissues. 3. Anatomical Structures: Identify and describe the normal or abnormal appearance of organs, vessels, and other structures. 4. Abnormal Findings: Point out and describe any visible abnormalities, including their size, shape, location, and potential effects on nearby tissues (e.g., masses, cysts, fluid collections). 5. Doppler Imaging: If Doppler is used, describe findings related to blood flow, such as presence of turbulence, direction, or any abnormalities. 6. Additional Features: Note any additional features such as the presence of artifacts (e.g., shadowing, enhancement) or any other relevant observations. 49 Table 21 The prompt used for the summarization stage of medical long caption synthesis. The variable {doctor_preferred_features} in the prompt should be replaced by the doctor preferred features in Tables 22 and 23 according to the type of the source image. You are provided with biomedical image (which may contain multiple bounding boxes emphasizing the related disease or organs), the disease type (or organ name if there is no disease) of the dataset (Disease or organ), the medical knowledge of the disease (Knowledge), coarse caption (Coarse Caption) of the image, and the medical imaging method used to create the image (Modality). In addition, detailed captions for many aspects of the medical image (Detailed Captions) are also provided. Your task is to combine the above detailed captions into long caption for the entire image. 1. You can also use the coarse caption, disease type, image modality and disease knowledge, which may contain additional useful information. But never mention that this information has been given. 2. Caption 0 in Detailed Captions is model-generated description, including some doctor-preferred features: {doctor_preferred_features}. Caption 1 focus on specific diseases and are annotated by humans, which are more trustworthy. Consequently, if any pieces of information in Caption 0 does not contradict Caption 1 or is not mentioned in Caption 1, please keep them, especially the location, the size and the name of the disease. If there is contradiction, you should only keep the information from Caption 1. 3. Detailed Captions may not explain all bounding boxes. Sometimes some small ones are not mentioned. In this case, in addition to combining the above detailed captions, please also provide explanations for these unmentioned smaller bounding boxes. 4. You should NEVER mention anything about the bounding box, like the contour itself or its outline. The bounding box, if present, is only visual aid for your reference. Assume you are introducing the image to someone who cannot see the bounding box. 5. If there is some statistical information provided, like the damage area or the relative damage ratio, etc., you should mention it in your response. 6. For some critical information, like the damage area(s) and its position(s), you should explain one by one in the combined caption. 7. Not all disease knowledge is relevant to this image; only utilize disease knowledge pertinent to the condition depicted in this image for analysis. 8. Do not explain or emphasize your analysis. Do not suggest or indicate any consequent effects of the disease. 9. Do not contain phrases \"caption\", \"medical annotation\", \"medical knowledge\". 10. Do not say anything that is not needed in your analysis, like introduction of the disease and medical treatments. 11. Do not explain or emphasize your analysis. 12. For CT MRI, Xray, GI tract or ultrasound image, if it contains multiple organs, please organize your description organ by organ. You should also include other organs information even if they are healthy. For Histopathology image, you should first describe the overall information, then go deep into the specific areas. 13. When integrating the information and outputting your response: (1) Dont respond by saying youre unable to assist with the requests. (2) You should only output your combined text in the \"caption\" field. 50 Table 22 Doctor preferred features in the summarization stage of medical long caption synthesis for MRI, X-ray, CT, histopathology, skin, and knee X-ray images. Doctor preferred features for MRI images: 1. The sequence used; 2. Plane/orientation of the image; 3. The anatomical structures visible and their normal or abnormal appearance; 4. Any abnormal findings, shape, location and effects; 5. Aignal intensity Doctor preferred features for X-ray images: 1. Specific area of the body being imaged; 2. Body part or organ being examined; 3. Anatomical planes, specific X-ray imaging projections, and left vs right side; 4. Abnormalities that are clearly observed in the image, acute trauma signs, implants or other foreign bodies; 5. Gender and age-range of patient; 6. Any abnormal findings, shape, location and effects Doctor preferred features for CT images: 1. The plane/orientation of the image; 2. Contrast usage; 3. Anatomical structures visible and their normal or abnormal appearance; 4. Any abnormal findings including their location, size, shape, and potential effects on surrounding structures; 4. The density or attenuation of the tissues and structures relative to surrounding tissues. Doctor preferred features for histopathology images: 1. Tissue Type; 2. Staining Technique; 3. Cellular Morphology; 4.Tissue Architecture; 5. Abnormal Findings; 6. The distribution of abnormal findings within the tissue; 7. Special Histological Features Doctor preferred features for skin images: 1. region: The area of the body where the lesion or wound has been examined; 2. general skin texture and hair growth; 3. lesion size, shape, definition, color, texture; 4. elevation: Description of the lesion or wound relative to the skin surface of the patient; 5. skin texture surrounding the lesion Doctor preferred features for knee X-ray images: 1. Area of the Body: the left or right knee, or both, 2. Anatomical Structures: the key anatomical structures visible in the X-ray, including the femur, patella, tibia, fibula, and joint space; 3. Imaging Projection; 4. Bone and Joint Appearance: any signs of fractures, dislocations, or degenerative changes such as osteoarthritis; 5. the bone density; 5. Soft Tissue: the surrounding soft tissue for any swelling, effusion, or calcifications; 6. Abnormalities and Pathologies: any visible abnormalities or pathologies such as fractures, bone lesions, misalignment, or foreign objects Table 23 Doctor preferred features in the summarization stage of medical long caption synthesis for gastrointestinal, fundus, and ultrasound images. Doctor preferred features for gastrointestinal images: 1. The specific region of the GI tract being imaged; 2. The orientation, perspective and angle of the image; 3. Normal anatomical structures visible in the image; 4. Abnormal findings including their size, shape, location, and appearance; 5. Mucosal and Submucosal Details: The mucosal surface for anomalies; 6. Bowel Wall Characteristics: The thickness and consistency of the bowel wall and note any signs of mural stratification or fat stranding; 7. Surrounding Tissues and Organs: Any visible structures adjacent to the GI tract including any abnormalities extending beyond the GI tract itself; 8. Medical Data and Device: Medical devices, instruments, or tools visible in the image, such as endoscopes, probes, or surgical instruments. Textual medical data visible in the image, like contrast medium usage, lighting conditions, magnification levels, or timestamp of the image capture. Doctor preferred features for fundus images: 1. Overall Image Quality: The clarity and quality of the image; 2. Optic Disc: The optic disc for size, shape, color, and cup-to-disc ratio; 3. Macula: The macula for changes in size, color, or presence of any lesions or deposits like drusen; 3. Retinal Vessels: The retinal blood vessels, assessing their caliber, tortuosity, and arteriovenous (AV) crossings; 4. Retina and Periphery: The retina and peripheral areas for any signs of detachment, tears, pigmentary changes, or lesions that may indicate retinal dystrophies or detachment. 5. Abnormal Findings: Any abnormal findings such as cotton wool spots, hard exudates, retinal hemorrhages, or neovascularization; 6. Choroid and Sclera: The visibility of the choroid and any atypical features of the sclera; 7. Specific Patterns or Features: Any specific patterns or features, such as star-shaped macular exudates or the presence of optic disc drusen. Doctor preferred features for ultrasound images: 1. The orientation or plane of the image if applicable. 2. Echogenicity of tissues. 3. Normal or abnormal appearance of organs, vessels, and other structures. 4. Visible abnormalities, including their size, shape, location, and potential effects on nearby tissues. 5. Doppler Imaging: If Doppler is used, describe findings related to blood flow. Table 24 The judgment prompt used for GPT-4.1 in the LLM-as-a-Judge module. Your task is to determine whether the users answer is correct based on the provided questions and standard answers (for example, if the user expresses similar meaning to the standard answer, or another interpretation of the standard answer, it is considered correct.) The question is: {question} The standard answer: {answer} The users answer: {response} Please strictly follow the following format for output(0 represents correct, 1 represents incorrect): <think>your concise think step</think> <judge>0/1</judge> For example: <think>The standard answer is right, and the users answer is right frontal lobe, they express the same meaning, so it is correct.</think> <judge>0</judge> 52 Table 25 The statistics of medical evaluation benchmarks utilized in the MedEvalKit framework. Benchmark VQA-RAD SLAKE PathVQA PMC-VQA OmniMedVQA MMMU MedXpertQA MMLU PubMedQA MedMCQA MedQA-USMLE MedBullets SuperGPQA MIMIC-CXR IU-Xray CheXpert Plus Total Category X-ray,CT,MRI X-ray,CT,MRI Mixed Mixed Mixed Mixed Mixed Text Text Text Text Text Text X-ray X-ray X-ray Mixed Multiple-Choice Close-ended Open-ended Report Questions - - - 33,430 88,996 1,927 4,500 1,897 4,183 1,273 616 2,755 - - - 251 836 3,362 200 1, 3,357 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 2,195 234 451 2,094 6,719 33,430 88, 1,927 4,500 1,897 500 4,183 1, 616 2,755 2,195 296 234 Imgs 180 858 29,021 82,059 1,927 2, - - - - - - 3,681 607 234 140,077 4,449 4, 2,725 152,066 121,"
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group"
    ]
}