{
    "paper_title": "The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities",
    "authors": [
        "Zhaofeng Wu",
        "Xinyan Velocity Yu",
        "Dani Yogatama",
        "Jiasen Lu",
        "Yoon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic \"hub\" which integrates information from various modality-specific \"spokes\" regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model's dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing."
        },
        {
            "title": "Start",
            "content": "THE SEMANTIC HUB HYPOTHESIS: LANGUAGE MODELS SHARE SEMANTIC REPRESENTATIONS ACROSS LANGUAGES AND MODALITIES Zhaofeng Wu(cid:227) Xinyan Velocity Yu(cid:226) Dani Yogatama(cid:226) (cid:227)MIT zfw@csail.mit.edu (cid:226)University of Southern California (cid:226) Allen Institute for AI Jiasen Lu(cid:226) Yoon Kim(cid:227)"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through transmodal semantic hub which integrates information from various modality-specific spokes regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the models dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing. 4 2 0 2 7 ] . [ 1 6 8 9 4 0 . 1 1 4 2 : r Figure 1: Examples of the semantic hub effect across input data types. For every other layer, we show the closest output token to the hidden state based on the logit lens. Llama-3s hidden states are often closest to English tokens when processing Chinese texts, arithmetic expressions, and code, in semantically corresponding way. LlaVa, vision-language model, and SALMONN, an audio-language model, have similar behavior when processing images/audio. As shown for the arithmetic expression example, models can be intervened cross-lingually or cross-modally, such as using English even though the input is non-English, and be steered towards corresponding effects. Boldface is only for emphasis."
        },
        {
            "title": "Under review",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Modern language and multimodal models (LMs)1 are capable of processing heterogeneous data types: text in different languages, non-linguistic inputs such as code and math expressions, and even other modalities such as images and sound. How do LMs process these distinct data types with single set of parameters? One strategy might be to learn specialized subspaces for each data type that are only employed when processing it. In many cases, however, data types that are surface-distinct share underlying semantic concepts. This is most obvious for sentences in different languages with the same meaning; but such shared concepts are present across other data types, e.g., between an image and its caption, or piece of code and its natural language description. The human brain, for example, is believed to have transmodal semantic hub (Patterson et al., 2007; Ralph et al., 2017) located in the anterior temporal lobe that integrates and stores semantic information from various modality-specific spokes (e.g., visual/auditory cortices). model, leveraging the structural commonalities across data types, could similarly project their surface forms into shared representation space, perform computations in it, and then project back out into surface forms when needed. To what extent is this idealized strategy adopted by actual models? Wendler et al. (2024) find that on simple synthetic tasks, Llama-2 (Touvron et al., 2023) maps various input languages into shared English space before projecting back out into another language, hinting that it leverages this shared representation scheme to an extent, at least for different languages. We show that this is much more general phenomenon: when model processes inputs from multiple data types, there is shared representation space, and this space is scaffolded by the LMs inherently dominant data type (usually English). By scaffolded, we mean that the shared space can be interpreted to an extent in the dominant data type via the logit lens (nostalgebraist, 2020). Following the cognitive science nomenclature, we call this shared representation space the LMs semantic hub. We first show that LMs represent semantically similar inputs from distinct data types (across languages, or between natural language and arithmetic expressions, code, formal semantic structures, and multimodal inputs) to be close to one another in intermediate LM layers. We further show that we can interpret these hidden representations to an extent using the LMs dominant data typee.g., when processing Chinese input, an English-dominant LM thinks in English before projecting back out to Chinese space. Finally, we perform intervention experiments showing that intervening in the shared representation space using the LMs dominant data type, predictably affects model output when processing other data types; that is, the shared representation space (and the processing of these representations through subsequent layers) is not vestigial byproduct of the models being trained on (say) English-dominant text, but causally impacts model behavior. Our work is complementary and distinct from prior work which finds structural similarities between the representation spaces of models trained (usually independently) on different data types, such as those showing that text representations from text-only LMs can be aligned to vision/audio representations of modality-specific models (Ilharco et al., 2021; Merullo et al., 2022; Li et al., 2023; Ngo & Kim, 2024; Huh et al., 2024; i.a.), and the literature on cross-lingual word embedding alignment (Mikolov et al., 2013; Artetxe et al., 2017; Conneau et al., 2018; Schuster et al., 2019; i.a.). We instead show that an LM trained on multiple data types represents and processes them in shared space without requiring explicit alignment. We hope our findings shed light on ways to more easily interpret the mechanisms of current models and motivate future work aimed at better controlling models using these insights."
        },
        {
            "title": "2 THE SEMANTIC HUB HYPOTHESIS",
            "content": "Let Xz be the domain of some data type where is the set of model-supported data types. E.g., for languages, XChinese could be all Chinese tokens, while for images XImage = [0, 255]wh3 could be the RGB values for an h-sized image patch. Consider function Mz : Sz mapping an input sequence into semantic representation space Sz (i.e., the hub), and verbalization function Vz : Sz Xz, sensible (implementation-agnostic) way to continue the sequence is to first encode the input into modalityz . Given an input prefix wz of length where wz 1:t 1Hereafter, we use the term language model loosely and also consider multimodal language models that process additional data modalities, since such models are commonly trained on top of text LM backbone. 2We release our code at https://github.com/ZhaofengWu/semantic-hub."
        },
        {
            "title": "Under review",
            "content": "agnostic representation min = Mz(w1:t), reason and formulate representation of possible futures to obtain mout Sz, and finally verbalize it via Vz(mout). An LM parameterizes similar process: it uses MLM to map various input data types into representation space SLM Rd (early layers), performs computations in the space (middle layers), and verbalizes the output via VLM (end layers and the LM head). However, it is unknown as to how different data types are structured in the representation space. For example, one possibility is that the LM partitions Rd into disjoint subspaces for each data type and processes them separately. We instead hypothesize that LMs, through training, learn to represent and process different data types in shared representation space that functions as modality-agnostic semantic hub. That is, semantically similar inputs wz1 1:t from distinct data typesfor example texts in different languages that are mutual translationsare similarly mapped in SLM; informally, MLM(wz1 1:t). However, absolute similarity measures (i.e., sim(MLM(wz1 1:t))) are generally difficult and unintuitive to interpret in high dimensional spaces (see for example Beyer et al. (1999)). We thus focus on relative similarity measures, taking semantically unrelated sequence uz2 1:t, and evaluating whether wz1 1:t) MLM(wz2 1:t), MLM(wz2 1:t and wz2 1:t is closer to wz 1:t than uz2 sim (MLM(wz1 1:t), MLM(wz2 1:t, as illustrated on the left of Figure 2: 1:t), MLM(uz2 1:t)) > sim (MLM(wz1 1:t )) . (1) Moreover, when the LM has dominant data type in training (e.g., English for Llama-2), we hypothesize that this shared representation space is anchored by tokens in (denoted as τ ), thereby allowing the semantic hub to be interpretable. We focus on anchor tokens that represent continuation of the input, which autoregressive LMs are trained to model. Informally, MLM(wz 1:t) is close to the embedding of token τ , which represents corresponding continuation in z, more than some τ in nondominant data type z, even when is the input data type. In Figure 2, e.g., with an English-dominant LM, its encoding of the Chinese prefix wz 1:t =这篇 论文太难 (trans. This paper is so hard to) should be closer to the representation of the continuation word in English τ than its Chinese translation τ Figure 2: An illustration of our hypothesis, where semantically equivalent inputs (across data types) have similar representations, and this representation is close to the continuation token in the dominant data type. Here, is English and is Chinese. (cid:16) sim MLM(wz =写. Formally: 1:t), emb(τ (cid:17) ) > sim (cid:16) MLM(wz 1:t), emb(τ (cid:17) ) . =write (2) 2.1 METHOD: TESTING THE SEMANTIC HUB HYPOTHESIS We test the semantic hub hypothesis for LMs by considering pairs of distinct data types, the dominant one and non-dominant one z, different for each experiment. When semantically related inputs are available (e.g., an image and its caption), we directly test Eq. 1 by using hℓ t, the LMs hidden state at position and layer ℓ, as MLM(wz We operationalize Eq. 2 via the logit lens (nostalgebraist, 2020), simple training-free approach for interpreting the hidden states of model. Transformer LMs produce the next-token distribution using softmax (cid:0)OhL (cid:1) (omitting the bias term) where is the output token embeddings (or unembedt dings) and hL is the final layer hidden state. Logit lens applies the same operation to the intermediate (cid:1). Logit lens has been found to produce t) := softmax (cid:0)Ohℓ layers ℓ [L] to obtain plogitlens( hℓ meaningful distributions that shed light on an LMs internal representations and computations. 1:t), and using cosine similarity for the similarity function. Under the logit lens, emb() in Eq. 2 is the output embedding of token. Using the dot product for sim(), Eq. 2 is equivalent to comparing the logit lens probabilities, plogitlens (cid:16) > plogitlens (cid:16) τ τ (cid:17) (cid:17) , (3) hℓ hℓ"
        },
        {
            "title": "Under review",
            "content": "(a) The cosine similarity of intermediate representations of English and Chinese parallel texts. (b) Same as (a), but subtracted by baseline over non-parallel texts. (c) Llama-3 logit lens log prob. of parallel English vs. Chinese tokens when processing Chinese text. Figure 3: Results for the multilingual experiments. The 95% CI is plotted in all. Parallel texts have similar representations. Hidden states for Chinese texts are close to the unembedding of English tokens. i.e., testing whether the probability of the continuation in the dominant language is more likely than the continuation in the original input data type. Since the logit lens is tailored for probing out single token, we usually consider short-enough verbalizations such that single BPE token can reliability identify it, such as when the two verbalizations are two single words that are semantic equivalents. But we also consider longer future verbalizations when its first token unambiguously suggests one interpretation in that context, which allows more flexibility. Since τ is unavailable in many multimodal models without vocabulary tokens for z; we only test Eq. 1 in those cases."
        },
        {
            "title": "3 EVIDENCE OF A SEMANTIC HUB",
            "content": "We apply our tests across diverse data types and models: inputs in different languages, arithmetic expressions, code, formal semantic structures, visual inputs, and audio. We consistently find evidence of shared representation space in all cases. 3.1 MULTILINGUAL Much past work has categorized language representations in multilingual LMs (Hua et al., 2024; Alabi et al., 2024; Tang et al., 2024b; Zhao et al., 2024; Zeng et al., 2024; i.a.). Wendler et al. (2024) recently find that when processing specific in-context learning (ICL) templates for highly synthetic lexical-level tasks (word repetition, word translation, etc.) in non-English languages, the intermediate hidden states of Llama-2 are closer to the unembeddings of English tokens than in the output language. This is consistent with our hypothesis, albeit constrained to simple synthetic task and one LM. We show that this shared representation space is general property of LMs and also occurs when processing naturalistic text. Experiment 1: Representations are similar for translations. Translation datasets enable direct test for Eq. 1, with semantically equivalent cross-lingual sentences as wz1 1:t and nonmatching sentence as uz2 1:t . We use the professionally-translated English-Chinese parallel sentences from Chen et al. (2016) (N = 5260). For each sentence pair, we use template to transform each sentence and compute the representation cosine similarity for each layer, using the last token position as the sentence representation, which has been shown to preserve information (Morris et al., 2023). We consider two English-dominant LMs, Llama-2 and Llama-3 (Llama-3-Team, 2024), one Chinesedominant LM, Baichuan-2 (Yang et al., 2023), and one multilingual LM, BLOOM (BigScience, 2023), specifically the 7B/8B variants. See A.1 for more details. 1:t and wz2 Figure 3a shows high raw cosine similarity. We also follow Eq. 1 and subtract the average similarity between non-matching sentence pairs as baseline, separately for each layer. Figure 3b shows that the similarity between translations is higher than this baseline, most prominently in the middle layers. These trends support the hypothesis that the middle layers act as the semantic hub of the LM. Notably, this trend also exists for BLOOM which does not have dominant pretraining language. Experiment 2: Representations are anchored by semantically-equivalent dominant-language tokens. We next test Eq. 3, i.e., whether continuations in the dominant language have higher probability than those in the input language at intermediate LM layers. For the English-dominant Llama-3, we use 1,000 Chinese prefixes wz 1:t from Wikipedia (Wikimedia-Foundation, 2023) as input."
        },
        {
            "title": "Under review",
            "content": "Figure 4: Language probabilities of English and Chinese (and the top language, when it is neither, which only happens for Bloom). Regardless of the input language, the dominant LM language is more salient in the early-middle layers, and the input language is more salient in the final layers. Bloom does not have clear intermediate latent language. t+1) and τ to be the next Chinese token (i.e., wz For each, we take τ to be the (first token of the) English translation of wz t+1. See A.1 for more details. Figure 3c plots the logit lens probability for the two tokens as well as the uniform distribution probability. In early layers, we cannot read out either token better than random chance. After layer 17, the model representations are substantially closer to the English token than the Chinese token until layer 31, showing that the model hidden space is indeed better scaffolded by English than Chinese. Next, we extend this analysis to consider global language-level trends pℓ(z) where is language, at some layer ℓ. We first compute p(w z), the token distribution under language z, by running the LM tokenizer on the language-specific split of the mC4 dataset (Xue et al., 2021). We then use Bayes rule to estimate p(z w) p(w z)p(z) with uniform prior p(z).3 We compute the probability of t) (cid:80) hℓ ts belonging to each language as p(z hℓ t). Finally, again with uniform prior assumption, we average p(z hℓ t) across tokens to obtain language distribution pℓ(z). If our hypothesis that the shared representation space is better scaffolded by the dominant language is true, we expect the dominant language to have the highest probability across input languages in the middle layers. wV p(z w)plogitlens(w hℓ Figure 4 shows the probability of English and Chinese (and of the top language when it is neither) for each layer on 10,000 English/Chinese Wikipedia sentences. When English-dominant models process Chinese text, Wendler et al.s finding generalizes, where English dominates in the intermediate layers and Chinese only dominates in the final layers. On the Chinese-dominant LM, this trend flips: when processing English text, its intermediate layers are closer to Chinese space and the final layers are closer to English space. For BLOOM, multilingual model with relatively balanced training language mixture, we do not see clear dominating language in the intermediate layers; when we manually inspect the closest token, in most cases we observe symbols with no clear semantics (though this does not mean it does not have unified representation space: see Exp. 1). 70B model trends are also highly similar to the 7B/8B ones. 3.2 ARITHMETIC We hypothesize that similar trend exists when LMs process arithmetic expressions where they route to shared space anchored by numerical words in English in intermediate layers. We consider simple expressions in the form of a=b+c or a=b*c; for simplicity, we restrict and to be at most two digits and to be single positive digit. Experiment 1: Representations are similar for translations. Here, we only consider the righthand side, b+c and b*c, as wz1 1:t in Eq. 1. Like in the multilingual case, we translate them into English (e.g., five plus three) as wz2 1:t, and evaluate the representation cosine similarity 3This prior obviously does not reflect the training language distribution, but in fact makes our trends even more salient, since using real (or estimated) p(z) would make p(z w) even larger for the dominant language."
        },
        {
            "title": "Under review",
            "content": "(a) Cosine similarity between an arithmetic expression in Arabic numerals vs. English words, broken down into separate categories. (b) Same as (a), but only the exact translation similarities subtracted by the others. (c) Logit lens log probability when predicting number, between either the number itself or its English equivalent. Figure 5: Results for the arithmetic experiments. The 95% CI is plotted in all. Expressions in Arabic numerals have similar representation as corresponding expressions in English, as well as the unembeddings of corresponding number words in English. between every English expression and every numeric expression, throughout layers. We group the pairwise cosine similarities in three buckets: (1) exact translation (e.g., 5+3 and five plus three; = 1123), (2) non-exact but same value (e.g., 5+3 and two plus six; = 13293), and (3) different value (N = 1247836). Figure 5a shows that exact translations have high cosine similarity, although this is to be expected since embeddings of numbers and their corresponding English words are near one another (thus even bag-of-word-embeddings should also have high similarity). More interestingly, we that the similarities are still higher when the surface forms are distinct but the meaning of the expression (i.e., the value of the expression) is the same. Next, like in 3.1, we subtract the cosine similarities among non-translation pairs as baseline (uz2 1:t). Figure 5b shows high similarity in the early-middle layers for translations over the baseline, but gradually decreasing to near 0. Experiment 2: Representations are anchored by semantically-equivalent English words. We hypothesize that, for some prefix such as a=b+, the intermediate representations hℓ are close to the English word for that would make the equality hold (see Figure 1). First, we randomly sample 100 such prefixes and take the representation of the last token at all layers. For each prefix, we plot the representation evolution throughout layers using PCA, as well as the unembeddings of numbers in English τ vs. numerals τ . Figure 6 shows that the representations indeed go through the space occupied by the English words in intermediate layers. Next, we repeat our logit lens experiments, inspecting the log probability of the following numeral token vs. its English version (N = 1123). Figure 5c shows that the two tokens have similar log probability until around layer 25, after which the numeral token dominates. Figure 6: The Llama-3 hidden representation evolution when predicting number, projected by PCA where the principal components are learned on the output embeddings of 20 number tokens, 10 in English and 10 numerals. 3.3 CODE Many recent LMs are trained on code corpora (Llama-3-Team, 2024; Gemini-Team, 2024; i.a.). We find that they similarly process code input by projecting them into unified representation space shared with regular language tokens. Figure 7 shows some examples, where the LM in the intermediate layers tends to verbalize the future in free-form English, unconstrained by program syntax. E.g., in the first program, given the Python prefix ... for for idx, in enumerate(numbers): idx2, e2 in enumerate(numbers, instead of the ground-truth continuation in Python ): if idx != idx2:, the most salient intermediate token is except, likely attempting to predict in English (for each element in numbers) except if it is equal to idx. Similarly, in list literal expression [1.0, 2.0, instead of continuing in Python 3.0, it predicts and, which is natural way to continue in English. In these cases, it is difficult to obtain semantically equivalent"
        },
        {
            "title": "Under review",
            "content": "(a) Llama-2 logit lens log probabilities (and the 95% CI) at commas in Python list literals, of the English and token (and baseline tokens) vs. the actual next token in the program, from MBPP. (b) The distance between Llama-2 hidden states when predicting function argument, to the unembedding of the arguments name (its semantic role) vs. the actual argument expression, in MBPP. Figure 8: Results for the code experiments. Code expressions are close to semantically meaningful free-form English words in early-middle layers, such as and in list literals and the arguments semantic role in function calls; in the final layers, the representation converges to the context-constrained Python token. English-Python prefix pairs like in 3.1 for testing Eq. 1,4 so we only test Eq. 3 across few targeted examples in Python. Experiment 1: Representations are anchored by semanticallylist equivalent English words: literals. We test systematically the list case, where hl is the hidden state after processing , during list processing. We use τ =and and further take τ to be the actual next token. Figure 8a shows that this trend holds on all such commas in the MBPP dataset (Austin et al., 2021) (N = 6923, including unit tests): as expected, in the final layers, the representation is closer to the ground truth next tokens unembedding, and closer to and in the middle layers. We also show the probability with two other tokens, or and not, as baselines, both of which are lower than and. Figure 7: Logit lens analysis on Llama-2 processing Python programs. For every other layer, we show the closest token (sometimes whitespace) to the hidden states before the grayed-out texts. The model tends to verbalize the future prediction in English that corresponds to the code continuations (in gray). Experiment 2: Representations are anchored by semanticallyequivalent English words: function call arguments. Function arguments have names in the definition, such as range(start, end, step); but when invoked, they are filled with actual context-appropriate expressions. We call the argument names semantic roles, and the context-specific expressions the surface forms, inspired by thematic relations in linguistics (Fillmore, 1968; Jackendoff, 1974; i.a.). With an example in Figure 1, we show that LMs predict the arguments by first thinking about their semantic role (τ ) and then instantiating with surface-constrained expressions (τ ). We extract all function calls and arguments from MBPP with simple filtering, resulting in 540 arguments (see A.2 for details). For each argument, we use the logit lens to inspect the hidden states hℓ at the preceding token (( or ,). For each argument, Figure 8b visualizes if the semantic role or the surface form is closer to each layers hidden state. The semantic role (τ ) dominates for the early to middle layers, and only in the final layers do the representations converge towards the surface form argument (τ ). 4We may argue that functions and their specifications constitute such pairs, but we found that their correspondence is often too abstract and non-exact to manifest as similar representations."
        },
        {
            "title": "Under review",
            "content": "(a) Similarity between sentence and its semantic structure, subtracted by baseline over non-parallel texts. (b) Similar as (a), but the baseline is computed by swapping the positions of names in the semantic structure. (c) Same as (b), but we shuffle the predicates in the semantic structure to ensure robustness. Figure 9: Representation similarity experiments for formal semantics. sentence and its semantic structure have high representation similarity, even with strict controls. (a) Agent (b) Recipient (c) Theme (d) Agent (e) Recipient (f) Theme Figure 10: Top: Logit lens probability of the thematic roles of verb arguments, grouped by the ground-truth argument thematic role in each subplot, for Llama-3. Bottom: The same quantity, subtracted by the average probability as the baseline, separately for each role. The 95% CI is plotted in all. The prefix representation before an argument with particular thematic role has high similarity with the unembedding of that role in English, when the baseline is adjusted for. 3.4 FORMAL SEMANTICS Much probing work has shown that semantic information can be probed out from LMs hidden states (Tenney et al., 2019; Wu et al., 2021; Li et al., 2021; i.a.). We show that this manifests without learned probe. We use the COGS data (Kim & Linzen, 2020), which contains synthetically generated English sentences and their semantic structures (in fairly standard format rooted in the Neo-Davidsonian tradition (Parsons, 1990)).5 E.g., the sentence Eleanor sold Evelyn the cake. has the representation *cake(x4); sell.agent(x1, Eleanor) AND sell.recipient(x1, Evelyn) AND sell.theme(x1, x4). Experiment 1: Representations are similar between sentence and its semantic structure. Like in 3.1 and 3.2, Figure 9a shows that the representation of sentence is closer to its semantic structure than non-matching baseline. As in the arithmetic expression case however, there is confounder: this could be due to surface lexical overlap between the two (e.g., Eleanor, Evelyn, cake, and sell/sold in our example) rather than deep understanding of their equivalence. To 5It also has similar number of active vs. passive sentences, thus providing clean testbed since one cannot simply use word order to predict the thematic role."
        },
        {
            "title": "Under review",
            "content": "(a) The cosine similarity difference between intermediate representations of matching images and captions, over non-matching ones. (b) The frequency of the closest token to LLaVas hidden states describing the image color, against baseline using white. (c) The cosine similarity difference between intermediate SALMONN representations of matching audios and labels, over non-matching ones. Figure 11: Results for the multimodal experiments. The 95% CI is plotted in all. Model representation of (a) visual and (c) audio inputs and their textual labels are similar. Furthermore, in (b), model representations of individual color patch tokens are close to English words for those colors. More results are in C. control for this, we find COGS sentences with two proper names like the above example (N = 2233), swap their positions in the semantic structure such that it no longer corresponds to the sentence, and yet the lexical overlap is unchanged. With this stronger baseline (where bag-of-words models would do no better than chance), we still see in Figure 9b that semantically matching pairs have more similar representations. On top of this, we further control potential positional confounders of predicates in the semantic structure by randomly swapping their positions, and results in Figure 9c have the same trend. We also note that Bloom, arguably the weakest model (smaller pretraining set and older architecture) does not do much better than chance, potentially suggesting that this representation strategy correlates with model capacity. Experiment 2: Word representations are correlated with their thematic roles in semantics theory. Formal semantic structures are naturally not dominant data type, and we hence do not expect Eq. 3 to hold. Instead, we perform logit-lens-style analysis that still tests Eq. 1, but with finer granularity, focusing on token-level representation similarity, specifically the arguments of verbs and their thematic roles. For example, Eleanor in the earlier example has the role agent and Evelyn is the recipient. We expect that, when predicting an argument, the hidden states are more similar to the corresponding thematic role than non-matching ones. We again only consider proper names in COGS, excluding those that start sentence, as in this case there is no context in which to predict the semantic role. This results in 3257 agents, 2583 recipients, and 714 themes. To avoid any memorization effects arising from the LMs being potentially trained on COGS, we further randomly replace the proper names with another one in COGS and make sure the new sentence does not appear in the dataset. For each proper name with given thematic role, we look at the logit lens probability of all roles. Figures 10a to 10c show that, for Llama-3, the role probabilities tend to peak in the middle-late layers and drop down in the final layers, familiar trend. Nevertheless, the corresponding role token does not always receive the highest probability. We believe this is because Llama-3 has prior that is closer to the word agent, which almost always has the highest probability. So we adjust for this prior by subtracting from each curve baseline probability, separately for each role and for each layer, that is the average role probability across all instances. Figures 10d to 10f show this posterior: when predicting the argument with given thematic role, the corresponding thematic role token is always the closest to the intermediate representations. We do not observe these trends in Llama-2. 3.5 VISUAL INPUT Past work has investigated the representation of separately trained vision and text models, often finding that their representation spaces are similarly structured and alignable (Merullo et al., 2022; Li et al., 2023; Huh et al., 2024; i.a.). We show that when trained together, vision-language models learn to project both modalities into joint representation space. Current vision-language models typically represent images by segmenting them into patches, embedding them into image tokens, and then feeding them into the transformer model along with other text tokens (Lu et al., 2023; 2024; Liu et al., 2023; i.a.). We hypothesize that the intermediate representations of the image patches are close to the corresponding language tokens that describe the scene. Experimental details are in A.3."
        },
        {
            "title": "Under review",
            "content": "Experiment 1: Representations are similar between an image and its caption. Though not constituting exact semantic equivalence, an image paired with its caption provides one possible test for Eq. 1. We take 1000 images and corresponding captions in the MSCOCO dataset (Lin et al., 2014) and measure their hidden states cosine similarity in LlaVa-7B (Liu et al., 2023) and Chameleon7B (Chameleon-Team, 2024). Again, we subtract the average similarity between non-matching image-caption pairs as baseline, separately for each layer. Figure 11a shows that semantically matching inputs across modalities are closer to one another than would be expected from chance, as in the translation experiments. As mentioned in 2.1, we do not perform test of dominant data type anchoring (Eq. 3) due to lack of non-text tokens in the vocabulary. But, like in 3.4, we perform fine-grained image-patch-level analysis using the logit lens. As toy setting for illustration, we inspect LlaVas representations of pure color images, specifically those in red, green, blue, and black. Figure 11b shows that, in up to more than 20% of the time in the intermediate layers (averaged across the patches and the four colors, = 2304), the closest token is the corresponding color word (out of all vocabulary tokens). C.1 details more comprehensive experiments where we similarly find an alignment between the patches and the caption, as well as between patch and its semantic segmentation label. 3.6 AUDIO Audio is another modality that is often modeled jointly with text (Lu et al., 2024; Gong et al., 2023; 2024; i.a.), and we perform similar experiments using SALMONN (Tang et al., 2024a), an audio-text model. We use the VGGSound dataset (Chen et al., 2020) which contains 10-second audio clips with labels, e.g., duck quacking or playing cello. Experiment 1: Representations are similar between audio and its label. We study the representation cosine similarity between an audio and its label description, and subtract from it baseline which is the average similarity between non-matching pairs, separately for each layer. On 1000 samples from VGGSound, we see in Figure 11c that semantically matching audios and labels have more similar representations in the intermediate layers. Like for visual inputs, we do not investigate the dominant data type anchoring effect, but instead use the logit lens to confirm that the representation alignment also occurs on token level, in C.2."
        },
        {
            "title": "INTERVENING IN THE SEMANTIC HUB",
            "content": "Prior work has argued that interpretability results should be tested under causal framework, to ensure that the observation is not vestigial byproduct of model training that has no actual effect on model behavior (Vig et al., 2020; Ravichander et al., 2021; Elazar et al., 2021; Chan et al., 2022; i.a.). In this section, we show that the semantic hub does causally affect model output. Specifically, semantically transforming hidden representations according to (dominant) English representations leads to predictable behavior changes in non-dominant data types. For different experiments, we use different kinds of intervention, and we explain our design decisions at the end of this section. We report hyperparameters and further experimental details in B. Multilingual. Past work has shown that (monolingual) interventions in the middle layers can steer the output of LMs in predictable ways (Subramani et al., 2022; Turner et al., 2024; Rimsky et al., 2024; i.a.). If the English-dominant LMs have shared representation space, we should be able to intervene on this space in English even when processing other languages. We use popular hidden space intervention technique, Activation Addition (ActAdd; Turner et al., 2024), which works by: (1) taking pair of contrasting steering words that semantically represent the steering effect (e.g., Good and Bad for sentiment steering), (2) taking their hidden state difference at an intermediate layer, and (3) scaling and adding the steering vector to the hidden states for the original forward pass of the regular generation process, at the same layer, at the beginning of the sequence. We generalize their sentiment-steering experiment cross-lingually. See Turner et al. (2024) for details. We consider two non-dominant languages, Spanish and Chinese, and take 1000 prefixes each from the InterTASS dataset (Spanish; Díaz-Galiano et al., 2018) and the multilingual Amazon reviews corpus (Chinese; Keung et al., 2020), and generate continuations either without modifications or intervened using ActAdd. As the steering vector, we use the difference between positive vs. negative sentiment trigger words, in the appropriate direction. Specifically, we use Good and Bad for English, Bueno and Malo for Spanish, and 好 and 坏 for Chinese. In addition to sentiment evaluation, we also"
        },
        {
            "title": "Under review",
            "content": "Table 1: Steering Llama-3s output sentiments using trigger words in English vs. the input language (either Spanish or Chinese). We report the mean sentiment, disfluency (perplexity), and relevance of the continuation, as well as the standard deviation across 10 seeds. Cross-lingual steering is consistently successful, sometimes even more than monolingual steering, without substantial damage in text fluency and relevance. Text Lang. Steering Dir. Steering Lang. Sentiment Disfluency () Relevance () Spanish Chinese None None None Spanish English Spanish English None Chinese English Chinese English 0.1430.022 7.351.19 0.8610.002 0.1250.034 0.1390. 0.1750.035 0.1590.026 10.542.39 8.752.20 7.982.04 7.351.01 0.8420.004 0.8570.002 0.8560.002 0.8590.003 0.1780. 11.063.12 0.8690.004 0.1520.040 0.1610.029 0.1530.034 0.1790.032 10.782.66 11.361.13 11.123.12 10.903. 0.8660.005 0.8640.004 0.8700.004 0.8690.003 measure the generation fluency and compute the relevance of the generation with the prefix using trained models, following Turner et al. (2024). Ideally, the intervention should achieve the desired sentiment without hurting text fluency and relevance, and the English intervention should be just as effective as the text language intervention (see B.1 for more details). Because we take intermediate layer representations of the steering words (step (1)), if the semantic hub is language-agnostic, we expect similar cross-lingual representations and in turn similar steering effects across steering languages. Table 1 shows that this is indeed true for Llama-3: ActAdd in the text language is often effective, achieving the intended effect on sentiment, with usually only statistically insignificant decrease in fluency and relevance. This aligns with the English-only findings in Turner et al. (2024). And intervening in English is similarly effective as using the text language. Table 2 (appendix) shows the results for Llama-2, with very similar trends. Arithmetic. We perform intervention using our arithmetic expressions in 3.2, for example 4=1+3. We intervene by attempting to modify the token after + to be one smaller, e.g. 2 here, and expect this to not only lead the model to output 2 instead of 3, but also fundamentally affects the models reasoning process and causes the model to patch this error with an additional suffix +1, i.e., 4=1+2+1. We use ActAdd except for adding the intervention vector (e.g., three two) only at the position of +.6 For all addition expressions in 3.2 (N = 846), we perform such intervention at an intermediate layer (25 for Llama-3 and 30 for Llama-2) and measure how often this leads to the model correctly outputting the decremented number followed by +1, versus unchanged, or changed to some other output. Figure 12a shows that, as the intervention coefficient (i.e., the scaling constant of the vector) increases, this procedure leads to the expected output for up to > 90% of the instances. Code. Based on our semantic role observation in 3.3, we intervene using the range function. We focus on two overloaded versions of range: range(start, end) and range(end). If the semantic hub causally affects model output, then we can intervene in it to select the function version to use after range( in some context, using the English words start and end. Specifically, we take all single-argument range(end) calls in the MBPP dataset (N = 159) and attempt to expand it into range(0, end). We use similar intervention method, except simply using the unembeddings of trigger tokens instead of intermediate LM hidden states. We use as trigger tokens (start end) and add the intervention vector to the hidden states corresponding to the open parenthesis ( at an intermediate layer (layer 17). For all these range calls in the dataset, we let Llama-27 generate without and with intervention. Figure 12b shows that, with increasing intervention strength, more instances are successfully steered to range(0, end), up to 67%. 6Another difference is that we do not use the hidden representation after seeing e.g. three, because that usually represents the next token. Instead, we use prefix that uniquely determines the number, e.g. Eight equals to five plus, and take the last token hidden representation, which is supposed to represent three. 7We do not consider Llama-3 in this case because its default behavior usually generates range(0, end) in the first place, and it is unclear how to steer from range(0, end) to range(end)."
        },
        {
            "title": "Under review",
            "content": "(a) Steering arithmetic expressions results to different value. singlerange(end) to be predicted as (b) Steering argument call double argument. (c) Replacing image representations of color with English tokens of another color, and expecting the model to predict the latter. (d) Steering mammal sounds to be predicted sounds as non-mammal using English words of non-mammals; vice versa. Figure 12: For (a) arithmetic (Llama-2 and -3), (b) code (Llama-2), (c) images (Chameleon), and (d) audio (SALMONN), steering model output using English words, for various intervention strengths ((a), (b), (d)) and layers ((c)). (a)-(c) measure successfulness with the proportion of instances steered to the correct output, and (d) measures the probability of predicting mammals. Overall, intervening in the unified representation space in English reliably leads to desired model output changes. Formal semantics. Unlike in the cases above where we modify the surface continuation by manipulating some underlying concept structure, natural language prefix usually licenses one single possible thematic role to follow. So we do not perform an intervention experiment here. Visual input. We show that we can steer the output of Chameleon by intervening on the image patches using language tokens and analyzing how this affects the textual output. Focusing on the color setup, if the representation for color is similar between visual and language inputs, we hypothesize that we can replace the image hidden states corresponding to one pure color image patch with the unembedding of the language token for another color, and mislead the model to perceive the new color when asked about the image color. Note that replacing the hidden state is more invasive intervention than addition. However, there is confounder: the intervened word may lexically bias the model to generate the same word, without performing reasoning that incorporates the new color, because the desired answer after intervening is equivalent to the intervention word itself (unlike in the previous cases). To control for this, we show two colors in one image and only intervene at the positions corresponding to one color: if the intervention unconditionally and lexically biases the generation to the new color, this effect would (incorrectly) affect both colors.8 We consider all color pairs using the same colors as in 3.5: red, green, blue, and black,9 and picking one color in the pair and intervene to new third color (N = 48). As the intervention, we start from layer ℓ and replace all hidden states at and after ℓ to be the unembedding of the new color minus the old color. We ask the model what the two colors in the image are, and only consider the intervention successful if the model answers both the new color and the other unintervened color correctly. Figure 12c shows the success rate across all ℓ: it gets as high as above 80%.10 We highlight that, for both this experiment and the earlier ones in this section, the interventions are not even necessarily guaranteed to lead sensible outputs, let alone correct ones. Audio. We perform similar intervention with SALMONN, with the same desideratum that the question-answering process should require some reasoning rather than outputting the intervened token as-is. We consider 1000 animal sounds in the VGGSound dataset, specifically only singleword animals, and ask Is this animal mammal? We intervene both on mammal sounds with random non-mammal word (and expecting the model to be more likely to reply with No) and vice versa. We perform the invention similarly to the code case, adding the unembedding difference between the new trigger word and the original animal name, scaled by constant, at layer 13. We 8We tested settings that require more sophisticated reasoning such as asking for country flag with the two colors, or asking about spatial relationships of the colors. They seem to be beyond the capability of Chameleon-7Beven without interventions, the model cannot answer the questions correctly. 9We tried other colors but Chameleon-7B, without interventions, cannot recognize those colors reliably 10One may argue this is conceptually similar to half-language half-image input. There are many distinctions: most importantly, half-image is not processable by Chameleon and severely goes out of its training distribution, since it only ever processes images of size exactly 512 512. Other distinctions include: the presence of special token marking the beginning of the image; our intervention repeats the new color token, once for each patch, rather than just one; and the token representation is held constant across layers rather than evolving; etc."
        },
        {
            "title": "Under review",
            "content": "measure the probability of the Yes token and the No token and compute the normalized Yes probability. Figure 12d visualizes the two cases across multiple intervention strengths. As the strength increases, the model is more likely to predict in the steered direction, again demonstrating cross-data-type intervention effectiveness. Note on the intervention methods used. In the majority of our experiments, we add an intervention vector to particular layer at particular input position, and the intervention vector is computed using the difference between the unembeddings of two trigger tokens in another data type, scaled by constant. This is simple and naturally follows the results of our logit lens experiments. However, this may not be convincing for data types highly parallel to English. For example, for the multilingual intervention where we use the difference between the unembeddings of, e.g., Good and Bad, the intervention may succeed only because this difference is similar to the unembedding difference between Bueno (trans. Good) and Malo (trans. Bad) due to cross-lingual embedding alignment (Mikolov et al., 2013; i.a.).11 So instead, as the intervention vector, we use not the unembedding difference but the difference between intermediate model hidden states, and they are expected to be the same across modalities as our hypothesis predicts. We do this for multilingual intervention and arithmetic intervention. The former, using ActAdd, further differs by adding the vector not at targeted position but in the beginning of the sentence. In addition to it being an established intervention method, we used it because in sentiment steering, there is no targeted position of interest (unlike for arithmetic), but we want to steer the sentence representation globally. Finally, we use more invasive intervention for visual inputs where we do not just add, but replace, hidden representations because we found it to work well, providing stronger result."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Our experiments across diverse languages and modalities give empirical support to the semantic hub hypothesis; language models seem to make efficient use of model capacity and learn to represent semantically similar inputs from different modalities near one another. Such semantic hub has been hypothesized to exist in the human brain (Patterson et al., 2007; Patterson & Ralph, 2016; Ralph et al., 2017). Inasmuch as both the human brain and language models are resource-constrained information processing systems, it is perhaps unsurprising that both systems make use of semantic hub. We note however that this strategy may not always be desirable property of language models. Wendler et al. (2024) conjectured that language models might inherit biases present in the training data of the dominant language; if true, an extreme implementation of this strategy could force an alignment of ideology across languages and potentially harm inclusivity. Similarly, internalizing arithmetic expressions in natural language may not be ideal as part of the algorithm with which language model implements arithmetic, considering their structural differences and the vast linguistic variation in number expressions in their composition (c.f. Danish tooghalvfems; trans. ninety-two, but compositionally representing 2+(5-0.5)20) or even bases (c.f. the base-23 numeral system in the Kalam language (Laycock, 1975)). In general, an over-reliance on the dominant data type could be detrimental."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Representation alignment between separately trained models. long line of work has investigated the representations of separately trained mono-data-type models, and showed that they can be aligned through transformation. In the multilingual case, it has been found that separately trained word embeddings for different languages can be aligned (Mikolov et al., 2013; Smith et al., 2017; Cao et al., 2020; i.a.). Similarly, prior work has shown that visual representations and text representations from different models can be mapped together (Merullo et al., 2022; Koh et al., 2023; Maniparambil et al., 2024; i.a.). Lu et al. (2021) showed that language-only models can be minimally finetuned to achieve high performance in other modalities. Our work, in contrast, looks at single static model that can process multiple types of input data, and finds that the resulting representations align, without needing transformation. Concurrent work (Luo et al., 2024) found similar alignments of task vectors (Ilharco et al., 2022; Hendel et al., 2023) in vision-language models. 11Note that this is less problematic for other data types such as code, where it is unlikely that the unembedding difference between end and start is similar to that between the actual surface form end argument and 0."
        },
        {
            "title": "Under review",
            "content": "Representation evolution throughout layers. Much past work has analyzed the representation evolution throughout transformer layers, inspecting how it affects reasoning (Yang et al., 2024), factuality (Chuang et al., 2024), knowledge (Jin et al., 2024), etc. From another angle, work on layer pruning and early exiting also speaks to the representation dynamics across layers (Gromov et al., 2024; Sanyal et al., 2024; i.a.). More mechanistically, Merullo et al. (2024), Todd et al. (2024), Hendel et al. (2023), i.a., try to more precisely characterize the representation changes algorithmically. Inspecting model hidden states. We adopted the logit lens for its simplicity which brings few confounders. However, alternatives exist, usually requiring some training (Belrose et al., 2023; Ghandeharioun et al., 2024; Templeton et al., 2024; i.a.). They allow for more expressive explanations, though at the risk of overfitting. Similar methods have been developed for other modalities, such as Toker et al. (2024). Testing our hypothesis using these methods would be valuable future work."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This work proposed and investigated the semantic hub hypothesis, which posits that language models represent semantically similar inputs from distinct modalities near one another in their intermediate layers. We find evidence of this phenomena across multiple language models and data types, and further observe that intervening in this space through the models dominant language (usually English) leads to predictable model behavior changes. ACKNOWLEDGMENTS We thank Alex Gu, Alexis Ross, Alisa Liu, Aryaman Arora, Asma Ghandeharioun, Cedegao E. Zhang, Han Guo, Jack Merullo, Linlu Qiu, Mor Geva, Naman Jain, Ruochen Zhang, Sarah Wiegreffe, Shushan Arakelyan, and Yung-Sung Chuang for discussions and help at various stages of this project. Figure 1 uses icons from flaticon.com."
        },
        {
            "title": "REFERENCES",
            "content": "Jesujoba Alabi, Marius Mosbach, Matan Eyal, Dietrich Klakow, and Mor Geva. The hidden space of transformer language adapters. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024. URL https://aclanthology.org/2024.acl-long.356. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning bilingual word embeddings with (almost) In Proceedings of Annual Meeting of the Association for Computational no bilingual data. Linguistics (ACL), 2017. URL https://aclanthology.org/P17-1042. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint, 2021. URL https://arxiv.org/abs/2108.07732. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint, 2023. URL https://arxiv.org/abs/2303.08112. Kevin S. Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When is nearest neighbor meaningful? In Proceedings of the 7th International Conference on Database Theory, ICDT 99, pp. 217235, Berlin, Heidelberg, 1999. Springer-Verlag. ISBN 3540654526. BigScience. Bloom: 176b-parameter open-access multilingual language model. arXiv Preprint, 2023. URL https://arxiv.org/abs/2211.05100. Steven Cao, Nikita Kitaev, and Dan Klein. Multilingual alignment of contextual word representations. In Proceedings of the International Conference on Learning Representations (ICLR), 2020. URL https://openreview.net/forum?id=r1xCMyBtPS. Chameleon-Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint, 2024. URL https://arXiv.org/abs/2405.09818."
        },
        {
            "title": "Under review",
            "content": "Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldwosky-Dill, Ryan Greenblatt, Jenny Causal Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, AI Alignscrubbing, ment Forum, 2022. URL https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/ causal-scrubbing-a-method-for-rigorously-testing. rigorously testing interpretability hypotheses. and Nate Thomas. method for Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audiovisual dataset. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020. URL https://ieeexplore.ieee.org/document/9053174. Song Chen, Gary Krug, and Stephanie Strassel. Gale phase 3 and 4 chinese newswire parallel text. Linguistic Data Consortium, 2016. URL https://catalog.ldc.upenn.edu/LDC2016T25. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. URL https://openreview. net/forum?id=Th6NyL07na. Alexis Conneau, Guillaume Lample, MarcAurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Word translation without parallel data. In Proceedings of International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1710.04087. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2020. URL https://aclanthology.org/2020. acl-main.747. Manuel Carlos Díaz-Galiano, Eugenio Martínez-Cámara, Miguel Ángel García Cumbreras, Manuel García Vega, and Julio Villena-Román. The democratization of deep learning in tass 2017. Proces. del Leng. Natural, 2018. URL https://api.semanticscholar.org/CorpusID: 13667878. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics (TACL), 2021. URL https://doi.org/10.1162/tacl_a_00359. Charles J. Fillmore. The case for case. In Emmon Bach and Robert T. Harms (eds.), Universals in Linguistic Theory, pp. 088. Holt, Rinehart and Winston, New York, 1968. Gemini-Team. Gemini: family of highly capable multimodal models. arXiv preprint, 2024. URL https://arxiv.org/abs/2312.11805. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscopes: unifying framework for inspecting hidden representations of language models. arXiv preprint, 2024. URL https://arxiv.org/abs/2401.06102. Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky, and James Glass. Joint audio In Proceedings of the IEEE Automatic Speech Recognition and and speech understanding. Understanding Workshop (ASRU), 2023. URL https://arxiv.org/abs/2309.14405. Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and understand. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=nBZBPXdJlC. Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A. Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint, 2024. URL https://arxiv. org/abs/2403.17887. Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. In Findings of the Association for Computational Linguistics: EMNLP, December 2023. URL https:// aclanthology.org/2023.findings-emnlp.624."
        },
        {
            "title": "Under review",
            "content": "Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear, 2017. URL https://spacy.io. Tianze Hua, Tian Yun, and Ellie Pavlick. mOthello: When do cross-lingual representation alignment and cross-lingual transfer emerge in multilingual models? In Findings of the Association for Computational Linguistics: NAACL 2024, 2024. URL https://aclanthology.org/2024. findings-naacl.103. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In Proceedings of Machine Learning Research (ICML), 2024. URL https: //proceedings.mlr.press/v235/huh24a.html. Gabriel Ilharco, Rowan Zellers, Ali Farhadi, and Hannaneh Hajishirzi. Probing contextual language models for common ground with visual representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2021. URL https://aclanthology.org/2021.naacl-main.422. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. R.S. Jackendoff. Semantic Interpretation in Generative Grammar. Studies in linguistics series. MIT Press, 1974. ISBN 9780262600071. Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, et al. Exploring concept depth: How large language models acquire knowledge at different layers? arXiv preprint, 2024. URL https://arxiv.org/ abs/2404.07066. Phillip Keung, Yichao Lu, György Szarvas, and Noah A. Smith. The multilingual Amazon reviews corpus. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https://aclanthology.org/2020.emnlp-main.369. Najoung Kim and Tal Linzen. COGS: compositional generalization challenge based on semantic interpretation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. URL https://aclanthology.org/2020.emnlp-main.731. Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for In Proceedings of the International Conference on Machine multimodal inputs and outputs. Learning (ICLR), 2023. URL https://arxiv.org/abs/2301.13823. Donald Laycock. Observations on Number Systems and Semantics. Pacific Linguistics, 1975. Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural language models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL-IJCNLP), 2021. URL https://aclanthology.org/2021.acl-long.143. Jiaang Li, Yova Kementchedjhieva, and Anders Søgaard. Implications of the convergence of language and vision model geometries. arXiv preprint, 2023. URL https://arxiv.org/abs/2302.06555. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), 2014. URL https://www.microsoft.com/ en-us/research/publication/microsoft-coco-common-objects-in-context/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://arxiv.org/abs/2304. 08485. The Llama-3-Team. The llama 3 herd of models. arXiv Preprint, 2024. URL https://arxiv.org/ abs/2407.21783."
        },
        {
            "title": "Under review",
            "content": "Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. In ProceedUNIFIED-IO: unified model for vision, language, and multi-modal tasks. ings of the International Conference on Learning Representations (ICLR), 2023. URL https: //openreview.net/forum?id=E01k9048soZ. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. URL https://arxiv.org/abs/2312.17172. Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 2021. Grace Luo, Trevor Darrell, and Amir Bar. Task vectors are cross-modal, 2024. URL https: //arxiv.org/abs/2410.22330. Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, and Noel E. OConnor. Do vision and language encoders represent the world similarly? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. URL https://arxiv.org/abs/2401. 05224. MDBG. Mdbg chinese-english dictionary (cc-cedict). MBDG, 2024. URL https://www.mdbg.net/ chinese/dictionary?page=cc-cedict. Downloaded: 2024-09-25. Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. arXiv preprint, 2022. URL https://arxiv.org/abs/2209.15162. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple Word2Vecstyle vector arithmetic. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2024. URL https://aclanthology.org/2024.naacl-long.281. Tomas Mikolov, Quoc Le, and Ilya Sutskever. Exploiting similarities among languages for machine translation. arXiv preprint, 2013. URL https://arxiv.org/abs/1309.4168. John Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander Rush. Text embeddings reveal (almost) as much as text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. URL https://aclanthology.org/2023.emnlp-main. 765. Jerry Ngo and Yoon Kim. What do language models hear? probing for auditory representations in language models. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2024. URL https://arxiv.org/abs/2402.16998. nostalgebraist. Interpreting gpt: the logit lens. LessWrong, 2020. URL https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens. Terence Parsons. Events in the Semantics of English: Study in Subatomic Semantics. MIT Press, 1990. Karalyn Patterson and Matthew Lambon Ralph. The hub-and-spoke hypothesis of semantic memory. In Neurobiology of language, pp. 765775. Elsevier, 2016. Karalyn Patterson, Peter Nestor, and Timothy Rogers. Where do you know what you know? the representation of semantic knowledge in the human brain. Nature reviews neuroscience, 8(12): 976987, 2007. Matthew Lambon Ralph, Elizabeth Jefferies, Karalyn Patterson, and Timothy Rogers. The neural and computational bases of semantic cognition. Nature reviews neuroscience, 18(1):4255, 2017. Abhilasha Ravichander, Yonatan Belinkov, and Eduard Hovy. Probing the probing paradigm: Does probing accuracy entail task relevance? In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2021. URL https://aclanthology. org/2021.eacl-main.295."
        },
        {
            "title": "Under review",
            "content": "Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner. Steering llama 2 via contrastive activation addition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024. URL https://aclanthology.org/ 2024.acl-long.828. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint, 2020. URL https://arxiv.org/abs/ 1910.01108. Sunny Sanyal, Sujay Sanghavi, and Alexandros G. Dimakis. Pre-training small base lms with fewer tokens. arXiv preprint, 2024. URL https://arxiv.org/abs/2404.08634. Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019. URL https://aclanthology.org/N19-1162. Samuel L. Smith, David H. P. Turban, Steven Hamblin, and Nils Y. Hammerla. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In Proceedings of the International Conference on Learning Representations (ICLR), 2017. URL https://openreview.net/forum? id=r1Aab85gg. Nishant Subramani, Nivedita Suresh, and Matthew Peters. Extracting latent steering vectors from pretrained language models. In Findings of the Association for Computational Linguistics: ACL, 2022. URL https://aclanthology.org/2022.findings-acl.48. Junyi Sun. Jieba: Chinese text segmentation tool. Github, 2024. URL https://github.com/fxsjy/ jieba. Accessed: 2024-09-25. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2024a. URL https://openreview.net/forum?id=14rn7HpKVk. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. Language-specific neurons: The key to multilingual capabilities in large language models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024b. URL https://aclanthology.org/2024.acl-long.309. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/ index.html. Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019. URL https://aclanthology.org/P19-1452. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. Function vectors in large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=AwyxtyMwaG. Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, and Yonatan Belinkov. Diffusion lens: Interpreting text encoders in text-to-image pipelines. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024. URL https://aclanthology.org/2024. acl-long.524. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint, 2023. URL https://arxiv.org/abs/2307.09288."
        },
        {
            "title": "Under review",
            "content": "Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint, 2024. URL https://arxiv.org/abs/2308.10248. and Stuart Shieber. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Investigating gender bias in language models using Singer, Information Processing Systems causal mediation analysis. (NeurIPS), 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 92650b2e92217715fe312e6fa7b90d82-Paper.pdf."
        },
        {
            "title": "In Advances in Neural",
            "content": "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: technical report. arXiv preprint, 2024. URL https: //arxiv.org/abs/2402.05672. Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. Do llamas work in English? on the latent language of multilingual transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024. URL https://aclanthology.org/2024. acl-long.820. Wikimedia-Foundation. Wikimedia downloads, 2023. URL https://dumps.wikimedia.org. Zhaofeng Wu, Hao Peng, and Noah A. Smith. Infusing Finetuning with Semantic Dependencies. Transactions of the Association for Computational Linguistics (TACL), 2021. URL https://doi. org/10.1162/tacl_a_00363. Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, and Christopher Potts. pyvene: library for understanding and improving pytorch models via interventions, 2024. URL https://arxiv.org/abs/2403.07809. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2021. URL https://aclanthology. org/2021.naacl-main.41. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. arXiv Preprint, 2023. URL https://arxiv.org/abs/2309.10305. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language In Proceedings of the Annual Meeting of the models latently perform multi-hop reasoning? Association for Computational Linguistics (ACL), 2024. URL https://aclanthology.org/2024. acl-long.550. Hongchuan Zeng, Senyu Han, Lu Chen, and Kai Yu. Converging to lingua franca: Evolution of linguistic regions and semantics alignment in multilingual large language models, 2024. URL https://arxiv.org/abs/2410.11718. Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. How do large language models handle multilingualism? arXiv preprint, 2024. URL https://arxiv.org/abs/ 2402.18815."
        },
        {
            "title": "Under review",
            "content": "A EXPERIMENTAL DETAILS FOR 3 We first note tokenization detail regarding prefix spaces. For example, _llama (where _ denotes whitespace) is token in the Llama-3 vocabulary, but not llama. When comparing token-level logit lens probabilities, we use the configuration that maximizes the probability, which may be different for each setting. For example, the token with the prefix space usually is more likely, but for arithmetic expressions a=b+c, after +, the surface token is more likely than _c. A.1 MULTILINGUAL For Experiment 1, for each sentence pair, we use template to transform each sentence. This is due to the automatic code-switching behavior of LMs. For an English model processing Chinese text, we expect the Chinese tokens to have high probabilities in the final layer because they need to be output; however, we observe these models tend to code-switch back to their dominant language after full sentence, which confounds our analysis. We therefore put the parallel sentences into template, {English Sentence} This represents (and the corresponding Chinese version), as the model is less likely to code-switch mid-sentence after represents. We experimented with other templates that led to similar results. Furthermore, for sentence in GALE(Chen et al., 2016), we make sure the transcript;unicode is not empty for both the source and the translation. For Experiment 2, due to tokenization, it is challenging to obtain exactly parallel English-Chinese tokens, and hence we perform aggressive filtering. We consider only text positions where the next BPE token (1) is valid Chinese word (as segmented by Jieba (Sun, 2024)), and (2) has an English translation (using the English-Chinese dictionary CC-CEDICT (MDBG, 2024)). E.g., 今天是开 心的一天 (Today is happy day), Llama-3 tokenizes it as [今天, 是, 开, 心, 的一, 天], while Jieba segments it as [今天, 是, 开心, 的, 一天]. We keep {今天, 是}. Furthermore, only 今天s translation is single token, the only token that survives the cutoff is 今天. A.2 CODE For all experiments using the MBPP dataset, we concatenate all dataset splits with total of 974 programs. For the function call argument experiment, we consider all non-zero-argument function calls in MBPP, excluding unit tests. We automatically identify the argument names (the semantic roles) by function inspection for built-in functions and by looking at the function definition for those defined in-context, and skip when this is not possible. We also ignore arguments whose semantic roles are generically called obj or object, and instances where the instantiated surface-form argument is the same as the semantic role. We look the hidden state corresponding to the previous token, either ( or ,, except when tokenization renders this impossible (e.g., when the previous token is merged with part of the surface argument; we find this happens often with the Llama-3 tokenizer and thus do not include it in this experiment). This leaves 540 arguments. A.3 VISION-LANGUAGE To pass the images through the model, we embed them in templates, only for the logit lens experiments. For the color experiment, we use the template USER: What is the color in the image?<image>n ASSISTANT:. For the caption and segmentation experiments, we use USER: What is in the image?n<image> ASSISTANT: for LlaVa and What is in the image?n<image> for Chameleon. For all caption and segmentation experiments, we use the MSCOCO 2017 dataset. In particular, for the segmentation evaluation, we use the MSCOCO 2017 panoptic segmentation labels. We evaluate the alignment score between all corresponding patches and labels. We consider patch and label as corresponding if there is an image segment with that label that occupies more than half of the pixels in the patch. EXPERIMENTAL DETAILS FOR 4 For both the code and vision-language intervention experiments, we use argmax decoding. We perform most of our intervention experiments using the pyvene library (Wu et al., 2024)."
        },
        {
            "title": "Under review",
            "content": "Table 2: Steering Llama-2s output sentiments using trigger words in English vs. the input language (either Spanish or Chinese). We report the mean sentiment, disfluency (perplexity), and relevance of the continuation, as well as the standard deviation across 10 seeds. Cross-lingual steering is consistently successful, sometimes even more than monolingual steering, without substantial damage in text fluency and relevance. Text Lang. Steering Dir. Steering Lang. Sentiment Disfluency () Relevance () Spanish Chinese None None None Spanish English Spanish English None Chinese English Chinese English 0.1440.014 8.580.57 0.8500. 0.1430.012 0.0970.024 0.1640.018 0.1490.015 8.840.79 8.990.72 9.110.50 8.350.30 0.8470.006 0.8470.005 0.8440.005 0.8490. 0.2230.036 14.632.65 0.8440.009 0.1170.080 0.1560.076 15.292.47 14.802.24 0.3590.077 0.2270. 545.941544.36 14.142.42 0.8400.011 0.8420.008 0.8390.010 0.8450.009 B.1 MULTILINGUAL For each language, we sample = 1000 instances from the training set of InterTASS for Spanish and the multilingual Amazon reviews corpus for Chinese. Following Turner et al. (2024), we use trained models for various metrics. We found that Llama models tend to code-switch back to English when processing texts in other languages, and discovered that we can mitigate this with an instruction: 接下来的文字全部是中文的 for Chinese and Todo el texto siguiente está en español. for Spanish (trans. All of the following text is in Chinese/Spanish.). We automatically evaluate the sentiment of the generation using DistillBERT-based (Sanh et al., 2020) model finetuned for multilingual sentiment analysis,,12 judge the generation fluency by taking the conditional perplexity of the generation given the prefix from Llama-3.1-70B (Llama-3-Team, 2024),13 and compute the relevance of the generation with the prefix by computing the cosine similarity between the generation and the prefix using XLM-R-Large-based (Conneau et al., 2020) model finetuned for sentence representation14 (Wang et al., 2024). All these models support both Spanish and Chinese. We perform ActAdd by passing both the positive and negative steering words through the LM, taking their hidden states at layer 17, computing their difference, scaling it by constant, and adding it to the normal generation forward pass also at layer 17, exactly following Turner et al. (2024), except we use scaling coefficient of 5, rather than 2 in their experiments, for which we observed larger effect. For generation, we use temperature of 1, top-p=0.3, and frequency penalty of 1, all following Turner et al. (2024), without tuning. We showed the Llama-3 intervention results in Table 1, and here in Table 2 we show the results on Llama2, with similar trends. TOKEN-LEVEL MULTIMODAL EXPERIMENTS USING THE LOGIT LENS As mentioned in 2.1, we do not perform logit lens analysis for the multimodal experiments. But we can perform logit-lens-style test for Eq. 1 that test representation equivalence not on sequence level but on token level. At each individual token (e.g., image patches), we measure the alignment of it representation and natural language description token of it, using the logit lens. 12https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student 13We Mistral-Nemo-Base (https://huggingface.co/mistralai/ using tried also Mistral-Nemo-Base-2407), and found similar trends. 14https://huggingface.co/intfloat/multilingual-e5-large"
        },
        {
            "title": "Under review",
            "content": "(a) Caption, LLaVa (b) Caption, Chameleon (c) Segment., LLaVa (d) Segment., Chameleon Figure 13: When processing an image patch, model logit lens probabilities of either the nouns in the corresponding caption or the patch segmentation label, as well as baseline for each with no correspondence between the patch and the label. The image representations better match the semantically corresponding English words. C.1 VISUAL INPUT. We consider the alignment of image tokens (corresponding to patches in the original image) to their description words. First, we use the nouns15 in the image caption as coarse descriptionalthough they do not precisely describe all patches, they should better align with the patches on average than nouns irrelevant to the image. We consider the same 1000 image captions as in 3.5. For each patch, we compute patch-caption alignment score by summing over the logit lens probabilities for all the nouns in the caption. We average this alignment score over all patches in all images, separately for each transformer layer. For the irrelevant nouns baseline, for each image, we compute the alignment score with an unrelated caption that has the smallest noun overlap with the groundtruth matching caption (we also normalize the number of nouns so that the score is comparable). Figures 13a and 13b show that the matching caption better aligns with the image patch representations than an unmatched caption for both LlaVa and Chameleon. Image segmentation labels, which annotate objects in specific image locations, provide finer-grained patch description. The setup is similar to captions, but the alignment score is not computed using the correspondence between each patch and each noun in the caption, but each patch with the corresponding object label. We say label correspond to the patch if more than half of the patchs pixels have that label. For the irrelevant token baseline, we compute the alignment by aligning each patch with different randomly chosen object category from all categories. Figures 13c and 13d show that, for LLaVa, the patches are much better aligned to the corresponding labels than randomly assigned labels (which have near-0 logit lens probability). For Chameleon, this is the case for only one middle layer, and not in statistically significant way, though, as we showed in 4, Chameleons latent space can be reliably steered using English tokens. C.2 AUDIO Unlike for vision-language models where we can map individual image patches to model input token positions, such correspondence does not exist in SALMONN. This limits us to positionagnostic evaluations like the captioning study, preventing finegrained analysis such as using segmentation labels. Similar to the captioning experimental design, we measure the average logit lens probabilities of the words in the label, and consider random label in the dataset with no word overlap as the baseline. On the same 1000 samples, Figure 14 shows familiar trend, where the audio hidden states are closer to semantically corresponding label words. We note that this is lower boundmany words in some labels, such as the prepositions in the label writing on blackboard with chalk, are unlikely to be represented in the audio hidden states. Figure 14: When SALMONN processes an audio clip, the logit lens probabilities of the English words in the audio label vs. another random label. The audio representations better match the semantically corresponding label in English. 15Words with NOUN or PROPN tags given by SpaCys en_core_web_trf model (Honnibal & Montani, 2017)."
        }
    ],
    "affiliations": [
        "MIT",
        "University of Southern California",
        "Allen Institute for AI"
    ]
}