{
    "paper_title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
    "authors": [
        "Henrique Schechter Vera",
        "Sahil Dua",
        "Biao Zhang",
        "Daniel Salz",
        "Ryan Mullins",
        "Sindhu Raghuram Panyam",
        "Sara Smoot",
        "Iftekhar Naim",
        "Joe Zou",
        "Feiyang Chen",
        "Daniel Cer",
        "Alice Lisak",
        "Min Choi",
        "Lucas Gonzalez",
        "Omar Sanseviero",
        "Glenn Cameron",
        "Ian Ballantyne",
        "Kat Black",
        "Kaifeng Chen",
        "Weiyi Wang",
        "Zhe Li",
        "Gus Martins",
        "Jinhyuk Lee",
        "Mark Sherwood",
        "Juyeong Ji",
        "Renjie Wu",
        "Jingxiao Zheng",
        "Jyotinder Singh",
        "Abheesht Sharma",
        "Divya Sreepat",
        "Aashi Jain",
        "Adham Elarabawy",
        "AJ Co",
        "Andreas Doumanoglou",
        "Babak Samari",
        "Ben Hora",
        "Brian Potetz",
        "Dahun Kim",
        "Enrique Alfonseca",
        "Fedor Moiseev",
        "Feng Han",
        "Frank Palma Gomez",
        "Gustavo Hernández Ábrego",
        "Hesen Zhang",
        "Hui Hui",
        "Jay Han",
        "Karan Gill",
        "Ke Chen",
        "Koert Chen",
        "Madhuri Shanbhogue",
        "Michael Boratko",
        "Paul Suganthan",
        "Sai Meher Karthik Duddu",
        "Sandeep Mariserla",
        "Setareh Ariafar",
        "Shanfeng Zhang",
        "Shijie Zhang",
        "Simon Baumgartner",
        "Sonam Goenka",
        "Steve Qiu",
        "Tanmaya Dabral",
        "Trevor Walker",
        "Vikram Rao",
        "Waleed Khawaja",
        "Wenlei Zhou",
        "Xiaoqi Ren",
        "Ye Xia",
        "Yichang Chen",
        "Yi-Ting Chen",
        "Zhe Dong",
        "Zhongli Ding",
        "Francesco Visin",
        "Gaël Liu",
        "Jiageng Zhang",
        "Kathleen Kenealy",
        "Michelle Casbon",
        "Ravin Kumar",
        "Thomas Mesnard",
        "Zach Gleicher",
        "Cormac Brick",
        "Olivier Lacombe",
        "Adam Roberts",
        "Yunhsuan Sung",
        "Raphael Hoffmann",
        "Tris Warkentin",
        "Armand Joulin",
        "Tom Duerig",
        "Mojtaba Seyedhosseini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 5 3 0 2 . 9 0 5 2 : r EmbeddingGemma: Powerful and Lightweight Text Representations Henrique Schechter Vera*, Sahil Dua*, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divya Sreepat, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca, Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hernández Ábrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram Rao, Waleed Khawaja, Wenlei Zhou, Xiaoqi Ren, Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong, Zhongli Ding, Francesco Visin, Gaël Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon, Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts, Yunhsuan Sung, Raphael Hoffmann, Tris Warkentin, Armand Joulin, Tom Duerig and Mojtaba Seyedhosseini EmbeddingGemma Team, Google We introduce EmbeddingGemma, new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves stateof-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research. 1. Introduction Text embedding models map natural language to fixed-length vector representations, placing semantically similar text near each other in the embedding space. This has made them popular choice for tasks like semantic similarity, information retrieval, and clustering. Recent research has built towards the goal of creating general-purpose models that are able to excel at many of these tasks simultaneously, attempting to develop the models themselves (Su et al., 2023) and crafting benchmarks focused on task and domain generalization (Muennighoff et al., 2022). As large language models (LLMs) have grown larger and more powerful, they have become integral in the ever-improving development of these general-purpose embedding models via techniques such as synthetic data generation and hard negative mining (Lee et al., 2024; Wang et al., 2024b), and by initializing embedding models with LLM weights (Ni et al., 2021). As result, much of the recent literature has centered around large models on the order of several billion parameters, such as NV-Embed (Lee et al., 2025a), GritLM-7B (Muennighoff et al., 2025), and E5-Mistral (Wang et al., 2024b). 1See Contributions and Acknowledgments section. Co-first authors; equal contributions. Corresponding author: hschechter@google.com 2025 Google Deepmind. All rights reserved EmbeddingGemma: Powerful and Lightweight Text Representations Figure 1 Comparison of top 20 embedding models under 500M parameters across MTEB multilingual and code benchmarks. We exclude models trained on more than 25% of the MTEB data to mitigate potential over-fitting. : Average excludes COIRCodeSearchNetRetrieval, which was unavailable for most models. An analogous figure for MTEB(English, v2) is available in Section A. However, while the trend towards larger models has pushed the boundaries of performance, it has also highlighted critical trade-off between capability and computational cost. State-of-the-art models are often too large and computationally expensive for many real-world applications, which often require low-latency, high-throughput inference. This is especially true for applications requiring on-device deployment, such as those involving sensitive data or offline access. This has led to growing interest in developing lightweight models that can deliver strong performance without the need for extensive computational resources (Belcak et al., 2025), presenting significant opportunity to further advance the capabilities of these smaller models. In this paper, we introduce EmbeddingGemma,2 novel, 308M parameter embedding model built on the architecture of the powerful Gemma 3 language model family (Team, 2025). In order to leverage richer encoder representations, we first adapt Gemma 3 into an encoder-decoder model using the UL2 (Tay et al., 2023) objective, following T5Gemma (Zhang et al., 2025a). We then initialize EmbeddingGemma from the encoder of this encoder-decoder model. During training, we distill the state-of-the-art Gemini Embedding (Lee et al., 2025b) using embedding matching (Kim et al., 2023). We also include spread-out regularizer to improve EmbeddingGemmas expressiveness and robustness. Building upon the success of Gecko and models that followed it (Lee et al., 2024, 2025b; Zhang et al., 2025b), we include task prompts and pre-finetuning stage to improve performance. Finally, we utilize model souping (Wortsman et al., 2022) to combine multiple finetuned checkpoints, opting to combine models trained on different finetuning mixtures rather than different hyperparameter configurations. This results in final embedding model with stronger, more generalizable representations. To rigorously assess EmbeddingGemmas capabilities, we conduct extensive evaluations on multiple benchmarks. We mostly utilize the Massive Text Embedding Benchmark (MTEB) test suite (Enevoldsen et al., 2025; Muennighoff et al., 2022); notably, its multilingual benchmark MTEB(Multilingual, v2) includes over 100 tasks spanning 250+ languages, 20 domains, and 9 task types such as classification, clustering, retrieval, and semantic similarity. As visualized in Figure 1, EmbeddingGemma achieves 2Our model is available at https://ai.google.dev/gemma/docs/embeddinggemma. EmbeddingGemma: Powerful and Lightweight Text Representations state-of-the-art performance on MTEB(Multilingual, v2), MTEB(Code), and MTEB(English, v2) leaderboards for models under 500M parameters, ranking first across all aggregate metrics:3 Borda count, mean over all task scores, and mean over all task type scores. Moreover, EmbeddingGemma represents drastic improvement over the previous state-of-the-art: on MTEB(Multilingual, v2), for example, EmbeddingGemma ranks 8th overall, 17 places above the second-best sub-500M parameter model. EmbeddingGemmas performance is so strong it is comparable to models nearly double its size  (Table 5)  . Impressively, EmbeddingGemmas lead persists even when truncating embeddings to as low as 128 dimensions or quantizing weights down to as low as 4-bit precision. We also conduct detailed ablation studies to elucidate the factors contributing to the stellar performance of EmbeddingGemma. We find that though initializing model parameters from decoderonly LLMs indeed greatly improves performance (Lee et al., 2025b; Zhang et al., 2025b), encoderdecoder models offer stronger starting point. This is thanks to the encoder-decoder architectures stronger contextual representations, which likely come from (i) the use of bidirectional attention and (ii) encoder parameters being able to specialize in input understanding (Tay et al., 2023; Zhang et al., 2025a). Counterintuitively, our experiments also show that simple pooling types tend to outperform attention pooling in embedding tasks despite using no learnable parameters. Our analysis also reveals that model souping works by varying finetuning mixtures instead of hyperparameters, perhaps in part due to our mixtures yielding models specialized in different areas. We also compare per-channel and per-block quantization-aware training and confirm they yield models robust to quantization. 2. EmbeddingGemma 2.1. Architecture EmbeddingGemma is an encoder-only transformer model adapted from pretrained 300M decoderonly Gemma 3 model. Specifically, we adapt the Gemma 3 model into an encoder-decoder model following the T5Gemma recipe (Zhang et al., 2025a), and then initialize EmbeddingGemma from the encoder of this encoder-decoder model. This way, EmbeddingGemma is able to harness the extensive world knowledge of Gemma 3. Using an encoder-decoder adaptation further ensures the encoder is able to produce expressive representations from the start. Specifically, given an input sequence of 𝐿 tokens, we first apply M𝑛, an 𝑛-layer transformer with bidirectional attention, and produce sequence of token embeddings Tembed = M𝑛(T) ℝ𝐿𝑑𝑀 , where 𝑑𝑀 is the model dimension used for the transformers inner representations. We then employ mean pooling P, which averages the token embeddings along the sequence axis to generate single embedding representing all the information in the input, producing Pembed = (Tembed) ℝ𝑑𝑀 . Next, we apply randomly initialized linear projection 𝑔 to upscale the embedding to an intermediate embedding dimension 𝑑𝑈, producing E𝑈 = 𝑔(Pembed) ℝ𝑑𝑈 . Finally, we apply another randomly initialized linear projection 𝑓 to scale the embedding to the target dimension 𝑑, resulting in = 𝑓 (E𝑈) ℝ𝑑. In EmbeddingGemma, we use 𝑛 = 24, 𝑑𝑀 = 768, 𝑑𝑈 = 3072, 𝑑 = 768. 2.2. Training Input Each training example includes query 𝑞𝑖, positive passage 𝑝+ 𝑖 , and (optionally) hard negative passage 𝑝 𝑖 . Each example also has prescribed task strings 𝑡𝑞 and 𝑡 𝑝 for queries and passages respectively, describing the nature of the task. For instance, for retrieval we use task: search result query: {content} and title: {title none} text: {content}. 3https://huggingface.co/spaces/mteb/leaderboard; September 23rd, 2025. 3 EmbeddingGemma: Powerful and Lightweight Text Representations The query and passages are embedded as vectors in ℝ𝑑: q𝑖 = 𝑓 (𝑔(P (M𝑛(𝑡𝑞 𝑞𝑖)))), 𝑖 = 𝑓 (𝑔(P (M𝑛(𝑡 𝑝 𝑝 𝑖 )))). (1) Objective EmbeddingGemma was trained using three different losses. The first is noise-contrastive estimation (NCE) loss with in-batch negatives. Given batch of size 𝐵 := B, we define the contrastive loss as: = 1 𝐵 𝐵 𝑖=1 log 𝑒sim(q𝑖,p+ 𝑖 )/𝜏 𝑤𝑖 𝑒sim(q𝑖,p 𝑖 )/𝜏 + (cid:205)𝐵 𝑗=1 𝟙TN(𝑖, 𝑗) 𝑒sim(q𝑖,p+ 𝑗 )/𝜏 (2) where sim(x, y) = xy/xy is cosine similarity, and 𝟙TN masks out false negatives from duplicates: 𝟙TN(𝑖, 𝑗) = (cid:40) 0 if 𝑞𝑖 = 𝑞 𝑗 or 𝑝+ 1 otherwise. 𝑖 = 𝑝+ 𝑗 , (3) The hardness weight 𝑤𝑖 represents how challenging (query, hard negative passage) pair is for the model to differentiate between, forcing it to learn more discriminative representations (Lan et al., 2025). This is defined as 𝑤𝑖 = exp(𝛼 sg(sim(q𝑖, 𝑖 ))), where sg() is the stop-gradient operator, and 𝛼 is hyperparameter that controls the strength of the weighting, experimentally set to 5.0. The stop-gradient ensures we are weighing based on the current difficulty and not differentiating through the weight factor itself. The second loss is based on the global orthogonal regularizer (GOR) introduced in Zhang et al. (2017). This loss encourages EmbeddingGemma to produce embeddings that are spread out over the embedding space, to fully utilize the expressive power of the embedding space. This also intends to ensure that a) the model is robust to quantization (especially embedding quantization), and that b) the embeddings produced by the model can be retrieved efficiently in vector databases using approximate nearest neighbor (ANN) algorithms. We define the spread-out loss as: LS = 1 𝐵 (𝐵 1) (q 𝑖 𝑗)2 + 𝑖, 𝑗 : 𝑖 𝑗 1 𝐵 (𝐵 1) 𝑖, 𝑗 : 𝑖 𝑗 (p+ 𝑖 p+ 𝑗 )2. (4) The idea is to make the embeddings of random pair of inputs have similar statistical properties (mean and second moment) as two points independently and uniformly sampled from the unit sphere. We use only the second moment term of the regularizers original definition, as we find this also sufficiently pushes the mean term towards its target value. The third and final is an embedding matching loss based on Kim et al. (2023). In contrast to previous distillation research which has relied only on the teachers query-document relevance scores as signal (de Souza P. Moreira et al., 2025; Santhanam et al., 2022), this loss directly aligns EmbeddingGemmas embedding space with that of teacher model, allowing it to more effectively learn from the larger, more powerful Gemini Embedding model. Note that unlike Kim et al. (2023), we apply embedding matching not only to queries and passages but also to hard negative passages, as we found this substantially improves performance. Intuitively, this serves the same purpose as using hard negative passages in NCE loss, as the model learns how the teacher discriminates between the queries and its corresponding hard negatives. We weigh these individual losses uniformly, letting = LQ + LP+ + LP . (5) 4 EmbeddingGemma: Powerful and Lightweight Text Representations We adapt the contrastive and spread-out losses using MRL (Kusupati et al., 2022), which splits each loss into 𝑘 separate losses applied to 𝑘 overlapping sub-dimensions of the embedding. We consider each of these individual losses equally, simply adding their values during training without any special weights. EmbeddingGemma provides 𝑑 = 768 dimensional embeddings, additionally supporting 512, 256, and 128 dimensional embeddings via MRL. 2.3. Recipe We train EmbeddingGemma on mixture of embedding tasks spanning various languages and task types. In total, including encoder-decoder training, EmbeddingGemma sees approximately 2.1T tokens, of which 314B are seen during pre-finetuning and 20B during finetuning. Encoder-Decoder Training We begin by adapting the decoder-only Gemma 3 model to an encoderdecoder model to get strong encoder with improved contextual representations. Following T5Gemma (Zhang et al., 2025a), we initialize the encoder-decoder with the Gemma 3 decoder-only checkpoint and further pretrain it on the Gemma 3 pretraining data with UL2 (Tay et al., 2023). We utilize the pretrained encoder as the backbone for our embedding model, providing EmbeddingGemma with strong initialization from which it inherits extensive world knowledge, such as the 100+ languages used to train Gemma 3. Pre-finetuning We then train the embedding model itself, starting with pre-finetuning stage. Here we train on large-scale unsupervised data, as has been previously shown to improve performance (Gao et al., 2022; Izacard et al., 2021). We use only (query, target) pairs as input, as the training mixture is large and noisy, so mining high-quality hard negatives is challenging. We opt for larger batch size in order to stabilize the gradient, and because this improves performance by effectively providing more (in-batch) negatives for each input example. The goal of this stage is to build the models generalization capabilities by training for large number of steps over diverse training set. As such, our pre-finetuning mixture spans both various, evenly weighted task typesquestion answering, sentence similarity, code retrieval, and web search tasksand various languages (natural and programming). As part of this, we leverage corpus containing billions of title and body text pairs crawled from websites, much like in previous work (Lee et al., 2024; Neelakantan et al., 2022; Wang et al., 2024a). Finetuning Next, we finetune the model using smaller but higher-quality mixture of task-specific datasets. Here, we opt for smaller batch size, and do utilize hard negatives. Note that both here and in pre-finetuning, each batch only contains examples from given dataset, as in-batch negatives are more difficult to discern, and thus more valuable for contrastive learning, coming from the same task. As in Gemini Embedding (Lee et al., 2025b), we utilize set of three different groups of tasks aimed at task diversity, language diversity, and coding capability, respectively. This includes subset of the academic datasets used by Gecko (Lee et al., 2024) and the synthetic datasets used by Gemini Embedding. However, instead of directly determining the task mixture rates based on fine-grained grid search, we use the mixture resulting from this procedure as seed for Bayesian optimization, alongside 10 other random mixtures sampled from Dirichlet distribution. This yields multiple mixtures which individually result in stronger performance than the seeds. Moreover, thanks to balance of explore-exploit objectives, these mixtures specialize in different domains, creating experts which synergize during souping. 5 EmbeddingGemma: Powerful and Lightweight Text Representations MTEB(Multi, v2) MTEB(Eng, v2) MTEB(Code) Precision Mean(Task) Mean(Type) Mean(Task) Mean(Type) Mean(Task) bf16 int8 Mixed int4 61.15 60.93 60.69 60. 54.31 53.95 53.82 53.61 69.67 69.49 69.32 69.31 65.11 64.84 64.82 64.65 68.76 68.70 68.03 67.99 Table 1 Performance of raw and quantized EmbeddingGemma checkpoints on MTEB benchmarks. : Per-block quantization. : Per-channel quantization with int4 for embedding, feedforward, and projection layers, and int8 for attention Initialization Mean (Task) Mean (Type) Bitext Mining Encoder-Decoder 60.4 53. Decoder-only Random 59.7 45.2 52.6 39.2 63.5 63.2 26.8 Class. Clus. 60.4 50.3 60.1 48.8 50.5 41.5 Inst. Retrieval Multi. Class. 4.3 0.8 -0.2 24.6 24.2 16.4 Pair Class. 81. 80.4 72.5 Rerank. Retrieval STS 63.1 61.6 49.1 60. 58.3 35.5 74.5 73.9 62.1 Table 2 Results using different initialization strategies. Model Souping Finally, we combine models from our finetuning runs by averaging their parameters, to improve our final models quality and robustness. We assessed various methods including merging checkpoints from various steps of single training run (Izmailov et al., 2018) and from multiple training runs using different hyperparameters (Wortsman et al., 2022). We find that mixtures obtained from Bayesian optimization work well together as souping ingredients: our final model is the result of an unweighted average of checkpoints from finetuning runs with each of the mixtures obtained with Bayesian optimization. Quantization-Aware Training We additionally provide quantized versions of EmbeddingGemma in standard quantization configurations. We provide checkpoints with three types of weight representations: int4 per-block, int8 per-block, and mixed-precision per-channel. We obtain these variants by applying quantization-aware training (Jacob et al., 2018) during the model finetuning stage, to minimize quality degradation for quantized checkpoints. Table 1 shows the quality is comparable between our quantized checkpoints and our raw checkpoint (evaluated at half-precision bf16 weights). 3. Ablation Studies We analyze some key design choices to better understand how they contributed to EmbeddingGemmas ability to produce high-quality embeddings across tasks and languages despite its small size. Models for ablations are finetuned on only one mixture and thus exclude model souping, unless indicated otherwise. We evaluate the models on MTEB(Multilingual, v2), as it covers various languages, domains, and task types. 3.1. Initialization Strategy Table 2 compares model performance when initialized with decoder-only Gemma 3 model and with an encoder-decoder based on that Gemma 3 model, trained as described in Section 2.3. Encoderdecoder initialization outperforms decoder-only initialization across various task types. This suggests the encoder-decoder is able to produce more expressive representations, as put forward in the literature. As baseline, we include the performance of model initialized with random weights, demonstrating both initialization strategies dramatically improve model performance. 6 EmbeddingGemma: Powerful and Lightweight Text Representations Pooling Mean Mean (Task) Mean (Type) Bitext Mining 60. 53.6 Last Token First Token Attention 59.7 59.9 60.2 52.6 52.9 53.1 Class. Clus. 60. 50.3 60.3 60.3 60.1 49.9 49.7 49.6 Inst. Retrieval Multi. Class. Pair Class. Rerank. Retrieval STS 4.3 1.9 2.2 1.5 24.6 23.1 23.8 23. 81.1 80.8 81.4 80.9 63.1 62.4 62.6 62.7 60.2 57.8 58.6 61. 74.5 73.8 73.8 74.1 63.5 63.1 63.5 63.5 Table 3 Results using different types of poolers. Pooling Mean (Task) Mean (Type) Bitext Mining Souped 61.2 54. Mix 1 Mix 2 Mix 3 60.4 60.4 60.1 53.4 53.6 53.3 64.4 63.5 63.5 63.9 Class. Clus. 60.9 51.2 60.4 60.4 60.4 50.6 50.3 50.5 Inst. Retrieval Multi. Class. Pair Class. Rerank. Retrieval STS 5.6 4.1 4.3 3.6 24. 23.1 24.6 24.3 81.4 81.1 81.1 81.4 63.3 63.0 63.1 62.8 62. 61.0 60.2 58.2 74.7 73.9 74.5 74.5 Table 4 Results using different training mixtures. Best result between the three mixtures is underlined. 3.2. Pooling Types Poolers aggregate the transformers token representations into single vector. In mean pooling, we compute the average of all the token representations. In first-token pooling, we simply take the representation of the first token and use it directly. In last-token pooling, we do the same, but with the last token of the sequence. In attention pooling, we utilize an attention mechanism to weigh and aggregate the token representations. Our pooled representations can be expressed as Pembed = softmax( 𝑄𝐾 )𝑉 ℝ1𝑑𝑀 , where the input is passed through the key and value matrices, 𝑑𝑀 𝐾, 𝑉 ℝ𝐿𝑑𝑀 , and the query matrix 𝑄 ℝ1𝑑𝑀 is learnable parameter. We specifically employ multi-head attention with four attention heads. The results in Table 3 indicate that mean pooling yields the best performance, despite attention pooling offering large amount of additional learnable parameters. This is consistent with the results from Suganthan et al. (2025), which found that simple pooling strategies outperformed attention pooling in encoder-only models for classification and regression tasks, suggesting that these results further extend to embedding tasks such as clustering. 3.3. Model Souping Here, we train models on different model soup ingredients and compare their performance to our final souped checkpoint: Table 4. The model soup not only improves on overall performance, but even outperforms the ingredients in each task type. This indicates that model souping works not only on runs with varied hyperparameter configurations, but also on runs with different finetuning mixtures altogether. Note also that each mixture yields an expert in different task types. 4. Evaluation We evaluate EmbeddingGemma on comprehensive set of benchmarks covering diverse task types, domains, languages, and language pairs. Specifically, we report numbers for the Massive Text Embedding Benchmark (Enevoldsen et al., 2025; Muennighoff et al., 2022), XOR-Retrieve (Asai et al., 2021), and XTREME-UP (Ruder et al., 2023). 7 EmbeddingGemma: Powerful and Lightweight Text Representations Open Models Embedding Gemma MultilingualBase Large Instruct mE Embedding Gecko GTE BGE-M3 Commercial API Models text-embeddingEmbed 0.6B Embeddingsv3 Multilingualv3 Embedding Embedding Qwen3 Gemini Cohere 3-large Jina Parameters 308M 278M 305M 560M 568M 572M 595M MTEB(Multi, v2) (Enevoldsen et al., 2025) Mean (Task) Mean (Type) 61.15 54.31 53.47 46.23 58.24 51.44 63.22 55.08 59.56 52. 58.37 50.66 64.34 56.01 64.40 60.90 51.17 5.61 43.57 55.99 43.83 0.42 - Bitext Mining - Classification - Clustering - Inst. Retrieval - Multilabel Class. 24.82 17.00 81.40 75.61 - Pair Class. 63.25 57.07 - Reranking 53.77 62.49 - Retrieval 68.83 74.73 - STS 71.79 80.13 79.11 72.23 65.25 66.83 58.77 60.35 64.94 57.17 52.33 45.65 40.88 50.75 44.33 5.09 -1.34 -3.11 -0.40 -0.74 24.59 18.38 20.10 22.91 19.82 80.83 79.27 80.76 80.86 80.49 61.41 57.09 62.79 62.61 60.72 64.65 54.60 57.12 56.50 55.76 74.12 77.13 76.17 76.81 72. MTEB(Eng, v2) (Enevoldsen et al., 2025) Mean (Task) Mean (Type) 66.38 69.67 65.11 62.33 65.53 61.21 70.70 64.88 68.37 59.59 79.28 71.82 54.59 5.18 29.16 83.63 65.58 67.71 79.40 73.30 67.67 61.12 58.93 53.23 51.41 70.50 62.17 62.95 60.27 46.89 46.89 -1.89 -2.68 22.74 22.03 79.88 79.17 64.07 63.89 59.16 59.27 74.80 71. 66.01 66.43 61.43 62.15 68.14 55.33 60.55 84.14 41. 47.72 7. 19.05 18.68 26.91 8.54 6.64 64. 58.03 74.57 75.54 57.26 65.73 90.42 68.76 18.80 MTEB(Code) (Enevoldsen et al., 2025) XOR-Retrieve (Asai et al., 2021) XTREME-UP (Ruder et al., 2023) Table 5 Comparison of popular embedding models on the Massive Text Embedding Benchmark: MTEB(Multilingual, v2), MTEB(English, v2), and MTEB(Code). We also show results on XOR-Retrieve and XTREME-UP, reporting Recall@5kt and MRR@10 respectively. : Averaged over all code tasks excluding COIRCodeSearchNetRetrieval, which was unavailable for most models. : For Gecko Embedding (Lee et al., 2024), we evaluate smaller version of the model discussed in the paper. 4.1. Tasks MTEB consists of massive collection of individual, quality-controlled evaluation tasks. Together, the subsets we consider cover 250+ natural languages and 14+ programming languages, 20 domains (e.g. legal, medical, news, programming, web), and 10 task types: bitext mining, classification, clustering, instruction retrieval, multilabel classification, pair classification, reranking, retrieval, semantic text similarity, and summarization. Our evaluations include 162 individual tasks, consisting of 131 tasks for MTEB(Multilingual, v2), 41 tasks for MTEB(English, v2), and 12 code retrieval tasks for MTEB(Code). XOR-Retrieve and XTREME-UP provide cross-lingual retrieval evaluations, with XOR-Retrieve pairing English passages with retrieval queries in 7 different languages and XTREME-UP pairing English passages with queries in 20 underrepresented Indo-European languages. 4.2. Methods We compare our model to the most popular general-purpose embedding models in two categories: open embedding models with fewer than billion parameters and models available through commercial APIs. For MTEB benchmarks, we additionally compare against the top models with fewer than 500M parameters in the respective MTEB leaderboards. We exclude models trained on more than 25% of the MTEB data to mitigate potential overfitting. 8 EmbeddingGemma: Powerful and Lightweight Text Representations Memory (MB) Parameters Rank Mean (Task) Mean (Type) Bitext Mining Class. Clus. Inst. Retrieval Multi. Class. Pair Class. Rerank. Retrieval STS 578 308M 8 Model Name EmbeddingGemma (768d) (512d) (256d) (128d) KaLM mini-v1 gte-multilingual-base bilingual-embedding-base KaLM mini-instruct-v1 bilingual-embedding-small multilingual-e5-base multilingual-e5-small snowflake-arctic-embed-m-v2.0 USER-bge-m3 Arabic-labse-Matryoshka granite-embedding-278m-multi. 1885 582 1061 1885 449 1061 449 1165 1370 1796 530 494M 305M 278M 494M 117M 278M 118M 305M 359M 470M 278M 25 26 28 29 34 36 39 41 42 44 45 61.2 60.7 59.7 58. 57.0 58.2 58.2 56.3 57.0 57.0 55.8 53.7 51.6 53.6 53.7 54.3 53.9 53.0 51.8 50.0 51.4 50.6 49.3 49.7 49.8 49.0 46.9 45.3 47.2 47.2 64.4 64.0 62.4 59.3 64.8 71.8 70.0 64.2 69.5 69.4 69.5 53.7 63.4 70.3 58.5 60.9 60.3 59.3 57. 57.6 57.2 59.4 57.4 58.1 58.2 56.2 54.4 53.2 55.1 54.1 51.2 50.9 50.5 50.6 45.6 44.3 44.7 45.9 43.8 41.8 40.8 42.2 40.0 39.6 41.4 5.6 4.8 4.4 3.8 -1.5 -0.7 -3.8 -2.6 -3.4 -2.7 -2.4 -3.3 -2.7 -1.8 -1.8 24.8 24.5 23.9 23. 20.7 19.8 20.7 21.6 19.2 20.2 19.0 17.0 18.6 19.9 17.7 81.4 81.5 81.5 81.3 77.7 80.5 77.4 77.5 77.1 77.2 76.9 74.9 80.6 80.8 75.7 63.3 62.7 61.8 60.8 60.6 60.7 59.4 58.2 59.3 60.2 60.4 61.7 51.5 51.6 59.9 62.5 61.5 58.8 55. 54.2 56.5 53.0 50.8 49.5 52.7 49.3 54.8 43.3 37.0 52.2 74.7 74.8 74.6 73.8 70.8 72.9 74.8 70.5 74.1 71.4 71.7 66.6 59.5 72.1 67.3 Table 6 Performance of top leaderboard models under 500M parameters on MTEB(Multilingual, v2). Rank is computed using Borda count against all models, regardless of parameter count. We run our evaluations with half-precision weights (bfloat16), using the prompt instructions detailed in the model card.4 We use context length of 512 tokens for most evaluation tasks, increasing to 1024 or 2048 for tasks such as LongEmbed Passkey Retrieval (Zhu et al., 2024), which require longer input sequences. 4.3. Results Overall Performance In Table 5, we present EmbeddingGemmas performance across MTEB benchmarks, XOR-Retrieve, and XTREME-UP, comparing it to popular general-purpose embedding models. Below, we also provide more thorough comparisons of MTEB benchmark performance, reporting performance for other top models as well as for EmbeddingGemmas lower-dimensional embeddings. EmbeddingGemma pushes the limit for quality across several benchmarks. EmbeddingGemma achieves the #1 rank and highest overall performance on the MTEB multilingual, English, and code leaderboards across models under 500M parameters, with significant lead over all previous top performing models on each of the metrics summarizing aggregate performance across tasks (Task Mean, Task Type Mean, and Borda rank). This gap persists when using fewer embedding dimensions: Even with 128-dimensional embeddings, EmbeddingGemma achieves the highest scores for Task Mean and Task Type Mean. EmbeddingGemma also achieves excellent scores on XOR-Retrieve and XTREME-UP; in sum, these results demonstrate it is state-of-the-art general-purpose model with proven capabilities spanning cross-lingual, multilingual, English, and code tasks. This performance advantage is not limited to small models: EmbeddingGemma is competitive with larger models, providing performance comparable to state-of-the-art models nearly double its size and with larger embedding dimensions; ranking #3, #2, and #2 across models under 1B parameters on the MTEB multilingual, English, and code leaderboards respectively; and outperforming commercial API models, with the notable exception of Gemini Embedding. MTEB(Multilingual, v2) We compare EmbeddingGemma to other top-ranked models from the MTEB multilingual benchmark in Table 6. Besides achieving the highest values for the aggregate scores, it leads in nearly all task types. Moreover, EmbeddingGemma offers drastic improvements in each of the task types it excels at, even going so far as to compete with larger models. For example, it outperforms Qwen3 Embedding 0.6B, the top-ranked model with fewer than 1B parameters, in instruction retrieval, multilabel classification, pair classification, and reranking  (Table 5)  . 4https://ai.google.dev/gemma/docs/embeddinggemma/model_card#prompt-instructions EmbeddingGemma: Powerful and Lightweight Text Representations Model Name EmbeddingGemma (768d) (512d) (256d) (128d) GIST-large-Embedding-v0 mxbai-embed-large-v1 UAE-Large-V1 GIST-Embedding-v0 bge-large-en-v1.5 GIST-small-Embedding-v0 NoInstruct-small-Embed.-v0 gte-large bge-base-en-v1.5 mini-gte SearchMap_Preview Memory (MB) Parameters Rank Mean (Task) Mean (Type) Class. Clus. Pair. Class. Rerank. Retrieval STS Summar. 578 308M 16 1278 639 1278 418 1242 127 127 639 390 253 1660 335M 335M 335M 109M 335M 33M 33M 335M 109M 66M 435M 23 27 28 33 36 41 42 43 44 46 47 69.7 69.2 68.4 66.7 66.3 66.3 66.4 65.5 65.9 64.8 - 64.8 65.1 65.1 64.1 65.1 64.6 64.0 62.7 62.0 62.0 61.8 61.4 61.9 60.7 - 60.9 60.8 60.7 60.5 87.6 87.4 86.9 85. 78.9 79.1 79.1 78.2 78.3 78.2 - 75.5 77.7 80.0 75.0 56.6 56.5 56.2 55.9 48.8 47.5 47.9 48.5 48.0 48.0 47.9 48.2 47.4 47.9 48.5 87.3 87.2 86.9 86.7 86.7 87.2 87.2 86.3 87.1 84.8 85.1 85.1 86.6 84.8 84.9 47.4 47.2 47.0 46. 48.8 48.1 48.4 47.5 48.3 47.3 47.0 47.8 46.7 46.9 47.9 55.7 54.0 51.4 46.0 54.5 55.4 55.9 53.6 55.4 52.0 53.7 53.3 54.8 53.2 52.2 83.6 83.6 83.8 83.3 84.4 84.4 84.4 83.3 82.8 82.8 82.8 83.3 82.1 81.6 81.6 37.6 36.2 35.9 34. 31.5 32.6 30.1 32.3 33.1 31.8 31.3 32.9 30.2 30.3 33.2 Table 7 Performance of top leaderboard models under 500M parameters on MTEB(Eng, v2). Rank is computed using Borda count against all models, regardless of parameter count. Memory (MB) Parameters Rank Mean All Mean -COIR AppsR. COIR CESR CFMT CSMT CSNCCR CSNR CTOC CTODL CQA SOQA ST2SQL Model Name EmbeddingGemma (768d) (512d) (256d) (128d) 578 308M KaLM mini-v1 KaLM mini-instruct-v1 gte-modernbert-base granite-embedding-125m-eng. gte-multilingual-base b1ade-embed snowflake-arctic-embed-m-v2.0 e5-large-v2 granite-embedding-eng.-r2 granite-embedding-small-eng.-r2 snowflake-arctic-embed-m-long 1885 1885 284 238 582 1278 1165 1278 284 91 522 494M 494M 149M 125M 305M 335M 305M 335M 149M 47M 137M 13 15 16 30 32 36 37 38 40 45 46 68.8 68.5 66.7 63.0 - - - 53.3 - 53.6 - - - - - 68.1 67.9 66.3 62.7 63.2 63.4 - 53.1 55.3 51.2 55.4 55.3 - - 53.1 84.4 84.3 82.4 79.5 46.8 46.9 56.4 11.8 12.0 6.7 10.8 14.2 14.0 13.5 7.4 75.5 74.6 71.8 65.9 - - 83.1 55.1 - 79.8 - - 64.7 60.5 - 62.1 60.9 56.8 50.2 60.0 54.6 - 58.8 51.4 55.8 59.3 57.3 - - 56.4 51.4 50.9 49.1 46.1 52.5 62.6 85.8 42.1 52.2 37.8 45.3 47.8 52.5 52.2 51.0 80.3 79.8 78.4 76.5 84.0 81.2 85.5 75.3 77.3 72.5 77.6 76.2 77.2 76.9 76. 73.7 73.1 71.0 65.7 59.5 59.6 93.4 47.6 53.8 51.8 52.8 60.0 47.7 48.4 47.1 90.1 89.7 88.5 86.0 88.0 87.0 - 76.9 89.4 86.7 80.3 83.2 - - 73.3 85.5 85.2 81.9 76.0 79.9 83.3 73.0 66.7 67.7 57.0 75.0 65.1 77.1 77.6 67. 33.5 35.0 36.5 36.4 34.0 36.4 36.1 29.6 35.6 22.3 28.2 32.4 35.0 33.6 30.6 43.6 44.1 42.8 38.5 33.6 29.6 42.2 36.6 34.0 35.3 36.8 32.1 37.0 35.6 35.2 86.5 86.0 84.0 79.7 92.4 92.8 90.9 89.8 87.1 83.5 90.7 89.9 91.8 90.0 89. 58.4 58.0 57.7 55.0 64.6 63.4 64.7 48.7 48.0 54.0 52.9 49.7 49.6 46.3 49.5 Table 8 Performance of top leaderboard models under 500M parameters on MTEB(Code, v1). Rank is computed using Borda count against all models, regardless of parameter count. MTEB(Eng, v2) Compared to the models in Table 7, EmbeddingGemma again achieves the highest overall scores and exceptional improvements in classification (+8.5), clustering (+7.8), and summarization (+4.4) compared to all other models. In fact, EmbeddingGemma also tops the sub1B-parameter leaderboard in these task types (again excluding models with possible data leakage due to training with over 25% of the MTEB data). MTEB(Code) In Table 8, we compare results for the individual tasks present in the MTEB(Code) benchmark. Note that only few models (including EmbeddingGemma) have been submitted to the MTEB(Code) leaderboard with evaluation metrics for all tasks. Specifically, most top models are missing COIRCodeSearchNetRetrieval (COIR); for this reason, we additionally report the mean score over the eleven remaining tasks, Mean -COIR. EmbeddingGemma achieves the best performance in this metric in addition to all other aggregate metrics. Observe that, compared to the second-best model, EmbeddingGemma provides dramatic performance increases in AppsRetrieval (+37.6) and CosQA (+10.0), both of which require retrieving relevant code given natural language queriescode contest problems and web queries, respectively. This highlights EmbeddingGemmas ability to create representations which not only work across languages, but also across domains. EmbeddingGemma: Powerful and Lightweight Text Representations Parameters Average as bho brx gbm gom gu hi hne kn mai ml mni mr mwr or pa ps sa ta ur EmbeddingGemma Gecko Embedding 308M 278M gte-multilingual-base 305M multiling.-e5-large-instr. 560M bge-m3 568M 572M jina-embeddings-v3 Qwen3-Embedding-0.6B 595M 7.11B Linq-Embed-Mistral 7.61B gte-Qwen2-7B-instruct voyage-3-large text-embedding-3-large 47.7 7.6 19.0 18.7 26.9 8.5 6.6 24.6 17.4 39.2 18.8 48.4 56.2 12.1 55.7 44.7 53.0 63.2 59.0 54.0 60.7 55.5 22.2 58.3 54.8 21.2 1.5 11.8 3. 10.7 12.9 20.4 10.4 10.3 13. 12.9 6.9 2.9 1.0 1.5 3. 45.5 35.2 55.8 50.3 48.6 8.5 8.1 1.1 2.5 8.5 17.6 21.2 26.5 9.8 4.6 23.8 14.7 34.3 18.2 22.4 21.9 31.7 7.9 9.6 38.1 22.7 44.8 28.8 2.7 1.5 1.8 0.1 0.3 8.6 5.4 7.9 3. 22.7 19.3 29.1 7.0 8.8 37.0 23.0 46.6 28.4 14.0 8.7 18.7 2.8 2.1 21.7 7.0 27.1 11.1 24.0 13.9 31.9 12.9 6.7 11.6 19.1 46.7 14.6 30.2 30.6 35.1 13.8 18.1 44.2 30.4 54.3 40.4 23.6 22.6 31.2 7.9 9.6 39.7 19.1 45.3 29.3 18.7 24.2 30.3 10.1 8.0 21.7 16.2 41.5 17. 26.7 24.0 32.6 8.7 10.9 38.5 25.9 48.3 31.1 22.5 8.6 35.2 11.5 7.0 10.2 21.7 45.3 15.6 5.5 6.3 5.4 0.9 0.1 14.7 7.2 19.2 2.9 25.1 23.0 31.8 12.2 10.2 31.4 23.8 45.5 25.5 13.0 17.3 23.1 6.7 2.0 10.7 11.3 18.7 24.5 29.2 11.2 8.0 8.3 19. 13.2 21.3 15.9 19.8 23.4 29.5 7.0 8.6 1.0 8.1 13.8 36.2 24.0 11.0 47.9 32.3 48.4 26.8 6.8 28.7 11.3 8.3 17.2 19.1 30.4 8.8 7.0 37.7 21.1 40.0 26.6 22.1 22.9 31.0 10.7 3.7 14.3 9.7 36.0 6.0 19.7 28.2 30.4 12.0 6.8 29.3 15.5 45.6 22. Table 9 Performance of top multilingual models on XTREME-UP (MRR@10). : For Gecko Embedding (Lee et al., 2024), we evaluate smaller version of the model discussed in the paper. XTREME-UP Table 9 shows results on XTREME-UP for EmbeddingGemma and other top multilingual models. The XTREME-UP cross-lingual retrieval task involves mapping queries in 20 underrepresented languages to English passages. EmbeddingGemmas performance is remarkable, vastly outperforming top models with billions of parameters as well as commercial API models. Moreover, against the selected open models, EmbeddingGemma achieves the strongest performance for each individual language evaluated, being outperformed only by commercial model in two languages. Note that even models with strong performance in the MTEB multilingual benchmark may struggle in XTREMEUP, such as Qwen3 Embedding 0.6B (Zhang et al., 2025b) and Jina Embeddings v3 (Sturua et al., 2024). This highlights EmbeddingGemmas exceptional capability in low-resource languages. 5. Future Work We intend to extend EmbeddingGemmas capabilities beyond text, into modalities such as image, audio, and video. This includes exploring new recipes to excel simultaneously at various unimodal (e.g. text text), cross-modal (e.g. text image), and multimodal (e.g. text+image image) use cases. Gemma 3 has demonstrated strong multimodal understanding, and previous work has shown promise in adapting multimodal large language models into multimodal embedding models, (Jiang et al., 2024; Jiang et al., 2025). However, these models use several billion parameters; we aim to fulfill the need for lightweight, natively multimodal embedding models able to be run on-device. 6. Conclusion EmbeddingGemma is lightweight yet powerful general-purpose embedding model built upon the Gemma 3 language model family. Our novel training recipe leverages knowledge from larger models through initialization from rich T5Gemma encoder and distillation from the highly-capable Gemini Embedding model. It also includes spread-out regularization and model merging with multiple optimized mixtures, to ensure that the resulting representations are expressive and generalizable. As result, EmbeddingGemma sets new standard for resource-efficient text embeddings. Our extensive evaluations demonstrate that EmbeddingGemma pushes the state-of-the-art across multilingual, English, and code benchmarks on the Massive Text Embedding Benchmark (MTEB) leaderboard for models under 500M parameters, and its performance is even comparable to models twice its size. This lead persists when using quantization for low-cost inference or truncating embeddings to reduce storage costs. By delivering state-of-the-art performance in compact size, EmbeddingGemma addresses the growing demand for models that enable faster, private, and offline-capable applications directly on user devices. We release our checkpoints to the community to accelerate research and development in efficient models. 11 EmbeddingGemma: Powerful and Lightweight Text Representations"
        },
        {
            "title": "References",
            "content": "A. Asai, J. Kasai, J. H. Clark, K. Lee, E. Choi, and H. Hajishirzi. Xor qa: Cross-lingual open-retrieval question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 547564, 2021. P. Belcak, G. Heinrich, S. Diao, Y. Fu, X. Dong, S. Muralidharan, Y. C. Lin, and P. Molchanov. Small language models are the future of agentic ai, 2025. URL https://arxiv.org/abs/2506.02153. G. de Souza P. Moreira, R. Osmulski, M. Xu, R. Ak, B. Schifferer, and E. Oldridge. Nv-retriever: Improving text embedding models with effective hard-negative mining, 2025. URL https:// arxiv.org/abs/2407.15831. K. Enevoldsen, I. Chung, I. Kerboua, M. Kardos, A. Mathur, D. Stap, J. Gala, W. Siblini, D. Krzemiński, G. I. Winata, et al. Mmteb: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. doi: 10.48550/arXiv.2502.13595. URL https://arxiv.org/abs/ 2502.13595. T. Gao, X. Yao, and D. Chen. Simcse: Simple contrastive learning of sentence embeddings, 2022. URL https://arxiv.org/abs/2104.08821. G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Towards unsupervised dense information retrieval with contrastive learning. CoRR, abs/2112.09118, 2021. URL https://arxiv.org/abs/2112.09118. P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. CoRR, abs/1803.05407, 2018. URL http://arxiv.org/ abs/1803.05407. B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27042713, 2018. doi: 10.1109/ CVPR.2018.00286. T. Jiang, M. Song, Z. Zhang, H. Huang, W. Deng, F. Sun, Q. Zhang, D. Wang, and F. Zhuang. E5-v: Universal embeddings with multimodal large language models, 2024. URL https://arxiv.org/ abs/2407.12580. Z. Jiang, R. Meng, X. Yang, S. Yavuz, Y. Zhou, and W. Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks, 2025. URL https://arxiv.org/abs/2410.05160. S. Kim, A. S. Rawat, M. Zaheer, S. Jayasumana, V. Sadhanala, W. Jitkrittum, A. K. Menon, R. Fergus, and S. Kumar. Embeddistill: geometric knowledge distillation for information retrieval, 2023. URL https://arxiv.org/abs/2301.12005. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3023330249. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ c32319f4868da7613d78af9993100e42-Paper-Conference.pdf. Z. Lan, L. Niu, F. Meng, J. Zhou, and J. Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning, 2025. URL https://arxiv.org/abs/2503.04812. 12 EmbeddingGemma: Powerful and Lightweight Text Representations C. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2025a. URL https://arxiv.org/ abs/2405.17428. J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, Y. Luan, S. M. K. Duddu, G. H. Abrego, W. Shi, N. Gupta, A. Kusupati, P. Jain, S. R. Jonnalagadda, M.-W. Chang, and I. Naim. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024. J. Lee, F. Chen, S. Dua, D. Cer, M. Shanbhogue, I. Naim, G. H. Ábrego, Z. Li, K. Chen, H. S. Vera, X. Ren, S. Zhang, D. Salz, M. Boratko, J. Han, B. Chen, S. Huang, V. Rao, P. Suganthan, F. Han, A. Doumanoglou, N. Gupta, F. Moiseev, C. Yip, A. Jain, S. Baumgartner, S. Shahi, F. P. Gomez, S. Mariserla, M. Choi, P. Shah, S. Goenka, K. Chen, Y. Xia, K. Chen, S. M. K. Duddu, Y. Chen, T. Walker, W. Zhou, R. Ghiya, Z. Gleicher, K. Gill, Z. Dong, M. Seyedhosseini, Y. Sung, R. Hoffmann, and T. Duerig. Gemini embedding: Generalizable embeddings from gemini, 2025b. URL https: //arxiv.org/abs/2503.07891. N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. doi: 10.48550/ARXIV.2210.07316. URL https://arxiv. org/abs/2210.07316. N. Muennighoff, H. Su, L. Wang, N. Yang, F. Wei, T. Yu, A. Singh, and D. Kiela. Generative representational instruction tuning, 2025. URL https://arxiv.org/abs/2402.09906. A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, J. Heidecke, P. Shyam, B. Power, T. E. Nekoul, G. Sastry, G. Krueger, D. Schnurr, F. P. Such, K. Hsu, M. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng. Text and code embeddings by contrastive pre-training, 2022. URL https://arxiv.org/abs/2201.10005. J. Ni, C. Qu, J. Lu, Z. Dai, G. H. Ábrego, J. Ma, V. Y. Zhao, Y. Luan, K. B. Hall, M.-W. Chang, and Y. Yang. Large dual encoders are generalizable retrievers, 2021. URL https://arxiv.org/abs/ 2112.07899. S. Ruder, J. H. Clark, A. Gutkin, M. Kale, M. Ma, M. Nicosia, S. Rijhwani, P. Riley, J.-M. Sarr, X. Wang, et al. Xtreme-up: user-centric scarce-data benchmark for under-represented languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 18561884, 2023. K. Santhanam, O. Khattab, J. Saad-Falcon, C. Potts, and M. Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction, 2022. URL https://arxiv.org/abs/2112.01488. S. Sturua, I. Mohr, M. K. Akram, M. Günther, B. Wang, M. Krimmel, F. Wang, G. Mastrapas, A. Koukounas, N. Wang, and H. Xiao. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URL https://arxiv.org/abs/2409.10173. H. Su, W. Shi, J. Kasai, Y. Wang, Y. Hu, M. Ostendorf, W. tau Yih, N. A. Smith, L. Zettlemoyer, and T. Yu. One embedder, any task: Instruction-finetuned text embeddings, 2023. URL https: //arxiv.org/abs/2212.09741. P. Suganthan, F. Moiseev, L. Yan, J. Wu, J. Ni, J. Han, I. Zitouni, E. Alfonseca, X. Wang, and Z. Dong. Adapting decoder-based language models for diverse encoder downstream tasks, 2025. URL https://arxiv.org/abs/2503.02656. Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri, T. Schuster, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler. Ul2: Unifying language learning paradigms, 2023. URL https://arxiv.org/abs/2205.05131. 13 EmbeddingGemma: Powerful and Lightweight Text Representations G. Team. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024a. URL https://arxiv.org/abs/2212. 03533. L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large language models, 2024b. URL https://arxiv.org/abs/2401.00368. M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022. URL https: //arxiv.org/abs/2203.05482. B. Zhang, F. Moiseev, J. Ainslie, P. Suganthan, M. Ma, S. Bhupatiraju, F. Lebron, O. Firat, A. Joulin, and Z. Dong. Encoder-decoder gemma: Improving the quality-efficiency trade-off via adaptation, 2025a. URL https://arxiv.org/abs/2504.06225. X. Zhang, F. X. Yu, S. Kumar, and S. Chang. Learning spread-out local feature descriptors. CoRR, abs/1708.06320, 2017. URL http://arxiv.org/abs/1708.06320. Y. Zhang, M. Li, D. Long, X. Zhang, H. Lin, B. Yang, P. Xie, A. Yang, D. Liu, J. Lin, F. Huang, and J. Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models, 2025b. URL https://arxiv.org/abs/2506.05176. D. Zhu, L. Wang, N. Yang, Y. Song, W. Wu, F. Wei, and S. Li. Longembed: Extending embedding models for long context retrieval, 2024. URL https://arxiv.org/abs/2404.12096. 14 EmbeddingGemma: Powerful and Lightweight Text Representations A. Full Results Task Name Performance Task Name Performance AILAStatutes AfriSentiClassification AlloProfClusteringS2S.v2 Alloprof Reranking AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna ArmenianParaphrasePC BUCC.v2 BelebeleRetrieval BibleNLPBitextMining BigPatentClustering.v2 BiorxivClusteringP2P.v2 BornholmBitextMining BrazilianToxicTweetsClassification BulgarianStoreReviewSentimentClassfication CEDRClassification CLSClusteringP2P.v2 CSFDSKMovieReviewSentimentClassification CTKFactsNLI CataloniaTweetClassification Core17InstructionRetrieval CovidRetrieval CyrillicTurkicLangClassification CzechProductReviewSentimentClassification DBpediaClassification DalajClassification DiaBlaBitextMining EstonianValenceClassification FaroeseSTS FilipinoShopeeReviewsClassification FinParaSTS FinancialPhrasebankClassification FloresBitextMining GermanSTSBenchmark GreekLegalCodeClassification GujaratiNewsClassification HALClusteringS2S.v2 HagridRetrieval IN22GenBitextMining IndicCrosslingualSTS IndicGenBenchFloresBitextMining IndicLangClassification IndonesianIdClickbaitClassification IsiZuluNewsClassification ItaCaseholdClassification JSICK KorHateSpeechMLClassification KorSarcasmClassification KurdishSentimentClassification LEMBPasskeyRetrieval LegalBenchCorporateLobbying MIRACLRetrievalHardNegatives MLQARetrieval MacedonianTweetSentimentClassification MalteseNewsClassification MasakhaNEWSClassification MasakhaNEWSClusteringS2S MassiveIntentClassification MedrxivClusteringP2P.v2 MultiEURLEXMultilabelClassification MultiHateClassification NTREXBitextMining NepaliNewsClassification News21InstructionRetrieval 37.37 44.47 52.82 79.69 84.23 63.59 59.59 71.54 92.68 98.75 72.38 12.68 41.64 52.10 34.63 22.34 71.26 52.76 41.45 34.49 79.34 51.23 6.31 78.93 58.63 58.63 94.27 50.28 83.93 38.29 65.30 40.52 25.22 86.45 55.35 84.67 29.03 82.78 29.34 98.92 74.40 43.07 87.09 46.62 60.87 26.42 70.36 84.40 11.58 58.10 59.97 60.75 95.08 66.20 78.95 45.31 33.08 74.93 43.46 62.70 44.11 4.34 61.00 73.87 95.48 11.45 NollySentiBitextMining NordicLangClassification NorwegianCourtsBitextMining NusaParagraphEmotionClassification NusaTranslationBitextMining NusaX-senti NusaXBitextMining OdiaNewsClassification OpusparcusPC PAC PawsXPairClassification PlscClusteringP2P.v2 PoemSentimentClassification PolEmo2.0-OUT PpcPC PunjabiNewsClassification RTE3 Robust04InstructionRetrieval RomaniBibleClustering RuBQReranking SCIDOCS SIB200ClusteringS2S SICK-R STS12 STS13 STS14 STS15 STS17 STS22.v2 STSB STSBenchmark STSES ScalaClassification SemRel24STS SentimentAnalysisHindi SinhalaNewsClassification SiswatiNewsClassification SlovakMovieReviewSentimentClassification SpartQA SprintDuplicateQuestions StackExchangeClustering.v2 StackOverflowQA StatcanDialogueDatasetRetrieval SwahiliNewsClassification SwednClusteringP2P SwissJudgementClassification T2Reranking TERRa TRECCOVID Tatoeba TempReasonL1 ToxicConversationsClassification TswanaNewsClassification TweetTopicSingleClassification TwitterHjerneRetrieval TwitterURLCorpus VoyageMMarcoReranking WebLINXCandidatesReranking WikiCitiesClustering WikiClusteringP2P.v2 WikipediaRerankingMultilingual WikipediaRetrievalMultilingual WinoGrande XNLI indonli 41.26 65.56 90.79 44.16 66.05 69.80 67.11 57.95 93.35 67.87 57.73 72.14 58.86 62.77 90.86 82.36 89.67 -0.94 41.88 71.26 18.43 26.53 81.37 79.32 86.42 83.67 89.35 84.42 71.20 81.64 88.16 82.31 50.77 65.22 65.49 65.67 57.00 73.27 10.68 97.03 90.94 86.47 46.27 65.95 40.04 57.74 67.54 65.15 80.35 51.35 1.00 82.93 31.15 73.02 72.04 86.90 60.99 10.16 92.02 27.03 89.88 90.00 59.40 81.73 60. Table 10 Full results of EmbeddingGemma on MTEB(Multilingual, v2). 15 EmbeddingGemma: Powerful and Lightweight Text Representations Task Name Performance AppsRetrieval COIRCodeSearchNetRetrieval CodeEditSearchRetrieval CodeFeedbackMT CodeFeedbackST CodeSearchNetCCRetrieval CodeSearchNetRetrieval CodeTransOceanContest CodeTransOceanDL CosQA StackOverflowQA SyntheticText2SQL 84.39 75.54 62.10 51.42 80.26 73.71 90.15 85.51 33.52 43.60 86.46 58.42 Task Name Performance AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna AskUbuntuDupQuestions BIOSSES Banking77Classification BiorxivClusteringP2P.v2 CQADupstackGamingRetrieval CQADupstackUnixRetrieval ClimateFEVERHardNegatives FEVERHardNegatives FiQA2018 HotpotQAHardNegatives ImdbClassification MTOPDomainClassification MassiveIntentClassification MassiveScenarioClassification MedrxivClusteringP2P.v2 MedrxivClusteringS2S.v2 MindSmallReranking SCIDOCS SICK-R STS12 STS13 STS14 STS15 STS17 STS22.v2 STSBenchmark SprintDuplicateQuestions StackExchangeClustering.v2 StackExchangeClusteringP2P.v2 SummEvalSummarization.v2 TRECCOVID Touche2020Retrieval.v3 ToxicConversationsClassification TweetSentimentExtractionClassification TwentyNewsgroupsClustering.v2 TwitterSemEval2015 TwitterURLCorpus 90.07 63.59 59.59 71.54 62.95 86.38 91.45 52.10 59.52 41.52 26.71 80.75 47.74 71.48 92.92 99.11 85.79 91.54 44.11 41.93 31.90 18.43 81.37 79.32 86.42 83.67 89.35 90.31 67.52 88.16 97.03 90.94 48.90 37.64 80.35 58.90 82.93 66.59 51.29 77.94 86.90 Table 11 Full results of EmbeddingGemma on MTEB(English, v2) (left) and MTEB(Code) (right). We include the comparison of top 20 embedding models under 500M parameters in MTEB(English, v2), excluding models trained on more than 25% of the MTEB data to mitigate potential over-fitting. Performance Language R@5000kt R@2000kt Arabic Bengali Finnish Japanese Korean Russian Telugu (ar) (bn) (fi) (ja) (ko) (ru) (te) 83.50 91.45 78.34 81.33 83.86 82.70 87. 75.08 85.86 71.97 72.61 78.25 78.90 83.19 Language Performance (MRR@10) Assamese Bhojpuri Boro Garhwali Konkani Gujrati Hindi Chhattisgarhi Kannada Maithili Malayalam Manipuri Marathi Marwari Odia Punjabi Pashto Sanskrit Tamil Urdu (as) (bho) (brx) (gbm) (gom) (gu) (hi) (hne) (kn) (mai) (ml) (mni) (mr) (mwr) (or) (pa) (ps) (sa) (ta) (ur) 48.44 56.21 12.14 55.71 44.65 52.95 63.19 58.96 54.04 60.74 55.46 22.16 58.27 54.81 21.21 45.50 35.21 55.81 50.34 48. Table 12 Full results of EmbeddingGemma on XOR-Retrieve (left) and XTREME-UP (right). 16 EmbeddingGemma: Powerful and Lightweight Text Representations B. Contributions and Acknowledgments Core Contributors (: equal contributions) Henrique Schechter Vera Sahil Dua Biao Zhang Daniel Salz Ryan Mullins Sindhu Raghuram Panyam Sara Smoot Iftekhar Naim Joe Zou Feiyang Chen Daniel Cer Contributors Alice Lisak Min Choi Lucas Gonzalez Omar Sanseviero Glenn Cameron Ian Ballantyne Kat Black Kaifeng Chen Weiyi Wang Zhe Li Gus Martins Jinhyuk Lee Mark Sherwood Juyeong Ji Renjie Wu Jingxiao Zheng Jyotinder Singh Abheesht Sharma Divya Sreepat Gemini Embedding (alphabetical order) Aashi Jain Adham Elarabawy AJ Co Andreas Doumanoglou Babak Samari Ben Hora Brian Potetz Dahun Kim Enrique Alfonseca Fedor Moiseev Feng Han Frank Palma Gomez Gustavo Hernández Ábrego Hesen Zhang Hui Hui Jay Han Karan Gill Ke Chen Koert Chen Madhuri Shanbhogue Michael Boratko Paul Suganthan Sai Meher Karthik Duddu Sandeep Mariserla Setareh Ariafar Shanfeng Zhang Shijie Zhang Simon Baumgartner Sonam Goenka Steve Qiu Tanmaya Dabral Trevor Walker Vikram Rao Waleed Khawaja Wenlei Zhou Xiaoqi Ren Ye Xia Yichang Chen Yi-Ting Chen Zhe Dong Zhongli Ding Gemma (alphabetical order) Francesco Visin Gaël Liu Jiageng Zhang Kathleen Kenealy Michelle Casbon Ravin Kumar Thomas Mesnard"
        },
        {
            "title": "Leadership\nZach Gleicher\nCormac Brick\nOlivier Lacombe\nAdam Roberts\nYunhsuan Sung\nRaphael Hoffmann\nTris Warkentin\nArmand Joulin\nTom Duerig\nMojtaba Seyedhosseini",
            "content": "17 EmbeddingGemma: Powerful and Lightweight Text Representations Acknowledgement Daniel Estrada Alva, Krzysztof Czuba, Qin Yin, Howard Zhou, Tomáš Ižo, Tom Aarsen, Aditya Kusupati, Samer Hassan, Aishwarya Nagarajan"
        }
    ],
    "affiliations": [
        "Google"
    ]
}