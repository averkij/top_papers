{
    "paper_title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
    "authors": [
        "Yoni Kasten",
        "Wuyue Lu",
        "Haggai Maron"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time. Aiming to significantly improve the efficiency of previous approaches, we present TracksTo4D, a learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using a single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using a low-rank approximation. TracksTo4D is trained in an unsupervised way on a dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TracksTo4D can reconstruct a temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95\\%. We further show that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 2 7 9 0 7 0 . 4 0 4 2 : r Fast Encoder-Based 3D from Casual Videos via Point Track Processing Yoni Kasten1 Wuyue Lu2 1NVIDIA Research 2Simon Fraser University Haggai Maron1,3 3Technion"
        },
        {
            "title": "Abstract",
            "content": "This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require long optimization time. Aiming to significantly improve the efficiency of previous approaches, we present TRACKSTO4D, learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from casual videos using single efficient feed-forward pass. To achieve this, we propose operating directly over 2D point tracks as input and designing an architecture tailored for processing 2D point tracks. Our proposed architecture is designed with two key principles in mind: (1) it takes into account the inherent symmetries present in the input point tracks data, and (2) it assumes that the movement patterns can be effectively represented using low-rank approximation.TRACKSTO4D is trained in an unsupervised way on dataset of casual videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments show that TRACKSTO4D can reconstruct temporal point cloud and camera positions of the underlying video with accuracy comparable to state-of-the-art methods, while drastically reducing runtime by up to 95%. We further show that TRACKSTO4D generalizes well to unseen videos of unseen semantic categories at inference time. Project page: https://tracks-to-4d.github.io."
        },
        {
            "title": "Introduction",
            "content": "Predicting 3D geometry in dynamic scenes is challenging problem. In this problem setup, we are given access to multiple images of scene taken sequentially, e.g., from monocular video camera, where both the content in the scene and the camera are moving. Our task is to reconstruct the dynamic 3D positions of the points seen in the images and the camera poses. This fundamental problem has gained significant interest from the research community over the years [6, 34, 23, 59], mainly due to its important applications in many fields such as robot navigation, autonomous driving and 3D reconstruction of general environments [17]. Importantly, in contrast to static scenes where the epipolar geometry constraints hold between the corresponding points of different views [15], determining the depth of moving point from monocular views is an ill-posed problem [3]. This causes standard Structure from Motion techniques [38, 51, 30] to be inadequate in this setup [22]. Previous work and limitations. Many existing approaches for the above problem make simplifying assumptions that limit their applicability to real-world scenarios. Methods based on orthographic camera models and low-rank assumptions use matrix factorization techniques [6, 23], but the orthographic camera assumption might not be realistic and may cause reconstruction errors. Techniques that incorporate depth priors often require lengthy optimization processes in order to make the depth estimates across frames consistent [22, 59]. Other physics-based approaches make assumptions about rigid bones [56, 54] or isometric deformable surfaces [34] and typically involve complex, slow Preprint. Under review. Figure 1: We present TRACKSTO4D, method for mapping set of 2D point tracks extracted from casual dynamic videos into their corresponding 3D locations and camera motion. At inference time, our network predicts the dynamic structure and camera motion in single feed-forward pass. Our network takes as input set of 2D point tracks (left) and uses several multi-head attention layers while alternating between the time dimension and the track dimension (middle). The network predicts cameras, per-frame 3D points, and per-world point movement value (right). The 3D point internal colors illustrate the predicted 3D movement level values, such that points with high/low 3D motion are presented in red/purple colors respectively. These outputs are used to reproject the predicted points into the frames for calculating the reprojection error losses. See details in the text. The reader is encouraged to watch the supplementary video visualizations. optimization per video. In addition, they may require foreground-background segmentation of the moving content, which is not always easily obtained. Alternatively, some methods are specifically tailored to certain object classes like humans [48], restricting their domain to those limited cases. Consequently, these prior methods are either not directly applicable to casual videos, or require long optimization time per video. Our approach. We propose TRACKSTO4D,1 novel approach for fast reconstruction of sparse dynamic 3D point clouds and camera poses from casual videos. Our main idea is to train neural network on multiple videos to learn the mapping from the input image sequence to sequence of the scenes 3D point clouds and camera poses. After training, the trained network can be efficiently applied to new image sequences using single feed-forward pass, avoiding costly optimization. To enhance the methods ability to generalize across different types of videos and scenes, we made crucial design choice: our approach processes point track tensors as input, rather than operating directly on the image sequence. Specifically, Each entry (n, p) in these tensors represents the 2D position of tracked point in specific video frame [6]. Our main insight is that point track tensors may exhibit more common motion patterns across casual video domains compared to image pixels. In other words, we argue that processing the raw point track data rather than scene-specific pixels or features may enable learning class and scene-agnostic internal feature representations for improved generalization. Importantly, recent advances in point tracking [10, 18] enable efficiently inferring these point tracks from casual videos using pre-trained models. These two properties make point track matrices an attractive input for our learning method. Following this design choice, we design our architecture according to two principles: (1) process point track tensors, which have unique structure, and (2) encode meaningful prior knowledge about the reconstruction problem, as the problem is ill-posed in general. In the following, we address these desired properties. First, we design network architecture that can effectively and efficiently handle point track inputs. To do that, we propose novel layer design that takes into account the symmetries of the problem: the mapping we aim to learn, from point track matrices to 3D point clouds and camera poses, preserves two natural symmetries: (i) the points being tracked can be arbitrarily permuted without affecting the problem; (ii) the frames containing these points exhibit temporal structure, adhering to an approximate time-translation symmetry. Following the Geometric Deep Learning paradigm [7], we build upon recent theoretical advances in equivariant learning [27] and integrate these two symmetries into our network architecture using dedicated attention and positional encoding mechanisms. 14D since we have three Euclidean coordinates with an additional time coordinate 2 Second, key challenge in predicting 3D dynamic motion and camera poses from 2D point tracks is that this problem is inherently ill-posed without additional constraints [3]. To address this, we integrate low-rank movement assumption into our architecture, following the seminal work of [6] which constrained output point clouds to be linear combinations of basis elements. Specifically, given an input point track tensor, our architecture equivariantly predicts small set of input-specific basis elements. The output point clouds at each time frame are then defined as linear combination of these basis elements, with the coefficients also predicted by the network. Notably, the first basis is assumed to fully represent the 3D static points in the video, while the remaining basis elements capture the 3D dynamic deviations. This structure effectively restricts the predicted point clouds to have more specific form, making the problem more constrained. Our approach is trained on dataset of extracted point track matrices [18] from raw videos without any 3D supervision by simply minimizing the reprojection errors, aiming to predict output point clouds that, after undergoing perspective projection, will return the original 2D point tracks. In our experiments, TRACKSTO4D is trained on the Common Pets dataset [40]. We evaluate our method on test data with GT camera poses and GT depth information for point tracks, and demonstrate that it produces comparable results to state-of-the-art methods, while having significantly shorter inference time by up to 95%. In addition, we show the methods ability to generalize to out-of-domain videos. Contributions. In summary, our contributions are (1) novel modeling of the dynamic reconstruction problem via learning on point tracks without 3D supervision; (2) novel deep learning architecture incorporating two key principles: accounting for the symmetry of the data and encoding low-rank structure in the predicted point clouds (3) Experiments demonstrating extremely fast inference time compared to baselines, accurate results, and strong generalization across other categories."
        },
        {
            "title": "2 Method",
            "content": "Problem formulation. Given video of frames, let RN 3 be pre-extracted 2D point tracks tensor (Fig. 1, left side). This tensor represents the two-dimensional information about set of world points that are tracked throughout the video. Each element in the tensor, Mi,j,:, stores three values: (x, y, o) where x, are respectively the observed horizontal and vertical locations of point in frame i, and {0, 1} indicates whether point is observed in frame or not. Our goal is to train deep neural network to map the input point tracks tensor into set of per-frame camera poses {Ri(M ), ti(M )}N i=1, where Ri(M ) SO(3), ti(M ) R3, Xi(M ) RP 3 (Fig. 1, right side). i=1 and per-frame 3D points {Xi(M )}N Overview of our approach. Our method receives RN 3 as input. This tensor is being processed by neural architecture composed of multi-head attention layers where the attention is applied in an alternating fashion on the and the dimensions in each layer. These layers are defined in Sec. 2.1. After composition of several such layers, the network uses the resulting features in RN d to predict camera poses in SO3 R3 and point clouds in RN 3. These point clouds are parameterized as linear combination of input specific point cloud bases B1(M ), . . . BK(M ) RP 3. This is discussed in detail in Sec. 2.2. Our network is trained in an unsupervised way on dataset of videos by minimizing the reprojection error and other regularization losses (Sec. 2.3) that are used to update the model parameters. Our pipeline is illustrated in Fig."
        },
        {
            "title": "2.1 Equivariant layers for point track tensors",
            "content": "Following the geometric deep learning paradigm, our goal is to design neural architecture that respects the underlying symmetries and structure of the data. Symmetry analysis. Our input is tensor RN 3 representing sequence of frames each with point coordinates. This structure gives rise to two key symmetries: First, the order of the points within each frame does not matter - in other words, permuting this axis results in an equivalent problem [27]. Formally, this axis has permutation symmetry SP where SP is the symmetric group on elements. Second, along the temporal axis, we have an approximate translation symmetry arising from the ordered video sequence. This means that shifting the time frames is required to result in the same shift in our output. We model this with cyclic group CN of order . Both symmetries are illustrated in Fig. 2. We note that while the cyclic group assumption 3 may not be entirely accurate, we still find it useful as it helps us to derive appropriate parametric layers for our data, similar to how the convolutional layer is derived for data with translational symmetries such as images. Taken together, the full symmetry group of the input space is the direct product = CN SP combining these time and point permutation symmetries, acting on RN 3 by ((t, σ) )n,p,j = Mt1(n),σ1(p),j for (t, σ) G2. Next, we will design an architecture equivariant to G, to ensure that the model takes into account the symmetries above. Linear equivariant layers. Point track tensors can be viewed as collection of individual point tracks, each of which exhibits translational symmetry. The scenario where an object comprises set of elements with their own symmetry group, such as set of images or graphs, was previously explored in [27]. In that work, the authors characterized the general linear equivariant layer structure in such cases, termed the Deep Sets for Symmetric Elements (DSS) layer. Building on the DSS approach, our basic linear equivariant layer for the point track tensors would take the form: (M ):,j = L1(M:,j) + (cid:88) j=1 L2(M:,j) (1) Figure 2: The symmetry structure of our problem. Frames (vertical) have time translation symmetry while points (horizontal) have set permutation symmetry. where Li are linear translation equivariant function (i.e. convolutions), M:,j RN are the columns of representing all the inputs for specific tracked point, (M ):,j RN is the output column and d, are the input and output feature channels respectively. To construct neural network, these layers can be interleaved with pointwise nonlinearities, similar to basic convolutional neural networks. Implementation via transformers and positional encoding. While the linear layer design is reasonable, it may not be the optimal choice. To enhance the model, we design new layer whose structure follows Equation (1), but incorporates nonlinear layers in the form of transformers [47]. Specifically, our layer is formulated similarly to Equation (1), but instead of convolutions (Li) and summations, it utilizes two self-attention mechanisms and suitable temporal positional encoding across the dimension. Formally, our basic layer : RN d RN d is computed via four steps, which are described below: qij = QMij, kij = KMij, vij = Mij Mij = (cid:88) i=1 exp(qij kij) l=1 exp(qij klj) (cid:80)N vij (2) qij = Mij, kij = Mij, vij = Mij (M )ij = (cid:88) j=1 exp(qij kij) l=1 exp(qij kil) (cid:80)P vij (3) Here, Mi,j Rd are the features associated with the j-th point in the i-th frame. The attention mechanism defined in the first equation above (2) is augmented with standard temporal positional encoding in the first layer and replaces the translation equivariant function Li applied to the columns of (Eq.(1)). The attention in the second equation (3) implements the set aggregation (summation) (also in Eq.(1)) applied to the rows of . As commonly done, we use transformers with 16 attention heads [47]."
        },
        {
            "title": "2.2 Constraining 3D motion and camera poses via low-rank assumption",
            "content": "Given our 2D tracks, we aim to characterize the motion of the points by decomposing them into the global camera motion and the 3D motion of objects in the scene. The 2D motion of static scene points provides useful constraints for estimating the camera motion. However, as previously mentioned, predicting camera and dynamic 3D motion solely from 2D motion is an ill-posed problem without additional constraints [3]. We tackle this challenge by adding two mechanisms to our architecture: (1) low-rank movement assumption; and (2) specific modeling of the static scene for camera estimation. 2This is different from the symmetry group studied in [29], where the temporal structure was not exploited. 4 Low-rank movement assumption. First, motivated by classical orthographic Non-Rigid Structure from Motion [6], we constrain the output points to be formulated by linear combination of input-specific basis elements. Specifically, given the input 2D point tracks, RN 3, our network predicts point clouds: B1(M ), . . . , BK(M ) RP 3 and (K 1) linear coefficients, {c1k(M )}K k=2 cik(M )Bk(M ), where Xi(M ) RP 3 is the 3D point cloud at frame i. The point clouds and coefficients are computed by taking the output of the last equivariant layer as defined in the previous section and applying invariant aggregations on the respective dimension resulting in equivariant and invariant outputs. See more details in the appendix. We note that we deliberately chose the coefficient of B1(M ) to be the constant 1, the reason is explained in the next paragraph. k=2 such that Xi(M ) = B1(M ) + (cid:80)K k=2, . . . {cN k(M )}K Specific modeling of the static scene for camera estimation. Frequently, casual video data of dynamic scenes contains many static regions, which can be used to determine camera poses [61]. We leverage this observation by treating the first basis element B1(M ) RP 3 as static approximation for all scene points and encourage B1(M ) as well as the output camera poses to explain the 2D observations according to this approximation using \"static\" reprojection loss (LStatic, defined in the next section). We note, however, that static point cloud is not likely to produce low reprojection errors for the non-static components, thus the reprojection error necessitates robustness to substantial errors from the non-static elements. To address this, our network predicts (equivariantly) motion level values γ1(M ), . . . , γP (M ) R+, one for each point in our dynamic point cloud, which we use to weight the reprojection errors from B1(M ). The main idea is to give less weight to non-static points so that the static projection loss can disregard them. Specifically, inspired by [59], each γi(M ) defines Cauchy distribution that models the reprojection errors for its associated world point, such that world point with higher γ is expected to produce wider error distribution. Empirically, as noted by [59], the Cauchy distribution tends to be more robust for modeling reprojection error uncertainties compared to Gaussian noise modeling [19]. Then, LStatic, minimizes the negative log-likelihood under this assumption. See details in Sec. 2.3."
        },
        {
            "title": "2.3 Training and losses",
            "content": "Model outputs. Given the input 2D point duces outputs as function of : RP 3, {c1k(M )}K k=2, . . . , {cN k(M )}K k=2 X1(M ), , XN (M ) RP 3,γ1(M ), . . . , γP (M ) R+ movement (R1(M ), t1(M )), . . . , (RN (M ), tN (M )) SO(3) R3 camera poses. We use these network outputs to define an self-supervised loss function with respect to the current network weights and which is defined by: tracks RN 3, our network prolinear bases and coefficients B1(M ), . . . , BK(M ) which define dynamic point cloud level values, and = λReprojectLReproject + λStaticLStatic + λNegativeLNegative + λSparseLSparse (4) Reprojection loss. The reprojection loss encourages the consistency between the output 3D point clouds and camera poses, to the input 2D observations: (cid:88) (cid:88)"
        },
        {
            "title": "M o",
            "content": "ijR(Xij, Ri, ti, xy ij ) (5) LReproject = 1 (cid:80)P j=1 ij (cid:80)N i= i=1 j=1 ij ) is the reprojection error when projecting the point Xij with the camera where R(Xij, Ri, ti, xy pose (Ri, ti) with respect to the measured point xy ij : (RT (Xij ti))1,2 (RT (Xij ti))3 R(Xij, Ri, ti, xy ij ) = (cid:13) (cid:13) (cid:13) (cid:13) xy ij (cid:13) (cid:13) (cid:13) (cid:13) (6) Static loss. As discussed in Sec. 2.2, to better constrain the camera poses, the first predicted basis element B1(M ) RP 3 defines static (fixed in time) point cloud approximation. Our network also predicts movement coefficient γj(M ) for each world point that defines zero-mean Cauchy distribution. Given γj and the reprojection error rij = R(B1j, Ri, ti, xy ij ) of the jth point of B1 that is projected by the ith camera, the negative log-likelihood of rij distributed according to the γj-zero-mean Cauchy distribution is proportional to: (cid:32) (cid:33) (7) r2 ij γj C(rij, γj) = log γj + 5 Table 1: Pet evaluation. Top: Baseline method results for structure or camera estimation (or both). Bottom: Our results with several configurations. (C),(D), or (CD) respectively indicate the object categories in the training set: cats, dogs, or both. BA and FT respectively indicate post-processing of Bundle Adjustment or fine-tuning. Abs Rel Dyn."
        },
        {
            "title": "All",
            "content": "δ < 1.25 All Dyn. δ < 1.252 All Dyn. δ < 1.253 All Dyn. ATE (mm) RPE Trans (mm) RPE Rot (deg) Time (min) D-SLAM [44] ParticleSFM [61] RCVD [22] CasualSAM [59] MiDaS [4] Ours (C) Ours (C)+BA Ours (C)+FT Ours (D) Ours (D)+BA Ours (D)+FT Ours (CD) Ours (CD)+BA Ours (CD)+FT - - 0.40 0.09 0.16 0.11 0.11 0.09 0.12 0.12 0.09 0.12 0.12 0.09 - - 3.6E+07 0.06 6.2E+ 0.08 0.08 0.06 0.08 0.08 0.06 0.08 0.08 0.06 - - 0.43 0.93 0.78 0.88 0.88 0.90 0.85 0.85 0.88 0.85 0.85 0.90 - - 0.72 0.97 0.71 0.92 0.92 0.96 0.91 0.91 0.96 0.91 0.91 0.96 - - 0.75 0.99 0. 0.99 0.99 1.00 0.99 0.99 1.00 0.98 0.98 1.00 - - 0.90 0.99 0.88 0.98 0.98 0.99 0.99 0.99 0.99 0.98 0.98 0.99 - - 0.92 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 - - 0.96 1.00 0. 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 5.08 12.79 43.95 6.90 - 8.96 4.22 4.00 8.03 4.19 3.98 8.11 4.21 3.98 3.60 6.95 25.77 3.95 - 3.79 2.86 2.74 3.54 2.83 2.74 3.68 2.86 2.74 0.20 0.51 2.31 0.22 - 0.23 0.17 0.16 0.23 0.17 0.16 0.24 0.17 0.16 0.16 11.00 20.00 1.3E+02 0.15 0.15 0.15 4.86 0.15 0.15 4.86 0.15 0.15 4.86 Table 2: Out-of-training-domain evaluation . Evaluation metrics on monocular videos from [58]. The table has the same structure as Tab. 1. Abs Rel Dyn. All δ < 1.25 All Dyn. δ < 1.252 All Dyn. δ < 1.253 All Dyn. D-SLAM [44] ParticleSFM [61] RCVD [22] CasualSAM [59] MiDaS [4] - - 0.19 0.05 2.8E+ - - 2.6E+05 0.03 2.7E+05 Ours (C) Ours (C)+BA Ours (C)+FT Ours (D) Ours (D)+BA Ours (D)+FT Ours (CD) Ours (CD)+BA Ours (CD)+FT 0.08 0.08 0.07 0.08 0.08 0.05 0.10 0.10 0.06 0.06 0.06 0.03 0.07 0.07 0.03 0.08 0.08 0.03 - - 0.69 0.95 0.59 0.89 0.89 0.94 0.92 0.92 0.97 0.93 0.93 0. - - 0.75 0.99 0.58 0.95 0.95 0.98 0.93 0.93 0.99 0.94 0.94 0.99 - - 0.95 0.99 0.73 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 - - 0.95 1.00 0.72 0.99 0.99 1.00 0.98 0.98 1.00 0.99 0.99 1. - - 0.96 1.00 0.83 0.99 0.99 1.00 0.99 0.99 0.99 1.00 1.00 0.99 - - 0.98 1.00 0.80 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 ATE (mm) 7.96 26.66 1.6E+02 7.81 - 32.06 8.67 7.98 33.77 8.40 8.15 36.17 8.62 8.04 RPE Trans (mm) RPE Rot (deg) 10.91 23.83 3.2E+02 10.09 - 47.99 12.36 11.64 51.64 12.06 11.88 53.94 12.49 11.84 0.07 0.20 3.43 0.06 - 0.45 0.08 0.08 0.61 0.08 0.09 0.67 0.08 0.09 Time (min) 0.18 2.13 7.00 22.00 0.02 0.04 0.04 0.59 0.04 0.04 0.59 0.04 0.04 0.59 Note, that this loss reduces the contribution of the reprojection errors for points with high γ, but also encourages γ to be small, i.e. encouraging the static point cloud to represent the dynamic scene when possible. Our static loss is the mean negative log-likelihood over all observed points in all frames: LStatic = 1 (cid:80)P j=1 ij (cid:88) (cid:88) i=1 j=1 (cid:80)N i="
        },
        {
            "title": "M o",
            "content": "ijC(R(B1j, Ri, ti, xy ij ), γj) (8) Regularization losses. As in [29] we regularize the observed points to be in front of the camera by: LNegative = (cid:88) (cid:88) i=1 j="
        },
        {
            "title": "M o",
            "content": "ij Min(RT (Xij ti))3, 0) (9) We further find it beneficial to regularize the deviation from the static approximation B1 to be sparse for static points, i.e. points with low γ values: LSparse ="
        },
        {
            "title": "1\nP (K − 1)",
            "content": "K (cid:88) (cid:88) k=2 j=1 1 3γj (Bkj1 + Bkj2 + Bkj3) (10) where γ is detached from the gradient calculation for this loss."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we conduct experiments to verify our proposed networks performance on real-world casual videos. We began by training the network on specific domains and then evaluated its accuracy and running time on unseen videos from both, training and unseen domains. Training procedure. We trained our network on the cat and dog partitions from the COP3D dataset [40], which contains diverse set of casual real-world videos of pets. Our networks were trained 6 Table 3: Ablation study. The contribution of different parts from our method. See details in the text. Abs Rel Dyn."
        },
        {
            "title": "All",
            "content": "δ < 1.25 All Dyn. δ < 1.252 All Dyn. δ < 1.253 All Dyn. Rep.(pix.) All Dyn. ATE (mm) RPE Trans (mm) RPE Rot (deg) Set of Sets No LStatic No γ K=30 K=2 DSS No LSparse Full 0.27 0.77 0.22 0.14 0.11 1.65 0.17 0.11 0.15 0.36 0.16 0.09 0.08 0.58 0.13 0.08 0.60 0.25 0.66 0.81 0.88 0.19 0.79 0. 0.77 0.46 0.73 0.90 0.91 0.35 0.80 0.92 0.87 0.48 0.93 0.97 0.98 0.34 0.95 0.99 0.94 0.70 0.91 0.98 0.98 0.60 0.94 0.98 0.97 0.68 0.99 0.99 1.00 0.47 1.00 1.00 0.99 0.82 0.97 0.99 1.00 0.74 0.99 1.00 9.86 1.00 4.54 4.88 8.58 63.75 4.57 3. 5.33 0.86 2.41 2.78 3.56 70.60 2.73 1.97 16.87 96.20 13.91 9.39 9.31 34.90 11.79 8.96 5.53 29.86 4.86 3.68 3.86 22.63 7.99 3.79 0.39 0.99 0.29 0.23 0.25 1.64 0.55 0.23 from scratch three times to test our generalization capability between semantic categories: once on the cat partition, once on the dog partition, and once on both partitions combined. Training technical details are provided in the appendix. We use = 12 bases in all our experiments (Sec. 2.2). Evaluation data. To assess our frameworks performance on pet videos, we curated new dataset3 consisting of 21 casual videos of dogs and cats, each video containing 50 frames. These videos were captured using an RGBD (RGB-Depth) sensor. The depth maps were used as ground truth for evaluating the reconstructed structure. We extracted the cameras by running COLMAP on the images while masking out the pet areas with dilatated masks provided by [65]. The cameras were scaled to millimeter units using the provided GT depth. Note that our network did not see this test data during training and it was not used to tune our hyperparameters. Additionally, to evaluate our method on out-of-domain evaluation data, we used the Nvidia Dynamic Scenes Dataset [58]. Specifically, while our network was trained on pet videos, this dataset contains other dynamic object types, e.g. human, balloon, truck, and umbrella, with different camera motion type. The dataset contains 8 dynamic scenes which are captured by 12 synchronized cameras, enabling accurate depth estimation which is treated as GT for evaluating monocular depth estimation. The ground truth camera poses were calculated by [38] with the synchronized multiview camera rig and the ground truth dynamic masks. Similarly to [24] we simulated 8 monocular dynamic video sequences using the camera rig, each with 24 frames, and used them for evaluation. Evaluation results. Qualitative visualizations are presented in Fig. 3.4 We also show visualization of the movement level values, γ in Fig. 4. For comparisons, we chose state-of-the-art methods that as our method, can be applied to raw casual videos that were captured by standard pinhole camera models and do not need any static or dynamic segmentation. We evaluated both, the camera poses and the structure accuracies. Comparison results for the pet-test-set and out-of-domain dataset are presented in Tables 1 and 2 respectively. The camera poses are evaluated compared to the GT, using the Absolute Translation Error(ATE), the Relative Translation Error(RTE), and the Relative Rotation Error(RRE) metrics after coordinates system alignment. We compare three training configurations of our method of training only on cats, only on dogs, and on both. As can be seen in the tables, the results are similar in all 3 cases. Our output camera poses as inferred by the network are already accurate and outperform some of the prior methods. We further show the results of our method after single and short round of Bundle Adjustment, which makes our method better than all baselines on the pet sequences, and comparable on the out-of-domain cases. Importantly, Tables 1 and 2 also compare the methods running times. It can be seen that our method, even with bundle adjustment, is the fastest camera prediction method. Tables 1 and 2 also show structure evaluation with the depth evaluation metrics [11] on the sampled point tracks. They demonstrate that our inferred structure is almost comparable to the state-of-the-art [59] while taking significantly shorter running times (a few seconds for our method versus more than two hours for [59] on pet videos). Short (0.6-5 minutes), per-sequence fine-tuning makes our methods accuracy comparable to [59], see appendix for details. In terms of running time, our method is bit slower than MiDaS [4], which only provides depth maps without cameras, but achieves much better results. 3While the COP3D dataset provides cameras that were extracted by COLMAP [38], we note that this evaluation data is insufficient in our case. This is because the dynamic structure was captured as well in part of their reconstruction which indicates that its reconstruction might not be accurate. Furthermore, the coordinates system units of these reconstructions are unknown. Finally, this dataset does not have any depth map information for evaluating the dynamic structure. 4The reader is encouraged to watch the supplementary videos for better 4D perception. 7 Figure 3: Qualitative Results. Top. Frames from 2 different test video sequences with point tracks marked with corresponding colors. Bottom. 3D visualization of our methods outputs, from two time stamps. The camera trajectory is present as gray frustums, whereas the current camera is marked in red. The reconstructed 3D scene points are presented in corresponding colors to the input tracks on the top. The scene is observed from the same viewpoint, enabling the visualization of the dynamic reconstructed structure. Figure 4: γ Visualization. We show visualization of the γ outputs of our network that are described in Sec. 2.2. In each video sequence, we show the input tracks, where each color visualizes its movement level value, γ. Purple marks static points with low γ whereas red marks dynamic points with high γ. Note, that our network did not get any direct supervision for these values, but only the raw point tracks predictions from [18]. The γ visualizations for cats were produced by the model that was only trained on dogs and vice versa. We note that our model generalizes well to out-of-domain (non-pet) cases as well. We note that in contrast to the other methods that predict the dynamic depth, ours does not use any depth-from-single-image prior. Ablation study To evaluate the contribution of our different method parts we run an ablation study which is presented in Tab. 3. In this study, the training was always done on the cat partition from COP3D and evaluated on our test data which contains dogs and cats. First, we performed an ablation study on our transformer architecture by taking the architecture suggested by [29] (\"Set of Sets\") or the DSS architecture that uses only linear layers [27] (\"DSS\"). As the table shows our architecture (\"Full\") achieved significantly better results. To test the losses in our framework, we also evaluated the following: (1) ignoring the γ outputs and using regular reprojection error on B1 for all points (\"No γ\"); (2) removing our sparsity loss (\"No LSparse\"); and (3) removing the static loss (\"No LStatic\"). In all cases, the error increased whereas in the later one, the results became much worse. We further ablate the choice of = 12 as the number of linear bases, by trying 2 extreme numbers of = 30, = 2 (we saw no significant differences when we used nearby choices such as = 11). As can be seen in the table, when = 30 the output is not regularized enough and produces higher depth error for the dynamic part. For = 2 the depth is regularized but the reprojection error (\"Rep. (pix.)\") gets higher due to over-regularization. Overall, this study justifies our design choices (\"Full\")."
        },
        {
            "title": "4 Related Work",
            "content": "Simultaneous Localization and Mapping (SLAM) and Structure from Motion (SfM) SfM pipelines seek to recover static 3D structure and camera poses from unordered images.[45, 41, 38, 1, 51]. Learning-free pipelines [38] are effective but require repeated applications of Bundle Adjustment 8 (BA) [46]. [29, 8] presented method for learning prior from dataset of multiview image sets, to accelerate SfM pipelines by using equivariant deep networks. Monocular Simultaneous Localization and Mapping (SLAM) methods [30, 31, 12, 57, 5, 50, 62, 63, 42] extract camera poses from video sequences while defining scene map with keyframes. These methods assume static scenes, fail to produce the cameras in scenes with large portions of dynamic motion, and cannot reproduce dynamic parts of the scene. DROID-SLAM [44] used synthetic data with ground truth 3D supervision for learning to predict camera poses via deep-based BA on keyframes while excluding dynamic objects. ParticleSfM [61] filters out 2D dynamic content for reproducing the cameras in dynamic scenes, using its pre-trained motion prediction network. Both, [44, 61] do not infer the dynamic 3D structure. Orthographic Non-Rigid SfM (NRSfM) Bregler et al. [6] introduced factorization method for computing non-rigid structure and rotation matrices from point track matrix, by assuming low dimensional basis model. While follow-up papers improved accuracies with different regularizations [23, 9, 33, 16] or neural representations [32, 21, 39], the orthographic camera model assumption is in general not valid for casual videos. Furthermore, these methods often assume background subtraction as preprocessing. Even though follow-up work proposed factorization solutions for pinhole cameras [14], its sensitivity to noise [17], makes it impractical for casual videos. Test-time optimization for dynamic scenes Recent methods [26, 22, 59, 60] finetuned the monocular depth estimation from pre-trained model [37, 36] using optical flow constraints [43], for obtaining consistent dense depth maps for monocular video. [59] further optimized motion maps for handling scenes with large dynamic motion. [52, 13] use depth from single image estimations, to improve novel view synthesis in dynamic scenes. [25] further optimizes for the unknown camera poses together with the dynamic radiance field optimization. [34, 35] model single deformable surface from monocular video by applying isometric constraints. LASR [54], ViSER [55] and BANMo [56] optimize for dynamic surface by assuming rigid bones and linear blend skinning weights. However, all the above-mentioned methods require per-scene optimization, resulting in slow inference. Recently, [40] presented the Common Pets in 3D (COP3D) dataset that contains casual, in-the-wild videos of pets, and used it to learn priors for novel view synthesis in dynamic scenes. Point tracking There has been recent advance in 2D point tracking by learning [18, 10], or optimization [49] techniques. Concurrently, [53] presented method for jointly performing 2D tracking and 3D lifting, by learning to track with depth supervision while applying an as-rigid-aspossible loss. However, their method cannot predict camera poses or identify static parts directly."
        },
        {
            "title": "5 Conclusions and limitations",
            "content": "We presented TRACKSTO4D, novel deep-learning framework that directly maps 2D motion tracks from casual videos into their corresponding dynamic structure and camera motion. Our approach features deep learning architecture that considers the symmetries in the problem with designed intrinsic constraints to handle the ill-posed nature of this problem. Notably, our network was trained using only raw supervision of 2D point tracks extracted by an off-the-shelf method [18] without any 3D supervision. Yet, it implicitly learned to predict camera poses and 3D structures while identifying the dynamic parts. During inference, our method demonstrates significantly faster processing times compared to previous methods while achieving comparable accuracy. Furthermore, our network exhibits strong generalization capabilities, performing well even on semantic categories that were not present in the training data. Limitations and future work. While our experiments demonstrated that our network is efficient, accurate, and capable of generalizing to unseen video categories, there are several limitations and future work directions that we would like to address. First, Our method cannot handle videos with too rapid motion, and in general, is limited by the accuracy of the tracking method [18]. We note that any future improvements with point tracking in terms of accuracy and inference time will directly improve our method as well. Our method assumes enough motion parallax to constrain the depth values and fails to generate accurate camera poses without it. future and interesting work would be to try combining depth-from-single-image prior as additional inputs to our network for handling cases with minimal motion parallax and improving accuracies. Acknowledgments HM is the Robert J. Shillman Fellow, and is supported by the Israel Science Foundation through personal grant (ISF 264/23) and an equipment grant (ISF 532/23)."
        },
        {
            "title": "References",
            "content": "[1] Agarwal, S., Furukawa, Y., Snavely, N., Simon, I., Curless, B., Seitz, S.M., Szeliski, R.: Building rome in day. Communications of the ACM 54(10), 105112 (2011) [2] Agarwal, S., Mierle, K., Team, T.C.S.: Ceres Solver (10 2023), https://github.com/ ceres-solver/ceres-solver [3] Akhter, I., Sheikh, Y., Khan, S., Kanade, T.: Nonrigid structure from motion in trajectory space. Advances in neural information processing systems 21 (2008) [4] Birkl, R., Wofk, D., Müller, M.: Midas v3.1 model zoo for robust monocular relative depth estimation. arXiv preprint arXiv:2307.14460 (2023) [5] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., Davison, A.J.: Codeslamlearning compact, optimisable representation for dense visual slam. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 25602568 (2018) [6] Bregler, C., Hertzmann, A., Biermann, H.: Recovering non-rigid 3d shape from image streams. In: Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662). vol. 2, pp. 690696. IEEE (2000) [7] Bronstein, M.M., Bruna, J., Cohen, T., Veliˇckovic, P.: Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 (2021) [8] Brynte, L., Iglesias, J.P., Olsson, C., Kahl, F.: Learning structure-from-motion with graph attention networks. arXiv preprint arXiv:2308.15984 (2023) [9] Dai, Y., Li, H., He, M.: simple prior-free method for non-rigid structure-from-motion factorization. International Journal of Computer Vision 107, 101122 (2014) [10] Doersch, C., Yang, Y., Vecerik, M., Gokay, D., Gupta, A., Aytar, Y., Carreira, J., Zisserman, A.: Tapir: Tracking any point with per-frame initialization and temporal refinement. arXiv preprint arXiv:2306.08637 (2023) [11] Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems 27 (2014) [12] Engel, J., Schöps, T., Cremers, D.: Lsd-slam: Large-scale direct monocular slam. In: European conference on computer vision. pp. 834849. Springer (2014) [13] Gao, C., Saraf, A., Kopf, J., Huang, J.B.: Dynamic view synthesis from dynamic monocular video. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 57125721 (2021) [14] Hartley, R., Vidal, R.: Perspective nonrigid shape and motion recovery. In: Computer Vision ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part 10. pp. 276289. Springer (2008) [15] Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge university press (2003) [16] Iglesias, J.P., Olsson, C., Valtonen Örnhag, M.: Accurate optimization of weighted nuclear norm for non-rigid structure from motion. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16. pp. 2137. Springer (2020) [17] Jensen, S.H.N., Doest, M.E.B., Aanæs, H., Del Bue, A.: benchmark and evaluation of non-rigid structure from motion. International Journal of Computer Vision 129(4), 882899 (2021) [18] Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., Rupprecht, C.: Cotracker: It is better to track together. arXiv preprint arXiv:2307.07635 (2023) [19] Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems 30 (2017) [20] Kingma, D.P., Ba, J.: Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) [21] Kong, C., Lucey, S.: Deep non-rigid structure from motion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15581567 (2019) [22] Kopf, J., Rong, X., Huang, J.B.: Robust consistent video depth estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16111621 (2021) [23] Kumar, S., Van Gool, L.: Organic priors in non-rigid structure from motion. In: European Conference on Computer Vision. pp. 7188. Springer (2022) [24] Li, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time view synthesis of dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 64986508 (2021) [25] Liu, Y.L., Gao, C., Meuleman, A., Tseng, H.Y., Saraf, A., Kim, C., Chuang, Y.Y., Kopf, J., Huang, J.B.: Robust dynamic radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1323 (2023) [26] Luo, X., Huang, J.B., Szeliski, R., Matzen, K., Kopf, J.: Consistent video depth estimation. ACM Transactions on Graphics (ToG) 39(4), 711 (2020) [27] Maron, H., Litany, O., Chechik, G., Fetaya, E.: On learning sets of symmetric elements. In: International conference on machine learning. pp. 67346744. PMLR (2020) [28] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM 65(1), 99106 (2021) [29] Moran, D., Koslowsky, H., Kasten, Y., Maron, H., Galun, M., Basri, R.: Deep permutation equivariant structure from motion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 59765986 (2021) [30] Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics 31(5), 11471163 (2015) [31] Newcombe, R.A., Lovegrove, S.J., Davison, A.J.: Dtam: Dense tracking and mapping in real-time. In: 2011 international conference on computer vision. pp. 23202327. IEEE (2011) [32] Novotny, D., Ravi, N., Graham, B., Neverova, N., Vedaldi, A.: C3dpo: Canonical 3d pose networks for non-rigid structure from motion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 76887697 (2019) [33] Oskarsson, M., Batstone, K., Astrom, K.: Trust no one: Low rank matrix factorization using hierarchical ransac. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 58205829 (2016) [34] Parashar, S., Pizarro, D., Bartoli, A.: Isometric non-rigid shape-from-motion with riemannian geometry solved in linear time. IEEE transactions on pattern analysis and machine intelligence 40(10), 24422454 (2017) [35] Parashar, S., Pizarro, D., Bartoli, A.: Robust isometric non-rigid structure-from-motion. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(10), 64096423 (2021) [36] Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction. ICCV (2021) [37] Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., Koltun, V.: Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(3) (2022) 11 [38] Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 41044113 (2016) [39] Sidhu, V., Tretschk, E., Golyanik, V., Agudo, A., Theobalt, C.: Neural dense non-rigid structure from motion with latent space constraints. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVI 16. pp. 204222. Springer (2020) [40] Sinha, S., Shapovalov, R., Reizenstein, J., Rocco, I., Neverova, N., Vedaldi, A., Novotny, D.: Common pets in 3d: Dynamic new-view synthesis of real-life deformable categories. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 48814891 (2023) [41] Sturm, P., Triggs, B.: factorization based algorithm for multi-image projective structure and motion. In: Computer VisionECCV96: 4th European Conference on Computer Vision Cambridge, UK, April 1518, 1996 Proceedings Volume II 4. pp. 709720. Springer (1996) [42] Teed, Z., Deng, J.: Deepv2d: Video to depth with differentiable structure from motion. arXiv preprint arXiv:1812.04605 (2018) [43] Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16. pp. 402419. Springer (2020) [44] Teed, Z., Deng, J.: Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems 34, 1655816569 (2021) [45] Tomasi, C., Kanade, T.: Shape and motion from image streams under orthography: factorization method. International journal of computer vision 9, 137154 (1992) [46] Triggs, B., McLauchlan, P.F., Hartley, R.I., Fitzgibbon, A.W.: Bundle adjustmenta modern synthesis. In: Vision Algorithms: Theory and Practice: International Workshop on Vision Algorithms Corfu, Greece, September 2122, 1999 Proceedings. pp. 298372. Springer (2000) [47] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) [48] Wan, Z., Li, Z., Tian, M., Liu, J., Yi, S., Li, H.: Encoder-decoder with multi-level attention for 3d human shape and pose estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1303313042 (2021) [49] Wang, Q., Chang, Y.Y., Cai, R., Li, Z., Hariharan, B., Holynski, A., Snavely, N.: Tracking everything everywhere all at once. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1979519806 (2023) [50] Wang, W., Hu, Y., Scherer, S.: Tartanvo: generalizable learning-based vo. In: Conference on Robot Learning. pp. 17611772. PMLR (2021) [51] Wu, C.: Towards linear-time incremental structure from motion. In: 2013 International Conference on 3D Vision-3DV 2013. pp. 127134. IEEE (2013) [52] Xian, W., Huang, J.B., Kopf, J., Kim, C.: Space-time neural irradiance fields for free-viewpoint video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 94219431 (2021) [53] Xiao, Y., Wang, Q., Zhang, S., Xue, N., Peng, S., Shen, Y., Zhou, X.: Spatialtracker: Tracking any 2d pixels in 3d space. arXiv preprint arXiv:2404.04319 (2024) [54] Yang, G., Sun, D., Jampani, V., Vlasic, D., Cole, F., Chang, H., Ramanan, D., Freeman, W.T., Liu, C.: Lasr: Learning articulated shape reconstruction from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1598015989 (2021) 12 [55] Yang, G., Sun, D., Jampani, V., Vlasic, D., Cole, F., Liu, C., Ramanan, D.: Viser: Video-specific surface embeddings for articulated 3d shape reconstruction. Advances in Neural Information Processing Systems 34, 1932619338 (2021) [56] Yang, G., Vo, M., Neverova, N., Ramanan, D., Vedaldi, A., Joo, H.: Banmo: Building animatable 3d neural models from many casual videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 28632873 (2022) [57] Yang, N., Stumberg, L.v., Wang, R., Cremers, D.: D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 12811292 (2020) [58] Yoon, J.S., Kim, K., Gallo, O., Park, H.S., Kautz, J.: Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 53365345 (2020) [59] Zhang, Z., Cole, F., Li, Z., Rubinstein, M., Snavely, N., Freeman, W.T.: Structure and motion from casual videos. In: European Conference on Computer Vision. pp. 2037. Springer (2022) [60] Zhang, Z., Cole, F., Tucker, R., Freeman, W.T., Dekel, T.: Consistent depth of moving objects in video. ACM Transactions on Graphics (TOG) 40(4), 112 (2021) [61] Zhao, W., Liu, S., Guo, H., Wang, W., Liu, Y.J.: Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In: European Conference on Computer Vision. pp. 523542. Springer (2022) [62] Zhao, W., Liu, S., Shu, Y., Liu, Y.J.: Towards better generalization: Joint depth-pose learning without posenet. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 91519161 (2020) [63] Zhou, H., Ummenhofer, B., Brox, T.: Deeptam: Deep tracking and mapping. In: Proceedings of the European conference on computer vision (ECCV). pp. 822838 (2018) [64] Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation representations in neural networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 57455753 (2019) [65] Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., Lee, Y.J.: Segment everything everywhere all at once. Advances in Neural Information Processing Systems 36 (2024)"
        },
        {
            "title": "A Supplementary results",
            "content": "A.1 Video results We provide supplementary video outputs of several cases from our test set. Each video presents the input video frames with set of pre-extracted point tracks that are used as input to our network and presented in corresponding colors (left side), and the output cameras and dynamic 3D structure (right side). The output camera trajectory is presented as gray frustums, whereas the current camera is marked in red. The reconstructed 3D scene points are presented in corresponding colors to the input tracks. Note that the outputs presented in the videos were obtained at inference time, with single feed-forward prediction, without any optimization or fine-tuning, on unseen test cases. A.2 Extended quantitative evaluation Per-sequence and mean quantitative comparisons for our 21 pet test videos are presented in Tab. 5 and Tab. 6. Tables with similar structure for the out-of-domain dataset are presented in Tab. 7 and Tab. 8. 13 Grid ATE (mm) size RPE Trans (mm) RPE Rot (deg) Inference Time Ours (cats only) Ours (cats only)+BA Ours (cats only) Ours (cats only)+BA Ours (cats only) Ours (cats only)+BA Ours (cats only) Ours (cats only)+BA Ours (cats only) Ours (cats only)+BA 15 15 12 12 10 10 7 7 5 5 8.96 4.22 9.18 4.36 8.91 4.44 9.06 4.93 10.29 8. 3.79 2.86 3.81 2.97 3.91 3.01 4.11 3.52 4.97 6.33 0.23 0.17 0.24 0.17 0.23 0.18 0.25 0.20 0.31 0.38 0.16(+8.6) seconds 0.40(+8.6) seconds 0.09(+7.8) seconds 0.24(+7.8) seconds 0.05(+7.7) seconds 0.16(+7.7) seconds 0.02(+7.6) seconds 0.08(+7.6) seconds 0.01(+7.6) seconds 0.05(+7.6) seconds Table 4: Tracking Grid Size Effect Quantitative evaluation of the effect of reducing the number of sampled point tracks at inference time. We measure the camera pose accuracy and the running time. We also mention the point tracks extraction time in parenthesis (e.g. +8.6 seconds) which is performed by [18] as preprocess. As can be seen, our method can handle smaller number of points but the accuracy slightly drops with fewer sampled points"
        },
        {
            "title": "B Implementation details",
            "content": "B.1 Architecture technical details For learning high frequencies we map each input coordinate to sinusoidal functions as in [28] with = 12. We use 3 pairs of attention layers, each of frames attention followed by point attention. Each point (after sinusoidal functions embedding) is mapped into R256 with linear layer. Each attention layer is function of the form : RN 256 RN 256 (see details above). Each attention uses 16 heads with K, Q, R(N or )64 followed by fully connected network with 1 hidden layer of 2048 features. We then average over the rows to get per-point features P0 RP 256 and over the columns to get per-frame features F0 RN 256. Finally, we map P0 to per-point outputs P1 RP (3K+1) ( basis points and γ) with linear layer, and F0 into per-camera outputs F1 RN (6+3+K1) (6 for the rotation parameters [64], 3 for the camera center, and 1 linear coefficients) using convolutional layer with kernel size of 31. B.2 Training details In total, we used 733 cat videos and 753 dog videos for training. We trained our networks for 7000 and 3500 epochs for the single-class and multi-class setups respectively. Training our method lasts about one week on single Tesla V100 GPU with 32GB memory. We used the Adam optimizer [20] with learning rate of 104. Our method assumes known camera internal parameters which are provided by the dataset and used to normalize the point tracks as preprocessing step. B.3 Other implementation details Point tracks sampling For building RN 3 we use the implementation of [18]. We sample uniform grid of 15 15 2D points, starting from frame number 0, 20, 40, . . . , and then track these points throughout the entire video (backward and forward). In Tab. 4 we show the effect of reducing the grid size at inference time, in terms of camera pose accuracy and running time. During training, at each iteration, we randomly sample 20-50 frames from the training videos and 100 point tracks, i.e. 20 50 and = 100. When sampling cameras and point tracks of size 3 from larger tensor of size 3 we only take point track if its starting tracking time is in the range [t 2 ], where is the first sampled index. At inference time we take all the available point tracks. In both, training and inference time, we keep only point tracks that are observed in more than 10 frames. 2 , + 3N Finetuning details For our fine-tuning (FT) in the main paper, we applied per-sequence fine-tunning of 500,100 iterations starting from our final checkpoint, for pets,out-of-domain data respectively. The 14 fine-tuning is done as post-processing by minimizing the original loss function on the given test video. Test Set We used the RGBD camera of the iPhone 11 to record our 21 test videos of dogs and cats. Each frame has resolution of 640 480 pixels. Note that the training set contained various types of resolutions. All pet owners who were photographed gave their permission for the animals to be photographed. For evaluation only, we define point track as dynamic if its associated GT mask value is 1 for at least 40 frames. The GT masks are obtained by running [65] and searching for labels of dogs and cats. They were only used for evaluation and not used by our method at all. We verified that this data includes enough dynamic motion, by also including several videos that COLMAP failed to reconstruct without the masks. We further verified manually that the camera trajectories look reasonable. The out-of-domain dataset contains video sequences with 24 frames, each of resolution of 546 288. The GT dynamic masks are provided by the dataset. For this dataset, for evaluation only, we define point track as dynamic if its associated GT mask value is 1 for at least 15 frames. Bundle Adjustment After inference, as optional refinement, we take the output static approximation B1 RP 3 and the output camera poses {R1, . . . , RN }, {t1, . . . , tN } and apply Bundle Adjustment (BA). We use 3D world point from B1 if its associated γ is below 0.008 for the pets dataset and 0.005 for out-of-domain dataset. We optimize reprojection errors of given observation Mi,j, only if it is observed, i.e. i,j = 1 and if the initial reprojection error is below 10 pixels. We use the BA implementation provided by [29], which is based on the Ceres package [2]. Running times All inference running times were computed on machine with NVIDIA RTX A6000 GPU and Intel(R) Core(TM) i7-9800X 3.80GHz CPU. Extracting point tracks with [18] took 8.6 and 2.5 seconds for each video on the pet-videos and out-of-domain videos respectively and included in the running time tables as part of our method inference time. In all training setups, we used: λReprojection = 50.0, λStatic = 1.0, Training technical details λNegative = 1.0, λSparse = 0.001. At the beginning of the training, we pre-train the camera poses to be located behind and facing the origin. This prevents cases in which the cameras are located in the middle of the initial point cloud s.t. many points have negative depths, which may result in bad convergence. More specifically, the pre-train loss is: LPretrain = 1 + Ri I2 . The pretrain runs until convergence (LPretrain < 104). During the main training, we detach gradients from B1 and (R1, t1) . . . , (RN , tN ) for LReproject to stabilize the training. Until epoch 50 we sample sequences of in the range [20, 22], and then we increase the range to [20, 50]. (cid:13)ti [0, 0, 15]T (cid:13) (cid:13) 2 (cid:13) 1 100 (cid:80)N i=1 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All Dyn All RCVD [22] 0.11 1.90E+08 0.94 0.71 1.00 0.85 1.00 0.89 0.29 0.19 0.45 0.68 0.85 0.93 0.99 1.00 0.54 0.20 0.20 0.66 0.52 0.89 0.91 0.98 0.79 0.22 0.10 0.76 MiDaS[4] 0.12 8.80E+05 0.87 0.74 1.00 0.87 1.00 0.91 0.16 0.18 0.75 0.76 0.99 0.95 1.00 0.99 0.10 0.55 0.94 0.66 1.00 0.75 1.00 0.80 0.25 0.24 0.53 0. Seq0 Seq1 Seq2 Seq3 CasualSAM[59] 0.05 0.11 1.00 0.96 1.00 0.97 1.00 0.97 0.16 0.09 0.81 0.92 0.89 0.96 1.00 1.00 0.06 0.06 0.99 0.98 0.99 0.99 1.00 0.99 0.07 0.06 0.97 0.97 Ours (C&D) 0.05 0.06 0.99 0.98 1.00 1.00 1.00 1.00 0.14 0.09 0.81 0.90 1.00 1.00 1.00 1.00 0.06 0.07 0.99 0.96 1.00 0.99 1.00 0.99 0.15 0.09 0.95 0.91 Ours (C&D) FT 0.06 0.06 0.98 0.97 1.00 0.99 1.00 1.00 0.12 0.07 0.86 0.92 1.00 1.00 1.00 1.00 0.03 0.06 0.99 0.98 1.00 0.99 1.00 0.99 0.05 0.06 0.99 0.97 Our (C) 0.05 0.06 0.99 0.98 1.00 1.00 1.00 1.00 0.13 0.09 0.90 0.94 0.99 1.00 1.00 1.00 0.03 0.06 0.99 0.97 1.00 0.99 1.00 0.99 0.07 0.08 0.97 0.92 Our (C) FT 0.06 0.06 0.98 0.97 1.00 0.99 1.00 1.00 0.11 0.07 0.87 0.92 1.00 1.00 1.00 1.00 0.04 0.06 0.99 0.98 1.00 0.99 1.00 0.99 0.05 0.06 0.98 0.97 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1."
        },
        {
            "title": "Dyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn",
            "content": "Seq4 Seq5 Seq6 Seq7 Seq8 Seq Seq10 Seq11 Seq12 Seq13 Seq14 0.27 0.85 0.59 0.92 0.31 0.17 0.44 0.80 0.87 0.97 1.00 1.00 0.09 0.12 0.97 0.86 1.00 0.97 1.00 0.99 0.35 0.14 0.47 0.83 0.65 0.91 0.99 0.99 0.39 0.22 0.45 0.65 0.68 0.86 0.93 0.97 0.47 0.20 0.18 0.72 0.69 0.91 0.97 0.99 0.86 0.33 0.05 0.65 0.28 0.78 0.68 0.90 0.08 0.11 0.99 0.92 1.00 0.99 1.00 1.00 0.34 0.17 0.47 0.74 0.86 0.94 0.94 0.99 0.37 0.16 0.19 0.73 0.97 0.98 1.00 1.00 0.31 3.31E+08 0.50 0.67 0.81 0.81 1.00 0.87 0.18 0.18 0.79 0.76 0.93 0.94 0. 0.99 0.92 1.00 0.95 0.08 0.27 0.98 0.65 1.00 0.90 1.00 0.94 0.08 3.75E+05 0.98 0.86 1.00 0.95 1.00 0.98 0.10 0.15 0.94 0.87 0.99 0.95 0.99 0.98 0.08 0.17 0.95 0.80 1.00 0.92 1.00 0.98 0.23 3.80E+04 0.68 0.64 0.97 0.84 1.00 0.92 0.26 0.20 0.59 0.74 0.85 0.91 1.00 0.98 0.06 0.24 1.00 0.73 1.00 0.87 1.00 0.95 0.14 0.38 0.79 0.67 1.00 0.78 1.00 0.86 0.16 0.13 0.79 0.85 1.00 0.98 1.00 1.00 0.29 0.35 0.50 0.69 0.84 0.87 1.00 0.93 0.17 0.34 0.72 0.55 0.97 0.83 1.00 1.00 0.99 1.00 0.99 0.06 0.09 1.00 0.96 1.00 0.98 1.00 0.99 0.05 0.03 1.00 0.98 1.00 1.00 1.00 1.00 0.04 0.05 0.98 0.96 0.99 0.99 1.00 1.00 0.09 0.06 0.92 0.97 0.99 0.99 1.00 1.00 0.05 0.03 0.99 0.99 1.00 1.00 1.00 1.00 0.18 0.09 0.80 0.92 0.95 0.98 1.00 1.00 0.02 0.03 1.00 1.00 1.00 1.00 1.00 1.00 0.06 0.05 0.92 0.97 0.99 0.99 1.00 1.00 0.05 0.05 0.98 0.98 1.00 1.00 1.00 1.00 0.09 0.06 0.93 0.97 1.00 0.99 1.00 1.00 0.03 0.03 0.99 1.00 1.00 1.00 1.00 16 1.00 0.98 1.00 0.99 0.27 0.12 0.54 0.84 0.88 0.98 1.00 1.00 0.07 0.08 0.99 0.91 1.00 0.97 1.00 0.99 0.05 0.04 0.99 0.98 1.00 0.99 1.00 1.00 0.15 0.10 0.84 0.86 0.98 0.98 1.00 0.99 0.09 0.08 0.97 0.89 1.00 0.99 1.00 1.00 0.22 0.21 0.46 0.55 0.91 0.89 1.00 0.97 0.02 0.03 1.00 1.00 1.00 1.00 1.00 1.00 0.10 0.08 0.90 0.90 0.96 0.99 0.99 1.00 0.10 0.08 0.98 0.95 1.00 0.99 1.00 1.00 0.15 0.10 0.82 0.91 1.00 0.98 1.00 0.99 0.05 0.04 0.99 0.99 1.00 1.00 1.00 1.00 0.99 1.00 0.99 0.15 0.11 0.75 0.92 1.00 0.99 1.00 1.00 0.07 0.05 0.98 0.97 1.00 1.00 1.00 1.00 0.03 0.05 0.99 0.96 1.00 0.98 1.00 0.99 0.11 0.06 0.91 0.96 0.98 0.99 0.99 1.00 0.05 0.04 0.99 0.99 1.00 1.00 1.00 1.00 0.17 0.09 0.63 0.88 1.00 0.99 1.00 1.00 0.02 0.02 1.00 1.00 1.00 1.00 1.00 1.00 0.07 0.05 0.92 0.96 0.98 0.99 1.00 1.00 0.05 0.06 0.97 0.94 1.00 0.98 1.00 1.00 0.11 0.07 0.92 0.96 1.00 0.99 1.00 0.99 0.04 0.03 0.99 0.99 1.00 1.00 1.00 1.00 0.98 1.00 0.99 0.19 0.10 0.65 0.89 0.99 1.00 1.00 1.00 0.06 0.06 0.99 0.96 1.00 0.98 1.00 1.00 0.04 0.05 0.99 0.97 1.00 0.99 1.00 1.00 0.12 0.09 0.90 0.91 0.98 0.98 1.00 0.99 0.09 0.07 0.98 0.97 1.00 1.00 1.00 1.00 0.22 0.22 0.45 0.52 0.85 0.86 1.00 0.97 0.02 0.04 1.00 0.99 1.00 1.00 1.00 1.00 0.10 0.07 0.90 0.94 0.97 0.99 0.99 1.00 0.09 0.08 0.98 0.95 1.00 0.99 1.00 1.00 0.14 0.09 0.85 0.93 1.00 0.99 1.00 0.99 0.04 0.03 0.99 0.99 1.00 1.00 1. 1.00 0.99 1.00 0.99 0.15 0.11 0.77 0.92 1.00 0.99 1.00 1.00 0.07 0.04 0.98 0.98 1.00 1.00 1.00 1.00 0.03 0.05 0.99 0.96 1.00 0.98 1.00 0.99 0.10 0.06 0.91 0.97 0.98 0.99 0.99 1.00 0.05 0.04 0.99 0.99 1.00 1.00 1.00 1.00 0.18 0.09 0.60 0.87 1.00 0.98 1.00 0.99 0.02 0.02 1.00 1.00 1.00 1.00 1.00 1.00 0.07 0.05 0.92 0.96 0.98 0.99 1.00 1.00 0.04 0.07 0.97 0.94 1.00 0.98 1.00 1.00 0.11 0.07 0.95 0.97 1.00 0.99 1.00 0.99 0.04 0.03 0.99 0.99 1.00 1.00 1.00 Seq15 Seq16 Seq17 Seq18 Seq Seq20 Mean STD Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1."
        },
        {
            "title": "All\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll",
            "content": "0.99 1.22 0.33 0.01 0.65 0.12 0.81 0.46 0.90 0.28 0.21 0.42 0.66 0.98 0.95 1.00 1.00 0.35 0.23 0.40 0.60 0.82 0.87 0.95 0.95 0.48 0.14 0.46 0.86 0.55 0.91 0.84 0.97 0.33 0.15 0.28 0.81 0.91 0.98 1.00 1.00 0.33 2.42E+08 0.34 0.50 0.93 0.77 0.99 0.83 0.40 3.63E+07 0.43 0.72 0.75 0.90 0.92 0.96 0.27 9.39E+07 0.29 0.10 0.26 0.07 0.15 0.05 0.92 0.12 0.39 0.77 0.65 1.00 0.83 1.00 0.89 0.12 0.21 0.92 0.79 1.00 0.93 1.00 0.96 0.12 0.15 0.88 0.81 0.99 0.94 1.00 0.98 0.21 0.32 0.60 0.65 0.95 0.84 1.00 0.92 0.14 0.27 0.80 0.74 0.97 0.87 0.99 0.92 0.27 0.45 0.49 0.42 0.90 0.72 0.99 0.85 0.16 6.16E+04 0.78 0.71 0.97 0.88 1.00 0.93 0.07 2.04E+05 0.17 0.11 0.05 0.07 0.00 0.05 1.00 0.15 0.09 0.95 0.97 0.99 0.99 0.99 0.99 0.07 0.05 0.98 0.99 1.00 1.00 1.00 1.00 0.14 0.08 0.76 0.89 0.98 0.98 1.00 1.00 0.10 0.05 0.93 0.98 0.98 0.99 0.99 1.00 0.06 0.03 0.97 0.99 1.00 1.00 1.00 1.00 0.21 0.04 0.61 0.97 1.00 1.00 1.00 1.00 0.09 0.06 0.93 0.97 0.99 0.99 1.00 1.00 0.05 0.02 0.10 0.03 0.02 0.01 0.00 0.01 1.00 0.27 0.15 0.62 0.80 0.93 0.97 1.00 0.99 0.14 0.06 0.81 0.96 1.00 1.00 1.00 1.00 0.11 0.07 0.93 0.94 0.99 0.98 0.99 0.99 0.12 0.10 0.87 0.91 1.00 0.98 1.00 0.99 0.04 0.03 0.97 0.99 1.00 1.00 1.00 1.00 0.27 0.05 0.44 0.95 1.00 1.00 1.00 1.00 0.12 0.08 0.85 0.91 0.98 0.98 1.00 1.00 0.08 0.04 0.18 0.10 0.03 0.02 0.00 0.01 1.00 0.10 0.09 0.96 0.94 0.99 0.99 1.00 0.99 0.11 0.06 0.88 0.97 1.00 1.00 1.00 1.00 0.12 0.06 0.89 0.94 0.98 0.99 0.99 1.00 0.07 0.09 0.96 0.96 0.99 0.98 1.00 0.98 0.05 0.03 0.97 0.99 0.99 1.00 1.00 1.00 0.29 0.05 0.37 0.95 1.00 1.00 1.00 1.00 0.09 0.06 0.90 0.96 1.00 0.99 1.00 1.00 0.06 0.02 0.15 0.03 0.01 0.01 0.00 0.00 1.00 0.18 0.18 0.79 0.70 0.96 0.92 1.00 0.99 0.14 0.06 0.80 0.96 1.00 1.00 1.00 1.00 0.11 0.07 0.91 0.93 0.98 0.99 0.99 1.00 0.11 0.08 0.88 0.95 0.99 0.98 1.00 0.99 0.05 0.05 0.98 0.99 0.99 1.00 1.00 1.00 0.26 0.05 0.49 0.95 1.00 1.00 1.00 1.00 0.11 0.08 0.88 0.92 0.99 0.98 1.00 1.00 0.07 0.04 0.16 0.11 0.03 0.03 0.00 0. 1.00 0.10 0.09 0.97 0.95 0.99 0.99 1.00 0.99 0.11 0.06 0.90 0.97 1.00 1.00 1.00 1.00 0.12 0.07 0.88 0.94 0.98 0.99 0.99 1.00 0.07 0.09 0.96 0.96 0.99 0.98 1.00 0.98 0.05 0.03 0.97 0.99 0.99 1.00 1.00 1.00 0.30 0.05 0.34 0.95 1.00 1.00 1.00 1.00 0.09 0.06 0.90 0.96 1.00 0.99 1.00 1.00 0.06 0.02 0.16 0.03 0.01 0.01 0.00 0.00 Table 5: Depth accuracy, pet test set. We show comparison to previous methods on the predicted depth for the point trajectories compared to their GT depths. We compare 4 ways of running our method. Ours (C&D), Ours (C): Our inference time outputs for model that was trained on cats and dogs or only on cats respectively. Ours (C&D) FT, Ours (C) FT: The outputs of our model trained on cats and dogs or cats respectively, after fine-tuning our losses for each specific video. As can be seen, fine-tuning improves our accuracy even more. 17 DROID-SLAM[44] ParticleSfM[61] RCVD[22] CasualSAM[59] Seq0 Seq1 Seq2 Seq3 Seq Seq5 Seq6 Seq7 Seq8 Seq9 Seq Seq11 Seq12 Seq13 Seq14 Seq15 Seq Seq17 Seq18 Seq19 Seq20 Mean"
        },
        {
            "title": "STD",
            "content": "ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) 3.71 3.05 0.14 1.91 1.32 0.18 3.13 3.62 0.08 5.13 5.76 0.17 2.59 2.05 0.11 1.07 1.02 0.09 26.08 10.57 0.66 2.25 2.34 0.15 1.23 1.07 0.07 21.74 9.04 0.62 1.47 1.84 0.22 1.71 1.65 0.08 1.70 2.24 0.09 1.23 1.19 0.13 5.42 3.40 0.18 7.69 7.95 0.22 5.04 4.53 0.28 1.12 1.15 0.10 2.98 4.05 0.23 1.45 1.65 0.11 8.13 6.20 0.36 5.08 3.60 0.20 6.63 2.80 0.16 6.10 3.22 0.18 3.83 1.49 0.23 4.68 5.82 0.21 2.26 2.00 0.07 4.38 2.07 0.11 0.83 0.74 0.07 31.07 11.01 0.67 28.48 6.13 0.59 38.79 50.57 4.78 18.52 7.96 0.47 1.75 1.15 0.12 3.60 1.54 0.08 3.64 2.63 0.12 2.38 1.40 0.14 5.15 3.38 0.19 61.06 17.57 0.58 5.06 4.54 0.29 34.07 12.30 1.11 6.25 5.21 0.29 2.23 1.72 0.11 4.37 3.57 0.20 12.79 6.95 0.51 16.32 10.88 1.01 64.67 26.92 2.53 38.44 25.23 2.43 39.46 27.27 1.89 29.55 24.18 2.38 56.16 21.36 1.05 14.22 6.51 1.63 48.31 24.21 4.03 47.25 23.81 2.29 44.06 25.46 1.73 43.45 21.35 3.47 22.49 24.22 2.18 19.10 16.34 1.80 20.82 18.50 2.03 33.49 17.33 2.12 1.05E+02 69.36 3.81 36.70 28.18 2.85 36.42 20.11 2.94 77.91 39.64 2.20 36.54 24.78 1.67 42.95 29.12 2.09 66.03 27.34 1.35 43.95 25.77 2.31 21.22 11.80 0.76 5.36 3.13 0.16 10.28 3.00 0.67 2.13 1.97 0.06 2.49 2.31 0.08 2.65 1.74 0.09 0.53 0.52 0.05 26.12 10.31 0.62 1.78 1.72 0.11 2.00 1.24 0.09 34.93 21.06 0.71 1.40 1.11 0.10 1.32 1.21 0.06 2.51 2.74 0.12 1.49 1.00 0.12 24.95 9.45 0.65 7.40 6.93 0.19 3.69 4.02 0.25 2.08 1.18 0.08 4.91 3.89 0.21 1.82 1.44 0.09 4.95 3.00 0.18 6.90 3.95 0.22 9.55 4.74 0.22 Ours (C) 5.60 3.63 0.22 13.21 2.65 0.29 4.57 3.24 0.12 7.83 3.13 0.13 4.21 2.40 0.15 1.44 0.88 0.12 27.10 10.44 0.69 4.53 2.32 0.20 4.78 1.54 0.13 24.15 6.89 0.37 3.03 1.87 0.21 3.12 3.12 0.12 6.23 4.04 0.23 4.10 1.72 0.19 6.09 4.07 0.26 36.51 10.19 0.30 4.52 4.47 0.31 6.52 2.14 0.17 7.73 4.34 0.24 8.51 3.23 0.25 4.30 3.30 0.20 8.96 3.79 0.23 9.06 2.52 0.13 Ours (C)+BA Ours (C)+FT 5.43 3.04 0.15 4.32 1.72 0.19 2.78 2.62 0.09 2.66 2.27 0.07 2.38 2.05 0.11 0.79 0.68 0.07 28.28 10.92 0.69 2.39 1.98 0.16 0.84 0.71 0.06 3.92 2.77 0.12 1.42 1.25 0.12 1.66 1.75 0.08 3.61 3.02 0.15 1.17 1.12 0.12 5.06 3.57 0.20 4.98 5.86 0.16 4.11 4.25 0.28 2.51 1.50 0.13 4.13 3.62 0.21 2.79 2.21 0.16 3.43 3.05 0.18 4.22 2.86 0.17 5.68 2.22 0.13 4.42 2.89 0.14 1.76 1.32 0.16 2.53 2.49 0.09 2.92 2.30 0.07 2.28 2.04 0.11 0.75 0.68 0.06 28.98 10.82 0.68 2.25 1.99 0.16 0.89 0.70 0.05 3.22 2.08 0.10 1.46 1.22 0.12 1.49 1.55 0.07 2.54 2.84 0.14 1.28 1.09 0.12 4.93 3.41 0.19 5.41 5.72 0.15 3.81 4.15 0.27 2.86 1.55 0.13 3.86 3.65 0.21 2.36 2.01 0.15 4.06 3.06 0.18 4.00 2.74 0.16 5.87 2.22 0.13 Table 6: Camera poses accuracy for pets. We show comparison to previous methods on the predicted camera poses. We compare 3 ways of running our method. Ours (C): Our inference time outputs (total inference time of 0.16 seconds) for model that was trained only on cats. Ours (C)+BA: Our inference time outputs, followed by short Bundle Adjustment (total inference time of 0.4 seconds) for model that was trained only on cats. Ours (C) FT: The outputs of the model that was trained only on cats after fine-tuning our losses for each specific video (total running time of about 5 minutes). As can be seen, after BA, our results are the most accurate compared to the other methods, and fine-tuning improves our accuracy even more. Balloon1 Abs Rel δ < 1.25 δ < 1.252 Dyn All Dyn All Dyn RCVD [22] 0.21 0.14 0.42 0.62 1. MiDaS[4] 0.12 0.34 0.89 0.72 0.98 CasualSAM[59] 0.04 0.02 0.98 0.99 0.99 Ours (C&D) 0.09 0.06 0.98 0.99 0.99 Ours (C&D) FT 0.03 0.01 0.98 0.99 0.99 18 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1.253 Abs Rel δ < 1.25 δ < 1.252 δ < 1."
        },
        {
            "title": "All\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll\nDyn\nAll",
            "content": "1.00 1.00 1.00 0.10 0.14 0.97 0.83 1.00 1.00 1.00 1.00 0.14 0.05 0.94 0.98 1.00 1.00 1.00 1.00 0.17 0.12 0.77 0.86 1.00 1.00 1.00 1.00 0.35 0.30 0.36 0.44 0.61 0.71 0.67 0.82 0.16 0.10 0.89 0.92 1.00 1.00 1.00 1.00 0.32 2.06E+06 0.26 0.50 0.99 0.88 1.00 1.00 0.08 0.10 0.91 0.84 1.00 1.00 1.00 1.00 0.19 2.58E+05 0.69 0.75 0.95 0.95 0.96 0.98 0.10 7.28E+05 0.29 0.20 0.14 0.10 0.12 0.06 0.81 0.99 0.88 2.21E+05 4.87E+05 0.95 0.76 0.99 0.86 0.99 0.93 0.55 4.75E+04 0.03 0.67 0.04 0.82 0.18 0.86 0.43 0.59 0.07 0.22 0.30 0.38 0.75 0.64 0.52 7.67E+03 0.49 0.59 0.72 0.78 0.83 0.87 0.24 1.09 0.59 0.29 0.95 0.42 0.97 0.53 0.13 0.22 0.81 0.71 0.99 0.94 1.00 0.99 0.51 1.62E+06 0.87 0.68 0.91 0.71 0.92 0.72 2.76E+04 2.70E+05 0.59 0.58 0.73 0.72 0.83 0.80 7.81E+04 5.69E+05 0.37 0.21 0.37 0.21 0.28 0.16 Balloon"
        },
        {
            "title": "Jumping",
            "content": "Playground Skating Truck Umbrella Mean"
        },
        {
            "title": "STD",
            "content": "1.00 1.00 1.00 0.04 0.01 1.00 1.00 1.00 1.00 1.00 1.00 0.01 0.01 0.99 1.00 1.00 1.00 1.00 1.00 0.05 0.02 0.99 0.99 1.00 1.00 1.00 1.00 0.08 0.07 0.93 0.96 0.99 0.98 1.00 0.99 0.15 0.02 0.76 0.99 0.97 1.00 1.00 1.00 0.03 0.03 0.99 1.00 1.00 1.00 1.00 1.00 0.02 0.02 1.00 1.00 1.00 1.00 1.00 1.00 0.05 0.03 0.95 0.99 0.99 1.00 1.00 1.00 0.05 0.02 0.08 0.01 0.01 0.01 0.00 0.00 1.00 1.00 1.00 0.04 0.06 0.99 0.97 1.00 1.00 1.00 1.00 0.15 0.06 0.98 1.00 0.98 1.00 1.00 1.00 0.07 0.05 0.96 0.97 1.00 1.00 1.00 1.00 0.16 0.15 0.64 0.78 0.98 0.91 0.98 0.98 0.12 0.05 0.93 1.00 0.97 1.00 1.00 1.00 0.14 0.14 0.94 0.79 1.00 1.00 1.00 1.00 0.03 0.05 1.00 0.99 1.00 1.00 1.00 1.00 0.10 0.08 0.93 0.94 0.99 0.99 1.00 1.00 0.05 0.04 0.12 0.10 0.01 0.03 0.01 0.01 1.00 1.00 1.00 0.03 0.01 1.00 1.00 1.00 1.00 1.00 1.00 0.01 0.01 0.98 1.00 1.00 1.00 1.00 1.00 0.07 0.04 0.96 0.97 0.99 1.00 1.00 1.00 0.16 0.08 0.89 0.94 0.94 0.97 0.94 0.98 0.10 0.04 0.93 0.99 0.97 1.00 0.97 1.00 0.08 0.05 1.00 1.00 1.00 1.00 1.00 1.00 0.02 0.02 1.00 1.00 1.00 1.00 1.00 1.00 0.06 0.03 0.97 0.99 0.99 1.00 0.99 1.00 0.05 0.02 0.04 0.02 0.02 0.01 0.02 0.01 Table 7: Depth accuracy for out-of-domain data [58]. We show comparison to previous methods on the predicted depth for the point trajectories compared to their GT depths. We compare 2 ways of running our method. Ours (C&D): Our inference time outputs for model that was trained on cats and dogs. Ours (C&D) FT: The outputs of our model trained on cats and dogs, after fine-tuning our losses for each specific video. As can be seen, fine-tuning improves our accuracy even more. DROID-SLAM[44] ParticleSfM[61] RCVD[22] CasualSAM[59] Balloon1 Balloon2 ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) 2.87 4.58 0.05 7. 5.81 6.51 0.08 13.51 1.7E+02 2.5E+02 3.35 3.5E+02 5.57 4.96 0.05 7.74 Ours (C) 21.47 27.73 0.40 41.25 Ours (C)+BA 4.17 6.76 0.07 10.22 Ours (C)+FT 4.14 6.76 0.07 9."
        },
        {
            "title": "STD",
            "content": "RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) ATE(mm) RPE T.(mm) RPE R.(deg.) 12.82 0.13 2.80 1.71 0.04 7.65 10.25 0.05 7.62 9.51 0.10 7.21 8.64 0.04 22.55 31.68 0.06 5.20 8.11 0.04 7.96 10.91 0.07 6.25 9.06 0.03 14.16 0.13 9.71 7.93 0.17 13.31 11.27 0.06 85.47 90.10 0.75 19.37 24.76 0.15 - - - 39.45 12.08 0.05 26.66 23.83 0.20 28.13 29.82 0.25 3.9E+02 3.21 1.1E+02 2.4E+02 3.34 2.8E+02 2.8E+02 3.09 1.1E+02 3.3E+02 4.69 78.24 3.2E+02 3.91 1.1E+02 3.6E+02 2.77 66.01 3.7E+02 3.05 1.6E+02 3.2E+02 3.43 1.0E+02 55.63 0.61 11.47 0.11 3.59 2.05 0.05 7.74 8.69 0.05 5.45 7.68 0.11 7.28 8.65 0.05 17.70 30.24 0.05 7.38 7.01 0.03 7.81 10.09 0.06 4.25 8.60 0.03 77.12 0.84 32.79 48.39 1.04 24.24 36.34 0.21 27.44 40.28 0.40 27.57 45.02 0.24 42.47 69.22 0.26 39.27 39.85 0.20 32.06 47.99 0.45 8.11 16.82 0. 16.80 0.17 4.11 3.20 0.07 8.38 11.35 0.07 6.47 8.00 0.10 9.21 11.19 0.07 19.49 34.61 0.05 7.33 6.98 0.03 8.67 12.36 0.08 4.89 9.86 0.04 16.78 0.17 3.73 3.16 0.06 8.61 12.13 0.08 5.06 6.42 0.10 8.88 11.44 0.07 17.53 28.37 0.05 5.99 8.03 0.03 7.98 11.64 0.08 4.50 7.95 0.04 Table 8: Camera poses accuracy for out-of-domain data [58]. We show comparison to previous methods on the predicted camera poses. We compare 3 ways of running our method. Ours (C): Our inference time outputs. Ours (C)+BA: Our inference time outputs, followed by short Bundle Adjustment for model that was trained only on cats. Ours (C) FT: The outputs of the model that was trained only on cats after fine-tuning our losses for each specific video"
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "Simon Fraser University",
        "Technion"
    ]
}