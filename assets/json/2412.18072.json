{
    "paper_title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks",
    "authors": [
        "Wan-Cyuan Fan",
        "Tanzila Rahman",
        "Leonid Sigal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions that maybe beyond the abilities of a naive user. To address these limitations, we introduce MMFactory, a universal framework that includes model and metrics routing components, acting like a solution search engine across various available models. Based on a task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visio-lingual tools from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering state-of-the-art solutions tailored to user problem specifications. Project page is available at https://davidhalladay.github.io/mmfactory_demo."
        },
        {
            "title": "Start",
            "content": "MMFactory: Universal Solution Search Engine for Vision-Language Tasks Wan-Cyuan Fan1,2 Tanzila Rahman1,2 Leonid Sigal1,2,3 1University of British Columbia 2Vector Institute for AI 3CIFAR AI Chair {wancyuan, trahman8, lsigal}@cs.ubc.ca 4 2 0 2 4 2 ] . [ 1 2 7 0 8 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "With advances in foundational and vision-language (VLM) models, and effective fine-tuning techniques, large number of both general and special-purpose models have been developed for variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions (e.g., code snippets for similar problems) that maybe beyond the abilities of naive user. To address these limitations, we introduce MMFactory, universal framework that includes model and metrics routing components, acting like solution search engine across various available models. Based on task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest diverse pool of programmatic solutions by instantiating and combining visio-lingual tools (e.g., detection, segmentation, VLMs) from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick solution that meets their unique design constraints. From the technical perspective, we also introduced committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering stateof-the-art solutions tailored to user problem specifications. 1. Introduction Large language models (LLMs), such as GPT [2] and Gemini [51, 52], have demonstrated powerful capabilities across various domains, significantly transforming how people approach their tasks and even their daily lives. Building on Illustration of MMFactory. Proposed MMFactory Figure 1. framework (a) contrasted with model routing approaches (c) and multimodal LLM with tools (b). Unlike both prior classes of methods, MMFactory proposes pool of programmatic solutions, composed of series of selected models from the pool, for given task while also benchmarking their performance and computational characteristics. See Section 1 for full discussion. these models, wide range of vision-language (VLM) or multimodal LLMs (MLLMs) [1, 5, 7, 35, 62] have been developed by integrating modality adapters and encoders into their frameworks. This advancement has resulted in stateof-the-art models capable of solving complex visual tasks. Despite the push for building AGI-like agents, that are all capable, even models like GPT-4o tend to be inferior, or lacking, on specific tasks [19, 31]. At the same time, with the development of fine-tuning techniques, customized or expert models tailored to specific tasks have become easier to develop. With different training data, fine-tuning approaches, and frameworks, models with varying specialties and characteristics are being introduced daily. One can imagine that in near future such models will be ubiquitous, 1 creating marketplace of agents with an overwhelming design choices for users to pick from and build on. In this scenario, routing approaches are needed that can take userdefined tasks, needs, and constraints, acting as search engine among all types of models, to provide suggested solutions for the user. Previous works in visual programming [22, 49] and multimodal language models (MLLMs) with tool integration [26, 38, 45] have explored using LLMs as planners to utilize external tools or APIs for solving complex visual tasks or to decompose tasks into sub-tasks. While these approaches have shown promise, there are several limitations to consider. First, existing methods assume single specialized tool for given sub-task (e.g., detection [36], segmentation [30], depth estimation [59]). This is overly simplistic, as variety of tools exist for any one sub-task, inculcating within particular family of models, that differ by backbone, number of parameters and overall performance. Second, these works generally overlook the users specific computation needs and constraints when generating solutions, resulting in inability to tailor solutions to particular hardware or deployment cost (e.g., user maybe willing to forgo 1% better performance if inference cost is reduced by 50%). Third, the proposed solutions are often tailored per specific example or scenario, which limits their generalization and applicability to other examples in the task, as shown in Fig. 1. Deployment of such solutions is problematic (e.g., no constant code path exist that maybe distilled to small model executable on an edge device). Addressing these limitations is essential for creating more versatile and user-centric framework for routing the solutions among different kinds of models in order to create custom agents capable of solving specific user problems in accordance to their specification. To address these challenges, in this work, we introduce MMFactory universal framework for automatic and programmatic development of task-specific agents. MMFactory (Fig. 1a) includes model and metric routing components; that, in combination, act as solution search engine for non-expert users. Based on task description (e.g., comparison of depth of points in an image), few sample input-output pairs (e.g., set of images with labeled points and which point is closest to camera in each), and (optionally) resource and/or performance constraints (e.g., compute limit), MMFactory can suggest diverse pool of programmatic solutions by instantiating and combining visual, LLM and VLM tools from its repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick solution that meets their unique design constraints. From the technical perspective, we also introduced committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Notably, unpublished and concurrent work of [41] also explores the idea of routing, but mainly for choosing single (most accurate) among the possible LLM / VLM models (see Fig. 1c). MMFactory framework is considerably more general and provides user with family of solutions and their performance characterization. In addition, our solutions, similar to visual programming [22, 49], are drawn from an exponential set of tools that can work in tandem with one another. Further, the fact that our framework proposes solutions that contain single executable code path, makes them much easier to deploy. Our contributions are multiple fold. First, to the best of our knowledge, this work is the first to explore routing across vision, language, and vision-language models. Second, our propose framework can provide multiple solutions in solution pool for user-defined tasks and constraints. Third, we introduce novel approach that combines routing and multi-agent solution proposer to deliver robust results. Fourth, unlike existing approaches, our proposed framework solves all instances of user-defined task collectively, rather than generating separate solutions for each instance. Fifth, experiments on two benchmarks demonstrate that our framework outperforms the state-of-the-art. 2. Related works Multimodal Large Language Models. Building on the recent success of large language models (LLMs) [4, 20, 42, 53], research trends have shifted toward enhancing these LLMs with multi-modal capabilities. Some of these MLLMs [34, 37, 39, 62] are created for general purpose, while others are designed for specific tasks, including coding [21, 27, 46], video understanding [13, 61], 3D [11, 24, 64], audio or speech [8, 15, 18], math [6, 54], scientific chart [17, 23, 40], and robotics [9, 60], which has demonstrated promising results. However, language-based models alone cant handle complex tasks very well. Multimodal models, which combine text and images, also face challenges, like misinterpreting context when information is split across text and visuals. They might connect unrelated details or miss important clues, leading to errors. Therefore, researchers are exploring tools and interactive systems to improve their understanding of multimodal information. Visual programming, LLMs with tools and Routing. As humans, when we face complex tasks, we decompose them into subtasks to understand them better or use tools to make them simpler. These concepts have been extended to neural networks, where previous works [3, 28] suggest that complex vision tasks are fundamentally compositional and can be divided into atomic perceptual units. Following this concept, visual programming [22, 49] and LLMs with tool [35, 44, 45] become prominent research 2 Figure 2. Overview of MMFactory. Our framework includes two primary components: Solution Router and Metric Router. The Solution Router generates pool of potential solutions for the task, while the Metric Router evaluates these solutions, estimating their performance and computational cost to generate performance curve. This curve enables users to select the model optimal for their task requirements. trends. Practically, visual programming focus on leveraging LLMs coding ability to decompose complex tasks into multi-step Python code with specialized vision tools. On the other hand, LLMs with tool use focus on teaching LLMs to use various types of tools to achieve image generation/editing [35], accessing web engines [38, 44], operating systems [43], etc. However, these methods have common problem that the multimodal modules are designed for specific tasks and cant be reused for similar ones. They also dont consider user constraints like model size, complexity, or preferences. This gave rise to routing-based approaches. In these approaches [25, 41, 47], router model can switch between stronger or weaker LLM during inference to balance cost with model performance. However, this method still needs the router LLM to be trained and cant offer versatile solutions based on user needs. It also relies on single model, which isnt enough to solve complex task efficiently. In contrast, our framework provides multiple options (i.e. solution pool) for users to choose from, and these options are versatile and can be reused across all instances of the task, rather than being limited to individual instance. From multi-modal Agents to multi-agent frameworks. Recently, due to the powerful reasoning, tool usage, and other capabilities of LLMs, these models have become essential building blocks in the development of artificial intelligence agents [10, 32, 48, 56] for many real-world applications, such as medicine [50], general tasks [12], and robotics [29]. Given the increasing complexity of tasks, an intuitive approach is to enhance the capabilities of agents by incorporating multiple agents into the task solving. Previous works have showcased that multi-agents conversations or debates can improve various capabilities, such as divergent thinking [33], factuality and reasoning abilities [16], and validation [57], and can even achieve automatic agent creation [12]. Among these, the most relevant to our work is AutoAgents [12], which introduces observers to monitor multi-agent conversations, helping ensure quality and coherence in responses. However, AutoAgents provides only one solution per prompt, while our approach offers multiple options with performance and cost details to help users choose the best fit. Additionally, AutoAgents relies on GPT-4s reasoning, limiting its flexibility with open-source models and restricting it to tasks like open-ended questions and creative writing. Our system, in contrast, supports any open-source model and can handle wide range of vision tasks. Most importantly, AutoAgents focus on dynamically creating multiple agents based on the task content and planning solutions. Our approach, however, focuses on solving tasks by routing different vision models and incorporating Python coding environment, which AutoAgents have not explored. 3. Methodology 3.1. Overview of MMFactory We introduce MMFactory, an universal framework designed not only to propose programmatic solutions based on user-defined task examples but also to provide estimated performance and time costs for each solution, allowing users to make informed choices. This framework functions like solution search engine and interface across various models, enabling access to models for task-solving without requiring extensive background knowledge. MMFactory has several unique features. In addition to proposing multiple solutions with estimated performance and cost plots, the solutions generated are general and can be applied across all examples within the specified task. Specifically, MMFactory consists of two key components: the Solution Router and Metric Router. The former can generate multiple general solutions for solving the task, while 3 3.2. Inputs Structure for Solution Router As mentioned in the previous section, our framework is designed to propose multiple solutions that leverage models in the model pool to solve the task. The challenging aspects of this task is that the router must not only understand the task but also comprehend the details of each model in the pool to ensure correct use in the solution. For such complex task, in addition to the initial task prompt, we have to provide extra details for the router, including definitions of the models in the model pool, requirements list, in-context examples, and the solution pool. For each task, the input prompt structure is detailed below (examples can also be found in the supplementary.) consisting of task-agnostic information: Model definitions d: Describes the details of each model in the model pool, including functionality, input arguments, return arguments, and example use cases. Requirements r: predefined list of requirements for the router to consider when generating solutions. In-context examples e: Following previous work [26], we provide four different output examples as references. Note that the in-context examples are not sampled from the user task O. Solution pool s: Showcases all previously generated If no solution exists, solutions (Python code only). EMPTY will be displayed. and user-specified task-specific instructions: User-specification u: Contains the task definition, example instances Oex sampled from the target task, and (optional) user constraints. Note that the task instances input includes images. User input is illustrated in Fig. 3. 3.3. Multi-agent solution router Taking all the aforementioned information as input, the goal of solution router is to propose novel solutions to solve the task at hand. To achieve that, inspired by multi-agent conversation works [16], we deploy multi-agent system for this complex problem. conversation is instantiated between two teams: the solution proposer team and the committee team. The proposer team generates ideas and solutions, while the committee team checks for correctness, redundancy, and alignment with requirements, providing feedback. Each team consists of members and leader. After gathering responses from their members, the leaders of two team exchange responses and collect feedback. By iteratively refining the solution based on this feedback, we achieve robust results. An illustration is provided in Fig. 4. We now detail each component and the conversation process within the multi-agent system. Please refer the supplement for example responses from all the agents in the solution Router. Solution Proposing Team. The solution proposing process Figure 3. Illustration of user specification inputs u. the later evaluate the solutions to estimate their performance and computation cost. The framework is illustrated in Fig. 2.1 Furthermore, we leverage advanced multimodal LLMs (e.g., GPT) as the solution and metric routers. For better understanding, we first introduce the necessary notations, followed by detailed explanation of these two modules in the following sections. As Problem Formulation and Notations. shown in Fig. 2, given user-specified task with inthese instances as set = stances, we represent {ˆo1, ˆo2, . . . , ˆon, on+1, . . . , oN }, where ˆoi = (Ii, qi, ai) and oi = (Ii, qi), with Ii, qi, and ai denoting the image set, task request prompt, and ground-truth answer for that instance, respectively. Note that only instances have groundtruth answers, referred to as example instances Oex = {ˆo1, ˆo2, . . . , ˆon}, where . The goal of the Solution Router, RS, is to propose programmatic solutions for the task based on the example instances so that the answers for all instances can be inferred by leveraging the proposed solutions. In practice, together with the example instances and predefined task-agnostic prompts (e.g., model definitions), we construct an input prompt for RS to generate solution pool = {s1, s2, . . . , sl}. Note that we set to enable model routing to perform reasoning to obtain the answer rather than simply memorizing the ground truth answers. Once solutions are obtained, the Metric Router, RM , samples subset with instances from to evaluate the performance of each solution in S. This evaluation yields set = {(p1, c1), (p2, c2), . . . , (pl, cl)}, where pi and ci denote the performance and computation cost of the i-th solution in S. Optionally, other metrics can also be logged. 1Our entire framework is built using Autogen [56], an open-source programming framework for agentic AI design that enables the development of multi-agent communication and Python code execution environments. 4 ensures that the proposed solution doesnt duplicate any existing solutions in the current solution pool. If the logic of proposed solution matches an existing one, it rejects the solution to avoid redundancy in the solution pool. Please refer to the supplemental for output examples. Conversation between solution proposer and committee. The interaction between the Solution Proposing and Solution Committee Teams refines solutions iteratively, as depicted in Fig. 4. As mentioned in the prior work [16], multi-agent conversation framework enhances reasoning and improves solution accuracy. However, excessive iterations can lead to error propagation. To address this, we require each committee member to deliver decision at every iteration, either accepting or rejecting the solution with feedback. If all committee members accept the solution, the iteration concludes. Recognizing that convergence is sometimes challenging, we enforce maximum number of iterations. At the end of the conversation, if the final solution is not redundant (as confirmed by the repetition checker), the most recent iterations solution is preserved. 3.4. Metric Router After model routing, we are able to collect pool of diverse solutions, = {s1, s2, . . . , sm} (see Fig. 2). The evaluation router further assesses these solutions, resulting in set = {(p1, c1), (p2, c2), . . . , (pm, cm)}, where pi and ci represent the performance and computation cost of the i-th solution in S. We introduce an evaluation router, similar to the solution router, which uses the multimodal LLMs reasoning to select the right metric based on the users task and the format of ground truth and predictions. Once the metric is chosen, we can proceed with performance testing and evaluation, estimating both the performance and cost of each solution. The user can also supply custom metric rendering evaluation router unnecessary; however, the choice of the metric may not itself be trivial for naive user. Input Structure. We again use MLLM (i.e., GPT-4) as the router to select metrics for evaluation. Below, we detail the input prompt for the router, comprising of task-agnostic: Metric Definitions: Provides details for each metric in the metric pool, including use cases, input arguments, return arguments, and examples. and user-derived task-specific instructions: Task Instances: Similar to the solution router, this includes task instructions and example instances sampled from the target task, along with ground truth answers and predictions from the solutions. Performance and Computation Cost Curve. For each proposed solution, we apply the aforementioned metric routing. Once metric is selected, we first choose larger test cases from the user-provided task. As shown in Fig. 2, we Figure 4. Illustration of multi-agent conversation. In the solution router, we have two team of agents performing conversation to get the final outputs. involves three key components: (i) analyzing existing solutions and committee feedbacks, (ii) outlining step-by-step high-level instructions, and (iii) developing Python code implementation. This process integrates analysis, creative problem-solving, and rigorous coding. We employ two agents for this purpose: the solution proposer Asp and the solution engineer Ase (see Fig. 4). The solution proposer Asp begins by reviewing existing solutions and generating novel approach with clear, high-level instructions, resulting in the ANALYSIS and THOUGHT sections of the output. Following this, the solution engineer Ase builds on the instructions provided to produce executable Python code, documented in the ACTION section. Together, the ANALYSIS, THOUGHT, and ACTION sections form comprehensive solution for further review. Please refer to the supplementary materials for output examples. Solution Committee Team. The Solution Committee oversees the quality and robustness of the generated solutions. Its main objectives are to verify that each solution meets predefined requirements, ensure code correctness and functionality, and check for redundancy with existing solutions. significant challenge is validating code logic beyond mere error-free execution. Therefore, we introduce code debugger that analyzes intermediate results. Additionally, with the code executor, we can provide the committee with intermediate outputs, enabling detailed, step-by-step review of the logic. As shown in Fig. 4, we introduce two additional agents with specific roles: requirement checker and code checker. The requirement checker evaluates whether the solution aligns with the specified requirements. Meanwhile, the code checker assesses both intermediate and final execution results to verify the accuracy and logical soundness of the code. In the final stage, the repetition checker 5 Method Depth Spatial Jigsaw Vis corr. Sem. Corr. Art Count Fun. Corr. Local. Multiview Refl. Fore. IQ Sim. OpenFlamingo-v2 [5] InstructBLIP-7B [14] InstructBLIP-13B [14] CogVLM [55] LLaVA-v1.5-7B [35] LLaVA-v1.5-13B [35] Ours (LLaVA-7B) Ours (LLaVA-13B) Qwen-VL-Max [7] Gemini Pro [20] Claude 3 OPUS [4] GPT-4o [42] GPT-4o (+ SoM + orig.) GPT-4o (+ Visprog) GPT-4o (+ Sketchpad) Ours (GPT-4o) 54.0 51.6 51.6 50.8 52.4 53.2 51.6 58. 58.9 50.0 57.3 74.2 75.0 46.8 83.9 80.3 43.4 56.6 65.7 67.1 61.5 67.8 78.8 69.9 77.6 67.1 57.3 69.2 82.5 37.8 81.1 81. 47.3 52.7 52.7 52.7 11.3 58.0 56.7 64.0 3.3 54.0 32.7 55.3 - - 70.7 75.3 Open-source multimodal LLMs 30.2 30.9 32.4 23.6 23.0 32. 32.4 34.5 29.3 22.1 20.7 54.0 - - 58.3 58.3 52.1 47.9 50.4 49.6 47.9 47.9 54.7 58.1 21.7 29.2 30.8 46.3 43.3 50. 41.2 47.2 API-based models 37.6 49.5 60.7 82.9 - - 77.19 83.0 55.8 65.0 49.2 51.7 - - 66.7 61. 36.2 23.9 22.3 23.9 21.5 20.8 21.5 23.9 28.5 32.3 22.3 39.2 - - 42.1 55.4 25.6 30.8 29.7 20.9 25.6 29.1 33.1 34. 22.7 37.2 31.4 75.0 - - 80.8 85.5 52.0 44.8 52.0 43.2 48.8 47.2 56.6 51.6 49.6 46.4 46.4 56.0 - - 65.4 59. 41.4 58.7 54.1 57.1 49.6 41.4 55.6 51.1 53.4 41.4 57.9 60.2 - - 45.6 60.2 43.3 29.9 46.3 26.9 36.6 45.5 37.0 45. 49.3 46.3 27.6 38.8 - - 33.1 35.1 15.9 29.6 13.6 24.2 28.0 27.3 26.5 26.5 47.7 45.5 62.1 85.6 - - 79.0 84. 23.3 23.3 26.0 26.7 24.0 28.0 23.3 28.0 22.0 27.3 21.3 30.0 - - 22.8 28.7 55.2 46.3 46.3 46.3 46.3 46.3 58.5 45. 51.5 55.9 70.6 65.4 - - 84.2 75.3 Table 1. Quantitative results. Experimental results on the BLINK benchmark [19]. denotes results from the previous work [26], and represents results collected via official codebase. The best result is highlighted in Bold and the second underlined."
        },
        {
            "title": "Model",
            "content": "Avg."
        },
        {
            "title": "Scene",
            "content": "InstructBLIP [14] LLaVA-v1.5-7B [35] MiniGPT-4 [63] OpenFlamingo [5] Qwen-VL-Chat [7] CogVLM [55] InternLM [62] GPT-4o [42] Ours (GPT-4o) 51.5 57.7 45.9 36.1 50.9 42.4 69.2 75.6 75.8 58.9 63.7 56.3 46.7 56.5 51.7 77.5 77.3 78. Id 49.7 62.4 49.2 42.3 47.6 43.5 73.5 79.7 78.3 Attri. Locat. 61.7 66.7 45.8 31.7 54.8 38.9 74.8 79. 79.7 35.1 51.3 37.9 33.4 46.9 33.8 65.4 71.0 70."
        },
        {
            "title": "Model",
            "content": "Count."
        },
        {
            "title": "Spatial",
            "content": "Inter. Reason."
        },
        {
            "title": "Text",
            "content": "InstructBLIP [14] LLaVA-v1.5-7B [35] MiniGPT-4 [63] OpenFlamingo [5] Qwen-VL-Chat [7] CogVLM [55] InternLM [62] GPT-4o [42] Ours (GPT-4o) 58.1 60.2 45.3 27.4 54.2 29.4 65.8 68.1 67.7 34.9 38.5 32.6 29.8 40.3 33.6 57.5 63.8 62. 47.4 47.4 47.4 29.9 55.7 45.4 71.1 78.6 80.6 55.9 59.8 57.1 47.7 55.0 53.5 75.8 81.2 84.5 61.4 69.0 41.8 35.6 47.4 51.5 61.2 69.8 69. Table 2. Quantitative results on Seedbench [31]. then perform evaluations, recording both performance and computation cost, and generate plot. This allows users to select solutions based on their preferences. Please see supplemental for further discussion on metric routing. 4. Experiments Datasets and Evaluation To verify the effectiveness of MMFactory, we conduct experiments on two benchmarks: BLINK [19] and Seedbench [31], and compare our model against previous works. These benchmarks contain various tasks covering visual perception and spatial understanding. BLINK includes 14 visual perception tasks with total of 3,807 multiple-choice questions, while SeedBench covers 9 classical spatial understanding tasks with total of 14k image-QA pairs, including scene understanding, instance interaction, and visual reasoning. There are some overlapping tasks between the two benchmarks; however, the main difference is that BLINK focuses on evaluating visual perception, where tasks are designed to be solvable by humans at glance while hard to answer correctly for MLLMs. In contrast, SeedBench emphasizes models visual spatial understanding, involving complex tasks with small objects or intricate descriptive prompts. For evaluation, since the tasks in these datasets are single-choice questions, we follow their protocol by using GPT to map the open-form predictions from MLLMs to the fixed set of choices and perform string matching to report accuracy for each task. 4.1. Quantitative Analysis In this subsection, we evaluate the effectiveness and performance of our MMFactory. Note that, to ensure fair comparison with previous SoTA models, we fix the multimodal LLMs to the same ones used in the compared methods for quantitative evaluation. For vision models, we use exactly the same models as those employed in the prior work on Visual Sketchpad [26]. Can MMFactory propose effective solutions? To verify this point, we conducted experiments on BLINK and SeedBench, reporting performance using three different multimodal LLMs (i.e., LLaVA-7B, LLaVA-13B, and GPT-4o) as fixed MLLMs. The results are shown in Tables 1 and 2. Our method demonstrates its ability to propose useful solutions with either comparable or improved performance relative to its own base model. Notably, with the routing approach, very significant performance boosts are observed in certain tasks, such as function correspondence (+15% over GPT-4o) and jigsaw solving (+20% over GPT4o), spatial understanding (+17% over LLaVA-7B), and jigsaw again (+6% over LLaVA-13B). Consistent performance improve6 Figure 5. Qualitative examples of MMFactory. MMFactory showcases its abilities to use and combine models by automatically constructing better prompts for MLLMs (in Sol 0) and developing solutions with similar logic but utilizing stronger models (in Sol 4). ments are also seen on SeedBench, particularly for multiinstance understanding tasks like instance interaction and reasoning, with 3% increase, verifying the effectiveness of our proposed solution router. Comparison with augmented frameworks for MLLMs We further compare our framework with other augmentation frameworks for MLLMs, such as SoM [58], Visprog [22], and Visual Sketchpad [26]. Visual Sketchpad [26] allows LMs to adjust their solution based on intermediate visual results from other tools. To demonstrate that our solution proposer with multi-agent cooperation can produce better solution plans than Visual Sketchpad, we fixed the visual tools and the LM as used in their approach and reported the performance of our proposed solutions in Table 1. Benefiting from multi-agent cooperation, our approach achieves comparable or better performance than the previous SoTA, highlighting the effectiveness of the solution proposer. Most importantly, our proposed solutions are general and not limited to specific samples within the task. As result, we significantly reduce the API calling cost; see Figure 7 for more details. Last but not least, comparing with previous visual programming work of Visporg, we achieve + 30% over depth and spatial tasks, demonstrating our approach can propose stronger pre-defined solution."
        },
        {
            "title": "Full model",
            "content": "(-) code debugger (-) code checker (-) requirement checker (-) repetition checker Acc 50.5 40.0 33.3 48.1 40.5 Error rate Avg. # sols 0.0 1.7 20.8 0.5 17.8 3.0 2.8 3.0 2.4 2.0 Table 3. Ablation. of significance of multi-agent conversation. 4.2. Qualitative Analysis Fig. 5 shows qualitative examples of our proposed MMFactory. It samples few examples from given task, defined by the users constraints and task details (e.g., image and prompt), and passes them to MMFactory. The solution proposer then generates pool of robust solutions for the task. Simultaneously, the metric router generates performance curve showing the trade-off between time cost and accuracy based on selected metrics (e.g. GPTScore). Unlike existing methods, our approach generates solution pool from which users can choose the best option based on their constraints. Additionally, our framework provides solutions tailored to the entire task, rather than to individual samples. Additional examples are provided in supplement. 4.3. Model Analysis Ablation studies of the multi-agent corporation. In the solution router, we leverage multi-agent conversation to im7 Figure 6. Ablation. Performance analysis with iteration. Lines in different colors represent different runs. Red cross denotes the highest performance in the run. prove the quality and robustness of the generated solutions. We conduct ablation studies on the multi-agent component of the proposer to verify this, with the results shown in Tab. 3. Specifically, we run the solution router on the first five tasks (listed in Tab. 1) in the BLINK dataset, with three runs per task, each allowing max of six conversation iterations. Without the code debugger, the code checker cannot access the intermediate results of the solution, resulting in significantly performance accuracy drop of 10%. Without the code checker, there is no feedback on execution results, which not only reduces the performance but also substantially increases the error rate during solution execution. Furthermore, after ablating the requirement checker, we observe both performance and solution correctness degrade compared to the full model. Lastly, without the repetition checker, the average number of proposed solutions decreases significantly by 33%, verifying the effectiveness of the repetition checker in enhancing solution diversity. Routing time and API calling cost. In our solution router, agents iteratively converse to generate the final solutions. As the number of existing solutions in the pool grows, the router may take more time to propose novel solution. Therefore, we further investigate the routing time cost with varying numbers of solutions in the pool. The average time cost per solution and per iteration is reported in Fig. 7 (top). We observe that the time cost per solution increases as the number of existing solutions grows. We assume this is due to the increasing complexity of the task, requiring the router to utilize the maximum number of iterations to derive the final solution. On average, it takes approximately 8 minutes to generate solution. Notably, since the generated solutions are applicable to all samples within task, we only need to perform solution routing once per task, rather than for each sample. We compare execution and routing costs with Visual Sketchpad in Fig. 7 (bottom). Execution cost refers to the time from input prompt to final answer, while routing cost is the time spent coordinating tools (execution time minus tool-calling time). One can find that with the pre-planned solutions, our execution cost is lower. Addi-"
        },
        {
            "title": "Model",
            "content": "Sketchpad [26] Ours Execution cost (sec) Variance Mean 43.86 19.96 29.43 9.74 Routing cost (sec) Variance Mean 30.90 18.20 0.00 0.00 Figure 7. Computational time. Solution generation cost plot (top). Average execution and routing cost per sample (bottom)."
        },
        {
            "title": "Jigsaw",
            "content": "Sketchpad [26] Ours 0.211 0.064 0.232 0.045 0.224 0.041 Vis. Corr. 0.281 0.034 Sem. Corr. 0.230 0. Table 4. API calling cost analysis per 10 samples (in USD). tionally, as the proposed solutions are reusable across all task instances, routing cost per sample is nearly zero, significantly less than the on-line routing in previous work. Furthermore, as we use GPT model for the solution router, we report the average API cost and compare it with previous work, Visual Sketchpad [26] (see Tab. 4). key benefit of our approach is that we perform routing only for few runs, with the produced solution applicable to all samples, significantly reducing the cost. In contrast, Sketchpad requires an API call for every sample, resulting in almost five times the cost of our approach on the BLINK dataset. Best answer happen in which run progressive performance analysis. In the solution router, we set maximum number of conversation iterations for the multi-agent cooperation. As mentioned in previous studies [16], multiagent conversation or debate can lead to error propagation, reducing performance after multiple iterations. To investigate this, we conducted experiments to analyze performance as the number of iterations increased, with results shown in Fig. 6. Specifically, we randomly selected 10 tasks from the BLINK dataset and ran our solution router to generate solutions, setting the maximum number of iterations to six. As shown in the figure, we observe that solutions with the best performance occur around 24 iterations. 5. Conclusion Selecting the right multimodal LLM for task can be difficult, especially without domain-specific knowledge or clear user requirements. In this paper, we present framework to help users select the most suitable solution from solution pool for given tasks based on their specific constraints. Our approach uses multi-agent debate mechanism to generate robust and well-reasoned solution. Unlike sample-specific solutions, our framework provides guidance that applies broadly across all examples for given task. Through extensive experiments, we demonstrate that our method outperforms current state-of-the-art approaches."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs, NSERC Canada Research Chair (CRC), and NSERC Discovery and Discovery Accelerator Supplement Grants. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, the Digital Research Alliance of Canada2, companies3 sponsoring the Vector Institute, and Advanced Research Computing at the University of British Columbia. Additional hardware support was provided by John R. Evans Leaders Fund CFI grant and Compute Canada under the Resource Allocation Competition award."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 1 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, 2016. 2 [4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. Technical Report, 2023. 2, 6 [5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 1, [6] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An arXiv preprint open language model for mathematics. arXiv:2310.10631, 2023. 2 [7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2), 2023. 1, 6 [8] Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2alliance.can.ca 3https://vectorinstitute.ai/#partners Audiolm: language modeling approach to audio generation. TASLP, 2023. 2 [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [10] Harrison Chase. LangChain, 2022. 3 [11] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 2 [12] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Borje Karlsson, Jie Fu, and Yemin Shi. Autoagents: framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023. 3 [13] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023. 2 [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning. NeurIPS, 2023. 6 [15] Nilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, David Huang, Prashant Mathur, Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, et al. Speechverse: large-scale generalizable audio language model. arXiv preprint arXiv:2405.08295, 2024. 2 [16] Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. 3, 4, 5, 8 [17] Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, Lu Yuan, and Leonid Sigal. On pre-training of multimodal language models customized for chart understanding. arXiv preprint arXiv:2407.14506, 2024. [18] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. Prompting large language models with speech recognition abilities. In ICASSP, 2024. 2 [19] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 1, 6 [20] Google Gemini Team. Gemini: family of highly capable multimodal models. technical report. Technical Report, 2023. 2, 6 [21] OpenAI GitHub. Github copilot, 2023. 2 [22] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In CVPR, 2023. 2, 7 [23] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. 2 [24] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. NeurIPS, 2023. 2 [25] Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. Routerbench: bencharXiv preprint mark for multi-llm routing system. arXiv:2403.12031, 2024. 3 [26] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. 2, 4, 6, 7, 8 [27] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 2 Johnson, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, Lawrence Zitnick, Inferring and executing programs for and Ross Girshick. visual reasoning. In ICCV, 2017. Bharath Hariharan, [28] Justin [29] Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol Min. Smart-llm: Smart multi-agent robot task planning using large language models. arXiv preprint arXiv:2309.10062, 2023. 3 [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2 [31] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, 2024. 1, 6 [32] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. NeurIPS, 2023. [33] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023."
        },
        {
            "title": "Encouraging divergent",
            "content": "[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2 [35] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023. 1, 2, 3, 6 [36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 2 10 [37] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. [38] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. NeurIPS, 2024. 2, 3 [39] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 2 [40] Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. arXiv preprint arXiv:2401.02384, 2024. 2 [41] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665, 2024. 2, 3 [42] OpenAI. Gpt-4 technical report. Technical Report, 2023. 2, [43] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir Patil, Ion Stoica, and Joseph Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023. 3 [44] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. 2, 3 [45] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. 2 [46] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. 2 [47] Tal Shnitzer, Anthony Ou, Mırian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, and Mikhail Yurochkin. Large language model routing with benchmark datasets. arXiv preprint arXiv:2309.15789, 2023. 3 [48] Significant Gravitas. AutoGPT. 3 [49] Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In ICCV, 2023. [50] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537, 2023. 3 [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [64] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In ICCV, 2023. 2 [52] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1 [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [54] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. arXiv preprint arXiv:2310.03731, 2023. 2 [55] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 6 [56] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. 3, 4 [57] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. An empirical study on challengarXiv preprint ing math problem solving with gpt-4. arXiv:2306.01337, 2023. 3 [58] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 7 [59] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [60] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip Yu. Large language models for robotics: survey. arXiv preprint arXiv:2311.07226, 2023. 2 [61] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2 [62] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang InternlmZhang, Haodong Duan, Hang Yan, et al. xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 1, 2, 6 [63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        }
    ],
    "affiliations": [
        "CIFAR AI Chair",
        "University of British Columbia",
        "Vector Institute for AI"
    ]
}