{
    "paper_title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
    "authors": [
        "Zhangquan Chen",
        "Manyuan Zhang",
        "Xinlei Yu",
        "Xufang Luo",
        "Mingze Sun",
        "Zihao Pan",
        "Yan Feng",
        "Peng Pei",
        "Xunliang Cai",
        "Ruqi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker."
        },
        {
            "title": "Start",
            "content": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views Zhangquan Chen1* Manyuan Zhang2 Xinlei Yu3 Yan Feng2 Peng Pei2 Xunliang Cai2 Ruqi Huang1 Xufang Luo Mingze Sun Zihao Pan2 1. Tsinghua Shenzhen International Graduate School, Tsinghua University 2. Meituan 3. National University of Singapore 5 2 0 2 1 ] . [ 1 2 3 6 8 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Though recent advances in visionlanguage models (VLMs) have achieved remarkable progress across wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https: //github.com/zhangquanchen/3DThinker. 1. Introduction Spatial understanding is critical capability for machines to interact with the real 3D world (e.g., embodied AI, autonomous driving) [61, 69, 73, 83]. These systems typically rely on ego-centric, multi-view observations, typically pro- *The work was conducted during the internship of Zhangquan Chen (czq23@mails.tsinghua.edu.cn) at Meituan. Project leader Corresponding author: ruqihuang@sz.tsinghua.edu.cn Figure 1. Illustration of our 3DThinker. Existing methods typically perform reasoning based solely on pure text or 2D visual cues, without fully exploiting the rich spatial and geometric information inherent in images. Other methods attempt to enhance the input by introducing auxiliary modalities (e.g., depth maps or coordinates), yet these often depend on additional annotations or external tools. In contrast, our framework enables VLMs to intrinsically form 3D mental representations during reasoning, thereby improving their spatial understanding. vided by multiple cameras simultaneously capturing limited views of their surroundings. These views are not interchangeable or purely visual; they inherently carry spatial semantics tied to the machines frame of reference [24]. Consequently, imagining the full scene and performing reasoning based on few limited views presents an essential problem for spatial intelligence [79]. Although recent VLMs are pretrained on large-scale imagetext corpora, their performance on such spatial reasoning tasks remains notably limited [8, 15, 36, 74]. The core bottleneck lies in their inability to extract 3D geometry embedded within images and their restricted capacity for spatial imagination. Recent advances have attempted to enhance the spatial reasoning capabilities of VLMs [13, 24, 34, 39, 40, 45, 68]. As illustrated in Fig. 1, existing methods can be broadly divided into two categories. The first category performs reasoning with pure text [7, 14, 39, 40, 45] or 2D visual cues [13, 17, 71], whose representational capacity for complex spatial layouts is inherently limited. To mitigate this limitation, methods such as MindCube [80] train models to generate cognitive maps of 3D layouts; however, they rely on birds-eye-vie (BEV) annotations to construct these maps. Ego3D [24] further employs external modelsGroundingDINO [38] for referring expression comprehension (REC) and DepthAnythingv2 [76] for depth estimation, to automatically generate cognitive maps Yet, constrained by the performance of these models, such methods often fail on low-resolution or uncurated images. The second category incorporates auxiliary modalities as additional inputs (e.g., point clouds, camera parameters) [9, 32]. However, these settings restrict the models applicability in real-world scenarios where only monocular images are available. Moreover, several recent methods invoke external encoder or tool-usage to obtain prior information (e.g., encoded 3D tokens [21], depth maps [5, 13, 40]). Importantly, these techniques do not constitute an intrinsic capability of the model and introduce additional inference overhead. These challenges motivate the need for new method that: G1) 3D-imaginable: can directly learn 3D geometry from limited 2D images; G2) Annotation-free: does not rely on densely annotated data; and G3) Intrinsic: requires no external priors or auxiliary models during inference. The most relevant mental model, Mirage [79], leverages ground-truth image embeddings for supervised training, facilitating the continuation of multimodal trajectory without the need for pixel-level image generation. However, the training of [79] is heavily reliant on ground-truth image supervision and remains constrained to the thinking with image paradigm, which prevents its effectiveness on (G1) and (G2). Nevertheless, it provides valuable inspiration, prompting us to introduce new novel framework, 3DThinker, which enables thinking with 3D mentaling. Unlike prior works that depend on external priors or complex training data construction, our method intrinsically integrates 3D representations into the VLMs, enabling unified reasoning and 3D latent generation within the model. For (G1), our framework enables the model to generate geometric representations from images during the reasoning process. Regarding (G2), we directly project the 3D latent to align with 3D foundation model, thereby circumventing the need for raw 3D data construction. Consequently, our model can inherently think with 3D without relying on any prior or auxiliary geometry encoder, corresponding to (G3). Simultaneously, since our method allows for the recovery of 3D representations(e.g., point clouds) from 3D latents via the projector, it significantly enhances the interpretability of the large reasoning model. Specifically, we first construct batch of Chain-ofThought (CoT) data that incorporates 3D special tokens. Our training framework then proceeds in two main stages. In the first stage, we perform supervised learning, where features from the 3D foundation model (e.g., VGGT [59]) are distilled into the native reasoning process of the VLM. To enable the model to think with 3D mentaling while maintaining textual coherence, we employ both 3D latent alignment loss and the cross-entropy loss. In the second stage, we employ reinforcement learning, optimizing the tokens across the entire sampling trajectory based solely on outcome-driven signals, while preserving the alignment of the 3D latent. That is, we refine 3D mentaling within the trajectory using only outcome as the optimization signal. Our contributions can be summarized as follows. We are the first to introduce the think with 3D mentaling framework, which operates without dependence on densely labeled training data (e.g., cognitive maps). We propose two-stage training framework (shown in Fig. 2), progressing from feature alignment to learning intrinsic geometry awareness from outcome-based signals, thus enabling 3D mentaling without any external prior. 3DThinker overcomes the lack of interpretability in latent reasoning. Specifically, 3DThinker enables the recovery of 3D representations from the latent space via projector during the reasoning process. Extensive experiments across multiple benchmarks demonstrate that 3DThinker consistently outperforms strong baselines. Furthermore, our results indicate that the effectiveness of 3DThinker generalizes well across different base VLMs, highlighting its broad applicability. 2. Related Work 2.1. Multimodal Reasoning Large language models (LLMs) have experienced rapid development, and demonstrated strong performance across wide range of tasks [11, 16, 35, 70, 82, 85, 92]. Building on these advances, recent works have highlighted that incontext learning, including intermediate rationales, can significantly enhance the performance of LLMs [22, 37, 49, 63, 64, 81, 93]. Current reasoning methods can be categorized into three types: pure-text, visual, and latent reasoning. Pure-text reasoning: [3, 10, 27, 29, 53, 56] elicit textual step-by-step reasoning inspired by [25]. They typically rely on textual descriptions, which can limit the reasoning capabilities when dealing with visual evidences that cannot be adequately described using pure textual language. Visual reasoning: to solve the problem mentioned above, some methods integrate visual evidences directly into the reasoning trajectory, whether in multi-hop or continuous modes. Some intrinsic multi-hop methods [12, 39, 51, 62], first generate detailed visual cues within the model itself (e.g., bounding boxes, coordinates, or masks), and then the Figure 2. The schematic illustration of our 3DThinker, framework that enables thinking with 3D mentaling. (1) Stage 1: 3DThinker is first trained under supervision using our constructed CoT data (see Sec. 3.1), aligning the generated 3D latents with the feature space of 3D foundation model. This alignment allows the model to leverage suitable 3D spatial mentaling while reasoning. (2) Stage 2: After supervised training, we further optimize the entire trajectory using only outcome signals, while maintaining the alignment of the 3D latents. further reasoning is conducted based on these cues. Other extrinsic tool-usage methods [44, 54, 67, 91], enhance the think with image capability by dynamically invoking external image tools. On the other hand, continuous methods like GRIT [20] and SIFThinker [13] generate continuous visual reasoning to enable iterative corrections during the single-step reasoning process. Latent reasoning: some studies have shown that incorporating intermediate hidden representations into LLMs can effectively enhance model capabilities [4, 18, 55, 77]. [26] replaces CoT tokens with continuous latent embeddings, allowing unconstrained reasoning in the latent space to tackle complex tasks. More recently, Mirage [79] and LVR [31] utilize special visual tokens alongside ordinary text during reasoning. They explore visual information within the model by implicitly supervising the generation of image latent, thereby enabling reasoning with 2D visual latent. While prior works primarily focus on enhancing reasoning ability in textual or 2D spaces, our method takes different perspective: we treat latent tokens as bridge for the model to think with 3D at mental-level, aligning more closely with human cognition. 2.2. Spatial Understanding Spatial understanding encompasses skills such as 3D imagination and spatial cognition, which are essential for perceiving and manipulating spatial relationships in both 2D and 3D environments [6, 19, 42, 43, 58, 72, 84, 86, 87]. Recently, much efforts have been dedicated to evaluating the spatial understanding ability of VLMs [24, 30, 41, 48, 80, 89, 90]. Additionally, several methods have been proposed to enhance spatial understanding. For example, [5, 17, 40, 45, 46] equip LLM with additional multiview, depth or point cloud inputs, essentially serving as input enhancement. Furthermore, 3DRS [28] introduces teacher model for 3D supervision to achieve explicit spatial representation alignment; however, this method requires input that includes the 3D coordinates corresponding to each pixel. Moreover, VLM-3R [21] employs implicit 3D tokens from pre-trained model (e.g., CUT3R [60]) to achieve spatial awareness by incorporating prior information, necessitating inference with extensively 3D foundation model. Recently, methods like MindCube [80] and Ego3D-VLM [24] have facilitated spatial understanding by constructing textual cognitive maps. Despite these advancements, existing methods often rely on input enhancement or constructed cognitive maps, necessitating complex data collection and annotation. However, 3DThinker enables 3D mentaling directly from multi views by learning 3D latent distilled from 3D foundation models, thereby facilitating spatial reasoning without relying on densely annotated data. 3. Methodology Human cognition is inherently rooted in the comprehension of 3D environments. Inspired from the cognitive mechanism of mental imagery, we propose 3DThinker, framework that enables VLMs to imagine 3D scenes during reasoning processes. In contrast to existing methods that reason with pure text or 2D visual cues, our framework integrates 3D representations into the interleaved multimodal trajectories. Specifically, 3DThinker generates compact latent embeddings that serve as 3D tokens, closely emulating the mental 3D scenes that humans intuitively imagine in spatial reasoning. As illustrated in Fig. 2, 3DThinker first aligns the VLM-generated 3D latent with the 3D foundation model, followed by reinforced training to optimize the trajectory. In this section, we will explain how we achieve this from three aspects: data generation, supervised training (stage 1), and reinforcement training (stage 2). 3.1. Data Generation Due to the fact that VLMs naturally only generate textual tokens, they require additional supervised training to learn how to produce interleaved reasoning patterns that incorporate 3D information. Therefore, we synthesize specific training corpora based on the 10K training data from MindCube dataset [80]. Given an image set from different views = {I1, I2, . . . , In}, question Q, and the ground truth response R, we employ high-level model (i.e., GPT-4o) to complete the reasoning chain. Specifically, we prompt the model to generate step-by-step reasoning that contains placeholders (3D special tokens), where these tokens represent imagined 3D scenes in the mind. Denote the response as: = (Q, I, R). (1) Here, represents the step-by-step reasoning process with embedded 3D placeholders, whose last layer hidden states are required to be consistent with features extracted from the 3D foundation model during supervised training. By prompting the large-scale reasoning VLM with various inputs, we are able to collect training dataset = (cid:8)(cid:0)Q(i), (i), R(i), o(i)(cid:1)(cid:9), where each o(i) contains interleaved text and 3D placeholders. Figure 3. generated 3D latent into the feature space of VGGT. Illustration of our projector, which transforms VLM3.2. Supervision for 3D Grounded Reasoning To teach the model reasoning with 3D, naive solution is to explicitly align its outputs with 3D representations (e.g., point cloud). However, this often depends on laborintensive data annotation and requires the model to have explicit 3D generation capabilities, which can be quite challenging. Instead, we introduce the 3D foundation model (i.e., VGGT [59]) during training, and distill its features to the 3D special token generated within the VLM reasoning process, thereby facilitating effective 3D-aware reasoning without the need for exhaustive manual labeling. Specifically, for each training example (Q, I, R, o) D, the reasoning trajectory can be decomposed into three sequential components through concatenation operations: = opre t3D opost, (2) where t3D = {t1, . . . , tk} represents the token sequence of human-like 3D mental imagery. The salient vectors Flatent = {h1, . . . , hk}, which operationalize the 3D cognitive tokens t3D, are extracted from the last layer hidden states of VLM fθ() with parameter θ. These salient vectors are recursively generated conditioned on the preceding context: hi = (cid:40) hidden,L θ hidden,L θ (Q, I, opre), (Q, I, opre, t1:i1), = 1, 2. (3) Concurrently, we can obtain patch-level visual features Fimages = fenc(I) from the image encoder, and acquire the geometry features F3D = fvggt(I) through the last layer of VGGT aggregator. To ensure dimensional consistency between the generated 3D latent features and the predicted geometry features, we employ the projector as illustrated in Fig. 3 to transform Flatent into compatible feature space: Fproj = rojector(Flatent, Fimages). (4) Our objective is to achieve optimal alignment between the projected 3D features derived from the VLM and the corresponding VGGT features. To this end, we formulate the 3D alignment as the Frobenius loss: L3D = Fproj F3D2 . (5) On the other hand, to ensure textual coherence while introducing 3D tokens, we employ cross-entropy loss to optimize the prediction of surrounding textual tokens. Specifically, the prediction of th textual tokens before is t3D conditioned on both the preceding response tokens and the original input sequence. where ri,t = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) denotes the likelihood ratio between the updated and old policies at step t. ϵ, β are hyperparameters, and DKL[πθ πref] represents the KL divergence [50] between the current policy model and the fixed reference model. The group-normalized advantage, denoted as ˆAi,t, is calculated by the task-specific reward ri,t. ˆAi,t = ri,t mean{r1,t, . . . , rN,t} std{ri,t, . . . , rN,t} + δ . (11)"
        },
        {
            "title": "Lpre",
            "content": "text = opre (cid:88) i=1 ℓCE (opre,i, fθ (Q, I, opre,<i)) . (6) Next, we will introduce several specifically designed rewards to achieve reinforced spatial mentaling. In contrast, textual tokens positioned after t3D incorporates textual 3D special tokens."
        },
        {
            "title": "Lpost",
            "content": "text = opost (cid:88) i=1 ℓCE (opost,i, fθ (Q, I, opre, t3D, opost,<i) . (7) Finally, the textual loss is formulated as follows: Ltext = Lpre text + Lpost text . (8)"
        },
        {
            "title": "The overall",
            "content": "training objective incorporates both 3D alignment and textual losses, thereby enabling the model to seamlessly incorporate 3D imagining into its textual reasoning process. Here, λ3D and λtext serve as hyperparameters that balances coefficients. Ltotal = λ3DL3D + λtextLtext. (9) 3.3. Reinforced Spatial Mentaling At the supervised training stage, our primary objective is to enable the model to perform textual reasoning while simultaneously generating formatted 3D tokens. Additionally, we pre-train the projector to achieve effective alignment of 3D latents. During the reinforced training stage, we expect to use only outcome signals to optimize the sampling trajectories and refine the imagined mental 3D representations as well. Specifically, we employ outcome-based grouprelative policy optimisation (GRPO) [52], while VGGT features are utilized to further optimize the 3D visual token generated by the model. Notably, the projector remains frozen in this stage. We formalize the RL framework as follows. For each question-images pair (Q, I), the reinforcement learning (RL) framework generates set of candidate completions {o1, . . . , oN } from the current policy πθold, and subsequently updates the policy to πθ by maximizing the following objective: (θ) = (cid:88)"
        },
        {
            "title": "1\nN",
            "content": "i=1 ri,t ˆAi,t 1 oi (cid:105) oi (cid:88) (cid:110) (cid:104) min t=1 clip(ri,t, 1 ϵ, 1 + ϵ) ˆAi,t, β DKL[πθ πref] (cid:111) , (10) the last layer hidden state of 3D special Reward for 3D visual token. After the supervised training, the model has begun to exhibit the ability of 3D mentaling during thinking process. To further optimize the 3D visual token in the reasoning process, we can extract token t3D (i.e., <latent start><latent pad> ... <latent end>) in each trajectory and perform optimization. Specifically, the projected features RL proj are computed based on Eq. 4 at step t, with VGGT features F3D serving as constraints during the RL stage. That is, the cosine similarity between the VGGT features and the projected features is calculated to serve as the reward r3D. r3D = 1 2 (1 + RL proj F3D RL projF3D ), (12) Reward for outcome-based optimization. We expect to optimize the entire trajectory using only the outcomebased signals, without relying on explicit annotations of intermediate processes. Thus, we design corresponding rewards for both format (rformat) and final answer (rans). (1) Format reward: the models output should adhere to the format: ...<latent start><latent pad>... <latent end>...<think>...</think><answer> </answer>. reward of 1.0 is assigned to re- ... sponses that strictly comply with this format. (2) Answer reward: we also provide the 0/1 binary reward by comparing the generated answer with the ground truth option. This outcome-based reward is evenly distributed across each token in the trajectory, including 3D visual tokens. So, the task reward ri,t is composite signal comprising the sum of three components: r3D, rformat and rans. 4. Experiments Evaluation metric. For multiple-choice questions, we use Accuracy, which is calculated based on exact matches between the models predictions and the ground truth. For numerical-answer questions, we use Mean Relative Accuracy (MRA) introduced by [75], metric that measures the Table 1. Accuracy comparison of generalist VLMs and our method (3DThinker) on MindCube-Tiny and Ego3D-Bench, with our training conducted on stage 1 (S1) and on both stage 1 and stage 2 (S1 + S2). The best results achieved based on different VLMs are bolded. The overall/average results of each model are highlighted in blue, with the best results among all models highlighted in red. Method gpt-4o-2024-11-20 gpt-4.1 glm-4.5v gemini-2.5-pro claude-sonnet-4 doubao-seed-1.6 o3-2025-04-16 Qwen2.5-VL-3B 3DThinker-S1Qwen2.5-3B 3DThinker-S1+S2Qwen2.5-3B Qwen2.5-VL-7B 3DThinker-S1Qwen2.5-7B 3DThinker-S1+S2Qwen2.5-7B Qwen2.5-VL-32B 3DThinker-S1Qwen2.5-32B 3DThinker-S1+S2Qwen2.5-32B Qwen2.5-VL-72B 3DThinker-S1Qwen2.5-72B 3DThinker-S1+S2Qwen2.5-72B InternVL3-8B 3DThinker-S1InternVL3-8B 3DThinker-S1+S2InternVL3-8B InternVL3-14B 3DThinker-S1InternVL3-14B 3DThinker-S1+S2InternVL3-14B InternVL3-38B 3DThinker-S1InternVL3-38B 3DThinker-S1+S2InternVL3-38B InternVL3-78B 3DThinker-S1InternVL3-78B 3DThinker-S1+S2InternVL3-78B LLaVA-OneVision-1.5-4B 3DThinker-S1LLaVA-O-1.5-4B 3DThinker-S1+S2LLaVA-O-1.5-4B LLaVA-OneVision-1.5-8B 3DThinker-S1LLaVA-O-1.5-8B 3DThinker-S1+S2LLaVA-O-1.5-8B MindCube-Tiny Rotation Among Around Overall Ego Dist. Obj. Dist. Loc. Closed-source Models Ego3D-Bench Obj. Mot. Travel Time Ego Mot. 37.0 45.5 28.0 84.0 49.5 87.0 86.5 37.4 44.0 55.5 36.5 43.5 55.0 39.5 45.0 56.5 40.0 42.5 57.0 37.0 43.0 55.0 36.0 42.0 54.5 32.5 39.0 53.5 38.5 43.5 57. 33.5 41.5 48.0 34.5 43.0 49.0 44.8 44.2 43.0 39.7 42.2 35.8 42.7 33.3 64.8 81.8 32.5 66.3 83.0 34.5 66.8 83.2 42.5 68.0 83.7 40.3 66.8 82.5 48.0 68.3 84.3 48.5 68.0 85.2 50.5 69.0 86.2 38.0 59.8 67.5 34.7 57.8 68.2 56.4 47.2 33.2 56.8 12.8 38.0 66. 46.1 45.1 37.8 52.2 36.6 46.1 56.6 33.2 62.7 75.2 34.7 64.4 76.0 37.6 65.1 76.7 42.5 64.5 77.1 30.3 72.4 75.2 38.4 76.4 76.0 43.6 77.2 77.2 44.4 73.6 77.6 33.2 51.7 49.9 58.5 48.9 55.2 71.3 Qwen2.5-VL Family [2] 21.5 36.1 41.6 32.7 47.9 54.0 45.4 52.0 62.2 42.4 49.9 61.1 InternVL3 Family [94] 25.8 43.8 54.6 46.0 56.2 63.5 35.4 44.8 54.7 54.6 59.8 69.9 63.2 79.2 79.2 55.6 77.2 77.6 56.0 76.8 78.0 57.4 77.2 78.8 LLaVA-OneVision-1.5 Family [1] 49.2 66.0 65.2 48.4 64.8 64.8 45.1 65.2 76.5 47.5 65.4 77.0 47.2 64.6 77.4 49.9 66.1 78. 39.7 40.0 40.2 30.3 35.1 36.5 39.8 57.8 63.2 37.9 56.7 63.7 26.5 36.2 39.6 50.2 36.5 50.8 59.3 29.4 39.4 46.0 31.5 44.5 52.3 40.7 51.9 61.9 38.6 45.9 59.9 28.7 44.4 56.1 35.6 49.1 59.9 31.0 47.0 58.1 48.4 53.1 61.0 37.1 39.1 39.9 36.6 39.0 41. 28.1 41.8 48.4 61.4 51.6 60.5 65.6 28.8 32.5 33.1 30.5 36.5 36.5 49.6 54.8 54.5 54.8 57.8 59.7 29.8 32.9 36.0 35.9 37.3 41.3 39.4 43.6 49.2 50.3 52.2 61.0 29.2 33.1 34.2 34.3 36.1 37.0 78.1 82.7 88.8 92.9 81.9 89.0 93.4 50.3 54.8 54.7 45.9 51.9 52.7 75.6 80.1 80.2 86.8 85.6 93. 54.1 60.6 67.2 63.2 70.0 78.3 66.6 73.1 86.9 77.7 80.1 91.9 51.4 51.1 51.9 44.9 44.9 46.2 56.7 62.6 73.4 75.5 55.1 67.3 80.1 41.9 46.2 53.3 44.0 51.3 56.6 74.1 79.4 86.6 68.9 75.6 84.9 54.8 61.2 69.4 65.9 71.8 80.2 64.9 68.6 80.4 70.0 72.5 88.6 51.8 52.6 52.3 51.9 53.2 53. 36.0 44.3 40.4 43.5 33.6 49.8 53.5 30.9 30.8 30.8 34.5 39.1 38.2 40.1 44.3 43.7 38.5 43.9 43.7 36.1 46.9 46.7 41.6 51.1 50.0 38.0 48.5 49.1 44.8 53.9 54.8 34.1 30.9 30.8 36.9 31.9 32.8 Ego Rel. Obj. Rel. Avg. 60.5 65.7 57.1 72.8 53.9 71.4 77.7 54.1 64.0 70.1 43.2 59.1 66.0 54.0 62.0 69.9 53.3 58.0 69.8 49.9 64.1 71.0 55.5 68.0 75.1 61.0 71.2 79.6 57.0 65.1 75.3 52.4 58.6 61.8 53.4 61.0 64.9 66.0 70.2 81.9 78.6 69.5 86.0 83. 56.1 69.7 76.9 66.5 73.9 83.1 79.0 83.1 86.0 80.5 80.8 87.8 65.2 72.1 81.9 70.1 77.7 84.0 77.3 79.1 85.9 76.6 78.0 83.9 73.5 73.8 73.8 74.4 73.8 77.2 48.1 56.9 59.9 66.7 53.9 66.3 73.0 39.1 46.7 50.8 41.1 50.5 54.9 57.3 63.5 68.1 58.0 62.2 70.0 43.1 53.3 60.4 51.7 60.2 66.5 51.7 59.5 68.0 59.9 64.3 73. 46.2 47.4 48.1 45.3 46.9 48.7 closeness of the models predictions to the ground truth values. Avg. denotes the mean value of all subset task. Hyper-parameters. For 3DThinker, in the stage 1, we set the MLP depth to 6, with the learning rate of 1e-4, latent size of 12, epoch of 10. In Eqn. 9, the hyper-parameters λ3D, λtext are uniformly set to 0.1 and 1, respectively. In the stage 2, we set the balancing coefficient of all three reward to 1, with the learning rate of 1e-5, the rollout number of 8. Additional details are provided in the Supp. Mat.. 4.1. Benchmarking Generalist VLMs In this section, we comprehensively investigate different training stages in 3DThinker across various generalist VLMs. We conduct experiments on MindCube-Tiny [80] and Ego3D-Bench [24] benchmarks, both of which are designed to evaluate the spatial understanding ability from limited views. As shown in Tab. 1, 3DThinker-full achieves consistent improvements over the generalist VLMs across all settings. On MindCube-Tiny, the overall performance gain ranges from 51.8% to 108.8%, while on Ego3D-Bench, the improvement spans 18.1% to 36.9%. Taking Qwen2.5VL-3B as an example, 3DThinker boosts performance on MindCube-Tiny by 88.9% (62.7 vs. 33.2) after stage 1, and further improves by 19.9% (75.2 vs. 62.7) after stage 2. Similarly, on Ego3D-Bench, we observe 19.3% improvement (46.7 vs. 39.1) after the stage 1 and an additional 8.8% gain (50.8 vs. 46.7) following the stage 2. AlTable 2. The evaluation of various baselines on the VSI-Bench [75], SPBench [34], CV-Bench [57], SPAR-Bench [88], ViewSpatialBench [33] and MMSI-Bench [78] datasets. [SI] denotes benchmarks with single image, whereas [MV] refers to multi-view images. The best-performing results under each base model are highlighted. Method VSI-Bench [75] [MV] SPBench [34] [SI, MV] CV-Bench [57] [SI] SPAR-Bench [88] [SI, MV] ViewSpatial-Bench [33] [SI, MV] MMSI-Bench [78] [MV] Avg. Qwen2.5-VL-3B [2] Spatial-MLLM-4B [65] SpatialLadder-3B [34] 3DThinker-S1Qwen2.5-3B 3DThinker-S1+S2Qwen2.5-3B Qwen2.5-VL-7B [2] SpaceR-7B [45] VILASR-7B [66] Video-R1 [23] 3DThinker-S1Qwen2.5-7B 3DThinker-S1+S2Qwen2.5-7B 29.4 47.3 45.7 53.2 59. 35.8 44.5 45.4 33.4 57.3 63.7 Qwen2.5-VL-3B Based Spatial Models 70.6 73.8 73.7 74.5 78.4 24.6 35.1 34.4 52.3 58.2 Qwen2.5-VL-7B Based Spatial Models 73.0 75.3 77.1 69.6 77.9 81. 30.2 37.1 37.8 31.5 56.3 63.3 38.5 48.4 70.6 54.8 60.2 42.9 54.0 53.9 42.8 61.5 68.3 35.6 43.6 44.2 59.5 64.7 37.9 45.5 46.1 36.1 61.7 68.6 26.5 31.5 29.2 37.7 41. 26.9 28.8 30.2 29.4 41.5 43.3 37.5 46.6 49.6 55.3 60.4 41.1 47.5 48.4 40.5 59.4 64.7 though the performance is slightly weaker on certain subtasks, e.g., Travel Time, this can be attributed to the need for richer contextual information to align the normalized 3D representations with the real-world. Remarkably, our model is trained without any Ego3D-specific data, yet it still achieves promising results on Ego3D-Bench, demonstrating strong cross-dataset generalization. This highlights that our think with 3D framework effectively enhances the models generalization capability across diverse spatial understanding scenarios. It is also worth noting that our best model, 3DThinker-S1+S2Qwen2.5-72B, outperforms all other models, both open-source and closed-source, including the latest O3 model (78.9 vs. 56.6 on MindCubeTiny, 73.3 vs. 73.0 on Ego3D-Bench). 4.2. Comparisons with Baselines We evaluate our method against several state-of-the-art (SOTA) approaches across diverse set of categories. Additional details are provided in the Supp. Mat.. Different Spatial Models. As shown in Tab. 2, we categorize the methods into two groups based on the types of base VLMs and then evaluate them across different benchmarks. For the Qwen2.5-VL-3B-based spatial models, 3DThinker surpasses the recent SOTA, SpatialLadder-3B, by 11.5% (55.3 vs. 49.6) in stage 1. This improvement is further enhanced to 21.8% (62.7 vs. 49.6) following stage 2. When using the Qwen2.5-VL-7B model, our method achieves even more remarkable results. 3DThinker outperforms the SOTA VILASR-7B by 22.7% (59.4 vs. 48.4) in stage 1, and by 33.7% (64.7 vs. 48.4) in stage 2. On the other hand, in contrast to methods that exhibit task-specific overfitting (e.g., SpatialLadder-3B on SPBench), 3DThinker demonstrates consistent improvement across all tasks, highlighting the robust spatial reasoning capability of our method. Additionally, unlike models such as Video-R1, which struggle on single-view tasks (e.g., underperforming the base model on CV-Bench), our method demonstrates strong perTable 3. Performance on Ego3D-Bench (Accuracy Avg.) in comparison between 3DThinker and Ego3D-VLM, employing series of VLMs with varying parameters. The best is highlighted. InternVL3 Qwen2.5-VL 8B 14B 38B 78B 60.1 66.1 68.0 71.8 60.4 66.5 68.0 73.3 7B 32B 72B 3B Ego3D-VLM [24] 44.4 54.3 65.5 69.5 3DThinker 50.8 54.9 68.1 70.0 Method Table 4. Results with Qwen2.5-VL-3B on MindCube-Tiny in terms of different training strategies. The best is highlighted. Method MindCube-Tiny Rotation Among Around Overall raw-QA SFT CoT SFT Aug-CGMap-FFR-Out-SFT Plain-CGMap-FFR-Out-SFT 3DThinker-S1Qwen2.5-3B GRPO CoT SFT + GRPO Aug-CGMap-FFR-Out-SFT+RL Plain-CGMap-FFR-Out-SFT+RL 3DThinker-S1+S2Qwen2.5-3B 34.5 36.0 49.5 47.5 44.0 36.5 36.5 53.0 48.0 55.5 52.5 54.3 52.5 62.3 64.8 49.3 55.2 76.8 79.2 81.8 66.0 65.2 66.4 67.6 72.4 64.8 65.6 70.0 68.4 75.2 52.3 53.4 55.2 60.8 62.7 50.6 54.1 70.7 70.7 75.2 formance on both single-image and multi-view tasks. This indicates that our 3D mental reasoning framework significantly enhances performance, even in single-image cases. Different Architectures and Parameter Scales. Tab. 3 compares our method with Ego3D-VLM on Ego3DBench across different model series and parameter scales. its cognitive map Although Ego3D-VLM constructs with the aid of external modulesspecifically, referring expression comprehension model (Grounding-DINOBase [38]) and depth estimator (Depth-Anything-V2Metric-Large [76])our method, which does not rely on any extrinsic priors at inference, still achieves superior performance. In particular, on Qwen2.5-VL-3B, 3DThinker yields notable 14.4% improvement (50.8 vs. 44.4). 4.3. Training Strategies To further demonstrate the effectiveness of our training paradigm, we compare 3DThinker against several repreTable 5. Ablation of different 3D latent size on MindCube-Tiny in terms of 3DThinker-S1Qwen2.5-3B. Latent Size Accuracy 4 60.2 8 60. 12 62.7 16 59.9 32 25.1 64 15.5 Table 6. Ablation of different designs including 3D special token position (Token Pos.), projector and rewards in terms of 3DThinker-S1+S2Qwen2.5-3B. Token Pos. Middle End VGGT-to-VLM w/o rformat w/o rans w/o r3D Projector Rewards Method Accuracy 42.0 74.3 74. 74.8 64.2 68.3 Full 75.2 Figure 4. The reasoning process for different cases is presented, along with the visualization of the 3D latent representations. provided in the Supp. Mat.. 4.5. Ablation Study sentative training strategies. Among them, Aug-CGMapFFR-Out and Plain-CGMap-FFR-Out serve as SOTA baselines introduced in [80]. Specifically, Aug-CGMap-FFROut performs reasoning with the augmented cognitive map (camera-view information included), whereas AugCGMap-FFR-Out relies solely on plain cognitive maps without augmentation. Under supervised training, our method surpasses rawQA SFT, CoT SFT, and even the cognitive-map-based SFT proposed in [80] by margin of 3.1% (62.7 vs. 60.8). The relatively smaller improvement observed in the rotation sub-category can be attributed to its requirement for dynamic spatial imagination. Since our think with 3D supervised framework primarily targets static spatial understanding, the RL stage further enhances its dynamic capability by optimizing whole reasoning trajectories. That is, through outcome-based RL, 3DThinker progressively refines the 3D latents across rollouts, achieving additional gains in both zero-RL and SFT-then-RL settings. Furthermore, 3DThinker achieves 6.4% improvement over the cognitive-map-based SFT-then-RL baseline (75.2 vs. 70.7), demonstrating its superior capability in integrating spatial reasoning with reinforcement learning. 4.4. Visualization We visualize the results of 3D mentaling in Fig. 4. During inference, we extract the last layer hidden states corresponding to the 3D special tokens. These 3D latents are projected into the VGGT feature space via the projector illustrated in Fig. 3. The projected features are subsequently processed by the DPT [47] of VGGT to generate point clouds. As shown in Fig. 4, the reconstructed mentaling point clouds roughly depict the underlying scene, where the clearer regions are typically correlated with prompt-relevant objects. This observation indicates that the 3D latents effectively encode the mental scene guided by the prompt intent. After reasoning with 3D mentaling, all three examples yield correct answers. Additional visualizations and analysis are Different 3D Latent Size. In Tab. 5, we ablate the effect of different latent sizes on the results. The results indicate that the optimal performance is achieved with the latent size of about 12. This is because smaller latent size limits the models representational capacity, while larger latent size can compromise the models natural expressive ability, leading to repetitive <latent start> outputs that fail to yield the final answer. Different Designs. As shown in Tab. 6, we first conduct an ablation study on the placement of the 3D special tokens. Beyond the approach in Sec. 3.3, where the special tokens is positioned at the beginning (before <think>), we also explore placing it between the <think> and </think>, as well as at the end (after </answer>). We observe that placing the 3D tokens in the middle disrupts natural language coherence: the 3D latent can resemble certain character features, leading to garbled text and premature output termination. This results in significant performance drop (75.2 vs. 42.0). In contrast, positioning the 3D tokens at the beginning or endwhere it is isolated from natural textyields significantly better performance. We also examine two potential projector configurations. The first maps the last layer hidden state of the VLM to the VGGT space (shown in Fig. 3), allowing the VLM features to be explicitly converted into 3D representations (e.g., point clouds) via the projector. The alternative compresses VGGT features directly into the VLM space (e.g., via adaptive average pooling), but this approach is unrecoverable to 3D representations. Given the interpretability, visualizability, and better performance (75.2 vs. 74.1) of the first approach, we adopt it as our projector strategy. Finally, we ablate the three rewards used in stage 2. Among them, the formatting requirement has minimal impact. In contrast, removing 3D alignment leads to substantial performance drop (75.2 vs. 68.3) due to the absence of stable constraints on the 3D latent. The final answer reward is also critical (75.2 vs. 64.2), serving as the sole groundtruth supervision signal and guiding optimization of each token across the entire rollout. 5. Conclusion and Limitation Conclusion. In this paper, we propose 3DThinker, framework for VLM to think with 3D spatial mentaling. Unlike recent methods that rely solely on pure text or 2D visual cues for reasoning, 3DThinker leverages geometric information embedded in images during the reasoning process for the first time. Additionally, our method does not rely on dense annotations or other external priors. To enable thinking with 3D spatial mentaling, we introduce twostage training scheme. Stage 1 distills geometric features from pretrained 3D model to warm up. Stage 2 optimizes the entire reasoning trajectory while maintaining 3D visual alignment based on the outcome signal. Experimental results show that our method outperforms previous methods across various benchmarks, establishing solid foundation for future exploration. Limitation & Future Work. (1) Our method recovers 3D mental representations from the last layer hidden state of the special tokens. However, these latents are not autoregressively incorporated into the framework. Thus, developing unified structure (e.g, unified tokenizer) could be key area for future improvement. (2) Exploring iterative 3D mentaling within the trajectory may provide additional benefits."
        },
        {
            "title": "References",
            "content": "[1] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. 6 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 7 [3] Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv preprint arXiv:2505.14231, 2025. 2 [4] Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024. 3 [5] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. arXiv preprint arXiv:2406.13642, 2024. 2, 3 [6] Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, et al. Has gpt-5 achieved spaarXiv preprint tial an empirical study. arXiv:2508.13142, 2025. 3 intelligence? [7] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 2 [8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 1 [9] Hanzhi Chen, Boyang Sun, Anran Zhang, Marc Pollefeys, and Stefan Leutenegger. Vidbot: Learning generalizable 3d actions from in-the-wild 2d human videos for zero-shot robotic manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2766127672, 2025. 2 [10] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in visionlanguage models with less than $3. https://github. com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. 2 [11] Zhangquan Chen, Chunjiang Liu, and Haobin Duan. three-phases-lora finetuned hybrid llm integrated with strong In International prior module in the education context. Conference on Artificial Neural Networks, pages 235250. Springer, 2024. [12] Zhangquan Chen, Xufang Luo, and Dongsheng Li. Visrl: Intention-driven visual perception via reinforced reasoning. arXiv preprint arXiv:2503.07523, 2025. 2 [13] Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, and Ruqi Huang. Sifthinker: arXiv Spatially-aware image focus for visual reasoning. preprint arXiv:2508.06259, 2025. 1, 2, 3 [14] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 2 [15] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 1 [16] Kexin Chu, Zixu Shen, Dawei Xiang, and Wei Zhang. Safekv: Safe kv-cache sharing in llm serving. In Machine Learning for Computer Architecture and Systems, 2025. 2 [17] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, et al. Mm-spatial: Exploring 3d spatial understanding in multimodal llms. arXiv preprint arXiv:2503.13111, 2025. 2, [18] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. 3 [19] Tianqi Ding, Dawei Xiang, Pablo Rivas, and Liang Dong. Neural pruning for 3d scene reconstruction: Efficient nerf acceleration. arXiv preprint arXiv:2504.00950, 2025. 3 [20] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. 3 [21] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. 2, 3 [22] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36:7075770798, 2023. 2 [23] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 7 [24] Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Yong Zhang, and Mohammad Akbari. Spatial reasoning with vision-language models in ego-centric multi-view scenes. arXiv preprint arXiv:2509.06266, 2025. 1, 2, 3, 4, 6, 7 [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [26] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 3 [27] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. 2 [28] Xiaohu Huang, Jingjing Wu, Qunyi Xie, and Kai Han. Mllms need 3d-aware representation supervision for scene understanding. arXiv preprint arXiv:2506.01946, 2025. 3 [29] Jiaming Ji, Jiayi Zhou, Hantao Lou, Boyuan Chen, Donghai Hong, Xuyao Wang, Wenqi Chen, Kaile Wang, Rui Pan, Jiahao Li, Mohan Wang, Josef Dai, Tianyi Qiu, Hua Xu, Dong Li, Weipeng Chen, Jun Song, Bo Zheng, and Yaodong Yang. Align anything: Training all-modality models to follow instructions with language feedback. 2024. 2 [30] Phillip Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, and Minhyuk Sung. Perspectiveaware reasoning in vision-language models via mental imagery simulation. arXiv preprint arXiv:2504.17207, 2025. 3 [31] Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, and Zicheng Liu. Latent visual reasoning. arXiv preprint arXiv:2509.24251, 2025. [32] Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, and Yichen Zhu. Injecting the 3d world into vision-language-action models. arXiv preprint arXiv:2503.07511, 2025. 2 Pointvla: [33] Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, et al. Viewspatial-bench: Evaluating multi-perspective spatial localization in visionlanguage models. arXiv preprint arXiv:2505.21500, 2025. 7 [34] Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Spatialladder: Progressive training for spatial reasoning in vision-language models. arXiv preprint arXiv:2510.08531, 2025. 1, 7 [35] Xinjin Li, Yu Ma, Yangchen Huang, Xingqi Wang, Yuzhen Lin, and Chenxi Zhang. Synergized data efficiency and compression (sec) optimization for large language models. In 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS), pages 586591, 2024. [36] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv preprint arXiv:2409.09788, 2024. 1 [37] Junhong Lin, Xinyue Zeng, Jie Zhu, Song Wang, Julian Shun, Jun Wu, and Dawei Zhou. Plan and budget: Effective and efficient test-time scaling on large language model reasoning, 2025. 2 [38] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 2, 7 [39] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. Spatialcot: Advancing spatial reasoning through coordinate alignment arXiv and chain-of-thought for embodied task planning. preprint arXiv:2501.10074, 2025. 1, 2 [40] Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, and Donglin Wang. Ssr: Enhancing depth perception in vision-language models via rationale-guided spatial reasoning. arXiv preprint arXiv:2505.12448, 2025. 1, 2, 3 [41] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso de Melo, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [42] Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, and Wenjun Mei. Wonderturbo: Generating arXiv preprint interactive 3d world in 0.72 seconds. arXiv:2504.02261, 2025. 3 [43] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Xinze Chen, Guanghong Jia, Guan Huang, and Wenjun Mei. Recondreamer-rl: Enhancing reinforcement learning via diffusion-based scene reconstruction. arXiv preprint arXiv:2508.08170, 2025. 3 [44] OpenAI. Introducing openai o3 and o4-mini, 2025. https://openai.com/index/introducing-o3-and-o4-mini/. 3 [45] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. 1, 2, 3, [46] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2025. 3 [47] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. 8 [48] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. 3 [49] Ayushman Sarkar, Mohd Yamani Idna Idris, and Zhenyu Yu. Reasoning in computer vision: Taxonomy, models, tasks, and methodologies. arXiv preprint arXiv:2508.10523, 2025. 2 [50] John Schulman. Approximating kl divergence. John Schulmans Homepage, 2020. [51] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models, 2024. 2 [52] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 5 [53] Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1style large vision-language model. https://github. com/om-ai-lab/VLM-R1, 2025. Accessed: 2025-0215. 2 [54] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 3 [55] Yu Sun, Yin Li, Ruixiao Sun, Chunhui Liu, Fangming Zhou, Ze Jin, Linjie Wang, Xiang Shen, Zhuolin Hao, and Hongyu Xiong. Audio-enhanced vision-language modeling with latent space broadening for high quality data expansion, 2025. 3 [56] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, and Salman Khan. Llamav-o1: Rethinking step-bystep visual reasoning in llms, 2025. [57] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. 7 [58] Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, and Xingang Wang. Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling. arXiv preprint arXiv:2507.05198, 2025. 3 [59] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: In Proceedings of Visual geometry grounded transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 2, 4 [60] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perIn Proceedings of the ception model with persistent state. Computer Vision and Pattern Recognition Conference, pages 1051010522, 2025. 3 [61] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, et al. Embodiedscan: holistic multimodal 3d perception suite towards embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1975719767, 2024. 1 [62] XuDong Wang, Shaolun Zhang, Shufan Li, Konstantinos Kallidromitis, Kehan Li, Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Segllm: Multi-round reasoning segmentation. arXiv preprint arXiv:2410.18923, 2024. 2 [63] Zixuan Wang, Yu Sun, Hongwei Wang, Baoyu Jing, Xiang Shen, Xin Dong, Zhuolin Hao, Hongyu Xiong, and Yang Song. Reasoning-enhanced domain-adaptive pretraining of multimodal large language models for short video content moderation, 2025. [64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2 [65] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. 7 [66] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 7 [67] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255, 2025. 3 [68] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: visualization-of-thought elicits spatial reasoning in large language models. Advances in Neural Information Processing Systems, 37:9027790317, 2025. 1 [69] Fei Xia, Amir Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 90689079, 2018. 1 [70] Dawei Xiang, Wenyan Xu, Kexin Chu, Zixu Shen, Tianqi Promptsculptor: Multi-agent arXiv preprint Ding, and Wei Zhang. based text-to-image prompt optimization. arXiv:2509.12446, 2025. 2 [71] Linhui Xiao, Xiaoshan Yang, Xiangyuan Lan, Yaowei Wang, and Changsheng Xu. Towards visual grounding: survey. arXiv preprint arXiv:2412.20206, 2024. 2 [72] Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, and Yong Li. Defining and evaluating visual language models basic spatial abilities: perspective from psychometrics. arXiv preprint arXiv:2502.11859, 2025. 3 [73] Tianyi Yan, Dongming Wu, Wencheng Han, Junpeng Jiang, Xia Zhou, Kun Zhan, Cheng-zhong Xu, and Jianbing Shen. Drivingsphere: Building high-fidelity 4d world for closedloop simulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2753127541, 2025. [74] Tianyi Yan, Junbo Yin, Xianpeng Lang, Ruigang Yang, Cheng-Zhong Xu, and Jianbing Shen. Olidm: Object-aware lidar diffusion models for autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91219129, 2025. 1 [75] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 5, 7 [76] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. 2, 7 [77] Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Do large language models laarXiv preprint and Sebastian Riedel. tently perform multi-hop reasoning? arXiv:2402.16837, 2024. 3 [78] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multiimage spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. 7 [79] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. 1, 2, [80] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv preprint arXiv:2506.21458, 2025. 2, 3, 4, 6, 8 [81] Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Junyi Chen, and Kun Wang. From physics to foundation models: review of ai-driven quantitative remote sensing inversion. arXiv preprint arXiv:2507.09081, 2025. 2 [82] Zhenyu Yu, Mohd Yamani Idna Idris, Pei Wang, Yuelong Xia, and Yong Xiang. Forgetme: Benchmarking the selective forgetting capabilities of generative models. Engineering Applications of Artificial Intelligence, 161:112087, 2025. 2 [83] Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, and Xing Wei. Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving. arXiv preprint arXiv:2505.17685, 2025. 1 [84] Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, and Xing Wei. Janusvln: Decoupling semantics and spatiality with dual implicit memory for vision-language navigation. arXiv preprint arXiv:2509.22548, 2025. 3 [85] Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, and Tingting Yu. Bridging the editing gap in llms: Fineedit for precise and targeted text modifications, 2025. [86] Jirong Zha, Yuxuan Fan, Xiao Yang, Chen Gao, and Xinlei Chen. How to enable llm with 3d capacity? survey of spatial reasoning in llm. arXiv preprint arXiv:2504.05786, 2025. 3 [87] J. Zhang, W. Zhang, C. Tan, X. Li, and Q. Sun. Yolo-ppa based efficient traffic sign detection for cruise control in autonomous driving. arXiv preprint arXiv:2409.03320, 2024. 3 [88] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025. 7 [89] Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Junqi Zhao, Allison Koenecke, Boyang Li, and Lu Wang. Sphere: Unveiling spatial blind spots in vision-language arXiv preprint models through hierarchical evaluation. arXiv:2412.12693, 2024. 3 [90] Weichen Zhang, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, and Xiao-Ping Zhang. Open3dvqa: benchmark for comprehensive spatial reasoning with multimodal large language model in open space. arXiv preprint arXiv:2503.11094, 2025. 3 [91] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [92] Pengfei Zhou, Weiqing Min, Chaoran Fu, Ying Jin, Mingyu Huang, Xiangyang Li, Shuhuan Mei, and Shuqiang Jiang. Foodsky: food-oriented large language model that can pass the chef and dietetic examinations. Patterns, 6(5), 2025. 2 [93] Xiaoling Zhou, Wei Ye, Zhemg Lee, Lei Zou, and Shikun Zhang. Valuing training data via causal inference for inIEEE Transactions on Knowledge and context learning. Data Engineering, 2025. 2 [94] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Meituan",
        "National University of Singapore",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University"
    ]
}