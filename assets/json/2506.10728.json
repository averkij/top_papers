{
    "paper_title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims",
    "authors": [
        "Priyanka Kargupta",
        "Runchu Tian",
        "Jiawei Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with scientific and political claims. However, a claim (e.g., \"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., \"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines."
        },
        {
            "title": "Start",
            "content": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims Priyanka Kargupta*, Runchu Tian, Jiawei Han Department of Computer Science, University of Illinois at Urbana-Champaign {pk36, runchut2, hanj}@illinois.edu 5 2 0 2 2 1 ] . [ 1 8 2 7 0 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely true or falseas is frequently the case with scientific and political claims. However, claim (e.g., vaccine is better than vaccine B) can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables more comprehensive, structured response that provides well-rounded perspective on given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose CLAIMSPECT, retrieval-augmented generation-based framework for automatically constructing hierarchy of aspects typically considered when addressing claim and enriching them with corpusspecific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., how many biomedical papers believe vaccine is more transportable than B?). We apply CLAIMSPECT to wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing nuanced claim and representing perspectives within corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines."
        },
        {
            "title": "Introduction",
            "content": "Scientific and political topics increasingly being consumed in the form of concise, attention-grabbing claims which lack the nuance needed to represent complex realities (Vosoughi et al., 2018; Allcott and Gentzkow, 2017; Lazer et al., 2018). Such claims are frequently oversimplified or confidently stated, despite being valid only under specific conditions or when evaluated from *Equal contribution. Figure 1: An example hierarchy of nuanced claim being deconstructed into aspects. Each node is enriched with relevant excerpts, the affirmative/neutral/opposing perspectives, and their respective evidence. certain perspectives. For instance, claim like vaccine is better than vaccine may appear straightforward but becomes inherently nuanced when specific aspects, such as efficacy, safety, and distribution logistics, are considered. Moreover, the ambiguous and fragmented nature of information shared on such platforms often allows such claims to be twisted or reframed as true or false to support conflicting narratives, complicating the task of verifying their validity (Sharma et al., 2019; Pennycook and Rand, 2021). Stance detection categorizes textual opinions as supportive, neutral, or opposing relative to target (Mohammad et al., 2016). However, documentsespecially those in scientific domainoften present range of stances across various aspects of claim. For instance, as illustrated in Figure 1, study might find Vaccine safer for adults than Vaccine while highlighting its significantly greater logistical challenges for widespread distribution. In this case, the paper supports the claim regarding safety for adults (note: not safety in its entirety) but opposes it concerning distribution. This complexity renders stance detection at the document level ineffective for nuanced, multifaceted claims. Fact-checking models often validate claims by retrieving evidence from large corpora or using web-integrated language models (Thorne et al., 2018; Popat et al., 2018; Zhang and Gao, 2023). While some methods now offer varied factuality judgements like mostly true or halftrue (Zhang and Gao, 2023), these are less effective in scientific contexts. Especially in evolving areas, finegrained scientific claims may be unsubstantiated due to lack of research or scientific consensus, rather than being outright false. This distinction is vital, as it highlights areas needing further exploration. For example, in Figure 1, relevant paper excerpts mapped to the Safety for Adults node show that an 80:20 ratio of affirmative to opposing stances towards the sub-aspect claim suggests consensus, whereas 60:40 ratio or sparse data signals limited research or disagreement. Such insights, crucial for understanding gaps in knowledge, are often overlooked by existing fact-checking frameworks. We address these challenges using CLAIMSPECT, framework which systematically deconstructs and analyzes claims by leveraging large language models (LLMs). ClaimSpect hierarchically partitions claim into tree of aspects and sub-aspects, enabling structured validation and the discovery of perspectives. This is accomplished by adopting the following principles: Principle #1: Claim trees capture the multidimensionality inherent in nuanced topics. As opposed to considering single target claim and the full document, we must first determine the relevant aspects discussed within the corpus itself in order to discover more targeted subclaims. However, it is essential to retain the hierarchical nature of such aspects. This is demonstrated in Figure 1, where certain aspects that are difficult to validate (e.g., safety) can typically be partitioned until they reach atomic sub-aspects that are more commonly considered (e.g., safety for children, safety for adults, and safety for elderly). Furthermore, these hierarchical relationships are often also reflected in how we naturally navigate formulating our own perspective towards given topic (either individually or collectively): parse through the existing knowledge on topic, consider different sub-angles of the problem based on this knowledge, retrieve more sub-angle specific knowledge, develop our opinions accordingly, and aggregate them to high-level opinion (Perony et al., 2013; Chen et al., 2022). Thus, this brings us to our next principle. Principle #2: Iterative, discriminative retrieval enhances LLM-based tree construction. LLMs have recently shown promise in automatic taxonomy enrichment and expansion, organizing data into hierarchies of categories and subcategories similar to our target aspect hierarchy (Shen et al., 2024b; Zeng et al., 2024b). However, these approaches often rely on general knowledge existing within the LLMs pre-training dataset, overlooking corpus-specific insights crucial for (1) uncovering fine-grained sub-aspects prevalent in domain-specific data, and (2) ensuring alignment with the task of determining corpus-wide consensus. To address this, we leverage retrieval-augmented generation (RAG), which has recently made advances in knowledge-intensive tasks by integrating external corpora or databases into the generation process (Lewis et al., 2020; Gao et al., 2023). We introduce an iterative RAG approach, which dynamically constructs the aspect hierarchy by retrieving relevant segments for an aspect node, using them to discover new sub-aspects. This ensures the taxonomy aligns closely with corpus-specific discussions of claims, aspects, and perspectives. We note that noisy retrieval often hinders reasoning performance (Shen et al., 2024a). In our setting, this may occur when certain retrieved excerpts overlap multiple semantically similar aspect nodes (e.g., safety for children vs. safety for adults), introducing noise when determining sub-aspects for only one aspect. To mitigate this, we introduce discriminative ranking mechanism that prioritizes segments discussing single aspect in-depth, enhancing sub-aspect discovery and the final aspect hierarchy. Principle #3: Perspectives enrich understanding beyond stance and consensus. For each aspect node in the hierarchy, we identify and cluster papers based on their stance (affirmative, neutral, opposing) using hierarchical text classification and stance detection. These clusters reveal not only the presence or absence of consensus but also the key perspectives within each stance. For example, as shown in Figure 1, the affirmative perspective might highlight Vaccine As lower rate of severe allergic reactions in adults, while the opposition focuses on its higher incidence of blood clotting. These perspectives offer transparency, uncover potential research gaps (e.g., if 80% of the affirmative papers do not address these blood clotting incidents), and provide critical context for framing nuanced claims. Overall, CLAIMSPECT utilizes structured approach to deconstruct nuanced claim into hierarchy of aspects, targeting holistic approach considering all aspects which could be used to validate the root claim. The framework comprises the following steps: (1) aspectdiscriminative retrieval, (2) iterative sub-aspect discovery, and (3) classification-based perspective discovery. Our contributions can be summarized as: From the best of our knowledge, CLAIMSPECT is the first work to formally deconstruct claims into hierarchical structure of aspects to determine consensus. We construct two novel datasets of real-world, scientific and political nuanced claims and corresponding corpora. Through experiments and case studies on realworld domains, we demonstrate that ClaimSpect performs hierarchical consensus analysis significantly more effectively than the baselines. Reproducibility: We provide our dataset and source code1 to facilitate further studies. 1https://github.com/pkargupta/ claimspect"
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Fact Checking. Fact-checking models (Thorne et al., 2018; Popat et al., 2018; Atanasova et al., 2019; Karadzhov et al., 2017) have leveraged external evidence to validate claims, but often treat claims as monolithic statements. Web-integrated methods (Zhang and Gao, 2023; Karadzhov et al., 2017) attempt to enrich fact-checking with additional context, but still fail to account for nuanced claims that cannot be clearly validated without considering diverse range of claim subaspects and their varying levels of evidence. In contrast, CLAIMSPECT acknowledges the nuance behind certain claims, utilizing corpus to help identify the various aspects that would be considered when validating claim enabling more multi-faceted and interpretable analysis. We note that CLAIMSPECT does not aim to validate given claim it simply aims to deconstruct the claim into hierarchy of aspects which could be used to validate it, posing potential perspectives towards the aspect of the claim, grounded in the corpus. We also note the adjacent task of evidence retrieval, where existing work explores organizing evidence according to fixed and flat (non-hierarchical) set of aspects: populations, interventions, and outcomes (Wadhwa et al., 2023). LLM-Based Taxonomy Generation. Recent advances in taxonomy generation (Shen et al., 2024b; Zeng et al., 2024b; Chen et al., 2023; Zeng et al., 2024a; Sun et al., 2024) have demonstrated the potential of large language models for structuring information hierarchically. However, these methods typically rely on static, domain-agnostic knowledge, limiting their adaptability to construct rich, fine-grained taxonomies (Sun et al., 2024). CLAIMSPECT addresses these limitations through corpus-aware, aspect-discriminative retrieval and iterative sub-aspect discovery, constructing rich taxonomy of aspects that is aligned with corpus. This allows us to identify the relevant segments to both given aspect but also perspective towards that aspect. Stance Detection Traditional stance detection (Mohammad et al., 2016) classifies opinions as supportive, neutral, or opposing towards target (e.g., claim). However, these approaches typically assign single stance to an entire document, overlooking the nuanced, aspectspecific stances present within many claims, especially in scientific and political contexts. Recent works (Zhang and Gao, 2023) have introduced more fine-grained judgments (e.g., mostly true), but similar to fact-checking methods, they often fail to capture the multi-faceted nature and rationale behind certain stances. By exploiting its constructed aspect hierarchy, CLAIMSPECT is able to infer viable supportive, neutral, and opposing perspectives towards an aspect and its associated papers."
        },
        {
            "title": "3 Methodology",
            "content": "Illustrated in Figure 2, CLAIMSPECT consists of the following steps: (1) aspect-discriminative retrieval, (2) iterative sub-aspect discovery, and (3) classificationbased perspective discovery."
        },
        {
            "title": "3.1.1 Task Definition\nWe assume that as input, the user provides a claim t0\n(e.g.,“Vaccine A is better than Vaccine B”) and a corpus\nD. In order to better reflect real-world settings, we do\nnot assume that each document d ∈ D is relevant to t0.",
            "content": "Definition 1 (CLAIM) statement or assertion that expresses position, which may require validation or scrutiny. It often encapsulates multiple dimensions that contribute to its overall truthfulness or validity. Definition 2 (ASPECT) specific component or dimension of claim that can be independently analyzed or evaluated. ClaimSpect aims to output hierarchy of aspects , where each aspect node (e.g., safety) within the hierarchy can be considered as descendant subclaim ti of the root user-specified claim, t0 (e.g., is safer vaccine than B). In other words, each aspect node ti should reflect relevant aspect that is important to consider when evaluating the root claim t0."
        },
        {
            "title": "3.1.2 Document Preprocessing\nFor each d ∈ D, we assume we have its full textual\ncontent (e.g., a full scientific paper). In order to have\nsmaller, context-preserving units of text for our frame-\nwork to retrieve, we segment each d into chunks using\nthe widely-recognized text segmentation method, C99\n(Choi, 2000). This method labels sentences with match-\ning tags if they pertain to the same topical group, which\nassists with retaining consecutive discussion of an as-\npect to a single segment.",
            "content": "3.1.3 Initial Coarse-Grained Aspect Discovery Given our weak supervision setting, where only the root claim t0 is provided, we first generate reliable, coarsegrained aspects to guide the retrieval-augmented hierarchy construction. These aspects are typically commonsense and do not require domain expertise to identify. Preliminary experiments confirm that LLMs can reliably identify them based on their expansive background knowledge alone. Thus, we prompt an LLM to generate coarse-grained aspects t0 0 (e.g., efficacy, safety, and distribution in Figure 1) that will serve as the children of t0 . For each aspect t0 , the model outputs its label, significance to t0, and list of = 10 relevant keywords. This initial subtree forms the foundation of our framework. The full prompt is in Appendix B.1."
        },
        {
            "title": "3.2 Aspect-Discriminative Retrieval",
            "content": "In order to construct rich, coarse-to-fine aspect hierarchy that is aligned with the corpus, we must identify similarly rich reference material from our corpus. In general, noisy retrieval often hinders reasoning performance (Shen et al., 2024a), which may negatively impact discovering subaspects of given node. Thus, in order to discover each subaspect ti of an aspect node ti, we must determine which segments Si from our corpus Figure 2: CLAIMSPECT deconstructs nuanced claim into hierarchy of aspects typically considered for validating the claim. We automatically discover the set of perspectives towards each aspect from the corpus. discuss ti. However, not all segments are equally informative for discovering subaspects. Specifically, high-quality, discriminative segment si for node ti contains the following features: (1) si discusses ti in depth and (2) si does not discuss tis siblings in breadth or depth. For instance, in Figure 1, segment regarding the side effects observed within clinical trial of Vaccine and on both children and adults discusses safety in more depth than if it only mentioned children. Furthermore, for discovering subaspects of safety for children, segment which independently discusses the safety for both children and adults would introduce additional noise into the subaspect generation process. Overall, it is important to rank these segments such that we select set which minimizes the noise we introduce into the retrievalaugmented discovery of subaspects, while maximizing the number of subaspects which we can discover. We formalize our discriminative ranking mechanism in the sections below:"
        },
        {
            "title": "3.2.1 Retrieval-Augmented Keyword Enrichment\nIn order to determine whether a segment discusses an\naspect ti in depth, we must first further enrich our un-\nderstanding of ti. We propose performing a retrieval-\naugmented keyword-based enrichment of ti, where each\nkeyword is likely to occur within segments relevant to ti\nand, thus, reflects either explicitly or implicitly the sub-\naspects of ti. For example, for the “efficacy” aspect, the\ncorresponding keywords are: neutralization, immune\nstimulation, post-dose antibody response, and waning\nimmunity. First, we use a retrieval embedding model to\nselect the top-n segments (based on cosine-similarity)\nfrom the entire corpus that are relevant to a ti-specific\nquery (its root, name, description, and keywords from\nSection 3.1.3):",
            "content": "Claim: [t0]; Aspect: [ti]: [generated description of ti]; Aspect Keywords: [generated keywords of ti]. We provide these initial top segments in addition to the root claim t0, the aspect label ti, and its description to the LLM in-context to identify 2k keywords. Given the same information and these keywords, we then merge similar or duplicate terms, while filtering irrelevant terms explicitly prompting the model to provide solely keywords. This set of terms Wi; Wi = k, grounds our discriminative segment ranking for node ti. We provide these two prompts in Appendix B.2."
        },
        {
            "title": "3.2.2 Discriminative Segment Ranking",
            "content": "In order to determine the most discriminative segments Si for aspect node ti, we first collect an initial large pool of segments using the same retrieval embedding-based method as Section 3.2.1. Our subsequent goal is to rank segment Si based on its discriminativeness: Target Score: Reward based on its likelihood to contain all relevant subaspects ti of ti. Distractor Score: Penalize based on the degree and depth of other sibling aspects that it discusses. We assume that tis keywords Wi implicitly and/or explicitly reflect many of its subaspects. Thus, we use them to approximate the depth of an aspect-specific discussion. We convert each keyword wi into descriptive query: [wi] with respect to [all ancestor nodes of wi]. By integrating the ancestors into the query, we influence the retention of tis hierarchical context; for example, we specifically reward segment if it discusses the safety of Vaccine and B, as opposed to merely safety. We embed each keyword query emb(w Wi) using the retrieval embedding model, in addition to embedding each segment emb(s) Si. More formally, we are given an aspect node th , which is child of parent node th and sibling node of tj =i. We are also provided with segment embedding emb(s) Si, all keyword query embeddings of th , emb(w) Wi, and all sibling keyword query embeddings, emb(w) =i. We compute the discriminative rank based on the following: is reDefinition 3 (TARGET SCORE) segment si warded based on weighted average (H) of its degree of similarity to all keywords Wi, implying deeper discussion of node ti and its subaspects. p(si, Wi) = (cid:18)(cid:20) sim(cid:0)emb(si), emb(w)(cid:1) Wi (cid:21)(cid:19) , where H(X) = (cid:80)X r=1 (cid:80)X r=1 1 xr 1 (1) We compute weighted average based on Zipfs Law (Powers, 1998), where word indexed at the r-th position will have weight of 1/r. This weighted average of the segment-keyword similarities is based on the assumption that the model will implicitly generate the keywords from most to least significant in other words, we weight the first term w1 Wi the highest, while weighing wX the lowest. For example, if si had similarities of [0.9, 0, 0] to Wi = {w1, w2, w3}, then p(si, Wi) = 0.5363. On the other hand, if the similarities were [0.7, 0.8, 0.7], p(si, Wi) = 0.7272. Overall, the target score will indicate segments discussion depth of aspect node ti how many keywords it aligns with and to what degree. Definition 4 (DISTRACTOR SCORE) segment si is penalized based on the breadth and depth of siblings discussed. The breadth is indicated by the mean target scoring between si and each Wj of tj =i. The depth is indicated by the max target scoring between si and each Wj of tj =i. discriminative segments Si, and the root claim t0, we prompt the model to determine set of at minimum two and at maximum subaspects for aspect t0. We provide this prompt in Appendix B.3. Definition 6 (SUBASPECT) more granular component of parent aspect ti that further refines tis evaluation and would be considered when specifically addressing the root claim t0. Each subaspect is represented in the same manner specified in Section 3.1.3: its label, description, and keywords. We continue constructing our aspect hierarchy in top-down fashion, as detailed in Algorithm 1. Algorithm 1 Iterative Subaspect Discovery 0 coarse_grained_aspects(t0) {Section 3.1.3} Require: Root Claim t0, Corpus D, max_depth=l 1: = initialize_tree(t0) {T .depth = 0} 2: t0 3: = queue(T 0) 4: while len(q) > 0 and T.depth do 5: 6: 7: 8: 9: 10: end while 11: return ti pop(q) enrich_node(t0, ti, D) {Section 3.2.1} Si rank_segments(t0, ti, D) {Section 3.2.2} subaspect_discovery(t0, ti, Si) ti q.append(T i) n(si, =i) = 0.5 (cid:18) 1 =i =i (cid:88) j=1 (cid:18) (cid:19) p(si, Wj) + 0.5 max (cid:2) j= 1,T (cid:3) =i (cid:0)p(si, Wj)(cid:1) (cid:19) We utilize the target and distractor scores to compute our overall discriminativeness score, which weighs the proximity between segment and its target aspect, relative to its overall and individual proximity to its distractor, sibling aspects. Definition 5 (DISCRIMINATIVENESS SCORE) segment si is rewarded based on weighted average (H) of its degree of similarity to all keywords Wi, while being penalized based on the breadth and depth of siblings discussed. d(si, h) = β p(si, ) γ n(si, =i) (3) In Equation 3, d(si, h) grows proportional to the target score, while falling proportional to the distractor score. We include the β and γ scaling factors for each in case users would like to customize their degree of reward or penalty. Ultimately, we rank each segment Si based on its discriminativeness score, taking the top-k segments which feature the richest discussion of target aspect ti in order to discover its subaspects. Ultimately, the output of Algorithm 1 is our final aspect hierarchy, serving as the basis for our consensus determination and perspective discovery process. (2)"
        },
        {
            "title": "3.4 Classification-Based Perspective Discovery",
            "content": "With the aspect hierarchy constructed, we must identify the complete set of corpus segments that (1) pertain to the root claim t0 and (2) align with an aspect node in hierarchy . Pinpointing papers discussing aspect node ti allows us to infer their perspective on ti and assess the presence and extent of consensus. However, as noted in Section 3.1.1, we cannot assume all corpus segments are relevant to the root claim-an assumption made in LLM-based taxonomy-guided hierarchical classification works (Zhang et al., 2024a). Thus, we must first filter out claim-irrelevant segments. Filtering. naive approach determines segment relevance per node via in-context prompting, but this scales poorly. Instead, we frame relevance filtering as binary search problem, identifying the relevanceirrelevance boundary. Specifically, we embed the claim label t0 (emb(t0)) and each child aspect t0 0 (emb([aspect_label] with respect to [t0])), computing the claim representation as: c0 = (cid:18) 1 2 emb(t0) + (cid:80)T 0 i=1 emb(t0 ) 0 (cid:19) (4)"
        },
        {
            "title": "3.3 Iterative Subaspect Discovery",
            "content": "In order to expand our aspect hierarchy, we iteratively exploit our aspect-discriminative retrieval as knowledge which grounds the LLMs subaspect discovery. Given the aspect node ti, its description, its corresponding We rank the encoded segments by cosine-similarity to c0 and use binary search to find the index where fewer than δ% of segments in window are relevant. This rank serves as our threshold, filtering out lower-ranked segments and retaining only those relevant to t0 (S 0). This optimization significantly reduces the quantity of relevance judgments necessary; the relevancy prompt is in Appendix B.4. Hierarchical Text Classification. With 0 and , we apply taxonomy-guided hierarchical classification to determine for each aspect node ti . Since our focus is retrieval-guided aspect hierarchy construction rather than classification, we adopt recent LLM-based hierarchical classification model (Zhang et al., 2024a), which enriches taxonomy nodes (e.g., adding keywords) to support its top-down classification of to ti. Perspective & Consensus Discovery. The final step of our pipeline is to determine the primary perspectives Pi = {ai, oi} towards each aspect node ti, where ai is the overarching supportive perspective and oi is the opposing perspective. We also seek to identify the papers which hold each of these perspectives (Di = Dsupp Dneutral ), accounting for papers which do not hold any clear perspective towards ti. Definition 7 (PERSPECTIVE) descriptive viewpoint expressed toward specific aspect ti of claim t0 in the form of an implicit or explicit stance towards ti (e.g., support, neutral, or oppose) and optionally, rationale. Dopp , Dopp , and Dneutral"
        },
        {
            "title": "We do not assume that Dsupp",
            "content": "are non-overlapping, as they may have multiple segments indicating different perspectives. For example, segment S mapped to Safety for Elders may discuss clinical trial showing increased anaphylactic shock in older patients when taking Vaccine A. However, another segment from the same paper may also note severe hives from Vaccine B. Thus, we allow for such flexibility. Recent studies have shown LLMs demonstrate powerful abilities in stance detection (Zhang et al., 2024b; Lan et al., 2024). Consequently, in order to discover these perspectives, we prompt the model to first determine the stance of each segment i: Supports Claim: either implicitly or explicitly indicates that the claim is true with respect to ti. Neutral to Claim: i is relevant to the claim and aspect, but does not indicate whether the claim is true with respect to ti. Opposes Claim: either implicitly or explicitly ini , Sneutral dicates that the claim is false with respect to ti. This forms the segment sets: Ssupp , and Sopp . We ask the model to summarize the perspective (stance and rationale) of each segment set: Pi. Both prompts are provided in Appendix B.5. Since we retain the original paper source of each segment, we are able to construct Di for each node ti. This indicates consensus; for instance, how many papers in held perspective psupp towards aspect ti. As our final output, we have the aspect hierarchy , the set of perspectives Pi, and their corresponding papers Di. We sample from the top 1% of the tokens and set the temperature based on the nature of the given task (same setting across all samples); we include these settings in Appendix C. We set the maximum depth of the aspect hierarchy to = 3."
        },
        {
            "title": "4.1 Dataset",
            "content": "In order to evaluate CLAIMSPECTs abilities to deconstruct nuanced claims into hierarchy of aspects and identify their corresponding perspectives, we construct two novel, large-scale datasets specific to our task, applied to both political (World Relations) and scientific (Biomedical) domains. To construct this dataset, we first manually collect 50 reference materials discussing (1) security-related international conflicts, and (2) biomedical safety-related studies. Then, we used GPT-4o (OpenAI et al., 2024) to generate nuanced claims based on these materials. Subsequently, we used the Semantic Scholar API (Allen Institute for AI, 2025) to collect meta information relevant literature based on these claims. Then, based on this meta information, we filtered the collected literature and retrieved the full texts. This way, for each claim, we obtained corresponding literature repository. We show the statistics of each of these datasets in Table 1. More details about the dataset construction can be found in Appendix D, including human study for validating the quality of the generated claims and their associated papers in Appendix D.5."
        },
        {
            "title": "Total",
            "content": "140 50 190 9,525 3,719 1,081,241 428,833 13,244 1,510, Table 1: # of claims, papers, and segments per dataset."
        },
        {
            "title": "4.2 Baselines",
            "content": "Our primary motivation for CLAIMSPECT is to demonstrate its capabilities of deconstructing nuanced claim into an aspect hierarchy and identifying corresponding perspectives. However, no existing methods tackle this novel task. Consequently, we choose to implement and compare our method with both RAG-driven and LLMonly approaches, detailed below. We run each baseline using both Llama ( ) and GPT-4o-mini ( ): 1. RAG-Based: Given claim and definition of an aspect hierarchy, we use the claim as query to retrieve relevant documents. We then provide the documents in-context when prompt the LLM to generate the aspect hierarchy."
        },
        {
            "title": "4 Experimental Design",
            "content": "We explore CLAIMSPECTs performance on an opensource model, Llama-3.1-8B-Instruct ( ). 2. Iterative RAG-Based: Given the claim, the definition of an aspect hierarchy, and the name/description of the current node ti, we construct detailed query to retrieve node-specific Table 2: Comparison between ClaimSpect and all baselines. Sibling granularity (Sib) is normalized; all others are scaled by 100. Since Iterative Zero-Shot is not grounded with corpus, there are no associated segments to each node. Thus, we omit its segment relevance scores (Seg). We bold the top score and underline the second-highest."
        },
        {
            "title": "Unique",
            "content": "Iterative Zero-Shot 97.85 41.94 58.01 Iterative RAG 97. 45.34 59.01 Iterative Zero-Shot 98.60 42.88 64. Iterative RAG 97.40 52.30 66."
        },
        {
            "title": "CLAIMSPECT",
            "content": "95.30 78.24 85.26 CLAIMSPECT - No Disc 99.00 79. 82.64 72.96 74.25 76.01 76.59 87. 85.43 98.33 44.44 57.04 42.79 97. 45.93 59.08 97.89 41.56 62. 46.93 94.37 50.07 64.21 43.23 97. 75.10 74.80 49.47 96.07 76.26 74. 77.17 76.17 77.55 77.05 86.26 87."
        },
        {
            "title": "Seg",
            "content": ""
        },
        {
            "title": "Seg",
            "content": "27.11 31.82 27.39 39. relevant documents. We then provide these documents in-context to prompt the LLM for generating the children subaspects ti of aspect ti. We also conduct an ablation study, No Discriminative (No Disc), where we remove discriminative ranking and instead replace it with semantic similarity-based ranking. For this, we compute the semantic similarity between each segment and our ti-specific query from Section 3.2.1."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "We design thorough automatic evaluation suite using GPT-4o-mini to determine the quality of our generated taxonomies, using both node-level and taxonomylevel metrics. For each judgment, we ask the LLM to provide additional rationalization: (Node-Wise) Node Relevance: For each aspect node ti and its respective path within the hierarchy, what is its relevance to the claim t0? Scored 0/1. (Node-Wise) Path Granularity: Does the path to node ti preserve the hierarchical relationships between its entities (is each child ti more specific than the parent ti)? Scored 0/1. (Level-Wise) Sibling Granularity: For each set of siblings within the hierarchy, does the overall set reflect the same level of specificity relative to their parent aspect ti? Scored from 1 to 4 (all different some most all same). (Node-Wise) Uniqueness: Does the aspect node ti have other overlapping nodes within the hierarchy ? Scored 0/1. (Node-Wise) Segment Quality: How many segments are relevant to the claim t0 and aspect ti? We compute the average proportion of relevant segments per node. In addition to automatically evaluating our aspect hierarchy, we also conduct supplementary human evaluation on 50 perspectives and their sampled segments, which CLAIMSPECT identifies from the corpus (Section 5.2)."
        },
        {
            "title": "5.1 Overall Performance & Analysis",
            "content": "Tables 2-3 demonstrate several key advantages of CLAIMSPECT over the baselines across various node and level-wise metrics for both the World Relations and Biomedical datasets. CLAIMSPECT is able to strongly enforce the hierarchical structure of the generated aspect hierarchy while preserving relevance to the corpus. Below, we present our core findings and insights. We also provide breakdown of ClaimSpects computational efficiency in Appendix E. Finally, we additionally conducted human-automatic evaluation agreement study in Appendix A. CLAIMSPECT excels in granular aspect discovery. As shown in Table 2, CLAIMSPECT significantly outperforms the baselines in metrics associated with node-level structure, particularly outperforming Iteraby 72.6% and 63.51% in preserving hiertive RAG archical relationships (path granularity) and by 44.48% and 26.61% in maintaining uniform sibling-level specificity (sibling granularity) for both datasets respectively. This demonstrates the methods ability to retrieve and organize aspects at targeted levels of granularity. These gains are similarly observed with the GPT-based baselines, despite relying on closed-source model. We attribute this gain to ClaimSpects iterative subaspect discovery (Section 3.3) being integrated with its aspectdiscriminative retrieval mechanism (Section 3.2), where the pool of segments grounding the subaspect discovery is iteratively updated based on the given aspect node. We can see that the No Disc ablation does experience some loss in granularity quality. It is important to note that No Disc does experience competitive and, at times, better performance; this is likely due to it considering more segments, which may or may not discuss multiple aspects. In contrast, the baseline methods retrieve broader, less focused segments, reducing their ability to discover fine-grained sub-aspects. Overall, this demonstrates that ClaimSpect is able to deconstruct claim into well-structured hierarchy of aspects. Table 3: Pairwise comparisons between all methods for each dataset. Each value is the percentage of samples within each dataset where the method is considered better. E-Tie denotes Explicit Tie; I-Tie denotes Implicit Tie. Method Pair (A vs. B) Zero-Shot vs RAG Zero-Shot vs CLAIMSPECT RAG vs CLAIMSPECT"
        },
        {
            "title": "No Disc",
            "content": "vs CLAIMSPECT Zero-Shot vs RAG Zero-Shot vs CLAIMSPECT RAG vs CLAIMSPECT"
        },
        {
            "title": "A Wins B Wins",
            "content": "E-Tie I-Tie"
        },
        {
            "title": "A Wins B Wins",
            "content": "E-Tie I-Tie 0.00 0.00 0.81 21. 0.00 0.00 0.00 33.06 97.58 90. 30.00 36.00 98.00 90.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 66. 2.42 8.87 2.22 0.00 0.00 48. 24.00 64.00 2.00 10.00 0.71 0. 7.14 22.22 95.55 95.55 28.00 47. 96.43 72.14 0.00 2.22 0.00 0. 0.00 0.00 0.71 75.55 2.22 4. 48.00 52.14 3.57 20.00 Figure 3: constructed Biomedical aspect hierarchy. All nodes and their # of segments from levels 1-2 are included; subset of the third level is highlighted. The # of papers mapped to each perspective is provided in parentheses. CLAIMSPECT constructs rich aspect hierarchy while preserving relevance. In Table 2, we observe that ClaimSpects constructed hierarchy features nodes that are 14.40% and 11.23% more unique than the top baseline on each dataset, respectively. This indicates that ClaimSpects hierarchies are richer in aspect quality, experiencing less overlap between aspects across the tree and supported by an increase in segment quality. Despite this significant boost in uniqueness, ClaimSpect only experiences 3.35% and 0.386% drop from the top baseline in aspect node relevance for the World Relations and Biomedical datasets, respectively. This highlights the strength of ClaimSpects retrieval-augmented keyword enrichment and aspect-discriminative retrieval (Sections 3.2.1 and 3.2), which prioritize segments that thoroughly discuss single aspect rather than shallow descriptions of multiple aspects. This allows us to discover richer set of unique and relevant subaspects at each level, throughout the hierarchy. CLAIMSPECT is overwhelmingly preferred over baselines. Table 3 presents pairwise comparisons between CLAIMSPECT and the baseline methods. These comparisons are judged by an LLM that is shown the aspect hierarchy outputs of methods and in both possible orders: vs. and vs. A. The LLM may (1) prefer either method or B, (2) declare an explicit tie (E-Tie), or (3) indicate an implicit tie (I-Tie), which occurs when the preferred method changes depending on the order of presentation (e.g., wins in vs. B, but wins in vs. A) (Shi et al., 2024). Across both datasets, CLAIMSPECT exhibits clear advantage, being preferred 92.95% of the time, with 6.69% average inconsistency rate across all settings and datasets. Specifically, when compared with Zero-Shot , CLAIMSPECT is judged superior in 97.58% and 95.55% of cases for World Relations and Biomedical datasets, respectively. Even against RAG , CLAIMSPECT outperforms in 90.00% and 72.14% of samples. This is stark contrast from the lack of strong preference between the baselines themselves, indicated by the 64.66% average implicit tie rate implying that there is no obvious qualitative preference between the two. Finally, we show that ClaimSpect and No Disc are similarly preferred, with ClaimSpect preferred slightly more often. Overall, these results validate that CLAIMSPECT constructs significantly more meaningful aspect hierarchies relevant to the claim."
        },
        {
            "title": "5.2 Perspective Discovery Analysis",
            "content": "CLAIMSPECT identifies nuanced, corpus-specific perspectives. We showcase qualitative analysis of nuanced claims aspect hierarchy, highlighting certain subtrees and the root nodes extracted perspectives, in Fig. 3. We observe each coarse-grained aspect (yellow nodes) well represents the various angles of the root claim that would be considered in validating it: what long-term vaccine studies currently exist, what is the current mRNA technology, and how is genetic impact currently assessed? We see that the path-specific dependencies are reflected within the descriptions of each aspect (e.g., mRNA Interaction with Host Genome involves both mRNA technology and potential genetic impact risks). Furthermore, these hierarchical relationships and claim relevance are preserved even in the final layer of the hierarchy (e.g., mRNA Interaction with Host Genome mRNA degradation patterns). Finally, we see that the perspectives mapped to the root node are informative, providing justification behind each stance. Note that ClaimSpect maps segments to each perspective, allowing us to identify the original paper sources and ultimately provide corpus-specific estimate of the consensus. Overall, this deconstructed view of the claim provides means to identify which and to what degree certain aspects have been explored (e.g., mRNA Technology has been more explored within the corpus compared to Genetic Impact Assessment). World Relations Biomedical 5 10 15 72% (50) 80% (20) 85% (19) 72% (50) 82% (20) 89% (9) Table 4: Human validation on the percentage of perspectives discovered by CLAIMSPECT which are grounded in at least one of associated segments. = # of segments considered. We provide the number of samples for each setting in parenthesis. Human annotators validate the grounding of discovered perspectives. To assess the validity of the perspectives discovered by CLAIMSPECT, we apply human evaluation to evaluate whether these perspectives are effectively grounded in the corpus. We randomly sampled perspectives along with their associated segments (each aspect node has three ass from the generated results across two datasets. The evaluation metric used was whether at least one segment in could provide grounding background knowledge for the corresponding perspective. As shown in Table 4, we found that the vast majority of cases (85% and 89% for each dataset respectively) are supported by specific literature segments. Furthermore, we can see that the support rate steadily increases as we retrieve more segments that are mapped to the perspective. This shows the perspectives identified by CLAIMSPECT are largely supported by the corpus."
        },
        {
            "title": "6 Conclusion",
            "content": "Our work introduces CLAIMSPECT, novel framework for deconstructing nuanced claims into hierarchy of corpus-specific aspects and perspectives. By integrating iterative, aspect-discriminative retrieval with hierarchical sub-aspect discovery and perspective clustering, CLAIMSPECT provides structured, comprehensive view of complex claims. Our experiments on two novel, large-scale datasets demonstrate that CLAIMSPECT constructs rich, corpus-aligned aspect hierarchies that are enriched with diverse and informative perspectives. This highlights its effectiveness as scalable and adaptable method for nuanced claim analysis across domains."
        },
        {
            "title": "7 Limitations & Future Work",
            "content": "The primary contribution of CLAIMSPECT is our retrieval-augmented framework for constructing an aspect hierarchy relevant for validating nuanced claim. In order to demonstrate the hierarchys potential, we apply it to the task of perspective discovery, involving (1) identifying which segments from the corpus are relevant to given aspect node, (2) determining the stance (or lack thereof) of the segment towards the claim and aspect, and (3) discovering the potential perspective of each of the stance-based segment clusters. Consequently, this step relies heavily upon an existing hierarchical classification model (Zhang et al., 2024a), as we do not claim novelty with respect to classification. Similarly, our classification-based perspective discovery (Section 3.4) is reliant on the LLMs fine-grained stance detection abilities although prior work (Zhang et al., 2024b; Lan et al., 2024) has shown precedence for its capabilities. Thus, the performance of the hierarchical classification and stance detection serves as bottleneck to our methods performance. For example, if the LLM-based stance detection has high recall but low precision for detecting segments which support the aspect of claim, then the method may overestimate the consensus behind certain perspective within the corpus. Likewise, if the detection has high precision but lower recall, it may underestimate the consensus. Nonetheless, our work aims to, overall, motivate the need to structure the aspects of certain nuanced claims before diving straight into their validation. Hierarchically analyzing nuanced claims opens up doors to many new avenues of research. First, CLAIMSPECT can be integrated with more systematic and/or tool-integrated fact validation systems, in an effort to build more robust fact-checking system. Furthermore, CLAIMSPECT can be applied to more targeted retrieval or question answering tasks where question, similar to nuanced claim, cannot easily be answered and may benefit from more structured output (similar to an aspect hierarchy)."
        },
        {
            "title": "8 Acknowledgements",
            "content": "This work was supported by the National Science Foundation Graduate Research Fellowship. The work was also supported in part by the BRIES Program No. HR0011-24-3-0325. This research used the DeltaAI advanced computing and data resource, which is supported by the National Science Foundation (award OAC 2320345) and the State of Illinois. DeltaAI is joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications. We thank Peter Bautista, Spencer Lynch, and Svitlana Volkova from Aptima, Inc. for their discussions on our work. We also thank Mihir Kavishwar for early ideation discussions."
        },
        {
            "title": "References",
            "content": "Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the 2016 election. Journal of Economic Perspectives, 31(2):211236. Allen Institute for AI. 2025. Semantic Scholar API. Accessed: 2025-02-15. Pepa Atanasova, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, Georgi Karadzhov, Tsvetomila Mihaylova, Mitra Mohtarami, and James Glass. 2019. Automatic fact-checking using context and discourse information. Journal of Data and Information Quality (JDIQ), 11(3):127. Boqi Chen, Fandi Yi, and Dániel Varró. 2023. Prompting or fine-tuning? comparative study of large language models for taxonomy construction. In 2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C), pages 588596. IEEE. Zhen-Song Chen, Xuan Zhang, Rosa Rodríguez, Witold Pedrycz, Luis Martínez, and Miroslaw Skibniewski. 2022. Expertise-structure and risk-appetiteintegrated two-tiered collective opinion generation framework for large-scale group decision making. IEEE Transactions on Fuzzy Systems, 30(12):5496 5510. Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In 1st Meeting of the North American Chapter of the Association for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997. Georgi Karadzhov, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, and Ivan Koychev. 2017. Fully automated fact checking using external sources. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 344353, Varna, Bulgaria. INCOMA Ltd. Xiaochong Lan, Chen Gao, Depeng Jin, and Yong Li. 2024. Stance detection with collaborative roleinfused llm-based agents. In Proceedings of the International AAAI Conference on Web and Social Media, volume 18, pages 891903. David MJ Lazer, Matthew Baum, Yochai Benkler, Adam Berinsky, Kelly Greenhill, Filippo Menczer, Miriam Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild, et al. 2018. The science of fake news. Science, 359(6380):10941096. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. 2016. Semeval2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 3141. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, et al. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. Jason Alan Palmer. 2024. pdftotext. Gordon Pennycook and David Rand. 2021. The psychology of fake news. Trends in Cognitive Sciences, 25(5):388402. Nicolas Perony, René Pfitzner, Ingo Scholtes, Claudio Tessone, and Frank Schweitzer. 2013. Enhancing consensus under opinion bias by means of hierarchical decision making. Advances in Complex Systems, 16(06):1350020. Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum. 2018. Declare: Debunking fake news and false claims using evidence-aware deep learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2232. David M. W. Powers. 1998. Applications and explanations of Zipfs law. In New Methods in Language Processing and Computational Natural Language Learning. PubMed. 2025. PubMed. https://pubmed.ncbi. nlm.nih.gov. Accessed: 2025-02-15. Google Scholar. 2025. Google Scholar. https:// scholar.google.com. Accessed: 2025-02-15. Karishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Minghui Zhang, and Yan Liu. 2019. Combating fake news: survey on identification and mitigation techniques. ACM Transactions on Intelligent Systems and Technology (TIST), 10(3):142. Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, and Wei Zhang. 2024a. Assessing implicit retrieval robustness of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 8988 9003, Miami, Florida, USA. Association for Computational Linguistics. Yanzhen Shen, Yu Zhang, Yunyi Zhang, and Jiawei Han. 2024b. unified taxonomy-guided instruction tuning framework for entity set expansion and taxonomy expansion. arXiv preprint arXiv:2402.13405. Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, and Soroush Vosoughi. 2024. Judging the judges: systematic investigation of position bias in pairwise comparative assessments by llms. arXiv preprint arXiv:2406.07791. Yushi Sun, Hao Xin, Kai Sun, Yifan Ethan Xu, Xiao Yang, Xin Luna Dong, Nan Tang, and Lei Chen. 2024. Are large language models good replacement of taxonomies? arXiv preprint arXiv:2406.11131. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9961011, Nusa Dua, Bali. Association for Computational Linguistics. Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Rui Li, Jinfeng Xiao, Jiaming Shen, and Jiawei Han. 2024a. Teleclass: Taxonomy enrichment and llm-enhanced hierarchical text classification with minimal supervision. arXiv preprint arXiv:2403.00165. Zhao Zhang, Yiming Li, Jin Zhang, and Hui Xu. 2024b. Llm-driven knowledge injection advances zero-shot and cross-target stance detection. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 371378."
        },
        {
            "title": "James",
            "content": "Andreas Vlachos, Christos Thorne, and Arpit Mittal. 2018. Christodoulopoulos, FEVER: large-scale dataset for fact extraction In Proceedings of the 2018 and VERification. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana. Association for Computational Linguistics. Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. Science, 359(6380):11461151. Somin Wadhwa, Vivek Khetan, Silvio Amir, and Byron Wallace. 2023. RedHOT: corpus of annotated medical questions, experiences, and claims on social media. In Findings of the Association for Computational Linguistics: EACL 2023, pages 809827, Dubrovnik, Croatia. Association for Computational Linguistics. Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, and Meng Jiang. 2024a. Chain-of-layer: Iteratively prompting large language models for taxonomy induction from limited examples. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 30933102. Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Zhenyu Wu, Shangbin Feng, and Meng Jiang. 2024b. Codetaxo: Enhancing taxonomy expansion with limited examples via code language prompts. arXiv preprint arXiv:2408.09070. Xuan Zhang and Wei Gao. 2023. Towards LLM-based fact verification on news claims with hierarchical step-by-step prompting method. In Proceedings of Human Annotator Background and Human-Automatic Alignment for Evaluation We perform human evaluation study to show the alignment between humans and GPT-4o-mini for evaluation. We use two human evaluators to evaluate randomly sampled cases for each of our proposed metrics: node relevance, path granularity, sibling granularity, uniqueness and segment quality. Our human evaluators are two volunteer graduate researchers (one PhD student and Masters student), with one having background in Biology + NLP (critical for our biomedical task evaluation). We used the following instructions to help guide them in the evaluation task, with an initial training period where the evaluators could familiarize themselves with the task (e.g., aspect taxonomies and their expected hierarchical relationships) and discuss examples with one another, but the full evaluation period was conducted independently: 1. General Instruction: Claims made by individuals or entities are often nuanced and cannot always be strictly categorized as entirely true or false, particularly in scientific and political contexts. Instead, claim can be broken down into its core aspects and sub-aspects, which are easier to evaluate individually. 2. Node Relevance: Given the claim: [claim], decide whether this path from the aspect tree is relevant to the analysis of the claim: [path] relevant or irrelevant? 3. Path Granularity: Given the claim: [claim], decide whether this path from the aspect tree has good granularity: [path]. Check whether the child node is more specific subaspect of the parent node. granular or non-granular? 4. Sibling Granularity: Given the claim: [claim], decide whether these siblings from parent node [parent] have good granularity. <proportion of granular siblings> 5. Uniqueness: Normally, we want the aspects and sub-aspects to be unique in the taxonomy. Given the claim: [claim], count how many nodes in this taxonomy are largely overlapping or almost equivalent. <total count of overlapping nodes> 6. Segment Quality: Given the claim: [claim], evaluate the quality of these segments for aspect [aspect node label]. [list of mapped segments] <total count of relevant segments to aspect node> and Intraclass Correlation Coefficient (high instability). Nonetheless, the evaluators have 100% and 96.97% agreement rate respectively. We choose Cohens κ or ICC based on the ordinal/continuous versus categorical nature of the metric. 1. The weighted κ for Path Granularity is 0.62 substantial agreement 2. The ICC1k for Sibling Granularity is 0.7806 good reliability 3. The ICC2k for Segment Quality is 0.7578 good reliability We also show the agreement rate below on 100 different samples across all metrics in Table 5. Rel Path Sib Unique Seg Agreement Rate 100% 85% 87.5% 96.97% 82% Table 5: Human-Automatic Agreement rates across different metrics. We can see that the LLM evaluation and human evaluators have high degree of alignment. We note that segment quality features the lowest alignment (albeit still relatively high rate), likely due to the more finegrained text understanding abilities required for verifying segment alignment to the parent node (where oftentimes, segments can be quite semantically dissimilar or only discuss sub-aspect of the node). Nonetheless, through these results, we can see that **our autoevaluation is reliable**."
        },
        {
            "title": "B Prompt Template",
            "content": "In this section, we present the prompts used in different modules of CLAIMSPECT. B.1 Coarse-Grained Aspect Discovery This is the prompt used to generate coarse-grained aspects for the root claim, including their labels, description, and relevant keywords to structure the initial retrieval-augmented hierarchy."
        },
        {
            "title": "Prompt",
            "content": "For the topic, {topic}, output the list of up to {k} aspects in JSON format. Since ClaimSpect has consistently high scores on Node Relevance and Uniqueness (not very many nodes are irrelevant or overlapping across the entire taxonomy), the scores from both the LLM and evaluators have very low variance. This greatly limits both Cohens B.2 Retrieval-Augmented Keyword Enrichment Following are the prompts used for retrieval-augmented keyword enrichment, instructing the LLM to refine and filter aspect-specific keywords for improved segment ranking. Prompt (Extraction) The claim is: {claim}. You are analyzing it with focus on the aspect {aspect_name}. The aspect, {aspect_name}, can be described as the following: {aspect_description} Please extract at most {2*max_keyword_num} keywords related to the aspect {aspect_name} from the following documents: {contents} Ensure that the extracted keywords are diverse, specific, and highly relevant to the given aspect. Only output the keywords and seperate them with comma. Your output should be in JSON format. Prompt (Filtering) Our claim is {claim}. With respective to the target aspect {aspect_name}, identify {min_keyword_num} to {max_keyword_num} relevant keywords from the provided list: {keyword_candidates}. {aspect_name}: {aspect_description} Merge terms with similar meanings, exclude relatively irrelevant ones, and output only the final keywords separated by commas. Your output should be in JSON format. B.3 Iterative Subaspect Discovery Following is the prompt used to iteratively guide the LLM in discovering and expanding subaspects for each aspect node based on discriminative retrieval and root claim context. Prompt Output the list of up to {k} subaspects of parent aspect {aspect} that would be considered when evaluating the claim, {topic}. claim: {topic} parent_aspect: {aspect}; {aspect_description} path_to_parent_aspect: {aspect_path} Provide your output in the following JSON format. B.4 Relevance Filtering Prompt am currently analyzing claim based on segment from the literature from several different aspects. The segment is: {segment} The claim is: {claim} The aspects are: {aspects} Please help me determine whether this segment is related to the claim so that can analyze this claim based on it from at least one of these aspects. Your output should be Yes or No in JSON format. B.5 Perspective Discovery Following are prompts used to for determining segment stances (support, neutral, or oppose) and summarizing perspectives, including rationales, for each aspect."
        },
        {
            "title": "Prompt",
            "content": "to {aspect_name} for You are stance detector, which determines the stance that segment from scientific paper has towards an aspect of specific claim. Oftentimes, scientific papers do not provide explicit, outright stances, so your job is to figure out what stance the data or statement that they are presenting implies. Segment: {segment.content} What is the segments stance specifically with respect if {claim}? {aspect_name} can be described as {aspect_description}. Claim: {claim} Aspect to consider: {aspect_name}: {aspect_description} Path to aspect: {aspect_path} Your stance options are the following: - supports_claim: The segment either implicitly or explicitly indicates that claim is true specific to the given aspect. - neutral_to_claim: The segment is relevant to the claim and aspect, but does not indicate whether the claim is true specific to the given aspect. - opposes_claim: The segment either implicitly or explicitly indicates that the claim is false specific to the given aspect. - irrelevant_to_claim: The segment does not contain relevant information on the claim and the aspect."
        },
        {
            "title": "C Generation Settings",
            "content": "This section details the temperature values used in various stages of our process and their respective roles. C.1 Overview of Temperature Settings Coarse-Grained Aspect Discovery (0.3): Used to generate high-level aspects related to the claim. lower temperature ensures structured and deterministic output. Following is the prompt used for relevance filtering, leveraging binary search on cosine-similarity rankings to efficiently identify and retain only the most relevant segments for each aspect. Subaspect Discovery (0.7): Used for identifying subaspects from ranked segments. higher temperature allows for more diversity while maintaining coherence."
        },
        {
            "title": "Positive Claim Generation Prompt",
            "content": "Scientific or political claims are often nuanced and multifaceted, rarely lending themselves to simple yes or no answers. To answer such questions effectively, claims must be broken into specific aspects for in-depth analysis, with evidence drawn from relevant scientific literature. We are currently studying such claims using this corpus: {context} Task: Generate 10 nuanced and diverse claims based on this corpus. The claims should adhere to the following criteria: 1. Diversity: The claims should be sufficiently varied: they should involve diverse sub-topics in the context 2. Complexity: The claims should be complex and controversial (and not necessarity true), requiring multi-aspect analysis rather than simplistic treatment. Avoid overly straightforward or simplistic claims. 3. Research Feasibility: The claims should not be too specific and should pertain to topics with likely body of existing literature to support evidence-based exploration. 4. Concision: The claims should be concise and focused in one short sentence. 5. Completeness: The claims should be complete and not require additional context to understand. Output: Provide the claims as list. OpenAI Chat Models (GPT-4o (OpenAI et al., 2024), GPT-4o-mini (OpenAI et al., 2024)) (0.3): Applied in various stages where GPT-4o models are used (e.g., aspect generation, classification), ensuring consistent responses. Subaspect Discovery (Aspect Ranking and Retrieval) (0.7): Used when extracting subaspects from ranked segments to balance creativity with relevance. C.2 General Trends Lower temperature (0.3) is used for structured and deterministic tasks such as aspect generation and classification. Higher temperature (0.7) is applied to subaspect discovery, where diversity and exploration are beneficial."
        },
        {
            "title": "D Dataset Construction",
            "content": "To evaluate the effectiveness of CLAIMSPECT, our nuanced claims analysis, we constructed two datasets covering two key domains: political (World Relations) and scientific (Biomedical). The dataset construction process consists of the following steps: D.1 Manual Seed Collection We begin by manually collecting set of seed claims from reliable sources such as Google Scholar (Google Scholar, 2025) and PubMed (PubMed, 2025). Specifically, we collect material from 7 papers in the World Relations domain and 50 papers in the Biomedical domain. These initial materials serve as context or specific topics for generating nuanced claims. D.2 Nuanced Claims Generation Using the literature collected in the previous step and definition of nuanced claims as context, we prompt GPT-4o (OpenAI et al., 2024) to generate nuanced claims related to the topics within these papers. To ensure diversity in claim perspectives, we employ two sets of prompts: one for generating claims that align with the perspectives in the literature and another for generating claims that diverge from them. The specific prompts used are detailed below."
        },
        {
            "title": "Claims for Biomedical Domain",
            "content": "Scientific or political claims are often nuanced and multifaceted, rarely lending themselves to simple yes or no answers. To answer such questions effectively, claims must be broken into specific aspects for in-depth analysis, with evidence drawn from relevant scientific literature. We are currently studying such claims using this corpus: {context} Task: Generate 10 nuanced and diverse claims based on this corpus. The claims should adhere to the following criteria: 1. Diversity: The claims should be sufficiently varied: they should involve diverse sub-topics in the context 2. Complexity: The claims should be complex and controversial (and not necessarity true), requiring multi-aspect analysis rather than simplistic treatment. Avoid overly straightforward or simplistic claims. 3. Research Feasibility: The claims should not be too specific and should pertain to topics with likely body of existing literature to support evidence-based exploration. 4. Concision: The claims should be concise and focused in one short sentence. 5. Completeness: The claims should be complete and not require additional context to understand. 6. The claims should be against the point of view in the context. Output: Provide the claims as list. We find that the generated nuanced claims are of high quality. They are content-rich, specific, and difficult to classify as simply true or false, aligning well with our task requirements. Below are some example claims from our datasets."
        },
        {
            "title": "Claims for World Relations",
            "content": "1. International collaborations under the Global Nuclear Security Program prioritize geopolitical alliances over immediate nuclear threat reduction. 2. Counteracting WMDs through international partnerships creates dependency and may hinder national self-sufficiency in threat reduction capabilities. 3. The effectiveness of the biological threat reduction component is questionable given the rise and global spread of emerging biological threats. 1. COVID-19 vaccine safety evaluations are compromised by inconsistent application of evidence standards across different data sources like RCTs and VAERS. 2. The rigid adherence to optimized distribution plans might inhibit the flexibility needed to respond to unforeseen disruptions in the vaccine supply chain. 3. Keeping manufacturing costs secret is essential for protecting proprietary processes and innovations in the pharmaceutical industry. D.3 Meta Information Collection To support the corpus-based analysis of each claim, we retrieve relevant literature using the Semantic Scholar API (Allen Institute for AI, 2025). Since our claims are highly nuanced and involve multiple concepts, directly searching for claims themselves does not yield useful matches based on literature titles and abstracts. To address this, we first perform keyword extraction for each claim. We then use the extracted keywords to query the Semantic Scholar API and retrieve up to 1000 related literature entries for each claim. D.4 Filtering and Full-Text Collection After obtaining the literature metadata, we first filter out entries with missing fields and retain the top 100 most relevant papers based on relevance. We then utilize the provided PDF URLs to download the full-text of the selected literature and convert them into plain text with pdftotext (Palmer, 2024). As result, we obtain comprehensive textual literature repository for each claim, ensuring rich contextual foundation for further analysis. This structured approach ensures robust dataset suitable for nuanced claims analysis across the domains. D.5 Human Validation of Generated Claims and"
        },
        {
            "title": "Assigned Papers",
            "content": "We conducted human evaluation study for validating 40 total claims20 on each dataset. We define the following binary criteria for claim validation: 1. Nuanced: Is the claim obviously true or false? 2. Relevant:"
        },
        {
            "title": "Is the claim relevant to the topic",
            "content": "(biomedical/world relations)? 3. Corpus-Aligned@k: At least papers are relevant to the claim within the corpus (this is computed on = 5 and = 10) We show the validation results in Table 6, which demonstrates the nuanced nature of the generated claims, their relevancy, and the presence of papers aligned to each claim. module. Thus, instead of sequentially determining claim relevancy for each segment, we sort the segments (which have more than 500 characters) and use binary search (O(log S)) to find the boundary of relevance-irrelevance. This leads to O(S log S) efficiency due to the sorting function 5 seconds during runtime, due to Python optimizations. * Perspective Discovery involves prompting the LLM for each filtered segments stance: O(S) 3 minutes We specifically use vLLM to optimize our LLM batched generation. To construct claim with 39 nodes and max depth of 3, it takes approximately 20 minutes to run on two NVIDIA RTX A6000s. We can see that in total, the core framework operations take 13 minutes and 42 seconds, with the remaining time dedicated to embedding computations (which can be done offline)."
        },
        {
            "title": "World Relations Biomedical",
            "content": "Nuanced Relevant Corpus-Aligned@5 Corpus-Aligned@10 0.9 1.0 0.95 0.65 1.0 1.0 0.8 0.65 Table 6: Human validation of claim quality across datasets."
        },
        {
            "title": "E Computational Efficiency",
            "content": "We specify the components of CLAIMSPECTs framework and their corresponding computational efficiency across the entire pipeline below. We consider the number of nodes within full aspect hierarchy as and the total number of segments within the corpus as S. We additionally provide rough time estimates based on an average sample. Coarse-Grained Aspect Discovery (Section 3.1.3) single LLM call: O(1) Aspect-Discriminative Retrieval (Section 3.2) Retrieval-Augmented Keyword Enrichment (Section 3.2.1) * Embedding the segments using the retrieval model takes the most amount of time (O(S)), but this can be computed offline given knowledge base. Retrieval itself is quite efficient since it is embeddingbased, and we use cosine-similarity to determine relevance (an efficient computation, especially in high-dimensional scenarios). * Enrich each node: O(N ) 10 seconds per node Discriminative Segment Ranking (Section 3.2.2) * We only compute the ranking on the top100 segments, so this operations efficiency is constant: O(1) * The target score and distractor score computation scales according to the number of aspects throughout the tree (since their # of associated keywords is constant): O(N ) 6 seconds for each node Iterative Subaspect Discovery (Section 3.3) single prompt per aspect node in hierarchy: O(N ) Classification-Based Perspective Discovery (Section 3.4) As mentioned in lines 442446, we reframe relevance filtering as binary search problem intentionally to optimize the efficiency of this"
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Illinois at Urbana-Champaign"
    ]
}