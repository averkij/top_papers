{
    "paper_title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning",
    "authors": [
        "Ziyan Wang",
        "Zheng Wang",
        "Jie Fu",
        "Xingwei Qu",
        "Qi Cheng",
        "Shengpu Tang",
        "Minjia Zhang",
        "Xiaoming Huo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts and a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best accuracy."
        },
        {
            "title": "Start",
            "content": "SLOW-FAST POLICY OPTIMIZATION: REPOSITIONBEFORE-UPDATE FOR LLM REASONING Ziyan Wang1,, Zheng Wang2,, Minjia Zhang2 Xiaoming Huo1 1 Georgia Institute of Technology {wzy, huo}@gatech.edu 2 University of Illinois Urbana-Champaign {zhengw10, minjiaz}@uiuc.edu Jie Fu Xingwei Qu Qi Cheng Shengpu Tang 5 2 0 2 5 ] . [ 1 2 7 0 4 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from lowquality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), simple yet efficient framework to address the above limitations via decomposing each step into three stages: short fast trajectory of inner steps on the same batch, reposition mechanism to control off-policy drift, and final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces number of rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93 fewer rollouts and 4.19 reduction in wall-clock time to match GRPOs best accuracy."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have recently achieved remarkable progress on complex multi-step reasoning tasks, especially in mathematics problem solving and scientific question answering (OpenAI et al., 2024; DeepSeek-AI et al., 2025b). central driver of these advances has been reinforcement learning (RL), which fine-tunes LLMs using reward signals tied to semantic correctness and solution quality. For example, DeepSeekMath (Shao et al., 2024) demonstrates that Group Relative Policy Optimization (GRPO) can substantially improve open-source LMs on mathematical reasoning, while DeepSeek-R1 (Guo et al., 2025) shows that RL signals alone can induce emergent reasoning behaviors across various domains. Despite these successes, GRPO inherits structural inefficiencies that make training fragile in LLM reasoning. During the early stages of training, when rollouts are weak or uninformative, stochastic rewards induce high-variance gradients that destabilize updates. Prior studies corroborate this issue: early rollouts often fail to provide useful signals (Zheng et al., 2025), reward shaping can trigger sudden collapse (Dai et al., 2025), and noisy group normalization further impairs learning under imbalanced responses (Shen et al., 2025). Although group normalization partially mitigates variance, GRPO still applies only single gradient update per batch, discarding information that multiple inner steps could exploit (Schulman et al., 2017; Engstrom et al., 2020). As result, reasoning RL training tends to be sample-inefficient, with early exploration fragile and progress highly sensitive to noise and rollout quality. Although the original GRPO paper (Shao et al., 2024) introduces Iterative GRPO to reuse rollout data, this is primarily designed for updating the reward model. We find that applying *Equal contribution. Corresponding authors. 1Project website is available at https://zkbig.github.io/Slow_Fast_Policy_ Optimization.github.io/."
        },
        {
            "title": "SFPO",
            "content": "Figure 1: Pipeline of SFPO at step s. Starting from the current policy πθs,0 , we first generate rollouts for training. Stage (Fast Trajectory): apply successive gradient updates on the same batch to obtain θs,K. Stage II (Reposition): interpolate between θs,K and the starting point θs,0 to form (cid:101)θs,K, controlling off-policy drift. Stage III (Slow Correction): perform one additional update on (cid:101)θs,K, yielding πθs+1,0 for the next step. this off-policy update to the policy model can degrade performance, and how to use it effectively to build strong reasoning abilities in LLMs remains an open question. In this work, we introduce SlowFast Policy Optimization (SFPO), simple yet efficient mechanism that augments on-policy policy-gradient methods such as GRPO while keeping the loss, rollout collection, and KL/clip regularization intact. Each training step is restructured into three coordinated stages: (i) fastmultiple inner updates on the same batch to stabilize the search direction; (ii) repositioninterpolation back toward the starting point to control off-policy drift; and (iii) slow an extra gradient correction to align with local curvature. This reposition-before-update design transforms noisy one-shot updates into structured trajectories, yielding more stable optimization, higher sample efficiency, and faster convergence. We empirically validate the efficacy of SFPO through extensive experiments across five models and size reasoning benchmarks. The evaluation shows that SFPO outperforms GRPO by up to 2.80 points on math reasoning benchmarks. It also uses up to 4.93 fewer rollouts while reducing wall-clock time by 4.19 to reach GRPOs best accuracy. We summarize our contributions as follows: We propose SFPO, plug-compatible update rule that reuses rollout batches via fastrepositionslow decomposition, with theoretical insights for each stage. SFPO introduces no changes to the underlying objective, rollout generation, or regularization, enabling drop-in integration into existing LLM reasoning pipelines. Through extensive experiments on math reasoning benchmarks, we show that SFPO consistently improves stability, sample efficiency, and convergence speed over GRPO."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Group Relative Policy Optimization (GRPO) (Shao et al., 2024) is policy gradient method that has been widely applied in large-scale LLM training pipelines. GRPO dispenses with an explicit value function and instead normalizes rewards across group of responses to the same prompt. Given an input q, the policy πθ generates candidate sequences {oi}G i=1. The normalized advantage for each candidate is defined as i=1 with corresponding rewards {ri}G (cid:98)Ai = rimean({ri}G i=1) std({ri}G i=1) . (1) This group-based normalization can be interpreted as form of reward shaping: by emphasizing relative differences among candidate sequences for the same input, GRPO strengthens the reliability of the gradient signal. Instead of embedding KL penalty inside the reward, GRPO directly regularizes the policy by including an explicit KL term between the learned policy and reference policy. The"
        },
        {
            "title": "SFPO",
            "content": "training objective is JGRP O(θ) = qP (Q),{oi}G i=1πθold (Oq) (cid:88) 1 1 oi oi (cid:88) min(ri,t(θ) (cid:98)Ai,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) (cid:98)Ai,t) βDKL[πθπref ] (2) t=1 i=1 where ri,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) is the token-level importance ratio between the new and old policies, ϵ is the clipping range that prevents overly aggressive updates, β controls the KL regularization strength, and πref is fixed reference policy. Here, denotes the input prompt, oi generated sequence, and oi,t its t-th token. Limitations of GRPO. Despite its popularity, GRPO inherits structural drawbacks from the underlying on-policy update rule. At each step, the parameters are updated via single stochastic gradient step estimated from batch of rollouts. The randomness of rewards leads to high-variance gradient estimates, making updates unstable. Group normalization partially dampens this effect but remains sensitive to fluctuations within each batch. Moreover, restricting each batch to single update discards potentially useful gradient information across inner steps, leading to inefficient data use and limited variance reduction. These drawbacks underscore the need for an update mechanism that can stabilize gradient directions while making more effective use of available samples."
        },
        {
            "title": "3 METHOD",
            "content": "To address the instability and inefficiency of one-shot policy updates, we propose Slow-Fast Policy Optimization (SFPO), simple yet general update mechanism that reuses rollouts more effectively while remaining plug-compatible with standard on-policy policy gradient algorithms. Each step consists of three stages: (i) fast trajectory of multiple inner updates on the same batch, (ii) reposition step that interpolates back toward the on-policy point to control drift, and (iii) slow correction via an extra-gradient update. As illustrated in Fig. 1 and Alg. 1, this fastrepositionslow design transforms noisy one-shot updates into well-structured update trajectories, yielding more stable optimization and higher sample efficiency without additional rollouts. Notation. To align with standard gradient-based optimization notation, we define L(θ) = (θ), so that maximizing the objective is equivalent to minimizing the loss L. 3.1 STAGE I: FAST TRAJECTORY In standard on-policy policy-gradient methods such as GRPO, each step is updated by single stochastic gradient: θs+1 = θs ηθL(θs), (3) where θL(θs) is estimated from one batch of rollouts. Such one-shot updates suffer from high variance and often drive the policy in unstable directions, especially during early training. SFPO mitigates this by performing multiple inner updates on the same batch or rollouts. Formally, starting from parameters θs,0 at the beginning of step s, we execute short fast trajectory of inner updates: θs,k+1 = θs,k ηθL(θs,k), = 0, . . . , 1. (4) This produces sequence θs,0 θs,1 θs,K, where each step refines the gradient direction using the same rollout data. Intuition. Unlike one-shot updates that rely on single noisy gradient, the displacement θs,K θs,0 = η K1 (cid:88) k=0 θL(θs,k) (5) captures the cumulative effect of sequential corrections. Even if some individual steps are perturbed by noise, their composition tends to damp idiosyncratic fluctuations and align with the underlying"
        },
        {
            "title": "SFPO",
            "content": "gradient direction. Geometrically, this can be viewed as integrating the local gradient field along short trajectory in parameter space rather than trusting single noisy vector at θs,0. As result, the update direction at the end of Stage is typically more stable and less sensitive to randomness in any single gradient estimate. Compact Mathematical Intuition. Let θk := θs,k and consider the inner steps θk+1 = θk ηL(θk) starting at θ0 = θs,0. In small neighborhood, linearizing the gradient field, L(θ) g0 + H0(θ θ0) with g0 = L(θ0) and H0 = 2L(θ0), yields the closed-form displacement θK θ0 (cid:2)I (I ηH0)K(cid:3) 0 g0, (6) where 0 denotes the (pseudo)inverse on the range of H0 (the λ 0 case is understood by continuity). Spectrally, along an eigen-direction with curvature λ 0, the scalar gain is (cid:0)1 (1 ηλ)K(cid:1)/λ: for small λ it behaves like Kη (steadily accumulating progress in gentle directions), while for larger λ it saturates below 1 (damping stiff-direction oscillations). Thus the fast trajectory acts as curvatureaware low-pass filter that stabilizes the endpoint direction relative to one-shot step. We use this as local intuition under sufficiently small step size (e.g., η < 1/H02) and in neighborhoods where positive-curvature directions dominate; in practice, KL/clip regularization and Stage II reposition mitigate adverse effects of negative curvature. 3.2 STAGE II: REPOSITION While the fast trajectory of Stage improves stability, it also changes the nature of the update from on-policy to off-policy. Since all inner steps θs,1, . . . , θs,K reuse the same rollouts generated at θs,0, the endpoint θs,K no longer corresponds to the distribution that produced those samples. This distribution mismatch is fundamental drawback of off-policy learning, as it biases gradient estimates and can destabilize training. Inspired by Lookahead Optimization (Zhang et al., 2019), SFPO introduces reposition step that interpolates the fast trajectory back toward its starting point: (cid:101)θs,K = θs,0 + α(θs,K θs,0), α [0, 1]. (7) Here α regulates the degree of off-policy drift: smaller values keep the update close to the original on-policy iterate, while larger values rely more on the fast trajectory at the risk of greater mismatch. Intuition. The interpolation is equivalent to solving linearized proximal subproblem around θs,0: min θ (cid:68) K1 (cid:88) k=0 θL(θs,k), θ θs,0(cid:69) + λ 2 θ θs,02, (8) whose unique solution coincides with (cid:101)θs,K for λ = 1 αη . Thus, α acts as an implicit trust-region radius: smaller α implies larger proximal weight λ, enforcing stronger contraction toward the on-policy point. 2 3.3 STAGE III: SLOW CORRECTION After repositioning, SFPO applies one more (slow) correction step at the interpolated point: θs+1 = (cid:101)θs,K ηθL((cid:101)θs,K). (9) This yields predictorcorrector structure: Stage produces stabilized fast trajectory, Stage II tempers off-policy drift via reposition, and Stage III applies slow correction aligned with the local curvature at the update point. 2A heuristic alternative is obtained if one replaces (cid:80) θL(θs,k) by the averaged gradient and uses the approximation θs,K θs,0 ηK g, leading to λ 1/(αηK)."
        },
        {
            "title": "SFPO",
            "content": "Algorithm 1 SFPO: unified fastrepositionslow update. Require: Initial policy πθ0,0 , dataset D, hyperparameters S, K, α0, η, ω, τ , loss L(θ) Initialize rolling buffer , stats (µ, σ) (0, 1), trigger index + for = 0, 1, . . . , 1 do Generate rollouts with the current policy πθs,0 on prompts from D. α α0 1[ < ] if α = 0 then (cid:101)θs,K θs,0 // Skip fast trajectory & reposition // Set α for this step from past trigger (non-anticipatory) else for = 0, 1, . . . , 1 do θs,k+1 θs,k ηθL(θs,k) end for (cid:101)θs,K θs,0 + α(cid:0)θs,K θs,0(cid:1) end if θs+1,0 (cid:101)θs,K ηθL((cid:101)θs,K) Zs Hsµs σs+ε if = + and Zs τ then (ε for numerical stability) // Stage I: Fast Trajectory // Stage II: Reposition // Stage III: Slow Correction + 1 // will set α = 0 for all future s end if end for return final policy πθS,0 Theoretical intuition. Under L-smoothness and sufficiently small η, the descent lemma implies E[L(θs+1)] L(θs,0) ηL(θs,0)2 + O(cid:0)η2L F(K, α)(cid:1), (10) where F(K, α) represents the combined effect of Stage (fast trajectory length K) and Stage II (reposition factor α). Intuitively, F(K, α) reflects balance between exploiting more gradient information and controlling distributional drift: increasing leverages the same rollout data across multiple steps but also amplifies off-policy mismatch, while larger α interpolates more aggressively toward the fast trajectory at the risk of greater instability. In practice, since increasing raises both wall-clock cost and F(K, α), we adopt small with moderately large α, while the slow correction in Stage III mitigates overshooting and preserves progress along the stabilized trajectory direction. 3.4 SCHEDULING α nonzero α is essential to exploit the stabilized fast trajectory when > 0: if α = 0, the reposition collapses to θs,0 and the fast trajectory is discarded, eliminating the benefit of Stage I. However, the same aggressiveness that helps during early training can be counter-productive near minimizer. When is large, nonzero α accelerates progress by moving along the stabilized fast direction; but as we approach minimum, the signal weakens while curvature and stochastic noise dominate, so large α amplifies drift and instability. This motivates an adaptive schedule for α. Why adapt α rather than K? controls both stabilization and wall-clock runtime: increasing reduces oscillations but incurs proportional compute cost, making mid-training changes impractical. By contrast, α is soft trust parameter: it decides how much of the stabilized fast trajectory is exploited, without changing runtime or discarding the already-computed steps. Thus α is the natural lever to adapt dynamically. In our implementation, α is scheduled online at each step. After Adaptive rule in practice. generating rollouts with the current policy, we compute the policy entropy Hs and maintain rolling buffer of the past ω entropy values (window size ω). Let µs and σs denote the mean and standard deviation within this buffer. We define the one-sided z-score If Zs τ for threshold τ , we mark the current step and set α = 0 for all s. Otherwise we keep α = α0. Intuitively, sharp entropy fluctuations signal that the policy is close to local optimum Zs = Hsµs σs . (11)"
        },
        {
            "title": "SFPO",
            "content": "where noise dominates, so interpolation should be disabled. This entropy-triggered schedule exploits the fast trajectory during the high-signal early stage of training, while reverting to pure on-policy updates near convergence for stability."
        },
        {
            "title": "3.5 UNIFIED SFPO UPDATE",
            "content": "Collecting the three stages, the unified SFPO update for each step is: θs+1 = θs,0 η K1 (cid:88) (cid:104) α θL(θs,k) k=0 (cid:125) (cid:123)(cid:122) (cid:124) fast trajectory & reposition (cid:105) , + θL((cid:101)θs,K) (cid:124) (cid:125) (cid:123)(cid:122) slow correction (12) As shown in Alg. 1, SFPO unifies the three stages into single update rule and serves as plugcompatible drop-in replacement for on-policy policy-gradient methods such as GRPO. Our experiments show that this structural change consistently improves stability and sample efficiency on diverse math reasoning benchmarks, with minimal engineering overhead."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Models. We conduct the reasoning rl training with our proposed SFPO on wide range of models: Qwen2.5-Math-1.5B (Yang et al., 2024), DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI et al., 2025a), Qwen3-4B-Base (Yang et al., 2025), Qwen2.5-Math-7B (Yang et al., 2024), and DeepSeekR1-Distill-Qwen-7B (DeepSeek-AI et al., 2025a). For Qwen2.5-Math-1.5B/7B, we set the context length in the training process as 4096 the same as their maximum context length; while for Qwen34B-Base and DeepSeek-R1-Distill-Qwen-1.5B/7B, we set the context length as 8192 for the better accuracy-efficiency tradeoff. Reasoning RL Training. We conduct the reasoning RL training on two different training datasets to test the effectiveness of SFPO on different scales of dataset. The first one is the combination of DAPO training dataset (Yu et al., 2025a) and Math training dataset (Hendrycks et al., 2021), which is total of approximate 24K data. The second one is the Skywork-OR1 Math RL training dataset (He et al., 2025b) with 105k data. The training batch size is set to 256, and the number of responses for each question is set to 8 by default. The total trianing step is set to 400 by default. All the training experiments are done based on verl (Sheng et al., 2025) with single 8xH100 GPUs node. More hyperparameters setting for reasoning RL training can be found in Appendix C. Baseline and Evaluation. We compare SFPO with vanilla GRPO and the base model without rl training on six commonly used mathematical reasoning benchmarks with variant difficulty: Math500 (Hendrycks et al., 2021), AIME24 (Art of Problem Solving, 2024a), AIME25 (MAA), AMC (Art of Problem Solving, 2024b), MinervaMath (Lewkowycz et al., 2022), Olympiad Bench (He et al., 2024). Each benchmark is evaluated multiple times with rollout temperature being 1, and we report the average Pass@1 accuracy by default. 4.2 MAIN RESULTS 4.2.1 MATH REASONING BENCHMARKS As illustrated in Table 1, our proposed SFPO consistently outperforms vanilla GRPO across all base models and benchmarks. Specifically, for small-scale models such as Qwen2.5-Math-1.5B and DS-distilled-Qwen-1.5B, SFPO demonstrates superior performance enhancements on math reasoning benchmarks, raising the average accuracy from 38.35 to 40.19 with an absolute gain of +1.84, and from 47.73 to 50.53 with gain of +2.80, respectively. The improvements are particularly pronounced on challenging tasks such as AIME24 and AIME25, where DS-distilled-Qwen-1.5B 3Although decaying schedule on α could be formal solution, we find little empirical difference, as shown in Fig. 7 of Sec. 5. Hence, for simplicity and efficiency, we downgrade SFPO to GRPO once the step reaches s."
        },
        {
            "title": "SFPO",
            "content": "Table 1: Performance on math reasoning benchmarks with DAPO and Math training dataset. Model Method Math-500 AIME24 AIME25 AMC Minerva Olympiad Avg Qwen2.5-Math-1.5B Qwen2.5-Math-7B DS-distilled-Qwen-1.5B DS-distilled-Qwen-7B Qwen3-4B-Base Base GRPO SFPO Base GRPO SFPO Base GRPO SFPO Base GRPO SFPO Base GRPO SFPO 55.55 77.15 78. 71.65 82.50 82.30 73.80 84.65 86.10 83.60 91.7 92.60 45.25 83.35 84.30 9.17 16.67 20.00 21.67 34.20 35. 16.67 30.00 32.50 28.33 50.00 54.17 2.50 16.67 21.67 5.83 11.67 15.00 9.17 20.83 20.83 20.00 23.33 30. 25.00 35.83 37.50 0.83 17.50 20.83 37.65 53.31 56.02 53.61 71.08 74.49 47.59 66.86 70.28 61.75 80.42 83. 20.48 59.03 57.23 17.74 31.89 32.07 27.02 36.76 36.94 28.86 31.71 32.81 41.73 43.65 44.49 15.99 38.78 40. 28.45 39.42 39.72 38.54 44.80 45.59 36.94 49.85 50.67 48.89 61.24 65.73 20.66 48.59 48.67 25.73 38.35 40. 36.94 48.36 49.19 37.27 47.73 50.53 48.21 60.47 63.04 17.62 43.99 45.59 achieves an absolute gain of +7.5 on AIME25. The larger models also exhibit similar performance gains. For Qwen2.5-Math-7B, SFPO raises the average accuracy from 48.36 to 49.19 with an absolute gain of +1.80. For DS-distilled-Qwen-7B, SFPO boosts the average accuracy from 60.47 to 63.04, corresponding to an absolute gain of +0.8. For Qwen3-4B-Base model, SFPO improves average accuracy from 43.99 to 45.59, an absolute gain of +1.60, highlighting its robustness across various models. Moreover, Table 2 demonstrates that SFPO can consistently achieve better training performance compared to GRPO for the larger training dataset Skywork-or1, proving the robustness of SFPO across different scales and distributions of training datasets. 4.2.2 TRAINING DYNAMICS. We compare the training dynamics between SFPO and GRPO to better understand the differences in optimization behavior as shown in Fig. 2 and Fig. 3. More comprehensive training dynamics for other models can be found in Appendix D. Table 2: Performance on AIME24/25 with Skywork-or1 training dataset. Model Method AIME24 AIME DS-Qwen-1.5B Base GRPO SFPO 20.40 32.92 34.17 17.90 25.83 27.50 Validation. From Fig. 2, we can clearly spot that SFPO consistently outperforms GRPO across all base models throughout the training process. Not only does SFPO achieve faster convergence in the early stages, but it also sustains higher global accuracy by the end of training. For example, Qwen3-4B-Base model achieves sharper rise and stabilizes at higher accuracy within only 150 training steps, while vanilla GRPO cannot surpass this accuracy even after 400 steps. Base GRPO SFPO 25.42 42.50 43.75 22.92 29.20 30.00 DS-Qwen-7B Response Length. Moreover, distinct training behaviors between SFPO and GRPO for DeepSeekR1-Distill-Qwen-7B are shown in Fig. 3 including the response length, entropy loss, and the reward throughout the training process. Specifically, GRPO gradually collapses to overly short responses while SFPO quickly converges to stable range of around 2700 tokens with better accuracy, highlighting SFPOs ability to regulate response length more effectively and avoid overthinking with verbose responses (Liu et al., 2025). Entropy. From Fig. 3(b), we can observe that SFPO makes models entropy loss lower compared to GRPO. Typically, lower entropy means weak exploration ability for reasoning models; however, the entropy reduction under SFPO mainly reflects the models ability to eliminate unproductive search paths early, rather than suppressing exploration altogether as evidenced by its sustained accuracy gains. In fact, the model still explores sufficiently broad set of reasoning trajectories; therefore, the lower entropy observed under SFPO should be viewed as sign of more efficient exploration rather than sign of limited exploration."
        },
        {
            "title": "SFPO",
            "content": "Figure 2: Average validation accuracy of different base models throughout the learning process. Figure 3: Training dynamics for DeepSeek-R1-Distilled-Qwen-7B, comparing GRPO and SFPO across response length, entropy loss, and reward. Reward Score. SFPO also achieves higher and more stable reward throughout the training process compared to GRPO, indicating stronger alignment with the reward function and more robust convergence. This is further reflected in its superior accuracy and well-controlled response length. 4.2.3 EFFICIENCY ANALYSIS. We evaluate the efficiency gains of SFPO over GRPO by comparing the total number of rollouts and the wall-clock time required to reach the same benchmark accuracy. As shown in Fig. 4, SFPO consistently surpasses GRPO in rollout efficiency and training speed across all model scales. Specifically, SFPO requires 3.21, 3.50, and 4.93 fewer rollouts than GRPO for DS-Qwen-1.5B, Qwen34B-Base, and DS-Qwen-7B, respectively, to reach the same best accuracy. This advantage directly translates into reduced training time, where SFPO achieves 2.62, 2.65, and 4.19 speedups over GRPO for the same models, significantly lowering the training cost. Note that SFPO does not introduce extra GPU memory overhead as it does not need to store the heavy optimizer status. The detailed profiling results for GPU memory usage in the training process can be found in Appendix B. These significant efficiency gains align with our expectations, since the primary bottleneck in the training process lies in rollout generation, which accounts for more than 70% of the overall inference time (He et al., 2025a). By substantially reducing the number of rollouts required and harnessing the reposition mechanism, SFPO alleviates this bottleneck and achieves faster training. Figure 4: Comparison of GRPO and SFPO. (a) Number of rollouts required to achieve the best accuracy of GRPO. (b) Corresponding training time."
        },
        {
            "title": "5 ANALYSIS AND ABLATION STUDY",
            "content": "Impact of α and K. As discussed in Sec. 3, the hyperparameters α and jointly determine the stability of SFPO. To better understand their impacts, we study two settings: small K=3 and large K=7, under varying α, as shown in Fig. 5. When is small, SFPO remains stable across different α"
        },
        {
            "title": "SFPO",
            "content": "Figure 5: Average training accuracy of different base models throughout the learning process. Figure 6: Comparison between SFPO w/ and w/o entropy control (EC). The blue dashed line indicates the stop step identified by z-score. Figure 7: Comparison between SFPO with different α decay strategies. The blue dashed line indicates the stop step identified by z-score. values and consistently outperforms GRPO. This aligns with our theoretical intuition that performing multiple inner updates on the same rollout batch, followed by reposition, effectively reduces gradient noise and produces more reliable update direction. However, when is large, the fast weights drift substantially from the original parameters. large α then amplifies this mismatch by pulling the slow weights too aggressively toward unstable fast trajectories, injecting noise and causing performance drops. As shown in Fig. 5(b), smaller α mitigates this drift and restores stabilityconfirming the interaction between and α. Interpolation Against Off-Policy Overfitting. When α = 1 (no interpolation), performance initially rises but steadily declines as training progresses. Without interpolation, the model rapidly adapts by overfitting to small batches of rolloutsyielding short-term gains but injecting growing noise into gradient updates, which causes instability and long-term degradation. This effect is amplified when is large, as shown in Fig. 5(b), where the fast weights drift substantially and the slow weights fully adopt these noisy trajectories, leading to sharp performance drops. In contrast, when α is small, interpolation effectively anchors the update near the on-policy region, mitigating distributional drift and stabilizing training. However, overly small α may under-utilize the stabilized fast trajectory, slightly slowing early-stage improvement. Overall, moderate α achieves the best balanceleveraging variance reduction from Stage while maintaining stability through controlled repositioning. The Importance of Stage III: Slow Correction. As shown in Fig. 5(d), incorporating slow correction consistently improves stability and accuracy over GRPO. Without this stage, the reposition stage leaves the iterate at an interpolated point that may deviate from the true descent direction. Intuitively, slow correction provides curvature-aware adjustment that realigns updates with the correct optimization trajectory. Beyond its immediate benefit, slow correction also provides natural interface for future extensions: As discussed in Sec. 7, one could, for instance, perform an additional rollout on small held-out meta-test batch after repositioning, enabling curriculum or meta-learning variants of SFPO without altering its overall structure. Necessity of Entropy Control. We evaluate the effect of adaptive entropy control (EC) on SFPO using the DS-Qwen-7B model. As shown in Fig. 6, removing EC causes noticeable accuracy drop after roughly 100 steps. This degradation coincides with rapid divergence of entropy loss, suggesting that the policy becomes unstable and overfits to noisy rollouts. These results highlight entropy control as key factor for maintaining the stability and reliability of SFPO. For simplicity, we implement entropy control through lightweight scheduling of α. As shown in Alg. 1, once predefined entropy-trigger is activated, α is set to 0, reverting SFPO to standard GRPO for subsequent steps. This simple mechanism effectively stabilizes training while retaining the efficiency gains of our fastrepositionslow framework."
        },
        {
            "title": "SFPO",
            "content": "Strategies for Decaying α. We further investigate alternative strategies for reducing the value of α once the stop step is identified by the z-score criterion. Specifically, we compare two representative approaches: (i) our default method, where α is directly set to zero after the trigger step s, and (ii) more gradual linear decay schedule, where α decreases to zero over several subsequent steps. As illustrated in Fig. 7, both strategies yield similar accuracy and stability curves, suggesting that the decay schedule itself has negligible influence on the overall performance of SFPO. This result implies that once the entropy-trigger condition is met, the role of α becomes marginal, and any further adjustment has limited benefit. Consequently, for simplicity and wall-clock efficiency, we adopt the direct reset rule of setting α = 0 after s. The formal procedure is summarized in Alg. 1, and additional details can be found in Sec. 3.4."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "RL for LLM Reasoning. OpenAI O1 (OpenAI) introduced paradigm shift in LLM reasoning by extending the reasoning horizon before final responses. DeepSeek-R1 (Guo et al., 2025) further advanced this line by open-sourcing both its training algorithm, the value-model-free GRPO (Shao et al., 2024), and model weights, achieving performance comparable to O1. Subsequent work has focused on stabilizing and simplifying GRPO: DAPO (Yu et al., 2025b) identifies entropy collapse as key challenge and proposes effective remedies, while Dr. GRPO (Liu et al., 2025) removes normalization terms without sacrificing performance. In contrast, SFPO is orthogonal to both families: it leaves the objective unchanged yet restructures the update itself into fastrepositionslow decomposition, improving variance reduction, stability, and sample efficiency in LLM reasoning. Data Efficiency for LLM Reasoning. Despite focusing on designing novel training pipelines, complementary line of work (Ivison et al., 2025; Xia et al., 2024; Muennighoff et al., 2025; Ye et al., 2025) improves the efficiency of LLM training through data filtering. One direction focuses on pruning data for supervised fine-tuning (Xia et al., 2024; Chen et al., 2023; Ivison et al., 2022). Another direction targets reinforcement learning, where studies (Muldrew et al., 2024; Liu et al., 2024; Das et al., 2024; Li et al., 2025; Fatemi et al., 2025; Wang et al., 2025) show that GRPO requires only small subset of the training data to improve reasoning ability. However, these methods largely optimize which data is used rather than how updates are performed. SFPO is orthogonal: it assumes no change to the data pipeline but restructures the policy update itself. By converting one-shot updates into fastrepositionslow trajectory, SFPO reduces variance and stabilizes learning, thereby yielding higher data efficiency even under random rollout sampling. Iterative GRPO. Iterative GRPO was introduced in (Shao et al., 2024) to reuse rollout data, with primary focus on reward modeling. Mroueh (2025) provided theoretical analysis under verifiable rewards, reformulating GRPO as KL-regularized contrastive loss and proving its iterative dynamics amplify success rates relative to the reference policy. Mroueh et al. (2025) examined both onand off-policy variants, deriving reward improvement conditions, introducing clipped surrogate objectives to stabilize training, and empirically showing off-policy GRPO can match or exceed on-policy performance. Unlike these modifications, SFPO preserves the on-policy GRPO update but restructures one-step optimization into fast-reposition-slow framework to tackle sample inefficiency and stabilize updates."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We proposed Slow-Fast Policy Optimization (SFPO), simple three-stage update framework that stabilizes early training by combining fast trajectory, reposition mechanism, and slow correction. SFPO directly addresses the instability of one-shot GRPO updates, achieving higher reasoning accuracy and fewer generated tokens without added wall-clock cost. Beyond these gains, its modular structure naturally opens avenues for curriculum or meta-learning extensions, and our entropy-triggered α schedule suggests richer adaptive rules. We believe SFPO offers not only an effective plug-in for current policy optimization, but also foundation for scalable and data-aware optimization strategies in the LLM reasoning area."
        },
        {
            "title": "REFERENCES",
            "content": "Art of Problem Solving. Aime problems and solutions. https://artofproblemsolving. com/wiki/index.php/AIME, 2024a. Accessed: 2025-04-20. Art of Problem Solving. Amc problems and solutions. https://artofproblemsolving. com/wiki/index.php?title=AMC, 2024b. Problems and Solutions, 2024b. Accessed: 2025-04-20. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. Muzhi Dai, Shixuan Liu, and Qingyi Si. Stable reinforcement learning for efficient reasoning. arXiv preprint arXiv:2505.18086, 2025. URL https://arxiv.org/abs/2505.18086. Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Active preference optimization for sample efficient rlhf. arXiv preprint arXiv:2402.10500, 2024. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye,"
        },
        {
            "title": "SFPO",
            "content": "Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: case study on ppo and trpo. arXiv preprint arXiv:2005.12729, 2020. Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. Concise reasoning via reinforcement learning. arXiv preprint arXiv:2504.05185, 2025. D. Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms through rl. Nature, 2025. URL https://doi.org/10.1038/s41586-025-09422-z. To appear / in press. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. URL https://arxiv.org/abs/2402.14008. Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, and Haibo Chen. History rhymes: Accelerating llm reinforcement learning with rhymerl, 2025a. URL https: //arxiv.org/abs/2508.18588. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report, 2025b. URL https: //arxiv.org/abs/2505.22312. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Hamish Ivison, Noah Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning using cross-task nearest neighbors. arXiv preprint arXiv:2212.00196, 2022. Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, and Pradeep Dasigi. Large-scale data selection for instruction tuning. arXiv preprint arXiv:2503.01807, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv.org/abs/2309.06180. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and et al. Solving In Advances in Neural Information quantitative reasoning problems with language models. Processing Systems (NeurIPS), volume 35, pp. 38433857, 2022. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025."
        },
        {
            "title": "SFPO",
            "content": "Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Enabling weak llms to judge response reliability via meta ranking. arXiv preprint arXiv:2402.12146, 2024. MAA. American invitational mathematics examination (aime). https://maa.org/ math-competitions/aime. Mathematics Competition Series; n.d.a. Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpos effective loss, dynamics, and success amplification. arXiv preprint arXiv:2503.06639, 2025. Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, and Jesus Rios. Revisiting group relative policy optimization: Insights into on-policy and off-policy training. arXiv preprint arXiv:2505.22257, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. William Muldrew, Peter Hayes, Mingtian Zhang, and David Barber. Active preference learning for large language models. arXiv preprint arXiv:2402.08114, 2024. OpenAI. Learning to reason with llms: Openai o1 preview. URL https://openai.com/ index/introducing-openai-o1-preview. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted"
        },
        {
            "title": "SFPO",
            "content": "Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Si Shen, Peijun Shen, Wenhua Zhao, and Danhao Zhu. Mitigating think-answer mismatch in llm reasoning through noise-aware advantage reweighting. arXiv preprint arXiv:2508.05928, 2025. URL https://arxiv.org/abs/2508.05928. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/ 3689031.3696075. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025a. URL https://arxiv.org/ abs/2503.14476."
        },
        {
            "title": "SFPO",
            "content": "Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey Hinton. Lookahead optimizer: steps forward, 1 step back. Advances in neural information processing systems, 32, 2019. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/2304.11277. Haizhong Zheng, Yang Zhou, Brian Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025."
        },
        {
            "title": "A LLM USAGE",
            "content": "Large language models (LLMs) were used solely to improve the writing of this paper, including grammar, clarity, and readability. They were not used for generating ideas, designing experiments, conducting analyses, or producing scientific content. All research contributions, technical claims, and conclusions are entirely the work of the authors."
        },
        {
            "title": "B GPU MEMORY PROFILING RESULTS",
            "content": "Figure 8: Comparison of GPU memory consumption during one RL training step between SFPO and GRPO for Deepseek-R1-Distill-Qwen-7B model. From Fig. 8, we can clearly observe that SFPO and GRPO demonstrate similar GPU memory consumption during one training step. This aligns with our expectation: SFPO does not need to store the heavy optimizer states and parameters but only need to store one copy of the model weight, which does not introduce significant overhead, especially when the model is sharded across GPUs."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "We conduct all experiments based on verl (Sheng et al., 2025) Rl training framework. For training the policy model, we use Pytorch Fully Sharded Data Parallel (FSDP) distributed training backend (Kwon et al., 2023). For inference, we use vLLM framework (Zhao et al., 2023). The major hyperparameters used in reasoning RL training are listed in Table 3. Note that we do not use KL-divergence term in the objective loss function as suggested by previous works (Yu et al., 2025a). Name Value Description Table 3: Hyperparameters for RL training. Training batch size Mini batch size Rollout temperature top_p Optimizer lr grad_clip 256 64 8 1.0 0.7 AdamW 1e-6 (Constant) Learning rate. 1.0 Global training batch size for RL training. Sub-batches used to split the global training batch. Number of responses to generate per sample. Sampling temperature for rollout generation. Top-p sampling parameter for rollout generation. The optimizer used. Gradient clipping threshold."
        },
        {
            "title": "D TRAINING DYNAMICS",
            "content": "We illustrate the training dynamics of reasoning RL on the DAPO and math datasets covering entropy loss, mean response length, and reward for Qwen2.5-Math-1.5B, DeepSeek-r1-Distilled-Qwen-1.5B, Qwen2.5-Math-7B, and Qwen3-4B-Base, as shown in Fig. 9, Fig. 10, Fig. 11, and Fig. 12, respectively. Overall, SFPO achieves higher rewards with smoother, more stable training with reducing entropy fluctuations and controlling response length, making it faster, more sample-efficient, and easier to tune than GRPO."
        },
        {
            "title": "SFPO",
            "content": "Figure 9: Training dynamics for Qwen2.5-Math-1.5B, comparing GRPO and SFPO across response length, entropy loss, and reward. Figure 10: Training dynamics for DeepSeek-R1-Distilled-Qwen-1.5B, comparing GRPO and SFPO across response length, entropy loss, and reward. Figure 11: Training dynamics for Qwen2.5-Math-7B, comparing GRPO and SFPO across response length, entropy loss, and reward. Figure 12: Training dynamics for Qwen3-4B-Base, comparing GRPO and SFPO across response length, entropy loss, and reward."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "University of Illinois Urbana-Champaign"
    ]
}