{
    "paper_title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
    "authors": [
        "Yuyang Hu",
        "Jiongnan Liu",
        "Jiejun Tan",
        "Yutao Zhu",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 6 2 7 4 0 . 1 0 6 2 : r Memory Matters More: Event-Centric Memory as Logic Map for Agent Searching and Reasoning Yuyang Hu, Jiongnan Liu, Jiejun Tan, Yutao Zhu, Zhicheng Dou Gaoling School of Artificial Intelligence, Renmin University of China yuyang.hu@ruc.edu.cn, dou@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, key capability for such agents is memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models."
        },
        {
            "title": "Introduction",
            "content": "With the rapid development of large language models (LLMs), agents have evolved from simple interfaces into systems capable of complex reasoning and long-term interaction with environments (Zhang et al., 2025d; Wang et al., 2024). To support such behaviors, agents require memory mechanisms that go beyond simple text generation capabilities (Ouyang et al., 2025; Zhang et al., 2025d). Ideally, similar to human memory, agent memory should serve not only as repository of 1 knowledge, but also as fundamental infrastructure that supports reasoning, planning, and decisionmaking (Wu et al., 2025a; Zhang et al., 2025c). Within the broader field of agent memory research, significant amount of attention has been directed toward factual memory (Zhang et al., 2025b; Hu et al., 2025). Factual memory refers to an agents capacity to manage explicit information about past events, users, and the external environment. Such memory supports context awareness, personalization, and long-horizon tasks. Despite significant progress in this area, current approaches face two primary limitations. First, regarding memory structure, most methods rely on flat representations where information is stored as independent text segments, as shown in Figure 1 (a) (Hu et al., 2025). While some recent studies have explored structured organizations (Xu et al., 2025; Rasmussen et al., 2025; Rezazadeh et al., 2025; Sun and Zeng, 2025; Li et al., 2025a), they often fail to capture essential logical relations, such as causality and temporal sequences (Figure 1 (b)) (Yang et al., 2025). Second, regarding memory utilization, prior work primarily depends on simple semantic matching (Rasmussen et al., 2025). This reliance limits memory to functioning as static storage system rather than an active component that guides the reasoning process. In contrast, human memory is organized hierarchically and connected through rich logical associations rather than as collection of isolated facts. Cognitive science offers theoretical support for this organization, particularly through Event Segmentation Theory (Baldassano et al., 2017; Zacks et al., 2007). According to this theory, humans naturally perceive continuous experience as series of discrete and meaningful events. These events form the backbone of long-term memory and are encoded with rich temporal and semantic information (Baldassano et al., 2017; Ezzyat and Davachi, 2011). This structured organization facilitates efficient retrieval. It enables the brain to selectively access relevant events by navigating Figure 1: Comparison among CompassMem and the traditional agent memory framework. structured network, which helps guide reasoning and planning in new situations (Anderson, 1983). Unfortunately, these capabilities are largely absent in existing agent memory systems. This disparity leads to critical research question: Can we structure agent memory in way that mimics human cognitive organization to support search and reasoning beyond isolated facts? Inspired by these cognitive principles, we propose CompassMem, an event-centric memory framework that explicitly models logical relations among memory units and leverages this structure to guide agent searching and reasoning. Unlike traditional approaches that store isolated text snippets, CompassMem incrementally constructs an Event Graph from experiences through event segmentation, relation extraction, and topic evolution. In this graph, nodes correspond to coherent event units, while edges encode logical dependencies such as causality and temporal order. During the inference phase, agents utilize the Event Graph as structured logic map rather than flat list. This structure provides directional cues that guide agent searching and reasoning. It allows agents to prioritize relevant information, follow meaningful logical connections, and avoid redundant retrieval. In this manner, memory goes beyond merely supplying content and actively guides the reasoning process to handle complex queries effectively. CompassMem achieves consistent and substantial improvements over strong baselines on LoCoMo and NarrativeQA, particularly on tasks requiring multi-hop and temporal reasoning. These results demonstrate that explicitly encoding logical structure into memory not only improves retrieval quality, but also enables memory to actively support reasoning, rather than serving as passive knowledge store. Our contributions are as follows: (1) We propose CompassMem, an event-centric memory framework that organizes experiences into event units connected by explicit logical relations. (2) In CompassMem, we design graph-based memory retrieval mechanism, enabling agents to actively navigate the Event Graph for logic-aware evidence collection, rather than just relying on flat, similarity-based memory access. (3) We evaluate CompassMem on dialogue and long-document benchmarks, observing consistent improvements and validating its effectiveness and generality."
        },
        {
            "title": "2 Related Work",
            "content": "Memory has been widely regarded as core capability of intelligent agents (Zhang et al., 2025d). Early systems such as MemGPT (Packer et al., 2023) manage long-term memory through paging and segmentation mechanisms, which inspired subsequent frameworks including MemOS (Li et al., 2025b) and MemoryOS (Kang et al., 2025). Methods such as Mem0 (Chhikara et al., 2025) and MemoryBank (Zhong et al., 2024) follow RAG-style paradigm, placing greater emphasis on memory organization and lifecycle management. Also, growing line of work explores structured memory representations. Representative examples include tree-based designs such as MemTree (Rezazadeh et al., 2025), graph-based memories like AMem (Xu et al., 2025), and more general hierarchical or compositional memory systems (Rasmussen et al., 2025; Zhang et al., 2025a; Wu et al., 2025b; Li et al., 2025a). These approaches demonstrate the benefit of introducing structure into memory, particularly for improving organization. In parallel, other studies investigate automatic memory management and adaptation from different perspectives (Yan et al., 2025b; Wang et al., 2025). While these methods enrich memory management, memory is still largely treated as passive storage. Our work designs an event-centric memory that explicitly encodes logical structure and actively guides searching and reasoning."
        },
        {
            "title": "3 Preliminary",
            "content": "In this section, we formalize the task setting and introduce the core concepts used in our approach. We consider an agent operating over stream of textual observations at time step t, denoted as Xt = 2 (xt,1, . . . , xt,n), where each xt,i is text unit such as dialogue turn or narrative sentence. Given the incoming observations and the previously stored memory M(t1), the agent updates its memory through construction process Φ: M(t) = Φ(Xt, M(t1)). Here, Φ first extracts sub-memory Mt from the current input stream, and then integrates it with the existing memory M(t1), yielding the updated memory M(t). (1) At inference time, given query Q, the agent performs query-dependent memory search through retrieval process Ψ: M(t) q= Ψ(q, M(t)), (2) where M(t) M(t) denotes the subset of memory selected for answering the query. The final response is generated by conditional generation function: (3) = F(q, M(t) q), which produces an output conditioned on the query and the retrieved memory. Our goal is to design more effective memory construction processes Φ and retrieval strategies Ψ to support higher-quality generation."
        },
        {
            "title": "4 Method",
            "content": "4.1 Overview As shown in Figure 2, CompassMem is an eventcentric memory framework designed to make memory an active guide for agent searching and reasoning. The core idea is to organize memory as structured hierarchical Event Graph, where experiences are stored as coherent event units connected by explicit logical relations. Memory is constructed incrementally from input streams by segmenting continuous observations into events, extracting relations among them, and integrating the resulting structures into the existing memory over time. During inference, the agent performs logic-aware memory search by actively navigating the Event Graph. Rather than retrieving isolated memories by similarity, the agent follows meaningful logical paths between related events and progressively collects relevant evidence, with the memory structure guiding both where to search and how to reason for complex, longhorizon queries. 4.2 Incremental Hierarchical Memory Construction We construct memory in an incremental manner. The system first segments the input into coherent events, then extracts explicit relations among these events, and finally integrates them into the memory through incremental graph updates. Event Segmentation Event Segmentation Theory (EST) (Baldassano et al., 2017) suggests that humans organize continuous experience into discrete and coherent events, which serve as fundamental units of long-term memory. An event is not an arbitrary text span, but meaningful unit obtained by segmenting continuous experience stream. Following this perspective, we prompt an LLM to identify events from the input stream and extract their attributes: Et = {eti}m (4) where each event eti Et is represented as eti = oti, τti, sti, πti. Here, oti denotes the span of observations belonging to the event, τti captures temporal information, sti is semantic summary, and πti denotes the set of involved participants. Relation Extraction memory composed of isolated events provides limited support for reasoning (Hu et al., 2025). In contrast, humans reason and form associations by following logical connections (Anderson, 1983). To enable structured retrieval and multi-step reasoning, we explicitly extract logical relations among event nodes using an LLM-based process: i=1 = Φseg(Xt), (5) Rt = {(eti, etj , ρtij )} = Φrel(Xt, Et), where each relation rij = (ei, ej, ρij) represents logical dependency between two events. The relation label ρij is drawn from an open-ended predicate set P, covering relations such as causal, temporal, motivation, and part-of, and allowing new relation types to be introduced as needed. Together, the extracted events Et and relations Rt form the current sub-memory Mt = (Et, Rt). Incremental Graph Update As memory grows over time, new events must be integrated while preserving coherence, so that newly acquired information can be connected to existing knowledge without introducing redundancy or semantic drift. We incrementally update the memory M(t1) by incorporating the new sub-memory Mt through three operations. Node Fusion & Expansion Each new event enew Et+1 is compared against existing events, 3 Figure 2: Overview of the proposed CompassMem framework, which contains mainly two part: Incremental Hierarchical Memory Construction and Active Multi-Path Memory Search where denotes the most similar existing event. The integration follows three cases. If enew is equivalent to e, the two events are merged. If logical relation between enew and is identified, an edge is added to link them. Otherwise, enew is inserted as new node. This process integrates new information while avoiding redundancy. Topic Evolution During memory search, exploration driven purely by local similarity may focus on single semantic aspect of query, which can be insufficient for complex questions involving multiple facets. To address this issue, we introduce topic layer over the accumulated event set (t) = (cid:83)t i=1 Ei. Each topic zk (t) represents semantic cluster of related events, and the topicevent associations are maintained in A(t), indicating which events belong to each topic. This topic layer provides coarse-grained semantic organization of events, which complements the finegrained logical structure defined by event relations and facilitates efficient multi-path exploration during memory search. At the initial stage (e.g., = 1), when no topic structure exists, we use K-means to perform topic clustering over the extracted events to initialize the topic set: (1) = {z1, z2, . . . , zk} = Φclu(E (1); k). (6) As memory grows over time, we update topic event associations in an online manner. For each newly integrated event enew Et+1, we identify its most similar topic from the existing topic set (t) based on semantic similarity. If the similarity exceeds threshold δ, the event is assigned to that topic; otherwise, new topic node is created to capture previously unseen semantic direction. This process incrementally updates both the topic set (t) and the topicevent associations A(t). To prevent semantic drift introduced by incremental updates, we periodically re-cluster all accumulated events: (t+1) Φclu(E (t+1); k) when mod = 0. This strategy balances stability during online updates with global coherence over memory growth. By treating temporally situated events as primary memory units, the resulting event graph preserves narrative structure and event-level semantics, which are often lost in triple-based representations (Yang et al., 2025). From this perspective, memory itself serves as an explicit logic map that 4 guides subsequent search and reasoning. Prompts for all memory construction processes are provided in the Appendix F.1. 4.3 Active Multi-Path Memory Search With the event graph constructed as structured logic map, memory search proceeds through active navigation and reasoning. Given query q, the goal is to retrieve small set of event nodes that provide sufficient evidence. CompassMem adopts principle of guided active evidence construction. Reasoning is performed through traversal of the event graph, while only distilled evidence is passed to the final answer model. To support this process, we implement memory search Ψ using three LLM-based agents: Planner, multiple Explorers, and an Responder. Prompts for all agents are provided in the Appendix F.2. 4.3.1 Planner Given query q, the Planner decomposes it into small set of subgoals, Hq = Ψplan(q), Hq [2, 5], (7) where each subgoal captures distinct aspect that the search should cover. The Planner maintains binary satisfaction vector {0, 1}K to indicate which subgoals have been supported by the currently collected evidence. This explicit progress signal provides notion of the search stage and guides exploration toward unsatisfied subgoals. If the search fails to terminate with sufficient evidence in the current round, the Planner performs gap-aware refinement. It generates refined query by conditioning on the current query, the collected evidence, and the remaining unsatisfied subgoals, q(r+1) = Ψref (q(r), Hq, s), where refinement focuses on unfinished subgoals. This design yields closed-loop process that alternates between exploration and query refinement. (8) 4.3.2 Explorer Active searching and reasoning over the memory is carried out by set of Explorer agents. Guided by the memory topology structure, each Explorer operates directly on the event graph and decides which nodes to retain as evidence and how exploration should proceed. Localization Before graph traversal, exploration is first localized to determine where to begin. Candidate event nodes are retrieved by ranking their embedding similarity to the query, and the top-k results are selected. Since these events are often highly similar and may focus on single aspect, the Explorer further selects candidates from the first distinct topic clusters appearing in the ranked list. From the resulting candidate set Cq, the starting nodes are selected as: Sq = Ψstart(q, Cq), where Ψstart denotes an LLM-based selection operator. The selected starting nodes are then inserted into globally maintained queue to initialize subsequent exploration. Navigation Guided by the event-graph topology, exploration proceeds step by step. At each visited event node e, an Explorer conditions on the query, the current subgoal status, the retained evidence, and the local graph context, including neighboring nodes. Based on this information, the Explorer chooses an action from the action space {SKIP, EXPAND, ANSWER}: = Ψcho(q, e, ˆE, (e), s), (9) where ˆE denotes the current evidence set and (e) denotes neighboring events with typed relations. SKIP discards the current node, EXPAND retains it as evidence and continues exploration, and ANSWER terminates the current path when sufficient evidence has been collected. When EXPAND is selected, the evidence set is updated as: ˆE (t+1) = (cid:40) ˆE (t), ˆE (t) {e}, if = SKIP, otherwise. (10) Each retained node is annotated with the subgoals it supports, enabling explicit progress tracking. This decision process operationalizes our key insight that topology carries logic: relations constrain exploration paths and guide reasoning over structured dependencies, rather than flat and isolated text. Coordination Multiple Explorers run in parallel, each initialized from different starting node. They share global state that records visited nodes, retained evidence, and subgoal progress. All candidate nodes encountered during traversal are scheduled through single global priority queue. The priority of candidate node is defined by its embedding similarity to unsatisfied subgoals: sim(cid:0)v(su), v(hj)(cid:1), (11) p(u) = max j:sj =0 where su denotes the summary of and hj denotes subgoal. This subgoal-driven scheduling reduces 5 Model Method Single-hop Multi-hop Open-domain Temporal Average F1 BLEU BLEU F1 BLEU F1 BLEU BLEU m - 4 - B 4 1 - 5 . 2 Q Non-Graph-based RAG Mem0 MemoryOS Graph-based HippoRAG A-Mem CAM CompassMem Non-Graph-based RAG Mem0 MemoryOS Graph-based HippoRAG A-Mem CAM CompassMem 52.19 47.65 48.62 54.84 44.65 50.58 57.36 49.79 42.58 46.33 42.45 33.75 50.39 61.02 46.80 38.72 42. 48.84 37.06 44.36 49.79 43.95 35.15 41.62 37.14 30.04 45.59 55.93 32.17 38.72 35.27 33.59 27.02 33.55 38.84 28.11 31.73 38. 27.57 22.09 34.50 42.32 23.59 27.13 25.22 25.46 20.09 24.18 27.98 21.43 24.82 29.26 20.62 15.28 24.62 32.66 23.21 28.64 20. 28.59 12.14 18.23 26.61 20.42 15.03 20.27 19.74 13.49 23.86 25.88 18.88 21.58 15.52 23.89 12.00 12.77 20.01 17.40 11.28 15. 15.81 10.74 20.84 22.01 30.77 48.93 41.15 48.17 45.85 44.14 57.96 24.73 28.96 32.24 30.66 27.19 44.70 47.18 25.99 40.51 30. 39.32 36.67 38.28 50.51 20.02 26.24 27.86 26.33 22.05 36.30 39.69 42.25 45.10 42.84 47.92 39.65 44.10 52.18 38.77 36.04 40. 35.85 28.98 44.64 52.52 36.47 35.90 35.47 41.02 32.31 37.43 44.09 33.18 29.91 34.89 30.53 24.47 38.27 46.17 Table 1: Performance comparison on the LoCoMo benchmark, covering single-hop, multi-hop, open-domain, and temporal settings. We report F1 and BLEU-1 scores (%). Best results are highlighted in bold, and second-best results are underlined. Model Method F1 BLEU GPT-4o-mini Qwen2.5-14B RAG Mem0 MemoryOS HippoRAG A-Mem CAM CompassMem RAG Mem0 MemoryOS HippoRAG A-Mem CAM CompassMem 28.99 29.98 25.58 28.77 27.01 33.55 39.04 25.82 26.94 22.17 22.10 25.37 27.87 35.90 25.68 23.34 21.74 23.04 23.17 29.74 35.23 20.65 22.01 19.32 17.77 20.94 23.47 28. Table 2: Results on 298 questions belonging to 10 documents randomly sampled from the NarrativeQA. We do the sample since the full test set contains over 10,000 questions and is prohibitively large for long-context evaluation. redundant exploration and promotes complementary coverage across paths, enabling efficient multipath reasoning over the event graph. 4.3.3 Responder The Responder is invoked when the global candidate queue becomes empty, and all subgoals are satisfied. If the queue becomes empty while some subgoals remain unsatisfied, the system returns to the Planner to start the second round search. Upon termination, the search returns concise 6 evidence set M(t) q= ˆE. If no evidence is retained, we fall back to the initial top-k retrieved candidates. The Responder then generates the final output, ensuring that generation conditions only on distilled evidence while reasoning is carried out through structured navigation on the logic map."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experimental Settings Benchmarks We evaluate CompassMem on two long-context reasoning benchmarks, LoCoMo and NarrativeQA. LoCoMo focuses on conversational question answering, while NarrativeQA targets narrative understanding. Detailed dataset descriptions are provided in the Appendix B.1. Backbone Models We use GPT-4o-mini as closed-source model, and Qwen2.5-14B-Instruct as an open-source model. Qwen is deployed with vLLM, while GPT is accessed via API. All methods use BGE-M3 for all mentioned embeddings. Baselines We compare CompassMem with nongraph baselines, including RAG, Mem0 (Chhikara et al., 2025), and MemoryOS (Kang et al., 2025), as well as graph-based baselines such as HippoRAG (Gutiérrez et al., 2025), A-Mem (Xu et al., 2025), and CAM (Li et al., 2025a). Official implementations or reported settings are used when Figure 3: Efficiencyperformance trade-off across memory frameworks. Scatter plots compare F1 with construction time, total processing time, per-question latency, and token consumption. available, with full implementation details provided in the Appendix B.3. 5.2 Main Results We now present the main experimental results and several key observations. Addational results are provided in the Appendix C. (1) Table 1 reports results on LoCoMo across question types. While most methods handle singlehop questions reasonably well, performance drops sharply on multi-hop and temporal QA. In contrast, CompassMem consistently achieves the strongest results. On GPT-4o-mini, it improves average F1 from 47.92% (HippoRAG) to 52.18%, with large gain on temporal questions (57.96% vs. 48.93%). On Qwen2.5-14B, CompassMem further reaches 52.52% F1 and achieves the best performance on all subsets. These results demonstrate the benefit of event-graph memory with logic-aware retrieval for reasoning-intensive QA. (2) Table 2 presents results on NarrativeQA, which requires long-range narrative understanding and evidence aggregation. CompassMem consistently outperforms all baselines, surpassing the strongest competitor CAM by over 5% F1 on GPT4o-mini and more than 8% F1 on Qwen2.5-14B. This demonstrates the effectiveness of event-centric memory with explicit relations for retrieving globally relevant evidence in long narratives. (3) Across both benchmarks, CompassMem shows consistent and robust improvements. Notably, the strongest baselines are generally graph-based, supporting the importance of structured memory. CompassMem further advances these methods by modeling memory at the event level with logicaware relations, yielding the largest gains on tasks that require complex retrieval and reasoning. (4) We further analyze efficiency on representative LoCoMo conversation set, ramdomly selected from ten sessions with identical settings, as shown Figure 4: Ablation results on LoCoMo. in Figure 3. CompassMem achieves low memory construction time cost, substantially lower than Mem0, A-Mem, and MemoryOS. Our total processing time and per-question latency are comparable to Mem0 and A-Mem, and markedly lower than MemoryOS. Although CompassMem uses more tokens, this cost is accompanied by substantial performance gains. Overall, CompassMem delivers strong reasoning improvements while maintaining practical computational efficiency. 5.3 Further Analysis We further conduct in-depth analyses to better understand the behavior of CompassMem. Ablation Study To examine the effectiveness of individual components in CompassMem, we conduct an ablation study by systematically removing key modules. Specifically, we evaluate variants that (i) remove topic clustering, (ii) replace event units with fixed-length chunks to eliminate event modeling, where the chunk length is set to the average size of extracted events (approximately 100 tokens), (iii) remove edges to discard explicit relations, (iv) disable query refinement to prevent second-round exploration, and (v) remove subgoal generation. Figure 4 reports the ablation results 7 Method Single-hop Multi-hop Open-domain Temporal F1 BLEU F1 BLEU F1 BLEU F1 BLEU 44.26 RAG MemO 37.50 MemoryOS 40.87 HippoRAG 46.12 44.57 A-Mem 45.79 CAM 37.98 31.76 35.84 40.58 39.37 38. 23.84 23.58 24.71 31.62 28.53 34.07 15.46 15.17 19.28 24.52 20.16 26.01 11.39 14.37 16.09 22.04 18.35 19.96 8.33 11.48 14.50 17.47 15.23 16.36 17.88 41.29 39.41 40.39 31.60 43.82 13.09 30.37 28.71 32.94 23.49 36. Ours 50.04 43.40 35.33 27.86 28. 23.25 49.35 38.91 Table 3: Results of LoCoMo on Qwen3-8B. analyzes the sensitivity of CompassMem to two localization hyperparameters: the direct retrieval size k, and the topic selection size p. Overall, introducing topic-based selection (p > 0) consistently improves performance compared to the no-clustering setting, and larger values of lead to steadily better results. This suggests that selecting starting nodes from multiple semantic topics helps diversify exploration and reduces bias toward single semantic view. Similarly, increasing the retrieval size provides broader pool of candidate events and yields monotonic performance gains, indicating that richer initial retrieval better supports downstream search. Impact of Model Thinking Ability Table 3 reports LoCoMo results on Qwen3-8B, backbone equipped with explicit thinking capability. All methods benefit from the stronger reasoning capacity, with noticeable improvements on multi-hop and temporal questions compared to non-thinking models. Nevertheless, CompassMem consistently achieves the best performance across all task categories. The gains indicate that explicit reasoning alone is insufficient. Effective memory organization and logic-aware retrieval remain critical for fully exploiting the backbones thinking ability."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented CompassMem, an event-centric memory framework that rethinks agent memory as structured logic map rather than flat storage. By organizing experiences into coherent events and explicitly modeling their logical relations, CompassMem enables memory to actively guide searching and reasoning. Experiments on dialogue and long-document demonstrate that this design provides strong and consistent benefits, particularly for reasoning-intensive tasks. We hope this work encourages future research on memory structures that more directly support long-horizon reasoning and decision-making in intelligent agents. Figure 5: Scaling results comparing fixed high-capacity and scale-consistent memory construction. Shaded areas show gains from stronger construction models. Figure 6: Sensitivity analysis of CompassMem with respect to localization hyperparameters across question categories. Removing any component leads to consistent performance drops, confirming the contribution of each module. In particular, multi-hop and temporal questions are most affected, while single-hop and open-domain questions show smaller degradation due to lower reasoning complexity. Impact of Model Size We examine the scalability of CompassMem. Figure 5 shows that CompassMem continues to improve as model scale increases when the same backbone is used for both memory construction and search. We further evaluate decoupled setting where memory is constructed with high-capacity model (Qwen2.532B) while search and response generation use smaller models. This configuration yields clear improvements over scale-matched baselines. These results suggest that high-quality memory structures built offline can effectively support downstream reasoning, even when paired with lightweight search models. Impact of Location Hyperparameters Figure"
        },
        {
            "title": "Limitations",
            "content": "While CompassMem shows consistent gains, it has several limitations. First, the quality of the Event Graph depends on event segmentation and relation extraction. In this work, we adopt naive LLM-based pipeline; more fine-grained and robust segmentation may further improve memory quality, and we leave this direction for future work. Second, our evaluation focuses on set of representative benchmarks. Demonstrating the effectiveness of CompassMem across broader range of tasks and agent settings would further strengthen its applicability."
        },
        {
            "title": "Ethical considerations",
            "content": "This work studies agent memory architectures for long-context reasoning and does not introduce new datasets. All experiments are conducted on publicly available benchmarks, LoCoMo and NarrativeQA, which do not contain sensitive personal information. We do not intentionally collect, infer, or generate content that identifies specific individuals."
        },
        {
            "title": "References",
            "content": "John Anderson. 1983. spreading activation theory of memory. Journal of verbal learning and verbal behavior, 22(3):261295. Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan Pillow, Uri Hasson, and Kenneth Norman. 2017. Discovering event structure in continuous narrative perception and memory. Neuron, 95(3):709 721. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building production-ready AI agents with scalable long-term memory. CoRR, abs/2504.19413. Sarah DuBrow, MJ Kahana, and AD Wagner. 2024. Event and boundaries. Oxford handbook of human memory, 1. Youssef Ezzyat and Lila Davachi. 2011. What constitutes an episode in episodic memory? Psychological science, 22(2):243252. Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From RAG to memory: Non-parametric continual learning for large language models. In ICML. OpenReview.net. Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, Senjie Jin, Jiejun Tan, Yanbin Yin, Jiongnan Liu, Zeyu Zhang, Zhongxiang Sun, Yutao Zhu, Hao Sun, Boci Peng, and 28 others. 2025. Memory in the age of ai agents. Preprint, arXiv:2512.13564. Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. 2025. Memory OS of AI agent. CoRR, abs/2506.06326. Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2017. The narrativeqa reading comprehension challenge. Preprint, arXiv:1712.07040. Rui Li, Zeyu Zhang, Xiaohe Bo, Zihang Tian, Xu Chen, Quanyu Dai, Zhenhua Dong, and Ruiming Tang. 2025a. CAM: constructivist view of agentic memory for llm-based reading comprehension. CoRR, abs/2510.05520. Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, Chen Tang, Simin Niu, Ding Chen, Jiawei Yang, Chunyu Li, Qingchen Yu, Jihao Zhao, Yezhaohui Wang, Peng Liu, Zehao Lin, Pengyuan Wang, Jiahao Huo, Tianyi Chen, Kai Chen, Kehang Li, and 20 others. 2025b. Memos: memory OS for AI system. CoRR, abs/2507.03724. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851 13870, Bangkok, Thailand. Association for Computational Linguistics. Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, and Tomas Pfister. 2025. Reasoningbank: Scaling agent self-evolving with reasoning memory. CoRR, abs/2509.25140. Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023. Memgpt: Towards llms as operating systems. CoRR, abs/2310.08560. Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: temporal knowledge graph architecture for agent memory. CoRR, abs/2501.13956. Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. 2025. From isolated conversations to hierarchical schemas: Dynamic tree memory representation for llms. In ICLR. OpenReview.net. Haoran Sun and Shaoning Zeng. 2025. Hierarchical memory for high-efficiency long-term reasoning in llm agents. Preprint, arXiv:2507.22925. 9 Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2025d. survey on the memory mechanism of large language model-based agents. ACM Trans. Inf. Syst., 43(6):155:1155:47. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In AAAI, pages 1972419731. AAAI Press. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024. survey on large language model based autonomous agents. Frontiers Comput. Sci., 18(6):186345. Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian J. McAuley, and Xiaojian Wu. 2025. Mem-α: Learning memory construction via reinforcement learning. CoRR, abs/2509.25911. Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu. 2025a. From human memory to AI memory: survey on memory mechanisms in the era of llms. CoRR, abs/2504.15965. Yaxiong Wu, Yongyue Zhang, Sheng Liang, and Yong Liu. 2025b. Sgmem: Sentence graph memory for long-term conversational agents. CoRR, abs/2509.21212. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-MEM: agentic memory for LLM agents. CoRR, abs/2502.12110. B. Y. Yan, Chaofan Li, Hongjin Qian, Shuqi Lu, and Zheng Liu. 2025a. General agentic memory via deep research. Preprint, arXiv:2511.18423. Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, and Yunpu Ma. 2025b. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. CoRR, abs/2508.19828. Zairun Yang, Yilin Wang, Zhengyan Shi, Yuan Yao, Lei Liang, Keyan Ding, Emine Yilmaz, Huajun Chen, and Qiang Zhang. 2025. EventRAG: Enhancing LLM generation with event knowledge graphs. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1696716979, Vienna, Austria. Association for Computational Linguistics. Jeffrey Zacks, Nicole Speer, Khena Swallow, Todd Braver, and Jeremy Reynolds. 2007. Event perception: mind-brain perspective. Psychological bulletin, 133(2):273. Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. 2025a. G-memory: Tracing hierarchical memory for multi-agent systems. CoRR, abs/2506.07398. Guibin Zhang, Muxin Fu, and Shuicheng Yan. 2025b. Memgen: Weaving generative latent memory for selfevolving agents. CoRR, abs/2509.24704. Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, and Shuicheng Yan. 2025c. Memevolve: MetaPreprint, evolution of agent memory systems. arXiv:2512.18746."
        },
        {
            "title": "A Event Segmentation Theory",
            "content": "Event Segmentation Theory (EST) (Baldassano et al., 2017; Zacks et al., 2007; Ezzyat and Davachi, 2011) is framework in cognitive science and neuroscience that explains how humans parse continuous streams of perceptual experience into meaningful units, or events. According to this theory, when perceiving dynamic environment, humans do not process information as an undifferentiated continuous flow. Instead, experience is automatically segmented into sequence of relatively stable event episodes. Within each event, representations remain coherent and stable; when salient change occurs, such as shift in scene, action goals, or environmental state, an event boundary is triggered, prompting the construction of new event representation model. This segmentation process operates not only at the perceptual level but also plays critical role in the encoding of event memories and their subsequent retrieval. Event Segmentation Theory emphasizes that human experience is not continuous whole, but rather is composed of series of identifiable event units. Such segmentation enhances perceptual efficiency and provides fundamental basis for memory structuring and information retrieval (DuBrow et al., 2024)."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Dataset Descriptions LoCoMo LoCoMo (Maharana et al., 2024) is benchmark of very long-term conversational dialogues designed to evaluate long-range memory and reasoning capabilities in agent systems. The dataset consists of 10 extended conversations, each spanning dozens of sessions and hundreds of dialogue turns, with an average of around 600 turns and roughly 16K tokens per conversation. Questions in the LoCoMo QA evaluation are annotated with answer locations and categorized into types such as single-hop, multi-hop, open-domain, temporal reasoning, and adversarial, targeting different memory and inference challenges. In our experiments on LoCoMo QA, we follow standard practice in related work and do not use adversarial question data, which aligns with previous evaluations (Chhikara et al., 2025; Yan et al., 2025a; Kang et al., 2025). NarrativeQA NarrativeQA (Koˇciský et al., 2017) is large-scale reading comprehension benchmark that assesses models ability to understand and reason over long narrative text such as books and movie scripts. The full NarrativeQA dataset contains on the order of tens of thousands of human-written questionanswer pairs associated with over thousand story documents, where questions require synthesis across global document structure rather than shallow pattern matching. Questions are constructed based on humangenerated abstractive summaries, encouraging deep narrative understanding and integrative reasoning beyond local context overlaps. In our evaluation, we randomly sampled 10 long documents from the NarrativeQA corpus and used their associated 298 QA pairs to measure performance on long-range narrative question answering. This sampling strategy is adopted because the full NarrativeQA test set contains 10,557 questions, making exhaustive evaluation computationally prohibitive. The selected documents have an average length of around 60,000 tokens, which still poses substantial challenge for long-context understanding and coherent evidence aggregation. B.2 CompassMem In CompassMem, we adopt fixed set of hyperparameters across all main experiments. During memory construction, newly extracted events are merged with existing ones when their semantic similarity exceeds threshold of 0.9, which helps reduce redundancy while preserving coherent event structure. In the topic evolution stage, we apply the same similarity threshold (0.9) when merging events into existing topics, and perform periodic re-clustering every 4 construction steps to maintain semantic coherence over time. For the LoCoMo benchmark, memory localization retrieves the topk=5 candidate events based on embedding similarity. To encourage multi-perspective exploration, candidates are selected from the top-p=5 distinct topic clusters. Topic clustering is performed using k-means, where the number of clusters is automatically determined by the current memory size as nclusters = max(2, min(nsamples/5, 50)) During memory search, we employ three parallel Explorer agents to conduct multi-path traversal over the Event Graph. Query refinement is limited to single additional round to control search complexity. Our choice of hyperparameters is motivated by the analysis in Section 5.3. For NarrativeQA, where documents are substantially longer, we increase the retrieval scope to topk=10 while keeping all other settings unchanged. This adjustment allows broader initial coverage without altering the overall search strategy. B.3 Baseline Description Mem0 (Chhikara et al., 2025): scalable long-term memory system that dynamically extracts, consolidates, and retrieves salient facts from ongoing dialogues or streams. It maintains compact set of memory entries by continually updating and merging similar facts, avoiding redundancy, and retrieves only the most relevant facts rather than reprocessing the full context. MemoryOS (Kang et al., 2025): hierarchical memory architecture designed specifically for AI agents in long conversational interactions. It organizes memory into multiple tiers (short-, mid-, and long-term stores) and coordinates four core modulesmemory storage, dynamic update, adaptive retrieval, and response generationto maintain continuity, context coherence, and personalization over long dialogues. HippoRAG (Gutiérrez et al., 2025): graphbased retrieval-augmented generation framework inspired by the hippocampal indexing theory of human long-term memory. It transforms documents into knowledge graph and uses Personalized PageRank over concept seeds to integrate information across disparate contexts, enabling efficient single-step multihop retrieval. This structure allows deeper integration of new experiences and improved retrieval for reasoning-intensive tasks compared to standard RAG. A-Mem (Xu et al., 2025): An agentic memory system for LLM agents that dynamically organizes memory entries into an interconnected network using principles from human note-taking methods. When new memories are added, it generates structured notes with multiple attributes and connects them to related historical memories, enabling continuous memory evolution and contextual organization beyond fixed schemas. CAM (Li et al., 2025a): structured memory framework grounded in constructivist theory, which organizes memory hierarchically and supports flexible integration and dynamic It maintains overlapping clusadaptation. ters and hierarchical summaries and explores memory structure during retrieval in way reminiscent of human associative processes, improving both performance and efficiency on long-text reading tasks. Implementation For baselines that rely on chunk-based retrieval, we apply unified preprocessing strategy by segmenting documents into fixed-length chunks of 512 tokens. For all such methods, we retrieve the top-5 most relevant chunks based on embedding similarity and use them as context for downstream reasoning or answer generation. This ensures consistent retrieval budget across chunk-based baselines. For memory-based baselines, we follow their original experimental settings and implementations as described in the corresponding papers or official codebases, without additional modification. This design ensures fair comparison while preserving the intended behavior of each baseline."
        },
        {
            "title": "Statistics",
            "content": "This section provides detailed analysis of the search and reasoning behavior of CompassMem on the LoCoMo benchmark. We report aggregated statistics to characterize efficiency, exploration dynamics, and the role of planning and refinement during memory search. C.1 Overall Statistics Table 4 summarizes the overall runtime, retrieval, and reasoning statistics across all 1,540 questions. On average, each query is processed within moderate and stable time budget, indicating that active navigation over the Event Graph does not lead to excessive overhead. The median runtime is close to the mean, suggesting consistent behavior across different queries. The Planner generates approximately three subgoals per question, providing structured guidance for exploration. While not all subgoals are fully satisfied, partial satisfaction is common, reflecting the varying availability of supporting evidence in memory. The high refinement rate indicates that 12 iterative query adjustment plays an important role in addressing uncovered aspects during search. Total Questions 1540 Time Metrics Total Time Avg. Time per Question Median Time Max Time Min Time Subgoal Metrics Avg. Subgoals Avg. Subgoal Satisfaction Fully Satisfied 32136.5 20.87 19.32 65.38 4.84 3.04 68.3% 594 (38.6%) Retrieval Metrics Avg. Retrieved Nodes Avg. Initial Nodes Avg. Similarity Traversal Metrics Avg. Paths Avg. Total Steps Avg. Path Length Max Path Length Avg. Max Rounds Action Distribution Total Actions EXPAND SKIP ANSWER Queue Metrics Avg. Initial Queue Size Avg. Max Queue Size Refinement Metrics Refinement Count Refinement Rate Kept Nodes Avg. Kept Nodes Max Kept Nodes No Kept Nodes 50.0 3.7 0.7374 2.5 7.5 2.84 11 2.4 11595 7348 (63.4%) 4230 (36.5%) 17 (0.1%) 3.7 3.7 1176 76.4% 3.15 14 Table 4: Overall search and reasoning statistics on LoCoMo. Path Length Count Percentage 1 2 3 4 5 6 7 8 9 10 11 172 410 336 217 155 169 21 32 18 6 4 11.2% 26.6% 21.8% 14.1% 10.1% 11.0% 1.4% 2.1% 1.2% 0.4% 0.3% Table 5: Distribution of exploration path lengths in memory search. runtime and exploration depth remain relatively stable, suggesting that the proposed search mechanism adapts robustly to different dialogue structures and content distributions. Variations in refinement rate and retained evidence reflect differences in reasoning complexity across items, rather than instability in the search process. C.3 Statistics by Question Category Table 7 summarizes the search and reasoning behavior of CompassMem across different question categories. Reasoning-intensive questions, particularly multi-hop, require longer search trajectories, as reflected by higher average steps and longer processing time. Temporal questions, while involving fewer steps on average, exhibit the highest refinement rate, indicating frequent use of query refinement to resolve temporal dependencies. Single-hop questions are generally easier, requiring fewer steps and refinements while maintaining high subgoal satisfaction rate. Overall, these patterns align well with the inherent complexity of each category and suggest that CompassMem adapts its search behavior according to task demands. C.4 Path Length Distribution Table 5 shows the distribution of exploration path lengths. Most paths are short, with the majority falling between two and four steps, indicating that useful evidence is typically reached through localized reasoning over event relations. Longer paths are rare and correspond to more complex queries requiring extended exploration, demonstrating that deep traversal is selectively invoked rather than pervasive. Case Study: Multi-hop Reasoning over the Event Graph We present representative multi-hop question from the LoCoMo benchmark to qualitatively illustrate how CompassMem performs logic-aware memory search and reasoning over the Event Graph. This example highlights how evidence is incrementally constructed through structured traversal rather than flat retrieval. The case query is: What kinds of artworks did the speaker mention creating after moving to the new city? C.2 Per-Item Aggregated Statistics Table 6 reports statistics aggregated by item groups in LoCoMo. Across different items, the average Answering this question requires linking events about relocation with later creative activities that are mentioned in separate dialogue segments. 13 Item #Q Avg. Time (s) Avg. Steps Refine % Avg. Kept locomo_item1 locomo_item2 locomo_item3 locomo_item4 locomo_item5 locomo_item6 locomo_item7 locomo_item8 locomo_item9 locomo_item10 152 81 152 199 178 123 150 191 156 21.73 21.21 20.02 22.28 19.57 21.25 17.89 20.05 21.96 22.80 7.9 7.6 6.9 8.5 6.9 7.8 5.7 7.1 8.0 8.8 82.9 80.2 79.6 71.9 77.5 77.2 80.7 70.7 76.9 70.9 3.3 3.1 2.8 3.7 2.3 3.4 2.0 3.1 3.6 4.0 Table 6: Aggregated search statistics per item group. Category #Q Avg. Time (s) Avg. Steps Subgoal Sat. % Refine % Multi-hop Temporal Open-domain Single-hop 282 321 96 841 24.61 18.64 24.49 20.05 10.1 5.9 9.2 7. 71.5 61.3 57.9 71.0 78.7 83.8 85.4 71.7 Table 7: Search and reasoning statistics by question category. Planner: Subgoal Decomposition Given the query, the Planner decomposes it into three subgoals: all paths, the agent explores 10 candidate nodes, retains 7 as evidence, and reaches an average path length of 2.84 steps. h1: Identify the event describing the speakers move to new city. h2: Find events mentioning artistic or creative activities after the move. h3: Extract the specific types of artworks mentioned. Query Refinement After the first exploration round, the Planner observes that h3 is only partially supported. It triggers single refinement step to focus on missing details: What specific forms of art did the speaker create after moving to the new city? The Planner initializes the subgoal satisfaction vector as = [0, 0, 0], which is updated as evidence is collected. This refined query guides second round of targeted exploration, mechanism triggered in 76.4% of LoCoMo questions overall. Localization: Selecting Starting Events Using embedding similarity, the system retrieves the top5 candidate events and selects starting nodes from 5 distinct topic clusters. Example starting events include: Moved to Chicago last summer for new job. have been spending weekends exploring art museums. Evidence Aggregation After refinement, the retained evidence set consists of the following key events: Moved to Chicago last summer for new job. started painting landscapes in my apartment. also experimented with stained glass designs. total of 3 starting nodes are inserted into the global exploration queue. Explorer: Multi-path Navigation and Evidence Collection Three Explorer agents traverse the Event Graph in parallel. At each visited event, the Explorer conditions on the query, current subgoals, retained evidence, and local graph relations. The retained evidence set is updated accordingly. Across These events jointly satisfy all subgoals, yielding = [1, 1, 1]. Answer Generation The Answerer generates the final response conditioned only on the distilled evidence: The speaker created paintings and stained glass artworks after moving. 14 illustrates Discussion. This how case CompassMem constructs answers through guided traversal over logically connected events. Rather than retrieving single text chunk, the agent across incrementally multiple paths, refines its search when gaps are detected, and reasons over event dependencies. This process mirrors human multi-step recall and demonstrates the advantage of event-centric memory for complex multi-hop reasoning. accumulates evidence"
        },
        {
            "title": "E Use of AI Assistants",
            "content": "We use ChatGPT to improve the presentations of this paper.1 1https://chatgpt.com/"
        },
        {
            "title": "F Prompt Templates",
            "content": "F.1 Memory Construction Event Extraction Prompt You are an expert information extraction system. Given multi-turn dialog, extract meaningful events and output ONE strict JSON object. Goals: Extract logically coherent events (E1, E2, ...) in chronological order. Each event represents complete logical unit. AGGRESSIVELY COMBINE related micro-events into comprehensive summaries to avoid fragmentation. Merge events that: Involve same participants discussing the same topic. Form logical sequence (decision + action + completion). Are temporally close and thematically related (within 3-5 utterances). Represent different aspects of the same situation/problem. Include follow-up questions, clarifications, or elaborations. PRESERVE ALL important details within each merged event summary. Include: Complete context and all key outcomes, results, and conclusions. Specific facts, numbers, dates, locations, and concrete details. Emotional states, reactions, and interpersonal dynamics. Technical details, requirements, and specifications. Any conditions, constraints, or limitations discussed. IMPORTANT: Include visual content descriptions for shared images. Constraints: List people involved as an array people (max 3). Do not output other entity types or attributes. Event Count: Extract 6-10 comprehensive events. Prioritize fewer, more detailed events over many fragmented ones. RESPONSE FORMAT: Output JSON only, no additional commentary. Event Relation Extraction Prompt You are an expert information extraction system. Given list of extracted events from dialog, identify meaningful pairwise relations between them and output ONE strict JSON object. Goals: Consider ALL unordered pairs of events within the same session (not only adjacent events). Extract pairwise event relations with SHORT, free-form label in type that best characterizes the link. Relation types can include: causal, motivation, enablement, follow_up, temporal_before, temporal_after, contrast, part_of, parallel, elaboration. These are examples, not closed set. Add relations only when meaningful. Prefer specific semantic links over trivial temporal ordering. It is acceptable to have no temporal edges if they add no insight. CRITICAL GUIDELINES: IMPORTANT: For temporal relations (follow_up, temporal_before, temporal_after), base them on the ACTUAL TIME when events occurred in the real world, NOT on when they are described in the dialog. Focus on the chronological sequence of reality. For each relation, cite minimal evidence utterance ids that support the linkage between the two events. RESPONSE FORMAT: Output JSON only, no additional commentary. Event Coreference & Overlap Prompt You are an expert at analyzing events and determining if they refer to the same real-world occurrence or have significant overlap. Given two event descriptions extracted from different dialog sessions, determine: 1. Whether they describe the SAME event (same occurrence at the same time). 2. Whether they have SIGNIFICANT OVERLAP (mention or relate to the same real-world situation/topic). Consider these factors: Do they involve the same people/participants? Do they describe the same actions, situations, or topics? Do they have compatible time references? Would merging their information create more complete picture of ONE event? Output JSON object with these exact keys: { 16 \"same_event\": boolean, \"has_overlap\": boolean, \"relation_type\": string null, \"reasoning\": string } F.2 Memory Search Action Decision Prompt // true if they are the same event // true if they refer to the same situation // suggest relation type if overlap // brief explanation You are an expert information evaluator. Your task is to decide which action to take for the current node based on how relevant and sufficient it is for answering the given question. You have THREE possible actions: 1. SKIP: The current node is NOT helpful for answering the question or satisfying any sub-goals. Use SKIP when the current node contains completely irrelevant information. The current node will be DISCARDED, not used in final answer. You should specify which neighbor node(s) to explore next, OR specify NONE if ALL neighbors are irrelevant. Multi-node selection rules: Maximum 3 nodes, only select HIGHLY relevant ones. 2. EXPAND: The current node IS helpful and helps satisfy some sub-goals, but NOT all sub-goals are satisfied yet. Use EXPAND when the current node contains useful information for one or more sub-goals. The current node will be KEPT and used in the final answer. Specify neighbor node(s) to explore next to satisfy remaining sub-goals, OR specify NONE if no neighbors are relevant. CRITICAL: You MUST indicate which sub-goals are now satisfied by this node + previously kept information. Only mark sub-goal as satisfied if you have DIRECT evidence. 3. ANSWER: Use ONLY when ALL sub-goals are SATISFIED (or nearly all). Use ANSWER when the previously kept information + current node together satisfy ALL sub-goals. The current node will be KEPT and exploration will STOP. CRITICAL: You MUST list ALL satisfied sub-goals to confirm completeness. Be conservative: If ANY sub-goal remains unsatisfied, use EXPAND instead. CRITICAL GUIDELINES: Check sub-goals systematically: For each action, explicitly evaluate which sub-goals are satisfied. ANSWER only when complete: Use ANSWER only when ALL (or all critical) sub-goals are satisfied. Navigate strategically: Choose next nodes that are likely to help satisfy remaining unsatisfied sub-goals. Be explicit about progress: Always indicate which sub-goals your current decision addresses. RESPONSE FORMAT (follow strictly): ACTION: [SKIP/EXPAND/ANSWER] NEXT_NODES: [NODE_ID1, NODE_ID2, ...] (or NONE) SATISFIED_SUBGOALS: [1, 3, 4] (REQUIRED for EXPAND/ANSWER; [] for SKIP) REASONING: [Brief explanation: (1) info provided, (2) sub-goals satisfied, (3) sub-goals remaining, (4) why chosen next nodes target remaining sub-goals] IMPORTANT: (1) For SKIP, SATISFIED_SUBGOALS must be []; (2) For EXPAND/ANSWER: provide list even if empty; (3) Only include sub-goals with DIRECT evidence; (4) Do NOT speculate. QUESTION: {question} {subgoals_text} PREVIOUSLY KEPT INFORMATION: {kept_nodes_info if kept_nodes_info else \"(No information kept yet)\"} CURRENT NODE INFORMATION: {current_info} NEIGHBOR NODES (available for exploration): {neighbor_info} Now, make your decision: Response Generation Prompt Your task is to answer the QUESTION based on the provided CONTEXT. Requirements: Be concise and direct: Provide ONLY the answer in the form of short phrase, not sentence. No explanations or additional commentary. Original wording: If the context contains direct statements that answer the question, use the original wording from the context. Inference: If the context doesnt have direct statements, you may summarize and infer the answer from the relevant information. 17 Time Reference Calculation: If there is question about time references (like \"last year\", \"two months ago\", etc.), calculate the actual date based on the memory timestamp. Example: If memory from 4 May 2022 mentions \"went to India last year,\" then the trip occurred in 2021. Specific Dates: Always convert relative time references to specific dates, months, or years. For example, convert \"last year\" to \"2022\" or \"two months ago\" to \"March, 2023\" based on the memory timestamp. Reasonable Justification: If you are uncertain or lack sufficient information, do not state that the information is insufficient. Instead, provide reasonable and well-justified answer based on general knowledge. Keep it brief: Keep your answer brief and to the point. CONTEXT: {context} QUESTION: {question} ANSWER: Refinement Query Prompt You are an assistant whose role is to generate refined search query to find missing information. ORIGINAL QUESTION: {original_question} SUB-GOALS STATUS: Satisfied sub-goals: {satisfied_text} Unsatisfied sub-goals: {unsatisfied_text} INFORMATION COLLECTED SO FAR: {context_so_far} TASK: Generate NEW search query that specifically targets the UNSATISFIED sub-goals. Your new query should: 1. Focus on the specific unsatisfied sub-goals. 2. Be clear and specific. 3. Use different keywords or phrases than the original question. 4. Target information that would help satisfy the remaining sub-goals. 5. NOT repeat the original question. RESPONSE FORMAT: New Query: [Your refined search query - single clear question or search phrase targeting unsatisfied sub-goals] Target Sub-goals: [List which sub-goal numbers this query aims to satisfy] Generate your response: Memory Node Selection Prompt You are selecting the most promising memory nodes to explore for answering question. QUESTION: {question} {subgoals_text} CANDIDATE NODES (retrieved by semantic similarity): {nodes_text} INSTRUCTIONS: Select the nodes that are HIGHLY LIKELY to contain information relevant to one or more sub-goals. Be selective: Only choose nodes whose summaries clearly indicate relevance to specific sub-goals. Maximum 5 nodes: Select at most 5 nodes to explore. Diversity: Try to select nodes that address different sub-goals if possible. Quality over quantity: Its better to select 2 highly relevant nodes than 5 marginally relevant ones. If nodes summary is vague or doesnt clearly relate to any sub-goal, DONT select it. Consider both the summary content and the similarity score. RESPONSE FORMAT: Selected Nodes: [NODE_ID1, NODE_ID2, ...] Reasoning: [Brief explanation of why each selected node is likely relevant to specific sub-goals] Now make your selection: 18 Cluster-based Node Selection Prompt You are selecting the most relevant memory node(s) to answer question. QUESTION: {question} AVAILABLE NODES: {nodes_text} INSTRUCTIONS: Select the node(s) that are HIGHLY relevant to answering the question. Be selective: Only choose nodes that are HIGHLY relevant to the question. Maximum 3 nodes: Select at most 3 nodes per cluster. If ONLY ONE node is clearly the most relevant, select just that one. Select multiple nodes (2-3) ONLY when they are ALL highly relevant AND provide complementary information: * Information is distributed across multiple memories about the SAME topic. * The question has multiple specific aspects that DIFFERENT nodes address. * Multiple nodes provide different pieces of the SAME answer. Consider the summary content, people involved, and time information. Do NOT select nodes that are only tangentially related or vaguely relevant. RESPONSE FORMAT: Selected Nodes: [NODE_ID1, NODE_ID2, ...] Reason: [Brief explanation of why these specific nodes are HIGHLY relevant] Strategic Planning Prompt You are strategic planning assistant. Your task is to analyze question and break it down into 2-5 specific sub-goals that need to be satisfied to fully answer the question. QUESTION: {question} INSTRUCTIONS: 1. Analyze what information components are needed to fully answer this question. 2. Break down the question into 2-5 specific, concrete sub-goals. 3. Each sub-goal should represent distinct piece of information needed. 4. Sub-goals should be: Specific and clear (not vague) Independently verifiable (can determine if its satisfied) Collectively sufficient (together they fully answer the question) Atomic (each sub-goal addresses ONE aspect) RESPONSE FORMAT (follow strictly): Sub-goal 1: [First specific information need] Sub-goal 2: [Second specific information need] Sub-goal 3: [Third specific information need] ... Now analyze the question and generate sub-goals:"
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}