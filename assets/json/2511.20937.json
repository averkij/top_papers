{
    "paper_title": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction",
    "authors": [
        "Qineng Wang",
        "Wenlong Huang",
        "Yu Zhou",
        "Hang Yin",
        "Tianwei Bao",
        "Jianwen Lyu",
        "Weiyu Liu",
        "Ruohan Zhang",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Manling Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/."
        },
        {
            "title": "Start",
            "content": "ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction Qineng Wang*1, Wenlong Huang*2, Yu Zhou3, Hang Yin2, Tianwei Bao1, Jianwen Lyu1, Weiyu Liu2, Ruohan Zhang2, Jiajun Wu2, Li Fei-Fei2, Manling Li1 1Northwestern University 2Stanford University 3UCLA * Equal contribution; Equal advising. (cid:128) Website Code Dataset Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in visual question answering (VQA) format. Framed as partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognitionaffordance recognition, actioneffect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric inputwhile avoiding low-level image synthesis that could confound the evaluation. We provide scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluate models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on inverse task than forward one and exhibit anthropocentric biases, including preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at enact-embodied-cognition.github.io. 5 2 0 N 6 2 ] . [ 1 7 3 9 0 2 . 1 1 5 2 : r Figure 1 ENACT casts embodied cognition evaluation as world modeling through egocentric interaction. Grounded in POMDP framework, ENACT considers two tasks from diverse activities and scenes: forward world modeling (ordering observations given actions) and inverse world modeling (ordering actions given observations). Evaluation shows that performance of VLMs drops as interaction horizons lengthen, performs better on inverse task, and lags behind humans. 1. Introduction Intelligent behavior in the physical world requires grounding abstract knowledge in interaction with the environment. Embodied cognition argues that intelligence is not passively acquired but enacted through continuous sensorimotor interaction with the world (Smith & Gasser, 2005). Recent advances in large foundation models such as VisionLanguage Models (VLMs) (OpenAI, 2025; DeepMind, 2025), although predominantly trained in disembodied fashion, have exhibited promising signs of interactive intelligence. Yet it remains unclear how to measure the extent to which embodied cognition emerges in these models. Prior work examines complementary slices of embodied cognition: spatial perception in static scenes (Ramakrishnan et al., 2024), linguistic reasoning and planning (Li et al., 2024b), and reasoning about interactions between primitive objects (Yi et al., 2019; Gao et al., 2025). Taxonomies such as Yang et al. (2025) attempt to catalogue embodied capabilities for VLMs but ultimately rely on subjective criteria. Consequently, unified objective that tightly couples egocentric perception with embodied interactions in everyday activities remains missing (Frick & M칬hring, 2016; Thompson, 2005; Clark, 2006; Barsalou, 2020). To this end, we introduce ENACT, benchmark that studies embodied cognition through world modeling via egocentric interaction in visual question answering (VQA) framework. Grounded in partially observable Markov decision process (POMDP, 칀str칬m, 1965, Figure 1), we formulate world modeling (Ha & Schmidhuber, 2018) as the evolution of egocentric visual observations of the environment conditioned on an agents actions, which are represented as scene graph (Johnson et al., 2017) changes derived from low-level physics-based simulator state. Specifically, we focus on two forms, forward world modeling and inverse world modeling, and they are formulated as sequence reordering for evaluating embodied cognition for VLM. This sequence-reordering VQA view isolates long-horizon interactive visual reasoning from photorealistic video prediction and forces models to reason about how sequences of embodied actions transform the environment from egocentric observations. In forward world modeling, given visual observation and sequence of actions, the model must reorder shuffled sequence of future visual observations. In inverse world modeling, given an ordered sequence of visual observations, the model must reorder the corresponding shuffled action sequence. Though seemingly narrow lens, answering these queries implicitly involves broad set of capabilities central to embodied cognition: affordance recognition, actioneffect reasoning, and embodied awareness, together with reasoning about contact and other low-level physical consequences encoded in our predicates, all from egocentric input. Under partial observability, it demands integrating observations and actions across extended horizons, posing challenges for interactive and long-term memory. Through the same lens, we also examine factors that inform future VLM data design, including existing biases toward human embodiment such as right-handedness and human-like egocentric viewpoints and intrinsics (e.g., FOV, aperture). Leveraging this unified lens, we additionally demonstrate how such evaluation data can be automatically and scalably generated within robotics simulator, such as BEHAVIOR (Li et al., 2024a). Given robot manipulation trajectory, we extract symbolic scene graphs leveraging ground-truth, physics-based simulation state (e.g., continuous poses and contacts), which is uniquely accessible in simulation compared to the real world. We then select the key-frames where the scene graph changes and sample subsequences of desired lengths to assemble the stateaction sequences for constructing the question-answer (QA) pairs. By additionally leveraging novel QA sampling scheme, our pipeline can generate up to millions of QAs from single episode, enabling massive scaling across the thousands of trajectories common in robotics datasets. Although we do not train or finetune VLMs here, the resulting data directly supports Figure 2 Overview of ENACT data curation pipeline. We first obtain aligned scene graphs (states) and RGB observations from mobile manipulation dataset in robotics simulation (BEHAVIOR). The trajectory is then segmented by identifying key-frames where an abstract state change occurs (i.e., the scene graph difference is non-empty). From this set of key-frames, we sample multiple key-frame trajectories, which are used to construct the forward and inverse world modeling VQA questions. Here 洧녜 refers to the number of all sampled trajectories across all step lengths. future embodied decision-making studies (Azzolini et al., 2025). We report two metrics at different granularities: Task Accuracy (exact ordering) and Pairwise Accuracy (fraction of adjacent pairs correctly ordered). Our experiments reveal that ENACT is challenging for current VLMs, which lag significantly behind human performance (Figure 1). This performance gap widens as the task horizon increases, where VLM accuracy degrades sharply with trajectory length while human performance remains high. We also find that all models consistently perform better on inverse than on forward world modeling. Furthermore, we uncover two notable biases: VLMs show clear preference for understanding right-handed dynamics, and the performance of representative models like GPT-5 mini drops significantly with non-human-eye-like camera intrinsics or viewpoints. Overall, our contributions are threefold: (1) We introduce ENACT, benchmark for evaluating embodied cognition via forward and inverse world modeling from egocentric interaction. (2) We provide an easily scalable data generation pipeline leveraging robotics simulation (BEHAVIOR) and provide dataset of 8,972 QAs synthesized from diverse, long-horizon interactions in everyday environments. (3) Experiments on state-of-the-art VLMs reveal widening gap to human performance with horizon length, anthropocentric biases (e.g., right-handed priors, human-like camera intrinsics), and real-world evaluations that mirror simulator trends with only limited sim-to-real gap. 2. ENACT: Egocentric Interactive Embodied Cognition Test 2.1. Problem Formulation We investigate the embodied cognition of VLMs by framing it as world modeling problem, which we evaluate using egocentric, interactive reasoning tasks. Formally, the benchmark is defined over state space S, whose elements are symbolic scene graphs derived from low-level simulator states G; an observation space R洧냩洧녥 3 of robots egocentric RGB views of the environment; and an action space whose elements are scene-graph differences 洧녩洧노 = 洧(洧멇롐, 洧멇롐1). symbolic scene graph is structured data model that represents the objects in scene as symbolic nodes (e.g., On(fridge)) and their relationships as edges (e.g., OnTop(pen, desk)). We view the underlying embodied task as POMDP. As shown in Figure 2, we first filter this raw data to identify all timestamps where an abstract scene graph state change occurs (i.e., the 3 scene-graph difference 洧(洧멇롐, 洧멇롐1) ). This process yields smaller, chronologically ordered set of segmented frames, which serve as the candidate pool for our benchmark. From the pool of segmented frames, we sample 洧녠 trajectories, each with chronologically ordered tuple 洧랢 = (洧녰0, , 洧녰洧1) of 洧 key frames. This initial abstraction into discrete decision epochs is similar to semi-MDP (Sutton et al., 1999). However, we treat each of these final keyframe trajectories as self-contained POMDP instance with scene graphs 洧녡洧랢 and observations 洧녝洧랢. For 洧녲 = 0, , 洧 2, the action connecting consecutive key frames is the visible scenegraph delta 洧녩洧녲 := 풊Vis(洧멇롐뒳롐+1, 洧멇롐뒳롐), where 풊Vis returns the subset of differences in 洧(洧멇롐뒳롐+1, 洧멇롐뒳롐) that are visible in both images. Together, these actions form discrete symbolic action space A. For notation simplicity, we relabel indices in 洧랢 for each key-frame trajectory to 洧랢 = (0, , 洧 1) and (洧멇롐, 洧녶洧녲) := (洧멇롐뒳롐, 洧녶洧녰洧녲). Building on these trajectories, we formalize two tasks. For forward world modeling, given the current image 洧녶0, the correctly ordered action sequence (洧녩0, . . . , 洧녩洧2), and shuffled list of observation images 洧녝 = (洧녶 ), the model outputs permutation 洧랥 Sym([洧 1]) that orders the images to match the actions: (洧녶 洧랥(1) , . . . , 洧녶 洧랥(洧1) ) = (洧녶1, . . . , 洧녶洧1). For inverse world modeling, given 洧녶0, the correctly ordered observation images (洧녶1, . . . , 洧녶洧1), and shuffled list of actions 洧냢 = (洧녩 ), the model outputs permutation 洧랦 Sym([洧 1]) that orders the actions to be consistent with the observation progression: (洧녩 1, . . . , 洧녶 0, . . . , 洧녩 洧랦(洧1) ) = (洧녩0, . . . , 洧녩洧2). 洧랦(1) , . . . , 洧녩 洧2 洧1 2.2. Key-Frame Trajectories Synthesis for Scalable Data Generation Segmented Frames with Abstract State Changes. Raw robot trajectories often contain long stretches with no semantic changes (e.g., gripper motion when opening the toolbox in Figure 2). We mark timestamp 洧노 whenever the simulator state makes minimal abstract state change, such as transitioning from not grasping to grasping drill with the right hand. The BEHAVIOR simulator exposes boolean and relational predicates, where flipping one predicate or updating relation is our atomic state change. time 洧노 enters the candidate pool if the scene-graph difference 洧(洧멇롐, 洧멇롐1) is nonempty. To avoid near-duplicate frames, we compare each new change with the last accepted segmented frame: we form predicate-level change signature 洧녫 洧녱 and keep 洧노 only if its cosine similarity with the previous signature 洧녫 洧녱1is below threshold. This yields chronological set of segmented frames = {洧노1 < < 洧노洧 } with (洧멇롐뫯롐, 洧녶洧노洧녰). Thresholds and further details are in the Appendix A.2.1. Key-Frame Trajectories Synthesis. From the segmented 洧 frames, we sample length-洧 key-frame trajectories 洧랢 = (洧녰0, . . . , 洧녰洧1) with 1 洧녰0 < < 洧녰洧1 洧, so indices do not need to be adjacent. Each candidate is strictly validated: for every 洧녲, the visible state change 풊Vis(洧멇롐뒳롐+1, 洧멇롐뒳롐) is nonempty, and the edited objects are visible in both images, except for object transitioning events (e.g., pineapple being diced), where transient occlusion is permitted. We then treat each valid key-frame trajectory as an individual POMDP instance, with 洧녡洧랢 and 洧냢洧랢 as defined in the problem formulation. To further scale data generation, we exploit that typically 洧 < 洧 (in practice 洧 10 while 洧 30), and we use skipping to convert trajectory construction into seat selection combinatorics problem, choosing 洧 seats out of 洧, which yields at most (cid:0)洧 (cid:1) distinct candidates from single trajectory. The detailed algorithm is in the Appendix A.2.2. 洧 World Modeling QA Generation. After obtaining the sequence of key-frame trajectories, we formulate the forward and inverse world modeling as sequence reordering visual-question answering (VQA) tasks. This formulation offers two advantages. First, it yields clean evaluation signal for long-horizon interactive visual reasoning without conflating performance with photorealistic video prediction. Second, it requires models to reason about how sequence of 4 Figure 3 Data sources and QA examples. ENACT is built from diverse, long-horizon activities performed by real robots (left). We provide examples for (mid) forward world modeling and (right) inverse world modeling. More QA examples and prompts are available in the Appendix A.3.3. embodied actions causally transforms the environment over multiple steps, while maintaining long-horizon spatial memory from purely egocentric observations in large home-scale scenes. 2.3. Dataset Overview and Evaluation Design Dataset Overview. We construct the benchmark from the BEHAVIOR simulator and challenge (Li et al., 2024a). BEHAVIOR Challenge provides diverse long-horizon home-scale activities, and we select 29 of them, using one trajectory per activity to recover aligned pairs {(洧멇롐, 洧녶洧노)}. Each trajectory is segmented into segmented frames K, then converted into key-frame trajectories and finally into two QA types: forward and inverse world modeling (examples in Figure 3). Across step lengths 洧 {3, . . . , 10} we sample about 560 items per 洧 for each QA type, yielding 8,972 total questions. The data uses common predicate classes (e.g., Open, Cooked, Grasping), with distributions shown in Figure 3; full statistics appear in Appendix A.3.1. Evaluation Design. Multiple valid answers can exist for given question, so we use an online verifier that accepts any predicted permutation, 洧랥 or 洧랦, consistent with the input constraints. We report two metrics: Task accuracy captures exact ordering, while Pairwise accuracy grants partial credit for near-correct sequences. Specifically, (1) Task accuracy measures success at the question level: question receives score 1 if the verifier accepts the full prediction and 0 otherwise, and the dataset score is TA = (1/D) (cid:205)洧논 1{accepted(洧논)}. (2) Pairwise accuracy measures stepwise consistency: for question with length 洧, we count adjacent pairs passing the verifiers local check (stateaction for forward; actionstate for inverse) and divide by 洧, then report the micro-average PA = (cid:0) (cid:205)洧논 #correct pairs in 洧논(cid:1) (cid:14) (cid:0) (cid:205)洧논 洧洧논 (cid:1), equivalent to averaging per-item pairwise scores when 洧 is fixed. Implementation details are in Appendix A.3.2. 3. Experiments and Analysis 3.1. World Modeling as Proxy for Evaluating Embodied Cognition Experimental Setup. (1) VLM evaluation setup. We evaluate ENACT with 7 proprietary VLMs from 3 families (OpenAI, 2025; DeepMind, 2025; Anthropic, 2025) and 23 open-weight models from 11 families (Wang et al., 2025b; Bai et al., 2025; Hong et al., 2025; MetaAI, 2025; Team et al., 2025; Lu et al., 2025; Yao et al., 2024; Azzolini et al., 2025; Team, 2024; Deng et al., 2025). For input, all images are resized to 512 512, and we use unified prompt template per QA type. 5 Model Proprietary Models GPT-5 GPT-5 mini GPT-5 nano Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.5 Flash-Lite Claude Sonnet 4 Forward World Modeling Inverse World Modeling 3 4 5 6 8 9 10 3 4 6 7 8 9 10 84.62 75.26 69.96 64.18 57.48 52.16 49.45 46.93 86.28 80.37 76.09 68.78 65.71 62.13 57.12 55.33 87.50 76.25 70.65 63.41 58.14 52.38 46.65 44.11 85.05 76.77 75.43 67.67 63.79 57.04 55.04 50.02 67.83 50.29 38.61 30.35 25.97 21.90 17.59 16.84 72.81 53.95 42.48 36.45 31.68 28.20 24.11 20.33 86.10 76.42 69.83 60.80 53.26 48.12 40.12 36.98 87.94 81.18 75.39 70.03 66.03 62.91 57.78 56.62 81.64 67.94 54.17 43.38 37.43 32.73 29.88 28.07 82.78 72.18 60.83 58.19 53.14 51.78 47.99 44.98 64.34 49.07 38.70 33.87 27.81 25.44 23.31 20.31 69.58 57.55 46.04 39.09 34.06 30.18 27.51 23.16 65.65 45.82 36.65 30.52 26.61 22.78 21.49 20.16 73.25 56.85 48.87 43.07 37.00 32.71 30.50 28. Open-Weight Models 74.30 59.99 47.65 38.78 30.83 25.69 21.60 19.67 80.59 69.28 57.04 51.53 46.95 41.68 37.36 37.93 GLM-4.5V Llama-4-Mav-17B-128E-Ins 72.47 52.09 43.87 35.30 29.90 25.89 22.79 20.49 72.55 62.60 50.52 43.10 35.17 31.68 28.10 25.80 75.79 62.25 50.83 45.85 37.84 32.88 27.85 25.24 82.26 70.09 60.61 53.38 45.90 39.35 34.12 30.56 InternVL3.5-241B-A28B 63.29 44.66 32.04 25.82 22.11 19.50 16.74 16.29 64.95 48.37 40.04 33.87 28.53 23.63 21.74 19.36 Gemma-3-27b-it 69.14 52.96 40.83 36.27 33.16 30.63 26.30 24.76 71.33 58.77 48.43 44.36 40.26 39.30 36.66 36.58 QVQ-72B-Preview 78.15 60.05 49.87 41.92 36.77 31.73 28.03 25.07 77.80 65.85 53.30 48.19 44.07 37.57 33.76 36.27 Qwen2.5-VL-72B-Ins 67.83 55.46 44.35 35.75 27.52 26.42 22.01 18.07 63.55 59.70 54.57 51.01 49.36 47.17 41.47 40.16 Qwen2.5-VL-32B-Ins 58.39 42.51 34.96 31.08 24.61 20.78 18.11 16.96 64.86 51.74 41.65 35.47 30.95 26.64 23.70 23.25 Ovis2.5-9B 60.75 38.73 33.65 25.47 24.81 21.40 21.56 18.33 69.23 53.08 47.35 39.55 34.87 30.63 27.05 25.71 MiniCPM-V-4.5 60.23 36.99 31.83 24.25 21.29 20.80 20.46 17.71 47.38 33.86 27.26 23.48 19.87 18.50 17.04 15.16 Idefics3-8B-Llama3 56.28 41.86 34.75 28.40 26.46 26.49 25.41 24.88 58.30 45.93 44.25 38.50 35.72 34.56 31.50 28.64 Cosmos-Reason1 30.24 40.19 29.65 25.37 22.75 19.45 17.84 15.87 56.73 52.85 40.09 35.44 29.67 24.39 28.70 18.91 BAGEL Human Performance 93.62 95.30 95.04 93.87 95.43 95.41 94.75 95.13 92.05 93.56 94.35 94.25 95.96 97.74 96.30 96.29 Table 1 Evaluation on ENACT (Pairwise Accuracy). Dark gray indicates the best result within each category (Proprietary or Open-Weight Models), and Light gray denotes the second-best result within the category. Complete results are in Table 9 (Task Accuracy) and Table 10 (Pairwise Accuracy). Models are instructed to return parsable Python list encoding permutation of indices. We apply the online verifier in Section 2.3 and report Task Accuracy and Pairwise Accuracy. (2) Human evaluation setup. We also recruit trained annotators to answer the benchmark under the same instructions as the models. For inter-annotator agreement (IAA), we uniformly stratify 240 items over QA type and step length and collect independent labels from three annotators. Krippendorffs 洧띺 = 0.83 indicates strong agreement. Full details are in Appendix B.2.1 and B.1.1. We visualize Task Accuracy for GPT-5 and human annotators in Figure 1. Since many models collapse at long horizons (洧 =810, near-zero task success), we focus on the more informative Pairwise Accuracy. The main results appear in Table 1. Is inverse world modeling easier than forward? Across families and step lengths, inverse consistently outperforms forward, with the margin widening as 洧 grows. For example, GPT-5 and Gemini 2.5 Pro maintain gaps at 洧 6, and open-weight models such as GLM-4.5V and Qwen2.5-VL also show higher inverse scores than forward for most 洧 (see Table 1). This asymmetry suggests that models handle retrospective textual reasoning better than the prospective visual simulation required for forward planning. How does performance change with step length? Accuracy decreases monotonically with 洧 for nearly every model, proprietary and open-weight alike. Shorter tasks (洧 4) are manageable for several VLMs, while longer tasks (洧 8) are challenging even for the strongest models. Pairwise Accuracy softens but does not alter this downward trend. This sharp performance decay reveals that VLMs struggle to track evolving physical states. Can VLMs achieve near-human performance? Human performance is far better than any evaluated VLM. SOTA VLMs such as GPT-5 and Gemini-2.5 Pro are comparable to humans only at step length 洧 = 3; their performance drops sharply as the horizon grows. This vast gap confirms that, compared with humans, VLMs still struggle with interactive embodied world modeling tasks. What is the performance comparison among VLMs? GPT-5 and Gemini 2.5 Pro are the 6 Figure 4 Real-World Evaluations. Key frames from the three real-world scenes used in our evaluation: kitchen, dinner table, and workspace. Together, these scenes contain diverse rigid, deformable, and articulated objects in diverse environments with varying lighting conditions. strongest overall in both forward and inverse settings. Several open-weight VLMs are competitive: InternVL3.5-241B-A28B, GLM-4.5V, and Qwen2.5-VL often close much of the gap and even surpass Claude 4 Sonnet in multiple settings. GPT-5 mini is also highly competitive, achieving the best score at several short and mid horizons (e.g., forward at 洧 = 3, 7, 8). Does Cosmos-Reason1, trained on embodied data, outperform other similar-sized models? We compare Cosmos-Reason1-7B and other similar-sized VLMs in Table 10 and Figure 19. For similar-sized models, Cosmos-Reason1-7B exhibits more stable and generally better performance than other VLMs when the interaction horizon exceeds 5 steps. (cid:17) Key Takeaways: World Modeling as Proxy for Evaluating Embodied Cognition Higher performance on the inverse task than the forward one highlights stronger language-based retrospection than action-conditioned visual reasoning. Long-horizon degradation reveals limited interactive, spatial memory under partial observability. The humanmodel gap shows that current VLMs are still far from robust embodied world models in mobile manipulation settings. Beyond these trends, we also confirm that augmenting key-frame selection with contactbased predicates derived from low-level physics yields qualitatively similar behavior (inverse > forward, strong long-horizon degradation); detailed results are provided in Appendix B, Table 11. 3.2. Sensitivity to Image Realism Since ENACT is generated in the BEHAVIOR simulator, we ask whether VLMs are sensitive to image realism and whether sim-to-real gap appears. To investigate this, we collect additional real-world videos and manually annotate them. Furthermore, leveraging the automated ENACT pipeline with simulation, we conduct an in-depth and larger-scale study of how different rendering configurations in the simulator affect performance. Real-World Experimental Setup. To assess whether the simulator findings transfer to real imagery, we construct real-world benchmark over three everyday scenes (kitchen, dinner table, and workspace). From videos recorded in each scene, we manually select segmented frames and annotate scene graph for every frame. Applying the ENACT pipeline to these annotations yields 960 real-world QA pairs, on which we evaluate InternVL3.5-241B-A28B. Examples from 7 the three scenes are shown in Figure 4. Simulation Experimental Setup. (1) Configuration. We use GPT-5 mini as the base model for SOTA VLMs due to its strong cost-performance balance shown in Table 1. For diversity, we also evaluate InternVL3.5-241B and report its performance across all settings in Figure 18. We evaluate step lengths 洧 {3, 6, 9}. For each 洧 and each QA type (forward, inverse), we sample 50 items, yielding 300 total QAs. We report, for each setting, the Pairwise Accuracy difference 풊 < 0.05 풊 = PAbaseline PAvariant and two-tailed unpaired p-values versus the baseline. will be considered as small change. (2) Image realism implementation. BEHAVIOR uses Isaac Sim (NVIDIA, 2025), our baseline uses Ray Tracing (NVIDIA, 2021) with default global effects. We evaluate three alternatives on realism spectrum: Realistic (segmented frames translated to real-world style using GPT-image-1 OpenAI (2025)), Path Tracing (higher-fidelity rendering, Kajiya (1986)), and Ray Tracing Only (Ray Tracing with global effects such as reflections and stage lights disabled). Detailed setup, prompts, and examples are in the Appendix B.4. Results are summarized in Figure 5 (panel A). Does real-world data change performance? As summarized in Table 2, InternVL3.5-241BA28B exhibits similar absolute accuracy on real-world data as in the simulator. On real videos, inverse queries consistently outperform forward ones, and accuracy drops sharply as the temporal horizon increases. Overall, real-world evaluation does not reveal significant simto-real gap, and the empirical trends are fully consistent with those observed in the simulator. This consistency validates the simulation as faithful proxy for evaluating real-world embodied cognition. Metric 4 5 6 7 8 10 3 4 5 6 8 9 10 Forward (Real-World) Inverse (Real-World) 73.33 50.00 33.33 13.33 8.33 3.33 0.00 0.00 90.00 85.00 55.00 38.33 21.67 13.33 6.67 3.33 Task Accuracy Pairwise Accuracy 80.00 67.78 61.25 49.00 38.89 37.14 26.88 25.74 90.00 88.33 72.92 57.67 44.17 43.57 31.04 26. Table 2 Real-world evaluation of InternVL3.5-241B-A28B. Task and pairwise accuracies (%) on 960 QA pairs generated from real-world manipulation videos. We report performance across different interaction horizons for both forward and inverse world modeling. Does rendering realism change performance? We find no statistically significant degradation or improvement across the spectrum. All settings have 洧녷 0.2 relative to the baseline, and observed deltas are small across both QA types and all step lengths (Figure 5, A; Figure 18, A), suggesting that the evaluated VLMs are robust to rendering variations in world modeling tasks. (cid:17) Key Takeaways: Image Realism Real-world evaluation mirrors our simulation findings with minimal sim-to-real gap, while simulation offers more controlled and reproducible testbed. Insensitivity to rendering variations indicates bottlenecks in multi-step interaction reasoning rather than low-level image realism. 3.3. Do VLMs Exhibit Anthropocentric Bias on Human Vision? VLMs are mostly trained on RGB images that mirror how humans typically see the world. However, different embodiments may have diverse camera configurations. We therefore test whether VLM performance is sensitive to camera configuration, i.e., if dataset bias is present. Experimental Setup. (1) Configuration. We reuse the setup from Section 3.2. We use Figure 5 Evaluations on image realism and anthropocentric bias on human vision through ENACT. Heatmaps show two-tailed unpaired t-test results against the baseline, using Pairwise Accuracy. 洧녷 < 0.05 is considered significant. Darker red means more significant. 풊 is the performance change from the baseline. If significant and 풊 < 0, the setting is worse than the baseline. C.2 reports the robots performance on the leftand right-hand predicates, where Mixing is the proportion of ground truth left or right cases that are predicted as the other hand (i.e., mixing one hand into the other). means standard error. GPT-5 mini as the base VLM, and report InternVL3.5-241B in the Appendix B.5. (2) Camera FOV. The baseline is Aperture 40. We examine Aperture 30, 60, 80, and Fisheye. Rendering and all other parameters are held fixed. (3) Camera Height. The baseline is (1.75 m) high for eye-level view used in Behavior replays. We test High (+0.5 m) and Low (0.25 m). We choose (0.25 m) since lower height will consistently make relevant objects invisible. Examples are in the Appendix B.5. Results are summarized in Figure 5 (panels B.1 and B.2). Does field of view matter? Figure 5 (B.1) shows that small change to Aperture 30 shows no significant difference from baseline ( 洧녷 > 0.1). Larger deviations hurt performance: Aperture 60, 80, and Fisheye are consistently and significantly worse than baseline across QA types and step lengths ( 洧녷 0.01), suggesting that the model performs better with human-like intrinsics. Does camera height matter? As shown in Figure 5 (B.2), increasing the camera height (High) significantly reduces GPT-5 minis accuracy in the forward setting with 풊 = 0.13. By contrast, the High inverse setting shows no statistically significant change, though with performance drop 풊 = 0.06. For the Low camera, both forward and inverse are not significantly different from the baseline, likely because the 0.25 shift remains within normal human height variation. (cid:17) Key Takeaways: Anthropocentric Bias on Human Vision Performance degradation on non-standard views implies VLMs are biased towards human-like egocentric viewpoints and intrinsics. Reliance on human-centric visual priors limits generalization to diverse robotic embodiments with non-standard optics. 9 3.4. Do VLMs Exhibit Anthropocentric Bias on Embodiment? To further understand VLM embodiment, we investigate two potential biases: self-awareness regarding the robots own body and handedness asymmetry, common trait in humans. Experimental Setup. We study these two aspects using distinct experimental setups. (1) Robot Appearance. To test for self-awareness, we assess whether VLMs can recognize their embodiment regardless of its appearance. We reuse the experimental configuration from Section 3.2, with GPT-5 mini as the base model. The baseline is the default black-and-white robot appearance. We test three variants: White Color, Random Color (robot color is randomized at each frame), and Skin Color (robot is rendered with human-like skin tone). (2) Handedness Asymmetry. Inspired by human motor control, where approximately 89% of the population is right-handed (Papadatou-Pastou et al., 2020), we investigate if VLMs exhibit similar dominant hand. We analyze this configuration with predicate-level error analysis of all tested VLMs and report GPT-5 mini in Figure 5. We isolate all errors related to the LeftGrasping and RightGrasping predicates. Using the framework described in Section 3.5, we frame our metrics in terms of Precision and Recall. We also report Mixing Rate, which measures the proportion of ground-truth state differences for one hand that the model incorrectly attributes to the other. Higher precision and recall with lower mixing indicate greater proficiency. Appearance examples and handedness analysis are in the Appendix B.6.1 and B.6.2. Are VLMs aware of their own embodiment, and is this awareness robust to changes in their visual appearance? As shown in Figure 5 and Figure 18 (panel C.1), altering the robots appearance has no statistically significant impact on performance for both GPT-5 mini and InternVL3.5-241B. For all variants (White, Random, Skin Color), the performance deltas are small (풊 < 0.05) and non-significant (all 洧녷 > 0.10), suggesting that the models understanding of its interaction with the world is not tied to specific visual representation of its body. Do VLMs exhibit handedness asymmetry in their interactions with the world? Our analysis of hand-related errors, summarized in Figure 5 (panel C.2), reveals consistent and strong asymmetry (complete error results are shown in Figure 40a and 40b). For both forward and inverse tasks, the right hand consistently outperforms the left hand across all metrics. Precision and recall are substantially higher for the right hand, while the mixing rate is significantly lower. For instance, in the forward task, 9.38% of true left-hand changes were incorrectly identified as right-hand changes, whereas only 4.67% of right-hand changes were misattributed to the left. Full analysis is in Appendix C.1. (cid:17) Key Takeaways: Anthropocentric bias on Embodiment VLMs are robust to the embodiment appearance variations. VLMs exhibit strong right-handed bias, which is consistent with human handedness distribution. 3.5. Error Analysis 3.5.1. Experimental Setup To gain deeper insight into the reasoning failures of VLMs, we design systematic error analysis framework. Evaluating errors directly from output permutations (e.g., comparing predicted order [3, 2, 1] to ground truth [2, 3, 1]) is difficult and often uninformative about the underlying cognitive mistakes. Exploiting the fact ground-truth symbolic scene graphs are readily accessible in simulation, we instead convert the models output into format that 10 allows for structured, fine-grained comparison with the ground truth. For the forward world modeling task, we take the models predicted permutation of images (洧녶 洧랥(洧1) ) = (洧녶1, . . . , 洧녶洧1) and compute the corresponding sequence of actions (i.e., visible state differences) that this ordering implies: 틙洧녩洧녲 := 풊Vis(洧 洧랥(洧녲) ). This yields predicted action sequence ( 틙洧녩0, , 틙洧녩洧2). For the inverse world modeling task, the model directly outputs predicted action sequence. 洧랥(1) , . . . , 洧녶 洧랥(洧녲+1) , 洧 With both predicted and ground-truth action sequence, we can perform pairwise comparison at each step 洧녲. Each action 洧녩洧녲 is set of atomic state differences (e.g., {Add Open(fridge), Remove Inside(basket, cabinet)}). By comparing the predicted set 틙洧녩洧녲 with the grounded-truth set 洧녩洧녲, we can categorize each atomic state difference. This comparison, similar to analyzing Venn diagram, yields three primary outcomes for each ground-truth state difference: (1) Correct: The state difference is present in both the ground-truth and predicted sets. (2) Omission: The state difference is in the ground-truth set but missing from the prediction. (3) Hallucination: The state difference is in the predicted set but not in the ground truth. Detailed setup is in Appendix C. We assume each state difference is an independent event and aggregate these counts across all actions and all questions in the dataset. Based on this framework, we classify errors into five main categories: 1. Entity Substitution. The model correctly identifies the state change predicate but applies it to the wrong object(s). 2. Polarity Inversion. The model correctly identifies both the object(s) and the predicate, but reverses the polarity of the change (e.g., remove instead of add). 3. Predicate Substitution. The model correctly identifies the object(s) involved but describes the state change with an incorrect predicate. 4. Hallucination. The model predicts state change that did not occur in the ground truth. 5. Omission. The model fails to predict ground-truth state change that occurred. 3.5.2. Error Distribution Analysis Our error analysis for GPT-5, shown in Figure 6, reveals that the vast majority of errors fall into two main categories: Omission and Hallucination. For the forward task, these two error types account for combined 81% of all failures. This figure is even higher for the inverse task, where they make up nearly 84% of errors. This indicates that the models primary challenge is not misinterpreting the specifics of known state change, but correctly identifying which changes occurred and which did not. While Omission and Hallucination errors Figure 6 Error Distribution, broken down by forward and inverse tasks, evaluated on GPT-5. are dominant in both settings, their distribution shifts between tasks. In forward modeling, Hallucination is the most common error at 43.9%, followed by Omission at 37.1%. Remarkably, in the inverse task, these two errors are perfectly balanced, each accounting for exactly 41.8% of all failures. Other error types are far less frequent. Polarity Inversion is more common in the forward setting (12.4%) than the inverse (9.2%). Entity Substitution is also slightly more prevalent in the forward task (6.3% vs. 5.4%). Finally, Predicate Substitution remains the rarest 11 error type, though it is more pronounced in the inverse setting (1.9%) compared to the forward task (0.3%). Detailed analysis is in Appendix C. (cid:17) Key Takeaways: Structural Error Analysis The dominance of hallucinations suggests models rely on learned textual priors rather than faithful visual grounding. High omission rates reveal fundamental deficit in tracking object persistence across complex state transitions under egocentric partial observation. 4. Related Work Embodied Cognition. The theory of Embodied Cognition argues that intelligence arises from an agents sensorimotor interaction within its environment, grounding abstract knowledge in perception and action (Gibson, 2014; Varela et al., 2017; Clark, 1998; Brooks, 1991; Oregan & No칢, 2001; Barsalou, 1999; Lakoff & Johnson, 2008). Grounded in the POMDP framework for decision making (Kaelbling et al., 1998), our work focuses on how spatial perception, physical interaction, and linguistic abstraction jointly support embodied cognition (Frick & M칬hring, 2016; Thompson, 2005; Clark, 2006; Barsalou, 2020). Rather than enumerating individual capabilities, we adopt unified lens of world modeling through egocentric interaction, using controlled, reproducible, and scalable simulation to contrast the behavior of current VLMs with humans. World Modeling. World models learn action-conditioned dynamics for imagination and planning (Ha & Schmidhuber, 2018; Hafner et al., 2019), achieving scalable gains from counterfactual rollouts (Hafner et al., 2023; Bruce et al., 2024; Agarwal et al., 2025; Janner et al., 2022; Wan et al., 2025). Many recent systems emphasize generative fidelity and long-horizon prediction in video or latent space (Bruce et al., 2024; Agarwal et al., 2025; Finn & Levine, 2017; Ebert et al., 2018). Complementary benchmarks evaluate control and prediction quality in visually rich settings (Tian et al., 2023; Chi et al., 2024; Yue et al., 2025), or study physical and scene understanding from non-interactive visual data (Bakhtin et al., 2019; Yi et al., 2019; Girdhar et al., 2020; Bear et al., 2021; Tung et al., 2023; Li et al., 2024a; Dang et al., 2025; Yang et al., 2025). Others focus on sequence-level temporal coherence and ordering (Qin et al., 2024; Chen et al., 2025). Aurora-Bench (Qiu et al., 2025) focuses on short-horizon and general-purpose video forward and inverse world modeling. As argued by Xing et al. (2025), world model should serve as sandbox for reasoning. Our benchmark is therefore designed to study forward and inverse ordering with an explicitly defined action space and scalable construction. VLMs in Embodied AI. VLMs are central to embodied agents, acting as high-level planners (Huang et al., 2022a; Ahn et al., 2022; Huang et al., 2023b, 2022b; Liang et al., 2022a; Huang et al., 2023a, 2024; Jiang et al., 2024; Wang et al., 2025a) or end-to-end policies (Zitkovich et al., 2023; Kim et al., 2024; Team et al., 2024; Driess et al., 2023). However, current applications are often confined to settings where involved reasoning is not required (Lynch et al., 2023). Correspondingly, many benchmarks emphasize instruction-following and goal-conditioned control (Das et al., 2018; Padmakumar et al., 2022; Mees et al., 2022; Fan et al., 2022; Sermanet et al., 2024; Li et al., 2024b; Yang et al., 2025; Gao et al., 2025), with less focus on the multistep, consequence-aware reasoning essential for long-horizon interaction. We address this gap with benchmark that uses egocentric interaction to evaluate an agents forward and inverse world modeling, requiring long-range interactive reasoning. GVL (Ma et al., 2024) casts value estimation as reordering task over observation sequences, whereas ENACT focuses on transition modeling over interleaved observation-action sequences with an explicitly defined scene graph action space that applies to interactions of any quality. 12 5. Conclusions and Limitations Conclusions. In this work, we introduced ENACT, benchmark designed to evaluate the extent to which embodied cognition emerges in VLMs trained on passive datasets. By framing evaluation as forward and inverse world modeling from egocentric interaction, ENACT assesses models understanding of environmental dynamics and the consequences of its actions. Grounded in POMDP, we cast this as two sequence-reordering tasks: forward world modeling, which predicts an ordered sequence of future states from actions, and inverse world modeling, which infers an ordered sequence of actions from state changes. Our extensive experiments reveal significant performance gap between state-of-the-art VLMs and humans, gap that widens dramatically as the interaction horizon increases. We consistently found that models solve the inverse problem more effectively than the forward one. Furthermore, our analysis uncovered strong embodied biases within these models, including preference for right-handed actions and significant performance drop with non-human-like camera perspectives. An in-depth error analysis showed that reasoning failures are primarily driven by the omission and hallucination of state changes. ENACT provides scalable and insightful tool for charting course toward more genuinely embodied artificial intelligence. Limitations. Our work has limitations primarily related to its scope. First, while we introduce several diagnostic tasks that reveal key model biases, this set is not exhaustive. The experiments on factors like camera configuration and agent appearance serve as foundational examples, but the ENACT framework is designed to be an extensible tool. It can support future, more complex investigations into much broader spectrum of different embodied-related settings. Second, due to the significant computational cost of evaluation on VLMs, the in-depth ablation experiments were necessarily focused on representative subset of models and data. broader evaluation across more architectures and larger data scales would be beneficial to generalize our findings. Furthermore, we do not explore finetuning VLM in this work, but we expect our automatic and scalable dataset can also bring benefits to improving VLMs embodied world modeling abilities. Additionally, due to the frequent physical inconsistency of generated rollouts and the difficulty of designing fair evaluation metrics, which often require costly human studies, we do not evaluate video generative models on ENACT (for unified VLMs, we evaluate BAGEL (Deng et al., 2025) and the result is shown in Table 9 and 10)."
        },
        {
            "title": "Ethics Statement",
            "content": "The ENACT benchmark was generated in the BEHAVIOR simulator to avoid the privacy risks associated with real-world human data; it contains no human subjects or personally identifiable information. All human annotators hired for evaluation were compensated at rates significantly exceeding their local minimum wage and were not exposed to any sensitive content. We acknowledge that the simulator may not fully capture the complexity of real-world environments, which can introduce biases and limit the generalizability of our findings. Furthermore, the large-scale models we evaluate carry significant computational and environmental cost. While ENACT is intended for academic research, we recognize that the technologies it helps develop could have dual-use applications."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To ensure full reproducibility, our complete codebase is available at our Github Repository. This repository contains all scripts for data generation using the BEHAVIOR simulator (Li et al., 2024a), evaluation of all Vision-Language Models, and analysis. Our implementation includes the automated verifier, prompt templates, and the code to replicate our main experiments, controlled ablation studies (Sections 3.2, 3.3, and 3.4), and human baseline evaluation. The full ENACT dataset is also publicly available."
        },
        {
            "title": "References",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Anthropic. Claude sonnet 4 model card. https://www.anthropic.com/claude/sonnet, May 2025. Release date: May 22, 2025. Karl Johan 칀str칬m. Optimal control of markov processes with incomplete state information i. Journal of mathematical analysis and applications, 10:174205, 1965. Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: new benchmark for physical reasoning. Advances in Neural Information Processing Systems, 32, 2019. Lawrence Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):577660, 1999. Lawrence Barsalou. Challenges and opportunities for grounding cognition. Journal of Cognition, 3(1):31, 2020. Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021. Rodney Brooks. Intelligence without representation. Artificial intelligence, 47(1-3):139159, 1991. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 14 Delong Chen, Willy Chung, Yejin Bang, Ziwei Ji, and Pascale Fung. Worldprediction: benchmark for high-level world modeling and long-horizon procedural planning. arXiv preprint arXiv:2506.04363, 2025. Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, et al. Eva: An embodied world model for future video anticipation. arXiv preprint arXiv:2410.15461, 2024. Andy Clark. Being there: Putting brain, body, and world together again. MIT press, 1998. Andy Clark. Language, embodiment, and the cognitive niche. Trends in cognitive sciences, 10(8): 370374, 2006. Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long Li, Liuyi Wang, Qinyang Zeng, Xin Li, and Lidong Bing. Ecbench: Can multi-modal foundation models understand the egocentric world? holistic embodied cognition benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2459324602, 2025. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 110, 2018. Knut De Swert. Calculating inter-coder reliability in media content analysis using krippendorffs alpha. Center for Politics and Communication, 15(1-15):3, 2012. Google DeepMind. Gemini 2.5 pro model card. https://storage.googleapis.com/model -cards/documents/gemini-2.5-pro.pdf, June 2025. Updated: June 27, 2025; released to General Availability on June 17, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362, 2022. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 27862793. IEEE, 2017. Andrea Frick and Wenke M칬hring. matter of balance: Motor control is related to childrens spatial and proportional reasoning skills. Frontiers in Psychology, 6:2049, 2016. Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, et al. Do vision-language models have internal world models? towards an atomic evaluation. arXiv preprint arXiv:2506.21876, 2025. 15 Gioele Gavazzi, Ambra Bisio, and Thierry Pozzo. Time perception of visual motion is tuned by the motor representation of human actions. Scientific reports, 3(1):1168, 2013. James Gibson. The ecological approach to visual perception: classic edition. Psychology press, 2014. Rohit Girdhar, Deva Ramanan, and Abhinav Gupta. Cater: diagnostic dataset for compositional actions and temporal reasoning. In International Conference on Learning Representations, 2020. David Ha and J칲rgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pp. 25552565. PMLR, 2019. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Andrew Hayes and Klaus Krippendorff. Answering the call for standard reliability measure for coding data. Communication methods and measures, 1(1):7789, 2007. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pp. arXiv2507, 2025. Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. arXiv preprint arXiv:2305.11176, 2023a. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pp. 91189147. PMLR, 2022a. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023b. Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022. Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, and Yunzhu Li. Roboexp: Action-conditioned scene graph via interactive exploration for robotic manipulation. arXiv preprint arXiv:2402.15487, 2024. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Leslie Pack Kaelbling, Michael Littman, and Anthony Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134, 1998. James Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, pp. 143150, 1986. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Klaus Krippendorff. Content analysis: An introduction to its methodology. (No Title), 1999. Klaus Krippendorff. Computing krippendorffs alpha-reliability. 2011. George Lakoff and Mark Johnson. Metaphors we live by. University of Chicago press, 2008. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart칤nMart칤n, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, and Li Fei-Fei. Behavior-1k: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. arXiv preprint arXiv:2403.09227, 2024a. Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37: 100428100534, 2024b. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022a. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022b. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, et al. Ovis2. 5 technical report. arXiv preprint arXiv:2508.11737, 2025. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. Yecheng Jason Ma, Joey Hejna, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, et al. Vision language models are in-context value learners. In The Thirteenth International Conference on Learning Representations, 2024. Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. 17 MetaAI. Llama 4 model card, April 2025. URL https://ai.meta.com/blog/llama-4-mul timodal-intelligence/. NVIDIA. Nvidia rtx ray tracing, 2021. URL https://developer.nvidia.com/rtx. Accessed: 2025-09-17. NVIDIA. Isaac Sim, 2025. URL https://github.com/isaac-sim/IsaacSim. Version 5.0.0, Apache-2.0 License. OpenAI. Gpt-5 system card. https://openai.com/index/gpt-5-system-card/, August 2025. Accessed: 2025-09-16. OpenAI. Introducing our latest image generation model in the api. OpenAI Blog, 2025. URL ht tps://openai.com/index/image-generation-api/. gpt-image-1 model capabilities and API release. Kevin Oregan and Alva No칢. sensorimotor account of vision and visual consciousness. Behavioral and brain sciences, 24(5):939973, 2001. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali NarayanChen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 20172025, 2022. Marietta Papadatou-Pastou, Eleni Ntolka, Judith Schmitz, Maryanne Martin, Marcus Munaf, Sebastian Ocklenburg, and Silvia Paracchini. Human handedness: meta-analysis. Psychological bulletin, 146(6):481, 2020. Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay Cohen, and Edoardo Ponti. Bootstrapping world models from dynamics models in multimodal foundation models. arXiv preprint arXiv:2506.06006, 2025. Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? arXiv preprint arXiv:2410.06468, 2024. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 645652. IEEE, 2024. Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):1329, 2005. Richard Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181211, 1999. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram칠, Morgane Rivi칟re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. 18 Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.g ithub.io/blog/qvq-72b-preview/. Evan Thompson. Sensorimotor subjectivity and the enactive approach to experience. Phenomenology and the cognitive sciences, 4(4):407427, 2005. Stephen Tian, Chelsea Finn, and Jiajun Wu. control-centric benchmark for video prediction. arXiv preprint arXiv:2304.13723, 2023. Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan Yamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Advances in Neural Information Processing Systems, 36:6704867068, 2023. Francisco Varela, Evan Thompson, and Eleanor Rosch. The embodied mind, revised edition: Cognitive science and human experience. MIT press, 2017. Chi Wan, Kangrui Wang, Yuan Si, Pingyue Zhang, Huang Huang, and Manling Li. Worldagen: Unified state-action prediction with test-time world model training. In NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning, 2025. URL https: //openreview.net/forum?id=egbFo1gvYp. Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, et al. Vagen: Reinforcing world model reasoning for multi-turn vlm agents. arXiv preprint arXiv:2510.16907, 2025a. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025b. Brad Wyble, Howard Bowman, and Mark Nieuwenstein. The attentional blink provides episodic distinctiveness: sparing at cost. Journal of experimental psychology: Human perception and performance, 35(3):787, 2009. Eric Xing, Mingkai Deng, Jinyu Hou, and Zhiting Hu. Critiques of world models. arXiv preprint arXiv:2507.05169, 2025. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, and Guanghui Ren. Ewmbench: Evaluating scene, motion, and semantic quality in embodied world models. arXiv preprint arXiv:2505.09694, 2025. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023."
        },
        {
            "title": "Table of Contents",
            "content": "A ENACT: Egocentric Interactive Embodied Cognition Test 22 A.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.2 Key-Frame Trajectories Synthesis for Scalable Data Generation . . . . . . . . . . 22 A.3 Dataset Statistics and Evaluation Design . . . . . . . . . . . . . . . . . . . . . . . 26 Experiments and Analysis B.1 Human Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.2 World Modeling as Proxy for Evaluating Embodied Cognition . . . . . . . . . B.3 Additional Ablation Experiments Common Setup . . . . . . . . . . . . . . . . . 38 B.4 Sensitivity to Image Realism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 B.5 Sensitivity to Camera Configurations . . . . . . . . . . . . . . . . . . . . . . . . . 44 B.6 Do VLMs Have Embodied Biases? . . . . . . . . . . . . . . . . . . . . . . . . . . 45 Error Analysis C.1 Methodology for Error Calculations . . . . . . . . . . . . . . . . . . . . . . . . . 47 C.2 Structural Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 C.3 Semantic Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 The Use of Large Language Models 60 21 A. ENACT: Egocentric Interactive Embodied Cognition Test A.1. Notations We list all the notations we used across the entire paper in the following two tables. Notation Short description Notation Short description 洧녢 [洧] 洧녶洧노 洧녠 Adj(洧녰) 洧(, ) 풊vis (, ) 洧녩洧녰 洧랢 洧냢洧랢 洧냥洧녞[, 洧녰] 1{} 洧녫 洧 # frames in raw replay index set {1, 2, . . . , 洧} segmented timestamps {洧노1 < < 洧노洧 } RGB image at time 洧노 space of scene graphs # sampled trajectories successors of node 洧녰 scene-graph difference (see long) visible-change extractor (see long) local action 洧멇롐+1 洧멇롐 key-frame trajectory (see long) action sequence along 洧랢 # paths of length  ending at 洧녰 predecessor set in backtracking Iverson bracket (true=1, false=0) component in signature 洧녩洧멇롐뒳롐 The entity involved in component 洧녰 image height and width # segmented key frames RGB & scene graph at timestamp 洧노洧녰 scene graph at time 洧노 target trajectory length (steps) adjacency matrix on [洧] (DAG) # edges in the DAG visibility predicate (see long) action space action from 洧녰 to 洧녱 state sequence along 洧랢 set of sampled trajectories end-node weight 洧냥洧녞 [洧, 洧녰] 洧냩, 洧녥 洧 (洧녶洧녰, 洧멇롐) 洧멇롐 洧 洧냦 洧냦 Vis() 洧녩洧녰 洧녱 洧녡洧랢  洧녻洧녰 Categorical(洧녻) weighted discrete distribution 洧, transition 洧랣 datasets The operation key in component The predicate in component Notation = {(洧녶洧노, 洧멇롐)}洧녢 洧(洧멇롐, 洧 洧녱) 洧노=1 Vis(洧(洧멇롐, 洧 洧녱)) 풊vis (洧멇롐, 洧 洧녱) {} 洧랢 = (洧녰1, . . . , 洧녰洧) 洧녡洧랢, 洧냢洧랢 洧냥洧녞[, 洧녰] recurrence M洧랢 = {洧멇롐뒥 }, {洧녩洧녰洧녰+1 洧녩洧멇롐뒳롐 洧녰 Longer description Raw replay trajectory with RGB observations 洧녶洧노 R洧냩洧녥 3 and scene graphs 洧멇롐 G. difference operator over scene graphs summarizing semantic changes (objects, relations, attributes) between frames 洧녰 and 洧녱. Predicate returning 1 iff the semantic difference is visually verifiable; induces an edge 洧녰 洧녱 when 洧녰 < 洧녱 and the predicate is true (frame skipping allowed). Action-level representation extracted from 洧(洧멇롐, 洧 洧녱); may be atomic or composite and can be empty when no visible semantic change exists. Key-frame trajectory: strictly increasing indices with valid edges 洧냦洧녰,洧녰+1 1, . . . , 洧 1. Sequences induced by 洧랢: 洧녡洧랢 = (洧멇롐 1 , . . . , 洧멇롐뒳롏 ), Base: 洧냥洧녞 [1, 洧녰] = 1. Recurrence: 洧냥洧녞 [, 洧녰] = (cid:205) 洧녱<洧녰 洧냥洧녞 [ 1, 洧녱] 洧냦 洧녱洧녰 for  = 2, . . . , 洧. 2 , . . . , 洧녩洧녰洧1洧녰洧 ). = 1 for all  = 洧냢洧랢 = (洧녩洧녰 1洧녰 }, 洧녞 Deterministic finite-horizon fragment induced by 洧랢 with transition 洧녞(洧멇롐뒥 , 洧녩洧녰洧녰+1 ) = 洧멇롐뒥+1 . Signature corresponding to an action 洧녩洧녰, transformed from natural language to predicatebased structural format. Table 5 Notation used throughout the paper. A.2. Key-Frame Trajectories Synthesis for Scalable Data Generation A.2.1. Segmented Frames with Abstract State Changes We provide examples of scene graph and our scene graph differences for two adjacent segmented frames, shown in Figure 7 and 8. Our frame selection process is iterative. For previously selected key-frame at time 洧노洧녰1, we search for the earliest subsequent frame 洧노洧녲 that satisfies set of criteria designed to ensure 22 semantic significance and visual clarity. First, to handle discrepancies where the rule-based simulator updates the scene graph before change is visually apparent (e.g., registering an object as OnTop upon initial contact), we introduce temporal stability filter: state change is only considered candidate if the resulting new state persists for at least 40 frames. At our simulators 30Hz rate, this corresponds to 1.3s, which is consistent with cognitive science findings that humans update attentional sub-events on the order of 1s (Wyble et al., 2009; Gavazzi et al., 2013), and empirically yields reliable keyframe segmentation for our home-scale manipulation tasks. This value is tunable hyperparameter rather than hard constraint. It can be adjusted for other environments or model classes within the same automated ENACT pipeline. Second, to prevent the recording of minor, oscillatory state changes, such as those that might occur from vibrations when robot carries an object (e.g., plate with pizza), we employ filtering algorithm to suppress these small fluctuations in the scene graph. Finally, to ensure that each selected key-frame represents sufficiently distinct change from the previous one, we implement similarity check. We convert the scene graph difference between the last selected frame 洧노洧녰1 and candidate frame 洧노洧녲 into one-hot vector, which serves as unique signature for that state change. We then compute the cosine similarity between the signature of the change at 洧노洧녲 and the signature of the previously accepted change at 洧노洧녰1. We aim to find balance between maximizing the number of segmented frames and ensuring each frame depicts clearly visible state change. Through empirical evaluation, we determined cosine similarity threshold of 0.97. candidate frame 洧노洧녲 is accepted only if its change signatures similarity to the previous one is below this threshold. This method effectively filters out nearduplicate frames while retaining rich, sequential set of key-frames that clearly chronicle the tasks progression. A.2.2. Key-Frame Trajectories Synthesis Given the set of 洧 segmented frames from the previous stage, the goal of Key-Frame Trajectory Synthesis (KFTS, see Algorithm 1) is to efficiently sample large number of valid trajectories of fixed length 洧. trajectory is defined as sequence of indices 洧랢 = (洧녰1, . . . , 洧녰洧) such that 1 洧녰1 < 洧녰2 < < 洧녰洧 洧. The key constraint is that for any two consecutive frames 洧녰洧녲 and 洧녰洧녲+1 in the trajectory, the state change between them must be semantically meaningful and visually verifiable. The KFTS algorithm, detailed in Algorithm 1, accomplishes this efficiently by converting the problem into path sampling on Directed Acyclic Graph (DAG) and using dynamic programming. The process consists of three main stages: 1. Directed Acyclic Graph (DAG) Construction: We first model the relationships between all segmented frames. The 洧 frames are treated as nodes in graph. directed edge exists from frame 洧녰 to frame 洧녱 (where 洧녰 < 洧녱) if and only if the state difference 洧(洧멇롐, 洧 洧녱) constitutes valid, visible transition. This validity is determined by predicate Vis(), which checks if the objects involved in the state change are clearly visible in both frames, as described in Section 2.2. This process results in an adjacency matrix 洧냦 for DAG, where 洧냦洧녰 洧녱 = 1 indicates valid one-step transition from frame 洧녰 to 洧녱. 2. Dynamic Programming Path Counting: Instead of enumerating all possible (cid:0)洧 (cid:1) combinations, we use dynamic programming (DP) to efficiently count the number of valid trajectories. We build DP table where 洧냥洧녞 [, 洧녰] stores the total number of valid trajectories of length  that terminate at frame 洧녰. The base case is 洧냥洧녞 [1, 洧녰] = 1 for all frames 洧녰, as any 洧"
        },
        {
            "title": "A Scene Graph Example",
            "content": "{ nodes: [ {name: robot_r1, category: agent, states: []}, {name: plate_94, category: plate, states: []}, {name: plate_93, category: plate, states: []}, {name: bowl_92, category: bowl, states: []}, {name: bowl_91, category: bowl, states: []}, {name: pizza_90, category: pizza, states: []}, {name: pizza_89, category: pizza, states: []}, {name: floors_zqjkvm_0, category: floors, states: []}, {name: breakfast_table_xftrki_0, category: breakfast_table, states: []}, {name: fridge_petcxr_0, category: fridge, states: [Open]}, {name: drop_in_sink_lkklqs_0, category: drop_in_sink, states: []}, {name: straight_chair_uofiqj_0, category: straight_chair, states: []}, {name: bottom_cabinet_rhdbzv_0, category: bottom_cabinet, states: []} ], Edges: [ {from: robot_r1, to: plate_93, states: [RightGrasping]}, {from: plate_94, to: pizza_90, states: [Under]}, {from: plate_94, to: breakfast_table_xftrki_0, states: [OnTop]}, {from: bowl_92, to: breakfast_table_xftrki_0, states: [OnTop]}, {from: bowl_91, to: breakfast_table_xftrki_0, states: [OnTop]}, {from: pizza_90, to: plate_94, states: [OnTop]}, {from: pizza_89, to: plate_93, states: [OnTop]}, {from: breakfast_table_xftrki_0, to: plate_94, states: [Under]}, {from: breakfast_table_xftrki_0, to: floors_zqjkvm_0, states: [OnTop]}, {from: straight_chair_uofiqj_0, to: floors_zqjkvm_0, states: [OnTop]} ] } Figure 7 scene graph representation detailing the entities (nodes) and their semantic or physical connections (edges) within the BEHAVIOR (Li et al., 2024a) environment."
        },
        {
            "title": "A Scene Graph Difference Example",
            "content": "2442: { type: diff, add: { nodes: [], edges: [ {from: robot_r1, to: plate_93, states: [RightGrasping]} ] }, remove: { nodes: [], edges: [ {from: plate_93, to: pizza_89, states: [Under]}, {from: plate_93, to: breakfast_table_xftrki_0, states: [OnTop]} ] } } Figure 8 An example of scene graph difference, representing state change by specifying the added and removed edges between objects. 24 Algorithm 1: KFTS: Key-Frame Trajectory Sampling Input: Segmented frames {(洧녶洧녰, 洧멇롐)}洧 Output: Set of key-frame trajectories  Build DAG: for 1 洧녰 < 洧녱 洧 do 洧냦洧녰 洧녱 [ Vis(洧(洧멇롐, 洧 洧녱)) ] 洧녰=1, step length 洧 2, samples 洧녠, predicate Vis DP counting: initialize 洧냥洧녞 [1, 洧녰] 1; for  = 2..洧 do for 洧녰 = 1..洧 do 洧냥洧녞 [, 洧녰] (cid:205) 洧녱<洧녰 洧냥洧녞 [ 1, 洧녱] 洧냦 洧녱洧녰 Weights: 洧녻洧녰 洧냥洧녞 [洧, 洧녰]; if (cid:205)洧녰 洧녻洧녰 = 0 then return Weighted backtracking sampling:  ; sample 洧녠 end-nodes 洧녰(洧) for 洧 = 1..洧녠 do 洧랢 [ 洧녰(洧) 洧 for  = 洧..2 do ], 洧녫洧녹洧 洧녰(洧) 洧 洧 Categorical(洧녻) { 洧녱 < 洧녫洧녹洧 洧냦 洧녱,洧녫洧녹洧 = 1 洧냥洧녞 [ 1, 洧녱] > 0 } if = then break sample 洧녱 with prob 洧냥洧녞 [ 1, 洧녱]; prepend 洧녱 to 洧랢; 洧녫洧녹洧 洧녱 if 洧랢 = 洧 then add 洧랢 to  return  single frame is valid path of length one. The table is filled using the recurrence: 洧냥洧녞 [, 洧녰] = 洧녱<洧녰 洧냥洧녞 [ 1, 洧녱] 洧냦 洧녱洧녰 This equation sums the number of valid paths of length  1 ending at any valid predecessor 洧녱 of frame 洧녰. After filling the table up to length 洧, the entry 洧냥洧녞 [洧, 洧녰] gives the exact number of distinct, valid, length-洧 trajectories that end at frame 洧녰. 3. Weighted Backtracking Sampling: With the DP table computed, we can sample trajectories efficiently without bias. To generate one trajectory, we first sample an end-node 洧녰洧 from all possible frames {1, . . . , 洧}. The sampling is weighted, with the probability of selecting frame 洧녰 being proportional to its weight 洧녻洧녰 = 洧냥洧녞 [洧, 洧녰]. This ensures that frames that can be part of more trajectories are more likely to be chosen as endpoints. Once the end-node 洧녰洧 is selected, we reconstruct the path backwards. To select the previous node 洧녰洧1, we consider all valid predecessors 洧녱 of 洧녰洧 (i.e., all 洧녱 < 洧녰洧 where 洧냦 洧녱,洧녰洧 = 1). We sample the predecessor 洧녱 with probability proportional to 洧냥洧녞 [洧 1, 洧녱]. This process is repeated iteratively: to find node 洧녰洧녲, we sample from the predecessors of 洧녰洧녲+1 with probabilities proportional to the values in the 洧냥洧녞 [洧녲, ] row. This weighted backtracking ensures that every valid trajectory of length 洧 has chance of being sampled, and the likelihood of sampling any specific path is uniform across all valid paths. We repeat this procedure 洧녠 times to generate the desired number of trajectories. This DP-based approach is highly scalable as its complexity is polynomial in 洧 and 洧, making it far more efficient than brute-force combinatorial search, especially when 洧 is large. Motivation of Scene Graph State-Action Spaces. We opt for symbolic scene graph representation of state and action, and while it may not capture the fine-grained details of low-level 25 dynamics, this abstraction is advantageous for our objectives for two primary reasons. First, our focus is on detecting semantic changes within scene, task that naturally aligns with the semantic abstraction of VLMs rather than the continuous motor trajectories. Second, the symbolic predicates we employ are grounded in practical robotics applications. This is demonstrated through our egocentric real-world experiments, confirming the real-world relevance of our chosen states. The feasibility of this symbolic approach is further substantiated by its use in guiding the data collection for the BEHAVIOR benchmark, where these predicates defined the goal conditions for simulated activities and enabled human annotators to clearly verify whether task states were successfully achieved. A.3. Dataset Statistics and Evaluation Design Table 6 The 11 predicate classes used to define abstract state changes in our benchmark. Predicate Classes"
        },
        {
            "title": "Contains\nToggledOn",
            "content": "Figure 9 The distribution of problems by the number of steps in our ENACT benchmark dataset is shown for both forward (left) and inverse (right) world modeling tasks. The dataset is balanced, with nearly uniform distribution of problems ranging from 3 to 10 steps. A.3.1. Dataset Statistics To ensure comprehensive evaluation of models reasoning capabilities across different time horizons, the sampled problems feature trajectory lengths varying from 3 to 10 steps. As illustrated in Figure 9, this dataset is intentionally balanced, featuring near-uniform distribution of problems for each step length across both task types. This balance ensures that our evaluation is not biased towards shorter or longer-term reasoning. 26 The abstract state changes that define the actions in our benchmark are grounded in set of 11 symbolic predicates. These predicates describe relationships between the agent and objects, as well as changes in object states. The complete list of predicates is detailed in Table 6. A.3.2. Evaluation Design From indices to dynamics. We grade what changes, not just which index. Each adjacent state pair yields an action signature 洧녩sig(洧멇롐1, 洧멇롐) = {洧녫 = (洧, 洧, 洧랣)}, turning scene-graph deltas into compact semantics (operation 洧 on entity 洧 and predicate 洧랣). For the reference sequence, we compute (i) the visible subset 洧냤洧녰 and (ii) the full set 洧냧洧녰. For prediction, we compute 洧냤洧녰 (full diff). This uses state differences as the models proxy answer and avoids brittle numeric matching. Online verifier. Forward dynamics. After reconstructing the shuffled storyboard, we compare the ground-truth index sequence 洧랦 and the prediction 洧랥. Exact acceptance: 洧랥 = 洧랦. Semantic acceptance (when lengths match): for all steps 洧녰, 洧냤洧녰 洧냤洧녰. Intuition: the predicted step must cover the references visible change. The overall decision is match = (exact OR semantic); length-mismatched predictions are not accepted (but still get pairwise credit below). Inverse dynamics. The model orders actions. Exact acceptance: indices match. Semantic acceptance (equal length): for all 洧녰, 洧냤洧녰 洧냧洧녰, i.e., the predicted action description can be concise subset of the full reference transition at that position. Again, match = (exact OR semantic). Metrics. Task accuracy (TA). Score 1 iff the verifier accepts the full prediction, else 0; average over the split: TA = 1 洧논 1{accepted(洧논)}. Pairwise accuracy (PA). Measures stepwise consistency. If lengths match, PA(洧논) = 1 洧 洧 洧녰=1 1{ 洧냤洧녰 洧냤洧녰 (forward) or 洧냤洧녰 洧냧洧녰 (inverse) }. Accepted predictions have PA(洧논) = 1. If lengths differ, we compute PA via monotone alignment between reference and predicted steps that maximizes the number of subset-satisfying pairs (forward/inverse rule as above). We report the micro-average: PA = (cid:205)洧논 #correct pairs in 洧논 (cid:205)洧논 洧洧논 . 27 Summary. Multiple valid answers are allowed via the subset rules: forward requires reference-visible predicted, inverse requires predicted reference-full. TA captures all-ornothing acceptance; PA gives graded credit for near-correct dynamics. A.3.3. ENACT Examples Figure 10 3-Step Forward World Modeling (left) and Inverse World Modeling (right) samples. B. Experiments and Analysis B.1. Human Annotation B.1.1. Annotation Interface & Human Performance Evaluation To establish an empirical upper bound on performance for the ENACT benchmark, we recruited three trained annotators to complete the same set of tasks assigned to the Vision-Language Models (VLMs). The annotators interacted with our customized human annotation interfaces implemented in Gradio, which are illustrated in Figure 13 for the Forward World Modeling tasks and in Figure 14 for the Inverse World Modeling tasks. Importantly, annotators followed exactly the same instructions and task prompts as those provided to the VLMs, ensuring fair and consistent comparison between human and model performance. This setup allows us to quantify the extent to which current VLMs approach human-level competence on the ENACT benchmark. B.1.2. Inter-Annotator Agreement Analysis To ensure the reliability of our human-generated labels, we conducted rigorous Inter-Annotator Agreement (IAA) analysis. The initial dataset was annotated by three annotators working on disjoint, non-overlapping subsets, which precluded direct agreement measurement. We therefore implemented systematic cross-annotation protocol. For each task type (forward and inverse) and for each of the eight step-length categories (from 3 to 10 steps), we randomly Figure 11 6-Step Forward World Modeling (left) and Inverse World Modeling (right) samples. 29 Figure 12 9-Step Forward World Modeling (left) and Inverse World Modeling (right) samples. 30 Figure 13 The annotation interface used for evaluating human performance on Forward World Modeling problems. Annotators are presented with Current State image (top left) and an ordered list of textual actions. The main task is to fill the Next State slots by selecting the correct image from the shuffled Candidate Image Library on the right. The annotator must follow the sequence of actions, using the result of the previous action as the starting point for the next, to determine the correct chronological order of all future states. Figure 14 The annotation interface used for evaluating human performance on Inverse World Modeling problems. Annotators are shown an ordered sequence of state transitions, displayed as pairs of Current State and Next State images. For each transition, their task is to select the correct action description from shuffled Candidate Action Library that caused the visual change between the two states. 32 sampled five questions from each annotators original assignment. This process created balanced IAA evaluation set totaling 240 unique questions. Each of these sampled questions was then reassigned to the two annotators who had not performed the original annotation. For example, the five items sampled from Annotator As work for given category were re-annotated independently by Annotator and Annotator C. Following this protocol, we assessed the resulting annotations using Krippendorffs Alpha (洧띺) (Krippendorff, 2011), robust statistical measure that is well-suited for this analysis as it accommodates multiple annotators and is resilient to missing data. Given that our annotation task involved ordering, we configured the analysis for ordinal data. The alpha coefficient is calculated based on the observed and expected disagreement among annotators, according to the formula: 洧띺 = 1 洧냥洧녶 洧냥洧 Here, 洧냥洧녶 represents the observed disagreement, which is calculated from the pairwise differences between all annotations for each item. 洧냥洧 is the expected disagreement, which represents the disagreement that would occur by chance, derived from the marginal distribution of all annotations. This balanced design ensured that every question in the IAA set received three independent labels, allowing for robust pairwise agreement calculation across all three pairs of annotators (A vs. B, vs. C, and vs. C) for each condition. To assess the stability of our 洧띺 coefficient, we computed 95% confidence intervals (CI) using the bootstrap percentile method with 1,000 resamples of the 240 evaluation items. Our analysis yielded an overall Krippendorffs Alpha of 洧띺 = 0.8320, with 95% bootstrap confidence interval of [0.7879, 0.8682]. Given the established standard (Krippendorff, 1999; Hayes & Krippendorff, 2007; De Swert, 2012), an alpha value above 0.80 indicates high level of reliability. This strong result confirms that our annotation guidelines are clear and consistently applied by the annotators. Pairwise agreement scores were also consistently high, further validating the reliability between individual annotators: Annotator A01 - B02: 洧띺 = 0.8180 Annotator A01 - C03: 洧띺 = 0. Annotator B02 - C03: 洧띺 = 0.8518 In addition to the chance-corrected alpha metric, we found that the annotators were in perfect agreement on 184 of the 240 selected questions, resulting in agreement rate of 76.67%. Collectively, these strong agreement metrics validate the reliability of our annotation process and the high quality of the resulting dataset. B.2. World Modeling as Proxy for Evaluating Embodied Cognition B.2.1. Experimental Setup To ensure fair and consistent comparison across all models, we employed standardized evaluation protocol. For each task type (forward and inverse world modeling), unified question Task Forward Inverse ActionEffect Reasoning Causal Inference Affordance Recognition Embodied Awareness Temporal Abstraction Table 7 We posit that ENACT reflects embodied world modeling reasoning rather than simple temporal correlation. Unlike passive video prediction, our formulation explicitly conditions state transitions on actions (Forward World Modeling) and infers actions from state changes (Inverse World Modeling), thereby evaluating VLMs as transition models. Based on your suggestion, we include table that relates ENACTs evaluation tasks to specific cognitive constructs. prompt template was used. All input images were resized to uniform resolution of 512 512 pixels before being passed to the models. To ensure deterministic and reproducible outputs, the decoding temperature for all models was set to 0. Models were instructed to return their answers as parsable Python list representing the permutation of indices, as shown in Figure 15 and Figure 16. comprehensive list of the specific models used in our evaluation is provided in Table 8. We deliberately choose one prompt template across all experiments because we follow the design choice mentioned in Liang et al. (2022b), that the models should adapt to users input, instead of the reverse case. Organization Model Name Release Date Full Name Evaluation Pipeline OpenAI Google GPT-5 GPT-5-mini GPT-5-nano Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.5 Flash-Lite Anthropic Claude Sonnet 4 Zhipu AI Meta Shanghai AI Lab GLM-4.5V GLM-4.1V-Thinking Llama-4-Scout-17B-16E-Ins Llama-4-Mav-17B-128E-Ins InternVL3.5-241B-A28B InternVL3.5-14B InternVL3.5-8B InternVL3.5-4B Google Alibaba AIDC OpenBMB Gemma-3-27b-it Gemma-3-12b-it Gemma-3-4b-it QVQ-72B-Preview Qwen2.5-VL-72B-Ins Qwen2.5-VL-32B-Ins Qwen2.5-VL-7B-Ins Qwen2.5-VL-3B-Ins Ovis2.5-9B Ovis2.5-2B MiniCPM-V-4.5 MiniCPM-o-2.6 Hugging Face Idefics3-8B-Llama Nvidia Cosmos-Reason1 2025-08 2025-08 2025-08 2025-06 2025-06 2025-06 2025-05 2025-08 20252025-04 2025-04 2025-08 2025-08 2025-08 2025-08 2025-03 2025-03 2025-03 2024-12 2025-01 2025-01 2025-01 2025-01 2025-08 2025-08 2025-08 20252024-08 2025-05 Proprietary Models gpt-5-2025-08-07 gpt-5-mini-2025-08-07 gpt-5-nano-2025-08-07 gemini-2.5-pro gemini-2.5-flash gemini-2.5-flash-lite claude-sonnet-4Open-Weight Models OpenAI API OpenAI API OpenAI API Gemini API Gemini API Gemini API Anthropic API GLM-4.5V GLM-4.1V-Thinking-FlashX Zhipu Foundation Model Open Platform API Zhipu Foundation Model Open Platform API meta-llama/Llama-4-Scout-17B-16E-Instruct meta-llama/Llama-4-Mav-17B-128E-Instruct OpenGVLab/InternVL3.5-241B-A28B OpenGVLab/InternVL3.5-14B OpenGVLab/InternVL3.5-8B OpenGVLab/InternVL3.5-4B google/gemma-3-27b-it google/gemma-3-12b-it google/gemma-3-4b-it Qwen/QVQ-72B-Preview Qwen/Qwen2.5-VL-72B-Instruct Qwen/Qwen2.5-VL-32B-Instruct Qwen/Qwen2.5-VL-7B-Instruct Qwen/Qwen2.5-VL-3B-Instruct AIDC-AI/Ovis2.5-9B AIDC-AI/Ovis2.5-2B openbmb/MiniCPM-V-4.5 openbmb/MiniCPM-o-2. HuggingFaceM4/Idefics3-8B-Llama3 nvidia/Cosmos-Reason1 ModelScope API ModelScope API Intern API Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Gemini API Gemini API Gemini API ModelScope API ModelScope API ModelScope API Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Hugging Face Transformers Table 8 Details of Vision Language Models (VLMs) assessed in this study. Forward World Modeling Prompt You are capable agent designed to infer multi-step forward dynamics transitions in embodied decision-making. Your goal is to predict the correct sequence of future states that result from applying given series of actions to an initial state. ## Your Task You will be provided with single **Current State Image** and set of shuffled **Future State Images** (labeled 1, 2, 3, etc.). To determine their correct order, 34 you must follow the sequence of actions provided below. 1. 2. 3. 4. 5. 6. Start with the **Current State Image**. Apply the **first action** from the Actions in Order list to this state. Find the **Future State Image** that matches the outcome of this action. This is the first state in the correct sequence. Next, apply the **second action** to the state you just identified. Find the corresponding image among the remaining future states. Continue this process until all actions have been applied and all future states have been ordered. ## Output Format Your response **must be only** Python list of integers representing the correct chronological order of the future state image labels. Do not include any other text, reasoning, or explanation. **Example:** If you determine the correct sequence is Next State 1 -> Next State 3 -> Next State 2, Your output must be: [1, 3, 2] ## Actions in Order {STATE_CHANGES} Now, please provide your answer in the requested format. Figure 15 The prompt used to evaluate VLMs on the multi-step Forward World Modeling task. The model must order shuffled future state images by reasoning over given action sequence."
        },
        {
            "title": "Inverse World Modeling Prompt",
            "content": "You are capable agent designed to infer multi-step inverse dynamics transitions in embodied decision-making. Your goal is to determine the correct chronological order of actions that caused the state transitions shown in sequence of images. ## Your Task You will be given an ordered sequence of images that show scene evolving over time, along with shuffled list of the actions that caused these changes. To solve this, you must: 1. Analyze the transition from the first image to the second. Determine the specific visual change that occurred. From the **Shuffled Actions** list provided below, identify the single action that best describes this change. Repeat this process for all subsequent pairs of images (second to third, third to fourth, etc.) until you have correctly ordered all the actions. 2. 3. ## Output Format Your response **must be only** Python list of integers representing the correct order of the action labels. Do not include any other text, reasoning, explanations, or code formatting. **Example:** If the correct sequence is [Action 2] -> [Action 3] -> [Action 1], your output must be: [2, 3, 1] ## Shuffled Actions {SHUFFLED_ACTIONS} Now, please provide your answer in the requested format. Figure 16 The prompt used to evaluate VLMs on the multi-step Inverse World Modeling task. The model must order set of shuffled actions by reasoning over an ordered sequence of state images. B.2.2. Detailed Results detailed examination of the full experimental results are presented in Table 9 (Task Accuracy) and Table 10 (Pairwise Accuracy). Model Proprietary Models GPT-5 GPT-5 mini GPT-5 nano Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.5 Flash-Lite Claude Sonnet 4 Open-Weight Models Forward World Modeling Inverse World Modeling 3 4 5 7 8 9 10 3 5 6 7 8 9 20.24 47.13 33.62 80.59 62.72 19.02 31.71 83.39 62.72 45.22 9.74 58.57 30.66 1.40 3.83 47.30 29.79 17.80 81.99 62.72 6.28 25.22 14.29 75.52 50.52 1.40 3.83 10.09 52.97 27.18 0.70 2.96 8.52 56.29 24.91 2.80 84.79 86.19 72.65 11.58 7.30 5.00 5.29 9.12 0.00 0.00 0.00 3.28 3.60 10.00 1.28 2.98 0.18 0.00 0.18 0.00 0.00 0.00 59.65 43.73 67.42 58.09 41.11 8.19 58.61 43.38 38.96 27.70 8.54 13.59 72.03 39.02 17.22 87.76 73.52 0.20 82.52 61. 69.06 42.33 19.83 72.73 42.16 24.17 33.68 29.67 3.14 33.51 17.98 13.86 0.88 4.71 2.63 6.98 24.04 17.15 13.50 18.07 1.05 0.36 23.68 15.88 6.20 0.73 1.46 13.00 8.60 0.00 14.40 3.80 0.00 1.00 66.08 40.77 18.09 57.52 28.40 11.30 5.04 21.43 32.58 14.78 GLM-4.5V GLM-4.1V-Thinking Llama-4-Scout-17B-16E-Ins 58.74 Llama-4-Mav-17B-128E-Ins 63.99 InternVL3.5-241B-A28B InternVL3.5-14B InternVL3.5-8B InternVL3.5-4B Gemma-3-27b-it Gemma-3-12b-it Gemma-3-4b-it QVQ-72B-Preview Qwen2.5-VL-72B-Ins Qwen2.5-VL-32B-Ins Qwen2.5-VL-7B-Ins Qwen2.5-VL-3B-Ins Ovis2.5-9B Ovis2.5-2B MiniCPM-V-4.5 MiniCPM-o-2.6 Idefics3-8B-Llama3 Cosmos-Reason1 BAGEL 67.83 43.38 14.81 46.33 25.09 54.72 22.13 54.55 22.82 53.15 21.78 51.22 52.80 20.56 60.84 29.79 71.68 40.42 51.40 32.75 22.73 23.17 45.98 13.76 47.55 23.00 39.69 17.77 48.43 19.16 26.05 17.07 16.20 48.08 45.45 21.43 25.87 17.77 8.54 2.26 1.74 4.36 21.22 12.02 1.05 3.48 1.05 5.39 2.09 6.43 0.87 5.57 1.05 6.09 0.17 1.57 2.09 8.17 7.84 18.96 3.48 10.09 0.52 5.39 0.70 5.91 2.96 10.61 0.87 5.91 1.92 8.35 1.57 5.22 0.52 2.26 0.52 5.04 2.09 3.83 1.57 0.35 0.70 1.57 4.71 0.00 1.22 0.52 0.17 0.17 0.70 0.70 3.32 0.52 0.17 0.17 1.05 0.52 0.52 0.00 0.17 0.17 0.17 0.35 0.18 0.18 0.35 1.05 0.00 0.18 0.00 0.18 0.00 0.00 0.00 1.23 0.00 0.00 0.00 0.18 0.00 0.18 0.00 0.00 0.00 0.00 0.18 0.00 0.00 0.00 0.00 64.34 34.32 0.00 0.00 71.50 49.30 0.00 81.99 59.76 0.36 0.00 66.43 45.12 23.65 0.00 0.00 63.99 40.24 20.00 0.00 0.00 63.64 32.93 16.00 0.00 0.00 63.46 31.88 14.61 0.00 0.00 9.74 52.80 27.53 0.00 0.00 52.45 18.12 0.00 0.00 3.83 66.96 40.24 16.87 0.00 0.00 0.00 53.48 39.34 33.45 19.13 0.00 0.00 70.10 41.11 16.52 0.00 0.00 56.64 32.75 13.39 0.00 0.00 62.76 35.54 16.00 0.00 0.00 48.43 23.87 0.00 0.00 8.52 68.01 37.98 22.09 0.00 0.00 38.64 27.35 11.30 0.00 0.00 46.33 16.72 0.00 0.00 2.96 51.92 29.09 12.02 0.00 0.00 56.29 35.89 14.61 0.00 0.00 11.69 79.55 57.32 32.52 20.38 0.87 73.43 39.37 12.00 4.53 1.75 2.96 10.26 4.19 24.35 11.85 15.18 40.35 24.22 5.93 11.85 3.49 6.79 2.27 5.75 1.57 5.05 1.75 2.79 0.17 1.92 3.84 6.97 29.74 17.77 11.52 6.11 8.89 1.05 5.23 1.05 5.75 1.75 6.27 0.00 1.57 3.66 9.41 0.52 2.44 0.00 1.57 0.52 3.31 2.97 7. 0.00 75.87 5.44 0.53 0.00 1.58 7.37 1.93 0.53 0.53 0.35 0.35 0.00 1.23 4.74 2.11 0.00 0.18 0.35 0.00 1.75 0.18 0.00 0.18 0.18 1.64 0.00 0.18 0.55 4.56 1.28 0.36 0.18 0.00 0.00 0.00 0.55 1.46 0.91 0.00 0.00 0.00 0.00 0.18 0.00 0.00 0.00 0.18 0.40 0.00 0.00 0.00 2.00 0.40 0.20 0.00 0.60 0.00 0.00 0.00 0.40 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0.00 0.00 Human Performance 90. 92.16 89.74 85.71 88.31 87.02 85.58 84.00 91.78 90.24 88.70 88.15 89.53 92.28 87.73 85.00 Table 9 Evaluation on ENACT (Task Accuracy). Dark gray indicates the best result within each category (Proprietary or Open-Weight Models), and Light gray denotes the second-best result within the category. B.2.3. Contact Experiment To verify that our conclusions are not an artifact of using only semantic scene graph predicates in ENACT, we add an ablation in which key frames are also gated on changes in binary contact relations between objects (e.g., touch / no-touch). Concretely, we augment the symbolic predicate set so that both state changes and contact changes trigger key-frame sampling, while keeping the rest of the pipeline unchanged, and re-evaluate InternVL3.5241B on the resulting trajectories. As shown in Table 11, the qualitative trends remain the same as in our main results. Inverse world modeling consistently outperforms forward modeling across all horizons (e.g., 86.67% vs. 90.00% task accuracy at 3 steps, and 3.45% vs. 16.67% at 7 steps), and both task and pairwise accuracies still drop substantially as the number of interaction steps increases for both directions. This suggests that our findings are robust to the choice of symbolic key-frame criteria and are not driven by sparsity introduced by state-change-only sampling. 36 Model Proprietary Models GPT-5 GPT-5 mini GPT-5 nano Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.5 Flash-Lite Claude Sonnet 4 Open-Weight Models Forward World Modeling Inverse World Modeling 3 4 5 6 8 9 10 3 4 6 7 8 9 10 75.26 84.62 87.50 76.25 67.83 50.29 86.10 76.42 67.94 81.64 49.07 64.34 45.82 65. 57.48 58.14 52.16 49.45 52.38 46.65 69.96 64.18 70.65 63.41 38.61 69.83 54.17 38.70 36.65 57.12 55.33 68.78 46.93 86.28 44.11 85.05 76.77 75.43 67.67 63.79 57.04 55.04 50.02 20.33 30.35 25.97 21.90 17.59 16.84 72.81 53.95 42.48 36.45 60.80 53.26 57.78 56.62 70.03 36.98 43.38 37.43 32.73 29.88 28.07 82.78 72.18 60.83 58.19 53.14 51.78 47.99 44.98 33.87 27.81 25.44 23.31 20.31 69.58 57.55 46.04 39.09 34.06 30.18 27.51 23.16 30.52 26.61 22.78 21.49 20.16 73.25 56.85 48.87 43.07 37.00 32.71 30.50 28.49 31.68 28.20 24.11 66.03 62.91 81.18 75. 48.12 40.12 65.71 62.13 80.37 76.09 87.94 74.30 GLM-4.5V GLM-4.1V-Thinking 67.31 Llama-4-Scout-17B-16E-Ins 68.18 Llama-4-Mav-17B-128E-Ins 72.47 InternVL3.5-241B-A28B InternVL3.5-14B InternVL3.5-8B InternVL3.5-4B Gemma-3-27b-it Gemma-3-12b-it Gemma-3-4b-it QVQ-72B-Preview Qwen2.5-VL-72B-Ins Qwen2.5-VL-32B-Ins Qwen2.5-VL-7B-Ins Qwen2.5-VL-3B-Ins Ovis2.5-9B Ovis2.5-2B MiniCPM-V-4.5 MiniCPM-o-2.6 Idefics3-8B-Llama3 Cosmos-Reason1 BAGEL 59.99 49.48 42.62 52.09 75.79 62.25 36.53 54.90 44.83 64.42 42.04 63.11 44.66 63.29 43.55 62.33 41.17 61.98 69.14 52.96 78.15 60.05 55.46 67.83 43.90 26.84 35.31 58.22 42.51 58.39 38.85 46.94 38.73 60.75 39.37 35.31 36.99 60.23 41.86 56.28 40.19 30. 80.59 37.84 51.53 82.26 70.09 60.61 69.28 57. 46.95 41.68 32.88 27.85 25.24 47.65 38.43 34.30 43.87 50.83 45.85 27.87 31.48 30.26 32.04 32.78 35.70 40.83 49.87 41.92 44.35 32.00 30.57 34.96 32.65 33.65 29.48 31.83 34.75 29.65 38.78 30.83 25.69 21.60 19.67 37.36 37.93 31.29 25.80 21.50 20.14 18.73 75.35 56.27 46.57 36.79 29.61 24.56 23.91 25.80 26.54 31.20 66.00 50.00 41.30 37.04 29.73 25.61 22.45 30.52 28.50 26.57 25.94 35.17 31.68 28.10 25.80 35.30 29.90 25.89 22.79 20.49 72.55 62.60 50.52 43.10 53.38 45.90 39.35 34.12 30.56 25.47 22.02 18.73 18.29 20.60 69.06 59.52 49.00 43.45 37.61 32.28 29.31 28.58 24.32 23.62 21.50 19.30 15.47 65.03 56.10 45.35 37.67 35.02 29.62 26.41 23.60 26.13 21.73 20.28 19.64 21.98 64.95 50.12 41.61 35.78 29.00 26.57 27.55 24.04 25.82 22.11 19.50 16.74 16.29 64.95 48.37 40.04 33.87 28.53 23.63 21.74 19.36 25.68 22.45 20.40 17.70 16.71 53.23 43.79 34.43 29.90 25.57 22.31 21.60 18.16 35.16 30.51 26.17 26.73 25.80 53.06 36.41 29.52 26.38 22.66 24.44 33.71 33.62 36.27 33.16 30.63 26.30 24.76 71.33 58.77 48.43 44.36 40.26 39.30 36.66 36.58 37.57 33.76 36.27 35.75 27.52 26.42 22.01 18.07 63.55 59.70 54.57 51.01 41.47 40.16 23.07 19.66 16.69 11.82 11.31 70.54 56.45 42.43 32.89 25.07 19.52 16.72 17.42 24.08 20.36 17.44 14.87 15.07 57.43 49.13 40.48 34.88 28.33 26.14 22.97 20.51 31.08 24.61 20.78 18.11 16.96 64.86 51.74 41.65 35.47 30.95 26.64 23.70 23.25 26.86 25.63 22.21 22.49 24.87 54.28 44.08 35.43 29.06 27.84 25.56 27.62 29.29 25.47 24.81 21.40 21.56 18.33 69.23 53.08 47.35 39.55 34.87 30.63 27.05 25.71 31.71 31.78 27.66 26.39 24.59 18.50 17.04 15.16 24.25 21.29 20.80 20.46 17.71 34.56 31.50 28.64 28.40 26.46 26.49 25.41 24.88 18.91 25.37 22.75 19.45 17.84 15.87 56.73 52.85 40.09 35.44 27.42 54.11 48.26 44.70 40.00 38.28 36.12 33.23 31.73 28.03 25.07 77.80 65.85 53.30 48.19 44. 23.48 19.87 38.50 35.72 29.67 24.39 28.70 49.36 47.17 33.86 45.93 27.26 44.25 47.38 58. 36.77 Human Performance 93.62 95.30 95.04 93.87 95.43 95.41 94.75 95.13 92.05 93.56 94.35 94.25 95.96 97.74 96.30 96. Table 10 Evaluation on ENACT (Pairwise Accuracy). Dark gray indicates the best result within each category (Proprietary or Open-Weight Models), and Light gray denotes the second-best result within the category. Metric 3 4 5 7 8 9 10 3 5 6 7 8 9 Forward (with contact changes) Inverse (with contact changes) Task Accuracy 86.67 43.33 36.67 20.00 3.45 3.33 0.00 0.00 90.00 73.33 30.00 26.67 16.67 3.33 6.67 0.00 Pairwise Accuracy 90.00 72.22 60.00 53.33 48.28 42.38 31.67 34.44 90.00 82.22 55.83 57.33 46.11 32.86 38.75 27.78 Table 11 Effect of including contact changes in key-frame selection. Task and pairwise accuracies (%) of InternVL3.5241B when key frames are triggered by both state changes and contact changes. Metric Vanilla (Natural Language) Forward Inverse 3 4 5 7 8 9 10 3 5 6 7 8 9 68.97 35.17 27.59 8.97 6.90 2.07 0.69 0.00 83.45 60.69 44.14 24.83 13.79 7.59 4.14 0.00 Task Accuracy Pairwise Accuracy 76.21 57.70 57.41 41.24 38.85 30.34 29.05 26.52 83.45 69.66 61.72 53.10 47.01 40.39 34.40 27.25 Symbolic Predicates Task Accuracy 67.59 40.00 20.00 9.66 4.14 0.69 1.38 0.00 79.86 51.03 40.00 23.45 8.97 5.52 2.76 0.73 Pairwise Accuracy 74.48 61.61 48.97 41.52 35.75 32.12 30.34 25.68 79.86 62.99 58.62 51.59 39.43 36.85 32.07 24.33 Emoji-Style Encodings Task Accuracy 65.52 44.14 18.62 11.03 8.28 0.69 0.69 0.00 77.24 48.28 35.17 23.45 12.41 7.59 2.76 0.00 Pairwise Accuracy 73.10 64.60 48.62 39.45 38.05 27.49 26.72 23.93 77.93 61.38 59.14 45.38 40.80 36.06 27.16 25.30 Table 12 Effect of action representation on InternVL3.5241B. Task and pairwise accuracies (%) on 2,304-QA subset of ENACT under three action encodings. 37 Model Data-Level Mismatch (%) Pair-Level Mismatch (%) GPT-5 Human InternVL3.5-241B 1.65 2.26 1.49 15.31 3.62 13.96 Table 13 Mismatch rates among semantically accepted predictions. Data-level and pair-level mismatch ratios (%) for cases where the semantic verifier accepts prediction but its predicate set is strict subset of the ground-truth transition. B.2.4. How Does Action Representation Affect VLMs Performance? Our primary goal is to evaluate VLMs under the standard interface of natural-language actions, but this leaves open whether the inverse advantage is merely consequence of language priors, i.e., models being better at mapping visuals to familiar verbs than to unfamiliar symbolic actions. To test this, we construct subset of ENACT trajectories with 2,304 QAs and compare three actionpredicate encodings while keeping the underlying videos and questions fixed: (1) the original natural-language descriptions (vanilla), (2) structured symbolic predicates, and (3) emoji-style encodings. Table 12 reports task and pairwise accuracies of InternVL3.5241B across horizons for all three settings. Across all representations, we observe the same qualitative pattern as in our main results: Inverse world modeling consistently outperforms forward modeling at comparable horizons, and performance for both directions degrades sharply as the number of interaction steps increases. While absolute accuracies vary slightly across encodings, the inverse > forward gap is preserved even with purely symbolic or emoji-style actions, suggesting that our conclusions are not driven solely by natural-language priors. B.2.5. How Often Do Accepted Predictions Omit Parts of Transitions? Our semantic verifier operates with subset inclusion: predicted transition is accepted as semantically correct if its predicate set is subset of the ground-truth transition and does not contain any predicates that contradict the ground truth. In other words, we allow partial correctness (omitting some true predicates), but never accept hallucinated predicates that conflict with the annotated transition. To quantify how often such omissions occur among accepted predictions, we measure mismatch ratios at two granularities: (i) data-level mismatch ratio, computed per QA as the fraction of semantically accepted predictions whose predicate set is strict subset of the ground truth; and (ii) pair-level mismatch ratio, computed per ordered pair in the reordering task as the fraction of accepted pairs where at least one element is strict subset of the corresponding ground truth transition. Results for GPT-5, InternVL3.5241B, and human annotators are shown in Table 13. At the data level, all models (and humans) exhibit very low mismatch rates (12%), indicating that most semantically accepted predictions recover the full transition. At the pair level, models show higher mismatch ratios than humans (around 1415% vs. 3.6%), reflecting that they occasionally capture only subset of the true transition when comparing two candidate steps. Overall, this suggests that while our verifier does grant some partial credit, such cases are relatively rare at the QA level and do not dominate the evaluation. B.3. Additional Ablation Experiments Common Setup To gain deeper insights into model sensitivities, we conducted series of controlled ablation experiments. This section outlines the common experimental framework that applies to our analyses of Image Realism (Section B.4), Camera Configurations (Section B.5), and Robot Appearance (Section B.6.1). 38 For these experiments, we selected two representative models. Given its strong balance of performance and computational cost in our main results, we chose GPT-5 mini as our primary model to represent state-of-the-art proprietary VLMs. To include strong open-weight counterpart, we also selected InternVL3.5-241B-A28B, which demonstrated robust performance among open models. Figure 17 Illustrative trajectories of Forward World Modeling and Inverse World Modeling for representative baseline question. In our summary heatmaps (Figure 5 for GPT-5 mini and Figure 18 for InternVL3.5-241B-A28B), we use 풊 to visualize the performance difference between variant and the baseline. To assess the statistical significance of these differences, we perform two-tailed unpaired Welchs t-test. An unpaired test is appropriate as each question is evaluated in an independent session. We specifically use Welchs t-test as it does not assume equal variance between the two groups being compared (baseline vs. variant). We report the p-value for each comparison and consider result to be statistically significant if 洧녷 < 0.05. We qualitatively classify any performance change where 풊 < 0.05 as small change. We show one baseline question and its images for both forward and inverse settings in Figure 17, and for other settings, we only show their images, as they all share the same question text and answers. B.4. Sensitivity to Image Realism Although the BEHAVIOR simulator is designed to be photo-realistic, we were curious whether sim-to-real gap might still exist due to subtle differences in rendering quality. Specifically, we sought to investigate if such gap affects performance on our world modeling tasks and to quantify the impact of rendering fidelity on the reasoning capabilities of state-of-the-art VisionLanguage Models, such as GPT-5 mini. In the following sections, we detail the experimental setup for evaluating model performance across various levels of image realism. 39 Figure 18 Ablation experiment results with InternVL3.5-241B-A28B on ENACT. Heatmaps show twotailed unpaired p-values against the baseline, using Pairwise Accuracy. 洧녷 < 0.05 is considered significant. Darker red means more significant. 풊 is the performance change from the baseline. If significant and 풊 < 0, the setting is worse than the baseline. C.2 reports the robots performance on the leftand right-hand predicates, where Mixing is the proportion of ground truth left or right cases that are predicted as the other hand (i.e., mixing one hand into the other hand). Note that, although InternVL3.5-241B-A28B performance is less significant than GPT-5 mini, the 풊 across unnatural camera configurations still remains high (> 0.05) when the same settings are significant for GPT-5 mini. Figure 19 Comparison between Cosmos-Reason1 and other similar-sized models. 40 B.4.1. Realistic: Generated Images as Real World Proxy Since our activities are diverse and complex, reproducing simulator outputs in the real world on one-to-one basis would incur prohibitively high costs. However, with the advent of powerful image generation models with the ability of image-scale reproduction (e.g., GPT-image-1), it is feasible to use them as real-world proxy to convert frames rendered by simulator into realistic styles, which provides cost-effective and well-aligned alternative. Constructing prompts for high-accuracy style transfer poses several challenges. First, since our segmented frames are extracted from replayed robot trajectory, the generated realistic frames corresponding to the trajectory must preserve consistent content and style, including object shapes and appearances, lighting conditions, material properties, and camera parameters. Second, image generation models often demonstrate instability and errors in understanding fine-grained structures of robotic arms (particularly the gripper) and in interpreting robotic actions. To mitigate these issues, we establish detailed set of rules and incorporate them into the prompt design (Figure 22), which improves both stability and fidelity in the generated outputs. (a) (b) Figure 20 Ablating image realism with GPT-5 and InternVL3.5-241B-A28B. (a) Forward dynamics; (b) Inverse dynamics. Bar plots report Pairwise Accuracy across four rendering settingsRealistic, Path Tracing, Ray Tracing (Baseline), and Ray Tracing Only. Error bars denote SEM. The baseline x-tick is bolded. Figure 21 Examples of simulator frames converted into realistic styles for both Forward World Modeling (left) and Inverse World Modeling (right) trajectories."
        },
        {
            "title": "Project Instruction and Prompt for Image Generation",
            "content": "## Below are the instructions and regulations, treat them as the sole, global reference for all image generations you are going to perform. ## Core Objective Convert simulator screenshots into photorealistic PBR images. Change style only; do not change content. ## Content Lock (Content-Locked) Preserve the count, position, size, geometry, and pose of all objects. The robot hand and knife angles, shapes, and actions must match exactly. ## Camera crop and viewpoint must remain unchanged. The outdoor scene must remain daytime; tree and fence silhouettes must not change. If realism conflicts with content, content fidelity takes precedence. ## Style Requirements * Lighting: Warm under-cabinet tungsten (3200-3600 K) + soft window daylight fill. * Tone: Filmic contrast, smooth highlight roll-off, no crushed blacks or blown highlights. * Camera: approximately 35 mm, f/2.8-4, shallow DOF; subject sharp with gently blurred background. * Shadows: Realistic soft shadows, contact shadows, and ambient occlusion. ## Materials: * Metal knife and trims: Brushed, anisotropic metal. * Robot: Matte polymer. * Cutting board and countertop: Sealed/oiled wood grain. * Glass/walls: Glossy glass with realistic reflections and refractions. * Post-processing: Subtle camera grain; light vignette. * Prohibited: Cartoonish look, plastic sheen, bloom, oversaturation, hard outline sharpening, fake lighting effects. ## Acceptance Criteria * Edge alignment: SSIM >= 0.95 (along object boundaries). * Segmentation: IoU >= 0.98 for robot, knife, cutting board, outdoors. * Color difference: delta Hue <= 3, delta <= 6. * Knife shape error: <= 1 px. * Outdoor tree/fence silhouette error: <= 1-2 px. ## Implementation Suggestions * Use low denoise strength 0.20-0.35, CFG 4-6. * Negative prompt: forbid new objects, geometry changes, cartoonish/oversaturated/plastic textures. * Detail pass: add micro-surface material detail + light film grain. Now, review and summarize what you have learned from these instructions. Following the instructions you have learned, transform the given image into realistic photograph style. Figure 22 The prompt used to generate realistic photographic style images from segmented frames (of replayed robot trajectory). B.4.2. Path Tracing Setup To generate the highest-fidelity images for our analysis of image realism, we utilized path tracing. This was achieved directly through the built-in, real-time path tracing engine provided by the NVIDIA Isaac Sim simulator. An example can be seen in Figure 23. B.4.3. Ray Tracing Only Setup This setup was designed to represent an intermediate rendering quality (representing unrealistic). While it still utilizes the ray tracing pipeline as its foundation, we manually disabled several advanced lighting and post-processing effects to reduce visual fidelity. Specifically, 42 Figure 23 The figure illustrates Forward World Modeling (left) and Inverse World Modeling (right) trajectories rendered using the path tracing engine in NVIDIA Isaac Sim. we turned off the following features: reflections, DLSS, ambient occlusion, sampled lighting, ambient light, and flow. The resulting visual style, which lacks these richer effects, can be seen in Figure 24. Figure 24 Examples of an intermediate rendering style created with simplified ray tracing pipeline for Forward World Modeling (left) and Inverse World Modeling (right) trajectories. Metric 3 4 5 6 7 9 10 Forward (with random perturbations) 68.51 1.44 37.47 2.11 25.75 1.74 10.80 1.59 4.60 2.11 1.61 0.80 0.92 0.40 0.00 0.00 Task Accuracy Pairwise Accuracy 75.52 1.19 59.77 1.82 54.48 2.54 42.25 1.30 37.36 1.30 30.90 0.59 28.76 0.43 25.98 1.01 Inverse (with random perturbations) 83.45 0.69 58.39 3.40 43.91 1.05 22.76 2.07 13.79 0.00 6.67 2.22 4.14 2.07 0.73 1.26 Task Accuracy Pairwise Accuracy 83.45 0.69 68.74 1.40 61.49 0.72 52.46 0.69 45.59 1.23 39.70 1.64 35.09 2.57 29.01 1. Table 14 Robustness to random perturbations in predicate deltas. Mean standard deviation (%) over three random seeds on 2,304-QA subset of ENACT for InternVL3.5241B. B.4.4. Robustness to Noise in Predicate Deltas To assess whether our conclusions are sensitive to noise in the abstract transitions, we run robustness study on subset of 2,304 QAs sampled from ENACT. For each trajectory, we randomly perturb the symbolic predicates in the abstract deltas and re-run both forward and inverse evaluations for InternVL3.5241B over three random seeds. Table 14 reports the mean standard deviation across seeds. Across all horizons, we observe only very small standard deviations, and the qualitative trends remain unchanged: Inverse world modeling consistently outperforms forward world modeling at comparable horizons, and performance for both directions degrades sharply as the number of steps increases. These results indicate that our findings are stable across activities and robust to random perturbations in the predicate deltas, rather than being driven by few particularly clean or favorable trajectories. 43 B.5. Sensitivity to Camera Configurations B.5.1. Camera Aperture Setup Our default baseline is aperture 40. We also investigate apertures 30, 60, and 80. Examples can refer to Figure 25, 26, and 27. Figure 25 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), captured with camera aperture of 30. Figure 26 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), captured with camera aperture of 60. Figure 27 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), captured with camera aperture of 80. B.5.2. Fisheye Lens Setup Isaac Sim provides the fisheye lens settings. We choose fisheyePolynomial, which is the most similar to daily fisheye lens, such as GoPro, as our evaluated target. The effect can be seen in the example Figure 29. B.5.3. Camera Height Setup The default setting height is 1.75 m, we also investigate the high (+0.5m) setting and low (0.25m)setting, and the examples are shown in Figure 30 and 31. 44 (a) (b) Figure 28 Ablating camera field-of-view (FOV) with GPT-5 and InternVL-3.5-241B-A28B. (a) Forward dynamics; (b) Inverse dynamics. Bar plots report Pairwise Accuracy across five lens settingsAperture 30, Aperture 40 (Baseline), Aperture 60, Aperture 80, and Fisheye. Error bars denote SEM; the baseline tick is bolded. Figure 29 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), captured with fisheye-style camera. Figure 30 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), captured from camera height of 2.25 m. B.6. Do VLMs Have Embodied Biases? B.6.1. Robot Appearance We test three variants: White Color, Random Color (robot color is randomized at each frame), and Skin Color (robot is rendered with human-like skin tone). Examples can be referred to Figure 33, 34, and 35. B.6.2. Handedness Based on our experimental setup (C.1). We further examine whether predictions involving agent interactions reflect real-world handedness asymmetry (typically favoring the right hand). In both humans and models, and across both task types, right-hand precision and recall sys45 Figure 31 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), captured from camera height of 1.5 m. (a) (b) Figure 32 Ablating camera height with GPT-5 and InternVL-3.5-241B-A28B. (a) Forward dynamics; (b) Inverse dynamics. Bar plots report Pairwise Accuracy across three viewpoints (High, Mid baseline, and Low). Error bars denote SEM; the baseline tick is bolded. Figure 33 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), with the robot gripper rendered in white. Figure 34 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), with the robot gripper rendered in random color at each frame. 46 Figure 35 Example trajectories of Forward World Modeling (left) and Inverse World Modeling (right), with the robot gripper rendered in human skinlike color. (a) (b) Figure 36 Ablating robot appearance with GPT-5 and InternVL-3.5-241B-A28B.(a) Forward dynamics; (b) Inverse dynamics. Bar plots report Pairwise Accuracy across four stylesWhite Color, Fancy (Baseline), Random Color, and Skin Color. Error bars denote SEM; the baseline tick is bolded. tematically exceed those of the left (Figures 38, 39). Furthermore, left-to-right mixing rate (ground-truth left-hand components wrongly predicted as right-hand ones) substantially exceeds the reverse (Figure 40). C. Error Analysis C.1. Methodology for Error Calculations Signature Modeling from Scene Graph-level Differences For error analysis, it is found hard to recognize predicate-level or semantic-level errors through natural language-based actions, i.e. visible differences between consecutive states. Hence, we parse the raw (natural language) action predicates as signature into sequence of unique state-change signature (洧녩sig 1 , . . . ). 0 , 洧녩洧멇롐뒳롐 from 洧랣to), 洧녫洧녰 ::= (洧, 洧1, 洧랣, 洧2) (洧, 洧, 洧랣) (transition, 洧, 洧랣 To further structure these signatures, each signature 洧녩sig is then modeled as finite set of components {洧녫1, 洧녫2, . . . }. Each component 洧녫洧녰 represents an atomic unit of state change. We distinguish three categories of components: edge components (addition or removal 洧 of predicates 洧랣 between two entities 洧1 and 洧2), node components (addition or removal 洧 of the predicate 洧랣 of entity 洧), and node transition components (transition from the previous predicate 洧랣 洧녭 洧洧녶洧녴 to new predicate 洧랣洧노洧녶 of an entity 洧). 洧 {add, remove} 洧녰 47 (a) (b) Figure 37 Illustration of overlaps between ground truth and GPT-5 predictions sets for left and right hands related signatures in forward and inverse tasks. The size of ellipses project the total counts of signatures, and overlaps denote matched signatures (center regions) or mixing errors (cross-hand overlaps). (a) (b) Figure 38 Precision of left/right hand related components prediction in (a) forward and (b) inverse tasks, with models Gemini2.5Pro, GPT-5, GPT-5 mini, InternVL3.5-241B-A28B, and Human. Error bars indicate the standard error (SE). Error Modeling from Signatures We categorize errors from two perspectives: structural and semantic. Structural errors concern the form of actions and include entity substitution (object replacement), predicate substitution (relation/attribute replacement), polarity inversion (add, remove or transition), omission, and hallucination. Semantic errors concern interpretation and are grouped into spatial relations (misplaced object positions), functional states (incorrect functionalities or status), material states (wrong physical properties), and agent interaction (misattributed agent actions). Both perspectives are based on comparing component sets of paired ground-truth and predicted signatures. For each pair, we compute set-level differences and classify components into missing (in ground truth only), matched (in both), and hallucinated (in prediction only). To support this categorization, we preprocess the signature dataset into structured data with these three groups of components, as outlined in Algorithm 2. To categorize structural errors, we define criteria for each component type (edge, node, transition node). Entity Substitution occurs when entities differ while other fields match; Predicate 48 (a) (b) Figure 39 Recall of left/right hand related components prediction in forward task, with models Gemini2.5Pro, GPT-5, GPT-5 mini, InternVL3.5-241B-A28B and Human. Error bars indicate the standard error (SE). (a) (b) Figure 40 Hand-mixing rate, i.e.the ratio of left/right hand-mixing to all ground truth left/right and components in (a) forward and (b) inverse task, with models Gemini2.5Pro, GPT-5, GPT-5 mini, InternVL3.5-241B-A28B and Human. Error bars indicate the standard error (SE). (a) (b) Figure 41 The amount of total errors made by Gemini2.5Pro, GPT-5, GPT-5 mini, InternVL3.5-241B-A28B, and Human, under (a) forward tasks and (b) inverse tasks. Substitution when the predicate differs; and Polarity Inversion when only the operation (add/remove) differs. After pairwise classification, remaining unmatched ground-truth components are categorized to Omission, and unmatched predicted components as Hallucination. After structural error categorization, each component is further labeled by semantic error type: Spatial Relations, Functional States, Material States, or Agent Interactions. Labeling uses predefined mapping table that links all observed predicates to their semantic categories. When component contains listed predicate, the table is consulted to assign its semantic label. The overall workflow of error detection and categorization is illustrated in Algorithm 3. Handedness Asymmetry Error Modeling To systematically capture handedness asymmetry, we compute for leftand right-hand components: precision (correct matches over predicted), recall (correct matches over ground truth), and the hand-mixing rate (the fraction of ground-truth left-hand components predicted as right, or vice versa). For computing the hand-mixing rate, we define leftright mixing at the level of each signaturelevel difference (with missing, matched, and hallucinated components). If the missing set contains left- (or right-) hand usage, while the hallucinated set lacks the same hand but includes the opposite one, then all missing components involving that hand are counted as left-to-right (or right-to-left) mixing, as outlined in Algorithm 4. C.2. Structural Error Analysis We compared error patterns in forward and inverse tasks across Gemini-2.5 Pro, GPT-5, GPT-5 mini, InternVL-3.5-241B-A28B, and human predictions (Figures 41). C.3. Semantic Error Analysis In our semantic error analysis (Figure 48), all systemsGemini-2.5 Pro, GPT-5, GPT-5 mini, InternVL-3.5-241B-A28B, and humansshow similar pattern: errors are concentrated in Spatial Relations and Agent Interaction, reflecting difficulties in reasoning about object positions and agent actions (e.g., left/right-hand grasping). task-dependent asymmetry also appears: spatial-relations errors are more common in forward tasks, while agent-interaction errors are higher in inverse tasks. For illustration, we sample representative GPT-5 cases for each semantic category under both settings (Figures 49, 50, 51, 52). 50 Figure 42 The structural error distributions of typical LLMs (GPT-5, GPT-5 mini, Gemini2.5Pro and InternVL3.5-241B-A28B (referred as InternVL3.5... in figure)) and Human-level prediction in both forward and inverse tasks. 51 Figure 43 Example of structural error Entity Substitution by GPT-5 under forward and inverse tasks. Figure 44 Example of structural error Predicate Substitution by GPT-5 under forward and inverse tasks. 52 Figure 45 Example of structural error Polarity Inversion by GPT-5 under forward and inverse tasks. Figure 46 Example of structural error Omission by GPT-5 under forward and inverse tasks. 53 Figure 47 Example of structural error Hallucination by GPT-5 under forward and inverse tasks. Algorithm 2: Action-level Parsing of Signatures Data Input: Dataset of signatures D洧멇롐뒳롐, each with ground-truth signatures 洧녩洧멇롐뒳롐 洧녮洧노 and predicted 洧녷 signatures 洧녩洧멇롐뒳롐 Output: Data of signatures Signatures filtering: 洧멇롐뒳롐 洧녮洧노 , 洧녩洧멇롐뒳롐 foreach (洧녩洧멇롐뒳롐 洧녮洧노 = 洧녩洧멇롐뒳롐 if 洧녩洧멇롐뒳롐 add (洧녩洧멇롐뒳롐 洧녷 ) D洧멇롐뒳롐 do 洧녷 then 洧녮洧노 , 洧녩洧멇롐뒳롐 洧녷 ) to 洧멇롐뒳롐 洧멇롐뒳롐 with missing, matched and hallucinated components else discard (洧녩洧멇롐뒳롐 洧녮洧노 , 洧녩洧멇롐뒳롐 洧녷 ) Action-pairwise Comparison: foreach (洧녩洧멇롐뒳롐 洧녮洧노 , 洧녩洧멇롐뒳롐 洧녷 ) 洧멇롐뒳롐 do /* 洧녫洧녴洧녰 : missing components, 洧녫洧녴洧녩 : matched components, 洧녫 : hallucinated components 洧녫洧녴洧녰 , 洧녫洧녴洧녩 , 洧녫 洧녮洧노 , 洧녩洧멇롐뒳롐 foreach (洧녫洧녮洧노, 洧녫 洧녷) (洧녩洧멇롐뒳롐 if 洧녫洧녮洧노 = 洧녫 洧녷 then 洧녷 ) do add 洧녫洧녮洧노 to 洧녫洧녴洧녩 else add 洧녫洧녮洧노 to 洧녫洧녴洧녰 add 洧녫 洧녷 to 洧녫 */ add (洧녫洧녴洧녰, 洧녫洧녴洧녩, 洧녫) to 洧멇롐뒳롐, discard (洧녩洧멇롐뒳롐 洧녮洧노 , 洧녩洧멇롐뒳롐 洧녷 ) return 洧멇롐뒳롐 54 Figure 48 The semantic error distributions of typical LLMs (GPT-5, GPT-5 mini, Gemini2.5Pro and InternVL3.5-241B-A28B (referred as InternVL3.5... in figure)) and Human-level prediction in both forward and inverse tasks. 55 Figure 49 Example of semantic error Spatial Relations by GPT-5 under forward and inverse tasks. Figure 50 Example of semantic error Functional States by GPT-5 under forward and inverse tasks. 56 Figure 51 Example of semantic error Material States by GPT-5 under forward and inverse tasks. Figure 52 Example of semantic error Agent Interactions by GPT-5 under forward and inverse tasks. 57 Algorithm 3: Action-level Structural and Semantic Error Categorization Input: Parsed signatures dataset Output: Categorized errors dataset D洧뉧롐洧 Structural errors categorization: 洧녞洧냪 , foreach 洧녩洧멇롐뒳롐 洧멇롐뒳롐, predicates 洧녷洧洧뉧롐놿롐 洧멇롐뒳롐 do (洧냤洧녴洧녰, 洧냤) (洧녫洧녴洧녰 (洧녩洧멇롐뒳롐), 洧녫(洧녩洧멇롐뒳롐)) (洧녫洧녴洧녰, 洧녫) FindPairwiseErrors (洧냤洧녴洧녰, 洧냤, polarity inversion); if (洧녫洧녴洧녰, 洧녫) then 洧녞洧녡 , 洧냦洧녡 , 洧녝洧 , 洧냩 洧냢 add (洧녫洧녴洧녰, 洧녫) to 洧녞洧냪 remove (洧녫洧녴洧녰, 洧녫) from 洧녩洧멇롐뒳롐 (洧녫洧녴洧녰, 洧녫) FindPairwiseErrors (洧냤洧녴洧녰, 洧냤, predicate substitution); if (洧녫洧녴洧녰, 洧녫) then add (洧녫洧녴洧녰, 洧녫) to 洧녞洧녡 remove (洧녫洧녴洧녰, 洧녫) from 洧녩洧멇롐뒳롐 (洧녫洧녴洧녰, 洧녫) FindPairwiseErrors (洧냤洧녴洧녰, 洧냤, entity substitution); if (洧녫洧녴洧녰, 洧녫) then add (洧녫洧녴洧녰, 洧녫) to 洧냦洧녡 remove (洧녫洧녴洧녰, 洧녫) from 洧녩洧멇롐뒳롐 foreach 洧녫洧녴洧녰 洧냤洧녴洧녰 do add 洧녴洧녰 to 洧녝洧 foreach 洧녫 洧냤 do add  to 洧냩 洧냢 /* PI: Polarity Inversion, PS: Predicate Substitution, ES: Entity Substitution, OM: Omission, HA: Hallucination */ D洧뉧롐洧 (洧녞洧냪, 洧녞洧녡, 洧냦洧녡, 洧녝洧, 洧냩 洧냢) Semantic errors labeling: foreach 洧녫 in D洧뉧롐洧 do foreach 洧녷洧洧뉧롐 in 洧녷洧洧뉧롐놿롐 do if 洧녷洧洧뉧롐 洧녫 then label 洧녫 with SemanticError(洧녷洧洧뉧롐) return D洧뉧롐洧 Algorithm 4: Dataset-Level Detection of LeftRight Hand Confusion Input: Dataset of signature-level differences D洧녬洧녰 洧녭 洧녭 = {(洧녫洧녴洧녰, 洧녫洧녴洧녩, 洧녫)} Output: Confusion dataset D뀛롐뀛롐洧녬 = {(D洧녳2洧, D洧2洧녳)} Left to right hand confusion: D洧녳2洧 foreach (洧녫洧녴洧녰, 洧녫洧녴洧녩, 洧녫) D洧녬洧녰 洧녭 洧녭 do if 洧녴 洧녫洧녴洧녰 that involves left hand then if  洧녫 that involves left hand then continue, else if  洧녫 that involves right hand then foreach 洧녴 洧녫洧녴洧녰 do if 洧녴 involves left hand then add 洧녴 to D洧녳2洧 Right to left hand confusion: D洧2洧녳 foreach (洧녫洧녴洧녰, 洧녫洧녴洧녩, 洧녫) D洧녬洧녰 洧녭 洧녭 do if 洧녴 洧녫洧녴洧녰 that involves right hand then if  洧녫 that involves right hand then continue, else if  洧녫 that involves left hand then foreach 洧녴 洧녫洧녴洧녰 do if 洧녴 involves right hand then add 洧녴 to D洧2洧녳 D뀛롐뀛롐洧녬 (D洧녳2洧, D洧2洧녳) return D뀛롐뀛롐洧녬 D. The Use of Large Language Models We used large language models (LLMs), including Googles Gemini 2.5 Pro and OpenAIs GPT-5, as auxiliary tools to assist with writing, editing, and conducting the literature review for this manuscript. All content was critically revised and fact-checked by the human authors to ensure its scientific validity and originality. The authors are fully responsible for all statements and conclusions presented in this paper. Specifically, we use LLMs for polishing our wording and writing, and we use LLMs to retrieve several related works."
        }
    ],
    "affiliations": [
        "Northwestern University",
        "Stanford University",
        "UCLA"
    ]
}