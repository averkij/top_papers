{
    "paper_title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
    "authors": [
        "Jinchao Ge",
        "Tengfei Cheng",
        "Biao Wu",
        "Zeyu Zhang",
        "Shiya Huang",
        "Judith Bishop",
        "Gillian Shepherd",
        "Meng Fang",
        "Ling Chen",
        "Yang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA."
        },
        {
            "title": "Start",
            "content": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery Jinchao Ge1 Tengfei Cheng1 Biao Wu2 Zeyu Zhang1 Shiya Huang1 Judith Bishop3 Gillian Shepherd3 Meng Fang2 Ling Chen2 Yang Zhao3 1AI Geeks 2Australian Artificial Intelligence Institute 3La Trobe University Equal contribution Project lead Corresponding author: y.zhao2@latrobe.edu.au 5 2 0 2 1 2 ] . [ 1 1 9 1 7 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. We ask how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with typeconditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing reusable resource for future research. Code and dataset will be available at https://github.com/ AIGeeksGroup/VaseVQA."
        },
        {
            "title": "Introduction",
            "content": "Analyzing cultural-heritage artifacts with Multimodal Large Language Models (MLLMs) remains challenging: general-purpose models lack domain expertise, and conventional supervised fine-tuning (SFT) often overfits superficial lexical patterns, yielding brittle reasoning for authentication and historical attribution. Ancient Greek pottery is an especially valuable testbed: vases are durable records that encode stylistic programs, workshop practices, and chronology (Smith et al., 2024). Robust, expert-level reasoning over such objects requires models to integrate fine-grained visual cues with culturally grounded textual knowledge. Figure 1 illustrates our approach on representative example. 1 Figure 1: chat example from VaseVL using QvQ72B (Qwen-Team, 2024) on an Archaic Greek kylix (540530 BCE) decorated in the black-figure technique. The model identifies vessel type, period, and technique. We address these issues with VaseVL, an SFTthen-RL system that turns evaluation into supervision. Starting from an SFT reference policy πref, we construct taxonomy of seven question types (Fabric, Technique, Shape, Provenance, Attribution, Date, Decoration), probe post-SFT performance to localize type-specific weaknesses, and then optimize with diagnosis-guided, taxonomyconditioned rewards that explicitly target those gaps. To activate reasoning while constraining distributional drift, VaseVL uses Group Relative Policy Optimization (GRPO) with KL penalty to πref, and reward combining keyword overlap and semantic similarity whose weights are conditioned on question type; shortcomings receive amplified weight. This design improves compositional robustness without sacrificing factual recall. To support systematic evaluation, we release VaseVQA, comprehensive benchmark of 31,773 images (with 11,693 single-view subset) and 93,544 visual questionanswer pairs spanning the seven types. Table 2 summarizes the dataset splits, including the Figure 2: Examples from VaseVQA. Each panel shows an image with its question and ground-truth answer. The seven question types probe factual recall and compositional, descriptive reasoning. number of images and questions in the training and test sets. The benchmark is VQA-centric and is accompanied by evaluation scripts tailored to each type (ANLS-based accuracy for factual types, specialized date-accuracy metric, and BLEU@1 for Decoration), enabling fair assessment of both lexical precision and semantic alignment. Figure 2 shows examples of questions and answers from the dataset. Across strong baselines, zero-shot generalpurpose MLLMs perform poorly on expert-level questions, underscoring the domain gap. SFT substantially improves factual recall (e.g., near-ceiling Fabric and strong Technique), but its reasoning remains brittle. VaseVL further improves on the SFT model precisely where reward shaping targets the gaps, achieving state-of-the-art results on style classification and historical attribution and delivering marked gains in compositional robustness. Our main contributions are summarized as follows: VQA-centric dataset of 31,773 images (with 11,693 single-view images) and 93,544 QA pairs over seven question types, plus typespecific evaluation scripts. An SFT-then-RL framework with diagnosisguided, taxonomy-conditioned rewards and GRPO with KL regularization that turns evaluation into supervision to activate robust reasoning. State-of-the-art performance on style classification and historical attribution with significant gains in compositional robustness over SFT-only baselines, providing reusable resource for future research."
        },
        {
            "title": "2 Related Work",
            "content": "VisionLanguage Models (VLMs). Pretrained visionlanguage models learn joint representations from large-scale multimodal corpora and have advanced wide range of tasks, including imagetext retrieval, VQA, grounding, and dialogue (Li et al., 2019; Chen et al., 2020; Zeng et al., 2021; Li et al., 2021; Song et al., 2025a,d,c; Huang et al., 2025; Liu et al., 2025). Visual instruction tuning (VIT) further adapts such models to follow multimodal instructions, exemplified by LLaVA (Liu et al., 2024), MiniGPT-4 (Zhu et al., 2023), and Gemini 1.5 (Google, 2024). Despite impressive generalization, these systems often lack the domain expertise required for specialized cultural-heritage analysis, where fine-grained visual cues must be integrated with historically grounded knowledge. Multimodal Agents. Progress in multimodal models has catalyzed agentic systems that plan, act, and reason across modalities, invoking tools as needed (Song et al., 2025b; Zhao et al., 2024; Ren and Liu, 2023). Datasets such as MineAgent 2 (Beibei et al., 2023), SeeAct (Boyuan et al., 2024), PPTAgent (Hao et al., 2025), PresentAgent (Shi et al., 2025) and MotionAgent (Xinyao et al., 2025) expose persistent weaknesses in longhorizon reasoning, decision making, and compositionalitylimitations that also manifest in expert cultural-heritage tasks. VQA Benchmarks. Foundational VQA datasets, including VQA (Antol et al., 2015), COCO-QA (Ren et al., 2015a), and Visual7W (Zhu et al., 2016a), enabled rapid progress but primarily cover generic objects and scenes. In culturalheritage domains, publicly available resources remain scarce. RePAIR (Tsesmelis et al., 2024) targets oracle bones, and HUST-OBS (Wang et al., 2024) focuses on fragment reconstruction, leaving gap for classical artifacts such as ancient Greek vases. Table 1 summarizes the key characteristics of major VQA and visual reasoning datasets. Our VaseVQA fills this gap with VQA-centric benchmark designed to probe factual recall (e.g., Fabric, Technique) and expert-level reasoning (Attribution, Decoration, Date, Provenance, Shape) under unified, type-aware evaluation protocol. In contrast to prior work that relies solely on SFT, our VaseVL explicitly diagnoses post-SFT failures and applies taxonomy-conditioned reward engineering to improve the targeted reasoning skills, yielding stronger compositional robustness while preserving factual accuracy. By releasing both the benchmark and method, we aim to facilitate reproducible progress on cultural-heritage understanding within the VLM community (Chen et al., 2023; David and Thamar, 2024; Fangyu et al., 2021; DeepSeek-AI, 2025)."
        },
        {
            "title": "3 Data Collection",
            "content": "Our dataset, VaseVQA, was constructed with focus on representing Ancient Greek culture, ensuring both the visual and textual components accurately reflect the regions rich cultural heritage. The Table 1, which presents major VQA and Visual Reasoning datasets, compared with the VaseVQA. On the Table 2, the VaseVQA dataset is divided into train and test datasets, including each image with 8 questions. In addition, Table 3 illustrates examples within the dataset, demonstrating the questions crafted for every sample. { \" id \": < unique_identifier >, \" image \": < image_path > , \" conversations \": [ { }, { \" from \": \" human \", \" value \": < question > \" from \": \" gpt \", \" value \": < ground truth > }, ... { \" from \": \" human \", \" value \": < question > \" from \": \" gpt \", \" value \": < ground truth > ] }, { } } Listing 1: Data format for VaseVQA annotations. 3."
        },
        {
            "title": "Image Collection",
            "content": "The images in this dataset were collected through collaborations with Ancient Greek archaeological institutions, museums, and cultural heritage centers (car). We focused on gathering images of classical funerary vases that are commonly found in Ancient Greek archaeological sites, ensuring diverse representation of artifacts across different cultural groups. The images include both complete objects and fragments, as well as images of the vases in their original burial contexts. This collection was designed to capture intricate details of materials, craftsmanship, and regional variations on the Table 3."
        },
        {
            "title": "3.2 Text Collection",
            "content": "The textual data for the dataset was derived from several key sources, including academic papers, archaeological reports, and expert annotations provided by Ancient Greek historians and cultural heritage experts. The texts are descriptions of the artifacts, detailing their material composition (such as red pottery or glazed ceramics), motifs (such as human, animal, or abstract designs), and the archaeological context (such as burial sites or ceremonial uses). These descriptions were translated and structured to align with the images and make the data accessible for vision-language tasks."
        },
        {
            "title": "3.3 Annotation",
            "content": "The dataset was labeled by team of archaeologists and cultural heritage experts, who annotated the"
        },
        {
            "title": "Venue",
            "content": "OE/MC DAQUAR(Malinowski and Fritz, 2014) COCO-QA(Ren et al., 2015b) VAQ V1.0(Agrawal et al., 2017) VQA V2.0(Goyal et al., 2017) CVR(Zellers et al., 2019) GQA(Hudson and Manning, 2019) RAVEN(Zhang et al., 2019) NLVR(Suhr et al., 2017) OK-VQA(Marino et al., 2019) VizWiz(Gurari et al., 2018) KVQA(Shah et al., 2019) CLEVR(Johnson et al., 2017) FM-IQA(Gao et al., 2015) NLVR2(Suhr and Artzi, 2019) TextVQA(Singh et al., 2019) FVQA(Wang et al., 2017) VISUAL GENOME(Krishna et al., 2017) VQA-CP(Agrawal et al., 2018) Visual Madlibs(Yu et al., 2015) SHAPES(Andreas et al., 2015) KB-VQA(Wang et al., 2015) ICQA(Hosseinabad et al., 2021) DVQA(Kafle et al., 2018) PathVQA(He et al., 2020) Visual7w(Zhu et al., 2016b) KRVQA(Cao et al., 2021) 1,449 123,287 204k 204k 110k 113,018 1,120,000 387,426 14,031 - 24K 100,000 158,392 107,292 28,408 2190 108,000 10,738 15,616 700 42,021 3,000,000 4,998 47,300 32,910 12,468 117,684 614K 1.1M 290K 22,669,678 70,000 31,418 14,055 31,173 999,968 316,193 29,680 45,336 5826 145, 360,001 244 2402 260,840 3,487,194 32,795 327,939 157,"
        },
        {
            "title": "VaseVQA",
            "content": "11,693 93,544 4 4 - - - 4 - VQA - 90 - - 12 7 12 23 - 3 7 7 8 Natural Natural Natural Natural Natural Natural Natural Synthetic Natural Natural Natural Natural Natural Synthetic Natural Natural Natural Natural Natural Synthetic Natural Synthetic - - Natural Natural"
        },
        {
            "title": "Natural",
            "content": "VQA VQA VQA VQA VR VR VR VR VQA - - VR - VR VQA VR VR VQA VR VR VQA VQA VQA VQA VQA VR"
        },
        {
            "title": "VQA",
            "content": "NIPS 2014 - ICCV 2015 CVPR 2017 CVPR 2019 CVPR 2019 CVPR 2019 ACL 2019 CVPR 2019 CVPR 2018 AAAI 2019 CVPR 2017 CVPR 2017 ACL 2019 CVPR 2019 CVPR 2019 - CVPR 2018 - - IJCAI 17 - - - CVPR 16 - -"
        },
        {
            "title": "OE\nOE\nOE\nBoth\nMC\nOE\nMC\nOE\nOE\nOE\nOE\nOE\nOE\nOE\nOE\nOE\nOE",
            "content": "- Binary OE OE OE MC MC MC OE Table 1: Main characteristics of major VQA and Visual Reasoning datasets."
        },
        {
            "title": "Split\nTrain\nTest",
            "content": "Images 9,354 2,"
        },
        {
            "title": "Total",
            "content": "11,"
        },
        {
            "title": "Categories",
            "content": "8 8 8 74,832 18,712 93,544 Material, Technique, Shape, Origin, Date, Decoration, Attribution, General Table 2: Data splits of the VQA dataset on ancient Greek vase attributes, showing the number of images, question types per image, total questions, and simplified attribute categories: Material, Technique, Shape, Origin, Date, Decoration, Attribution, Genera. images with several key attributes. These include the material (e.g., red pottery, glazed ceramics), pattern type (e.g., human figures, animal motifs, abstract symbols), excavation layer, radiocarbon dating estimates, manufacturing techniques (e.g., hand-built, wheel-thrown, firing temperature), and the contextual use of the object (e.g., funerary or ceremonial). The experts also identified restoration marks, if applicable. As illustrated in Listing 1, the JSON data structure presents the requirement for vision-language annotations. The labeling process ensures high level of detail and accuracy in reflecting the cultural and historical context of each artifact."
        },
        {
            "title": "4 Methods\nOn the vase dataset (ancient Greek pottery recog-\nnition), given a tuple (x, q, a∗) with image x, ques-\ntion q, and ground-truth answer a∗, we build on the\nSFT reference policy πref and activate its reasoning",
            "content": "ability via reinforcement learning (RL), learning policy πθ(ˆa x, q) that attains expert-level accuracy and compositional robustness. Concretely, we employ type-conditioned reward derived from post-SFT error diagnosis while constraining drift from πref: max θ E(x,q,a) Eˆaπθ(x,q) (cid:2) R(q, ˆa, a)(cid:3) (1) λ KL(πθ πref) . To obtain stable updates that are insensitive to crosstype reward-scale differences, we adopt Group Relative Policy Optimization (GRPO): for each prompt (x, q), we sample completions {ˆa(k)}K k=1 and compute rewards r(k), form per-prompt baseline = 1 k=1 r(k), and define group-relative adK vantages (cid:80)K A(k) = r(k) r. (2) We then optimize clipped PPO-style surrogate using these A(k), together with the KL regularizer in Vase example:"
        },
        {
            "title": "User\nVaseVL",
            "content": "What is the fabric of the vase? The fabric of the vase is ATHENIAN. What is the technique of the vase? The technique of the vase is RED-FIGURE. What is the shape name of the vase? The shape name of the vase is CUP B. What is the provenance of the vase? The provenance of the vase is not available. What is the date of the vase? The date of the vase is -450 to -400. What is the attribution of the vase? The vase is attributed to CODRUS by BURN CODRUS by UNKNOWN. What is the decoration of the vase? The decoration of the vase is A,B: THEATRICAL, DRAPED SATYRS, WITH STORK, BOX AND SANDAL, ARYBALLOI, OINOCHOE, LYRE AND STAFFS, ONE CONFRONTING DRAPED YOUTH I: AMAZON ON HORSEBACK. Table 3: Example of VaseVQA Dataset Table: Eight attribute-specific questions (fabric, technique, shape, provenance, date, attribution, decoration, and overall details) paired with visual input, presented in conversational Q&A format to analyze an Athenian red-figure cup (450400 BCE) attributed to the Codrus Painter. Equation (1), thereby suppressing drift while effectively enhancing post-SFT reasoning. An overview of the complete training pipeline is illustrated in Figure 3."
        },
        {
            "title": "4.1 Reward Design\nTo guide the model’s optimization, we formulate a\ncomprehensive reward function. This function is\ndesigned to be sensitive to the shortcomings of the\nSupervised Fine-Tuning (SFT) model and adapt-\nable to different types of questions.",
            "content": "Our reward function is built upon two fundamental metrics that assess the quality of the generated answer, ˆa, against the ground-truth reference, a, from both lexical and semantic perspective. Keyword-based Score (skw): This component measures the lexical overlap of key terms, prioritizing factual correctness. skw = K(ˆa) K(a) K(ˆa) K(a) (3) Semantic Similarity Score (ssem): This component evaluates the semantic alignment between the prediction and the reference using vector embeddings, capturing the contextual meaning. ssem = cos(f (ˆa), (a)) + 1 2 (4) where ˆa is the models predicted answer. is the ground-truth answer. K() is function that extracts set of keywords from given text. () is an embedding function that maps text to its semantic vector representation."
        },
        {
            "title": "4.2 Reward Shaping",
            "content": "Recognizing that the importance of lexical accuracy versus semantic correctness varies across different question types, we combine the fundamental components using adaptive weights determined by the question q. R(q) = β1(q)skw + β2(q)ssem (5) where the weight vector (β1(q), β2(q)) is conditioned on the question type. For example, for factual questions where precision is critical, skw may be weighted more heavily, while for descriptive questions, the focus may shift to ssem. To intensify the training focus on specific areas where the SFT model is known to perform poorly, Figure 3: Overall framework of VaseVL. The proposed pipeline integrates supervised fine-tuning (SFT) with reinforcement learning under the Group Relative Policy Optimization (GRPO) paradigm. Given vase image x, question q, and the reference answer a, the model refines its reasoning ability by balancing lexical and semantic rewards while constraining policy drift from πref. an additional weighting factor can be applied to amplify the reward signal for these challenging question types. R(q) = w(q) R(q) where the weight w(q) is defined as: w(q) = (cid:40) > 1, 1, if Cshort; otherwise. (6) (7) Here, Cshort represents the predefined set of shortcoming-prone question types that require targeted improvement."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct series of experiments to validate the effectiveness of our proposed model, VaseVL. We evaluate our model on the newly introduced VaseVQA benchmark and compare its performance against various strong baselines, including general-purpose Multimodal Large Language Models (MLLMs) and fine-tuned version of the model without reinforcement learning. Our evaluation is structured around the taxonomy of seven distinct question types to provide granular analysis of the models capabilities in expert-level reasoning."
        },
        {
            "title": "5.1 Evaluation Metrics",
            "content": "Given the diverse nature of the questions in our VaseVQA benchmark, single metric is insufficient to capture the full spectrum of model performance. Therefore, we employ tailored evaluation protocol that applies the most appropriate metric for each question type, as implemented in our evaluation script. Accuracy (ANLS-based) For factual, shortanswer questions such as Fabric, Technique, Shape, Provenance, and Attribution, we use an accuracy metric based on Average Normalized Levenshtein Similarity (ANLS). This \"soft\" accuracy measure, inspired by the ST-VQA benchmark, is robust to minor character-level variations and OCR-like errors, making it more suitable than exact string matching for evaluating factual correctness. Date Accuracy For the Date question, we utilize specialized accuracy evaluator designed to parse and compare date information. This metric correctly handles various date formats and ranges (e.g., \"550-525 BC\"), ensuring fair assessment of the models ability to extract temporal information. BLEU@1 Score For the descriptive Decoration question, where answers are longer and more varied, we use the BLEU@1 score. This metric mea-"
        },
        {
            "title": "Model",
            "content": "Param."
        },
        {
            "title": "Accuracy",
            "content": "Bleu@"
        },
        {
            "title": "Fabric Technique Shape Provenance Date Attribute Decoration Overall",
            "content": "Qwen2-VL Qwen2-VL-instruct Qwen2.5-VL-instruct Qwen2-VL LLaVA Vicuna MiniCPM Qwen-2.5-VL Qwen-2.5-VL 0.5B 2B 3B 7B 7B 7B 8B 3B 7B 10.50 - - 1.69 11.56 - 0.29 0.29 0.00 Zero-shot 0.08 30.60 18.75 24.00 - - 0.03 0.02 0.00 3.97 6.03 3.02 37.66 44.60 0.24 - 0.14 0. VaseVL (Ours) 3B 99.95 95.93 83.99 41.52 58.62 58.62 21.24 - - 0.97 0.00 0. 73.67 - 0.65 0.65 - - - - 14.86 17.95 14.10 60.99 61.21 2.85 28.41 - 0.14 0.00 0.00 39.87 60.83 3.41 1.94 1.54 2.00 6.28 1.14 1.15 2.29 1. 9.82 0.18 1.80 1.66 0.93 4.78 1.22 2.52 2.55 3.00 75.71 Table 4: Performance comparison on the VaseVQA benchmark."
        },
        {
            "title": "Model",
            "content": "Param."
        },
        {
            "title": "Accuracy",
            "content": "Bleu@"
        },
        {
            "title": "Fabric Technique Shape Provenance Date Attribute Decoration Overall",
            "content": "Qwen-2.5-VL Qwen2.5-VL-SFT VaseVL (Ours) 3B 3B 3B 13.33 99.96 99.95 19.95 94.99 95.93 14.82 83.98 83.99 5.27 71.67 73. 3.58 37.96 39.87 11.50 56.96 60.83 4.82 2.57 9.82 11.41 74.25 75.71 Table 5: Ablation study results. After designing task-specific prompts (refer to Appendix), Qwen2.5-VL serves as the zero-shot baseline. Qwen2.5-VL-SFT denotes the SFT version of the preceding model, while the final row reports the performance of our proposed VaseVL. sures the precision of individual words (unigrams) between the generated answer and the ground truth. It effectively evaluates the presence of key descriptive terms without overly penalizing stylistic differences in sentence structure, serving as proxy for compositional understanding."
        },
        {
            "title": "5.2\nWe initialize from a general-purpose MLLM and\ninstruction-tune on D = (xi, qi, a∗\nthe\nSFT objective is",
            "content": "i )i = 1N . LSFT(θ) = , (x,q,a)D (cid:88) t=1 log πθ! (cid:16) tx, q, <t (cid:17) . (8) The SFT model πref is used both as stable reference for RL and as probe to evaluate per-type performance over (e.g., Fabric, Technique), from which we select shortcoming subset Cshort! !T for targeted improvement. For SFT we adopt standard LoRA recipe (rank 8, cutoff length 1024, perdevice batch 1 with 8 gradient accumulation, cosine schedule with learning rate 1104, 1 epochs, warmup ratio 0.1, bf16 enabled). After SFT, we perform RL focused on Cshort with GRPO-style setup: 8 rollouts per prompt, temperature 0.9, one 7 iteration per batch with KL penalty coefficient 0.04, training for 2 epochs at learning rate 1106."
        },
        {
            "title": "5.3 Main Results",
            "content": "Performance against Baselines We first evaluate the zero-shot performance of various generalpurpose MLLMs on our VaseVQA benchmark. As shown in Table 4, prominent models including Qwen-VL, LLaVA, and MiniCPM exhibit limited proficiency in this specialized domain. Most models struggle to provide accurate answers, with scores often near zero for expert-level questions like Attribution and Provenance. This highlights significant domain gap and underscores the necessity of domain-specific fine-tuning for tasks requiring deep cultural heritage knowledge. Ablation Study To validate our proposed SFTthen-RL approach, we conduct an ablation study, with results presented in Table 5. We establish three key models for comparison: zero-shot baseline using tailored prompt (Qwen-2.5-VL), strong supervised fine-tuned baseline (Qwen2.5VL-SFT), and our full model (VaseVL). The SFT baseline substantially improves over the zero-shot model, achieving high accuracy on factual recall tasks such as Fabric (99.96%) and Technique (94.99%). However, its performance on more complex reasoning tasks remains constrained. Our full model, VaseVL, which incorporates the diagnosis-guided reinforcement learning stage, improves upon the SFT-only baseline. While maintaining strong performance on factual questions, VaseVL shows its key advantages in the areas targeted by our reward engineering. Notably, the score for Attribution improves from 56.96% to 60.83%, demonstrating enhanced reasoning for historical attribution. more pronounced improvement is seen in the Decoration task, where the BLEU@1 score increases from 2.57 to 9.82. This gain validates that the RL phase effectively remedies the SFT models weakness in compositional and descriptive capabilities, confirming that our approach successfully \"turns evaluation into supervision.\""
        },
        {
            "title": "6 Social Impact",
            "content": "Building foundational Multimodal Large Language Model (MLLM) agent for Ancient Greek Pottery is essential for preserving and promoting this invaluable cultural heritage. Ancient Greek pottery serves as crucial historical record, offering insights into the daily lives, artistic expressions, and mythological narratives of ancient Greek civilization. The durability of pottery, even when fragmented, makes it one of the most significant archaeological artifacts for understanding the chronological evolution of ancient Greece. The complex visual and textual data associated with Greek pottery, including stylistic variations, inscriptions, and production techniques, lend themselves to an advanced AI-driven approach to support accurate documentation, analysis, and accessibility. Models such as VaseVL, the first MLLM agent dedicated to Ancient Greek Pottery, can play pivotal role in cultural preservation and education. By integrating multimodal learning, VaseVL enables detailed visual recognition of pottery styles, shapes, and decorative techniques while contextualizing them with historical and textual information. This capability not only aids archaeologists and historians in identifying and classifying pottery more efficiently but can also enhance public engagement by making Greek pottery more accessible to scholars, educators, and enthusiasts worldwide. The social impact of VaseVL extends beyond academic research. By digitizing and analyzing vast collections of Greek pottery, VaseVL contributes to global heritage conservation efforts, mitigating the risk of cultural erosion. Additionally, it can aid in the detection of illicit trade and forgery of ancient artifacts by providing authentication tools based on stylistic and compositional analysis. Museums and educational institutions can leverage VaseVL to create interactive learning experiences, fostering deeper appreciation for ancient Greek culture. In summary, VaseVL represents transformative step in the integration of AI with cultural heritage preservation. By harnessing the power of MLLMs, it ensures that the artistic, historical, and archaeological significance of Ancient Greek pottery is not only safeguarded but also made more accessible and comprehensible for future generations."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we address the challenge of equipping Multimodal Large Language Models with robust, expert-level reasoning for the specialized domain of ancient Greek pottery. We introduced VaseVL, an SFT-then-RL system that turns evaluation into supervision by diagnosing the performance gaps of fine-tuned model and targeting these weaknesses with type-conditioned, compositionality-oriented rewards. Our experiments, conducted on the newly created VaseVQA benchmark, validate this approach. The results demonstrate that VaseVL improves upon strong SFT-only baseline, particularly in the challenging tasks of historical attribution and compositional description, which were the explicit targets of our reward engineering. This study not only provides state-of-the-art model for cultural heritage analysis but also demonstrates the efficacy of diagnosis-guided reward engineering as methodology for building expert MLLMs. The release of our VaseVQA benchmark further contributes valuable resource for future research in specialized-domain multimodal understanding."
        },
        {
            "title": "References",
            "content": "Beazley archive pottery database. Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018. Dont just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 49714980. 8 Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. 2017. C-vqa: compositional split of the visual question answering (vqa) v1. 0 dataset. arXiv preprint arXiv:1704.08243. Gong Cheng, Junwei Han, and Xiaoqiang Lu. 2017. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):18651883. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. Deep compositional question answering with neural module networks. arXiv preprint corr abs/1511.02799 (2015). arXiv:1511.02799. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. 2015 IEEE International Conference on Computer Vision (ICCV), IEEE, pages 24252433. Yu Beibei, Shen Tao, Na Hongbin, Chen Ling, and Li Denqi. 2023. Mineagent: Towards remote-sensing mineral exploration with multimodal large language models. arXiv:2412.17339. Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle Alexander, David Jacobs, and Peter Belhumeur. 2014. Birdsnap: Large-scale fine-grained visual categorization of birds. In CVPR, pages 20112018. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101mining discriminative components In ECCV, pages 446461. with random forests. Springer. Zheng Boyuan, Gou Boyu, Kil Jihyung, Sun Huan, and Su Yu. 2024. Gpt-4v(ision) is generalist web agent, if grounded. arXiv:2401.01614. Qingxing Cao, Bailin Li, Xiaodan Liang, Keze Wang, and Liang Lin. 2021. Knowledge-routed visual question reasoning: Challenges for deep representation embedding. IEEE Transactions on Neural Networks and Learning Systems. Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. 2019. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987. Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, XiuYi Chen, Jing Shi, Shuang Xu, and Bo Xu. 2023. Vlp: survey on vision-language pre-training. Machine Intelligence Research, 20(1):3856. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104120. Springer. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. 2014. Describing textures in the wild. In CVPR, pages 3606 3613. Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, pages 215223. JMLR Workshop and Conference Proceedings. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. 2016. The cityscapes dataset for semantic urban scene understanding. In CVPR, pages 32133223. Romero David and Solorio Thamar. 2024. Questioninstructed visual descriptions for zero-shot video question answering. arXiv:2402.10698v2. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In CVPR, pages 248 255. Ieee. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc) challenge. IJCV, 88(2):303338."
        },
        {
            "title": "Liu",
            "content": "Fangyu,"
        },
        {
            "title": "Bugliarello",
            "content": "Ponti Edoardo Maria, Reddy Siva, Collier Nigel, and Elliott Desmond. 2021. Visually grounded reasoning across languages and cultures. arXiv:2109.13238v2. Emanuele, Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR workshop, pages 178178. IEEE. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to machine? dataset and methods for multilinarXiv preprint gual image question answering. arXiv:1505.05612. Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, pages 33543361. IEEE. Ian Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, and 1 others. 2013. Challenges in representation learning: report on three machine learning contests. In ICONIP, pages 117124. Springer. 9 Gemini Team Google. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Agrim Gupta, Piotr Dollar, and Ross Girshick. 2019. Lvis: dataset for large vocabulary instance segmentation. In CVPR, pages 53565364. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 36083617. Zheng Hao, Kong Hao, Zheng Jia, Zhou Weixiang, Lin Hongyu, Lu Yaojie, He Ben, Han Xianpei, and Sun Le. 2025. Pptagent: Generating and evaluating presentations beyond text-to-slides. arXiv preprint arXiv:2501.03936v3. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. JSTARS, 12(7):22172226. Sayedshayan Hashemi Hosseinabad, Mehran Safayani, and Abdolreza Mirzaei. 2021. Multiple answers to question: new approach for visual question answering. The Visual Computer, 37(1):119131. Ting Huang, Zeyu Zhang, and Hao Tang. 2025. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478. Drew Hudson and Christopher Manning. 2019. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. 2017. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. NeurIPS, 33:26112624. Jonathan Krause, Jia Deng, Michael Stark, and Li FeiFei. 2013. Collecting large-scale dataset of finegrained cars. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, and 1 others. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123(1):3273. Alex Krizhevsky, Geoffrey Hinton, and 1 others. 2009. Learning multiple layers of features from tiny images. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324. Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Yong Jae Lee, Houdong Hu, Zicheng Liu, and 1 others. 2022. Elevater: benchmark and toolkit for evaluating language-augmented visual models. arXiv preprint arXiv:2204.08790. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:96949705. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV, pages 740 755. Springer. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems, 36. Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. 2025. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. 2013. Fine-grained arXiv preprint visual classification of aircraft. arXiv:1306.5151. 10 Mateusz Malinowski and Mario Fritz. 2014. multiworld approach to question answering about realworld scenes based on uncertain input. Advances in neural information processing systems, 27:1682 1690. Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: KnowledgeIn Proceedings aware visual question answering. of the AAAI Conference on Artificial Intelligence, volume 33, pages 88768884. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31953204. Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. Rareact: arXiv video dataset of unusual interactions. preprint arXiv:2008.01018. Anand Mishra, Karteek Alahari, and CV Jawahar. 2012. Scene text recognition using higher order language priors. In BMVC-British machine vision conference. BMVA. Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, NamGyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. 2014. The role of context for object detection and semantic segmentation in the wild. In CVPR, pages 891898. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. 2011. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011. Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated flower classification over large number of classes. In ICVGIP, pages 722729. IEEE. Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. 2012. Cats and dogs. In CVPR, pages 34983505. IEEE. Qwen-Team. 2024. Qvq: To see the world with wisdom. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR. Mengye Ren, Ryan Kiros, and Richard Zemel. 2015a. Exploring models and data for image question answering. arXiv:1505.02074. Mengye Ren, Ryan Kiros, and Richard Zemel. 2015b. Exploring models and data for image question answering. Advances in neural information processing systems, 28:29532961. Jingwei Shi, Zeyu Zhang, Biao Wu, Yanjie Liang, Meng Fang, Ling Chen, and Yang Zhao. 2025. Presentagent: Multimodal agent for presentation video generation. arXiv preprint arXiv:2507.04036. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83178326. Tyler Jo Smith, Ethan Gruber, and Nicholas Harokopos. 2024. 22 kerameikos. org and digital accessibility for ancient greek vases. Technology, Crafting and Artisanal Networks in the Greek and Roman World: Interdisciplinary Approaches to the Study of Ceramics, page 255. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In EMNLP, pages 16311642. Zirui Song, Qian Jiang, Mingxuan Cui, Mingzhe Li, Lang Gao, Zeyu Zhang, Zixiang Xu, Yanbo Wang, Chenxi Wang, Guangxian Ouyang, and 1 others. 2025a. Audio jailbreak: An open comprehensive benchmark for jailbreaking large audio-language models. arXiv preprint arXiv:2505.15406. Zirui Song, Guangxian Ouyang, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Fu Yujie, Zeyu Zhang, Shiyu Jiang, Miao Fang, and 1 others. 2025b. Hazards in daily life? enabling robots to proactively In Proceedings of detect and resolve anomalies. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 73997415. Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, and 1 others. 2025c. Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large vision-language models. arXiv preprint arXiv:2505.16517. Zirui Song, Jingpu Yang, Yuan Huang, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, and Xiuying Chen. 2025d. Geolocation with real human gameplay data: large-scale dataset and human-like reasoning framework. arXiv preprint arXiv:2502.13759. Xuan Ren and Lingqiao Liu. 2023. You can generate it again: Data-to-text generation with verification and correction prompting. arXiv preprint arXiv:2306.15933. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402. 11 Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2011. The german traffic sign recognition benchmark: multi-class classification competition. In IJCNN, pages 14531460. IEEE. Licheng Yu, Eunbyung Park, Alexander Berg, and Tamara Berg. 2015. Visual madlibs: Fill in the blank image generation and question answering. arXiv preprint arXiv:1506.00278. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67206731. Yan Zeng, Xinsong Zhang, and Hang Li. 2021. Multi-grained vision language pre-training: AlignarXiv preprint ing texts with visual concepts. arXiv:2111.08276. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. 2019. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53175327. Jinman Zhao, Zifan Qian, Linbo Cao, Yining Wang, Yitian Ding, Yulan Hu, Zeyu Zhang, and Zeyong Jin. 2024. Role-play paradox in large language models: reasoning performance gains and ethical dilemmas. arXiv preprint arXiv:2409.13979. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. 2017. Scene In CVPR, pages parsing through ade20k dataset. 633641. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li FeiFei. 2016a. Visual7w: Grounded question answering in images. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, page 49955004. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li FeiFei. 2016b. Visual7W: Grounded Question Answering in Images. In IEEE Conference on Computer Vision and Pattern Recognition. Alane Suhr and Yoav Artzi. 2019. Nlvr2 visual bias analysis. arXiv preprint arXiv:1909.10411. Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217223. Theodore Tsesmelis, Luca Palmieri, Marina Khoroshiltseva, Adeela Islam, Gur Elkin, Ofir Itzhak Shahar, Gianluca Scarpellini, Stefano Fiorini, Yaniv Ohayon, Nadav Alali, Sinem Aslan, Pietro Morerio, Sebastiano Vascon, Elena Gravina, Maria Cristina Napolitano, Giuseppe Scarpati, Gabriel Zuchtriegel, Alexandra Spühler, Michel E. Fuchs, and 4 others. 2024. Re-assembling the past: The repair dataset and benchmark for real world 2d and 3d puzzle solving. arXiv:2410.24010. Bastiaan Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. 2018. Rotation equivariant cnns for digital pathology. In International Conference on Medical image computing and computerassisted intervention, pages 210218. Springer. Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):2413 2427. Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. 2015. Explicit knowledgebased reasoning for visual question answering. arXiv preprint arXiv:1511.02570. Pengjie Wang, Kaile Zhang, Xinyu Wang, Shengwei Han, Yongge Liu, Jinpeng Wan, Haisu Guan, Kuang Zhebin, Lianwen Jin, Xiang Bai, and Yuliang Liu. 2024. An open dataset for oracle bone script recognition and decipherment. arXiv:2401.15365. Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE. Liao Xinyao, Zeng Xianfang, Wang Liao, Yu Gang, Lin Guosheng, and Zhang Chi. 2025. Motionagent: Finegrained controllable video generation via motion field agent. arXiv:2502.03207. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:6778. Task Dataset Year Classes Training Testing Evaluation Metric et et et et al., al., (Deng (Fei-Fei (Krizhevsky MNIST (LeCun et al., 1998) [link] al., Image Classification Caltech-101 2004) [link] PASCAL VOC 2007 Classification (Everingham et al., 2010) [link] Oxford 102 Folwers (Nilsback and Zisserman, 2008) [link] CIFAR-10 2009) [link] CIFAR-100 (Krizhevsky et al., 2009) [link] ImageNet-1k 2009) [link] SUN397 (Xiao et al., 2010) [link] SVHN (Netzer et al., 2011) [link] STL-10 (Coates et al., 2011) [link] GTSRB al., (Stallkamp 2011) [link] KITTI Distance (Geiger et al., 2012) [link] IIIT5k (Mishra et al., 2012) [link] Oxford-IIIT PETS (Parkhi et al., 2012) [link] Stanford Cars 2013) [link] FGVC Aircraft 2013) [link] RecogniFacial tion 2013 (Goodfellow et al., 2013) [link] Rendered SST2 (Socher et al., 2013) [link] Describable Textures (DTD) (Cimpoi et al., 2014) [link] Food-101 (Bossard 2014) [link] Birdsnap (Berg et al., 2014) [link] RESISC45 2017) [link] CLEVR Counts (Johnson et al., 2017) [link] PatchCamelyon (Veeling et al., 2018) [link] EuroSAT (Helber et al., 2019) [link] Hateful Memes al., 2020) [link] Country211 2021) [link] (Radford Emotion (Krause (Cheng (Kiela (Maji al., al., al., al., al., et et et et et et Image-Text Retrieval Action Recognition Object Detection Semantic Segmentation Flickr30k (Young et al., 2014) [link] COCO Caption (Chen et al., 2015) [link] UCF101 (Soomro et al., 2012) [link] Kinetics700 al., 2019) [link] RareAct (Miech et al., 2020) [link] (Carreira et COCO 2014 Detection (Lin et al., 2014) [link] COCO 2017 Detection (Lin et al., 2014) [link] LVIS (Gupta et al., 2019) [link] ODinW (Li et al., 2022) [link] PASCAL VOC 2012 Segmentation (Everingham et al., 2010) [link] PASCAL Content (Mottaghi et al., 2014) [link] Cityscapes 2016) [link] ADE20k (Zhou et al., 2017) [link] (Cordts al., et"
        },
        {
            "title": "VaseVQA",
            "content": "1998 2004 2007 2008 2009 2009 10 20 102 10 100 60,000 3,060 5, 2,040 10,000 6,085 Accuracy Mean Per Class 4,952 11-point mAP 6, Mean Per Class 50,000 10,000 Accuracy 50,000 10, Accuracy 2009 1000 1,281,167 50,000 Accuracy 2010 2011 2011 2011 2012 2012 2012 2013 2013 2013 2014 2014 2014 2017 2017 2019 2020 397 10 10 43 4 36 37 196 8 2 47 102 500 45 2 10 2 19,850 73,257 1,000 26,640 6,770 2,000 3,680 8, 6,667 19,850 26,032 8,000 12,630 Accuracy Accuracy Accuracy Accuracy 711 Accuracy 3,000 3, Accuracy Mean Per Class 8,041 Accuracy 3,333 Mean Per Class 32, 3,574 Accuracy 7,792 3,760 1,821 Accuracy 1,880 Accuracy 75,750 25,250 Accuracy 42,283 3, 2,149 25,200 Accuracy Accuracy 2,000 500 Accuracy 294, 32,768 Accuracy 10,000 8,500 5,000 500 Accuracy ROC AUC 211 43,200 21,100 Accuracy 2014 2015 2012 2020 2014 2017 2019 2022 2012 2016 2017 2025 - - 101 700 80 80 1203 314 20 459 150 8 31,783 82,783 - 5,000 Recall Recall 9,537 494, 1,794 31,669 Accuracy Mean(top1, top5) 7,607 - mWAP, mSAP 83, 41,000 box mAP 118,000 5,000 box mAP 118,000 5,000 20070 box mAP box mAP 1464 1449 mIoU 2975 25574 9534 5105 mIoU mIoU 2000 2339 mIoU Accuracy & BLEU Table 6: Summary of the widely-used visual recognition datasets for VLM evaluation. [link] directs to dataset websites."
        }
    ],
    "affiliations": [
        "AI Geeks",
        "Australian Artificial Intelligence Institute",
        "La Trobe University"
    ]
}