{
    "paper_title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
    "authors": [
        "Chenrui Fan",
        "Ming Li",
        "Lichao Sun",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 1 5 6 0 . 4 0 5 2 : r Preprint. Under review. Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill? Chenrui Fan1*, Ming Li1*, Lichao Sun2, Tianyi Zhou1 1University of Maryland; 2Lehigh University {cfan24, minglii, tianyi}@umd.edu Project: https://github.com/tianyi-lab/MiP-Overthinking The Answer to the Great Question... Of Life, the Universe and Everything... is... Forty-two, said Deep Thought, with infinite majesty and calm. The Hitchhikers Guide to the Galaxy"
        },
        {
            "title": "Abstract",
            "content": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to large extent, which we name as the MiP-Overthinking. Such failures are against the test-time scaling law but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem."
        },
        {
            "title": "Introduction",
            "content": "Reasoning abilities in large language models (LLMs) have become cornerstone of advanced AI applications (Huang & Chang, 2023; Li et al., 2024; Ahn et al., 2024; Wang et al., 2025), powering breakthroughs in mathematical reasoning (Xiong et al., 2025; Xia et al., 2025), code generation (Liu et al., 2024), and commonsense question answering (Wang & Zhao, 2023). These gains often stem from the scaling law of model/dataset sizes (Kaplan et al., 2020) in both pre-training (Shao et al., 2024) and post-training, which unlock emergent capabilities such as step-by-step reasoning and reflection skills witnessed on OpenAIs GPT-o1 (OpenAI, 2024b) and the open-source DeepSeekR1 (DeepSeek-AI et al., 2025). By leveraging supervised fine-tuning (SFT) on expert responses (Ye et al., 2025; Muennighoff et al., 2025) and/or reinforcement learning (RL) (DeepSeek-AI et al., 2025), these models are tailored to produce detailed multi-step reasoning paths, whose length increase usually associated with improved performance on complex tasks such as math reasoning and programming. Despite the fascinating reasoning capabilities exhibited on recent models, there is growing concern about the efficiency and quality of the long reasoning process (Sui et al., 2025). Chen et al. (2025b) first raises the overthinking problem in reasoning LLMs, which is reflected by the excessively long reasoning paths generated for extremely simple queries. For example, even for questions like What is the answer of 2 plus 3?, existing reasoning models might generate hundreds of response tokens. *Equal Contribution. 1 Preprint. Under review. Figure 1: Illustration of MiP-Overthinking. When queried by questions with missing premises, the response length of reasoning models increases excessively, and they cannot abstain from answering with MiP identified. The left shows query with an undefined variable, while the right compares well-defined GSM8K question with its MiP variant (with critical numerical condition removed). Reasoning models responses to MiP questions are much longer than those for well-defined questions and those generated by non-reasoning models. The left corner of each response report the response length and thinking time by DeepSeek-R1. In particular, the ill-posed queries are unsolvable due to the lack of necessary premise or condition. We call the reasoning failure for the ill-posed queries Overthinking under Missing Premise (MiPOverthinking). For example, the simplest MiP question is What is the value of a? 1 , as shown on the left part of Figure 1. Without providing any other information regarding a, it is evidently unsolvable. However, DeepSeek-R1 generates thousands of tokens and spends several minutes thinking about this question before outputting the final meaningless answer. In this paper, we find that trivial type of ill-posed queries will significantly exacerbate the overthinking of reasoning models, resulting in excessively redundant and meaningless thinking. In contrast, humans and even non-reasoning models are often immune to such scenarios and quickly end up by questioning the validity of the given query, indicating the critical thinking capability. This exposes risk of the abuse of thinking patterns and lack of critical thinking on the models trained for deep thinking. Ideally, model with critical thinking skills is expected to identify the missing premise and quickly respond by either requesting clarification or gracefully indicating that it cannot proceed (Cole et al., 2023; Amayuelas et al., 2024). MiP-Overthinking differs from the widely discussed overthinking issue (Cuadron et al., 2025), in which the query is usually well-defined, but model applies much more reasoning than necessary for little benefit. MiP-Overthinking, by contrast, happens when the question itself is ill-posed and lacks sufficient information to be solved. For example, the right of Figure 1 presents well-defined question from GSM8K and MiP variant, where the latter triggers drastic increase of the generated tokens on recent reasoning models compared with the general overthinking. Overthinking can be presented by the length difference between models addressing the same well-defined questions, while MiPOverthinking can be presented by the additional tokens generated due to MiP. MiP-Overthinking further reveals the lack of critical thinking that questions the validity of ill-posed questions and quickly identifies MiP, thus abstaining from answering the questions. Moreover, we observe that reasoning models ineffective and redundant thinking often cannot stop even after successful notice of MiP, violating the expectation of test-time scaling law. Hence, MiP-Overthinking indicates potential drawbacks of current training recipes of reasoning models. To systematically investigate this issue, we construct suite of MiP questions designed to trigger the overthinking failures in controlled way. These include synthetic questions generated by Rule-based Formula (queries where formula reference is empty or nonsensical) and careful modifications of established datasets across diverse levels of difficulties, including SVAMP, GSM8K, and MATH500. On the modified datasets of MiP questions, we empirically evaluate wide range of state-of-the-art 1In The Hitchhikers Guide to the Galaxy, the supercomputer Deep Thought spends hundreds of years to answer the the Ultimate Question of Life, the Universe, and Everything as 42, and we observe that DeepSeek-R1 spends thousands of tokens to answer What is the value of as 2, which we find them interestingly alike. 2 Preprint. Under review. LLMs, from reasoning models to non-reasoning models and from open-sourced models to proprietary models, to ensure the generalizability of our findings. Our analysis is mainly based on three evaluation metrics, the length of generated responses, the accuracy on well-defined questions, and the abstain rate on ill-posed questions with MiP. Main Contributions: We present the first in-depth study of Overthinking under Missing Premise (MiP-Overthinking), which reveals critical shortcoming in existing reasoning models: Although they appear to follow coherent reasoning patterns, they lack genuine critical thinking capabilities. To systematically analyze this issue, we curate four MiP datasets covering various difficulty levels and three ill-posed question generation strategies, i.e., Rule-Based Generation, Body-Question Swapping, and Essential-Premise Removal. We then evaluate wide range of large language models including reasoning-based and non-reasoning ones. Our empirical results illuminate the differences in how models handle well-defined vs. MiP questions, ultimately offering insights into the limitations of existing reasoning models. Our key findings: 1. Missing premise in questions induces reasoning models to generate significantly longer (2 to 4 more tokens) responses than general overthinking on well-defined questions. The increased tokens fail to help identify MiP in the ill-posed questions, surprisingly contradicting the widelydiscussed test-time scaling law. 2. In contrast, given MiP questions, non-reasoning models generate consistently shorter responses and quickly identify MiP, demonstrating greater robustness to the absence of critical information. 3. Reasoning models respond differently to well-defined vs. MiP questions: they mostly follow stable chain-of-thoughts for the former, but are often trapped in self-doubt loop, repeatedly revisiting the question, and guessing the user intentions under MiP, resulting in an explosion of tokens. 4. Reasoning models often can notice the existence of MiP or identify it at an early stage, but they hesitate to commit to this judgment and keep outputting ineffective thinking."
        },
        {
            "title": "2.1 Definition of Missing Premise",
            "content": "Prior to introducing the construction our dataset and analyzing the behavior of reasoning models on problems with missing premises, we formally define the Missing Premise (MiP) problem to establish rigorous foundation for our subsequent analysis. Definition 1 (Missing Premise Problem). Let be question, and let = {P1, . . . , Pn} be set of premises. Define the function mapping premises and question to the set of logically valid answers as: (P, Q) = {A A, is an answer resolving Q} (1) where denotes logical entailment. Consider proper subset = {Pi} for some Pi P. The tuple (P, Q) forms missing premise problem if and only if: (P, Q) = 1 and (P, Q) = (2) This indicates that the removed premise Pi is essential for uniquely determining the logically valid answer to the question Q. According to Definition 1, an ideal reasoning system should efficiently identify the absence of critical premise and terminate its inference process upon recognizing that the available information is insufficient to derive unique solution to the given problem. However, our empirical analysis in Section 3.2 demonstrates that state-of-the-art reasoning models consistently fail to exhibit this capability. Instead, these models engage in extensive, redundant reasoning chains that consume significant computational resources without ultimately identifying the missing premise. 3 Preprint. Under review."
        },
        {
            "title": "Example",
            "content": "MiP-Formula What is the value of ln(a + b)? Diff MiP-SVAMP MiP-GSM8K MiP-MATH Paco had 26 salty cookies and 17 sweet cookies. He ate 14 sweet cookies and 9 salty cookies. How many salty cookies did Paco have left? How many pencils does she have? James decides to run 3 sprints 3 times week. He runs 60 meters each sprint. How many total meters does he run week? There are 360 people in my school. 15 take calculus, physics, and chemistry, and 15 dont take any of them. 180 take calculus. Twice as many students take chemistry as take physics. 75 take both calculus and chemistry, and 75 take both physics and chemistry. Only 30 take both physics and calculus. How many students take physics?"
        },
        {
            "title": "Count Pair",
            "content": "50 300 582 Method Rule-Based Generation BodyQuestion Swapping EssentialPremise Removal 58 EssentialPremise Removal Table 1: Statistics and examples of our curated MiP datasets. For GSM8K and MATH, premise is removed from the original questions (crossed out) to create MiP questions. Diff represents the (estimated) difficulty for models to identify MiP. Count denotes the number of questions in the subset. Pair indicates whether each MiP question is associated with well-defined original question. Method indicates the method used to generate the MiP question."
        },
        {
            "title": "2.2 Overview of Data Construction",
            "content": "To systematically investigate this MiP-Overthinking issue, we construct suite of MiP questions in controllable manner. Our MiP questions are sourced from 3 math datasets across different difficulties. In addition, we also construct synthetic dataset consisting of formulas with unassigned variables. Our ill-posed question generation employs three distinct methods covering three difficulty levels and three strategies to create MiP questions: Rule-Based Generation: This approach generates MiP questions through principled formula construction process, where unassigned variables serve as the missing premises. Body-Question Swapping: We introduce logical inconsistencies by deliberately mismatching problem bodies with their corresponding questions from the original dataset. This creates scenarios where the premises and queries are fundamentally incompatible. Essential-Premise Removal: Through careful analysis of existing well-formed questions, we identify and remove critical premises that are necessary for logical resolution. This transformation preserves the questions structure while rendering it unsolvable. The following sections provide detailed overview of our data construction process for each dataset category. For comprehensive implementation details and additional methodological considerations, we refer readers to Appendix B. MiP-Formula. We construct dataset of 50 synthetic unsolvable formulas in rule-based manner. The formulas are generated recursively through combinations of variables and operators, with maximum recursion depth of three. While these formulas may appear complex at glance, their unsolvability should be immediately apparent due to the presence of undefined variables. MiP-SVAMP. We utilize SVAMP (Patel et al., 2021), benchmark dataset with elementary-schoollevel math problems, where each instance consists of problem body and an associated question. We generate MiP question by randomly permuting the problem bodies and associated questions and then manually inspect them to avoid inadvertent cases. The resulting problems contain clear logical inconsistencies between their body and question components, which is easy for human to identify. MiP-GSM8K. We further utilize GSM8K (Cobbe et al., 2021), more complex mathematics dataset than SVAMP. The questions in GSM8K typically contain multiple numerical conditions and require certain reasoning capabilities to arrive at solutions. We first identify the questions containing two or three numerical conditions and then randomly eliminate one numerical condition per question before conducting human verification to filter out those questions that are still solvable in some way. 4 Preprint. Under review. Figure 2: Response lengths, accuracy on well-defined questions, and abstain rate of reasoning/nonreasoning models on MiP questions from our MiP-GSM8K dataset. (1) Existing reasoning models generate significantly longer responses for MiP questions than well-defined questions, while nonreasoning models generate responses of similar lengths for both types of questions, indicating MiP-Overthinking for reasoning models. (2) For both questions, reasoning models generate longer responses than non-reasoning models, indicating General Overthinking. (3) Although the longer responses by reasoning models slightly improve the accuracy for well-defined questions, it does not enhance the abstain rate for MiP questions, indicating contradiction on the test-time scaling law. Compared with previous MiP questions, questions from this source require the basic logical analysis of models to identify that the question is unsolvable. MiP-MATH. For MATH 500 dataset (Hendrycks et al., 2021), which contains challenging mathematical questions at the competition level, it is difficult to build rule-based filtering mechanism. Thus, we manually select 58 questions that are feasible for constructing the MiP questions and remove one necessary premise from the question. Due to the sophisticated nature of this data source, identifying the insufficiency of these instances requires substantial mathematical reasoning capabilities, testing models ability to recognize unsolvability in complex mathematical contexts."
        },
        {
            "title": "3.1 Evaluation Metrics",
            "content": "To systematically evaluate model responses under MiP, we conduct experiments with diverse set of reasoning and non-reasoning models. For each model, we analyze calculate the following metrics for the responses across different datasets: Response Length: The average number of tokens in the response, incorporating both reasoning steps and final answer components. Abstain Rate for MiP Question: The proportion of answers where the model explicitly identifies the missing premise and either declines to provide an answer or requests additional information necessary for solving the problem. Accuracy for Well-defined Question: The proportion of answers where the model produces definitive response that aligns with the reference answer. For datasets without reference answers (MiP-Formula and MiP-SVAMP), we only calculate the abstain rate for the questions. Response evaluation is performed using GPT-4o as an automated evaluator. Detailed experimental procedures and evaluation protocols are provided in Appendix A."
        },
        {
            "title": "3.2 Main Results",
            "content": "Figure 2 compares average response length, accuracy on well-defined questions, and the abstain rate on MiP questions across range of state-of-the-art LLMs, revealing several significant patterns in model behavior. 5 Preprint. Under review. Model Type MiP-Formula Length Abstain MiP-SWAMP Length Abstain Non-Reasoning Models Type MiP-GSM8K Length Abstain MiP-MATH Length Abstain Qwen2.5-32B-Instruct MiP GPT-4o Gemini 1.5 Gemma-2-27B-IT MiP MiP MiP 285 338 453 204 44.0 70. 20.0 85.7 128 122 52 Phi-3-medium-128k MiP 1465 48.0 125 98. 96.3 99.0 92.0 98.7 Reasoning Models GPT-o MiP 1123 GPT-o1mini MiP 958 GPT-o3mini MiP 1025 DS Distill Qwen2.5-32B MiP 12911 78.0 66. 76.0 42.0 581 639 1299 DeepSeek R1 MiP 4757 6.0 1996 S1.1-32B QwQ-32B"
        },
        {
            "title": "MiP",
            "content": "5284 18."
        },
        {
            "title": "MiP",
            "content": "7937 0.0 3487 99.0 96.7 93. 88.3 84.3 57.0 56.3 MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined 219 246 202 212 103 131 148 210 216 838 348 762 449 1516 384 2302 519 3246 5024 1896 4780 1896 44.0 0.5 46.9 0.5 54.5 0.5 53.8 0. 47.8 1.0 55.7 0.3 40.0 1.2 23.7 1.4 24.6 0.2 16.5 0.2 16.0 0. 10.1 0.2 525 1114 487 472 568 502 338 305 427 4189 2502 2193 1913 3772 1553 9876 3246 7268 3200 9322 5037 10242 15.4 1.9 15.4 1.9 5.8 0.0 38.5 11.5 23.1 3.8 30.8 0. 25.0 0.0 11.5 0.0 5.8 0.0 3.8 1.9 15.4 0.0 1.9 0.0 Table 2: Comparing response length and abstain rate across different MiP datasets. Shorter lengths and higher abstain rates are preferred. For each column, the top-3 preferred values are colored in green, otherwise red. MiP-Overthinking, reflected by longer response with low abstain rate, is commonly observed on most existing reasoning models across all datasets, indicating critical drawback of existing reasoning models. Firstly, existing reasoning models (left side of the figure) display an explosive increase in response length when facing the MiP questions, often producing 2 4 more tokens than general overthinking on well-defined questions. For example, QwQ-32B (Team, 2025) and DeepSeek-R1 (DeepSeekAI et al., 2025) exhibit substantial increase from already long reasoning paths on well-defined questions (approximately 1, 000 tokens for simple GSM8K questions) to highly lengthy outputs (more than 3, 000 tokens) under missing premise conditions. On the contrary, no similar issues exist for non-reasoning models (right side of the figure), which generate similar token counts for both types of well-defined and MiP questions. This phenomenon directly illustrates the NiP-Overthinking phenomenon as introduced in the paper. Secondly, comparing the token lengths on well-defined questions between the reasoning and nonreasoning models, reasoning models tend to produce longer responses, even for simple questions, than non-reasoning models, underscoring the inefficient and verbose responses of existing reasoning models. For example, for the non-reasoning models, it only takes approximately 200 tokens for them to generate the responses for well-defined questions, while taking 1, 000 tokens for DeepSeek-R1 and 1, 800 tokens for QWQ-32B to answer the exactly same questions. However, the explosive increase in extra tokens does not lead to corresponding large accuracy improvements, shown in the green line, highlighting the issue of the General Overthinking. Finally, the abstain rates (red line) on MiP questions reveal that although some reasoning models (e.g., GPT-o1) have promising capabilities in abstaining from the MiP questions, most of the other reasoning models are not able to abstain from the given MiP questions correctly despite the dramatically long reasoning paths. This phenomenon indicates that although most existing reasoning models have thinking and reasoning capabilities to some extent, they lack the critical thinking capabilities to reject ill-posed questions. By contrast, non-reasoning models, though they are not explicitly trained for reasoning, tend to strike better balance, generating shorter answers that are more likely to acknowledge MiP when the question is ill-posed. This phenomenon reveals surprising contradiction on test-time scaling law. Moreover, Table 2 further presents the comparisons on length and abstain rate on other MiP datasets we curated. The preferred results are colored green (shorter responses and higher abstain rate for 6 Preprint. Under review."
        },
        {
            "title": "Type",
            "content": "Alternatively Cnt."
        },
        {
            "title": "Wait",
            "content": "Cnt. Check Cnt."
        },
        {
            "title": "But",
            "content": "Cnt. Hypothesis Cnt."
        },
        {
            "title": "Step",
            "content": "Cnt. Non-Reasoning Models Qwen2.5-32B GPT-4o Gemini 1. Gemma-2-27B DS-Distill Qwen DeepSeek R1 S1."
        },
        {
            "title": "QwQ",
            "content": "MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined MiP Well-defined 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 11.5 0.1 16.9 1.7 42.0 4.0 47.0 6.7 0.0 0. 0.0 0.0 11.4 15.2 38.0 40. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0."
        },
        {
            "title": "Reasoning Models",
            "content": "19.7 0.4 14.4 3.5 21.9 6.0 19.4 6.4 19.3 10. 15.9 13.0 1.0 0.2 3.8 2.5 5.5 3.0 5.0 3. 0.0 0.0 0.0 0.0 0.8 1. 2.5 1.6 0.3 0.1 0.3 0.1 0.1 0.0 0.1 0. 40.1 0.8 49.4 7.3 87.2 13.1 66.1 11.9 0.2 0. 0.1 0.1 39.3 42.1 74.1 54. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 38.4 0.4 44.7 4. 84.8 7.8 94.1 12.4 0.0 0.0 0.0 0. 38.0 40.4 77.0 81.7 4.3 5.6 4.7 6. 1.6 3.8 5.2 5.7 54.9 12.7 54.2 21.2 79.9 29.0 97.9 39. -1.3 -1.5 -2.2 -0.5 42.2 33. 50.9 58.7 Table 3: Comparisons of reasoning-related token counts on MiP-GSM8K dataset. Hypothesis category includes several key words, including perhaps, maybe, and might. Step represents the step counts, spited by nn, where negative values are colored in green and positive in red. denotes the difference between MiP and well-defined questions. When facing MiP questions, reasoning models encounter explosive growths on reasoning-related tokens and steps, indicating severe abuse of thinking patterns, while non-reasoning models use fewer steps for MiP questions than well-defined ones. MiP questions), and the worse results are colored red, from which we can easily discover that reasoning models are prone to generate long responses while having low abstain rates across all datasets, indicating the consistent MiP Overthinking issue of existing reasoning models. In addition, by comparing the behaviors of models on different datasets, we can observe that for the relatively harder dataset (MiP-MATH), all models generate relatively longer responses and obtain lower abstain rates, indicating that harder MiP questions require reasoning capabilities."
        },
        {
            "title": "3.3 Thinking Patterns through Tokens",
            "content": "To gain deeper insight into the MiP-Overthinking issue, we compare the reasoning-related token distribution on the MiP-GSM8K dataset. As shown in Table 3, we break down the average usages of several token patterns related to the thinking process, as well as the number of steps for each model to solve the given questions. Specifically, values of alternatively, wait, check, and but can be directly counted from the model responses, including the thinking paths of reasoning models. Hypothesis category includes several key words, including perhaps, maybe, and might. Step represents the step counts, spited by nn. Reasoning models exhibit much higher occurrence of tokens such as alternatively, wait, and check, compared with non-reasoning models, whose frequencies remain close to zero, indicating their advanced thinking capabilities. However, when moving from well-defined to MiP questions, reasoning models encounter explosive growths on reasoning-related tokens, indicating large redundancy in thinking patterns. Moreover, when comparing the changes of steps, reasoning models exhibit large increase in step count for MiP questions, while non-reasoning models typically show fewer steps, suggesting they quickly conclude the question is unanswerable. With this gap, together with the consistently better abstain rates of the non-reasoning models, we conclude that the lengthy reasoning steps are mostly redundant and indicate self-doubt thinking patterns for reasoning models. 7 Preprint. Under review."
        },
        {
            "title": "3.4 Step-level Similarities",
            "content": "To further assess how redundant the generated content becomes under MiP conditions, we examine the step-level similarity within the models responses on our MiP-GSM8K dataset. Specifically, we divide each response into discrete steps, split by nn, and compute pairwise cosine similarity scores with embeddings generated by all-MiniLM-L6-v2 (Reimers & Gurevych, 2019). The visualization is shown in Figure 3, where each value in the heatmap metrix represents the averaged cosine similarities between the corresponding step index. The average similarity score for well-defined question is 0.45 and 0.50 for MiP response. The variance is 7.9e-3 and 8.2e-4 respectively. Figure 3: The step-level similarity heatmaps for s1.1 responses towards well-defined (left) and MiP (right) questions in MiP-GSM8K dataset. To avoid differences in matrix size, we only consider responses with more than 50 steps and visualize the average simialrity matrix across first 50 steps. The heatmap for MiP questions has higher averaged similarity and lower standard variance, also shown in the heatmap, which indicates the considerable redundancy in its content when responding to MiP questions. As shown in the figure, responses to MiP questions have greater overall similarity across steps and lower standard variance, indicating the considerable redundancy in the content. This means, in many instances, the model revisits similar partial reasoning or repeats previous sentences with only minor changes, showing potential self-trapping issue. Together, these patterns confirm that MiP questions induce high degree of repetitive content in reasoning models. Rather than terminating early to conclude for insufficient premise, the models fill their reasoning paths with repetitive re-checks and reiterations, significantly inflating token usage without improving real abstain rates."
        },
        {
            "title": "3.5 Thinking Patterns through Example",
            "content": "To further understand what happens in the reasoning chain of reasoning models when faced an ill-post input, we present an example of reasoning models response to MiP question in Figure 4. We summarize five major thinking patterns we found in the example and highlight them with different colors. We can observe from the example that the model abuses these patterns to generate long responses, while the responses are not only redundant but also not helpful for the model to abstain from the given MiP question. More examples can be found in the appendix D."
        },
        {
            "title": "4.1 Do Models know premises are missing?",
            "content": "To investigate whether reasoning models recognize the potential unsolvability of questions during their reasoning process, we conducted detailed analysis of their reasoning chains. We segmented each reasoning chain into discrete steps using nn as delimiters and performed step-wise verification to detect whether models express doubt on the question solvability. We introduce two key metrics for this analysis: In-Process Suspicion Rate, which measures the percentage of responses where the model expresses doubt about solvability during reasoning, and First Suspicion Index, which captures the average step number at which the model first suspects the missing premise. To ensure robust evaluation, we employed GPT-4o to assess each step three times, using majority voting for our final step-level result. The quantitative results of this analysis are presented in Table 4. As we can see from the table, most of the existing reasoning models have suspected that the given question might be unsolvable at the very early stage of their reasoning process, demonstrating the ability of reasoning models to recognize the potential MiP. However, these reasoning models lack critical thinking capabilities: they are prone to keep digging the given unsolvable question by re-visiting the question and related definitions again and again and again, rather than question the solvability of the given question. Thus, as visualized in Figure 5, despite existing reasoning 8 Preprint. Under review. Figure 4: An example of reasoning model (s1.1-32B) response to MiP question. The response exhibits five distinct thinking patterns, highlighted in different colors: 1Revisit Question (yellow), where the model reexamines the original query; 2Visit Knowledge (red), where the model accesses domain-specific knowledge; 3Propose Assumption (blue), where the model proposes and investigates various hypotheses; 4Self Doubt (green), where the model questions its own reasoning and expresses uncertainty; and 5Pause/Check (purple), where the model pauses to review previous steps. These patterns demonstrate the models complex but potentially inefficient reasoning process when confronted with missing premises. models suspecting the solvability of most of the given MiP questions, they only abstain very small proportion of them. 9 Preprint. Under review. Model In-Process Suspicion Rate In-Process First Suspicion Index DeepSeek-R1 DS-Qwen QwQ DeepSeek-R1 DS-Qwen QwQ MiP-Formula MiP-GSMR S1.1 100% 100% 1.16 1.42 95.5% 2.01 83.3% 3.90 S1.1 99.6% 100% 1.61 1.77 100% 1. 100% 1.36 Table 4: The in-process insufficiency suspicion information across different reasoning models on MiP-Formula and MiP-GSMR datasets. The in-process insufficiency suspicion is defined as when the reasoning model suspects the given question is unsolvable during its thinking process. In-Process Suspicion Rate represents how many percent of the samples trigger the in-process suspicion. First Suspicion Index is the averaged step index where the model first suspects the questions validity. Most reasoning models do notice the existence of MiP at the very early steps, but they still suffer from low abstain rate and cannot confidently stop the thinking. Based on the above observations, we conclude that reasoning models actually have the capabilities to find out that the given MiP question is not solvable, but they do not dare to abstain it. Thus, our MiP-Overthinking issue indicates the lack of critical thinking abilities of reasoning models. Figure 5: The transition flow between in-process suspicion of MiP and the final successful abstention on different reasoning models. For each Sankey diagram, the left bars represent whether the model suspects the given question is unsolvable during its thinking process, i.e., Suspected or Unsuspected; the right bars represent the final abstention, categorized into Abstain (preferred) or Non-abstain. Most existing reasoning models have suspected that the given question might be unsolvable, but only for very small portion, the models insist on their suspicion. 4.2 What Caused MiP-Overthinking? Figure 2 demonstrates that MiP-Overthinking manifests across both RL-based and SFT-based reasoning models. We hypothesize this phenomenon primarily originates from inadequate length constraints during the rule-based reinforcement learning phase of RL-based models, subsequently propagating to SFT-based models through distillation. Current RL-based reasoning models predominantly employ rule-based training focused on format and accuracy rewards (Shao et al., 2024; Sui et al., 2025), with some incorporating step or length rewards to promote thorough reasoning (Face, 2025). This approach can lead to reward hacking, where models explore excessive reasoning patterns to achieve correct answers (Aggarwal & Welleck, 2025; Shen et al., 2025; Luo et al., 2025). To demonstrate the transmissibility of this behavior through distillation (Xu et al., 2024), we finetune Qwen-2.5-7B-Instruct using small-scale 50 MiP responses generated by DeepSeek-R1 on the MiPFormula dataset. As shown in Figure 6, the finetuned model exhibits clear MiP-Overthinking characteristics when evaluated on GSM8K: significantly increased response lengths for both MiP and welldefined questions, emergence of length disparity between MiP and well-defined responses previously absent in the original model, and decreased abstain rates. 10 Figure 6: Comparison of response length, abstain rate of MiP, and accuracy of welldefined questions before and after tuning on 50 responses from DeepSeek-R1 on the MiPFormula dataset. The results demonstrate rapid onset of MiP-Overthinking behavior after exposure to small number of MiP examples during fine-tuning. Preprint. Under review."
        },
        {
            "title": "5.1 Reasoning Large Language Model",
            "content": "Recent advances in Large Language Models (LLMs) have sparked significant research interest in enhancing their reasoning capabilities (Ahn et al., 2024; Besta et al., 2025; Chen et al., 2025a). Research has focused on improving these capabilities through various post-training approaches. Several studies have employed reinforcement learning techniques to guide models toward more effective reasoning strategies (Shao et al., 2024; Xiong et al., 2025; Cui et al., 2025). Additionally, researchers have demonstrated that instruction tuning on carefully curated, high-quality datasets can significantly enhance reasoning performance (Ye et al., 2025; Muennighoff et al., 2025). While Reasoning Models have demonstrated impressive performance on various benchmarks, recent studies have begun to critically examine the quality and efficiency of their reasoning processes. Xia et al. (2025) conducted comprehensive analysis of RLMs reasoning quality, revealing significant redundancy in their solution approaches. Further investigations (Chen et al., 2025b; Cuadron et al., 2025; Qu et al., 2025; Liu et al., 2025) identified concerning overthinking phenomenon, where reasoning model generate unnecessarily verbose solutions even for simple problems. Building on these observations, Kumar et al. (2025) demonstrated the potential security implications of this behavior by developing slowdown attack that exploits overthinking through input perturbation."
        },
        {
            "title": "5.2 Test-time Scaling",
            "content": "In contrast to earlier research on training-time scaling laws (Kaplan et al., 2020), recent literature has increasingly focused on test-time performance scaling strategies, which aim to enhance model performance by optimizing inference-time token generation (Snell et al., 2024; OpenAI, 2024a). These approaches can be categorized into several primary methodologies: parallel sampling techniques (Brown et al., 2024; Levi, 2024), which generate multiple candidate responses and select the optimal output; sequential refinement approaches (Snell et al., 2024; Lee et al., 2025), which enable iterative improvement of previous outputs; and tree-based methods (Gandhi et al., 2024; Hou et al., 2025), which combine elements of both parallel and sequential approaches. While the prevailing consensus suggests that increased token generation during inference enhances reasoning capabilities, our investigation reveals concerning counterpoint: under certain conditions, extended responses can lead to computational inefficiency and, paradoxically, degraded performance outcomes."
        },
        {
            "title": "5.3 Models’ Behavior Study in Ambiguous Condition",
            "content": "LLMs are prone to hallucination (Huang et al., 2025; Xu et al., 2025), generating non-existent conditions that compromise trustworthiness. An essential aspect of reliability is the ability to abstain under uncertainty. Prior work (Cole et al., 2023; Amayuelas et al., 2024; Zhou et al., 2023) has proposed benchmarks assessing LLMs recognition of knowledge limits when facing ambiguous or challenging queries. Different from theirs, our study explores reasoning models under MiP condition. Surprisingly, we find these specialized models exhibit prolonged reasoning and inferior performance."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce the Overthinking under Missing Premise (MiP-Overthinking) issue, which is widespread but still under-explored phenomenon for current reasoning models. In this phenomenon, when faced with ill-defined unsolvable questions with missing premises, existing models generate dramatically long responses while having very low abstain rates. With systematic investigation of this phenomenon, our findings show that while these models sometimes suspect the given MiP question is not solvable in the early state of the thinking process, they typically fail to act on those suspicions and instead generating repetitive and redundant thinking traces with the final answer that does not address the missing premises, indicating lack of critical thinking capability. This behavior highlights pressing gap: current training recipes for reasoning models, which emphasize thorough chains of thought, do not sufficiently reward critical thinking or early exit from unsolvable tasks. 11 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, and etc. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/ abs/2404.14219. Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. In Neele Falk, Sara Papi, and Mike Zhang (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 225237, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology. org/2024.eacl-srw.17/. Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Wang. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models, 2024. URL https://arxiv.org/abs/2305.13712. Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwasniewski, Jurgen Muller, Łukasz Flis, Hannes Eberhard, Hubert Niewiadomski, and Torsten Hoefler. Reasoning language models: blueprint, 2025. URL https://arxiv.org/abs/ 2501.11223. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-ofthought for reasoning large language models, 2025a. URL https://arxiv.org/abs/2503. 09567. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025b. URL https: //arxiv.org/abs/2412.21187. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Jeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions, 2023. URL https://arxiv. org/abs/2305.14613. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks, 2025. URL https://arxiv.org/abs/2502. 08235. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/ abs/2502.01456. 12 Preprint. Under review. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, and etc. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https: //arxiv.org/abs/2501.12948. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. Stream of search (sos): Learning to search in language, 2024. URL https: //arxiv.org/abs/2404.03683. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling, 2025. URL https://arxiv.org/abs/2501.11651. Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey, 2023. URL https://arxiv.org/abs/2212.10403. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, January 2025. ISSN 1558-2868. doi: 10.1145/3703155. URL http://dx.doi.org/10.1145/3703155. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian. Overthink: Slowdown attacks on reasoning llms, 2025. URL https: //arxiv.org/abs/2502.02542. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. Evolving deeper llm thinking, 2025. URL https://arxiv.org/abs/ 2501.09891. Noam Levi. simple model of inference scaling laws, 2024. URL https://arxiv.org/abs/ 2410.16377. Ming Li, Yanhong Li, and Tianyi Zhou. What happened in llms layers when trained for fast vs. slow thinking: gradient perspective. arXiv preprint arXiv:2410.23743, 2024. Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, and Reyhaneh Jabbarvand. Codemind: framework to challenge large language models for code reasoning, 2024. URL https:// arxiv.org/abs/2402.09664. Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, and Bryan Hooi. Efficient inference for large reasoning models: survey, 2025. URL https://arxiv.org/abs/2503.23077. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025. URL https://arxiv.org/abs/2501.12570. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Preprint. Under review. OpenAI. Learning to reason with llms, 2024a. URL https://openai.com/index/ learning-to-reason-with-llms/. OpenAI. OpenAI o1 System Card, December 2024b. URL https://cdn.openai.com/ o1-system-card-20241205.pdf. OpenAI. OpenAI o1-mini System Card, September 2024c. URL https://openai.com/ index/openai-o1-mini-advancing-cost-efficient-reasoning/. OpenAI. OpenAI o3-mini System Card, January 2025. URL https://cdn.openai.com/ o3-mini-system-card-feb10.pdf. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, and etc. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303. 08774. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 20802094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 168. URL https://aclanthology.org/2021.naacl-main.168. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, XianSheng Hua, Bowen Zhou, and Yu Cheng. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond, 2025. URL https://arxiv.org/abs/ 2503.21614. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 39823992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410/. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025. URL https://arxiv.org/abs/2503.04472. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/ 2408.03314. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models, 2025. URL https://arxiv.org/abs/2503.16419. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, and etc. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024a. URL https://arxiv.org/abs/2403.05530. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, and etc. Gemma 2: Improving open language models at practical size, 2024b. URL https://arxiv.org/abs/2408.00118. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. 14 Preprint. Under review. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey, 2025. URL https: //arxiv.org/abs/2503.12605. Yuqing Wang and Yun Zhao. Gemini in reasoning: Unveiling commonsense in multimodal large language models, 2023. URL https://arxiv.org/abs/2312.17661. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy, 2025. URL https://arxiv.org/abs/2404.05692. Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Selfrewarding correction for mathematical reasoning, 2025. URL https://arxiv.org/abs/ 2502.19613. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models, 2024. URL https://arxiv.org/abs/2402.13116. Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models, 2025. URL https://arxiv.org/abs/2401.11817. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: How expressions of uncertainty and overconfidence affect language models, 2023. URL https://arxiv.org/ abs/2302.13439. 15 Preprint. Under review."
        },
        {
            "title": "A Detailed Experimental Setup",
            "content": "A.1 Models . . . . . . A.2 Evaluation Metrics . . A.3 Generation Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Examples of Model Response",
            "content": "17 17 17 17 17 19 16 Preprint. Under review."
        },
        {
            "title": "A Detailed Experimental Setup",
            "content": "A.1 Models We leverage series of non-reasoning and reasoning model for our study, from both open-source and proprietary source with different training recipes. The non-reasoning models we use include Qwen2.532B-Instruct Team (2024), Gemma-2-27B-it Team et al. (2024b), Phi-3-medium-128k Abdin et al. (2024) ,GPT-4o OpenAI et al. (2024) and Gemini1.5 Team et al. (2024a). The reasoning models we use are QwQ-32B Team (2025), DeepSeek-R1-Distill-Qwen-32B DeepSeek-AI et al. (2025), S1.1 Muennighoff et al. (2025), DeepSeek-R1 DeepSeek-AI et al. (2025), GPT-o1 OpenAI (2024b), GPT-o1mini OpenAI (2024c) and GPT-o3mini OpenAI (2025). A.2 Evaluation Metrics In Section 3.2, we measure response length by considering both reasoning and answer components. For open-source models, we employ model-specific tokenizers to calculate token counts, while for proprietary models, we obtain generation lengths via their APIs. To determine abstain rates, we parse responses by paragraphs (delimited by nn) and analyze the final two paragraphs as the models conclusion. These conclusions, along with reference answers when available, are evaluated by GPT-4o to assess whether the model provides definitive answer or abstains. For data sets with reference answers (GSM8K and MATH), GPT-4o also evaluates the correctness of the response. The prompt we use for evaluation can be found in Appendix C. A.3 Generation Setting For all open-source models, we employ greedy decoding and utilize the default chat template specific to each model. We deliberately omit system prompts prior to posing questions to maintain consistency across evaluations. For proprietary models, we adhere to their default parameter configurations as provided by their respective APIs. In the case of GPT-o1mini and GPT-o3mini, we configure the reasoning effort parameter to the medium setting by default."
        },
        {
            "title": "B Data Construction Details",
            "content": "To systematically investigate this MiP-Overthinking issue, we construct suite of MiP questions in controllable manner. Our MiP questions are sourced from 3 math datasets across different qualities, including SVAMP, GSM8K, and MATH 500. In addition, we also construct synthetic dataset, rule-based Formula, for evaluation. MiP-Formula. We construct dataset of 50 synthetic unsolvable formulas in rule-based manner. The formulas are generated recursively through combination of variables and operators, with maximum recursion depth of three. The variable set comprises numerical values, Latin letters, and Greek symbols. The operator set includes arithmetic operators (+, ), set operators (, ), mathematical functions (sin, sqrt), and construct operators (, ). To ensure the formulas are fundamentally unsolvable, we enforce the inclusion of at least one unassigned variable in each formula, excluding commonly recognized mathematical or physical constants such as e, π, and g. While these formulas may appear complex at glance, their unsolvability should be immediately apparent due to the presence of undefined variables. MiP-SVAMP. We utilize SVAMP (Patel et al., 2021), benchmark dataset comprising 1, 000 elementary-school-level mathematical word problems, where each instance consists of problem body and an associated question. The MiP questions can be generated by randomly permuting the problem bodies and associated questions. To maintain dataset integrity, we manually select 300 permuted questions after thorough human evaluation to eliminate any inadvertently solvable questions that may exist. The resulting problems contain clear logical inconsistencies between their body and question components, making their unsolvability readily apparent without additional context. MiP-GSM8K. We further utilize GSM8K (Cobbe et al., 2021), grade school mathematics dataset that presents more complex challenges compared to SVAMP. The questions in GSM8K typically 17 Preprint. Under review. contain multiple numerical conditions and require certain reasoning capabilities to arrive at solutions. The MiP question can be constructed by randomly removing necessary premise from the original solvable question. We first identify the questions containing two or three numerical conditions and then randomly eliminate one numerical condition per question. Subsequently, thorough human verification is conducted to filter out those questions that are still solvable in some way and finally obtain 582 MiP questions. Compared with previous MiP questions, questions from this source require the basic logical analysis of models to identify that the question is unsolvable. MiP-MATH. For the MATH dataset (Hendrycks et al., 2021), which comprises challenging competition-level mathematical questions, it is hard to build rule-based filtering mechanism before human evaluation. Thus, we directly read through all the questions in MATH500 and manually select 58 questions that are feasible for constructing the MiP questions and remove one necessary premise from the question. Due to the sophisticated nature of this data source, identifying the insufficiency of these instances requires substantial mathematical reasoning capabilities, testing models ability to recognize unsolvability in complex mathematical contexts."
        },
        {
            "title": "C Prompt Template for Evaluation",
            "content": "As we need LLM-as-a-judge to evaluate the open-end generations of the models in various experiment in this study, in this section we showcase the prompt template we use for each kind of evaluation. For the evaluation of the models answer accuracy and abstain rate, we adopt the following prompt templates designed for paired and non-paired data, respectively. As we observe that some models, for example Gemma-2-27B-IT, often output an additional nn at the end of response, we take the last two paragraph segmented by nn to avoid pasing in an empty string."
        },
        {
            "title": "Prompt Template for Response Evaluation of Formula and SVAMP",
            "content": "You are helpful assistant that evaluates the quality of models answer. You will be given question and models answer. You need to evaluate the correctness of the models answer. If the answer explicitly says that the condition of the question is insufficient, you should return 0. If the model provides an answer that is number or formula with variables, you should return 1. Please only return the number, no other text. Model answer: [model answer short] Figure 7: The prompt we use to evaluate the accuracy and abstain rate of the model on Formula and SVAMP. [model answer short] is the last two paragraphs of the model answer and [reference answer] is the answer for the orginal dataset. Prompt Template for Response Evaluation of GSM8K and MATH You are helpful assistant that evaluates the quality of models answer. You will be given question and models answer. You need to evaluate the correctness of the models answer. If the model output says that the condition of the question is insufficient, you should return 0. Otherwise, if the model give clear answer and matches the reference answer, you should return 1. If the models answer does not match the reference answer, you should return 2. Please only return the number, no other text. Model answer: [model answer short] Reference answer: [reference answer] Figure 8: The prompt we use to evaluate the accuracy and abstain rate of the model on GSM8K and MATH. [model answer short] is the last two paragraphs of the model answer and [reference answer] is the answer for the orginal dataset. We use the prompt template in Figure 9 to find the first paragraph that the model suspected missing premise. We pass in the response sequentially by paragraph until the GPT-4o give positive response. In practice we find it is not very stable, so we repeat this process for 3 times and use the median value. 18 Preprint. Under review. Prompt Template for Response Evaluation of GSM8K and MATH You are helpful assistant. You will be given paragraph which is part of the answer to question. You need to identify if the paragraph doubt that the answer depends on some other unspecified condition. Paragraph: [paragraph] Answer in one word, yes or no. Figure 9: The prompt we use to judge if the model suspect there is missing premise in the response paragraph. [paragraph] is the part of the model response spited by nn ."
        },
        {
            "title": "D Examples of Model Response",
            "content": "In this section, we present some examples of the model response of both non-reasoning and reasoning model on MiP data. As we can see from Figure 10 and Figure 11, the non-reasoning models soon identify the missing premise issue of the question. They either abstain from answering the question, as in Figure 10, or friendly invite the user to provide more information. However, as we can see from Figure 11 and Figure 13, reasoning models generate extremely verbose answers on these two apparently premise missing problems. What is worse, they fail to abstain to answer the question. The response in Figure 11 arrives at an absurd answer, and the model in Figure 13 generates hallucinated answer based on its assumption rather than provided information. Model Response for MiP-Formula Question from Gemini-1.5 Question: What is the value of ((γ)) + Ξ? Model Response: The expression ((γ)) + Ξ involves two symbols that need to be defined to determine the value of the expression. γ is the Euler-Mascheroni constant, approximately equal to 0.5772156649. The double parentheses notation ((γ)) is not standard mathematical notation. Without further definition of this notation, we cannot evaluate this term. Ξ is the Riemann Xi function. It is related to the Riemann zeta function, but the notation Ξ itself isnt specific numerical value. The Riemann Xi function is defined as function of complex variable, not constant. So Ξ by itself doesnt have specific numerical value unless specific value is substituted for its argument. Therefore, we cannot determine the value of ((γ)) + Ξ without knowing what ((γ)) and the argument of Ξ represent. The expression is incomplete without definitions for these symbols. Final Answer: The final answer is unde ined Figure 10: An example of model response from Gemini 1.5 on MiP-Formula dataset. The model quickly identify the missing premise and abstain to answer. 19 Preprint. Under review. Model Response for MiP-GSM8k Question from GPT-4o Question: Kyle bought last years best-selling book for $19.50. What was the original price of the book? Model Response: To determine the original price of the book, we generally need to know the discount or markdown percentage applied to calculate its selling price of $19.50. If Kyle bought the book for $19.50 and we know it was discounted, we can use the following formula to find out the original price: Selling Price = Original Price (Original Price Discount Rate) Without additional information about discounts, promotions, or markdown rates applied, its not possible to precisely determine the original price solely from the amount Kyle paid. If you provide the discount percentage or rate, can help calculate the original price for you! Figure 11: An example of model response from GPT-4o on MiP-GSM8k dataset. The model quickly identify the missing premise and ask the user for more information. Preprint. Under review. Model Response for MiP-Formula Question from s1.1-32B Question: What is the value of ((γ)) + Ξ? Model Response: Okay, so need to figure out the value of ((γ)) + Ξ. Hmm, first off, should probably clarify what these symbols represent. The problem uses double parentheses around gamma, so ((γ)), and then the Greek letter Xi (Ξ). wonder if these are standard mathematical symbols or if theyre defined in specific context that might not be aware of. Let me start by recalling what know about gamma. In mathematics, γ often refers to the EulerMascheroni constant, which is approximately 0.5772. Its important constant in number theory and analysis, related to the harmonic series and the gamma function. But here, the notation is ((γ)). The double parentheses could mean something specific. Maybe its notation from particular field or textbook? For example, in some contexts, double parentheses might denote an interval or floor function, but usually, floor is denoted by brackets like (γ). Alternatively, in physics, double parentheses could indicate some kind of tensor notation, but Im not sure. Then theres the Xi (Ξ). Xi is another Greek letter, but in mathematics, Ξ could stand for various things. In set theory, sometimes Xi is used as variable for ordinals, or in other areas like particle physics, Ξ represents certain particles. But again, without context, its hard to say. Maybe its function or constant defined in the problems context? Wait, the question is just asking for the value of ((γ)) + Ξ without any additional information. That makes me think these symbols might be part of puzzle or play on notation rather than standard mathematical constants. Let me consider if this could be trick question. The double parentheses around gamma((γ)) maybe thats referring to the number of letters in the word gamma or something like that? Lets see: Gamma is spelled G-A-M-M-A, which is 5 letters. So ((γ)) could be 5. Then Ξ is the Greek letter Xi, which is the 14th letter of the Greek alphabet. So maybe Ξ represents 14? Then 5 + 14 would be 19. Is that possible? But is there standard where Greek letters are assigned numerical values like Roman numerals? Wait, in Greek numerals, each letter does have numerical value. Let me check that. In the Greek numeral system, letters are assigned values as follows: ...............................................................67 more paragraphs here............................................................... But just to check another angle: In the context of APL programming language, certain symbols have specific meanings, but dont think thats relevant here. Alternatively, in chemistry, γ is used to denote surface tension, and Ξ is not standard symbol. In engineering, γ is sometimes the shear strain. But again, not relevant. Alternatively, in statistics, gamma is distribution, and Xi could be parameter, but again, without context. Alternatively, the symbols might be from specific equation in paper or book, but since dont have that reference, cant confirm. Given all these considerations, and trying to think of what the answer might be expected to be in problem like this, its likely either 17 or 63. If the question is from math competition or puzzle, they might be expecting the Greek numeral values. For example, in the Greek numeral system, γ is 3 and Ξ is 60, so 3 + 60 = 63. Alternatively, if its based on their positions in the alphabet, 3 + 14 = 17. ...............................................................21 more paragraphs here............................................................... Therefore, the answer is 63. The double parentheses around gamma might be typo or just for decoration, but given the problems phrasing, interpreting them as Greek numerals seems the most logical. *Final Answer** 63 < /think>To determine the value of ((γ) + Ξ), we need to consider the possible meanings of the symbols (γ) and (Ξ). 1. **Greek Numerals**: In the Greek numeral system, each letter has specific numerical value. The letter (γ) (gamma) represents the value 3, and the letter (Ξ) (Xi) represents the value 60. 2. **Double Parentheses**: The notation ((γ)) might be red herring or simply emphasize that (γ) is being used as numeral. The parentheses do not change the numerical value of (γ). Combining these values, we add the numerical values of (γ) and (Ξ): - (γ = 3) - (Ξ = 60) Thus, the value of ((γ) + Ξ) is (3 + 60 = 63). 63 Figure 12: An example of response from s1.1 model on MiP-Formula data. The model spend lots of time doing inefficient and redundant reasoning before outputting meaningless result. 21 Preprint. Under review. Model Response for MiP-GSM8k Question from DeepSeek-R1 Question: Kyle bought last years best-selling book for $19.50. What was the original price of the book? Model Response: Okay, lets see. Kyle bought book for $19.50, and the question is asking for the original price. Hmm, wait second. The problem mentions that its last years best-selling book, so maybe it was on sale or discounted? The question doesnt explicitly state that there was discount, but it does say last years, which might imply that the book is now being sold at reduced price. But hold on, the problem doesnt give any specific information about discount percentage or markdown. Without that, how can we calculate the original price? Maybe theres some context or common knowledge Im missing here. For example, sometimes best-selling books are sold at standard discount, but thats not universal rule. Alternatively, maybe the question is testing understanding of something else, like sales tax or profit margins. But if it were sales tax, the original price would be less than $19.50, and the total after tax would be more. But the question is asking for the original price, implying that $19.50 is the price after some change. Wait, perhaps the problem is incomplete? The user just says Kyle bought last years best-selling book for $19.50. What was the original price of the book? If theres no additional information given, like percentage discount or mention that the price was reduced, then its impossible to determine the original price. But maybe need to make an assumption here. Since its last years best-selling book, its common for such books to be discounted when new edition comes out or after some time. Let me think, maybe the standard discount is 25% or something? But the problem doesnt specify. Wait, perhaps the problem was part of larger set where the discount rate was mentioned earlier, and this is follow-up question. But as it stands, with the information given here, theres no way to calculate the original price. Alternatively, maybe the question is trick question, where the original price is the same as what Kyle paid. But that doesnt make sense because it specifies last years best-selling book, which might imply it was sold at different price before. Let me check the problem statement again: Kyle bought last years best-selling book for $19.50. What was the original price of the book? Theres no mention of discount, sale, or any other financial context. If were to approach this mathematically, lets denote the original price as P. If there was discount, then $19.50 is equal to minus some discount. But without knowing the discount rate, we cant set up an equation. For example, if the book was discounted by 20%, then the original price would be $19.50 / 0.80. But since we dont have that percentage, we cant proceed. ...............................................................24 more paragraphs here............................................................... Alternatively, if the answer is expected to be $19.50, but that doesnt make sense because the question is asking for the original price, implying it was different. Alternatively, maybe the problem is in another language where original price refers to the price without tax, but in the US, sales tax isnt included in the price, so if $19.50 is including tax, the original pre-tax price would be less. But again, without tax rate, cant compute. Alternatively, if the problem is from country where tax is included, but the question is asking for the pre-tax price, but again, no tax rate given. This is really confusing. Since the problem is presented in English and mentions dollars, likely US context. In the US, sales tax is added at checkout, so the price Kyle paid would be the shelf price plus tax. If the question is asking for the pre-tax price, but again, no tax rate given. Alternatively, maybe the book was on sale, and the original price is the same as the sale price, but that contradicts the wording. Given all this, think the most plausible answer, even though its based on an assumption, is $26, assuming 25% discount. Therefore, the original price was $26.<think> To determine the original price of the book, we need to consider that Kyle likely purchased it at discounted price since its last years best-selling book. common assumption for such scenarios is **25% discount**. Let be the original price. If the discounted price is $19.50, this represents 75% of the original price (100% - 25% discount): [0.75P = 19.50] Solving for P: [P = 19.50 *Original price of the book:** 26 0.75 = 26] Figure 13: An example of model response from DeepSeek-R1 on MiP-GSM8k dataset. After thinking for long time, the model hallucinates an answer based on its assumption of discount rate."
        }
    ],
    "affiliations": [
        "Lehigh University",
        "University of Maryland"
    ]
}