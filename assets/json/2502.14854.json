{
    "paper_title": "CLIPPER: Compression enables long-context synthetic data generation",
    "authors": [
        "Chau Minh Pham",
        "Yapei Chang",
        "Mohit Iyyer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA)."
        },
        {
            "title": "Start",
            "content": "CLIPPER: Compression enables long-context synthetic data generation Chau Minh Pham1 Yapei Chang1 Mohit Iyyer1,2 1University of Maryland, College Park 2University of Massachusetts Amherst {chau,yapeic,miyyer}@umd.edu 5 2 0 F 0 2 ] . [ 1 4 5 8 4 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER,1 compression-based approach for generating synthetic data tailored to narrative claim verificationa task that requires reasoning over Instead of book to verify given claim. generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naïve approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets new stateof-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA)."
        },
        {
            "title": "Introduction",
            "content": "Due to the high cost of human-annotated data, LLM developers increasingly rely on synthetic data (generated by LLMs) to boost instruction following and reasoning capabilities (Ding et al. 2023; Lambert et al. 2024; Yang et al. 2025, inter alia). As the context size of LLMs extends to millions of tokens, it is important to ensure that we have scalable and performant strategies to create synthetic data for 1CLIPPER stands for Compressing Long InPuts. 2Code is available at: github.com/chtmp223/CLIPPER Figure 1: Results on CLIPPERs test set and NoCha for baselines, small closed models, and CLIPPER models. Fine-tuning on our synthetic data significantly improves narrative claim verification. long-context tasks. Prior work creates such data by (1) selecting long document or smaller chunk within; and (2) prompting an LLM to generate input/output pairs for various tasks using the selected text. (Bai et al., 2024; Dubey et al., 2024). While this strategy is effective for tasks like summarization and QA, we show that it breaks down for more complex reasoning-oriented tasks like narrative claim verification, in which model must judge whether statement about long input text is true or false. The majority of narrative claims in the NoCha benchmark (Karpinska et al., 2024), which was created by human readers of fictional books, can only be verified by global reasoning over events, characters, and relationships. This poses an immense challenge to even the best LLMs: OpenAIs o1-preview currently leads with an accuracy of 67.4% (far below human performance). If no LLM can reliably solve the task, how can we produce and validate synthetic data for it?"
        },
        {
            "title": "We tackle this challenge by introducing",
            "content": "3Creating data for this task is challenging even for humans: NoCha was created by people who first read an entire book and then were paid $1.7 per claim they wrote about that book. Figure 2: Overview of the CLIPPER pipeline. (1) Compression: We generate chapter outlines and book summaries using an LLM. Our books average 90,437 tokens, our outlines average 8,745 tokens, and our summaries average 618 tokens. (2) Synthetic claim generation: We ask LLMs to generate true and false claims based on the outlines and summaries. Each generated claim comes with chain-of-thought. The book texts, generated claims, and corresponding chains-of-thoughts are then used for supervised finetuning. CLIPPER, synthetic data generation pipeline that operates in two stages (Figure 2). First, long document is compressed by an LLM into summaries and/or outlines that contain salient events and descriptions. Then, the LLM is prompted to generate claims based on the compressed narrative, with optional instructions to write claims that require reasoning over multiple chapters to verify. Each claim is accompanied by chain-of-thought reasoning that grounds it within specific chapters where relevant events occur. Compared to the naïve strategy of prompting an LLM with the entire (uncompressed) book, CLIPPER significantly reduces noise in the generated claims and increases groundedness of generated explanations, while also costing roughly half as much. Why does this work? Prior work has shown that LLMs are high-quality summarizers of long documents (Chang et al., 2024; Kim et al., 2024). In addition, by operating on compressed representations, we address the known degradation of instruction-following in long-context settings (Wu et al., 2024b; Levy et al., 2024) and thus reduce the complexity of the claim generation process. To verify the effectiveness of CLIPPER, we use it to generate dataset containing 19K claims about public-domain fictional books. Fine-tuning openweight models like Llama-3.1-8B-Instruct (Dubey et al., 2024), ProLong-512K-8B-Base (Gao et al., 2024b) and Qwen2.5-7B-Instruct (Qwen et al., 2024) on this dataset, yields large improvements on narrative claim verification and positive transfer to other narrative-related tasks (e.g., NarrativeQA, MuSR). For instance, fine-tuning Llama-3.1-8BInstruct on our data doubles its NoCha performance (from 16.5% to 32.2%) and almost triples its test set performance (from 27.9% to 76.0%). Our finetuned Qwen model sets new state of the art on NoCha for <10B models, outperforming closed models like Gemini 1.5 Flash 8B (Team et al., 2024) and OpenAI o1-mini (OpenAI, 2024). While our approach is promising for improving long-context reasoning in open-weight models, our best models fall well short of the performance reached by closed LLMs such as o1 on NoCha. The performance gap between NoCha and CLIPPER-test likely stems from the nature of the claims, as CLIPPER-test features synthetic claims based on model-generated outlines, while NoChas human-written claims require reasoning about details often absent from such outlines. We analyze where our fine-tuned models can still improve, discovering that they benefit more from training on claims whose evidence is localized to single chapter (and not more complex multi-chapter claims). We hope future work will leverage our data generation pipeline for fine-tuning larger models (e.g., >70B) on other long-context tasks to further improve global reasoning abilities. 2 CLIPPER: generating high-quality synthetic data via compression In long-context settings, synthetic datasets have typically been created by selecting lengthy documents from an existing corpus and using an LLM to generate input-output pairs given either the entire document (Bai et al., 2024) or random excerpts (Dubey et al., 2024; Yang et al., 2025). For our task, however, we show that these methods are insufficient: Providing the LLM with the entire document results in much noisier data, as we show that producing high-quality, complex claims about long narratives is fundamentally difficult task even for the best models (2.2). Providing only an excerpt from the long document, on the other hand, precludes the model from generating claims that require global reasoning across the entire book. We thus develop two-stage strategy, CLIPPER, which first compresses the narrative into chapter outlines and summaries, then prompts an LLM to produce claims and chain-of-thoughts grounded in the compressed narrative (2.3). We use the dataset generated by CLIPPER to fine-tune openweight models in 3. All prompts used in this section can be found in A.5."
        },
        {
            "title": "2.1 Task setup\nBefore describing how CLIPPER works, we first\nestablish the task definition for narrative claim\nverification, then explain how we collect the books\nthat serve as the foundation for this task.",
            "content": "In narrative claim verification, Task definition: an LLM is given book and claim about the book. The task is to determine whether the claim is true or false while providing clear explanation for its decision. key aspect of the task is the inclusion of true/false narrative minimal pairs (Karpinska et al., 2024), where each false claim closely resembles its true claim counterpart but contains subtle inaccuracies (illustrated in Figure 2; see Table 4 for more examples). The model is considered accurate only if it correctly verifies both claims in pair, which reduces the chances of the model being correct for the wrong reason. Gathering public domain books: We collect 479 fictional books (e.g. Lewis Carrolls Alice in Wonderland, Agatha Christies The Murder of Roger Ackroyd) from Project Gutenberg,4 with an average length of 90K tokens5 and 23 chapters.6 4https://www.gutenberg.org 5All token counts reported in this paper are computed using o200k_base from https://github.com/openai/tiktoken. 6We do not include books longer than 128K tokens as many open-weight models cannot process anything beyond While using these texts could raise concerns about LLMs memorizing both the source material and generated claims, A.2 shows that potential exposure to these texts in training data does not impact the task performance of baseline models."
        },
        {
            "title": "2.2 Naïve claim generation using book texts",
            "content": "A simple approach for synthetic data generation is to prompt an LLM to generate claims directly from the book text. However, we show that this NAÏVE method falls short in long-context setting, which motivates us to develop CLIPPER (2.3). The NAÏVE method: We provide Claude-3.5Sonnet-v1 with the entire book text and prompt it to generate pairs of true/false claims along with corresponding chain-of-thought reasoning in zero-shot manner. Note that we cannot use few-shot prompting due to the length of the book text. We finally prompt Claude to remove any duplicated claims among the generated claims.8 Human validation: We manually annotate 52 claims generated by NAÏVE based on six books  (Table 5)  . Across these claims, we identify four types of errors. Table 1 shows that NAÏVE produces 11.5% invalid claims, often due to mislabeled false claims that are actually valid. It also generates high number of misattributed (28.9%) claims that cite the wrong chapters in the produced chain-ofthought. 17.3% of the claims are duplicated, likely because Claude, lacking structured input, hallucinates source chapters. 15.4% of the claims also include explicit references to chapter numbers or direct quotes,9 which makes claim verification significantly easier by revealing the evidence location. Beyond these errors, the NAÏVE pipeline is costly at an estimate of $0.07 USD per claim. To generate 19K claims, this would cost around $1,330."
        },
        {
            "title": "2.3 Claim generation with CLIPPER",
            "content": "To produce more valid and grounded claims, we use compressed representations of the book conthat number. We clean the manually downloaded books by removing supplementary content to prevent models from using these metadata as shortcuts for event retrieval. 7https://chatgptiseatingtheworld.com/wpcontent/uploads/2025/01/Unredacted-Reply-of-Plaintiffs1.pdf shows that Llama models might have been trained on LibGen book data. 8We do not ask Claude to validate the generated claims, as this is much more challenging task (as seen by Claudes 40.3% accuracy on NoChaone that resembles the very problem we aim to address in this paper. 9This happens despite explicit formatting instructions. CATEGORY NAÏVE CLIPPER ERROR DEFINITION EXAMPLE Invalid 11.5% 9.1% Misattribution 28.9% 4.6% 15.4% 0.0% Explicit references Duplication 17.3% 3.0% The claim is incorrect with respect to the book text, or the true/false claim pair is invalid. Anne rejects three marriage proposals during her time at Redmond College: from Charlie Sloane, Gilbert Blythe, and Roy Gardner, all because she doesnt love them. (This false claim is not entirely false because Anne really didnt love them or wasnt initially aware of her romantic feelings.). The claim is valid, but the associated explanation does not cite the correct chapters. Dr. Sheppard...was the last person known to have seen Roger Ackroyd alive at 8:50 PM on the night of the murder, and he later assisted Hercule Poirot in the investigation while simultaneously concealing Ralph Paton in nursing home. (The explanation cites Chapter 1, 4, 16, and 20, but misses Chapter 24, which mentions that Ralph is in nursing home) The claim is easier to verify since it includes direct quotes and chapter references, eliminating the need for event retrieval. Alices pursuit of the White Rabbit, which begins with her following him down rabbit hole in Chapter 1, continues throughout her adventure, including an encounter in the King and Queen of Hearts court in Chapter 11 where the Rabbit acts as herald. The claim describes the same events as another. Although their content is similar, differences in wording may allow both to pass our deduplication process. \"Dorian Grays cruel rejection of Sibyl Vane after her poor performance as Juliet leads to her suicide, which Dorian callously dismisses by attending the opera the following night, resulting in the first noticeable change in his portrait [...]\" versus \"Dorian Grays cruel rejection of Sibyl Vane after her poor performance as Juliet leads to her suicide, causing the first visible change in his portrait [...] culminate in his murder of Basil Hallward years later [...].\" Table 1: Error types among claims produced by NAÏVE (52) and CLIPPER (66) based on six books from Table 5. Examples are selected from NAÏVE claims. CLIPPER produces much fewer claims with errors than NAÏVE. tent, namely chapter outlines and book summaries. These intermediate forms help anchor claims to specific events in the book, reducing the need to search through the entire text for relevant details. Additionally, this approach makes it easier to generate claims about lower-level events, addressing major limitation of the NAÏVE approach. We take two-step approach to generating claims: (1) first compressing the books into chapter outline and book summary and then (2) generating pairs of true/false claims at different scopes based on these compressed representations. (1a) Compressing books into summaries: Book summaries provide global context for claim generation, ensuring that each claim is consistent with the entire book. We prompt GPT-4o to summarize the entire book into few paragraphs. The summaries average 618 tokens in length. (1b) Compressing books into chapter outlines: Chapter outlines provide list of fine-grained events that can be used to construct grounded claims. We prompt Claude to generate the outline chapter-by-chapter.10 Specifically, we provide each chapter text and instruct the LLM to return structured outline, which includes synopsis, major events (57 per chapter), and character list. Our compression rate is 10.0%, calculated by averaging the ratio of chapter outline length (8,745 tokens on average) to full book length (90,437 tokens on average) across all books. 10We set temperature=0.0, max_tokens=4096. We use Claude instead of GPT-4o because Claude includes more concrete and objective events for the outline, whereas GPT-4o tends to interpret events before including them. (2a) Generating claims from compressed narratives: Now that we have compressed our book into chapter outlines and book summaries, we use these components to generate true/false claims. To enable reasoning across different token ranges, we synthesize claims at two different scopes: > Book-level claims: Claude is prompted to identify 23 key events from the outlines of at least 2 chapters, then use them to generate claim. These claims require models to have global understanding spanning multiple parts of the book. > Chapter-level claims: Given the book summary and single chapter outline, Claude is instructed to identify 2-3 key events of the chapter and use them to write claim. While these claims do not necessitate global reasoning, they still require the model to search for correct chapter within long text and perform intra-chapter reasoning (4.5). (2b) Deduplicating and validating generated claims: We use Claude to deduplicate generated claims (i.e. removing claims that have similar wording). As an additional filtering step, we use GPT-4o to validate the claims against the source chapter outlines by prompting it to assess whether all parts of claim are supported by the outline.11 This step further emphasizes an advantage of CLIPPER: unlike the NAÏVE approach, our method allows for claim verification using the compressed chapter outline. To evaluate the reliability of LLM-based filtering, we manually review 72 claim pairs and only disagree in one instance, where GPT-4o deems claim valid that we find too subjective. Overall, 59.4% 11We opt for GPT-4o to mitigate potential self-biases (Xu et al., 2024b; Panickssery et al., 2024; Li et al., 2025). of the original claims are removed as duplicates, while 2.4% are filtered out as invalid."
        },
        {
            "title": "2.4 Human validation of CLIPPER claims",
            "content": "We use the same setup as described in 2.2 to manually evaluate 66 claims generated by the CLIPPER pipeline. Table 1 provides detailed breakdown of issues flagged in these claims, such as explicit references, invalidity, duplication, or misattribution. Notably, 83.3% of the 66 claims are found to be completely free of errorsa significant improvement compared NAÏVEs 26.9%. CLIPPER also costs less at $0.05 USD per claim compared to NAÏVE ($0.07 per claim) and human annotators ($1.7 per claim based on Karpinska et al. 2024)."
        },
        {
            "title": "2.5 Validating CLIPPER chain-of-thoughts",
            "content": "We evaluate the groundedness of chain-of-thought reasoning for claims in our dataset by prompting an LLM to check if every event mentioned in the chain are supported by the chapter outline. We compute accuracy as the percentage of events in the CoT for true claims that are grounded in the book. To scale up evaluation, we use an LLM judge, DeepSeekR1-Distill-Llama-70B (DeepSeek-AI et al., 2025), instead of human evaluation. We find that 98.5% of CoTs are grounded. The remaining ungrounded CoTs typically involve events open to multiple interpretations. Compared to NAÏVE, CLIPPERs CoTs are significantly easier to verify due to their explicit chapter references."
        },
        {
            "title": "CLIPPER data",
            "content": "Having shown that CLIPPER produces synthetic data of high quality, we now investigate the effects of training on such data. We apply supervised finetuning (SFT) to three models on our dataset: ProLong-512K-8B-Base (Gao et al., 2024c),13 Llama-3.1-8B-Instruct (Dubey et al., 2024), and Qwen2.5-7B-Instruct (Team, 2024).14 Our top model, Llama-CLIPPER, achieves nearly three times the test set performance of Llama-Instructboosting accuracy from 27.9% to 76%while showing substantial gains in longcontext reasoning and narrative understanding on 12See detailed cost analysis in A.4. 13Despite the name, this model has undergone instruction tuning before. The ProLong team ran continual pre-training on Llama-3-8B-Instruct to get this model. 14In subsequent sections, we refer to these models as ProLong-Base, Llama-Instruct, and Qwen-Instruct. tasks like NoCha, NarrativeQA, and MuSR. Moreover, all of our models outperform all existing <10B models on the NoCha benchmark."
        },
        {
            "title": "3.1 Training setup",
            "content": "Data splits: We divide our dataset into three parts: 16K claims (8K true/false pairs) for training, 2K for validation, and 1K for testing. Notably, the books in the test set do not overlap with those in the training or validation sets. For each entry, we combine the book text and claim to form the user prompt, and include the chain of thought reasoning along with the final answer as the assistants message (see Figure 16). Hyperparameters: After performing hyperparameter tuning, we find that learning rate of 1e-6 and batch size of 16 yield the best performance on our dev set.15 We then fine-tune Qwen-Instruct, Llama-Instruct, and ProLong-Base using this configuration for one epoch on our full training set. Ablation on the effect of claim scope: Our dataset consists of 8K book-level claims and 8K chapter-level claims. To evaluate how claim scope affects performance, we fine-tune ProLong-Base separately on each subset, resulting in ProLongCLIPPER-chapter and ProLong-CLIPPER-book. Ablation on the effect of data length: Previous research has found that SFT on short data can improve long-context performance in tasks like QA and summarization more effectively than SFT on long data (Dubey et al., 2024; Gao et al., 2024c). Given that our dataset consists of lengthy texts averaging 90K tokens, we investigate whether fine-tuning on shorter documents can similarly enhance long-context claim verification. We use WritingPrompts (Fan et al., 2018), dataset of 300K human-written stories with an average length of 742 tokens. Since the texts are much shorter, we directly extract claims without generating outlines or summaries.16 We collect 19K claims and train on ProLong-Base to get ProLong-WritingPrompts.17 15We perform hyperparameter tuning on learning rates of 1e5, 1e-6, and 1e-7, along with batch sizes of 16 and 32. Tuning is done for one epoch on subset of 2K training samples. Due to high GPU costs (each epoch takes 5 hours), we only conduct tuning on ProLong-Base only. 16We use prompt similar to the one in 2.3. 17After doing hyperparameter tuning on 2K training samples, we decide on the learning rate of 1e-5 and batch size of 16 as the best training configurations. We tested learning rates of 1e-5, 1e-6, 1e-7 and batch sizes of 8, 16, 32, 64. Models CLIPPER-test NoCha NarrativeQA Bench QA MuSR Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct ProLong-512K-8B-Instruct Qwen2.5-7B-CLIPPER Llama-3.1-8B-CLIPPER ProLong-512K-8B-CLIPPER ProLong-512K-8B-WritingPrompts 51.0% 27.9% 34.5% 73.9% 76.0% 75.0% 63.0% 24.1% 16.5% 16.9% 32.4% 32.2% 32.3% 24.1% 40.3% 47.7% 44.0% 46.0% 49.0% 49.0% 31.0% 35.3% 47.8% 42.6% 42.3% 46.5% 38.5% 35.8% 41.2% 40.3% 42.3% 45.2% 43.6% 44.5% 45.2% Table 2: Model accuracy on claim verification (CLIPPER-test, NoCha) and narrative understanding benchmarks (NarrativeQA, Bench QA, MuSR). Finetuning models using CLIPPER improves performance on claim verification and narrative understanding."
        },
        {
            "title": "3.2 Evaluation",
            "content": "Beyond claim verification, we expect that training on our synthetic dataset will also improve performance on related tasks. Therefore, we include both reasoning and narrative understanding benchmarks that vary in input lengths and tasks. Claim verification: To measure accuracy, we follow NoChas approach to calculate the percentage of cases in which model correctly verifies both true and false claims within given pair. CLIPPER-test contains 1,000 true/false claim pairs drawn from 53 books, evenly split between book-level and chapter-level claims. NoCha (Karpinska et al., 2024) consists of 1,001 true/false claim pairs about recent fiction books (up to 336k tokens). These claims, crafted by annotators familiar with the books, are much harder to verify compared to those in CLIPPER-test. We explore whether SFT on the relatively simpler claims in CLIPPER can generalize to this challenging set of examples. General narrative understanding: We use three existing benchmarks as detailed below. NarrativeQA (Koˇciský et al., 2018) is longform Q&A benchmark that requires models to process entire books or movie scripts to answer provided questions. The benchmark consists of 1,572 stories and summaries as well as 46,675 humanwritten questions. We use the HELMET implementation (Yen et al., 2024) for this benchmark. Bench QA (Zhang et al., 2024) is long-form Q&A benchmark that requires models to answer 351 questions about novels. We use the HELMET implementation, but remove the maximum output length limit, and use GPT-4os judgment as metric instead of ROUGE F1 (see C.1 for explanation). MuSR (Sprague et al., 2024) includes 756 algorithmically generated problems such as murder mysteries, object placement questions, and team allocation optimization. We use the LM Harness (Gao et al., 2024a) implementation."
        },
        {
            "title": "4 Results & analysis",
            "content": "Our fine-tuned models set new state of the art for <10B models on long-context reasoning benchmarks while also improving baseline performance on related narrative understanding tasks. Below, we discuss our results in detail."
        },
        {
            "title": "4.1 CLIPPER models outperform baselines on",
            "content": "narrative claim verification On CLIPPER-test, our fine-tuned models significantly outperform the instruct models they are initialized from (referred to as baselines),18 as shown in Table 2. For example, Qwen-CLIPPER achieves over 20% performance gain compared to QwenInstruct, while Llama-CLIPPER sees nearly triple the performance of Llama-Instruct. These substantial improvements demonstrate the effectiveness of CLIPPER-generated data. Fine-tuning on our data improves performance on NoCha: similar trend is observed on NoCha. The performance improvements range from an 8% gain for strong baselines like QwenInstruct to dramatic twofold increase for weaker baselines such as Llama-Instruct and ProLongInstruct. It is worth noting that all three baseline models initially perform below the random chance baseline of 25%, but our fine-tuned models consistently surpass this threshold. Performance gap between CLIPPER-test and NoCha: We note that the performance gap between NoCha and CLIPPER-test is likely due to the 18ProLong-CLIPPER is initialized from ProLong-Base instead of ProLong-Instruct. However, since performing evaluation intended for instruct models on continually pretrained model may not be ideal, we exclude ProLong-Bases results from Table 2. As shown in Table 19, ProLong-Base performs significantly worse than ProLong-Instruct on CLIPPER-test. Claim: \"Jacques consistently uses pastoral themes to express his love for Belle-bouche, first through metaphorical discussions of shepherds and shepherdesses, and later while walking through an orchard and discussing Arcadian lovers.\" Llama-Instructs CoT: The statement claims that Jacques consistently uses pastoral themes to express his love for Belle-bouche. To evaluate this statement, we need to examine the text and identify instances where Jacques uses pastoral themes to express his love for Belle-bouche. The text mentions that Jacques discusses crooks, flowers, and shepherdesses with Belle-bouche, and later, while walking through an orchard, he talks about Arcadian lovers, specifically mentioning Strephon, Chloe, Corydon, Daphnis, and Narcissus. These instances suggest that Jacques does use pastoral themes to express his love for Belle-bouche. However, we need to consider whether this is consistent pattern throughout the text.\" Answer: TRUE Llama-CLIPPERs CoT: Here are the relevant details from the text: Chapter 2: Jacques engages in metaphorical discussion about shepherds and shepherdesses, expressing his love for Belle-bouche. Chapter 17: Jacques and Belle-bouche walk through the orchard, discussing the twentieth of May as day for lovers. Relationship: Recurring theme of pastoral love In Chapter 2, Jacques engages in metaphorical discussion about shepherds and shepherdesses to express his love for Belle-bouche. This theme continues in Chapter 17, where Jacques and Belle-bouche walk through an orchard, discussing the twentieth of May as day for lovers. This connection shows Jacques consistent use of pastoral imagery to convey his romantic feelings for Belle-bouche throughout the story.\" Answer: TRUE Table 3: Comparison of CoT responses produced by Llama-Instruct and Llama-CLIPPER for claim from John Esten Cookes The Youth of Jefferson. Llama-CLIPPERs CoT response contains more explicit chapter references (highlighed in green) and details compared to that of Llama-Instruct (highlighted in red). nature of the events involved in the claims. While CLIPPER-test consists of synthetic claims derived from events in model-generated outlines, NoChas human-written claims may involve reasoning about low-level details that may not typically appear in such generated outlines. Future work could incorporate more low-level events into chapter outlines to create more diverse set of claims."
        },
        {
            "title": "4.2 Fine-tuning on CLIPPER improves on other",
            "content": "narrative reasoning tasks Beyond long-context reasoning, our models also show improvements in narrative understanding and short-context reasoning tasks. On NarrativeQA, which requires comprehension of movie scripts or full books, our best-performing models, LlamaCLIPPER and ProLong-CLIPPER, achieve 2% and 5% absolute improvement over their respective baselines. Similarly, on MuSR, short-form reasoning benchmark, our strongest model, QwenCLIPPER, achieves 45.2% accuracy, surpassing the 41.2% baseline. However, these improvements are not consistent across all tasks. On Bench QA, only Qwen-CLIPPER outperforms the baseline by approximately 7%. In contrast, Llama-CLIPPER and ProLong-CLIPPER show slight performance declines of up to 4%. Thus, while fine-tuning on CLIPPER data improves performance on reasoning and some aspects of narrative understanding, its transferability is not universal across domains."
        },
        {
            "title": "4.3 Short-context claim data is less helpful",
            "content": "Contrary to prior studies suggesting short-form data benefits long-context tasks (Dubey et al., 2024; Gao et al., 2024c) more than long data, our results show otherwise. While ProLong-WritingPrompts, trained on short data, outperforms baselines, it underperforms across all four long-context benchmarks compared to models fine-tuned on our data. This underscores the need for high-quality longcontext data generation pipelines like CLIPPER."
        },
        {
            "title": "4.4 Finetuning on CoTs results in more",
            "content": "informative explanations We evaluate the groundedness of CoT reasoning generated by our fine-tuned models using DeepSeek-R1-Distill-Llama-70B (2.5). Here, reasoning chain is counted as grounded when every plot event in the chain can be found in the chapter outline that it cites. Table 21 shows that fine-tuning significantly improves groundedness across all models, with ProLong-CLIPPER achieving the highest rate (80.6%), followed closely by Llama-CLIPPER (75.9%). Looking closer at the content of the explanations  (Table 3)  , the baseline model (Llama-Instruct) often gives generic response without citing any evidence, whereas Llama-CLIPPER explicitly references Chapter 9 and specifies the cause-and-effect relationship."
        },
        {
            "title": "4.5 Small models struggle with book-level",
            "content": "reasoning Trained only on 8K chapter-level claims, ProLongCLIPPER-chapter outperforms ProLong-CLIPPERbook on both chapterand book-level test subsets  (Table 20)  . This likely reflects the limitations of smaller models (7B/8B) in handling the complex reasoning required for book-level claims, aligning with prior findings (Qi et al., 2024). The performance gap between the models is modest (4.2%), and we leave exploration of larger models (>70B) to future work due to compute constraints."
        },
        {
            "title": "4.6 Fine-tuned models have a difficult time",
            "content": "verifying False claims To study cases where fine-tuned models struggle, we analyze Qwen-CLIPPER outputs. Among 1,000 book-level claim pairs in CLIPPER-test, the model CATEGORY FREQ (%) TRUE CLAIM FALSE CLAIM Event 43.2 Person 31.6 Object 15. Location 13.7 Time 6.3 The Polaris unit, initially assigned to test new audio transmitter on Tara, explores the planets surface using jet boat without landing. The Polaris unit, initially assigned to test new audio transmitter on Tara, explores the planets surface by landing their spaceship. The cattle herd stolen from Yeager by masked rustlers is later found in General Pasquales possession at Noche Buena. The cattle herd stolen from Yeager by masked rustlers is later found in Harrisons possession at Noche Buena. The alien structure Ross enters contains both chamber with jelly-like bed and control panel capable of communicating with other alien vessels. The alien structure Ross enters contains both chamber with metal bed and control panel capable of time travel. Costigan rescues Clio twice: first from Roger on his planetoid, and later from Nevian city using stolen space-speedster. Costigan rescues Clio twice: first from Roger on his planetoid, and later from Triplanetary city using stolen space-speedster. Jean Briggerlands meeting with ex-convicts Mr. Hoggins and Mr. Talmot, where she suggests burglary target, follows failed attempt on Lydias life involving speeding car on the sidewalk. Jean Briggerlands meeting with ex-convicts Mr. Hoggins and Mr. Talmot, where she suggests burglary target, precedes failed attempt on Lydias life involving speeding car on the sidewalk. Affect 4.2 David Mullins, who initially expresses skepticism about Chesters hiring, later fires Chester on false pretenses and immediately replaces him with Felix. David Mullins, who initially expresses enthusiasm about Chesters hiring, later fires Chester on false pretenses and immediately replaces him with Felix. Table 4: taxonomy of perturbations that cause false claim to be misclassified as true. Corresponding details in true and false claims are respectively highlighed in green and red. Frequencies do not sum to 100%, as some claims receive multiple labels. Category definitions and more detailed analysis in D.2. fails to verify 37 true claims and 97 false claims, aligning with NoCha findings (Karpinska et al., 2024) that models struggle more with false claims. We investigate perturbations that make false claims appear true and present taxonomy with examples in Table 4, with further details in D.2."
        },
        {
            "title": "5 Related work",
            "content": "Long-context language modeling: The context windows of LLMs have expanded significantly (OpenAI et al., 2024; Dubey et al., 2024; Team et al., 2024; Yang et al., 2025), thanks to position interpolation and extrapolation techniques (Press et al., 2022; Su et al., 2023; Peng et al., 2023), and efficient attention implementation (Dao et al., 2022; Dao, 2023; Liu and Abbeel, 2023). In addition, longer training data has been utilized during the continual pretraining stage (Dubey et al., 2024; Lieber et al., 2024; Xiong et al., 2023) or alignment stage (Bai et al., 2024; Xiong et al., 2024; An et al., 2024). Instruction-tuning data generation: Shortform instruction-tuning data generation methods either induce instruction data from sample outputs (Honovich et al., 2022; Zhou et al., 2023; Li et al., 2024) or generate instruction-output pairs simultaneously (Wang et al., 2023b). In long-context scenarios, instruction-following data is synthesized through instruction induction from long-form documents (Bai et al., 2024; Pham et al., 2024; Köksal et al., 2023), random document segments (Xiong et al., 2023; Dubey et al., 2024; Yang et al., 2025), or bootstrapping short documents (An et al., 2024; Xu et al., 2024a; Wu et al., 2024a; Wang et al., 2024). CLIPPER uses instruction induction from compressed document representations. Reasoning alignment: Previous work have explored variety of approaches to improving LLMs reasoning abilities, including inference-time scaling (OpenAI, 2024; DeepSeek-AI et al., 2025; Muennighoff et al., 2025), prompting (Wei et al., 2023; Kojima et al., 2023; Yao et al., 2023; Wang et al., 2023a), and finetuning LLMs on reasoning data (Chung et al., 2022; Huang et al., 2023; Puerto et al., 2024; Yeo et al., 2025). These reasoning data are either human-written rationale (AlKhamissi et al., 2023) or chain of thoughts distilled from larger models (Hsieh et al., 2023; Li et al., 2023; Ho et al., 2023; Zelikman et al., 2022). We find that finetuning on CoTs improves model explanations."
        },
        {
            "title": "6 Conclusion",
            "content": "We present CLIPPER, compression-based synthetic data generation pipeline tailored to narrative claim generation. Using CLIPPER, we create dataset of 19K true/false claims at both the book and chapter level. Our fine-tuned models achieve state-of-the-art performance among <10B models on claim verification and narrative understanding tasks. Future work could explore the impact of book-level claims on larger models and experiment with approaches to generating more challenging claims to bridge the gap between synthetic test set and human-written benchmarks like NoCha."
        },
        {
            "title": "Limitations",
            "content": "We only perform hyperparameter tuning on ProLong-Base due to the high cost of the training process. To put things into perspective, training model on our full test set requires approximately 50 hours on 8 A100 GPUs, each costing $2 per hour to rent. Even training on our tuning subset takes 6 hours. Therefore, extending training further is prohibitively expensive. Similarly, we do not hire human annotators to write claims for our dataset due to the prohibitive cost and the need for numerous annotators who have thoroughly read the books  (Table 5)  . While this decision may result in less complex claims (4.1), our approach offers greater adaptability to new books while significantly reducing costs."
        },
        {
            "title": "Ethical considerations",
            "content": "All scientific artifacts, including generative models and book texts from Project Gutenberg, are used in accordance with their intended purpose to ensure ethical and responsible research practices."
        },
        {
            "title": "Acknowledgment",
            "content": "We extend special gratitude to Marzena Karpinska for benchmarking our models on NoCha and giving us helpful insights into the models behaviors. This project was partially supported by awards IIS2046248, IIS-2312949, and IIS-2202506 from the National Science Foundation (NSF)."
        },
        {
            "title": "References",
            "content": "Badr AlKhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, and Mona Diab. 2023. OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models. In Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 128138. ArXiv:2305.12001 [cs]. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. 2024. Make your llm fully utilize the context. Preprint, arXiv:2404.16811. Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024. LongAlign: Recipe for Long Context Alignment of Large Language Models! arXiv preprint. ArXiv:2401.18058 [cs]. Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. BooookScore: systematic exploration of book-length summarization in the era of LLMs. arXiv preprint. ArXiv:2310.00785 [cs]. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv preprint. ArXiv:2210.11416 [cs]. Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. Preprint, arXiv:2307.08691. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Preprint, arXiv:2205.14135. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. 2025. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing Chat Language Models by Scaling High-quality Instructional Conversations. arXiv preprint. ArXiv:2305.14233 [cs]. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889898, Melbourne, Australia. Association for Computational Linguistics. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024a. framework for few-shot language model evaluation. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024b. How to Train Long-Context arXiv preprint. Language Models (Effectively). ArXiv:2410.02660. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. 2024c. How to train long-context language models (effectively). Preprint, arXiv:2410.02660. Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023. News summarization and evaluation in the era of gpt-3. Preprint, arXiv:2209.12356. Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1485214882, Toronto, Canada. Association for Computational Linguistics. Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. 2022. Instruction induction: From few examples to natural language task descriptions. Preprint, arXiv:2205.10782. Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model In Findings of the Association for Compusizes. tational Linguistics: ACL 2023, pages 80038017, Toronto, Canada. Association for Computational Linguistics. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10511068, Singapore. Association for Computational Linguistics. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. 2023. DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. arXiv preprint. ArXiv:2309.14509 [cs]. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. One Thousand and One Pairs: \"novel\" challenge for long-context language models. arXiv preprint. ArXiv:2406.16264 [cs]. Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit FABLES: Evaluating faithfulness and content selection in book-length summarization. arXiv preprint. ArXiv:2404.01261 [cs]. Iyyer. 2024. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners. Preprint, arXiv:2205.11916. Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. 2023. LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction. arXiv preprint. ArXiv:2304.08460 [cs]. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2024. Tülu 3: Pushing frontiers in open language model post-training. Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models. arXiv preprint. ArXiv:2402.14848 [cs]. Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. 2025. Preference leakage: contamination problem in llm-as-a-judge. Preprint, arXiv:2502.01534. Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic chain-of-thought distillation: Small models can also think step-by-step. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2665 2679, Toronto, Canada. Association for Computational Linguistics. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. 2024. Self-alignment with instruction backtranslation. Preprint, arXiv:2308.06259. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Itay Dalmedigos, Erez Safahi, Jhonathan Osin, Shaked Meirom, Yonatan Belinkov, Shai ShalevShwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. 2024. Jamba: hybrid transformer-mamba language model. Preprint, arXiv:2403.19887. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Hao Liu and Pieter Abbeel. 2023. Blockwise parallel transformer for large context models. Advances in neural information processing systems. Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics, 6:317328. Place: Cambridge, MA Publisher: MIT Press. Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. OpenAI. 2024. o1 System Card. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. Preprint, arXiv:2404.13076. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. Preprint, arXiv:1912.01703. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. YaRN: Efficient Context Window Extension of Large Language Models. arXiv preprint. ArXiv:2309.00071 [cs]. Chau Minh Pham, Simeng Sun, and Mohit Iyyer. 2024. Suri: Multi-constraint Instruction Following for Long-form Text Generation. arXiv preprint. ArXiv:2406.19371 [cs]. Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. arXiv preprint. ArXiv:2108.12409 [cs]. Haritz Puerto, Tilek Chubakov, Xiaodan Zhu, Harish Tayyar Madabushi, and Iryna Gurevych. 2024. Fine-tuning with divergent chains of thought boosts reasoning through self-correction in language models. Preprint, arXiv:2407.03181. Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, and James Glass. 2024. Quantifying generalization complexity for large language models. Preprint, arXiv:2410.01769. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical Report. ArXiv:2412.15115 [cs]. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2024. MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning. arXiv preprint. ArXiv:2310.16049 [cs]. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint. ArXiv:2104.09864 [cs]. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Qwen Team. 2024. Qwen2.5: party of foundation models. Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, and Furu Wei. 2024. Bootstrap your own context length. Preprint, arXiv:2412.18860. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. Preprint, arXiv:2212.10560. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint. ArXiv:2201.11903. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Huggingfaces transformers: State-of-the-art natural language processing. Preprint, arXiv:1910.03771. Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li. 2024a. Long context alignment with short instructions and synthesized positions. Preprint, arXiv:2405.03939. Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, and Wei Zhang. 2024b. LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios. arXiv preprint. ArXiv:2411.07037. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level prompt engineers. Preprint, arXiv:2211.01910. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. 2023. Effective Long-Context Scaling of Foundation Models. arXiv preprint. ArXiv:2309.16039 [cs]. Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos. 2024. From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data. Preprint, arXiv:2406.19292. Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad Shoeybi, and Bryan Catanzaro. 2024a. Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities. Preprint, arXiv:2407.14482. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Yang Wang. 2024b. Pride and prejudice: Llm amplifies self-bias in selfrefinement. Preprint, arXiv:2402.11436. An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. 2025. Qwen2.5-1M Technical Report. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Preprint, arXiv:2305.10601. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. Helmet: How to evaluate longcontext language models effectively and thoroughly. arXiv preprint arXiv:2410.02694. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. Preprint, arXiv:2502.03373. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Preprint, arXiv:2203.14465. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024. bench: Extending long context evaluation beyond 100k tokens. Preprint, arXiv:2402.13718."
        },
        {
            "title": "A Data Collection",
            "content": "A.1 Books used in manual analysis Table 5 lists six books used in our manual analysis. These books are chosen due to the annotators familiarity with the content, which eases the manual verification process. A.2 Does memorization have an effect on claim verification performance? We measure the performance of the models used for finetuning on our test set, with and without book text. We provide the book title and author name where the book text is not provided. Our hypothesis is that if the model does better than the random chance baseline (25% accuracy) without the book text, then the claims are either too easy or can be verified without even reasoning over the texts. Table 6 shows that all baseline models perform below random chance, significantly trailing behind the performance achieved when the book text is included in the claim verification prompt. These results indicate that even if model has memorized the book texts or generated claims, such memorization does not affect its performance on the task itself. A.3 Are the True/False claims distinguishable without the book texts? We ask the question of whether distinguishing between True and False claims is inherently too easy. If so, then the high performance of the fine-tuned models may be attributed merely to their ability to detect formatting cues rather than actually reasoning. To investigate this, we prompt both baseline and fine-tuned models to verify claims without providing any book texts or metadata. Our hypothesis is that if model performs better than random chance under these conditions, then the claims are likely too easily distinguishable based on their formatting alone. As shown in Table 7, even after fine-tuning, the models perform only marginally above random guessing. We conclude that, without the contextual information from the book text, True/False claims are not easily distinguishable. A.4 Cost Analysis each stage of the pipeline is performed by Claude. Table 9 shows the estimated per claim cost for the naïve versus main approach based on estimated cost for 6 books. For human annotation, NoCha (Karpinska et al., 2024) reports that their total cost of annotating 1,001 claim pairs is $3,327 USD, so each claim costs around $1.7. A.5 Prompts Table 10 shows stages to construct CLIPPER, mapped to their corrresponding prompts. A.6 Using DeepSeek-Distill to measure CoT groundedness We evaluate the model on 66 annotated claims from 2.4 and measure its agreement with human annotations  (Table 11)  . Among the models tested, DeepSeek-Distill aligns most closely with human judgments, with only one instance of disagreement, outperforming other models like GPT-4o (10 disagreements) and LLaMA-3.1-70B-Instruct (3 disagreements). Although Llama-70B performs comparably, it fails to provide clear explanations for its decisions and instead generating generic reasoning messages that lack specificity to samples. Therefore, we use DeepSeek-Distill to measure CoT groundedness in our dataset."
        },
        {
            "title": "B Training",
            "content": "B.1 Codebases To fine-tune models in the Llama family, we adopt the ProLong codebase,19 which integrates PyTorch (Paszke et al., 2019) and Hugging Face (Wolf et al., 2020) for model training, FlashAttention2 (Dao, 2023) for efficient attention computation, and DeepSpeed Ulysses (Jacobs et al., 2023) for sequence parallelism, enabling training across 8 A100 GPUs. For fine-tuning Qwen-Instruct (Qwen-Instruct), we use the 360-LlamaFactory codebase,20 modification of Llama-Factory21 that incorporates sequence parallelism via zigzag ring attention (Liu and Abbeel, 2023; Liu et al., 2023). We choose ProLong-Base (ProLong-base) over ProLong-Instruct (ProLong-instruct) based on small fine-tuning experiment, where we fine-tune both ProLong-instruct and ProLong-base on 2K training examples. This experiment shows that Table 8 shows the cost incurred by running each stage of our data synthesis pipeline. With the exception of deduplication, which is done by GPT-4o, 19https://github.com/princeton-nlp/ProLong 20https://github.com/Qihoo360/360-LLaMA-Factory 21https://github.com/hiyouga/LLaMA-Factory"
        },
        {
            "title": "Prompt for Chapter Outline Generation",
            "content": "Your task is to create detailed and objective outline for Chapter {order} of book. You will be provided with the text of Chapter {order}. Ensure that your outline faithfully represents the content of the text. First, carefully read the text of Chapter {order}: <current_chapter> {curr} <current_chapter> Finally, create an outline for Chapter {order} using the following format: <synopsis>A one-sentence summary of the current chapter.</synopsis> <events>A chronological list of at most 7 major events in the chapter. The list should formatted as numbered list. Each event should be one sentence long, describing specific details on what happens, where it happens, and which characters are involved. DO NOT include subjective interpretation of the events.</events> <characters>A numbered list of characters in the chapter. Include only those that are mentioned in the major events. Each character should have the format: [character full name]: [character role/relationship with the main characters], [physical appearance (if mentioned)], [personality (if mentioned)], first seen at [the first setting the character is in], last seen at [last setting the character is in].</characters> Now, create an objective outline for Chapter {order} based on the provided text. Ensure that your outline is concise, coherent, and accurately represents the content of the chapter. <synopsis> [One-sentence summary of the current chapter.] </synopsis> <events> 1. [Event 1: Specific details on specific details on what happens, where it happens, and which characters are involved.] 2. [Event 2: Specific details on what happens, where it happens, and which characters are involved.] 3. [Event 3: Specific details on what happens, where it happens, and which characters are involved.] 4. [Event 4: Specific details on what happens, where it happens, and which characters are involved.] 5. [Event 5: Specific details on what happens, where it happens, and which characters are involved.] 6. [Event 6: Specific details on what happens, where it happens, and which characters are involved.] 7. [Event 7: Specific details on what happens, where it happens, and which characters are involved.] </events> <characters> 1. [Character 1: Character role/relationship with the main characters, physical appearance (if mentioned), personality (if mentioned), first seen at the first setting the character is in, last seen at the last setting the character is in.] (Repeat the format for each character mentioned in the major events.) </characters> Remember to focus on the objective representation of the chapter content and avoid adding personal opinions or interpretations. Good luck! Figure 3: Prompt for generating chapter outlines in our dataset. Title Author Publication Year Number of Tokens Number of Chapters Anne of the Island Alice in Wonderland The Murder of Roger Ackroyd The Picture of Dorian Gray Frankenstein The Adventures of Tom Sawyer Mark Twain L. M. Montgomery Lewis Carroll Agatha Christie Oscar Wilde Mary Shelley 1915 1865 1926 1890 1818 111,337 36,691 98,602 105368 97,574 97,968 41 12 27 20 24 35 Table 5: Six books used in our manual analysis. Books are chosen due to familiarity with the content."
        },
        {
            "title": "Prompt for Generating Book Summaries",
            "content": "Your task is to write summary for the book below, make sure to include vital information related to key events, backgrounds, settings, characters, their objectives, and motivations. You must briefly introduce characters (with their full name), places, and other major elements if they are being mentioned for the first time in the summary. The book may feature non-linear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you should organize the summary so it presents consistent and chronological narrative. The summary must be within 1000 words. The summary should span multiple paragraphs and should be written as single continuous narrative, not as list of bullet points or an outline. DO NOT include the book name in the summary. #Book {book} #Summary Figure 4: Prompt for generating book summaries. Models No Text With Text ProLong-Instruct Llama-Instruct Qwen-Instruct 0.0% 0.0% 0.0% 35.6% 32.8% 51.4% NAÏVE CLIPPER Cost per claim (book-level) Cost per claim (chap-level) $0.09 $0.04 $0.07 $0.02 Table 6: Accuracy on CLIPPERs test set (with and without book texts). Table 9: Estimated cost for our NAÏVE vs CLIPPER approach (rounded to two decimal places) Models Before SFT After SFT ProLong-Instruct Llama-Instruct Qwen-Instruct 0.0% 20.2% 21.7% 25.2% 13.8% 22.9% Table 7: Accuracy on CLIPPERs test set (no book text or metadata provided). Stage Book summary generation Chapter outline generation Book-level claim synthesis Chapter-level claim synthesis Deduplication Verification Total Cost $0.0021 $0.0107 $0.0129 $0.0172 $0.0021 $0.0064 $0.0514 Prompt Chapter outline generation Book summary generation Chapter-level claim extraction Book-level claim extraction Claim deduplication Claim verification Chapter-level claim extraction (NAÏVE) Book-level claim extraction (NAÏVE) Figure 3 4 5 6 7 8, 9, 10, 11, 12 15 14 Table 10: Figure references for each prompt. Judge Models % Agreement GPT-4o Llama-3.1-70b-Instruct DeepSeek-Distill-Llama-70B 84.8% 95.5% 98.5% Table 8: Cost to run pipeline per claim (in US dollars, rounded to four decimal places). Table 11: Percentage of times LLM judges for chain of thought groundedness agree with our manual annotation over 66 samples in Section 2.4. Prompt for Extracting Chapter-level Claims Your task is to create factual statements that incorporate multiple events from Chapter { chapter} of book. These factual statements must be objective and specific, grounded in the specific chapter, and consistent with the entire book. The included events must be specific. The facts cannot contain any interpretations, speculations, or subjective statements. In addition to each fact, provide minimally corrupted version of the fact, which sounds plausible but is wrong based on the entire book. To create valid facts, follow these guidelines step-by-step: 1. Carefully read through the book. 2. In <brainstorm> section: - Identify different events that are strongly related to one another in single chapter. - Consider the relationship between these events, but DO NOT include this brainstorming in the resulting fact. If meaningful relationship is found, move on to the next step; otherwise, proceed to the next fact. 3. Formulate your facts based on this analysis. Ensure that your facts: - Contain single sentence - Are coherent with the entire book - Include multiple detailed and strongly related events from single chapter in the book - Are self-contained and independent of other facts - No part of the fact should be subjective interpretations or speculations - Do not contradict or duplicate existing facts - Do not contain chapter information (e.g. \"In Chapter x, ...\") - Do not contain quotes from the book. 4. Formulate minimally corrupted version of each fact. - Only one aspect of the fact, such as an atomic detail or the relationship between details, should be altered. - The corrupted fact should sound plausible but be clearly wrong based on the entire book. - The corrupted fact should be similar with the original fact in terms of length and sentence structure to make it harder to verify solely based on surface-level features. First, read the book: <book> {book} </book> Finally, review the existing facts: <existing_facts> {existing_facts} </existing_facts> Now, generate as many valid facts as possible based on the provided chapter. Return \"No meaningful fact\" if there is no valid fact. Present your facts in the following format: <facts> <fact_1> <brainstorm> [Your brainstorm notes here] </brainstorm> Fact: [Your fact here] Fact Reasoning: [Your explanation here (including the chapter associated with each event)] Source: [Chapter involved] Corrupted Fact: [Your corrupted fact here] Corrupted Fact Reasoning: [Your explanation here] </fact_1> [Continue with additional facts...] </facts> Remember to create facts that are objectively valid, coherent with the entire book, and demonstrate strong, meaningful relationships between the events from Chapter {chapter}. No part of the fact can include subjective interpretations or generalizations. The relationship must be meaningful. The included events must be SPECIFIC and DETAILED!!!!! Figure 5: Prompt for extracting chapter-level claims Prompt for Extracting Book-level Claims Your task is to create factual statements that incorporate multiple events from multiple chapters of book. These factual statements must be objective and specific, grounded in the involved chapters, and consistent with the entire book. The included events must be specific. The facts cannot contain any interpretations, speculations, or subjective statements. In addition to each fact, provide minimally corrupted version of the fact, which sounds plausible but is wrong based on the entire book. To create valid facts, follow these guidelines step-by-step: 1. Carefully read through the book. 2. In <brainstorm> section: - Identify different events that are strongly related to one another in multiple chapters. - Consider the relationship between these events, but DO NOT include this brainstorming in the resulting fact. If meaningful relationship is found, move on to the next step; otherwise, proceed to the next fact. 3. Formulate your facts based on this analysis. Ensure that your facts: - Contain single sentence - Are coherent with the entire book - Include multiple detailed and strongly related events from multiple chapters in the book - Are self-contained and independent of other facts - No part of the fact should be subjective interpretations or speculations - Do not contradict or duplicate existing facts - Do not contain chapter information (e.g. \"In Chapter x, ...\") - Do not contain quotes from the book. 4. Formulate minimally corrupted version of each fact. - Only one aspect of the fact, such as an atomic detail or the relationship between details, should be altered. - The corrupted fact should sound plausible but be clearly wrong based on the entire book. - The corrupted fact should be similar with the original fact in terms of length and sentence structure to make it harder to verify solely based on surface-level features. First, read the book: <book> {book} </book> Finally, review the existing facts: <existing_facts> {existing_facts} </existing_facts> Now, generate as many valid facts as possible based on the provided book. Return \"No meaningful fact\" if there is no valid fact. Present your facts in the following format: <facts> <fact_1> <brainstorm> [Your brainstorm notes here] </brainstorm> Fact: [Your fact here] Fact Reasoning: [Your explanation here (including the chapter associated with each event)] Source: [Chapters involved] Corrupted Fact: [Your corrupted fact here] Corrupted Fact Reasoning: [Your explanation here] </fact_1> [Continue with additional facts...] </facts> Remember to create facts that are objectively valid, coherent with the book, and demonstrate strong, meaningful relationships between the events from multiple chapters. No part of the fact can include subjective interpretations or generalizations. The relationship must be meaningful. The included events must be SPECIFIC and DETAILED!!!!! Figure 6: Prompt for extracting book-level claims"
        },
        {
            "title": "Prompt for Deduplicating Claims",
            "content": "You will be given list of facts. Your task is to identify all duplicate facts within this list. fact is considered duplicate if it is exactly the same as another fact, or if it contains the same information as another fact, even if worded differently. Here is the list of facts: <fact_list> {fact_list} </fact_list> To identify the duplicate facts, follow these guidelines step by step: 1. Read through the list of facts carefully. 2. Identify any facts that are exact duplicates of each other, or that convey the same atomic information using different wording. 3. Return list of facts that are duplicates of one other, along with an explanation of why they are duplicates. 4. DO NOT return facts that are not duplicates of any other fact in the list. Here's an example of how your output should look: <example> <example_fact_list> 1. Jim worked hard, so he got promotion. 2. Jim's hard work paid off, and he was promoted. 3. Jim and Sarah worked together on the project. 4. Jim worked hard, so he received praise from his boss, who promoted him. </example_fact_list> <example_answer> - 1, 2: These two facts convey the same atomic information but are worded differently. </example_answer> </example> Remember, your goal is to identify all duplicate facts, whether they are exact matches or convey the same information in different words. Be thorough in your analysis and clear in your explanations. If there is no duplication in the list, output \"No duplicates found.\" <answer> - [Index of duplicate facts, separated by commas]: [Explanation of why they are duplicates] - [Index of duplicate facts, separated by commas]: [Explanation of why they are duplicates] (... and so on) </answer> Figure 7: Prompt for de-duplicating claims Prompt for Verifying Claims with GPT-4o (Part 1) You will receive book summary, chapter outline, and claim extracted from that outline. Your task is to verify whether the claim contains detailed information, presents meaningful relationship, and shows consistency with both the book summary and chapter outline. To verify the claim, follow these steps: 1. Read the summary, outline, and claim carefully to understand the context. 2. Decompose the claim into atomic parts. 3. Analyze each atomic part: a. Is the part grounded in the events? b. Does this part contradict any information in the summary or outline? Keep in mind that some books may have discontinuous plots or events, so just because detail is mentioned before or after another in the summary does not mean they are temporally related. 4. Evaluate the relationship between the atomic parts: a. Is the relationship objectively valid and meaningful? b. Is the relationship subjective interpretation or assumption not explicitly stated in the summary or outline? c. Does the relationship make sense based on the book summary and chapter outline? 5. Based on your analysis, provide your reasoning and verification result. Your reasoning should explain why you believe the claim is or is not valid based on the information provided in the book summary and chapter outline. First, read the book summary: <book_summary> {book_summary} </book_summary> Next, review the chapter outline: <chapter_outline> {chapter_outline} </chapter_outline> Finally, consider the following claim: <claim> {claim} </claim> Here are two examples of valid and invalid claims: Figure 8: Prompt for verifying claims with GPT-4o (Part 1) Prompt for Verifying Claims with GPT-4o (Part 2) <example_1> <example_summary> Laura Hand, Daniel Knowe, and Mo Gorch mysteriously return from realm of death with altered memories, orchestrated by their enigmatic music teacher, Mr. Anabin, and the sinister Bogomil . As they grapple with their new realities, including Mo's discovery of his grandmother's death and Daniel's complex feelings for Laura's sister, Susannah, they face trials set by Anabin and Bogomil to remain in the living world. Alongside Bowie, another returned soul, they uncover the truth about their deaths while dealing with eerie supernatural encounters. Laura's newfound magical abilities strain her relationship with Susannah, leading to increasing tensions as Susannah begins to remember the truth. As the trio navigates their altered lives, they become entangled in larger, dangerous game orchestrated by Malo Mogge, Anabin, and Bogomil, who guard the door between life and death. Mo and Susannah discover that Harmony guitar, hidden by Susannah, is the key sought by Malo Mogge, powerful entity seeking immense power. The story culminates in chaotic battle in Lovesend, where Laura, consumed by grief, vows to kill Malo Mogge. After absorbing Mogge's magic and becoming powerful goddess, Laura faces the challenge of balancing her divine powers with her passion for music, while the other characters embrace their new roles. </example_summary> <example_chapter_outline> ## Chapter 15 outline 1. Susannah gets frustrated about Laura and Daniel being close to each other and smashes Laura 's old guitar in fit of anger and frustration. 2. Mo eats breakfast casserole made by Jenny and buys doughnuts and bagels on his way to Laura's house, feeling mix of hunger and sadness. 3. Mo arrives at the Hands' house, where Laura and Daniel are eating ramen to satisfy their unusual hunger. 4. Daniel reveals that he and Laura have swapped ears due to Mr. Anabin's magic, and they discuss the implications of this mistake. 5. Mo shares his encounter with mysterious figure outside his house, leading to heated discussion about Bogomil and Mr. Anabin. 6. The trio creates list of goals to navigate their situation, including staying alive, figuring out how they died, and learning to do magic. 7. They attempt to perform magic by trying to transform saltshaker into hairless cat but fail, leading to further frustration. 8. Mo leaves the Hands' house, and they discover that the entire yard and house are covered in thousands of moths, adding to the surreal nature of their situation. 9. Laura finds the broken guitar pieces in her room, causing confusion and suspicion among the trio. </example_chapter_outline> <example_claim_1> Laura finds the broken guitar pieces in her room, which Susannah smashed in fit of anger and frustration. </example_claim_1> <example_verification_1> <reasoning> 1. Analysis of each atomic part: a. Laura finds the broken guitar pieces in her room: This part is grounded in event 9 of Chapter 15. b. Susannah smashed the guitar in fit of anger and frustration: This part is grounded in event 1 of Chapter 15. 2. Analysis of the relationship between atomic parts: a. The relationship is temporal, and thus valid. Figure 9: Prompt for verifying claims with GPT-4o (Part 2) Prompt for Verifying Claims with GPT-4o (Part 3) b. The relationship is explicit and not subjective interpretation. c. The relationship is grounded in the chapter outline. There is no contradicting information in the summary. Since all considerations are satisfied, the claim is VALID. </reasoning> <result> VALID </result> </example_verification_1> <example_claim_2> Mo shares his encounter with mysterious figure because Laura and Daniel discuss the implications of their swapped hands due to Mr. Anabin's magic. </example_claim_2> <example_verification_2> <reasoning> 1. Analysis of atomic parts: a. Mo shares his encounter with mysterious figure: This part is grounded in event 5 of Chapter 15. b. Laura and Daniel discuss the implications of their swapped hands due to Mr. Anabin's magic: There is no mention of hands being swapped. Even though event 4 discusses the swapping of ears, it does not relate to hands. 2. Analysis of the relationship between atomic parts: a. The relationship is NOT VALID because there is no direct link between Mo sharing his encounter and Laura and Daniel discussing the implications of their swapped hands. b. The relationship is subjective interpretation and not explicitly grounded in the summary or outline. c. The relationship is grounded in the chapter outline. There is no contradicting information in the summary. Since 1a., 2a., and 2b. are not satisfied, the claim is INVALID. </reasoning> <result> INVALID </result> </example_verification_2> Figure 10: Prompt for verifying claims with GPT-4o (Part 3) Prompt for Verifying Claims with GPT-4o (Part 4) <example_claim_3> Jenny cooks breakfast for Mo because he feels mix of hunger and sadness. </example_claim_3> <example_verification_3> <reasoning> 1. Analysis of atomic parts: a. Jenny cooks breakfast for Mo: This part is grounded in event 2 of Chapter 15. b. Mo feels mix of hunger and sadness: This part is grounded in event 2 of Chapter 15. 2. Analysis of the relationship between atomic parts: a. The relationship is INVALID. There is no indication that Jenny cooked breakfast for Mo because he felt mix of hunger and sadness. The events are happening simultaneously but are not causally connected. b. The relationship is subjective interpretation and not explicitly grounded in the summary or outline. c. The relationship is grounded in the chapter outline. There is no contradicting information in the summary. Since 2a. and 2b. are not satisfied, the claim is INVALID. </reasoning> <result> INVALID </result> </example_verification_3> </example_1> <example_2> <example_summary> Sarah Lee discovers an ancient wooden box with strange symbols in her attic, which contains journal revealing the history of secret society and prophecy about the return of powerful being known as \"The Shadow.\" As unusual events begin to plague her town, Sarah, along with her friend Mark, uncovers clues that connect these occurrences to the prophecy. They find hidden chamber beneath the town containing ancient texts and artifacts, including weapon capable of banishing \"The Shadow.\" With this weapon, they confront member of the secret society who attempts to summon \"The Shadow,\" and after tense battle, Sarah successfully uses the weapon to banish the being, restoring peace to the town. Throughout the story, the connection between the ancient artifact, the journal, and the unfolding events reveals the central role of the wooden box and the secret society in the impending danger. Sarah and Mark's journey highlights their struggle to protect their town from supernatural forces while deciphering the mysterious symbols and prophecies tied to the powerful entity, \"The Shadow.\" </example_summary> <example_chapter_outline> ## Chapter 3 outline Figure 11: Prompt for verifying claims with GPT-4o (Part 4) Prompt for Verifying Claims with GPT-4o (Part 5) 1. Sarah discovers an ancient artifact in her attic, an intricately carved wooden box with strange symbols. 2. She finds an old journal in the box, detailing the history of secret society that once protected the town. 3. The journal reveals prophecy about the return of powerful being known as \"The Shadow.\" 4. Sarah decides to keep the discovery to herself, fearing that revealing it would cause panic . ## Chapter 8 outline 1. Sarah begins to notice strange occurrences around town, like unusual weather patterns and eerie shadows. 2. She consults the journal again and discovers passage that seems to describe unusual event patterns as signs of \"The Shadow's\" return. 3. Sarah's friend Mark, who is local historian, suggests that they investigate further by visiting the town's library. 4. At the library, they find more texts related to the secret society and \"The Shadow.\" 5. Mark went home to rest after long day, where he met his mother. <example_chapter_outline> <example_claim> Sarah decides to keep the discovery to herself, which reveals the challenges in Mark and Sarah 's friendship. </example_claim> <example_verification> <reasoning> 1. Analysis of atomic parts: a. Sarah decides to keep the discovery to herself: This part is grounded in event 4 of Chapter 3. b. \"reveals the challenges in Mark and Sarah's friendship\": This part is subjective interpretation and not explicitly grounded in the summary or outline. 2. Analysis of the relationship between atomic parts: a. The relationship is not meaningful, as there is no direct connection between Sarah keeping the discovery to herself and revealing challenges in Mark and Sarah's friendship. b. The relationship is subjective interpretation and not explicitly grounded in the summary or outline. c. The relationship does not contradict any information in the summary or outline. Since 1b., 2a., and 2b. are not satisfied, the claim is INVALID. </reasoning> <result> INVALID </result> <example_claim> Sarah finds an old journal in the room, which prompted Mark to go home and meet his mother. </example_claim> <example_verification> <reasoning> 1. Analysis of atomic parts: a. Sarah finds an old journal in the room: This part is grounded in event 2 of Chapter 3. b. Mark went home to rest after long day, where he met his mother: This part is grounded in event 5 of Chapter 8. Figure 12: Prompt for verifying claims with GPT-4o (Part 5) Prompt for Verifying Claims with GPT-4o (Part 6) 2. Analysis of the relationship between atomic parts: a. The relationship is not meaningful, as there is no direct connection between Sarah finding the journal and Mark going home to meet his mother. b. The relationship is subjective interpretation and not explicitly grounded in the summary or outline. c. The relationship does not contradict any information in the summary or outline. Since 2a. and 2b. are not satisfied, the claim is INVALID. </reasoning> <result> INVALID </result> </example_verification> </example_2> Now, it's your turn to verify the claim based on the provided book summary, chapter outline, and claim. Present your response in the following format: <verification> <reasoning> [Provide your detailed reasoning here, explaining why the claim is or is not meaningful and coherent with the book summary and chapter outline.] </reasoning> <result> [State whether the claim is VALID or INVALID. Use VALID if the claim portrays meaningful relationship and is coherent with the book summary and chapter outline. Use INVALID if it does not.] </result> </verification> Remember to base your verification solely on the information provided in the book summary, chapter outline, and the claim itself. Verify that the relationship makes sense and is objectively valid. Do not introduce external information or make assumptions beyond what is given. Figure 13: Prompt for verifying claims with GPT-4o (Part 6) Prompt for Generating Book-level Claims in NAÏVE Your task is to create factual statements that incorporate multiple events from multiple chapters of book. These factual statements must be objective and specific, grounded in the involved chapters, and consistent with the entire book. The included events must be specific. The facts cannot contain any interpretations, speculations, or subjective statements. In addition to each fact, provide minimally corrupted version of the fact, which sounds plausible but is wrong based on the entire book. To create valid facts, follow these guidelines step-by-step: 1. Carefully read through the book. 2. In <brainstorm> section: - Identify different events that are strongly related to one another in multiple chapters. - Consider the relationship between these events, but DO NOT include this brainstorming in the resulting fact. If meaningful relationship is found, move on to the next step; otherwise, proceed to the next fact. 3. Formulate your facts based on this analysis. Ensure that your facts: - Contain single sentence - Are coherent with the entire book - Include multiple detailed and strongly related events from multiple chapters in the book - Are self-contained and independent of other facts - No part of the fact should be subjective interpretations or speculations - Do not contradict or duplicate existing facts - Do not contain chapter information (e.g. \"In Chapter x, ...\") - Do not contain quotes from the book. 4. Formulate minimally corrupted version of each fact. - Only one aspect of the fact, such as an atomic detail or the relationship between details, should be altered. - The corrupted fact should sound plausible but be clearly wrong based on the entire book. - The corrupted fact should be similar with the original fact in terms of length and sentence structure to make it harder to verify solely based on surface-level features. First, read the book: <book> {book} </book> Finally, review the existing facts: <existing_facts> {existing_facts} </existing_facts> Now, generate as many valid facts as possible based on the provided book. Return \"No meaningful fact\" if there is no valid fact. Present your facts in the following format: <facts> <fact_1> <brainstorm> [Your brainstorm notes here] </brainstorm> Fact: [Your fact here] Fact Reasoning: [Your explanation here (including the chapter associated with each event)] Source: [Chapters involved] Corrupted Fact: [Your corrupted fact here] Corrupted Fact Reasoning: [Your explanation here] </fact_1> [Continue with additional facts...] </facts> Remember to create facts that are objectively valid, coherent with the book, and demonstrate strong, meaningful relationships between the events from multiple chapters. No part of the fact can include subjective interpretations or generalizations. The relationship must be meaningful. The included events must be SPECIFIC and DETAILED!!!!! Figure 14: Prompt for generating book-level claims in NAÏVE. Prompt for Generating Chapter-level Claims in NAÏVE Your task is to create factual statements that incorporate multiple events from Chapter { chapter} of book. These factual statements must be objective and specific, grounded in the specific chapter, and consistent with the entire book. The included events must be specific. The facts cannot contain any interpretations, speculations, or subjective statements. In addition to each fact, provide minimally corrupted version of the fact, which sounds plausible but is wrong based on the entire book. To create valid facts, follow these guidelines step-by-step: 1. Carefully read through the book. 2. In <brainstorm> section: - Identify different events that are strongly related to one another in single chapter. - Consider the relationship between these events, but DO NOT include this brainstorming in the resulting fact. If meaningful relationship is found, move on to the next step; otherwise, proceed to the next fact. 3. Formulate your facts based on this analysis. Ensure that your facts: - Contain single sentence - Are coherent with the entire book - Include multiple detailed and strongly related events from single chapter in the book - Are self-contained and independent of other facts - No part of the fact should be subjective interpretations or speculations - Do not contradict or duplicate existing facts - Do not contain chapter information (e.g. \"In Chapter x, ...\") - Do not contain quotes from the book. 4. Formulate minimally corrupted version of each fact. - Only one aspect of the fact, such as an atomic detail or the relationship between details, should be altered. - The corrupted fact should sound plausible but be clearly wrong based on the entire book. - The corrupted fact should be similar with the original fact in terms of length and sentence structure to make it harder to verify solely based on surface-level features. First, read the book: <book> {book} </book> Finally, review the existing facts: <existing_facts> {existing_facts} </existing_facts> Now, generate as many valid facts as possible based on the provided chapter. Return \"No meaningful fact\" if there is no valid fact. Present your facts in the following format: <facts> <fact_1> <brainstorm> [Your brainstorm notes here] </brainstorm> Fact: [Your fact here] Fact Reasoning: [Your explanation here (including the chapter associated with each event)] Source: [Chapter involved] Corrupted Fact: [Your corrupted fact here] Corrupted Fact Reasoning: [Your explanation here] </fact_1> [Continue with additional facts...] </facts> Remember to create facts that are objectively valid, coherent with the entire book, and demonstrate strong, meaningful relationships between the events from Chapter {chapter}. No part of the fact can include subjective interpretations or generalizations. The relationship must be meaningful. The included events must be SPECIFIC and DETAILED!!!!! Figure 15: Prompt for generating chapter-level claims in NAÏVE. ProLong-base outperforms ProLong-instruct by 61.6% and 59.7%, respectively. Figure 16: How we structure our finetuning prompts, which include system message, user message, and assistant message. Placeholders (colored in light cayenne) will be replaced with actual text from the dataset. The contents between <context>, <explanation>, and <answer> tags are generated (Section 2.3). B.2 Hyperparameter Tuning Table 12 summarizes the performance of each configuration from our hyperparameter tuning experiment on 100 samples from CLIPPERs dev set."
        },
        {
            "title": "C Evaluation",
            "content": "C.1 Configuration for Bench QA"
        },
        {
            "title": "Evaluation",
            "content": "In HELMET (Yen et al., 2024), for the Bench QA task, the default configuration sets the output maximum length to 10 tokens and uses ROUGE F1 (Lin, 2004) as the evaluation metric. Upon closer examination of the outputs from both models, we identify critical flaws in the default setup. These findings eventually motivate us to remove the maximum length restriction and adopt the LLMas-a-judge evaluation approach using GPT-4o as Learning Rate Batch Size Dev Set Accuracy 1e-5 1e-6 1e-7 1e-5 1e-6 1e-7 16 16 16 32 32 32 26% 74% 71% 34% 73% 69% Table 12: Hyperparameter tuning results. Each model is fine-tuned for 1 epoch and tested on subset of 100 samples from our dev set. the judge. Below, we provide more details on our analysis. Setting max output length to 10 tokens frequently cause truncations: In Table 13, we show the taxonomy we derive from our analysis. Here, we define truncation to be when models response is heavily cut off, making it impossible to determine the correctness of the output. Out of all 100 evaluated examples, Qwen2.5-7BInstructs outputs get truncated 24 times, while Qwen-CLIPPERs outputs get truncated 45 times. After removing the 10-token maximum length restriction22, we observe that 9 of the 24 previously truncated outputs from Qwen-Instructshould be counted as correct. For Qwen-CLIPPER, this correction is even more significant, with 25 of the 45 truncated outputs being technically correct. We combine these numbers with numbers from the four rows in Table 14 that indicate correctness, and find that Qwen2.5-7B-Instruct has an overall accuracy of 44%, while Qwen-CLIPPER has 54%. ROUGE F1 is not reliable metric: If we use ROUGE F1 as the metric, Qwen2.5-7BInstruct achieves score of 27.4, while QwenCLIPPERachieves score of 18.0. This result sharply contrasts with the accuracies we obtain in the preceding paragraph, and does not reflect the actual performance of the models. Lots of prior work have shown that ROUGE correlates poorly with human judgment (Goyal et al., 2023; Chang et al., 2024). Our manual analysis reveals that this metric is overly sensitive to length, and does not capture the correctness of the model outputs. We show several examples in Table 15. 22Without the maximum length limit, Qwen2.5-7BInstructs outputs are on average 20.9 tokens long, and QwenCLIPPERs outputs have 25.8 tokens on average. CATEGORY DEFINITION EXAMPLE QUESTION EXAMPLE GROUND-TRUTH EXAMPLE MODEL ANSWER Cases that should be counted as incorrect Wrong The models answer is evidently wrong. What is the home Edna moves into in New Orleans called? The pigeon house The text does not provide information about Edna moving Full match Correct (Phrasing) Correct (Long) Correct (Short) Cases that should be counted as correct The models answer perfectly matches the ground-truth answer. Which among Annalisa, Seb, Peyton, and Gannonmarie is not Mrs. Bronwyns child? Peyton Peyton The models answer is correct, but it has different phrasing than the ground-truth answer. The models answer is correct, but it is longer than the groundtruth. The models answer is correct, but it is shorter than the groundtruth. How old is Felicity at the start of his narration? Thirty Felicity is 30 years old at What kind of pet does Madame Bowen keep? cat Madame Bowen keeps cat as pet. Why is Tasha Teigan out of jail? He has been released on parole. Paroled. Cases where correctness is ambiguous Truncation The models answer has been heavily truncated, making it impossible to tell the correctness of the answer. What is to be built in place of the Lars home on Wickham Place? Flats The house on Wickham Place is to be replaced Table 13: Taxonomy from our analysis on the Bench QA outputs of Qwen2.5-7B-Instruct and Qwen-CLIPPER. Example model outputs are from Qwen-CLIPPER except the one for Correct (Short)\", which is from Qwen2.5-7BInstruct (all generated under the default setup where maximum output tokens is set to 10). CATEGORY QWEN-INST QWEN-BC Cases that should be counted as incorrect Wrong 41 26 Cases that should be counted as correct Full match Correct (Phrasing) Correct (Long) Correct (Short) 17 3 13 4 7 18 0 Cases where correctness is ambiguous Truncation 24 45 Table 14: Raw counts of taxonomy categories for QwenInstruct and Qwen-CLIPPER, with outputs generated using the default maximum length of 10 tokens."
        },
        {
            "title": "D Results",
            "content": "D."
        },
        {
            "title": "Impact of chapter distance and book\nlength on test set performance",
            "content": "Figure 17 shows that test set accuracy peaks when the distance between chapters in claim is around 4060K tokens (roughly the midpoint of book). When that gap shrinks below or stretches beyond 60K tokens, performance dips by about 10%, leaving no definitive pattern beyond this sweet spot. Figure 17: Accuracy of CLIPPER-Prolong-balanced on the test set (book-level claims), grouped by the distance (in tokens) between source events in each claim. We also find that overall book length does not strongly influence accuracy, except in cases where the text exceeds 110K tokens. In these longer works, accuracy is about 5% higher than in shorter books, as shown in Figure 18. While this slight edge may hint at advantages in more expansive narratives, the models broader performance remains steady across most book lengths. We finally examine the possible effect of event placement on ProLong-CLIPPERs performance on the test set. Interestingly, there is no strong \"lostin-the-middle\" effect regarding event placement in the book (Liu et al., 2024). As shown in Figure 19, accuracy is usually the highest when the claim involves events that appear at the beginning (0-0.4, QUESTION GROUND-TRUTH MODEL ANSWER ROUGE F1 EXPLANATION How old is Felicity at the start of his narration? What gender does Harris predict Cal will be? Thirty MALE Felicity is 30 years old at Harris predicts that Cal will be boy. 0 0 When is Jarods birthday? NOVEMBER Jarods birthday is on November 16 0.22 In which state is Gopher Prairie located? Minnesota Gopher Prairie is located in Minnesota. This is 0. The model is correct, but it uses the numerical form of the number. The model is correct, but it phrases it differently, resulting in no word overlap. The model is completely wrong, but it gets the same score as the model answer in the row below, which contains correct answer. The model is correct, but it gets the same score as the wrong model answer above, just because the output is much longer than the ground-truth. Table 15: Examples showing that ROUGE-F1 is an unreliable metric. Figure 18: ProLong-CLIPPERs performance on test set, grouped by the number of tokens in each book. around 82%) rather than at the end of the book (0.8-1, around 78%). true claims and 97 false claims. This pattern is consistent with findings from NoCha (Karpinska et al., 2024), which highlight that models tend to have greater difficulty verifying false claims. Notably, in 95 cases, the model successfully validates the true claim but fails to validate the corresponding false claim. This raises an important question: What specific perturbations make false claim appear true to the model? Through careful manual analysis, we derive taxonomy of such perturbations and present them in Table 4. The most frequent perturbations are changes to events (43.2%) and people (31.6%), such as altering actions or misattributing roles. Less frequent but notable are modifications to objects (15.8%), locations (13.7%), time (6.3%), and affect (4.2%). All these perturbations introduce plausible-sounding variations that the model may struggle to detect without fully understanding the narrative. Figure 19: Accuracy of ProLong-CLIPPER on the test set (chapter-level claims), grouped by the event placement in the book (0-0.2 includes events are at the beginning, while 0.8-1 includes events towards the end). D.2 False claim error analysis In Table 16, we provide detailed definitions for each category from the false claim error analysis in 4.6. To explore instances where fine-tuned models still struggle, we conduct an in-depth analysis of Qwen-CLIPPER outputs. Of the 1,000 book-level claims in the test set, the model fails to verify D.3 Full results on LM Harness and"
        },
        {
            "title": "HELMET",
            "content": "Table 17 shows the results of all models on popular short-form benchmarks. Overall, our fine-tuned models, especially Qwen-CLIPPER, do not degrade that significantly from the baseline models even though it has been fine-tuned on longer data. Table 18 shows the results of HELMET on recall, RAG, passage re-ranking, and retrieval tasks. Overall, fine-tuned models achieve synthetic recall and RAG scores comparable to the baseline models, while generally delivering improved re-ranking and more robust ICL performance."
        },
        {
            "title": "DEFINITION",
            "content": "Refers to the alteration or misrepresentation of the actions, occurrences, or processes described in claim. Involves substituting or misattributing individuals involved in claim. Concerns the manipulation or substitution of physical items or artifacts mentioned in claim."
        },
        {
            "title": "Location",
            "content": "Relates to changing or misrepresenting the places where events occur."
        },
        {
            "title": "Affect",
            "content": "Pertains to the sequencing or timing of events being distorted or swapped. Deals with altering the emotional state, attitude, or disposition described in claim. Table 16: Definitions for each category of perturbations that cause false claim to be misclassified as true in the error analysis in 4.6. Models IFEval BBH Math lvl5 GPQA MMLU-Pro Arc-Challenge GSM8K HellaSwag WinoGrande Llama-Instruct Qwen-Instruct Prolong-instruct-noft Qwen-CLIPPER ProLong-CLIPPER Llama-CLIPPER ProLong-WritingPrompts ProLong-CLIPPER-book ProLong-CLIPPER-chapter 59.35 54.00 58.87 50.65 7.91 45.43 11.75 6.39 4.59 50.93 54.60 49.86 55.50 48.03 50.02 47.32 49.31 49.64 12.81 24.80 5. 22.51 5.42 12.77 3.59 5.53 5.63 31.96 33.40 29.35 33.82 27.88 30.59 30.73 29.47 29.77 37.77 43.80 32.43 44.49 32.35 37.55 26.29 32.38 32.35 51.54 53.20 58. 53.84 50.68 53.50 50.51 54.78 54.27 75.06 77.70 68.06 78.32 60.80 74.91 39.04 62.02 61.22 59.05 61.80 80.75 61.71 60.44 78.65 76.36 79.27 79.14 74.19 69.20 74. 69.06 73.24 73.40 70.64 72.93 74.27 Table 17: Performance on popular short-form benchmarks (evaluated using Language Model Evaluation Harness). D.4 Performance of ProLong-Base on claim verification and narrative understanding benchmarks Table 19 shows accuracy of ProLong-Baseon longcontext reasoning and narrative understanding benchmarks. Even though ProLong-Bases test set performance is much worse than ProLong-Instruct, performance on other narrative understanding tasks is comparable between the two models. Synthetic Recall (Ruler) RAG Re-ranking ICL Model niah_mk_2 recall niah_mk_3 recall niah_mv recall json_kv Llama-Instruct Qwen-Instruct ProLong-Instruct ProLong-CLIPPER Qwen-CLIPPER Llama-CLIPPER ProLong-WritingPrompts 98 100.0 98.0 99 81 98 99 88 98.0 98.0 92 45 99 87 78.75 83.3 46.8 27.75 45 26 31. 96 98.8 99.3 99 45 100 100 48.17 20.3 54.3 52.5 38.16 48.5 50.5 80.67 47.3 91.3 92 66.5 85.17 89. 56 24.0 57.7 51.33 36 55.67 51.33 nqh 13.66 0 25.0 27.50 3.17 22.90 18.30 triviaqa hotpotqa msmarco trec_coarse trec_fine 73 78.0 86.0 92 73 87 72 20.0 59.0 72 52 81 65 91 6.0 92.0 90 87 90 91 91 11.0 94.0 94 78 95 88 7.0 89.0 90 86 88 Table 18: Performance on HELMET for recall, RAG, passage re-ranking, and retrieval tasks. Fine-tuned models achieve synthetic recall and RAG scores comparable to the baseline models, while generally delivering improved re-ranking and more robust ICL performance. CLIPPER-Test NarrativeQA MuSR Bench QA 23.9% 46.0% 39.8% 42.5% Table 19: Performance of ProLong-Base on longcontext reasoning and narrative understanding benchmarks. Even though ProLong-Bases test set performance is much worse than ProLong-Instruct, performance on other narrative understanding tasks is comparable between the two models. ProLong-CP-book ProLong-CP-chap Test-book Test-chapter Overall 74.8% 75.2% 75.0% 78.2% 80.2% 79.2% Table 20: Test set performance of models trained exclusively on either book-level claims or chapter-level claims, with accuracy measured for book-level, chapterlevel, and overall claims. CP stands for CLIPPER. Models Groundedness Qwen-Instruct Llama-Instruct ProLong-Instruct Qwen-CLIPPER Llama-CLIPPER ProLong-CLIPPER 11.9% 16.8% 19.6% 67.1% 75.9% 80.6% Table 21: Percentage of grounded chain of thoughts being generated by baseline and finetuned models. Our fine-tuned models generate much more grounded chain of thoughts."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park",
        "University of Massachusetts Amherst"
    ]
}