{
    "paper_title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference",
    "authors": [
        "Sihan Chen",
        "Dan Zhao",
        "Jongwoo Ko",
        "Colby Banbury",
        "Huiping Zhuang",
        "Luming Liang",
        "Tianyi Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to $2.94\\%$ in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 2 4 9 1 . 5 0 5 2 : r WINA: WEIGHT INFORMED NEURON ACTIVATION FOR ACCELERATING LARGE LANGUAGE MODEL INFERENCE Sihan Chen2 Dan Zhao3 Jongwoo Ko1 Colby Banbury1 Huiping Zhuang4 Luming Liang1 Tianyi Chen1 1Microsoft 2Renmin University of China 3New York University 4South China University of Technology Equal contributions. Work is done at Microsoft. May 27,"
        },
        {
            "title": "ABSTRACT",
            "content": "The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), novel, simple, and trainingfree sparse activation framework that jointly considers hidden state magnitudes and the column-wise ℓ2-norms of weight matrices. We show that this leads to sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to 2.94% in average performance at the same sparsity levels, across diverse set of LLM architectures and datasets. These results position WINA as new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina."
        },
        {
            "title": "Introduction",
            "content": "While large language models (LLMs) have revolutionized the field of natural language processing, offering unprecedented capabilities in variety of applications, such as text generation (Li et al., 2024; Cheng et al., 2025), translation (Hendy et al., 2023; sea, 2025), understanding (Chang et al., 2024; Tschannen et al., 2025), and grounding (Zhao et al., 2025; Hui et al., 2025) their growing size and complexity make controlling their computation costs challenging. They often require substantial computational resources, particularly during inference, making reducing inference costs without degrading output quality central challenge. One strategy has been to activate only sub-network of the full model (Jacobs et al., 1991) during inference using Mixture of Experts (MoE) architecture, which has already seen adoption in popular and widely-used LLMs like GPT4 (Achiam et al., 2023) and Mistral (Jiang et al., 2023). Other methods include model distillation, where smaller model is trained using knowledge distilled from larger teacher model to route inference requests more efficiently. However, these approaches can require considerable amount of training, which can also be computationally costly. An alternative is training-free sparse activation, which retains the original dense model but selectively omits weights or neurons at inference time. These training-free methods avoid training or retraining and can be applied to off-the-shelf models. They leverage criteria such as hidden-state magnitudes, weight importance, weight statistics, or additional validation data to determine which parts of the model to deactivate, thereby accelerating inference. Primary author, chensihan@ruc.edu.cn. Corresponding author, Tianyi.Chen@microsoft.com. WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference However, current training-free methods exhibit critical limitations. Most notably, they ignore the influence of weight matrices on error propagation. Specifically, these approaches fail to account for how interactions between input elements and the weight matrix during forward propagation affect model outputs, leading to accumulated approximation errors in sparse activation. Contributions. In this paper, we propose WINA: simple, easy-to-use, training-free framework that performs sparse activation based on the magnitude of hidden states and the column-wise ℓ2-norm of the weight matrix. By combining activation strength with weight importance, our thresholds directly reflect how much each activation can influence the next layer. This design provides theoretical guarantees that the total approximation error remains bounded and is lower than that of other comparable approaches. In contrast, methods like TEAL rely exclusively on the distribution of hidden-state magnitudes to decide which activations to keep and which to deactivate. Ignoring weight magnitudes in this way may discard highly influential activations or retain many low-impact ones, leading to suboptimal trade-offs between efficiency and output quality. Our framework overcomes these limitations by integrating weight statistics into the selection process, achieving finer control over sparsity and tighter bounds on the resulting approximation error. We evaluate WINA on multiple widely-used LLMs (ranging from 7B to 14B parameters) across several popular benchmark datasets. Compared with state-of-the-art training-free methods such as TEAL (Liu et al., 2024) and CATS (Lee et al., 2024), achieves superior model performance at identical sparsity levels, with significantly less performance degradation. We also establish theoretical error bounds for our methodology, providing formal support for the experimental results and validating our methods effectiveness. In summary, our detailed contributions include as follows. Tight Approx Error Layer Generality Hetero Sparsity WINA TEAL CATS Weighted-informed Activation: we introduce novel sparse activation method that jointly considers hidden state magnitudes and the column-wise ℓ2-norms of weight matrices. This allows for selecting neurons that are not only strongly activated but also those that have larger influence on downstream layers, leading to more informed construction of sub-network during inference. Theoretically Tighter Approximation Error: we conduct formal analysis to demonstrate that our weightinformed activation mechanism yields lower expected output error compared to prior methods (e.g., TEAL) under mild assumptions, including column-wise orthogonality of weights and monotonic activation functions, with guarantees extendable to multi-layer architectures. Numerical Experiments: we perform extensive evaluations on multiple LLMs, including Qwen-2.5 (Bai et al., 2023), LLaMA series (Touvron et al., 2023), and Phi-4 (Abdin et al., 2024), demonstrate that our method achieves superior accuracy under various sparsity levels. In particular, WINA maintains better performance as sparsity increases, highlighting its robustness and practical utility across diverse tasks and model scales. The rest of our paper is organized as follows. We begin by reviewing related works in Section 2. We detail our methodology in Section 3 and review our experimental results in Section 4. We conclude with discussion on future directions in Section 5."
        },
        {
            "title": "2 Related Work",
            "content": "Sparse Activation. Modern sparse activation activation approaches fall into two principal paradigms: training-based methods and training-free methods. Training-based methods typically employ trainable router to learn to dynamically select activated experts for each token, with the Mixture-of-Experts (MoE) architecture (Jacobs et al., 1991) serving as the foundational framework. In this framework, each expert operates an individual component of the model, as only the relevant experts are activated for each input during inference, achieving significant computational savings. This paradigm has been expanded through many iterations and variants. The sparsely-gated mixture of experts layer (Shazeer et al., 2017) integrates MoE into recurring neural networks (RNNs). Works like GShard (Lepikhin et al., 2020) and the Switch Transformer (Fedus et al., 2022) extend MoEs to the Transformer architecture (Raffel et al., 2020) while others combine several approaches, such as WideNet (Xue et al., 2022), reduces the size of the MoE model by initially compressing the model before transitioning into MoE. Works like MoEBert (Zuo et al., 2022) decomposes the FFN layer of pre-trained dense model into multiple experts based on importance-guided adaptation and then refines the model through distillation. LLM in Flash (Alizadeh et al., 2023) employs low-rank predictor to determine which intermediate neurons are activated. 2 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Figure 1: Overview of WINA. WINA performs training-free sparse activation by selecting the most influential input dimensions based on both hidden state magnitudes and the column-wise ℓ2-norms of weight matrices. This joint criterion ensures accurate sub-network activation at each layer during inference, preserving model performance while reducing computational overhead. Training-free methods, in contrast, do not rely on learnable router, instead using predefined or calculated criteria to perform sparse activation. Methods (Han et al., 2015b) can utilize magnitude-based weight pruning or global activation pruning (Wen et al., 2016b) to apply fixed sparsity pattern regardless of input. For instance, Q-Sparse (Wang et al., 2024) produces sparsity as function of input magnitudes, achieving sparsity rates of 60% with reasonable performance degradation. CATS (Lee et al., 2024) applies sparse activation on SwiGLU outputs within gated MLP layers, achieving performance comparable to the original dense model while achieving 25% model sparsity. In contrast, TEAL (Liu et al., 2024) extends magnitude-based activation sparsity to all network layers, achieving 40-50% model-wide sparsity across architectures with minimal performance impact. However, current sparse activation methods suffer from noticeable limitations. They determine activation elements solely based on the magnitude of hidden states, neglecting the crucial influence of the weight matrix, which results in suboptimal error control. Relations to Model Structured Pruning. Although WINA shares the shared goal of reducing inference cost with structured pruning methods (via similar paradigm by searching sub-network), its philosophy and mechanism differ substantially. Traditional model pruning removes redundant parameters from deep neural networks (Han et al., 2015a; Frankle and Carbin, 2018; Frantar and Alistarh, 2023), often requiring fine-tuning to restore performance (Lin et al., 2019; He et al., 2018; Wen et al., 2016a; Li et al., 2020; Zhuang et al., 2020; Chen et al., 2017, 2021a, 2020). To enhance the quality of the pruned sub-networks, recent advances introduce knowledge-transfer mechanisms during pruning (Chen et al., 2021b, 2023c,b,a, 2024; Qu et al., 2025) or apply post-hoc distillation (Ko et al., 2024, 2025) to improve accuracy. However, these approaches typically involve additional training stages, making them less suitable for scaling to large foundation models. In contrast, WINA is training-free, plug-and-play sparse activation framework that dynamically selects high-performing sub-networks at inference time without modifying or retraining the model. This makes WINA particularly well-suited for deployment in resource-constrained or latency-sensitive environments."
        },
        {
            "title": "3 Methodology",
            "content": "We now present WINA, framework for sparse activation that preserves critical elements while zeroing out nonessential components in each layers input. As illustrated in Figure 1, WINA jointly considers both the input tensor and the associated weight matrix, rather than relying solely on input magnitudes. During inference, it activates only the most influential neurons, effectively constructing sparse sub-network that maintains the expressive power of the original model. 3 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference 3.1 Problem Statement Main Problem. Consider deep neural network (DNN) consisting of layers. We denote the weight matrix of the l-th layer as Wl Rmlnl and the corresponding input as an arbitrary tensor Rnlsl for {1, ..., L}, representing the full information content. Our goal is to identify set of binary activation gates = {g1, , gL}, where each nl , such that the deviation between the models original output and the gated output is minimized: gl {0, 1} minimize g1,,gL M(X) M(X G)2 . (1) Since obtaining the complete set of possible inputs is generally infeasible, we instead use sampled subset to approximate it. The activation gating operates in the input vector space to reduce output deviation. With this observation, we can reformulate the original problem into per-layer version to make the problem more tractable. Refined Problem. Given weight matrix Rmn and sampled input vector Rn, the standard linear transn such that the formation is x. Our objective then becomes identifying an activation gate or mask {0, 1} masked output ym (m x) approximates the original by solving: minimize m{0,1}n (m x)2 . (2) 3.2 Weight Informed Gate Function Motivation. Many current sparse activation methods (e.g., Q-sparse (Wang et al., 2024), CATS (Lee et al., 2024), TEAL (Liu et al., 2024)) operate via top-K gating mechanism governed by the absolute values of the hidden states: 1 mi = { 0 if xi is among the top-K values in x, otherwise (3) However, this approach ignores the critical role that weight matrices play. Specifically, how each element of the preceding input interacts with the weight matrix in the forward propagation. This mismatch motivates us to propose WINA, method that jointly considers both inputs and weight matrices to minimize the approximation error for better performance. Formalization. specific criteria: In WINA, we construct binary activation gates by selecting the top-K components according to 1 mi = { 0 if xici is among the top-K values in c, otherwise, (4) where Rn represents the column-wise ℓ2 norm of and denotes the Hadamard or element-wise product. The choice of can be adapted to different use cases, ranging from (1) coarse-grained universal criterion where shared is applied across all layers to (2) fine-grained layer-specific strategy that assigns individually to better minimize approximation error. 3.3 Theoretical Analysis WINA also offers theoretical advantages, capable of achieving more optimal bound on the approximation error than TEAL. To demonstrate, we first present Lemma for single-layer network. Lemma 3.1 (Optimal approximation error over single layer). Let Rn be an input vector and Rmn be matrix satisfying column-wise orthogonality: = In where In is an identity matrix. For any target sparsity level N+ satisfying < n, the expected deviation between the original network output and the gated output via WINA is less or equal to that of TEALs. Formally: [W xWINA 2 2] [W xTEAL 2 2] , where xWINA is the sparse input via WINA, retaining the elements activated with the largest xj W,j2, and xTEAL is the sparse input via TEAL, retaining the elements with the largest xj. Proof. See Appendix. Using our single-layer Lemma 3.1, we can extend it to linear layers. As stated in Theorem 3.2 below, we see that WINA still achieves smaller approximation error than TEAL in the layer case. 4 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Theorem 3.2 (Optimal approximation error over consecutive layer). Let Rd0 be an input vector and {W (ℓ) denote the weight matrices of an -layer neural network, where each (ℓ) {1, . . . , } with = such that every matrix (ℓ) with ℓ is column-wise orthogonal, i.e., (W (ℓ) Idℓ1 . For any target sparsity level N+ with < minℓ{1,...,N } dℓ, the expected deviation satisfies: } ℓ=1 Rdℓdℓ1 . Suppose there exists subset = (ℓ) ) where yWINA denotes the output produced by WINA; yTEAL is the output of TEAL; and is the original dense network output without any sparsification. [yWINA 2] [yTEAL 2 2] , (5) Proof. See Appendix. Using these, we now consider realistic deep neural networks equipped with various activation functions. Our results remain valid for large class of activation functions provided that they satisfy the monotonicity property (e.g., ReLU and several of its variants, sigmoidal and softmax, etc). Like before, we start with the simple single-layer case before extending to the multi-layer case. For completeness, we explicitly state the definition below. Definition 3.3 (Monotonic increasing function (MIF)). function is monotonically increasing if for any x1 x2, then (x1) (x2). Lemma 3.4 (Optimal approximation error over single layer with MIF). Let Rn be an input vector, Rmn be matrix that satisfies column-wise orthogonality, and be an activation function that is MIF. For any target sparsity level N+ satisfying < n, the expected deviation between the original output and the gated output via WINA gating mechanism is less than or equal to that of TEAL gating mechanism. Formally: [f (W xWINA) (W x) 2 2] [f (W xTEAL) (W x) 2 2] , where xWINA is the sparse input via WINA, retaining the elements with the largest xj W,j2, and xTEAL is the sparse input via TEAL, retaining the elements with the largest xj. Proof. See Appendix. ℓ=1 denote the weight matrices of an -layer neural network, where each (ℓ) } Finally, we extend this theorem to the case of multi-layer network with MIF activations. Theorem 3.5 (Optimal approximation error over consecutive layer with MIF). Let Rd0 be an input vector and Rdℓdℓ1. Suppose there {W (ℓ) exists subset {1, . . . , } with = such that every matrix (ℓ) with ℓ is column-wise orthogonal, i.e., (W (ℓ) = Idℓ1. Let be an activation function satisfying the monotonic increasing property. For any target sparsity level N+ with < minℓ{1,...,N } dℓ, the expected deviation satisfies: (ℓ) ) where yWINA denotes the output produced by WINA; yTEAL is the output of TEAL; and is the original dense network output without any sparsification. [yWINA 2] [yTEAL 2 2] , (6) Proof. See Appendix. Remark. Many commonly used activation functions are monotonically increasing, such as ReLU, LeakyReLU, etc., or nearly monotonically increasing, such as SiLU. This fact largely ensures the generality of WINA across wide range of deep neural network architectures. 3.4 From Theory to Practice Motivation. In Section 3.3, our theoretical analysis relies on the assumption of column-wise orthogonality of the relevant weight matrices, i.e., = when, in reality, LLMs can violate the column-wise orthogonality condition. To bridge this gap between theory and practice, while preserving the theoretical error bounds, we propose tensor transformation framework that enforces column-orthogonality in the relevant weight matrices of the model. Transformation Protocol. Given weight matrix , we can enforce column-wise orthogonality by multiplying from the right by an orthogonal matrix such that the product has orthogonal columns. Specifically, we perform Singular Value Decomposition (SVD) on : = ΣV 5 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference where and are orthogonal matrices, and Σ is diagonal matrix containing the singular values of . To achieve column-orthogonality, we set = and transform as follows: ˆW = This transformation guarantees that the resulting matrix satisfies the column-orthogonality: ˆW ) ˆW = ΣU Σ = Σ2 ( (7) To ensure that the models final output remains unchanged after this transformation, we compensate for its effects using computational invariance (Ashkboos et al., 2024); more specifically, we enforce column-wise orthogonality constraints on the key projection matrices Wk in the self-attention layer and the gate projection matrices Wgate in the MLP layer via SVD-based transformation. We then propagate these transformations through adjacent layers and adjust the residual connections accordingly to maintain computational invariance. During inference, we employ the proposed activation criterion on these transformed column-orthogonal matrices, while using the conventional inputbased activation criterion for the remaining matrices as typically done in sparse modeling."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Models. To demonstrate the effectiveness of WINA and ensure coverage across different model families and sizes, we provide our results on four models: Qwen-2.5-7B (Dong et al., 2024), Llama-2-7B (Touvron et al., 2023), Llama-38B (Dubey et al., 2024), and Phi-4-14B (Abdin et al., 2024). Data. We use the Alpaca dataset (Taori et al., 2023) to construct hidden states distribution and compute thresholds for each layer. The Alpaca dataset is an instruction-following dataset for fine-tuning language models, released by research team from Stanford University with the aim of building and sharing an LLaMA model that follows instructions. The dataset contains 52,000 instructions and demonstrations generated by OpenAIs text-davinci-003 engine. Evaluation. We use lm-evaluation-harness pipeline (Gao et al., 2023) for our evaluations on an extensive suite of downstream tasks, including PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2019), HellaSwag (Zellers et al., 2019), Arc Challenge (Clark et al., 2018), MMLU (Hendrycks et al., 2020), and GSM8K (Cobbe et al., 2021). Baselines. In practice, the gating strategy can be either top-k-based or threshold-based (e.g., TEAL (Liu et al., 2024) and CATS (Lee et al., 2024)). Threshold-based approaches typically determine gating thresholds by statistically analyzing hidden state distributions from general-purpose dataset. However, directly applying these thresholds during evaluation may cause mismatch between the actual and target sparsity levels, due to potential distributional shifts between the training and evaluation datasets. To avoid this issue and ensure fair comparison across methods, we adopt the top-k based gating strategy in our experiments. To eliminate the potential effect introduced by the transformation process, we introduce an additional baseline, TEALTransform. In this variant, the TEAL approach is applied to the transformed model, retaining the elements with the largest absolute values x. This controlled baseline enables fair comparison of different sparse activation strategies. To further improve performance, we assign layer-specific sparsity ratios instead of uniform sparsity across the model. Given global sparsity target, we leverage the greedy algorithm proposed in TEAL to iteratively configure per-layer sparsity levels such that the aggregate sparsity meets the global budget. This adaptive allocation enables prioritization of computational resources for more critical parameter groups, improving overall performance. 4.2 Controlled Sparsity Experiments. Here, we provide an empirical comparison of WINA against TEAL-based baselines (e.g., TEAL and TEAL-transform) across different sparsity levels (25% to 65%) to demonstrate the effectiveness of our proposed algorithm under various experimental settings. Qwen-2.5-7B. We evaluate WINA on Qwen2.5-7B (Yang et al., 2024) across various sparsity levels (i.e, 25% 65%) under the controlled sparsity setting. As shown in Table 1, WINA consistently matches or outperforms both TEAL and TEAL -transform across all sparsity levels. Notably, as sparsity increases, the performance gap between WINA and the baselines becomes more pronounced. For instance, at 65% sparsity, WINA outperforms TEAL by 2.94% and TEAL-transform by 1.41% on average. This trend indicates that WINA is more robust under high sparsity, likely due to its ability to retain the most influential activations by jointly considering hidden state magnitudes and weight norms. Particularly on harder tasks such as GSM8K and HellaSwag, WINA maintains relatively strong performance even when aggressive sparsification is applied. 6 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference (a) QWen-2.5-7B (b) Llama-2-7B (c) Llama-3-8B (d) Phi-4-14B Figure 2: Sparsity-performance frontiers. Sparsity-performance across Qwen-2.5-7B, Llama-2-7B, Llama-3-8B, and Phi-4-14B. Table 1: Results of controlled sparsity experiments over Qwen-2.5-7B"
        },
        {
            "title": "Baseline",
            "content": "TEAL (Liu et al., 2024) TEAL-transform"
        },
        {
            "title": "Sparsity",
            "content": "PiQA WinoGrande HellaSwag Arc-c MMLU GSM8K"
        },
        {
            "title": "Avg",
            "content": "- 25% 40% 50% 65% 25% 40% 50% 65% 25% 40% 50% 65% 79.71 79.27 78.40 78.62 73. 80.09 79.71 78.56 76.06 80.05 78.40 78.67 76.17 72.85 78.56 77.28 75.02 63.35 72.77 72.30 68.67 61.33 72.69 70.56 69.30 61. 78.93 72.77 73.09 69.77 62.67 78.65 77.73 75.74 67.30 78.58 78.02 76.48 70.09 51.11 51.19 52.65 51.02 42. 51.79 51.28 50.00 44.20 51.37 50.94 50.85 42.92 71.93 71.30 70.20 67.72 54.95 71.56 69.93 67.28 56.06 71.51 70.54 67.99 59. 83.32 82.87 78.32 71.42 34.95 83.09 77.18 71.49 32.60 83.93 79.83 72.25 38.36 72.98 72.83 71.66 68.93 55. 72.99 71.52 68.62 56.93 73.02 71.38 69.26 58.34 Llama-2-7B. On Llama-2-7B, WINA again shows strong performance under various sparsity constraints. As shown in Table 2, WINA achieves the highest average accuracy at 25% sparsity, outperforming both TEAL-based baselines and the full model. While performance naturally degrades at the extreme 65% sparsity level, WINA still offers the best accuracy, suggesting its robustness under aggressive pruning. Llama-3-8B. The results on Llama-3-8B further emphasize WINAs resilience to pruning, as summarized in Table 3. While TEAL slightly outperforms at the 25% level, WINA leads in all remaining sparsity configurations, culminating in +1.06% and +2.41% over TEAL at 50% sparsity and 65% sparsity, respectively. Notably, WINA sustains particularly strong performance on reasoning-intensive tasks like GSM8K and ARC Challenge, where other methods show significant drops under compression. These patterns suggest that WINA is not only compression-friendly but also capable of preserving complex decision-making abilities under tight computational budgets. Phi-4-14B. WINA also delivers robust performance on Phi-4-14B across all tested sparsity levels, as detailed in Table 4. It consistently either matches or exceeds the accuracy of both TEAL and TEAL-transform, and achieves the top average score at every sparsity setting. At the highest sparsity of 65%, for instance, WINA improves upon TEAL and TEAL-transform by +2.01% and +0.86%, respectively. Its ability to retain high performance on complex benchmarks such as GSM8K and MMLU, even under severe pruning, highlights its stability. These outcomes demonstrate that WINA can effectively preserve key reasoning mechanisms in large-scale models, making it well-suited for sparsity-constrained deployments. WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Table 2: Results of controlled sparsity experiments over Llama-2-7B"
        },
        {
            "title": "Baseline",
            "content": "TEAL (Liu et al., 2024) TEAL-transform"
        },
        {
            "title": "Sparsity",
            "content": "PiQA Arc-c WinoGrande HellaSwag MMLU GSM8K"
        },
        {
            "title": "Avg",
            "content": "- 25% 40% 50% 65% 25% 40% 50% 65% 25% 40% 50% 65% 79.05 46. 78.18 77.53 77.53 74.43 78.45 77.69 78.07 74.32 78.45 77.91 78.35 74.59 45.99 44.45 41.21 33.87 46.42 45.48 43.77 37.71 46.16 45.56 44.45 37. 68.98 69.85 67.88 67.25 62.12 69.14 68.43 66.54 63.77 69.69 67.32 67.96 63.93 76.00 76.01 75.32 73.57 64. 75.93 75.18 73.48 66.49 75.95 75.52 73.65 66.55 41.82 41.30 38.66 34.71 27.05 41.75 39.22 36.28 29.11 42.14 39.58 36.55 28. 13.87 13.34 11.07 8.79 3.56 13.42 11.05 10.24 3.64 14.10 11.07 9.63 3.18 54.34 54.11 52.49 50.51 44. 54.19 52.84 51.40 45.51 54.42 52.83 51.76 45.82 Table 3: Results of controlled sparsity experiments over Llama-3-8B TEAL (Liu et al., 2024) TEAL-transform"
        },
        {
            "title": "Sparsity",
            "content": "PiQA Arc-c WinoGrande HellaSwag MMLU GSM8K"
        },
        {
            "title": "Avg",
            "content": "- 25% 40% 50% 65% 25% 40% 50% 65% 25% 40% 50% 65% 80.79 53. 80.25 79.11 78.24 73.34 80.85 79.43 77.69 73.23 80.79 79.60 78.35 73.45 53.16 48.98 48.12 37.37 53.50 50.60 48.38 39.51 53.16 50.09 49.06 40. 72.61 73.32 71.82 70.01 63.46 73.16 70.88 69.06 61.96 73.24 71.27 70.32 62.67 79.17 78.85 77.43 74.83 61. 78.85 77.36 75.70 65.25 78.96 77.54 75.12 64.89 62.20 61.85 59.26 54.50 32.07 61.57 59.23 54.82 38.66 61.54 58.82 55.26 38. 50.19 48.07 39.27 27.37 4.17 47.99 40.11 29.49 5.08 48.29 41.85 29.34 7.05 66.38 65.58 62.65 58.51 45. 65.99 62.94 59.19 47.28 66.00 63.20 59.57 47."
        },
        {
            "title": "Baseline",
            "content": "Table 5: (G)FLOPs over different sparsity across diffrent model architecture. Sparsity QWen2.5-7B Llama-2-7B Llama-3-8B PhiBaseline 0.25 0.4 0.5 0.65 7.07 5.44 ( 23.1%) 4.46 ( 36.9%) 3.81 ( 46.1%) 2.83 ( 60.0%) 6.61 4.99 ( 24.5%) 4.02 ( 39.2%) 3.37 ( 49.0%) 2.40 ( 63.7%) 7.50 5.76 ( 23.2%) 4.71 ( 37.2%) 4.01 ( 46.5%) 2.97 ( 60.4%) 14.15 10.74( 24.1%) 8.69 ( 38.6%) 7.33 ( 48.2%) 5.28 ( 62.7%) Acceleration. In addition to performance gains, WINA yields substantial computational acceleration across all evaluated LLMs. As shown in Table 5, WINA reduces the overall (G)FLOPs by up to 60.0% on Qwen-2.5-7B, 63.7% on Llama-2-7B, 60.4% on Llama-3-8B, and 62.7% on Phi-4-14B at the 65% sparsity level. These consistent reductions in floating point operations could translate to faster inference speeds and lower computational costs, validating WINAs effectiveness as practical solution for deployment under tight resource constraints. 8 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Table 4: Results of controlled sparsity experiments over Phi-4-14B"
        },
        {
            "title": "Baseline",
            "content": "TEAL (Liu et al., 2024) TEAL-transform"
        },
        {
            "title": "Sparsity",
            "content": "PiQA WinoGrande HellaSwag Arc-c MMLU GSM8K"
        },
        {
            "title": "Avg",
            "content": "- 25% 40% 50% 65% 25% 40% 50% 65% 25% 40% 50% 65% 81.28 81.07 80.79 80.63 77. 80.96 81.18 79.82 77.64 81.01 81.18 81.39 78.24 76.80 75.45 73.80 71.98 66.06 74.59 74.19 72.38 68.51 75.37 72.45 73.95 70. 81.93 81.92 81.21 80.06 74.26 81.60 80.94 79.79 74.72 81.91 81.44 81.75 77.10 55.97 56.23 54.95 53.84 50. 55.63 54.61 53.92 52.47 56.31 56.06 54.95 51.11 77.06 76.63 75.10 73.52 65.17 76.68 75.99 74.51 66.64 76.60 76.44 75.83 70. 90.22 89.84 88.02 86.13 74.37 89.92 90.07 88.02 77.18 90.22 90.67 87.57 77.10 77.21 76.86 75.98 74.36 68. 76.56 76.50 74.74 69.86 77.57 76.71 75.91 70.72 In this paper, we introduce WINA, training-free sparse activation framework that selects active neurons based on both hidden state magnitudes and the column-wise ℓ2-norms of subsequent weight matrices. By combining these two signals, WINA addresses key limitations of prior methods such as TEAL, which rely solely on hidden state magnitudes and often suffer from suboptimal sparsity-performance trade-offs and distribution mismatch across layers. Our theoretical analysis demonstrates that WINA achieves tighter bound on approximation error compared to existing approaches, under mild assumptions. To bridge the gap between theoretical guarantees and practical deployment in pre-trained LLMs, we further adopted tensor transformation protocol that enforces column-orthogonality in weight matrices without altering model output. Our extensive experiments across multiple LLM architectures and benchmarks also validate WINAs superior performance under controlled sparsity settings, establishing it as new state-of-the-art in the domain of training-free sparse activation."
        },
        {
            "title": "References",
            "content": "Joint speech and text machine translation for up to 100 languages. Nature, 637(8046):587593, 2025. M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C. C. Del Mundo, M. Rastegari, and M. Farajtabar. Llm in flash: Efficient large language model inference with limited memory. arXiv preprint arXiv:2312.11514, 2023. S. Ashkboos, M. L. Croci, M. G. do Nascimento, T. Hoefler, and J. Hensman. Slicegpt: Compress large language models by deleting rows and columns, 2024. URL https://arxiv.org/abs/2401.15024. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu. Qwen technical report, 2023. Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145, 2024. T. Chen, F. E. Curtis, and D. P. Robinson. reduced-space algorithm for minimizing ℓ1-regularized convex functions. SIAM Journal on Optimization, 27(3):15831610, 2017. 9 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference T. Chen, B. Ji, Y. Shi, T. Ding, B. Fang, S. Yi, and X. Tu. Neural network compression via sparse optimization. arXiv preprint arXiv:2011.04868, 2020. T. Chen, T. Ding, B. Ji, G. Wang, Y. Shi, J. Tian, S. Yi, X. Tu, and Z. Zhu. Orthant based proximal stochastic gradient method for ℓ1-regularized optimization. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 1418, 2020, Proceedings, Part III, pages 5773. Springer, 2021a. T. Chen, B. Ji, T. Ding, B. Fang, G. Wang, Z. Zhu, L. Liang, Y. Shi, S. Yi, and X. Tu. Only train once: one-shot neural network training and pruning framework. In Advances in Neural Information Processing Systems, 2021b. T. Chen, T. Ding, B. Yadav, I. Zharkov, and L. Liang. Lorashear: Efficient large language model structured pruning and knowledge recovery. arXiv preprint arXiv:2310.18356, 2023a. T. Chen, T. Ding, Z. Zhu, Z. Chen, H. Wu, I. Zharkov, and L. Liang. Otov3: Automatic architecture-agnostic neural network training and compression from structured pruning to erasing operators. arXiv preprint arXiv:2312.09411, 2023b. T. Chen, L. Liang, T. Ding, Z. Zhu, and I. Zharkov. Otov2: Automatic, generic, user-friendly. arXiv preprint arXiv:2303.06862, 2023c. T. Chen, X. Qu, D. Aponte, C. Banbury, J. Ko, T. Ding, Y. Ma, V. Lyapunov, I. Zharkov, and L. Liang. Hesso: Towards automatic efficient and user friendly any neural network training and pruning. arXiv preprint arXiv:2409.09085, 2024. M. Cheng, S. L. Blodgett, A. DeVrio, L. Egede, and A. Olteanu. Dehumanizing machines: Mitigating anthropomorphic behaviors in text generation systems. arXiv preprint arXiv:2502.14019, 2025. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168. Y. Dong, Z. Liu, Y. Xu, Y. Cui, W. Che, T. Sun, and T. Liu. Qwen2: Scaling up language models with data mixture of expert quality, 2024. URL https://huggingface.co/Qwen/Qwen2-7B. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. E. Frantar and D. Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noach, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a. S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both weights and connections for efficient neural networks, 2015b. Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang. Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. A. Hendy, M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr, H. Matsushita, Y. J. Kim, M. Afify, and H. H. Awadalla. How good are gpt models at machine translation? comprehensive evaluation, 2023. URL https://arxiv. org/abs/2302.09210. Z. Hui, Y. Li, T. Chen, C. Banbury, K. Koishida, et al. Winclick: Gui grounding with multimodal large language models. arXiv preprint arXiv:2503.04730, 2025. WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. J. Ko, S. Kim, T. Chen, and S.-Y. Yun. Distillm: Towards streamlined distillation for large language models. arXiv preprint arXiv:2402.03898, 2024. J. Ko, T. Chen, S. Kim, T. Ding, L. Liang, I. Zharkov, and S.-Y. Yun. Distillm-2: contrastive approach boosts the distillation of llms. arXiv preprint arXiv:2503.07067, 2025. D. Lee, J.-Y. Lee, G. Zhang, M. Tiwari, and A. Mirhoseini. Cats: Contextually-aware thresholding for sparsity in large language models, 2024. URL https://arxiv.org/abs/2404.08763. D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. J. Li, T. Tang, W. X. Zhao, J.-Y. Nie, and J.-R. Wen. Pre-trained language models for text generation: survey. ACM Computing Surveys, 56(9):139, 2024. Y. Li, S. Gu, C. Mayer, L. V. Gool, and R. Timofte. Group sparsity: The hinge between filter pruning and decomposition for network compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80188027, 2020. S. Lin, R. Ji, Y. Li, C. Deng, and X. Li. Toward compact convnets via structure-sparsity regularized filter pruning. IEEE transactions on neural networks and learning systems, 31(2):574588, 2019. J. Liu, P. Ponnusamy, T. Cai, H. Guo, Y. Kim, and B. Athiwaratkun. Training-free activation sparsity in large language models, 2024. URL https://arxiv.org/abs/2408.14690. X. Qu, D. Aponte, C. Banbury, D. P. Robinson, T. Ding, K. Koishida, I. Zharkov, and T. Chen. Automatic joint structured pruning and quantization for efficient neural network training and compression. arXiv preprint arXiv:2502.16638, 2025. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. H. Touvron, T. Lavril, G. Izacard, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama 2: Open foundation and fine-tuned chat models, 2023. M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans, L. Beyer, Y. Xia, B. Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. H. Wang, S. Ma, R. Wang, and F. Wei. Q-sparse: All large language models can be fully sparsely-activated. arXiv preprint arXiv:2407.10969, 2024. W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks. arXiv preprint arXiv:1608.03665, 2016a. W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks, 2016b. F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You. Go wider instead of deeper. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 87798787, 2022. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. H. Zhao, T. Chen, and Z. Wang. On the robustness of gui grounding models against image attacks. arXiv preprint arXiv:2504.04716, 2025. WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference T. Zhuang, Z. Zhang, Y. Huang, X. Zeng, K. Shuang, and X. Li. Neuron-level structured pruning using polarization regularizer. Advances in Neural Information Processing Systems, 33, 2020. S. Zuo, Q. Zhang, C. Liang, P. He, T. Zhao, and W. Chen. Moebert: from bert to mixture-of-experts via importanceguided adaptation. arXiv preprint arXiv:2204.07675, 2022. WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Proof of Lemma 3.1 Proof. Let original network output and the gated output via general-format sparsification is: (x) = {ixi = 0} be the set of indices of zero elements at x. The output deviation between the =0 (xI=0 x) 2 = iI=0 = ( iI=0 2 xiW,i 2 xiW,i) iI=0 x2 W,i xiW,i) ( iI=0 xjxiW ,jW,i 2 2 + ijI=0 xjxiW ,jW,i = jI=0 = iI=0 The expected output deviation for WINA is: 2 WINA eWINA = xI=0 2 x2 W,i 2 + ijI=0 = iI= WINA WINA xjxiW ,jW,i. Since is assumed to be column orthogonal, the cross-term expectations vanish, and the expected output error is determined solely by the main term: eWINA = iI= WINA x2 W,i 2 2. Because WINA sparsification sets the smallest xici terms to zero, we have the mask of WINA reaches out the lower bound of approximation error for single layer network, i.e., gWINA(x) = argmin g{0,1}n (x x) 2 . (8) Thus, the above indicates that WINA sparsification achieves the tight lower bound of the approximation error, including those of TEAL and CATS. Proof of Theorem 3.2 Proof. We prove this by mathematical induction. Step 1: Base case = 2. The output error for sparse activation parameterized via mask is: y(2) = (2) = (2) = (2) = (2) y(2) (y(1) (y(1) (y(1) (y(1) g(2) g(2) g(2) g(2) ) (2)y(1) ) (2)y(1) y(1) y(1) ) + (2) ) + (2) + (2)y(1) (2)y(1) (y(1) y(1) ) (W (1)x g(1) (1)x) Let: (1) = diag(g(1) 1), (2) = diag(g(2) 1), (1) = diag(g(1) ). 13 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Then, let and be 1)) (g(2) (y(1) = (2) = (2) (x g(1) (W (1) = (2)(2)W (1)M (1)x (x (g(1) = (2)W (1) 1)) ) (g(2) 1)) = (2)W (1)(1)x. Since Eu + 2 = Eu 2 2 + Ev E[uv] = E[x(1) + 2E(uv), the expected value of the cross-term is: )(2)W (1)M (1)x] (W (1) (W (2) )] ) ) (W (2) ](W (1) ]E[xx ) (W (2) ) = E[tr(W (2)(2)W (1)M (1)xx(1) ]W (1)E[M (1)(1) = tr (W (2)E[(2) (W (1) (W (2) ) ) ) Since E[M11] = E[g(1) deviation via sparse activation g, (g(1) 1)] = 0, the cross-term expectation E[uv] is zero. Thus, the expected output e(2) 2 = E[u + = E[W (2) ] g(2) (y(1) y(1) ) ] + E[W (2) (W (1)x g(1) (1)x) ] (9) Upon Lemma 3.1, we have that E[W (2) Next, we compare E[W (2) 2 (1) WINA (1)x) (1) ) 2 ] given ] E[W (2) (2) WINA and (W (1)x (2) TEAL. (1) TEAL (1)x) ] (W (1)x (1) g(2) (y(1) (y E[W (2) = E[ jI=0(g(2)) g(2) iI=0(g(2)) 2 y(1) ) (1) (1) i ] (W (2) ,j ) (2) ,i ] =E[ jI=0(g(2)) (1) ) (y 2 (2) 2 ,j + i,jI=0(g(2)) ij (1) (1) (W (2) ,j ) (2) ,i ] = jI=0(g(2)) (2) ) (c 2E(y (1) ) 2 + i,jI=0(g(2)) ij (W (2) ,j ) (2) ,i E[y (1) (1) ] = jI=0(g(2)) (2) ) (c 2E(y (1) ) 2, where the last line is due to (2) is column-orthogonal, the cross-terms expectation is zero. Because WINA sparsification sets the smallest (y E[W (2) y(1) gWINA g(2) (y(1) gWINA) (1) (2) ) ] E[W (2) 2 terms to zero, we have: (y(1) gTEAL g(2) y(1) gTEAL ) ] Therefore, we have that (2) WINA (2) TEAL. Step 2: Inductive proof for > 2. Assume for some 2 that (N ) WINA (N ) TEAL. Define the exact output of (N + 1) layer network: = (N +1)y(N ), y(N ) = (N ) (1)x 14 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference The output via mask g(N +1) is that y(N +1) = (N +1) = (N +1) (y(N ) ((y(N ) g(N +1) g(N +1) ) (N +1)y(N ) ) y(N ) ) + (N +1) (y(N ) y(N ) ) The expected output deviation is: eN +1 = EW (N +1) (y(N ) g(N +1) y(N ) ) 2 + EW (N +1) (y(N ) y(N ) 2, ) the cross-term zeros out because of the assumption. Upon induction assumption, for the second term, we have that EW (N +1) (y(N ) gWINA y(N ) ) EW (N +1) (y(N ) gTEAL y(N ) 2. ) For the first term, we have that E[W (N +1) = E[ (y(N ) y(N ) g(N +1) (N ) ) (N ) 2 ] (W jI=0(g(N +1)) iI=0(g(N +1)) (N +1) 2E(y ) (c = = jI=0(g(N +1)) jI=0(g(N +1)) (N +1) (c 2E(y ) (N ) 2, ) (N +1) ,j ) (N +1) ,i ] (N ) ) 2 + (W i,jI=0(g(N +1)) ij (N +1) ,j ) (N +1) ,i E[y (N ) (N ) ] where the last line is due to (N +1) is column-orthogonal, the cross-terms expectation is zero. Since WINA retains the largest (N ) cN +1 , thus: E[W (N +1) (y(N ) (N +1) WINA y(N ) 2 ] E[W (N +1) (y(N ) ) (N +1) TEAL y(N ) 2 ]. ) Consequently, we reach the conclusion that (N +1) WINA (N +1) TEAL . Proof of Lemma 3.4 (10) (11) (12) (13) Proof. Let be the error term of the output via sparse activation parameterized with g, = (x (1 g)) = i=1 Wi,xi (1 gi). Using Taylor expansion and ignoring higher-order terms (assuming is small), the output deviation given an activation function is: (Wi,x + g,i) (Wi,x) (W,ix)i. 15 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Thus, the expected squared output deviation between the original output and the gated output approximates to: eg = Ef (W + g) (W x) 2 = (Wi,x + g,i) (Wi,x) 2 [ (Wi,x)g,i 2 ] i=1 i=1 i=1 i=1 i=1 i=1 = = = = E[ 2f (Wi,x)2 g,i] E[ 2f (Wi,x)(Wi,xi (1 gi)) 2 ] E[ 2f (Wi,x)] i=1 E(Wi,xi (1 gi)) 2 E[ 2f (Wi,x)] iI=0(g) E[c2 x2 ] Because WINA sparsification select the smallest x2 c2 terms to zero, we have that eWINA eTEAL. (14) Proof of Theorem 3.5 Proof. We prove this by mathematical induction. Step 1: Base case = 2. The output error for sparse activation via g(2) is: y(2) y(2) = (2) = (2) = (2) g(2) (f (y(1) ) g(2) (f (y(1) g(2) (f (y(1) ) (2)f (y(1) ) ) (2)f (y(1) ) (y(1) )) + (2) ) + (2)f (y(1) (f (y(1) ) (2)f (y(1) )) . ) (y(1) ) Let: Then, let and be (1) = diag(g(1) ), (2) = diag(g(2) 1). = (2) (f (y(1) ) (g(2) 1)) = (2)M (2)f (W (1)M (1)x) Let = (2) [f (W (1)M (1)x) (W (1)x)]. = (2) (2), then the expected value of the cross-term becomes: E[uv] = E[f (W (1)M (1)x) (W (1)x)] = E[f (W (1)M (1)x) (W (1)x)] = Dii (M (2) (2)M (2)f (W (1)M (1)x) (2) DM (2)f (W (1)M (1)x) )ii (f (W (1)M (1)x)i (W (1)x)i) (W (1)M (1)x)i When (2) = 1, (M (2) )ii = 0, and the corresponding terms disappear. When (2) = 0, (M (2) )ii = 1. Therefore: [uv] = ig(2) =0 Dii (f (W (1)M (1)x)i (W (1)x)i) (W (1)M (1)x)i 16 WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference Since follows symmetric distribution with mean 0, and (1) has orthogonal columns, the distributions of (1)M (1)x and (1)x are symmetric. For any activation function , the cross-term cancels out under the symmetric distribution. Thus, the expected output deviation becomes 2 2 e(2) = E[u + ] + E[v ] = E[u = E[W (2)M (2)f (W (1)M (1)x) Here, the latter one yields the below due to Lemma 3.4. (1) WINAx) (W (1)x)] [f (W (1)M 2 Next, we compare the former term. We have that: E[W (2) =E ) g(2) (y E[W (2) (f (y(1) 2 jI=0(g(1)) =E jI=0(g(1)) iI=0(g(1)) (1) ) (y 2 ] ] + E[W (2) [f (W (1)M (1)x) (W (1)x)] 2 2] ] E[W (2) [f (W (1)M (1) TEALx) (W (1)x)] 2 ]. ) (2)f (y(1) ) (1) (1) ) (W ) (y ] (2) ,j ) 2 (2) ,i (2) ,j + i,jI=0(g(1)) ij (y (1) (1) ) (y )(W (2) ,j ) (2) ,i = jI=0(g(1)) = jI=0(g(1)) (2) ) 2Ef (y (1) g,j ) 2 (c (W (2) ,j ) (2) ,i Ef (y (1) )f (y (1) ) + i,jI=0(g(1)) ij (2) ) 2Ef (y (1) g,j ) 2, (c where the last line is due to (2) being column-orthogonal, thereby the cross-terms expectation is zero. Because WINA sparsification sets the smallest (f (y (2) WINA) (2)f (y(1) ) (f (y(1) ) E[W (2) Thus, we have that (1) g,j )c 2 (2) ) ] E[W (2) (2) WINA (2) TEAL. 2 terms to zero, we have that (f (y(1) ) (2) TEAL) (2)f (y(1) ) Step 2: Inductive proof for > 2. Assume for 2, the below holds (N ) WINA Consider the output of (N + 1) layers network, i.e., y(N +1) The output deviation via sparse activation of is: y(N +1) =W (N +1) =W (N +1) y(N +1) (f (y(N ) ((f (y(N ) ) g(N +1) ) g(N +1) (N ) TEAL. = (N +1)f (y(N ) ). ) (N +1)f (y(N ) ) (y(N ) ) )) + (N +1) (f (y(N ) ) (y(N ) )) The expected output deviation is: = EW (N +1) ) g(N +1) the cross-term zeros out because of the assumption. (f (y(N ) eN +1 (y(N ) 2 + EW (N +1) (f (y(N ) ) (y(N ) )) 2, )) Upon the induction assumption, the second term yields that EW (N +1) For the first term, we have that (f (y(N ) gWINA) (y(N ) 2 EW (N +1) (f (y(N ) gTEAL) (y(N ) 2. )) )) EW (N +1) = jI=0(g(N )) (f (y(N ) (N ) (y g,j ) ) g(N +1) (N +1) ,j 2 (y(N ) 2 )) 2 2 + i,jI=0(g(N )) ij (y (N ) g,i )f (y (N ) g,j )(W (N +1) ,j ) (N +1) ,i = jI=0(g(N )) = jI=0(g(N )) (y (N ) g,j ) 2 (N +1) ,j 2 2 (y (N ) g,j ) 2c2 . 17 2 ] (15) (16) WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference (N ) Since WINA retains the largest E[f (y g,j ) (N +1) WINA (y(N ) (f (y(N ) EW (N +1) Consequently, we conclude that ) 2c j ], therefore: (N +1) WINA (N +1) TEAL . 2 EW (N +1) (f (y(N ) ) )) (N +1) TEAL (y(N ) 2. )) Resources Used & Limitations The total run time of our experiments were run using two A100 80GB GPUs for couple of days. In terms of limitations, we focus the comparisons of our approach with current leading methodologies for sparse activation (i.e., TEAL (Liu et al., 2024) and CATS (Lee et al., 2024)). Naturally, we are unable to compare with all existing sparse activation methodologies and prior works, but, instead, we use these TEAL and CATS as they currently represent the current upper bound of optimal performance-efficiency trade-offs; as such, we use these approaches to compare against in order to ensure our performance tests and comparisons are robust and fair."
        }
    ],
    "affiliations": [
        "Microsoft",
        "New York University",
        "Renmin University of China",
        "South China University of Technology"
    ]
}